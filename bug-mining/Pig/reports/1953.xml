<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 20:02:58 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF Jira</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[PIG-3655] BinStorage and InterStorage approach to record markers is broken</title>
                <link>https://issues.apache.org/jira/browse/PIG-3655</link>
                <project id="12310730" key="PIG">Pig</project>
                    <description>&lt;p&gt;The way that the record readers for these storage formats seek to the first record in an input split is to find the byte sequence 1 2 3 110 for BinStorage or 1 2 3 19-21|28-30|36-45 for InterStorage. If this sequence occurs in the data for any reason (for example the integer 16909166 stored big endian encodes to the byte sequence for BinStorage) other than to mark the start of a tuple it can cause mysterious failures in pig jobs because the record reader will try to decode garbage and fail.&lt;/p&gt;

&lt;p&gt;For this approach of using an unlikely sequence to mark record boundaries, it is important to reduce the probability of the sequence occuring naturally in the data by ensuring that your record marker is sufficiently long. Hadoop SequenceFile uses 128 bits for this and randomly generates the sequence for each file (selecting a fixed, predetermined value opens up the possibility of a mean person intentionally sending you that value). This makes it extremely unlikely that collisions will occur. In the long run I think that pig should also be doing this.&lt;/p&gt;

&lt;p&gt;As a quick fix it might be good to save the current position in the file before entering readDatum, and if an exception is thrown seek back to the saved position and resume trying to find the next record marker.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12687778">PIG-3655</key>
            <summary>BinStorage and InterStorage approach to record markers is broken</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="szita">&#193;d&#225;m Szita</assignee>
                                    <reporter username="jeffplaisance">Jeff Plaisance</reporter>
                        <labels>
                    </labels>
                <created>Wed, 8 Jan 2014 01:08:01 +0000</created>
                <updated>Mon, 15 Sep 2025 11:29:04 +0000</updated>
                            <resolved>Sat, 22 Jul 2017 11:40:59 +0000</resolved>
                                    <version>0.2.0</version>
                    <version>0.3.0</version>
                    <version>0.4.0</version>
                    <version>0.5.0</version>
                    <version>0.6.0</version>
                    <version>0.7.0</version>
                    <version>0.8.0</version>
                    <version>0.8.1</version>
                    <version>0.9.0</version>
                    <version>0.9.1</version>
                    <version>0.9.2</version>
                    <version>0.10.0</version>
                    <version>0.11</version>
                    <version>0.10.1</version>
                    <version>0.12.0</version>
                    <version>0.11.1</version>
                                    <fixVersion>0.18.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>13</watches>
                                                                                                                <comments>
                            <comment id="13864954" author="jeffplaisance" created="Wed, 8 Jan 2014 01:10:35 +0000"  >&lt;p&gt;see also &lt;a href=&quot;https://issues.apache.org/jira/browse/PIG-648&quot; title=&quot;BinStorage fails when it finds markers unexpectedly in the data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PIG-648&quot;&gt;&lt;del&gt;PIG-648&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/PIG-691&quot; title=&quot;BinStorage skips tuples when ^A is present in data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PIG-691&quot;&gt;&lt;del&gt;PIG-691&lt;/del&gt;&lt;/a&gt;, and &lt;a href=&quot;https://issues.apache.org/jira/browse/PIG-814&quot; title=&quot;Make Binstorage more robust when data contains record markers&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PIG-814&quot;&gt;&lt;del&gt;PIG-814&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13997678" author="vanaepi" created="Wed, 14 May 2014 15:55:07 +0000"  >&lt;p&gt;I suspect I am suffering from this issue. We are generating serialized objects in an UDF returning DataByteArray. The script runs in two mapred jobs and fails during the map stage of the second job with the following error message :&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;java.lang.RuntimeException: Unexpected data type 48 found in stream.&lt;br/&gt;
at org.apache.pig.data.BinInterSedes.readDatum(BinInterSedes.java:422)&lt;br/&gt;
at org.apache.pig.data.BinInterSedes.readDatum(BinInterSedes.java:313)&lt;br/&gt;
at org.apache.pig.data.utils.SedesHelper.readGenericTuple(SedesHelper.java:144)&lt;br/&gt;
at org.apache.pig.data.BinInterSedes.readDatum(BinInterSedes.java:344)&lt;br/&gt;
at org.apache.pig.impl.io.InterRecordReader.nextKeyValue(InterRecordReader.java:113)&lt;br/&gt;
at org.apache.pig.impl.io.InterStorage.getNext(InterStorage.java:77)&lt;br/&gt;
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.nextKeyValue(PigRecordReader.java:211)&lt;br/&gt;
at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:483)&lt;br/&gt;
at org.apache.hadoop.mapreduce.task.MapContextImpl.nextKeyValue(MapContextImpl.java:76)&lt;br/&gt;
at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.nextKeyValue(WrappedMapper.java:85)&lt;br/&gt;
at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:139)&lt;br/&gt;
at org.apache.hadoop.mapred.MapTask.runNew&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I suspect that by accident the exact same sequence that is used as a record splitter also occurs somewhere in one of our serialized objects but the amount of records is so large that I&apos;m unsure how to go about isolating the issue in order to reproduce it. A custom UDF returning the exact same byte sequence does not cause the script to fail, which seems odd imo.&lt;/p&gt;</comment>
                            <comment id="14496974" author="mwagner" created="Wed, 15 Apr 2015 20:58:07 +0000"  >&lt;p&gt;This happened to us as well. The &quot;Unexpected data type&quot; message is a signal that this has happened. It can also show up as ClassCastExceptions further down in the pipeline. I have a patch which I&apos;ll prepare for trunk.&lt;/p&gt;</comment>
                            <comment id="16082255" author="szita" created="Tue, 11 Jul 2017 14:13:28 +0000"  >&lt;p&gt;This ticket was not updated for a while so unless any objections I&apos;m happy to take this over as our customers have also encountered this problem.&lt;/p&gt;</comment>
                            <comment id="16082281" author="szita" created="Tue, 11 Jul 2017 14:30:24 +0000"  >&lt;p&gt;I agree that we should follow Hadoop&apos;s approach and generate a longer and random record marker instead of 0x010203.&lt;br/&gt;
I propose we use this ticket for fixing InterStorage, doing the same approach for BinStorage would cause an incompatibility for files already written in the past (and I also saw some other problems in BinStorage e.g. writing bigdecimals..)&lt;/p&gt;

&lt;p&gt;So for InterStorage we&apos;re not bound by any contract and we can change its format freely.&lt;br/&gt;
I uploaded a fix &lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12876629/12876629_PIG-3655.0.patch&quot; title=&quot;PIG-3655.0.patch attached to PIG-3655&quot;&gt;PIG-3655.0.patch&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;: InterRecordWriter will now generate a random record marker the same way &lt;a href=&quot;https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/SequenceFile.java#L870&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Hadoop does it for a SequenceFile&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;InterRecordReader will read this 16 bytes long magic sequence at initialization time from the beginning of the file (+3 because I also kept the original marker 0x010203). Later during reading records it will always compare the record marker seen with the one the reader was initialized with.&lt;br/&gt;
In theory this should result in much less collisions with data, the original sequence was 3+1 bytes (+1 for marker a Tuple type), but now we have 20 bytes in total (original 3 + 16 random + 1 tuple type)&lt;/p&gt;</comment>
                            <comment id="16082498" author="szita" created="Tue, 11 Jul 2017 16:46:32 +0000"  >&lt;p&gt;I realized some users might find the new overhead too much for their use case, so I&apos;m attaching a new patch &lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12876653/12876653_PIG-3655.1.patch&quot; title=&quot;PIG-3655.1.patch attached to PIG-3655&quot;&gt;PIG-3655.1.patch&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt; with the record marker sequence length being configurable.&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rohini&quot; class=&quot;user-hover&quot; rel=&quot;rohini&quot;&gt;rohini&lt;/a&gt; what&apos;s your opinion?&lt;/p&gt;</comment>
                            <comment id="16083620" author="szita" created="Wed, 12 Jul 2017 08:05:11 +0000"  >&lt;p&gt;Only fixing some imports in &lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12876782/12876782_PIG-3655.2.patch&quot; title=&quot;PIG-3655.2.patch attached to PIG-3655&quot;&gt;PIG-3655.2.patch&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;</comment>
                            <comment id="16083882" author="nkollar" created="Wed, 12 Jul 2017 12:11:45 +0000"  >&lt;p&gt;Unit tests passed, looks good to me!&lt;/p&gt;</comment>
                            <comment id="16084419" author="knoguchi" created="Wed, 12 Jul 2017 18:03:27 +0000"  >&lt;p&gt;While we wait for Rohini (or others) for the review, can you check &lt;tt&gt;TestFRJoin2.testTooBigReplicatedFile&lt;/tt&gt; ?  I think this started failing after your patch.  (You probably just need to adjust the max in the test.)&lt;/p&gt;</comment>
                            <comment id="16084688" author="szita" created="Wed, 12 Jul 2017 21:09:59 +0000"  >&lt;p&gt;Thanks for taking a look Koji!&lt;/p&gt;

&lt;p&gt;Indeed this is just a test issue I will fix it in the next patch. For making it clearer I&apos;ll introduce the following in this test:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; expectedReplicateSize = 2*(3+PigConfiguration.PIG_INTERSTORAGE_RECORDMARKER_SIZE_DEFAULT+1 +1+1)
                          + 1*(3+PigConfiguration.PIG_INTERSTORAGE_RECORDMARKER_SIZE_DEFAULT+1 +1);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt; I&apos;m also running unit tests now on trunk, so I&apos;ll wait until those are finished before attaching a new patch (I&apos;ve been running unit tests on 0.12.0 first because the customer is on that version + E2E tests)&lt;/p&gt;</comment>
                            <comment id="16084853" author="rohini" created="Wed, 12 Jul 2017 22:40:40 +0000"  >&lt;p&gt;SequenceFile uses a sync marker every couple of records (after SYNC_INTERVAL bytes) to be used to determine boundaries while reading across splits unlike InterStorage logic which writes out record marker for every record which is actually bad. &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/hadoop/blob/62857be2110aaded84a93fc9891742a1271b2b85/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/SequenceFile.java#L1397-L1402&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/hadoop/blob/62857be2110aaded84a93fc9891742a1271b2b85/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/SequenceFile.java#L1397-L1402&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Can we get rid of the original record markers and just write the sync marker similar to SequenceFile? We can keep sync interval and sync marker size as configurable parameters.&lt;/p&gt;

&lt;p&gt;For a default 2000 sync interval and 16 byte sync marker size, for 16 records&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;overhead would be roughly same for record sizes of 399 bytes. It would take 48 bytes in both cases&lt;/li&gt;
	&lt;li&gt;Any records with lesser size will have lesser overhead. For eg: If record size is 125 bytes, 3 byte record marker will have 48 bytes overhead for 16 records while there will be only one 16 byte sync marker.&lt;/li&gt;
	&lt;li&gt;For records of size (especially case of bags) bigger than 399 bytes, it will have higher overhead than 3 byte markers.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;If we use 10 byte as default sync marker size, for 10 records&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Overhead would be roughly same for record sizes of 600 bytes. It would take 30 bytes in both cases.&lt;/li&gt;
	&lt;li&gt;Any records with lesser size will have lesser overhead. For eg: If record size is 200 bytes, 3 byte record marker will have 30 bytes overhead for 10 records while there will be only one 10 byte sync marker.&lt;/li&gt;
	&lt;li&gt;For records of size (especially case of bags) bigger than 600 bytes, it will have higher overhead than 3 byte markers.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;So 10 byte sync marker size should be decent for our user cases. Anyone running into collision can increase it to 16 bytes.&lt;/p&gt;</comment>
                            <comment id="16084862" author="rohini" created="Wed, 12 Jul 2017 22:47:44 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=szita&quot; class=&quot;user-hover&quot; rel=&quot;szita&quot;&gt;szita&lt;/a&gt;,&lt;br/&gt;
   In future, you can consider using mock.Storage for tests like these. Simplifies test writing by not having to create input files or cleaning up output files and also runs faster.&lt;/p&gt;</comment>
                            <comment id="16085398" author="szita" created="Thu, 13 Jul 2017 08:59:04 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rohini&quot; class=&quot;user-hover&quot; rel=&quot;rohini&quot;&gt;rohini&lt;/a&gt; I see that a sync interval approach (rather than per record approach) is resulting less overhead - however, what happens if the record reader is starting to read a split where there are multiple records are to be found before the first sync marker? I think this would result in data loss, because we would be skipping bytes until the marker is found (see 2nd split (row) below):&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;M.size = 10, M.interval = 1000, mr.max.split=1500 (rows below denote splits)

[M] [record(500)] [record(500)] [M] [record(480)-
(20)] [record(500)] [M] [record(500)] [record(470)-
(30)]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="16085832" author="rohini" created="Thu, 13 Jul 2017 15:06:56 +0000"  >&lt;p&gt;In the first split, currently we read past the split end till we reach a record marker. That is why we skip and read after the first record marker in second split. Logic will remain the same except that it will be sync marker now. In the first split, records should be processed after the end till a syncmarker is reached. If the sync marker itself is divided across split boundaries, you will have to read till the next sync marker. Same as the case with record markers.&lt;/p&gt;</comment>
                            <comment id="16085951" author="szita" created="Thu, 13 Jul 2017 16:25:57 +0000"  >&lt;p&gt;I see.. Well actually right now the way we read past a split is to finish the current record, and then check if we have reached / are past the split.end, and we don&apos;t care about record markers there..&lt;/p&gt;

&lt;p&gt;But then I guess this is how we want this to be if we have sync markers with sync intervals: we don&apos;t want to depend on split end, but rather on next sync marker (or the end of file of course).&lt;br/&gt;
Is this what you had in mind?&lt;/p&gt;</comment>
                            <comment id="16085985" author="rohini" created="Thu, 13 Jul 2017 16:51:57 +0000"  >&lt;p&gt;Yes. Same as SequenceFile.Reader.&lt;/p&gt;</comment>
                            <comment id="16091634" author="szita" created="Tue, 18 Jul 2017 14:37:07 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rohini&quot; class=&quot;user-hover&quot; rel=&quot;rohini&quot;&gt;rohini&lt;/a&gt; I&apos;ve attached &lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12877809/12877809_PIG-3655.3.patch&quot; title=&quot;PIG-3655.3.patch attached to PIG-3655&quot;&gt;PIG-3655.3.patch&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt; with the sync interval approach. I&apos;m currently running unit and e2e tests, will update once I have a result.&lt;/p&gt;</comment>
                            <comment id="16093766" author="szita" created="Wed, 19 Jul 2017 20:57:28 +0000"  >&lt;p&gt;I had clean unit test run and a clean e2e full test suite run. Please let me know what you think of the latest patch&lt;/p&gt;</comment>
                            <comment id="16094759" author="rohini" created="Thu, 20 Jul 2017 14:23:34 +0000"  >&lt;p&gt;Few comments:&lt;br/&gt;
1) Instead of using out.size(), you should making it FSDataOutputStream.getPos() and change InterRecordWriter to take FSDataOutputStream in the constructor. DataOutputStream.size() is limited to INTEGER.MAX_VALUE and wraps around on overflow.&lt;/p&gt;

&lt;p&gt;2) Can you add a comment explaining the logic&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; expectedReplicateSize = PigConfiguration.PIG_INTERSTORAGE_SYNCMARKER_SIZE_DEFAULT + 2*(1 +1+1)
	                                       + PigConfiguration.PIG_INTERSTORAGE_SYNCMARKER_SIZE_DEFAULT + 1*(1 +1);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I have couple of more questions. Will catch you on chat for those.&lt;/p&gt;</comment>
                            <comment id="16094798" author="rohini" created="Thu, 20 Jul 2017 15:06:29 +0000"  >&lt;p&gt;Comments on the read logic:&lt;br/&gt;
1) In readSyncFullyOrEOF() method, &lt;tt&gt;if (b == -1) return false&lt;/tt&gt; should be done separately for the first byte read and not inside the while block. If we encounter -1 half way reading sync marker, it should throw the Corrupt data file exception.&lt;br/&gt;
2) skipUntilMarkerOrEOF() should be skipUntilMarkerOrSplitEndOrEOF(). If you have not found the syncmarker after reading till split end, then you need to bail. Currently it reads till EOF which is wasteful. With that change you can also get rid of  &lt;tt&gt;if (in.getPosition()-syncMarker.length &amp;gt; end) return false&lt;/tt&gt; condition in nextKeyValue()&lt;br/&gt;
3) Could you add a test for sync marker starting exactly at second split end. Listing out all we need including what you already have&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Sync marker only at the beginning of the file. Nothing between splits.&lt;/li&gt;
	&lt;li&gt;Sync marker ending exactly at the first split end&lt;/li&gt;
	&lt;li&gt;Sync marker starting exactly at the second split beginning&lt;/li&gt;
	&lt;li&gt;Sync marker in between two split boundaries (sync marker starting at first split and ending at second split)&lt;/li&gt;
	&lt;li&gt;Sync marker after first split end (few bytes after beginning of the second split)&lt;br/&gt;
Can you add these as comments to each test as well to make it more clear. Test names are good, but a comment will help understand better.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="16095132" author="szita" created="Thu, 20 Jul 2017 18:08:03 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rohini&quot; class=&quot;user-hover&quot; rel=&quot;rohini&quot;&gt;rohini&lt;/a&gt; Thanks for useful the comments! I&apos;ve addressed them in &lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12878221/12878221_PIG-3655.4.patch&quot; title=&quot;PIG-3655.4.patch attached to PIG-3655&quot;&gt;PIG-3655.4.patch&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;&lt;br/&gt;
For making the test cases more clear with comments - I decided to illustrate the bytes themselves as they are organized into splits. I think that is worth more than any words.&lt;/p&gt;

&lt;p&gt;I&apos;m starting another run of testing and will update later as usual.&lt;/p&gt;</comment>
                            <comment id="16095229" author="rohini" created="Thu, 20 Jul 2017 19:26:07 +0000"  >&lt;p&gt;Some comments on the latest patch:&lt;br/&gt;
1) Would be good if we got rid of the i &amp;gt; 0 check within the loop. b == -1 check can also be removed when throwing exception&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; ((&lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt;) b != syncMarker[0]) {
         &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; IOException(&lt;span class=&quot;code-quote&quot;&gt;&quot;Corrupt data file, expected sync marker at position &quot;&lt;/span&gt; + in.getPosition());
      } 
      &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; i = 1;                                                           
      &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; (i &amp;lt; syncMarker.length) {                                      
          b = in.read();
          &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; ((&lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt;) b != syncMarker[i]) {
              &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; IOException(&lt;span class=&quot;code-quote&quot;&gt;&quot;Corrupt data file, expected sync marker at position &quot;&lt;/span&gt; + in.getPosition());
          }                                                                                                                  
          ++i;
      }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;2) The split illustration is very useful and makes the test easier to understand. Thanks for adding that. Still it would be good to have the one line explanation of the test in the beginning of the javadoc as well for readability. &lt;/p&gt;

&lt;p&gt;3) Please do shorten the test name of testSyncMarkerOverlappingMarkerWithSplitsWithoutMarkerBeforeAndAfter or move it into testSyncMarkerOverlappingMarker() test case itself.&lt;/p&gt;

&lt;p&gt;4) testSyncMarkerOneMarkerAtBeginningOnly - Test has testInterStorageSyncMarker(1024, 10, 2000L) which means there will only be one split. Can you lower split size so that there is at least 3 splits?&lt;/p&gt;

&lt;p&gt;5) Representation of [ 22 ] has spaces while other record sizes don&apos;t&lt;/p&gt;</comment>
                            <comment id="16095997" author="szita" created="Fri, 21 Jul 2017 09:03:17 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rohini&quot; class=&quot;user-hover&quot; rel=&quot;rohini&quot;&gt;rohini&lt;/a&gt; please see your comments addressed in &lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12878323/12878323_PIG-3655.5.patch&quot; title=&quot;PIG-3655.5.patch attached to PIG-3655&quot;&gt;PIG-3655.5.patch&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The representation of [ 22 ] was deliberately decorated with spaces so that it makes an impression of being a longer sequence of bytes - I think at first look it helps to see all those records through quicker. For the rest of your questions I&apos;ve made the adjustments.&lt;/p&gt;</comment>
                            <comment id="16096588" author="szita" created="Fri, 21 Jul 2017 17:34:39 +0000"  >&lt;p&gt;The test results came back all green.&lt;/p&gt;</comment>
                            <comment id="16096604" author="rohini" created="Fri, 21 Jul 2017 17:42:59 +0000"  >&lt;p&gt;+1.&lt;/p&gt;
</comment>
                            <comment id="16097259" author="szita" created="Sat, 22 Jul 2017 11:40:32 +0000"  >&lt;p&gt;&lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12878323/12878323_PIG-3655.5.patch&quot; title=&quot;PIG-3655.5.patch attached to PIG-3655&quot;&gt;PIG-3655.5.patch&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt; committed to trunk, thanks for the review Rohini, Nandor and Koji!&lt;/p&gt;</comment>
                            <comment id="16098342" author="szita" created="Mon, 24 Jul 2017 13:10:11 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rohini&quot; class=&quot;user-hover&quot; rel=&quot;rohini&quot;&gt;rohini&lt;/a&gt; it seems like Spark is writing some NULLs after the last record, and I made InterStorage more strict than it was: it used to eat and skip over these bytes of garbage but now it&apos;s throwing an &lt;a href=&quot;https://builds.apache.org/job/Pig-spark/lastCompletedBuild/testReport/org.apache.pig.test/TestEvalPipeline/testCogroupAfterDistinct/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;exception&lt;/a&gt; when it finds this NULL. I suppose we could adjust the condition for EOF in &lt;tt&gt;readDataOrEOF&lt;/tt&gt; as per &lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12878609/12878609_PIG-3655.sparkNulls.patch&quot; title=&quot;PIG-3655.sparkNulls.patch attached to PIG-3655&quot;&gt;PIG-3655.sparkNulls.patch&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;</comment>
                            <comment id="16099066" author="rohini" created="Mon, 24 Jul 2017 20:18:34 +0000"  >&lt;blockquote&gt;&lt;p&gt;it seems like Spark is writing some NULLs after the last record&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;  Wouldn&apos;t it be better if we fix the code that writes unnecessary NULLs instead?&lt;/p&gt;</comment>
                            <comment id="16100087" author="szita" created="Tue, 25 Jul 2017 14:14:01 +0000"  >&lt;p&gt;It seems like this is an effect of code in JoinGroupSparkConverter.java and PairRDDFunctions.scala.&lt;/p&gt;

&lt;p&gt;If the POPackage operator returns a &lt;a href=&quot;https://github.com/apache/pig/blob/trunk/src/org/apache/pig/backend/hadoop/executionengine/spark/converter/JoinGroupSparkConverter.java#L211&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;POStatus.STATUS_NULL&lt;/a&gt;, instead of skipping and awaiting the next tuple &lt;a href=&quot;https://github.com/apache/pig/blob/trunk/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigGenericMapReduce.java#L438&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;as MR mode does&lt;/a&gt; the method in Spark converter returns with null. This null will be picked up by the caller in Spark code and then &lt;a href=&quot;https://github.com/apache/spark/blob/v1.6.1/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala#L1113&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;gets written into the (FS) outputstream&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This is how we end up with NULLs in the files.&lt;/p&gt;

&lt;p&gt;What I can think of as a fix is to return with a special kind of Tuple which the record writer knows it has to skip; as it can be seen in &lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12878809/12878809_PIG-3655.sparkNulls.2.patch&quot; title=&quot;PIG-3655.sparkNulls.2.patch attached to PIG-3655&quot;&gt;PIG-3655.sparkNulls.2.patch&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;. Although this seems like a hack, the other option I can think of is to skip NULLs while reading in InterRecordReader - but then it&apos;s just handling the sympthoms and not the root cause.&lt;br/&gt;
Any ideas &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rohini&quot; class=&quot;user-hover&quot; rel=&quot;rohini&quot;&gt;rohini&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=kellyzly&quot; class=&quot;user-hover&quot; rel=&quot;kellyzly&quot;&gt;kellyzly&lt;/a&gt;?&lt;/p&gt;</comment>
                            <comment id="16100880" author="rohini" created="Tue, 25 Jul 2017 22:28:40 +0000"  >&lt;blockquote&gt;&lt;p&gt;What I can think of as a fix is to return with a special kind of Tuple which the record writer knows it has to skip&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;  This is hacky as you mentioned and do not prefer doing that.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;If the POPackage operator returns a POStatus.STATUS_NULL, instead of skipping and awaiting the next tuple as MR mode does the method in Spark converter returns with null. This null will be picked up by the caller in Spark code and then gets written into the (FS) outputstream.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;   Spark also should be doing the same. It should read next record (pkgOp.getNextTuple()) if STATUS_NULL is received instead of returning null. &lt;/p&gt;</comment>
                            <comment id="16101355" author="kellyzly" created="Wed, 26 Jul 2017 08:47:33 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=szita&quot; class=&quot;user-hover&quot; rel=&quot;szita&quot;&gt;szita&lt;/a&gt;:  Can you add a filter to filter the empty tuple in rdd in JoinGroupSparkConverter?&lt;br/&gt;
from &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;   &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; rdd.toJavaRDD().map(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; GroupPkgFunction(pkgOp)).rdd();
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;to &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;  &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; rdd.toJavaRDD().map(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; GroupPkgFunction(pkgOp)).filter(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Function&amp;lt;Tuple, &lt;span class=&quot;code-object&quot;&gt;Boolean&lt;/span&gt;&amp;gt;() {
                @Override
                &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;Boolean&lt;/span&gt; call(Tuple objects) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; Exception {
                    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt;( objects == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;){
                        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;;
                    }  &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt;{
                        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;;
                    }
                }
            }).rdd();

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt; have not fully compiled or tested above code. just for reference.&lt;/p&gt;</comment>
                            <comment id="16101910" author="rohini" created="Wed, 26 Jul 2017 16:42:28 +0000"  >&lt;p&gt;What if there is actually a null tuple in output?&lt;/p&gt;</comment>
                            <comment id="16102348" author="kellyzly" created="Wed, 26 Jul 2017 22:01:11 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=szita&quot; class=&quot;user-hover&quot; rel=&quot;szita&quot;&gt;szita&lt;/a&gt;: can you provide simple script which i can reproduce the error?  sorry i have not read all the comments, so may be my understanding is not right.&lt;/p&gt;

&lt;blockquote&gt;

&lt;p&gt;it seems like Spark is writing some NULLs after the last record&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;  this only happens in this case or all cases in spark mode?   If only in this case, can you provide the script? thanks!&lt;/p&gt;</comment>
                            <comment id="16107163" author="szita" created="Mon, 31 Jul 2017 11:31:13 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rohini&quot; class=&quot;user-hover&quot; rel=&quot;rohini&quot;&gt;rohini&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=kellyzly&quot; class=&quot;user-hover&quot; rel=&quot;kellyzly&quot;&gt;kellyzly&lt;/a&gt; let&apos;s move the Spark related conversation to a new Jira ticket and continue there instead: &lt;a href=&quot;https://issues.apache.org/jira/browse/PIG-5277&quot; title=&quot;Spark mode is writing nulls among tuples to the output &quot; class=&quot;issue-link&quot; data-issue-key=&quot;PIG-5277&quot;&gt;PIG-5277&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13091166">PIG-5277</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="12310050">
                    <name>Regression</name>
                                            <outwardlinks description="breaks">
                                        <issuelink>
            <issuekey id="13205719">PIG-5373</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12876629" name="PIG-3655.0.patch" size="12311" author="szita" created="Tue, 11 Jul 2017 14:14:10 +0000"/>
                            <attachment id="12876653" name="PIG-3655.1.patch" size="17298" author="szita" created="Tue, 11 Jul 2017 16:41:46 +0000"/>
                            <attachment id="12876782" name="PIG-3655.2.patch" size="16642" author="szita" created="Wed, 12 Jul 2017 08:01:31 +0000"/>
                            <attachment id="12877809" name="PIG-3655.3.patch" size="23865" author="szita" created="Tue, 18 Jul 2017 14:35:49 +0000"/>
                            <attachment id="12878221" name="PIG-3655.4.patch" size="26328" author="szita" created="Thu, 20 Jul 2017 18:04:42 +0000"/>
                            <attachment id="12878323" name="PIG-3655.5.patch" size="27208" author="szita" created="Fri, 21 Jul 2017 08:59:58 +0000"/>
                            <attachment id="12878809" name="PIG-3655.sparkNulls.2.patch" size="7008" author="szita" created="Tue, 25 Jul 2017 14:58:03 +0000"/>
                            <attachment id="12878609" name="PIG-3655.sparkNulls.patch" size="837" author="szita" created="Mon, 24 Jul 2017 12:09:18 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>8.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>366778</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 16 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1r80v:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>367089</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>