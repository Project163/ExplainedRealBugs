<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 20:00:40 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF Jira</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[PIG-4443] Write inputsplits in Tez to disk if the size is huge and option to compress pig input splits</title>
                <link>https://issues.apache.org/jira/browse/PIG-4443</link>
                <project id="12310730" key="PIG">Pig</project>
                    <description>&lt;p&gt;Pig sets the input split information in user payload and when running against a table with 10s of 1000s of partitions, DAG submission fails with&lt;br/&gt;
java.io.IOException: Requested data length 305844060 is longer than maximum&lt;br/&gt;
configured RPC length 67108864&lt;/p&gt;</description>
                <environment></environment>
        <key id="12779214">PIG-4443</key>
            <summary>Write inputsplits in Tez to disk if the size is huge and option to compress pig input splits</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="rohini">Rohini Palaniswamy</assignee>
                                    <reporter username="rohini">Rohini Palaniswamy</reporter>
                        <labels>
                    </labels>
                <created>Tue, 3 Mar 2015 22:50:51 +0000</created>
                <updated>Thu, 25 Jun 2015 10:02:45 +0000</updated>
                            <resolved>Fri, 6 Mar 2015 21:24:07 +0000</resolved>
                                    <version>0.14.0</version>
                                    <fixVersion>0.15.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="14345933" author="rohini" created="Tue, 3 Mar 2015 22:56:14 +0000"  >&lt;p&gt;More details on the issue with HCatLoader is in&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/TEZ-2144?focusedCommentId=14337288&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14337288&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/TEZ-2144?focusedCommentId=14337288&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14337288&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Created &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-9845&quot; title=&quot;HCatSplit repeats information making input split data size huge&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-9845&quot;&gt;&lt;del&gt;HIVE-9845&lt;/del&gt;&lt;/a&gt; to fix the issue with HCatalog. &lt;a href=&quot;https://issues.apache.org/jira/browse/TEZ-2144&quot; title=&quot;Compressing MRInput Split Distributor payload&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TEZ-2144&quot;&gt;TEZ-2144&lt;/a&gt; is to have compression so that the serialized size is less and this workaround of serializing the splits to disk will not be required.&lt;/p&gt;</comment>
                            <comment id="14350498" author="rohini" created="Fri, 6 Mar 2015 15:52:06 +0000"  >&lt;p&gt;Patch adds two settings&lt;/p&gt;


&lt;p&gt;1) pig.compress.input.splits&lt;br/&gt;
    This compresses the pig input split information if it is not a FileSplit. Compressing FileSplit did not give much benefits. This can be turned on for HCatLoader till &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-9845&quot; title=&quot;HCatSplit repeats information making input split data size huge&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-9845&quot;&gt;&lt;del&gt;HIVE-9845&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/TEZ-2144&quot; title=&quot;Compressing MRInput Split Distributor payload&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TEZ-2144&quot;&gt;TEZ-2144&lt;/a&gt; are fixed. If &lt;a href=&quot;https://issues.apache.org/jira/browse/TEZ-1244&quot; title=&quot;Typos in RootInputDataInformationEvent docs &quot; class=&quot;issue-link&quot; data-issue-key=&quot;TEZ-1244&quot;&gt;&lt;del&gt;TEZ-1244&lt;/del&gt;&lt;/a&gt; is fixed, we can always turn this of for Tez as compressing the whole payload will compress way better than compressing individual splits.&lt;br/&gt;
2) pig.tez.input.splits.mem.threshold&lt;br/&gt;
    Write input splits to disk in Tez if this threshold is hit. Default is 32MB which is half of the default 64MB protobuf transfer limit.&lt;/p&gt;

&lt;p&gt;This patch also has an additional change that removes MRJobConfig.MAPREDUCE_JOB_CREDENTIALS_BINARY from tez payload as any API that calls TokenCache.obtainTokensForNamenodes on the task will make it fail if pig was run via Oozie. This is because the value will be set to the credential file path in the Oozie launcher job which will not be available on the tasks. This issue was hit by Hive on Tez running with Oozie. &lt;a href=&quot;https://issues.apache.org/jira/browse/MAPREDUCE-3727&quot; title=&quot;jobtoken location property in jobconf refers to wrong jobtoken file&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAPREDUCE-3727&quot;&gt;&lt;del&gt;MAPREDUCE-3727&lt;/del&gt;&lt;/a&gt; is a related issue.&lt;/p&gt;</comment>
                            <comment id="14350810" author="daijy" created="Fri, 6 Mar 2015 20:01:07 +0000"  >&lt;p&gt;+1&lt;/p&gt;</comment>
                            <comment id="14350909" author="rohini" created="Fri, 6 Mar 2015 21:24:07 +0000"  >&lt;p&gt;Committed to trunk. Thanks for the review Daniel.&lt;/p&gt;</comment>
                            <comment id="14356277" author="rohini" created="Wed, 11 Mar 2015 04:35:54 +0000"  >&lt;p&gt;TestTezAutoParallelism.testSkewedJoinIncreaseIntermediateParallelism was actually failing due to &lt;a href=&quot;https://issues.apache.org/jira/browse/TEZ-2192&quot; title=&quot;Relocalization does not check for source&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TEZ-2192&quot;&gt;&lt;del&gt;TEZ-2192&lt;/del&gt;&lt;/a&gt;. Attached patch which is a short term workaround fixes that issue till &lt;a href=&quot;https://issues.apache.org/jira/browse/TEZ-2192&quot; title=&quot;Relocalization does not check for source&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TEZ-2192&quot;&gt;&lt;del&gt;TEZ-2192&lt;/del&gt;&lt;/a&gt; is fixed in Tez. More details in &lt;a href=&quot;https://issues.apache.org/jira/browse/TEZ-2192?focusedCommentId=14356259&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14356259&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/TEZ-2192?focusedCommentId=14356259&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14356259&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14356285" author="daijy" created="Wed, 11 Mar 2015 04:43:37 +0000"  >&lt;p&gt;+1 for &lt;a href=&quot;https://issues.apache.org/jira/browse/PIG-4443&quot; title=&quot;Write inputsplits in Tez to disk if the size is huge and option to compress pig input splits&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PIG-4443&quot;&gt;&lt;del&gt;PIG-4443&lt;/del&gt;&lt;/a&gt;&lt;del&gt;Fix&lt;/del&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/TEZ-2192&quot; title=&quot;Relocalization does not check for source&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TEZ-2192&quot;&gt;&lt;del&gt;TEZ-2192&lt;/del&gt;&lt;/a&gt;.patch.&lt;/p&gt;</comment>
                            <comment id="14356307" author="rohini" created="Wed, 11 Mar 2015 05:05:54 +0000"  >&lt;p&gt;Attaching another patch. Previous one had a bug in adding resource to vertex.&lt;/p&gt;</comment>
                            <comment id="14356315" author="daijy" created="Wed, 11 Mar 2015 05:13:02 +0000"  >&lt;p&gt;+1&lt;/p&gt;</comment>
                            <comment id="14356319" author="rohini" created="Wed, 11 Mar 2015 05:14:25 +0000"  >&lt;p&gt;Committed to trunk. Thanks Daniel for the review.&lt;/p&gt;</comment>
                            <comment id="14595756" author="angel2014" created="Mon, 22 Jun 2015 10:50:38 +0000"  >&lt;p&gt;I have a script in PIG that loads data from Hive using org.apache.hive.hcatalog.pig.HCatLoader. This script works fine in Pig 0.14, but in Pig 0.15 I&apos;m getting this error:&lt;/p&gt;

&lt;p&gt;Requested data length 160452289 is longer than maximum configured RPC length 67108864&lt;/p&gt;

&lt;p&gt;In Pig 0.14 I had to deal with this issue too, but I could always make it work by reducing the number of splits in the Hive tables created by Sqoop (using no more than 60 splits). Is there any special configuration needed?&lt;/p&gt;</comment>
                            <comment id="14596201" author="rohini" created="Mon, 22 Jun 2015 16:46:04 +0000"  >&lt;p&gt;Just to be sure, are you getting this error with Pig on Tez or Mapreduce? And the error is while submitting the job or after it completes and fetching task reports?&lt;/p&gt;</comment>
                            <comment id="14597328" author="angel2014" created="Tue, 23 Jun 2015 08:21:05 +0000"  >&lt;p&gt;I&apos;m only getting this error with Pig on Tez (on Mapreduce works fine) and it is while submitting the job (&quot;Cannot submit DAG&quot;).&lt;/p&gt;

&lt;p&gt;I&apos;m using HDP 2.2.0.0-2041, so in order to test Pig 15, I&apos;ve built this version from its sources, packaged it and added its dependencies to an unique zip file and uploaded to my HDFS. I&apos;m also using LzoCodec to compress the intermediate temporary files (but it doesn&apos;t work without using LzoCodec either).&lt;/p&gt;

&lt;p&gt;I execute these commands to run my script:&lt;/p&gt;

&lt;p&gt;export JAVA_HOME=/usr/jdk64/jdk1.7.0_67&lt;br/&gt;
export HDP_VERSION=2.2.0.0-2041&lt;br/&gt;
export HADOOP_HOME=/usr/hdp/$HDP_VERSION/hadoop&lt;br/&gt;
export HIVE_HOME=/usr/hdp/$HDP_VERSION/hive&lt;br/&gt;
export HCAT_HOME=/usr/hdp/$HDP_VERSION/hive-hcatalog&lt;br/&gt;
export PIG_OPTS=&quot;-DUSE_TEZ_SESSION=true -Dtez.lib.uris=/hdp/apps/2.2.0.0-2041/tez-0.15.0/tez.tar.gz -Dpig.tmpfilecompression=true -Dpig.tmpfilecompression.codec=lzo -Dtez.runtime.intermediate-output.should-compress=true -Dtez.runtime.intermediate-output.is-compressed=true -Dtez.runtime.intermediate-output.compress.codec=com.hadoop.compression.lzo.LzoCodec -Dtez.runtime.intermediate-input.is-compressed=true -Dtez.runtime.intermediate-input.compress.codec=com.hadoop.compression.lzo.LzoCodec -Dtez.runtime.compress.codec=com.hadoop.compression.lzo.LzoCodec&quot;&lt;/p&gt;

&lt;p&gt;./pig-0.15.0-src/bin/pig -useHCatalog -x tez -f myscript.pig &lt;/p&gt;</comment>
                            <comment id="14598365" author="rohini" created="Tue, 23 Jun 2015 21:08:38 +0000"  >&lt;p&gt;That is odd. This patch is supposed to fix the exact same issue and we have been running fine with this patch for months now. Do you see the below message from this patch in your logs?&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;log.info(&lt;span class=&quot;code-quote&quot;&gt;&quot;Writing input splits to &quot;&lt;/span&gt; + inputSplitsDir
                            + &lt;span class=&quot;code-quote&quot;&gt;&quot; &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; vertex &quot;&lt;/span&gt; + vertex.getName()
                            + &lt;span class=&quot;code-quote&quot;&gt;&quot; as the serialized size in memory is &quot;&lt;/span&gt;
                            + splitsSerializedSize + &lt;span class=&quot;code-quote&quot;&gt;&quot;. Configured &quot;&lt;/span&gt;
                            + PigConfiguration.PIG_TEZ_INPUT_SPLITS_MEM_THRESHOLD
                            + &lt;span class=&quot;code-quote&quot;&gt;&quot; is &quot;&lt;/span&gt; + spillThreshold);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If not, I suspect that you still have the old pig.jar in your classpath and it is not Pig 0.15 that is running.  &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I&apos;ve built this version from its sources, packaged it and added its dependencies to an unique zip file and uploaded to my HDFS.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;  This comment also seems to indicate that you have replaced pig jar in hdfs. Not sure why you need pig in HDFS. You need to replace the pig-0.15.0-core-h2.jar in the pig client installation from the node you are running the script.&lt;/p&gt;</comment>
                            <comment id="14599061" author="angel2014" created="Wed, 24 Jun 2015 08:15:03 +0000"  >&lt;p&gt;I&apos;m using Pig 15 ... or so it seems ...&lt;/p&gt;

&lt;p&gt;   INFO  org.apache.pig.Main - Apache Pig version 0.15.0-SNAPSHOT (r: unknown) compiled Jun 18 2015, 15:39:42&lt;/p&gt;

&lt;p&gt;but I don&apos;t see that message because in my case the previous if condition is false (splitsSerializedSize=1163342 , spillThreshold=33554432)&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;                &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt;(splitsSerializedSize &amp;gt; spillThreshold) {
                    ...
                    log.info(&lt;span class=&quot;code-quote&quot;&gt;&quot;Writing input splits to &quot;&lt;/span&gt; + inputSplitsDir
                            + &lt;span class=&quot;code-quote&quot;&gt;&quot; &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; vertex &quot;&lt;/span&gt; + vertex.getName()
                            + &lt;span class=&quot;code-quote&quot;&gt;&quot; as the serialized size in memory is &quot;&lt;/span&gt;
                            + splitsSerializedSize + &lt;span class=&quot;code-quote&quot;&gt;&quot;. Configured &quot;&lt;/span&gt;
                            + PigConfiguration.PIG_TEZ_INPUT_SPLITS_MEM_THRESHOLD
                            + &lt;span class=&quot;code-quote&quot;&gt;&quot; is &quot;&lt;/span&gt; + spillThreshold);
                    ...                       
                } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
                    &lt;span class=&quot;code-comment&quot;&gt;// Send splits via RPC to AM
&lt;/span&gt;                    userPayLoadBuilder.setSplits(splitsProto);
                }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I don&apos;t know if it&apos;s relevant, but comparing the differences in the AM syslog file between Pig 14 and Pig 15, I found this message only while executing Pig 15:&lt;/p&gt;

&lt;p&gt;   INFO  org.apache.tez.client.TezClient - Using org.apache.tez.dag.history.ats.acls.ATSHistoryACLPolicyManager to manage Timeline ACLs&lt;/p&gt;

&lt;p&gt;Do you have any test I could probe in my environment?&lt;/p&gt;

&lt;p&gt;On the other hand, when I said I uploaded an unique zip to my HDFS, I meant only the TEZ 0.7.0 libraries and its dependencies &lt;br/&gt;
(-Dtez.lib.uris=/hdp/apps/2.2.0.0-2041/tez-0.15.0/tez.tar.gz). I have to add this argument to my PIG_OPTS in order to overwrite my /etc/tez/conf/tez-site.xml settings (this file is managed by HDP and, by default, it&apos;s pointing to the TEZ 0.5.2 shipped with HDP 2.2.0.0.2041).&lt;/p&gt;</comment>
                            <comment id="14600374" author="rohini" created="Wed, 24 Jun 2015 23:34:06 +0000"  >&lt;blockquote&gt;&lt;p&gt;Requested data length 160452289 is longer than maximum configured RPC length 67108864&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;but I don&apos;t see that message because in my case the previous if condition is false (splitsSerializedSize=1163342 , spillThreshold=33554432)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;    In your first error you said 160MB (160452289). This time you mentioned 1MB (1163342). Can you recheck? For 1MB, it should not be failing. Also could you paste your full stacktrace so that we can see where exactly is the error.&lt;/p&gt;</comment>
                            <comment id="14600532" author="angel2014" created="Thu, 25 Jun 2015 01:54:25 +0000"  >&lt;p&gt;&lt;b&gt;MASTER_NODE LOG&lt;/b&gt; (from where I&apos;m executing Pig)&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;2015-06-25 03:32:48,978 [PigTezLauncher-0] ERROR org.apache.pig.backend.hadoop.executionengine.tez.TezJob - Cannot submit DAG - Application id: application_1434957727568_0731
org.apache.tez.dag.api.TezException: com.google.protobuf.ServiceException: java.io.EOFException: End of File Exception between local host is: &quot;MASTER_NODE&quot;; destination host is: &quot;AM_NODE&quot;:40698; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
        at org.apache.tez.client.TezClient.submitDAGSession(TezClient.java:476)
        at org.apache.tez.client.TezClient.submitDAG(TezClient.java:391)
        at org.apache.pig.backend.hadoop.executionengine.tez.TezJob.run(TezJob.java:161)
        at org.apache.pig.backend.hadoop.executionengine.tez.TezLauncher$1.run(TezLauncher.java:187)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: com.google.protobuf.ServiceException: java.io.EOFException: End of File Exception between local host is: &quot;MASTER_NODE&quot;; destination host is: &quot;AM_NODE&quot;:40698; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:246)
        at com.sun.proxy.$Proxy32.submitDAG(Unknown Source)
        at org.apache.tez.client.TezClient.submitDAGSession(TezClient.java:469)
        ... 8 more
Caused by: java.io.EOFException: End of File Exception between local host is: &quot;MASTER_NODE&quot;; destination host is: &quot;AM_NODE&quot;:40698; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
        at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
        at org.apache.hadoop.ipc.Client.call(Client.java:1472)
        at org.apache.hadoop.ipc.Client.call(Client.java:1399)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
        ... 10 more
Caused by: java.io.EOFException
        at java.io.DataInputStream.readInt(DataInputStream.java:392)
        at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:966)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;b&gt;AM_NODE LOG&lt;/b&gt; (the requested data length may have changed because I deleted and imported again two of the Hive tables)&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;2015-06-25 03:22:22,437 WARN [Socket Reader #1 for port 42481] ipc.Server: Requested data length 158480507 is longer than maximum configured RPC length 67108864.  RPC came from MASTER_NODE
2015-06-25 03:22:22,438 INFO [Socket Reader #1 for port 42481] ipc.Server: Socket Reader #1 for port 42481: readAndProcess from client MASTER_NODE threw exception [java.io.IOException: Requested data length 158480507 is longer than maximum configured RPC length 67108864.  RPC came from MASTER_NODE]
java.io.IOException: Requested data length 158480507 is longer than maximum configured RPC length 67108864.  RPC came from MASTER_NODE
        at org.apache.hadoop.ipc.Server$Connection.checkDataLength(Server.java:1459)
        at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1521)
        at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:762)
        at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:636)
        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:607)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;b&gt;SPLITS AND SIZES&lt;/b&gt; (all the serialized sizes are less than the spillThreshold=33554432)&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;2015-06-25 03:21:34,277 [main] INFO  org.apache.tez.mapreduce.hadoop.MRInputHelpers - NumSplits: 9, SerializedSize: 1163342
2015-06-25 03:21:34,570 [main] INFO  org.apache.tez.mapreduce.hadoop.MRInputHelpers - NumSplits: 28, SerializedSize: 2207068
2015-06-25 03:21:34,858 [main] INFO  org.apache.tez.mapreduce.hadoop.MRInputHelpers - NumSplits: 28, SerializedSize: 1479632
2015-06-25 03:21:35,517 [main] INFO  org.apache.tez.mapreduce.hadoop.MRInputHelpers - NumSplits: 85, SerializedSize: 17841999
2015-06-25 03:21:35,773 [main] INFO  org.apache.tez.mapreduce.hadoop.MRInputHelpers - NumSplits: 1, SerializedSize: 191480
2015-06-25 03:21:36,087 [main] INFO  org.apache.tez.mapreduce.hadoop.MRInputHelpers - NumSplits: 3, SerializedSize: 581998
2015-06-25 03:21:36,475 [main] INFO  org.apache.tez.mapreduce.hadoop.MRInputHelpers - NumSplits: 11, SerializedSize: 2580419
2015-06-25 03:21:36,897 [main] INFO  org.apache.tez.mapreduce.hadoop.MRInputHelpers - NumSplits: 1, SerializedSize: 166474
2015-06-25 03:21:37,337 [main] INFO  org.apache.tez.mapreduce.hadoop.MRInputHelpers - NumSplits: 4, SerializedSize: 936317
2015-06-25 03:21:37,780 [main] INFO  org.apache.tez.mapreduce.hadoop.MRInputHelpers - NumSplits: 1, SerializedSize: 130474
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I&apos;m getting the same error in another Pig script with 18 HCatLoaders but both scripts work fine in Pig 14/Tez 0.5.2.&lt;/p&gt;</comment>
                            <comment id="14600838" author="rohini" created="Thu, 25 Jun 2015 08:01:59 +0000"  >&lt;p&gt;Ok. I get the problem. The sizes of splits of the 18 loaders add up and cross 64 MB. Since this patch only checks that a single loader&apos;s split size does not exceed threshold, it does not fix the issue. Not sure how it could have worked with Pig 0.14. Only thing I can think of is that if the hadoop version has changed between when Pig 0.14 was installed and Pig 0.15 is installed. This RPC bounds check was only added in Hadoop 2.6 (&lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-10940&quot; title=&quot;RPC client does no bounds checking of responses&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-10940&quot;&gt;&lt;del&gt;HADOOP-10940&lt;/del&gt;&lt;/a&gt;). Can you check if that is the case?&lt;/p&gt;

&lt;p&gt;If you had &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-9845&quot; title=&quot;HCatSplit repeats information making input split data size huge&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-9845&quot;&gt;&lt;del&gt;HIVE-9845&lt;/del&gt;&lt;/a&gt;, you would not be hitting this issue. From the pig side, you can try running with pig -Dpig.compress.input.splits=true and that should mostly fix it. If it still does not work, then you can try pig -Dipc.maximum.data.length=268435456 (256 MB).&lt;/p&gt;</comment>
                            <comment id="14600954" author="angel2014" created="Thu, 25 Jun 2015 10:02:45 +0000"  >&lt;p&gt;In theory, I&apos;m using Hadoop 2.6 in both cases. Adding the parameter -Dipc.maximum.data.length=268435456 ... worked wonders!&lt;/p&gt;

&lt;p&gt;Thanks a lot.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12779215">HIVE-9845</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12777672">TEZ-2144</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="12310050">
                    <name>Regression</name>
                                                                <inwardlinks description="is broken by">
                                        <issuelink>
            <issuekey id="12781038">TEZ-2192</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12703069" name="PIG-4443-1.patch" size="29383" author="rohini" created="Fri, 6 Mar 2015 15:38:49 +0000"/>
                            <attachment id="12703837" name="PIG-4443-Fix-TEZ-2192-2.patch" size="8048" author="rohini" created="Wed, 11 Mar 2015 05:05:54 +0000"/>
                            <attachment id="12703833" name="PIG-4443-Fix-TEZ-2192.patch" size="8388" author="rohini" created="Wed, 11 Mar 2015 04:35:54 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 21 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i26bf3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>