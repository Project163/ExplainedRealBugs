diff --git a/CHANGES.txt b/CHANGES.txt
index 6d424a01e..baa02ad3f 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -131,6 +131,8 @@ IMPROVEMENTS
 
 BUG FIXES
 
+PIG-4404: LOAD with HBaseStorage on secure cluster is broken in Tez (rohini)
+
 PIG-4375: ObjectCache should use ProcessorContext.getObjectRegistry() (rohini)
 
 PIG-4334: PigProcessor does not set pig.datetime.default.tz (rohini)
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/tez/TezJobCompiler.java b/src/org/apache/pig/backend/hadoop/executionengine/tez/TezJobCompiler.java
index f5f49266c..76f3101f4 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/tez/TezJobCompiler.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/tez/TezJobCompiler.java
@@ -27,7 +27,6 @@ import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.security.Credentials;
 import org.apache.hadoop.yarn.api.records.LocalResource;
 import org.apache.hadoop.yarn.exceptions.YarnException;
 import org.apache.pig.PigException;
@@ -58,7 +57,7 @@ public class TezJobCompiler {
     public DAG buildDAG(TezPlanContainerNode tezPlanNode, Map<String, LocalResource> localResources)
             throws IOException, YarnException {
         DAG tezDag = DAG.create(tezPlanNode.getOperatorKey().toString());
-        tezDag.setCredentials(new Credentials());
+        tezDag.setCredentials(tezPlanNode.getTezOperPlan().getCredentials());
         TezDagBuilder dagBuilder = new TezDagBuilder(pigContext, tezPlanNode.getTezOperPlan(), tezDag, localResources);
         dagBuilder.visit();
         return tezDag;
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/TezOperPlan.java b/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/TezOperPlan.java
index aefaf8747..9447fc8c3 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/TezOperPlan.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/TezOperPlan.java
@@ -32,6 +32,7 @@ import java.util.Map;
 import java.util.Set;
 
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.security.Credentials;
 import org.apache.hadoop.yarn.api.records.LocalResource;
 import org.apache.pig.backend.hadoop.executionengine.tez.TezResourceManager;
 import org.apache.pig.backend.hadoop.executionengine.tez.util.TezCompilerUtil;
@@ -54,7 +55,14 @@ public class TezOperPlan extends OperatorPlan<TezOperator> {
 
     private int estimatedTotalParallelism = -1;
 
+    private Credentials creds;
+
     public TezOperPlan() {
+        creds = new Credentials();
+    }
+
+    public Credentials getCredentials() {
+        return creds;
     }
 
     public int getEstimatedTotalParallelism() {
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/optimizer/LoaderProcessor.java b/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/optimizer/LoaderProcessor.java
index ae70234a6..eac99f117 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/optimizer/LoaderProcessor.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/tez/plan/optimizer/LoaderProcessor.java
@@ -24,6 +24,7 @@ import java.util.List;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapreduce.InputFormat;
 import org.apache.hadoop.mapreduce.Job;
 import org.apache.pig.LoadFunc;
@@ -47,13 +48,28 @@ import org.apache.pig.impl.util.UDFContext;
 import org.apache.tez.mapreduce.hadoop.MRInputHelpers;
 
 public class LoaderProcessor extends TezOpPlanVisitor {
-    private Configuration conf;
+
+    private static final Log LOG = LogFactory.getLog(LoaderProcessor.class);
+
+    private TezOperPlan tezOperPlan;
+    private JobConf jobConf;
     private PigContext pc;
-    private static final Log log = LogFactory.getLog(LoaderProcessor.class);
-    public LoaderProcessor(TezOperPlan plan, PigContext pigContext) {
+
+    public LoaderProcessor(TezOperPlan plan, PigContext pigContext) throws VisitorException {
         super(plan, new DependencyOrderWalker<TezOperator, TezOperPlan>(plan));
+        this.tezOperPlan = plan;
         this.pc = pigContext;
-        this.conf = ConfigurationUtil.toConfiguration(pc.getProperties());;
+        this.jobConf = new JobConf(ConfigurationUtil.toConfiguration(pc.getProperties()));
+        // This ensures that the same credentials object is used by reference everywhere
+        this.jobConf.setCredentials(tezOperPlan.getCredentials());
+        this.jobConf.setBoolean("mapred.mapper.new-api", true);
+        this.jobConf.setClass("mapreduce.inputformat.class",
+                PigInputFormat.class, InputFormat.class);
+        try {
+            this.jobConf.set("pig.pigContext", ObjectSerializer.serialize(pc));
+        } catch (IOException e) {
+            throw new VisitorException(e);
+        }
     }
 
     /**
@@ -76,22 +92,25 @@ public class LoaderProcessor extends TezOpPlanVisitor {
         ArrayList<String> inpSignatureLists = new ArrayList<String>();
         ArrayList<Long> inpLimits = new ArrayList<Long>();
 
-        Job job = Job.getInstance(conf);
-        conf = job.getConfiguration();
-        conf.setBoolean("mapred.mapper.new-api", true);
-        conf.setClass("mapreduce.inputformat.class",
-                PigInputFormat.class, InputFormat.class);
-        conf.set("pig.pigContext", ObjectSerializer.serialize(pc));
         List<POLoad> lds = PlanHelper.getPhysicalOperators(tezOp.plan,
                 POLoad.class);
 
+        Job job = Job.getInstance(jobConf);
+        Configuration conf = job.getConfiguration();
+
         if (lds != null && lds.size() > 0) {
-            for (POLoad ld : lds) {
-                LoadFunc lf = ld.getLoadFunc();
-                lf.setLocation(ld.getLFile().getFileName(), job);
+            if (lds.size() == 1) {
+                for (POLoad ld : lds) {
+                    LoadFunc lf = ld.getLoadFunc();
+                    lf.setLocation(ld.getLFile().getFileName(), job);
 
-                // Store the inp filespecs
-                inp.add(ld.getLFile());
+                    // Store the inp filespecs
+                    inp.add(ld.getLFile());
+                }
+            } else {
+                throw new VisitorException(
+                        "There is more than one load for TezOperator "
+                                + tezOp);
             }
         }
 
@@ -139,7 +158,7 @@ public class LoaderProcessor extends TezOpPlanVisitor {
                 try {
                     maxCombinedSplitSize = Long.parseLong(tmp);
                 } catch (NumberFormatException e) {
-                    log.warn("Invalid numeric format for pig.maxCombinedSplitSize; use the default maximum combined split size");
+                    LOG.warn("Invalid numeric format for pig.maxCombinedSplitSize; use the default maximum combined split size");
                 }
             }
             if (maxCombinedSplitSize > 0)
@@ -159,6 +178,7 @@ public class LoaderProcessor extends TezOpPlanVisitor {
         try {
             tezOp.getLoaderInfo().setLoads(processLoads(tezOp));
         } catch (Exception e) {
+            e.printStackTrace();
             throw new VisitorException(e);
         }
     }
diff --git a/src/org/apache/pig/backend/hadoop/hbase/HBaseStorage.java b/src/org/apache/pig/backend/hadoop/hbase/HBaseStorage.java
index bec38e043..064d8758f 100644
--- a/src/org/apache/pig/backend/hadoop/hbase/HBaseStorage.java
+++ b/src/org/apache/pig/backend/hadoop/hbase/HBaseStorage.java
@@ -16,8 +16,6 @@
  */
 package org.apache.pig.backend.hadoop.hbase;
 
-import java.io.DataInput;
-import java.io.DataOutput;
 import java.io.IOException;
 import java.lang.reflect.InvocationTargetException;
 import java.lang.reflect.Method;
@@ -48,8 +46,8 @@ import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hbase.HBaseConfiguration;
-import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.client.Delete;
+import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.client.Scan;
 import org.apache.hadoop.hbase.filter.BinaryComparator;
@@ -88,6 +86,7 @@ import org.apache.pig.ResourceSchema.ResourceFieldSchema;
 import org.apache.pig.StoreFuncInterface;
 import org.apache.pig.StoreResources;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigSplit;
+import org.apache.pig.backend.hadoop.executionengine.shims.HadoopShims;
 import org.apache.pig.backend.hadoop.hbase.HBaseTableInputFormat.HBaseTableIFBuilder;
 import org.apache.pig.builtin.FuncUtils;
 import org.apache.pig.builtin.Utf8StorageConverter;
@@ -811,7 +810,9 @@ public class HBaseStorage extends LoadFunc implements StoreFuncInterface, LoadPu
         List<Class> classList = new ArrayList<Class>();
         classList.add(org.apache.hadoop.hbase.client.HTable.class); // main hbase jar or hbase-client
         classList.add(org.apache.hadoop.hbase.mapreduce.TableSplit.class); // main hbase jar or hbase-server
-        classList.add(com.google.common.collect.Lists.class); // guava
+        if (!HadoopShims.isHadoopYARN()) { //Avoid shipping duplicate. Hadoop 0.23/2 itself has guava
+            classList.add(com.google.common.collect.Lists.class); // guava
+        }
         classList.add(org.apache.zookeeper.ZooKeeper.class); // zookeeper
         // Additional jars that are specific to v0.95.0+
         addClassToList("org.cloudera.htrace.Trace", classList); // htrace
@@ -1208,9 +1209,10 @@ public class HBaseStorage extends LoadFunc implements StoreFuncInterface, LoadPu
         return new RequiredFieldResponse(true);
     }
 
+    @Override
     public void ensureAllKeyInstancesInSameSplit() throws IOException {
-        /** 
-         * no-op because hbase keys are unique 
+        /**
+         * no-op because hbase keys are unique
          * This will also work with things like DelimitedKeyPrefixRegionSplitPolicy
          * if you need a partial key match to be included in the split
          */
@@ -1225,7 +1227,7 @@ public class HBaseStorage extends LoadFunc implements StoreFuncInterface, LoadPu
             throw new RuntimeException("LoadFunc expected split of type TableSplit but was " + split.getClass().getName());
         }
     }
- 
+
 
     /**
      * Class to encapsulate logic around which column names were specified in each
