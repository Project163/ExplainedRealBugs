diff --git a/CHANGES.txt b/CHANGES.txt
index 15ee7cbee..2f3716741 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -30,6 +30,8 @@ PIG-2207: Support custom counters for aggregating warnings from different udfs (
 
 IMPROVEMENTS
 
+PIG-3591: Refactor POPackage to separate MR specific code from packaging (mwagner via cheolsoo)
+
 PIG-3449: Move JobCreationException to org.apache.pig.backend.hadoop.executionengine (cheolsoo)
 
 PIG-3765: Ability to disable Pig commands and operators (prkommireddi)
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/fetch/FetchOptimizer.java b/src/org/apache/pig/backend/hadoop/executionengine/fetch/FetchOptimizer.java
index d801f6f6e..b2b20ecd3 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/fetch/FetchOptimizer.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/fetch/FetchOptimizer.java
@@ -32,19 +32,16 @@ import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOpera
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCollectedGroup;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCombinerPackage;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCounter;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCross;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODemux;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODistinct;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFRJoin;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POGlobalRearrange;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POJoinPackage;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeCogroup;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMultiQueryPackage;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PONative;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POOptimizedForEach;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage;
@@ -230,16 +227,6 @@ public class FetchOptimizer {
             planFetchable = false;
         }
 
-        @Override
-        public void visitCombinerPackage(POCombinerPackage pkg) throws VisitorException {
-            planFetchable = false;
-        }
-
-        @Override
-        public void visitMultiQueryPackage(POMultiQueryPackage pkg) throws VisitorException {
-            planFetchable = false;
-        }
-
         @Override
         public void visitSplit(POSplit spl) throws VisitorException {
             planFetchable = false;
@@ -270,11 +257,6 @@ public class FetchOptimizer {
             planFetchable = false;
         }
 
-        @Override
-        public void visitJoinPackage(POJoinPackage joinPackage) throws VisitorException {
-            planFetchable = false;
-        }
-
         @Override
         public void visitCross(POCross cross) throws VisitorException {
             planFetchable = false;
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/AccumulatorOptimizer.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/AccumulatorOptimizer.java
index 3638b5ccb..19671ca9a 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/AccumulatorOptimizer.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/AccumulatorOptimizer.java
@@ -37,10 +37,10 @@ import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOpe
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.UnaryExpressionOperator;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODistinct;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSortedDistinct;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.Packager;
 import org.apache.pig.data.DataType;
 import org.apache.pig.impl.PigContext;
 import org.apache.pig.impl.plan.DepthFirstWalker;
@@ -71,13 +71,19 @@ public class AccumulatorOptimizer extends MROpPlanVisitor {
             return; 
         }
         
+        Packager pkgr = ((POPackage) po_package).getPkgr();
+        // Check that this is a standard package, not a subclass
+        if (!pkgr.getClass().equals(Packager.class)) {
+            return;
+        }
+
         // if POPackage is for distinct, just return
-        if (((POPackage)po_package).isDistinct()) {
+        if (pkgr.isDistinct()) {
             return;
         }
         
         // if any input to POPackage is inner, just return
-        boolean[] isInner = ((POPackage)po_package).getInner();
+        boolean[] isInner = pkgr.getInner();
         for(boolean b: isInner) {
             if (b) {
                 return;
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/CombinerOptimizer.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/CombinerOptimizer.java
index 18a382bfb..87a9200a3 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/CombinerOptimizer.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/CombinerOptimizer.java
@@ -24,41 +24,38 @@ import java.util.Map;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-
-import org.apache.pig.PigException;
 import org.apache.pig.FuncSpec;
+import org.apache.pig.PigException;
 import org.apache.pig.PigWarning;
-import org.apache.pig.data.DataType;
 import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
-import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.MROperPlan;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.MROpPlanVisitor;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.MROperPlan;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.ConstantExpression;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.CombinerPackager;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODistinct;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLimit;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCombinerPackage;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPartialAgg;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPreCombinerLocalRearrange;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSort;
+import org.apache.pig.data.DataType;
 import org.apache.pig.impl.plan.CompilationMessageCollector;
+import org.apache.pig.impl.plan.CompilationMessageCollector.MessageType;
 import org.apache.pig.impl.plan.DependencyOrderWalker;
 import org.apache.pig.impl.plan.DepthFirstWalker;
-import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.impl.plan.NodeIdGenerator;
+import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.impl.plan.PlanException;
 import org.apache.pig.impl.plan.PlanWalker;
 import org.apache.pig.impl.plan.VisitorException;
-import org.apache.pig.impl.plan.CompilationMessageCollector.MessageType;
 import org.apache.pig.impl.plan.optimizer.OptimizerException;
 import org.apache.pig.impl.util.Pair;
 
@@ -265,8 +262,10 @@ public class CombinerOptimizer extends MROpPlanVisitor {
                 // as it needs to act differently than the regular
                 // package operator.
                 mr.combinePlan = new PhysicalPlan();
-                POCombinerPackage combinePack =
-                    new POCombinerPackage(pack, bags);
+                CombinerPackager pkgr = new CombinerPackager(pack.getPkgr(),
+                        bags);
+                POPackage combinePack = pack.clone();
+                combinePack.setPkgr(pkgr);
                 mr.combinePlan.add(combinePack);
                 mr.combinePlan.add(cfe);
                 mr.combinePlan.connect(combinePack, cfe);
@@ -304,20 +303,7 @@ public class CombinerOptimizer extends MROpPlanVisitor {
                 // Change the package operator in the reduce plan to
                 // be the POCombiner package, as it needs to act
                 // differently than the regular package operator.
-                POCombinerPackage newReducePack =
-                    new POCombinerPackage(pack, bags);
-                mr.reducePlan.replace(pack, newReducePack);
-
-                // the replace() above only changes
-                // the plan and does not change "inputs" to 
-                // operators
-                // set up "inputs" for the operator after
-                // package correctly
-                List<PhysicalOperator> packList = new ArrayList<PhysicalOperator>();
-                packList.add(newReducePack);
-                List<PhysicalOperator> sucs = mr.reducePlan.getSuccessors(newReducePack);
-                // there should be only one successor to package
-                sucs.get(0).setInputs(packList);
+                pack.setPkgr(pkgr.clone());
             } catch (Exception e) {
                 int errCode = 2018;
                 String msg = "Internal error. Unable to introduce the combiner for optimization.";
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java
index b7ce7e2c4..742796d90 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java
@@ -827,7 +827,8 @@ public class JobControlCompiler{
                 }
                 if (!pigContext.inIllustrator)
                     conf.set("pig.reduce.package", ObjectSerializer.serialize(pack));
-                conf.set("pig.reduce.key.type", Byte.toString(pack.getKeyType()));
+                conf.set("pig.reduce.key.type",
+                        Byte.toString(pack.getPkgr().getKeyType()));
 
                 if (mro.getUseSecondaryKey()) {
                     nwJob.setGroupingComparatorClass(PigSecondaryKeyGroupComparator.class);
@@ -840,9 +841,11 @@ public class JobControlCompiler{
                 }
                 else
                 {
-                    Class<? extends WritableComparable> keyClass = HDataType.getWritableComparableTypes(pack.getKeyType()).getClass();
+                    Class<? extends WritableComparable> keyClass = HDataType
+                            .getWritableComparableTypes(
+                                    pack.getPkgr().getKeyType()).getClass();
                     nwJob.setOutputKeyClass(keyClass);
-                    selectComparator(mro, pack.getKeyType(), nwJob);
+                    selectComparator(mro, pack.getPkgr().getKeyType(), nwJob);
                 }
                 nwJob.setOutputValueClass(NullableTuple.class);
             }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRCompiler.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRCompiler.java
index 5dddab794..51014eb0f 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRCompiler.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRCompiler.java
@@ -57,6 +57,8 @@ import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOpe
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.JoinPackager;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.LitePackager;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCollectedGroup;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCounter;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCross;
@@ -65,7 +67,6 @@ import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOpe
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POGlobalRearrange;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POJoinPackage;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLimit;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange;
@@ -73,8 +74,6 @@ import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOpe
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PONative;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage.PackageType;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackageLite;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPartitionRearrange;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PORank;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSkewedJoin;
@@ -83,6 +82,8 @@ import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOpe
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStream;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POUnion;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.Packager;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.Packager.PackageType;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.util.PlanHelper;
 import org.apache.pig.backend.hadoop.executionengine.shims.HadoopShims;
 import org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil;
@@ -1104,9 +1105,9 @@ public class MRCompiler extends PhyPlanVisitor {
         try{
             nonBlocking(op);
             phyToMROpMap.put(op, curMROp);
-            if (op.getPackageType() == PackageType.JOIN) {
+            if (op.getPkgr().getPackageType() == PackageType.JOIN) {
                 curMROp.markRegularJoin();
-            } else if (op.getPackageType() == PackageType.GROUP) {
+            } else if (op.getPkgr().getPackageType() == PackageType.GROUP) {
                 if (op.getNumInps() == 1) {
                     curMROp.markGroupBy();
                 } else if (op.getNumInps() > 1) {
@@ -1758,11 +1759,12 @@ public class MRCompiler extends PhyPlanVisitor {
             curMROp.customPartitioner = op.getCustomPartitioner();
             
             POPackage pkg = new POPackage(new OperatorKey(scope,nig.getNextNodeId(scope)));
-            pkg.setKeyType(DataType.TUPLE);
-            pkg.setDistinct(true);
+            Packager pkgr = pkg.getPkgr();
+            pkgr.setKeyType(DataType.TUPLE);
+            pkgr.setDistinct(true);
             pkg.setNumInps(1);
             boolean[] inner = {false}; 
-            pkg.setInner(inner);
+            pkgr.setInner(inner);
             curMROp.reducePlan.add(pkg);
             
             List<PhysicalPlan> eps1 = new ArrayList<PhysicalPlan>();
@@ -1908,11 +1910,12 @@ public class MRCompiler extends PhyPlanVisitor {
 			
 			// create POPakcage
 			POPackage pkg = new POPackage(new OperatorKey(scope,nig.getNextNodeId(scope)), rp);
-			pkg.setKeyType(type);
+            Packager pkgr = pkg.getPkgr();
+            pkgr.setKeyType(type);
 			pkg.setResultType(DataType.TUPLE);
 			pkg.setNumInps(2);
 			boolean [] inner = op.getInnerFlags();
-			pkg.setInner(inner);            
+            pkgr.setInner(inner);
 			pkg.visit(this);       
 			compiledInputs = new MapReduceOper[] {curMROp};
 			
@@ -2150,8 +2153,11 @@ public class MRCompiler extends PhyPlanVisitor {
         mro.setMapDone(true);
         
         if (limit!=-1) {
-        	POPackageLite pkg_c = new POPackageLite(new OperatorKey(scope,nig.getNextNodeId(scope)));
-        	pkg_c.setKeyType((fields.length>1) ? DataType.TUPLE : keyType);
+            POPackage pkg_c = new POPackage(new OperatorKey(scope,
+                    nig.getNextNodeId(scope)));
+            LitePackager pkgr = new LitePackager();
+            pkgr.setKeyType((fields.length > 1) ? DataType.TUPLE : keyType);
+            pkg_c.setPkgr(pkgr);
             pkg_c.setNumInps(1);
             //pkg.setResultType(DataType.TUPLE);            
             mro.combinePlan.add(pkg_c);
@@ -2191,11 +2197,14 @@ public class MRCompiler extends PhyPlanVisitor {
 	        lr_c2.setResultType(DataType.TUPLE);
 	        mro.combinePlan.addAsLeaf(lr_c2);
         }
-        
-        POPackageLite pkg = new POPackageLite(new OperatorKey(scope,nig.getNextNodeId(scope)));
-        pkg.setKeyType((fields == null || fields.length>1) ? DataType.TUPLE :
-            keyType);
-        pkg.setNumInps(1);       
+
+        POPackage pkg = new POPackage(new OperatorKey(scope,
+                nig.getNextNodeId(scope)));
+        LitePackager pkgr = new LitePackager();
+        pkgr.setKeyType((fields == null || fields.length > 1) ? DataType.TUPLE
+                : keyType);
+        pkg.setPkgr(pkgr);
+        pkg.setNumInps(1);
         mro.reducePlan.add(pkg);
         
         PhysicalPlan ep = new PhysicalPlan();
@@ -2447,10 +2456,12 @@ public class MRCompiler extends PhyPlanVisitor {
         mro.setMapDone(true);
         
         POPackage pkg = new POPackage(new OperatorKey(scope,nig.getNextNodeId(scope)));
-        pkg.setKeyType(DataType.CHARARRAY);
+        Packager pkgr = new Packager();
+        pkg.setPkgr(pkgr);
+        pkgr.setKeyType(DataType.CHARARRAY);
         pkg.setNumInps(1);
         boolean[] inner = {false}; 
-        pkg.setInner(inner);
+        pkgr.setInner(inner);
         mro.reducePlan.add(pkg);
         
         // Lets start building the plan which will have the sort
@@ -2738,35 +2749,30 @@ public class MRCompiler extends PhyPlanVisitor {
 
         public static void replaceWithPOJoinPackage(PhysicalPlan plan, MapReduceOper mr,
                 POPackage pack, POForEach forEach, String chunkSize) throws VisitorException {
-            String scope = pack.getOperatorKey().scope;
-            NodeIdGenerator nig = NodeIdGenerator.getGenerator();
-            POJoinPackage joinPackage;
-            joinPackage = new POJoinPackage(
-                        new OperatorKey(scope, nig.getNextNodeId(scope)), 
-                        -1, pack, forEach);
-            joinPackage.setChunkSize(Long.parseLong(chunkSize));
+            JoinPackager pkgr = new JoinPackager(pack.getPkgr(), forEach);
+            pkgr.setChunkSize(Long.parseLong(chunkSize));
+            pack.setPkgr(pkgr);
             List<PhysicalOperator> succs = plan.getSuccessors(forEach);
-            if (succs!=null)
-            {
-                if (succs.size()!=1)
-                {
+            if (succs != null) {
+                if (succs.size() != 1) {
                     int errCode = 2028;
-                    String msg = "ForEach can only have one successor. Found " + succs.size() + " successors.";
-                    throw new MRCompilerException(msg, errCode, PigException.BUG);
+                    String msg = "ForEach can only have one successor. Found "
+                            + succs.size() + " successors.";
+                    throw new MRCompilerException(msg, errCode,
+                            PigException.BUG);
                 }
             }
             plan.remove(pack);
-            
             try {
-                plan.replace(forEach, joinPackage);
+                plan.replace(forEach, pack);
             } catch (PlanException e) {
                 int errCode = 2029;
-                String msg = "Error rewriting POJoinPackage.";
+                String msg = "Error rewriting join package.";
                 throw new MRCompilerException(msg, errCode, PigException.BUG, e);
             }
-            mr.phyToMRMap.put(forEach, joinPackage);
-            LogFactory.
-            getLog(LastInputStreamingOptimizer.class).info("Rewrite: POPackage->POForEach to POJoinPackage");
+            mr.phyToMRMap.put(forEach, pack);
+            LogFactory.getLog(LastInputStreamingOptimizer.class).info(
+                    "Rewrite: POPackage->POForEach to POPackage(JoinPackager)");
         }
 
     }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRUtil.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRUtil.java
index 93de6d55b..79d418f79 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRUtil.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRUtil.java
@@ -65,10 +65,10 @@ public class MRUtil {
         mro.mapPlan.addAsLeaf(lr);
         
         POPackage pkg = new POPackage(new OperatorKey(scope,nig.getNextNodeId(scope)));
-        pkg.setKeyType(DataType.TUPLE);
+        pkg.getPkgr().setKeyType(DataType.TUPLE);
         pkg.setNumInps(1);
         boolean[] inner = {false};
-        pkg.setInner(inner);
+        pkg.getPkgr().setInner(inner);
         mro.reducePlan.add(pkg);
         
         mro.reducePlan.addAsLeaf(getPlainForEachOP(scope, nig));
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java
index d3ebeb3ec..7f06e2f18 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java
@@ -60,7 +60,7 @@ import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.MRPrin
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.POPackageAnnotator;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.XMLMRPrinter;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POJoinPackage;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.JoinPackager;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
 import org.apache.pig.backend.hadoop.executionengine.shims.HadoopShims;
 import org.apache.pig.impl.PigContext;
@@ -637,8 +637,8 @@ public class MapReduceLauncher extends Launcher{
         comp.getMessageCollector().logMessages(MessageType.Warning, aggregateWarning, log);
 
         String lastInputChunkSize =
-                pc.getProperties().getProperty(
-                        "last.input.chunksize", POJoinPackage.DEFAULT_CHUNK_SIZE);
+            pc.getProperties().getProperty(
+                "last.input.chunksize", JoinPackager.DEFAULT_CHUNK_SIZE);
 
         String prop = pc.getProperties().getProperty(PigConfiguration.PROP_NO_COMBINER);
         if (!pc.inIllustrator && !("true".equals(prop)))  {
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MultiQueryOptimizer.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MultiQueryOptimizer.java
index 64f0ee16a..5152a4ca0 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MultiQueryOptimizer.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MultiQueryOptimizer.java
@@ -18,10 +18,10 @@
 package org.apache.pig.backend.hadoop.executionengine.mapReduceLayer;
 
 import java.util.ArrayList;
+import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
-import java.util.Iterator;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
@@ -31,14 +31,15 @@ import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.MROpPl
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.MROperPlan;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.MultiQueryPackager;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODemux;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMultiQueryPackage;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.Packager;
 import org.apache.pig.data.DataType;
 import org.apache.pig.impl.io.PigNullableWritable;
 import org.apache.pig.impl.plan.NodeIdGenerator;
@@ -686,28 +687,33 @@ class MultiQueryOptimizer extends MROpPlanVisitor {
             PhysicalPlan to, int initial, int current, byte mapKeyType) throws VisitorException {                    
         POPackage pk = (POPackage)from.getRoots().get(0);
         from.remove(pk);
+        Packager fromPkgr = pk.getPkgr();
  
-        if(!(pk instanceof POMultiQueryPackage)){
+        if (!(fromPkgr instanceof MultiQueryPackager)) {
             // XXX the index of the original keyInfo map is always 0,
             // we need to shift the index so that the lookups works
             // with the new indexed key
-            addShiftedKeyInfoIndex(initial, pk); 
+            addShiftedKeyInfoIndex(initial, fromPkgr);
         }
          
         int total = current - initial;
         
-        POMultiQueryPackage pkg = (POMultiQueryPackage)to.getRoots().get(0);        
+        MultiQueryPackager toPkgr = (MultiQueryPackager) ((POPackage) to
+                .getRoots().get(0)).getPkgr();
         int pkCount = 0;
-        if (pk instanceof POMultiQueryPackage) {
-            List<POPackage> pkgs = ((POMultiQueryPackage)pk).getPackages();
-            for (POPackage p : pkgs) {
-                pkg.addPackage(p);
+        if (fromPkgr instanceof MultiQueryPackager) {
+            List<Packager> pkgs = ((MultiQueryPackager) fromPkgr)
+                    .getPackagers();
+            for (Packager p : pkgs) {
+                ((MultiQueryPackager) toPkgr).addPackager(p);
                 pkCount++;
             }
-            pkg.addIsKeyWrappedList(((POMultiQueryPackage)pk).getIsKeyWrappedList());
-            addShiftedKeyInfoIndex(initial, current, (POMultiQueryPackage)pk);
+            toPkgr.addIsKeyWrappedList(((MultiQueryPackager) fromPkgr)
+                            .getIsKeyWrappedList());
+            addShiftedKeyInfoIndex(initial, current,
+                    (MultiQueryPackager) fromPkgr);
         } else {
-            pkg.addPackage(pk, mapKeyType);
+            toPkgr.addPackager(fromPkgr, mapKeyType);
             pkCount = 1;
         }
         
@@ -740,14 +746,15 @@ class MultiQueryOptimizer extends MROpPlanVisitor {
             throw new OptimizerException(msg, errCode, PigException.BUG);
         }
 
-        if (pkg.isSameMapKeyType()) {
-            pkg.setKeyType(pk.getKeyType());
+        if (toPkgr.isSameMapKeyType()) {
+            toPkgr.setKeyType(fromPkgr.getKeyType());
         } else {
-            pkg.setKeyType(DataType.TUPLE);
+            toPkgr.setKeyType(DataType.TUPLE);
         }            
     }
     
-    private void addShiftedKeyInfoIndex(int index, POPackage pkg) throws OptimizerException {
+    private void addShiftedKeyInfoIndex(int index, Packager pkg)
+            throws OptimizerException {
         /**
          * we only do multi query optimization for single input MROpers
          * Hence originally the keyInfo would have had only index 0. As
@@ -761,7 +768,8 @@ class MultiQueryOptimizer extends MROpPlanVisitor {
          * addition should be the same as the "value" in the existing Entry. After
          * addition, we should remove the older entry
          */
-        Map<Integer, Pair<Boolean, Map<Integer, Integer>>> keyInfo = pkg.getKeyInfo();
+        Map<Integer, Pair<Boolean, Map<Integer, Integer>>> keyInfo = pkg
+                .getKeyInfo();
         byte newIndex = (byte)(index | PigNullableWritable.mqFlag);
         
         Set<Integer> existingIndices = keyInfo.keySet();
@@ -792,9 +800,9 @@ class MultiQueryOptimizer extends MROpPlanVisitor {
      * @throws OptimizerException 
      */
     private int addShiftedKeyInfoIndex(int initialIndex, int onePastEndIndex,
-            POMultiQueryPackage mpkg) throws OptimizerException {
+            MultiQueryPackager mpkgr) throws OptimizerException {
         
-        List<POPackage> pkgs = mpkg.getPackages();
+        List<Packager> pkgs = mpkgr.getPackagers();
         // if we have lesser pkgs than (onePastEndIndex - initialIndex)
         // its because one or more of the pkgs is a POMultiQueryPackage which
         // internally has packages.
@@ -810,7 +818,7 @@ class MultiQueryOptimizer extends MROpPlanVisitor {
         int i = 0;
         int curIndex = initialIndex;
         while (i < end) {
-            POPackage pkg = pkgs.get(i);
+            Packager pkg = pkgs.get(i);
             addShiftedKeyInfoIndex(curIndex, pkg);
             curIndex++;
             i++;
@@ -823,12 +831,14 @@ class MultiQueryOptimizer extends MROpPlanVisitor {
             PhysicalPlan to, int initial, int current, byte mapKeyType) throws VisitorException {
         POPackage cpk = (POPackage)from.getRoots().get(0);
         from.remove(cpk);
+        Packager cpkgr = cpk.getPkgr();
         
         PODemux demux = (PODemux)to.getLeaves().get(0);
                 
-        POMultiQueryPackage pkg = (POMultiQueryPackage)to.getRoots().get(0);
+        MultiQueryPackager toPkgr = (MultiQueryPackager) ((POPackage) to
+                .getRoots().get(0)).getPkgr();
         
-        boolean isSameKeyType = pkg.isSameMapKeyType();
+        boolean isSameKeyType = toPkgr.isSameMapKeyType();
         
         // if current > initial + 1, it means we had
         // a split in the map of the MROper we are trying to
@@ -844,21 +854,21 @@ class MultiQueryOptimizer extends MROpPlanVisitor {
         // POLocalRearranges.
         int total = current - initial;
         int pkCount = 0;
-        if (cpk instanceof POMultiQueryPackage) {
-            List<POPackage> pkgs = ((POMultiQueryPackage)cpk).getPackages();
-            for (POPackage p : pkgs) {
-                pkg.addPackage(p);
+        if (cpkgr instanceof MultiQueryPackager) {
+            List<Packager> pkgrs = ((MultiQueryPackager) cpkgr).getPackagers();
+            for (Packager p : pkgrs) {
+                toPkgr.addPackager(p);
                 if (!isSameKeyType) {
                     p.setKeyType(DataType.TUPLE);
                 }
                 pkCount++;
             }
         } else {
-            pkg.addPackage(cpk);
+            toPkgr.addPackager(cpkgr);
             pkCount = 1;
         }
 
-        pkg.setSameMapKeyType(isSameKeyType);
+        toPkgr.setSameMapKeyType(isSameKeyType);
         
         if (pkCount != total) {
             int errCode = 2146;
@@ -868,10 +878,10 @@ class MultiQueryOptimizer extends MROpPlanVisitor {
 
         // all packages should have the same key type
         if (!isSameKeyType) {
-            cpk.setKeyType(DataType.TUPLE);          
+            cpk.getPkgr().setKeyType(DataType.TUPLE);
         } 
         
-        pkg.setKeyType(cpk.getKeyType());
+        toPkgr.setKeyType(cpk.getPkgr().getKeyType());
         
         // See comment above for why we flatten the Packages
         // in the from plan - for the same reason, we flatten
@@ -936,7 +946,7 @@ class MultiQueryOptimizer extends MROpPlanVisitor {
     private PhysicalPlan createDemuxPlan(boolean sameKeyType, boolean isCombiner) 
         throws VisitorException {
         PODemux demux = getDemux(isCombiner);
-        POMultiQueryPackage pkg= getMultiQueryPackage(sameKeyType, isCombiner);
+        POPackage pkg = getMultiQueryPackage(sameKeyType, isCombiner);
         
         PhysicalPlan pl = new PhysicalPlan();
         pl.add(pkg);
@@ -1186,11 +1196,14 @@ class MultiQueryOptimizer extends MROpPlanVisitor {
         return demux;
     } 
     
-    private POMultiQueryPackage getMultiQueryPackage(boolean sameMapKeyType, boolean inCombiner){
-        POMultiQueryPackage pkg =  
-            new POMultiQueryPackage(new OperatorKey(scope, nig.getNextNodeId(scope)));
-        pkg.setInCombiner(inCombiner);
-        pkg.setSameMapKeyType(sameMapKeyType);
+    private POPackage getMultiQueryPackage(boolean sameMapKeyType,
+            boolean inCombiner) {
+        POPackage pkg = new POPackage(new OperatorKey(scope,
+                nig.getNextNodeId(scope)));
+        MultiQueryPackager pkgr = new MultiQueryPackager();
+        pkgr.setInCombiner(inCombiner);
+        pkgr.setSameMapKeyType(sameMapKeyType);
+        pkg.setPkgr(pkgr);
         return pkg;
     }   
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PhyPlanSetter.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PhyPlanSetter.java
index 933363daa..cdb73c19b 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PhyPlanSetter.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PhyPlanSetter.java
@@ -20,10 +20,56 @@ package org.apache.pig.backend.hadoop.executionengine.mapReduceLayer;
 import java.util.List;
 
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.*;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.*;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.Add;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.ConstantExpression;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.Divide;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.EqualToExpr;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.GTOrEqualToExpr;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.GreaterThanExpr;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.LTOrEqualToExpr;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.LessThanExpr;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.Mod;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.Multiply;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.NotEqualToExpr;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POAnd;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POBinCond;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POIsNull;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POMapLookUp;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.PONegative;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.PONot;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POOr;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.PORegexp;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserComparisonFunc;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.Subtract;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCollectedGroup;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODemux;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODistinct;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFRJoin;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POGlobalRearrange;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLimit;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeCogroup;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PONative;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POOptimizedForEach;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPartialAgg;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPreCombinerLocalRearrange;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PORank;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSkewedJoin;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSort;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStream;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POUnion;
 import org.apache.pig.impl.plan.DependencyOrderWalker;
 import org.apache.pig.impl.plan.VisitorException;
 
@@ -83,11 +129,6 @@ public class PhyPlanSetter extends PhyPlanVisitor {
         pkg.setParentPlan(parent);
     }
 
-    @Override
-    public void visitCombinerPackage(POCombinerPackage pkg) throws VisitorException{
-        pkg.setParentPlan(parent);
-    }
-
     @Override
     public void visitPOForEach(POForEach nfe) throws VisitorException {
         super.visitPOForEach(nfe);
@@ -250,11 +291,6 @@ public class PhyPlanSetter extends PhyPlanVisitor {
         mapLookUp.setParentPlan(parent);
     }
 
-    @Override
-    public void visitJoinPackage(POJoinPackage joinPackage) throws VisitorException{
-        joinPackage.setParentPlan(parent);
-    }
-
     @Override
     public void visitCast(POCast cast) {
         cast.setParentPlan(parent);
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigCombiner.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigCombiner.java
index 773a22cb6..d3045c4d8 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigCombiner.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigCombiner.java
@@ -35,7 +35,7 @@ import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POJoinPackage;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.JoinPackager;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage;
 import org.apache.pig.data.Tuple;
 import org.apache.pig.impl.PigContext;
@@ -153,7 +153,7 @@ public class PigCombiner {
             // tuples out of the getnext() call of POJoinPackage
             // In this case, we process till we see EOP from
             // POJoinPacakage.getNext()
-            if (pack instanceof POJoinPackage)
+            if (pack.getPkgr() instanceof JoinPackager)
             {
                 pack.attachInput(key, tupIter.iterator());
                 while (true)
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigGenericMapReduce.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigGenericMapReduce.java
index eea5ce36f..a463a1562 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigGenericMapReduce.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigGenericMapReduce.java
@@ -23,8 +23,6 @@ import java.util.ArrayList;
 import java.util.Iterator;
 import java.util.List;
 
-import org.joda.time.DateTimeZone;
-
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
@@ -40,7 +38,7 @@ import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POJoinPackage;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.JoinPackager;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.util.PlanHelper;
@@ -60,6 +58,7 @@ import org.apache.pig.impl.util.Pair;
 import org.apache.pig.impl.util.SpillableMemoryManager;
 import org.apache.pig.impl.util.UDFContext;
 import org.apache.pig.tools.pigstats.PigStatusReporter;
+import org.joda.time.DateTimeZone;
 
 /**
  * This class is the static Mapper &amp; Reducer classes that
@@ -396,7 +395,7 @@ public class PigGenericMapReduce {
             // tuples out of the getnext() call of POJoinPackage
             // In this case, we process till we see EOP from 
             // POJoinPacakage.getNext()
-            if (pack instanceof POJoinPackage)
+            if (pack.getPkgr() instanceof JoinPackager)
             {
                 pack.attachInput(key, tupIter.iterator());
                 while (true)
@@ -583,7 +582,7 @@ public class PigGenericMapReduce {
         @Override
         protected void setup(Context context) throws IOException, InterruptedException {
             super.setup(context);
-            keyType = pack.getKeyType();
+            keyType = pack.getPkgr().getKeyType();
         }
 
         /**
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/SecondaryKeyOptimizer.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/SecondaryKeyOptimizer.java
index 54740a0a7..292d48aa0 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/SecondaryKeyOptimizer.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/SecondaryKeyOptimizer.java
@@ -30,10 +30,10 @@ import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOpe
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.PORelationToExprProject;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.JoinPackager;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODistinct;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POJoinPackage;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLimit;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage;
@@ -221,7 +221,8 @@ public class SecondaryKeyOptimizer extends MROpPlanVisitor {
         PhysicalOperator currentNode = root;
         POForEach foreach = null;
         while (currentNode != null) {
-            if (currentNode instanceof POPackage && !(currentNode instanceof POJoinPackage)
+            if (currentNode instanceof POPackage
+                    && !(((POPackage) currentNode).getPkgr() instanceof JoinPackager)
                     || currentNode instanceof POFilter
                     || currentNode instanceof POLimit) {
                 List<PhysicalOperator> succs = mr.reducePlan
@@ -372,7 +373,7 @@ public class SecondaryKeyOptimizer extends MROpPlanVisitor {
                 }
             }
             POPackage pack = (POPackage) root;
-            pack.setUseSecondaryKey(true);
+            pack.getPkgr().setUseSecondaryKey(true);
         }
     }
 
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/plans/POPackageAnnotator.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/plans/POPackageAnnotator.java
index 47137d510..29b97c485 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/plans/POPackageAnnotator.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/plans/POPackageAnnotator.java
@@ -27,11 +27,10 @@ import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOpe
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POJoinPackage;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.CombinerPackager;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.LitePackager;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackageLite;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCombinerPackage;
 import org.apache.pig.impl.plan.DepthFirstWalker;
 import org.apache.pig.impl.plan.VisitorException;
 import org.apache.pig.impl.plan.optimizer.OptimizerException;
@@ -78,7 +77,7 @@ public class POPackageAnnotator extends MROpPlanVisitor {
             if(pkg != null) {
                 // if the POPackage is actually a POPostCombinerPackage, then we should
                 // just look for the corresponding LocalRearrange(s) in the combine plan
-                if(pkg instanceof POCombinerPackage) {
+                if (pkg.getPkgr() instanceof CombinerPackager) {
                     if(patchPackage(mr.combinePlan, pkg) != pkg.getNumInps()) {
                         int errCode = 2085;
                         String msg = "Unexpected problem during optimization." +
@@ -148,24 +147,6 @@ public class POPackageAnnotator extends MROpPlanVisitor {
         public void visitPackage(POPackage pkg) throws VisitorException {
             this.pkg = pkg;
         };
-        
-        /* (non-Javadoc)
-         * @see org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor#visitJoinPackage(org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POJoinPackage)
-         */
-        @Override
-        public void visitJoinPackage(POJoinPackage joinPackage)
-                throws VisitorException {
-            this.pkg = joinPackage;
-        }
-        
-        /* (non-Javadoc)
-         * @see org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor#visitCombinerPackage(org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPostCombinerPackage)
-         */
-        @Override
-        public void visitCombinerPackage(POCombinerPackage pkg)
-                throws VisitorException {
-            this.pkg = pkg;
-        }
 
         /**
          * @return the pkg
@@ -201,7 +182,7 @@ public class POPackageAnnotator extends MROpPlanVisitor {
             loRearrangeFound++;
             Map<Integer,Pair<Boolean, Map<Integer, Integer>>> keyInfo;
 
-            if (pkg instanceof POPackageLite) {
+            if (pkg.getPkgr() instanceof LitePackager) {
                 if(lrearrange.getIndex() != 0) {
                     // Throw some exception here
                     throw new RuntimeException("POLocalRearrange for POPackageLite cannot have index other than 0, but has index - "+lrearrange.getIndex());
@@ -210,7 +191,7 @@ public class POPackageAnnotator extends MROpPlanVisitor {
 
             // annotate the package with information from the LORearrange
             // update the keyInfo information if already present in the POPackage
-            keyInfo = pkg.getKeyInfo();
+            keyInfo = pkg.getPkgr().getKeyInfo();
             if(keyInfo == null)
                 keyInfo = new HashMap<Integer, Pair<Boolean, Map<Integer, Integer>>>();
             
@@ -227,9 +208,9 @@ public class POPackageAnnotator extends MROpPlanVisitor {
             keyInfo.put(Integer.valueOf(lrearrange.getIndex()), 
                 new Pair<Boolean, Map<Integer, Integer>>(
                         lrearrange.isProjectStar(), lrearrange.getProjectedColsMap()));
-            pkg.setKeyInfo(keyInfo);
-            pkg.setKeyTuple(lrearrange.isKeyTuple());
-            pkg.setKeyCompound(lrearrange.isKeyCompound());
+            pkg.getPkgr().setKeyInfo(keyInfo);
+            pkg.getPkgr().setKeyTuple(lrearrange.isKeyTuple());
+            pkg.getPkgr().setKeyCompound(lrearrange.isKeyCompound());
         }
 
         /**
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/plans/PhyPlanVisitor.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/plans/PhyPlanVisitor.java
index abb16ffdd..f30406f39 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/plans/PhyPlanVisitor.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/plans/PhyPlanVisitor.java
@@ -20,8 +20,57 @@ package org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans;
 import java.util.List;
 
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.*;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.*;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.Add;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.ConstantExpression;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.Divide;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.EqualToExpr;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.GTOrEqualToExpr;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.GreaterThanExpr;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.LTOrEqualToExpr;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.LessThanExpr;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.Mod;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.Multiply;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.NotEqualToExpr;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POAnd;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POBinCond;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POIsNull;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POMapLookUp;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.PONegative;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.PONot;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POOr;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.PORegexp;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserComparisonFunc;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.Subtract;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCollectedGroup;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCounter;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCross;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODemux;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODistinct;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFRJoin;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POGlobalRearrange;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLimit;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeCogroup;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PONative;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POOptimizedForEach;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPartialAgg;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPartitionRearrange;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPreCombinerLocalRearrange;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PORank;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSkewedJoin;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSort;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStream;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POUnion;
 import org.apache.pig.impl.plan.PlanVisitor;
 import org.apache.pig.impl.plan.PlanWalker;
 import org.apache.pig.impl.plan.VisitorException;
@@ -89,14 +138,6 @@ public class PhyPlanVisitor extends PlanVisitor<PhysicalOperator,PhysicalPlan> {
         //do nothing
     }
     
-    public void visitCombinerPackage(POCombinerPackage pkg) throws VisitorException{
-        //do nothing
-    }
- 
-    public void visitMultiQueryPackage(POMultiQueryPackage pkg) throws VisitorException{
-        //do nothing
-    }
-    
     public void visitPOForEach(POForEach nfe) throws VisitorException {
         List<PhysicalPlan> inpPlans = nfe.getInputPlans();
         for (PhysicalPlan plan : inpPlans) {
@@ -242,10 +283,6 @@ public class PhyPlanVisitor extends PlanVisitor<PhysicalOperator,PhysicalPlan> {
         // TODO Auto-generated method stub
         
     }
-    
-    public void visitJoinPackage(POJoinPackage joinPackage) throws VisitorException{
-        //do nothing
-    }
 
     public void visitCast(POCast cast) {
         // TODO Auto-generated method stub
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/plans/PlanPrinter.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/plans/PlanPrinter.java
index ff828017d..64953b372 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/plans/PlanPrinter.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/plans/PlanPrinter.java
@@ -30,7 +30,25 @@ import java.util.Set;
 
 import org.apache.pig.PigException;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.*;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.MultiQueryPackager;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCollectedGroup;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCounter;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODemux;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFRJoin;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POGlobalRearrange;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPartialAgg;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PORank;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSkewedJoin;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSort;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POUnion;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.Packager;
 import org.apache.pig.impl.plan.DepthFirstWalker;
 import org.apache.pig.impl.plan.Operator;
 import org.apache.pig.impl.plan.OperatorPlan;
@@ -175,11 +193,14 @@ public class PlanPrinter<O extends Operator, P extends OperatorPlan<O>> extends
           else if(node instanceof POForEach){
             sb.append(planString(((POForEach)node).getInputPlans()));
           }
-          else if (node instanceof POMultiQueryPackage) {
-              List<POPackage> pkgs = ((POMultiQueryPackage)node).getPackages();
-              for (POPackage pkg : pkgs) {
-                  sb.append(LSep + pkg.name() + "\n");
+          else if(node instanceof POPackage){
+            Packager pkgr = ((POPackage) node).getPkgr();
+            if(pkgr instanceof MultiQueryPackager){
+              List<Packager> pkgrs = ((MultiQueryPackager) pkgr).getPackagers();
+              for (Packager child : pkgrs){
+                  sb.append(LSep + child.name() + "\n");
               }
+            }
           }
           else if(node instanceof POFRJoin){
             POFRJoin frj = (POFRJoin)node;
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/plans/XMLPhysicalPlanPrinter.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/plans/XMLPhysicalPlanPrinter.java
index 892c26f2f..cb90b1cbc 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/plans/XMLPhysicalPlanPrinter.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/plans/XMLPhysicalPlanPrinter.java
@@ -33,6 +33,7 @@ import javax.xml.transform.stream.StreamResult;
 
 import org.apache.pig.PigException;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.MultiQueryPackager;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCollectedGroup;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODemux;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFRJoin;
@@ -40,26 +41,27 @@ import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOpe
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMultiQueryPackage;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSkewedJoin;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSort;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.Packager;
 import org.apache.pig.impl.plan.DepthFirstWalker;
 import org.apache.pig.impl.plan.OperatorPlan;
 import org.apache.pig.impl.plan.VisitorException;
 import org.apache.pig.impl.util.MultiMap;
 import org.w3c.dom.Document;
 import org.w3c.dom.Element;
+import org.w3c.dom.Node;
 
 
 public class XMLPhysicalPlanPrinter<P extends OperatorPlan<PhysicalOperator>> extends
-        PhyPlanVisitor {
-    
+PhyPlanVisitor {
+
     private Document doc = null;
     private Element parent = null;
-    
+
     public XMLPhysicalPlanPrinter(PhysicalPlan plan, Document doc, Element parent) {
         super(plan, new DepthFirstWalker<PhysicalOperator, PhysicalPlan>(plan));
         this.doc = doc;
@@ -84,7 +86,7 @@ public class XMLPhysicalPlanPrinter<P extends OperatorPlan<PhysicalOperator>> ex
             transformer.setOutputProperty(OutputKeys.OMIT_XML_DECLARATION, "yes");
             transformer.setOutputProperty(OutputKeys.INDENT, "yes");
             transformer.setOutputProperty("{http://xml.apache.org/xslt}indent-amount", "2");
-            
+
             StringWriter sw = new StringWriter();
             StreamResult result = new StreamResult(sw);
             DOMSource source = new DOMSource(doc);
@@ -94,7 +96,7 @@ public class XMLPhysicalPlanPrinter<P extends OperatorPlan<PhysicalOperator>> ex
             e.printStackTrace();
         }
     }
-    
+
     private Element createAlias(PhysicalOperator po) {
         Element aliasNode = null;
         String alias = po.getAlias();
@@ -112,8 +114,8 @@ public class XMLPhysicalPlanPrinter<P extends OperatorPlan<PhysicalOperator>> ex
             depthFirst(leaf, parentNode);
         }
     }
-    
-    
+
+
     private void visitPlan(PhysicalPlan pp, Element parentNode) throws VisitorException {
         if(pp!=null) {
             XMLPhysicalPlanPrinter<PhysicalPlan> ppp =
@@ -121,15 +123,15 @@ public class XMLPhysicalPlanPrinter<P extends OperatorPlan<PhysicalOperator>> ex
             ppp.visit();
         }
     }
-    
-    
+
+
     private void visitPlan(List<PhysicalPlan> lep, Element parentNode) throws VisitorException {
         if(lep!=null)
             for (PhysicalPlan ep : lep) {
                 visitPlan(ep, parentNode);
             }
     }
-    
+
     private Element createPONode(PhysicalOperator node) {
         Element PONode = doc.createElement(node.getClass().getSimpleName());
         PONode.setAttribute("scope", "" + node.getOperatorKey().id);
@@ -150,18 +152,18 @@ public class XMLPhysicalPlanPrinter<P extends OperatorPlan<PhysicalOperator>> ex
             Element loadFile = doc.createElement("loadFile");
             loadFile.setTextContent(((POLoad)node).getLFile().getFileName());
             PONode.appendChild(loadFile);
-            
+
             Element isTmpLoad = doc.createElement("isTmpLoad");
             isTmpLoad.setTextContent(Boolean.valueOf(((POLoad)node).isTmpLoad()).toString());
             PONode.appendChild(isTmpLoad);
         }
         return PONode;
     }
-    
+
 
     private void depthFirst(PhysicalOperator node, Element parentNode) throws VisitorException {
         Element childNode = null;
-        
+
         List<PhysicalPlan> subPlans = new ArrayList<PhysicalPlan>();
         if(node instanceof POFilter){
             subPlans.add(((POFilter) node).getPlan());
@@ -177,12 +179,11 @@ public class XMLPhysicalPlanPrinter<P extends OperatorPlan<PhysicalOperator>> ex
             subPlans = ((POSplit)node).getPlans();
         } else if (node instanceof PODemux) {
             subPlans = ((PODemux)node).getPlans();
-        } else if (node instanceof POMultiQueryPackage) {
-            childNode = createPONode(node);   
-            List<POPackage> pkgs = ((POMultiQueryPackage)node).getPackages();
-            for (POPackage pkg : pkgs) {
-                childNode.appendChild(createPONode(pkg));
-            }
+        } else if(node instanceof POPackage){
+            childNode = createPONode(node);
+            Packager pkgr = ((POPackage) node).getPkgr();
+            Node pkgrNode = createPackagerNode(pkgr);
+            childNode.appendChild(pkgrNode);
         } else if(node instanceof POFRJoin){
             childNode = createPONode(node);
             POFRJoin frj = (POFRJoin)node;
@@ -198,11 +199,11 @@ public class XMLPhysicalPlanPrinter<P extends OperatorPlan<PhysicalOperator>> ex
             MultiMap<PhysicalOperator, PhysicalPlan> joinPlans = skewed.getJoinPlans();
             if(joinPlans!=null) {
                 List<PhysicalPlan> inner_plans = new ArrayList<PhysicalPlan>();
-            	inner_plans.addAll(joinPlans.values());   
-            	visitPlan(inner_plans, childNode);
+                inner_plans.addAll(joinPlans.values());
+                visitPlan(inner_plans, childNode);
             }
         }
-        
+
         if (childNode == null) {
             childNode = createPONode(node);
             if (subPlans.size() > 0) {
@@ -210,17 +211,29 @@ public class XMLPhysicalPlanPrinter<P extends OperatorPlan<PhysicalOperator>> ex
             }
         }
         parentNode.appendChild(childNode);
-        
+
         List<PhysicalOperator> originalPredecessors = mPlan.getPredecessors(node);
         if (originalPredecessors == null) {
             return;
         }
-        
+
         List<PhysicalOperator> predecessors =  new ArrayList<PhysicalOperator>(originalPredecessors);
-        
+
         Collections.sort(predecessors);
         for (PhysicalOperator pred : predecessors) {
             depthFirst(pred, childNode);
         }
     }
+
+    private Node createPackagerNode(Packager pkgr) {
+        Element pkgrNode = doc.createElement(pkgr.getClass().getSimpleName());
+        if (pkgr instanceof MultiQueryPackager) {
+            List<Packager> pkgrs = ((MultiQueryPackager) pkgr)
+                    .getPackagers();
+            for (Packager child : pkgrs) {
+                pkgrNode.appendChild(createPackagerNode(child));
+            }
+        }
+        return pkgrNode;
+    }
 }
\ No newline at end of file
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POCombinerPackage.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/CombinerPackager.java
similarity index 67%
rename from src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POCombinerPackage.java
rename to src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/CombinerPackager.java
index 9105a0e19..77a95b6ed 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POCombinerPackage.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/CombinerPackager.java
@@ -24,40 +24,26 @@ import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
-import org.apache.pig.data.BagFactory;
 import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataType;
 import org.apache.pig.data.InternalCachedBag;
 import org.apache.pig.data.NonSpillableDataBag;
 import org.apache.pig.data.Tuple;
-import org.apache.pig.data.TupleFactory;
 import org.apache.pig.impl.io.NullableTuple;
-import org.apache.pig.impl.plan.NodeIdGenerator;
-import org.apache.pig.impl.plan.OperatorKey;
-import org.apache.pig.impl.plan.VisitorException;
+import org.apache.pig.impl.io.PigNullableWritable;
 import org.apache.pig.impl.util.Pair;
 /**
  * The package operator that packages the globally rearranged tuples into
  * output format after the combiner stage.  It differs from POPackage in that
  * it does not use the index in the NullableTuple to find the bag to put a
- * tuple in.  Instead, the inputs are put in a bag corresponding to their 
+ * tuple in.  Instead, the inputs are put in a bag corresponding to their
  * offset in the tuple.
  */
-public class POCombinerPackage extends POPackage {
-    /**
-     * 
-     */
-    private static final long serialVersionUID = 1L;
-
-    private static BagFactory mBagFactory = BagFactory.getInstance();
-    private static TupleFactory mTupleFactory = TupleFactory.getInstance();
-
+public class CombinerPackager extends Packager {
     private boolean[] mBags; // For each field, indicates whether or not it
-                             // needs to be put in a bag.
-    
+    // needs to be put in a bag.
+
     private Map<Integer, Integer> keyLookup;
-    
+
     private int numBags;
 
     /**
@@ -67,11 +53,8 @@ public class POCombinerPackage extends POPackage {
      * @param bags for each field, indicates whether it should be a bag (true)
      * or a simple field (false).
      */
-    public POCombinerPackage(POPackage pkg, boolean[] bags) {
-        super(new OperatorKey(pkg.getOperatorKey().scope,
-            NodeIdGenerator.getGenerator().getNextNodeId(pkg.getOperatorKey().scope)),
-            pkg.getRequestedParallelism(), pkg.getInputs());
-        resultType = pkg.getResultType();
+    public CombinerPackager(Packager pkg, boolean[] bags) {
+        super();
         keyType = pkg.keyType;
         numInputs = 1;
         inner = new boolean[1];
@@ -83,23 +66,14 @@ public class POCombinerPackage extends POPackage {
         }
         numBags = 0;
         for (int i = 0; i < mBags.length; i++) {
-            if (mBags[i]) numBags++;            
+            if (mBags[i]) numBags++;
         }
     }
 
-    @Override
-    public String name() {
-        return "POCombinerPackage" + "[" + DataType.findTypeName(resultType) + "]" + "{" + DataType.findTypeName(keyType) + "}" +" - " + mKey.toString();
-    }
-    
-    @Override
-    public void visit(PhyPlanVisitor v) throws VisitorException {
-        v.visitCombinerPackage(this);
-    }
-    
     /**
      * @param keyInfo the keyInfo to set
      */
+    @Override
     public void setKeyInfo(Map<Integer, Pair<Boolean, Map<Integer, Integer>>> keyInfo) {
         this.keyInfo = keyInfo;
         // TODO: IMPORTANT ASSUMPTION: Currently we only combine in the
@@ -109,40 +83,40 @@ public class POCombinerPackage extends POPackage {
         // has an index of 0. When we do support combiner in Cogroups
         // THIS WILL NEED TO BE REVISITED.
         Pair<Boolean, Map<Integer, Integer>> lrKeyInfo =
-            keyInfo.get(0); // assumption: only group are "combinable", hence index 0
+                keyInfo.get(0); // assumption: only group are "combinable", hence index 0
         keyLookup = lrKeyInfo.second;
     }
 
     private DataBag createDataBag(int numBags) {
-    	String bagType = null;
+        String bagType = null;
         if (PigMapReduce.sJobConfInternal.get() != null) {
-   			bagType = PigMapReduce.sJobConfInternal.get().get("pig.cachedbag.type");       			
-   	    }
-                		          	           		
-    	if (bagType != null && bagType.equalsIgnoreCase("default")) {
-    		return new NonSpillableDataBag();
-    	}
-    	return new InternalCachedBag(numBags);  	
+            bagType = PigMapReduce.sJobConfInternal.get().get("pig.cachedbag.type");
+        }
+
+        if (bagType != null && bagType.equalsIgnoreCase("default")) {
+            return new NonSpillableDataBag();
+        }
+        return new InternalCachedBag(numBags);
     }
-    
+
     @Override
-    public Result getNextTuple() throws ExecException {
-        int keyField = -1;
+    public Result getNext() throws ExecException {
+        if (bags == null) {
+            return new Result(POStatus.STATUS_EOP, null);
+        }
+
         //Create numInputs bags
         Object[] fields = new Object[mBags.length];
         for (int i = 0; i < mBags.length; i++) {
-            if (mBags[i]) fields[i] = createDataBag(numBags);            
+            if (mBags[i]) fields[i] = createDataBag(numBags);
         }
-        
+
         // For each indexed tup in the inp, split them up and place their
         // fields into the proper bags.  If the given field isn't a bag, just
         // set the value as is.
-        while (tupIter.hasNext()) {
-            NullableTuple ntup = tupIter.next();
-            Tuple tup = (Tuple)ntup.getValueAsPigType();
-            
-            int tupIndex = 0; // an index for accessing elements from 
-                              // the value (tup) that we have currently
+        for (Tuple tup : bags[0]) {
+            int tupIndex = 0; // an index for accessing elements from
+            // the value (tup) that we have currently
             for(int i = 0; i < mBags.length; i++) {
                 Integer keyIndex = keyLookup.get(i);
                 if(keyIndex == null && mBags[i]) {
@@ -159,13 +133,15 @@ public class POCombinerPackage extends POPackage {
                 }
             }
         }
-        
-        // The successor of the POCombinerPackage as of 
+
+        detachInput();
+
+        // The successor of the POPackage(Combiner) as of
         // now SHOULD be a POForeach which has been adjusted
         // to look for its inputs by projecting from the corresponding
-		// positions in the POCombinerPackage output.
-		// So we will NOT be adding the key in the result here but merely 
-        // putting all bags into a result tuple and returning it. 
+        // positions in the POPackage(Combiner) output.
+        // So we will NOT be adding the key in the result here but merely
+        // putting all bags into a result tuple and returning it.
         Tuple res;
         res = mTupleFactory.newTuple(mBags.length);
         for (int i = 0; i < mBags.length; i++) res.set(i, fields[i]);
@@ -176,4 +152,10 @@ public class POCombinerPackage extends POPackage {
 
     }
 
+    @Override
+    public Tuple getValueTuple(PigNullableWritable keyWritable,
+            NullableTuple ntup, int index) throws ExecException {
+        return (Tuple) ntup.getValueAsPigType();
+    }
+
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POJoinPackage.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/JoinPackager.java
similarity index 56%
rename from src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POJoinPackage.java
rename to src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/JoinPackager.java
index 82f11ac6d..4ee0d6764 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POJoinPackage.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/JoinPackager.java
@@ -17,37 +17,27 @@
  */
 package org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators;
 
+import java.util.Iterator;
 import java.util.List;
 
 import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
-import org.apache.pig.data.BagFactory;
 import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataType;
 import org.apache.pig.data.InternalCachedBag;
 import org.apache.pig.data.NonSpillableDataBag;
 import org.apache.pig.data.Tuple;
-import org.apache.pig.impl.io.NullableTuple;
 import org.apache.pig.impl.plan.NodeIdGenerator;
 import org.apache.pig.impl.plan.OperatorKey;
-import org.apache.pig.impl.plan.VisitorException;
+import org.apache.pig.pen.Illustrator;
 
-public class POJoinPackage extends POPackage {
+public class JoinPackager extends Packager {
 
-    private static final long serialVersionUID = 1L;
-    
     private POOptimizedForEach forEach;
     private boolean newKey = true;
     private Tuple res = null;
-    private boolean lastInputTuple = false;
-    private static final Tuple t1 = null;
     private static final Result eopResult = new Result(POStatus.STATUS_EOP, null);
-    private boolean firstTime = true;
-    private boolean useDefaultBag = false;
 
     public static final String DEFAULT_CHUNK_SIZE = "1000";
 
@@ -56,16 +46,18 @@ public class POJoinPackage extends POPackage {
     private DataBag[] dbs = null;
 
     private int lastBagIndex;
-    
-    public POJoinPackage(OperatorKey k, int rp, POPackage p, POForEach f) {
-        super(k, rp);
-        String scope = getOperatorKey().getScope();
+
+    private Iterator<Tuple> lastBagIter;
+
+    public JoinPackager(Packager p, POForEach f) {
+        super();
+        String scope = f.getOperatorKey().getScope();
         NodeIdGenerator nig = NodeIdGenerator.getGenerator();
         forEach = new POOptimizedForEach(new OperatorKey(scope,nig.getNextNodeId(scope)));
         if (p!=null)
         {
             setKeyType(p.getKeyType());
-            setNumInps(p.getNumInps());
+            setNumInputs(p.getNumInputs());
             lastBagIndex = numInputs - 1;
             setInner(p.getInner());
             setKeyInfo(p.getKeyInfo());
@@ -79,107 +71,43 @@ public class POJoinPackage extends POPackage {
         }
     }
 
-    @Override
-    public void visit(PhyPlanVisitor v) throws VisitorException {
-        v.visitJoinPackage(this);
-    }
-
-    @Override
-    public String name() {
-        String fString = forEach.getFlatStr();
-        return "POJoinPackage" + "(" + fString + ")" + "[" + DataType.findTypeName(resultType) + "]" +" - " + mKey.toString();
-    }
-
     /**
-     * Calls getNext to get next ForEach result. The input for POJoinPackage is 
-     * a (key, NullableTuple) pair. We will materialize n-1 inputs into bags, feed input#n 
+     * Calls getNext to get next ForEach result. The input for POJoinPackage is
+     * a (key, NullableTuple) pair. We will materialize n-1 inputs into bags, feed input#n
      * one tuple a time to the delegated ForEach operator, the input for ForEach is
      * 
      *     (input#1, input#2, input#3....input#n[i]), i=(1..k), suppose input#n consists
-     *     
+     * 
      * of k tuples.
      * For every ForEach input, pull all the results from ForEach.
-     * getNext will be called multiple times for a particular input, 
-     * it returns one output tuple from ForEach every time we call getNext, 
+     * getNext will be called multiple times for a particular input,
+     * it returns one output tuple from ForEach every time we call getNext,
      * so we need to maintain internal status to keep tracking of where we are.
      */
     @Override
-    public Result getNextTuple() throws ExecException {
-        
-        if(firstTime){
-            firstTime = false;
-            if (PigMapReduce.sJobConfInternal.get() != null) {
-                String bagType = PigMapReduce.sJobConfInternal.get().get("pig.cachedbag.type");
-                if (bagType != null && bagType.equalsIgnoreCase("default")) {
-                    useDefaultBag = true;
-                }
-            }
-        }
-        // if a previous call to foreach.getNext()
-        // has still not returned all output, process it
-        if (forEach.processingPlan)
-        {
-            forEachResult = forEach.getNextTuple();
-            switch (forEachResult.returnStatus)
-            {
-            case POStatus.STATUS_OK:
-            case POStatus.STATUS_NULL:
-            case POStatus.STATUS_ERR:
-                return forEachResult;
-            case POStatus.STATUS_EOP:
-                break;
-            }
-        }
-        
-        NullableTuple it = null;
-        
+    public Result getNext() throws ExecException {
+        Tuple it = null;
+
         // If we see a new NullableTupleIterator, materialize n-1 inputs, construct ForEach input
         // tuple res = (key, input#1, input#2....input#n), the only missing value is input#n,
         // we will get input#n one tuple a time, fill in res, feed to ForEach.
         // After this block, we have the first tuple of input#n in hand (kept in variable it)
         if (newKey)
         {
-            lastInputTuple = false;
-            //Put n-1 inputs into bags
+            // Put n-1 inputs into bags
             dbs = new DataBag[numInputs];
             for (int i = 0; i < numInputs - 1; i++) {
-                dbs[i] = useDefaultBag ? BagFactory.getInstance().newDefaultBag() 
-                // In a very rare case if there is a POStream after this 
-                // POJoinPackage in the pipeline and is also blocking the pipeline;
-                // constructor argument should be 2 * numInputs. But for one obscure
-                // case we don't want to pay the penalty all the time.        
-                        : new InternalCachedBag(numInputs-1);                    
+                dbs[i] = bags[i];
             }
+
             // For last bag, we always use NonSpillableBag.
             dbs[lastBagIndex] = new NonSpillableDataBag((int)chunkSize);
-            
-            //For each Nullable tuple in the input, put it
-            //into the corresponding bag based on the index,
-            // except for the last input, which we will stream
-            // The tuples will arrive in the order of the index,
-            // starting from index 0 and such that all tuples for
-            // a given index arrive before a tuple for the next
-            // index does.
-            while (tupIter.hasNext()) {
-                it = tupIter.next();
-                int itIndex = it.getIndex();
-                if (itIndex!= numInputs - 1)
-                {
-                    dbs[itIndex].add(getValueTuple(it, itIndex));
-                }
-                else
-                {
-                    lastInputTuple = true;
-                    break;
-                }
-                if(getReporter()!=null) {
-                    getReporter().progress();
-                }
-            }
+
+            lastBagIter = bags[lastBagIndex].iterator();
+
             // If we don't have any tuple for input#n
             // we do not need any further process, return EOP
-            if (!lastInputTuple)
-            {
+            if (!lastBagIter.hasNext()) {
                 // we will return at this point because we ought
                 // to be having a flatten on this last input
                 // and we have an empty bag which should result
@@ -187,7 +115,7 @@ public class POJoinPackage extends POPackage {
                 newKey = true;
                 return eopResult;
             }
-            
+
             res = mTupleFactory.newTuple(numInputs+1);
             for (int i = 0; i < dbs.length; i++)
                 res.set(i+1,dbs[i]);
@@ -202,81 +130,85 @@ public class POJoinPackage extends POPackage {
                 }
             }
             newKey = false;
-            
-			// set up the bag with last input to contain
-			// a chunk of CHUNKSIZE values OR the entire bag if
-			// it has less than CHUNKSIZE values - the idea is in most
-			// cases the values are > CHUNKSIZE in number and in 
-			// those cases we will be sending the last bag
-			// as a set of smaller chunked bags thus holding lesser
-			// in memory
-			
-			// the first tuple can be directly retrieved from "it"
-			dbs[lastBagIndex].add(getValueTuple(it, it.getIndex()));
-			for(int i = 0; i < chunkSize -1 && tupIter.hasNext(); i++) {
-			    it = tupIter.next();
-			    dbs[lastBagIndex].add(getValueTuple(it, it.getIndex()));
-			}
+        }
 
-			// Attach the input to forEach
-            forEach.attachInput(res);
-            
-            // pull output tuple from ForEach
-            Result forEachResult = forEach.getNextTuple();
-            {
-                switch (forEachResult.returnStatus)
-                {
+        // Keep attaching input tuple to ForEach, until:
+        // 1. We can initialize ForEach.getNext();
+        // 2. There is no more input#n
+        while (lastBagIter.hasNext() || forEach.processingPlan) {
+            // if a previous call to foreach.getNext()
+            // has still not returned all output, process it
+            while (forEach.processingPlan) {
+                forEachResult = forEach.getNextTuple();
+                switch (forEachResult.returnStatus) {
                 case POStatus.STATUS_OK:
-                case POStatus.STATUS_NULL:
                 case POStatus.STATUS_ERR:
                     return forEachResult;
+                case POStatus.STATUS_NULL:
+                    continue;
                 case POStatus.STATUS_EOP:
                     break;
                 }
             }
-        }
-        
-        // Keep attaching input tuple to ForEach, until:
-        // 1. We can initialize ForEach.getNext();
-        // 2. There is no more input#n
-        while (true)
-        {
-            if (tupIter.hasNext()) {
+
+            if (lastBagIter.hasNext()) {
                 // try setting up a bag of CHUNKSIZE OR
                 // the remainder of the bag of last input
                 // (if < CHUNKSIZE) to foreach
                 dbs[lastBagIndex].clear(); // clear last chunk
-                for(int i = 0; i < chunkSize && tupIter.hasNext(); i++) {
-                    it = tupIter.next();
-                    dbs[lastBagIndex].add(getValueTuple(it, it.getIndex()));
+                for (int i = 0; i < chunkSize && lastBagIter.hasNext(); i++) {
+                    it = lastBagIter.next();
+                    dbs[lastBagIndex].add(it);
                 }
-            }
-            else
-            // if we do not have any more tuples for input#n, return EOP
-            {
+            } else {
                 detachInput();
-                newKey = true;
                 return eopResult;
             }
+
             // Attach the input to forEach
             forEach.attachInput(res);
-            
+
             // pull output tuple from ForEach
             Result forEachResult = forEach.getNextTuple();
             {
-                switch (forEachResult.returnStatus)
-                {
+                switch (forEachResult.returnStatus) {
                 case POStatus.STATUS_OK:
-                case POStatus.STATUS_NULL:
                 case POStatus.STATUS_ERR:
                     return forEachResult;
+                case POStatus.STATUS_NULL:
+                    continue;
                 case POStatus.STATUS_EOP:
                     break;
                 }
             }
         }
+        detachInput();
+        return eopResult;
+    }
+
+    @Override
+    public void attachInput(Object key, DataBag[] bags, boolean[] readOnce)
+            throws ExecException {
+        checkBagType();
+
+        this.key = key;
+        this.bags = bags;
+        this.readOnce = readOnce;
+        // JoinPackager expects all but the last bag to be materialized
+        for (int i = 0; i < bags.length - 1; i++) {
+            if (readOnce[i]) {
+                DataBag materializedBag = getBag();
+                materializedBag.addAll(bags[i]);
+                bags[i] = materializedBag;
+            }
+        }
+        if (readOnce[numInputs - 1] != true) {
+            throw new ExecException(
+                    "JoinPackager expects the last input to be streamed");
+        }
+        this.newKey = true;
     }
-    
+
     public List<PhysicalPlan> getInputPlans() {
         return forEach.getInputPlans();
     }
@@ -302,4 +234,15 @@ public class POJoinPackage extends POPackage {
     public void setChunkSize(long chunkSize) {
         this.chunkSize = chunkSize;
     }
+
+    @Override
+    public void setIllustrator(Illustrator illustrator) {
+        this.illustrator = illustrator;
+        forEach.setIllustrator(illustrator);
+    }
+
+    @Override
+    public String name() {
+        return this.getClass().getSimpleName() + "(" + forEach.getFlatStr() + ")";
+    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/LitePackager.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/LitePackager.java
new file mode 100644
index 000000000..6b1817d3e
--- /dev/null
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/LitePackager.java
@@ -0,0 +1,156 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * 
+ */
+package org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators;
+
+import java.util.HashMap;
+import java.util.LinkedList;
+import java.util.Map;
+import java.util.Map.Entry;
+
+import org.apache.pig.backend.executionengine.ExecException;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.impl.io.NullableTuple;
+import org.apache.pig.impl.io.PigNullableWritable;
+import org.apache.pig.impl.util.IdentityHashSet;
+import org.apache.pig.impl.util.Pair;
+import org.apache.pig.pen.util.ExampleTuple;
+import org.apache.pig.pen.util.LineageTracer;
+
+/**
+ * This package operator is a specialization
+ * of POPackage operator used for the specific
+ * case of the order by query. See JIRA 802
+ * for more details.
+ */
+public class LitePackager extends Packager {
+
+    private PigNullableWritable keyWritable;
+
+    @Override
+    public boolean[] getInner() {
+        return null;
+    }
+
+    @Override
+    public void setInner(boolean[] inner) {
+    }
+
+    /**
+     * Make a deep copy of this operator.
+     * @throws CloneNotSupportedException
+     */
+    @Override
+    public LitePackager clone() throws CloneNotSupportedException {
+        LitePackager clone = (LitePackager) super.clone();
+        clone.inner = null;
+        if (keyInfo != null) {
+            clone.keyInfo = new HashMap<Integer, Pair<Boolean, Map<Integer, Integer>>>(keyInfo);
+        }
+        return clone;
+    }
+
+    /**
+     * @return the distinct
+     */
+    @Override
+    public boolean isDistinct() {
+        return false;
+    }
+
+    /**
+     * @param distinct
+     *            the distinct to set
+     */
+    @Override
+    public void setDistinct(boolean distinct) {
+    }
+
+    /**
+     * Similar to POPackage.getNext except that
+     * only one input is expected with index 0
+     * and ReadOnceBag is used instead of
+     * DefaultDataBag.
+     */
+    @Override
+    public Result getNext() throws ExecException {
+        if (bags == null) {
+            return new Result(POStatus.STATUS_EOP, null);
+        }
+
+        Tuple res;
+
+        //Construct the output tuple by appending
+        //the key and all the above constructed bags
+        //and return it.
+        res = mTupleFactory.newTuple(numInputs+1);
+        res.set(0,key);
+        res.set(1, bags[0]);
+        detachInput();
+        Result r = new Result();
+        r.returnStatus = POStatus.STATUS_OK;
+        r.result = illustratorMarkup(null, res, 0);
+        return r;
+    }
+
+    /**
+     * Makes use of the superclass method, but this requires an additional
+     * parameter key passed by ReadOnceBag. key of this instance will be set to
+     * null in detachInput call, but an instance of ReadOnceBag may have the
+     * original key that it uses. Therefore this extra argument is taken to
+     * temporarily set it before the call to the superclass method and then
+     * restore it.
+     */
+    @Override
+    public Tuple getValueTuple(PigNullableWritable keyWritable,
+            NullableTuple ntup, int index) throws ExecException {
+        PigNullableWritable origKey = this.keyWritable;
+        this.keyWritable = keyWritable;
+        Tuple retTuple = super.getValueTuple(keyWritable, ntup, index);
+        this.keyWritable = origKey;
+        return retTuple;
+    }
+
+    @Override
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+        if (illustrator != null) {
+            ExampleTuple tOut = new ExampleTuple((Tuple) out);
+            LineageTracer lineageTracer = illustrator.getLineage();
+            lineageTracer.insert(tOut);
+            if (illustrator.getEquivalenceClasses() == null) {
+                LinkedList<IdentityHashSet<Tuple>> equivalenceClasses = new LinkedList<IdentityHashSet<Tuple>>();
+                for (int i = 0; i < numInputs; ++i) {
+                    IdentityHashSet<Tuple> equivalenceClass = new IdentityHashSet<Tuple>();
+                    equivalenceClasses.add(equivalenceClass);
+                }
+                illustrator.setEquivalenceClasses(equivalenceClasses, parent);
+            }
+            illustrator.getEquivalenceClasses().get(eqClassIndex).add(tOut);
+            tOut.synthetic = false; // not expect this to be really used
+            illustrator.addData((Tuple) tOut);
+            return tOut;
+        } else
+            return (Tuple) out;
+    }
+}
+
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POMultiQueryPackage.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/MultiQueryPackager.java
similarity index 63%
rename from src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POMultiQueryPackage.java
rename to src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/MultiQueryPackager.java
index d6041741a..622f2aa8d 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POMultiQueryPackage.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/MultiQueryPackager.java
@@ -19,50 +19,46 @@ package org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOp
 
 import java.util.ArrayList;
 import java.util.Collections;
-import java.util.Iterator;
 import java.util.List;
 
 import org.apache.pig.PigException;
 import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
+import org.apache.pig.backend.hadoop.HDataType;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
 import org.apache.pig.data.DataType;
 import org.apache.pig.data.Tuple;
 import org.apache.pig.impl.io.NullableTuple;
 import org.apache.pig.impl.io.NullableUnknownWritable;
 import org.apache.pig.impl.io.PigNullableWritable;
-import org.apache.pig.impl.plan.OperatorKey;
-import org.apache.pig.impl.plan.VisitorException;
-import org.apache.pig.backend.hadoop.HDataType;
 
 /**
- * The package operator that packages the globally rearranged tuples 
+ * The package operator that packages the globally rearranged tuples
  * into output format as required by multi-query de-multiplexer.
  * <p>
  * This operator is used when merging multiple Map-Reduce splittees
- * into a Map-only splitter during multi-query optimization. 
+ * into a Map-only splitter during multi-query optimization.
  * The package operators of the reduce plans of the splittees form an
- * indexed package list inside this operator. When this operator 
- * receives an input, it extracts the index from the key and calls the 
+ * indexed package list inside this operator. When this operator
+ * receives an input, it extracts the index from the key and calls the
  * corresponding package to get the output data.
  * <p>
  * Due to the recursive nature of multi-query optimization, this operator
  * may be contained in another multi-query packager.
  * <p>
- * The successor of this operator must be a PODemux operator which 
+ * The successor of this operator must be a PODemux operator which
  * knows how to consume the output of this operator.
  */
-public class POMultiQueryPackage extends POPackage {
-    
+public class MultiQueryPackager extends Packager {
+
     private static final long serialVersionUID = 1L;
-    
+
     private static int idxPart = 0x7F;
 
-    private List<POPackage> packages = new ArrayList<POPackage>();
+    private List<Packager> packagers = new ArrayList<Packager>();
 
     /**
-     * If the POLocalRearranges corresponding to the reduce plans in 
+     * If the POLocalRearranges corresponding to the reduce plans in
      * myPlans (the list of inner plans of the demux) have different key types
      * then the MultiQueryOptimizer converts all the keys to be of type tuple
      * by wrapping any non-tuple keys into Tuples (keys which are already tuples
@@ -72,118 +68,44 @@ public class POMultiQueryPackage extends POPackage {
      * to "unwrap" the tuple to get to the key
      */
     private ArrayList<Boolean> isKeyWrapped = new ArrayList<Boolean>();
-    
+
     /*
      * Indicating if all the inner plans have the same
-     * map key type. If not, the keys passed in are 
+     * map key type. If not, the keys passed in are
      * wrapped inside tuples and need to be extracted
-     * out during the reduce phase 
+     * out during the reduce phase
      */
     private boolean sameMapKeyType = true;
-    
+
     /*
-     * Indicating if this operator is in a combiner. 
+     * Indicating if this operator is in a combiner.
      * If not, this operator is in a reducer and the key
      * values must first be extracted from the tuple-wrap
      * before writing out to the disk
      */
     private boolean inCombiner = false;
-    
-    transient private PigNullableWritable myKey;
 
-    /**
-     * Constructs an operator with the specified key.
-     * 
-     * @param k the operator key
-     */
-    public POMultiQueryPackage(OperatorKey k) {
-        this(k, -1, null);
-    }
+    private PigNullableWritable keyWritable = null;
 
     /**
-     * Constructs an operator with the specified key
-     * and degree of parallelism.
-     *  
-     * @param k the operator key
-     * @param rp the degree of parallelism requested
-     */
-    public POMultiQueryPackage(OperatorKey k, int rp) {
-        this(k, rp, null);
-    }
-
-    /**
-     * Constructs an operator with the specified key and inputs.
-     *  
-     * @param k the operator key
-     * @param inp the inputs that this operator will read data from
-     */
-    public POMultiQueryPackage(OperatorKey k, List<PhysicalOperator> inp) {
-        this(k, -1, inp);
-    }
-
-    /**
-     * Constructs an operator with the specified key,
-     * degree of parallelism and inputs.
-     * 
-     * @param k the operator key
-     * @param rp the degree of parallelism requested 
-     * @param inp the inputs that this operator will read data from
-     */
-    public POMultiQueryPackage(OperatorKey k, int rp, List<PhysicalOperator> inp) {
-        super(k, rp, inp);
-    }
-
-    @Override
-    public String name() {
-        return "MultiQuery Package [" + isKeyWrapped + "] - " +  getOperatorKey().toString();
-    }
-
-    @Override
-    public boolean supportsMultipleInputs() {
-        return false;
-    }
-
-    @Override
-    public void visit(PhyPlanVisitor v) throws VisitorException {
-        v.visitMultiQueryPackage(this);
-    }
-
-    @Override
-    public boolean supportsMultipleOutputs() {
-        return false;
-    }
-    
-    @Override
-    public void attachInput(PigNullableWritable k, Iterator<NullableTuple> inp) {
-        tupIter = inp;
-        myKey = k;
-    }
-
-    @Override
-    public void detachInput() {
-        tupIter = null;
-        myKey = null;
-    }
-
-    /**
-     * Appends the specified package object to the end of 
+     * Appends the specified package object to the end of
      * the package list.
      * 
      * @param pack package to be appended to the list
      */
-    public void addPackage(POPackage pack) {
-        packages.add(pack);        
+    public void addPackager(Packager pkgr) {
+        packagers.add(pkgr);
     }
-    
+
     /**
-     * Appends the specified package object to the end of 
+     * Appends the specified package object to the end of
      * the package list.
      * 
      * @param pack package to be appended to the list
      * @param mapKeyType the map key type associated with the package
      */
-    public void addPackage(POPackage pack, byte mapKeyType) {
-        packages.add(pack);        
+    public void addPackager(Packager pkgr, byte mapKeyType) {
+        packagers.add(pkgr);
         // if mapKeyType is already a tuple, we will NOT
         // be wrapping it in an extra tuple. If it is not
         // a tuple, we will wrap into in a tuple
@@ -192,52 +114,58 @@ public class POMultiQueryPackage extends POPackage {
 
     /**
      * Returns the list of packages.
-     *  
+     * 
      * @return the list of the packages
      */
-    public List<POPackage> getPackages() {
-        return packages;
+    public List<Packager> getPackagers() {
+        return packagers;
     }
 
     /**
      * Constructs the output tuple from the inputs.
-     * <p> 
-     * The output is consumed by for the demultiplexer operator 
-     * (PODemux) in the format (key, {bag of tuples}) where key 
+     * <p>
+     * The output is consumed by for the demultiplexer operator
+     * (PODemux) in the format (key, {bag of tuples}) where key
      * is an indexed WritableComparable, not the wrapped value as a pig type.
      */
     @Override
-    public Result getNextTuple() throws ExecException {
-        
-        byte origIndex = myKey.getIndex();
+    public Result getNext() throws ExecException {
+        if (bags == null) {
+            return new Result(POStatus.STATUS_EOP, null);
+        }
+
+        byte origIndex = keyWritable.getIndex();
 
         int index = (int)origIndex;
         index &= idxPart;
-        
-        if (index >= packages.size() || index < 0) {
+
+        if (index >= packagers.size() || index < 0) {
             int errCode = 2140;
-            String msg = "Invalid package index " + index 
-                + " should be in the range between 0 and " + packages.size();
+            String msg = "Invalid package index " + index
+                    + " should be in the range between 0 and "
+                    + packagers.size();
             throw new ExecException(msg, errCode, PigException.BUG);
         }
-                  
-        POPackage pack = packages.get(index);
-        
+
+        Packager pkgr = packagers.get(index);
+
         // check to see if we need to unwrap the key. The keys may be
-        // wrapped inside a tuple by LocalRearrange operator when jobs  
+        // wrapped inside a tuple by LocalRearrange operator when jobs
         // with different map key types are merged
-        PigNullableWritable curKey = myKey;
-        if (!sameMapKeyType && !inCombiner && isKeyWrapped.get(index)) {                                       
-            Tuple tup = (Tuple)myKey.getValueAsPigType();
-            curKey = HDataType.getWritableComparableTypes(tup.get(0), pack.getKeyType());
+        PigNullableWritable curKey = keyWritable;
+        if (!sameMapKeyType && !inCombiner && isKeyWrapped.get(index)) {
+            Tuple tup = (Tuple) keyWritable.getValueAsPigType();
+            curKey = HDataType.getWritableComparableTypes(tup.get(0),
+                    pkgr.getKeyType());
             curKey.setIndex(origIndex);
         }
-            
-        pack.attachInput(curKey, tupIter);
 
-        Result res = pack.getNextTuple();
-        pack.detachInput();
-        
+        pkgr.attachInput(curKey, bags, readOnce);
+
+        Result res = pkgr.getNext();
+        pkgr.detachInput();
+        detachInput();
+
         Tuple tuple = (Tuple)res.result;
 
         // the object present in the first field
@@ -249,7 +177,7 @@ public class POMultiQueryPackage extends POPackage {
         // which needs a PigNullableWritable first field so
         // it can figure out the index. Therefore we need
         // to add index to the first field of the tuple.
-                
+
         Object obj = tuple.get(0);
         if (obj instanceof PigNullableWritable) {
             ((PigNullableWritable)obj).setIndex(origIndex);
@@ -266,12 +194,12 @@ public class POMultiQueryPackage extends POPackage {
             myObj.setIndex(origIndex);
             tuple.set(0, myObj);
         }
-        // illustrator markup has been handled by "pack"
+        // illustrator markup has been handled by "pkgr"
         return res;
     }
 
     /**
-     * Returns the list of booleans that indicates if the 
+     * Returns the list of booleans that indicates if the
      * key needs to unwrapped for the corresponding plan.
      * 
      * @return the list of isKeyWrapped boolean values
@@ -279,7 +207,7 @@ public class POMultiQueryPackage extends POPackage {
     public List<Boolean> getIsKeyWrappedList() {
         return Collections.unmodifiableList(isKeyWrapped);
     }
-    
+
     /**
      * Adds a list of IsKeyWrapped boolean values
      * 
@@ -290,7 +218,7 @@ public class POMultiQueryPackage extends POPackage {
             isKeyWrapped.add(b);
         }
     }
-    
+
     public void setInCombiner(boolean inCombiner) {
         this.inCombiner = inCombiner;
     }
@@ -307,4 +235,16 @@ public class POMultiQueryPackage extends POPackage {
         return sameMapKeyType;
     }
 
+    @Override
+    public int getNumInputs(byte index) {
+        return packagers.get(((int) index) & idxPart).getNumInputs(index);
+    }
+
+    @Override
+    public Tuple getValueTuple(PigNullableWritable keyWritable,
+            NullableTuple ntup, int index) throws ExecException {
+        this.keyWritable = keyWritable;
+        return packagers.get(((int) index) & idxPart).getValueTuple(
+                keyWritable, ntup, index);
+    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPackage.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPackage.java
index 86314d94f..34f6e7086 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPackage.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPackage.java
@@ -21,32 +21,29 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.Iterator;
-import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
 
 import org.apache.pig.backend.executionengine.ExecException;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
 import org.apache.pig.data.AccumulativeBag;
 import org.apache.pig.data.BagFactory;
-import org.apache.pig.data.InternalCachedBag;
 import org.apache.pig.data.DataBag;
 import org.apache.pig.data.DataType;
+import org.apache.pig.data.InternalCachedBag;
+import org.apache.pig.data.ReadOnceBag;
 import org.apache.pig.data.Tuple;
 import org.apache.pig.data.TupleFactory;
 import org.apache.pig.impl.io.NullableTuple;
 import org.apache.pig.impl.io.PigNullableWritable;
-import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.impl.plan.NodeIdGenerator;
-import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
+import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.impl.plan.VisitorException;
-import org.apache.pig.impl.util.IdentityHashSet;
 import org.apache.pig.impl.util.Pair;
-import org.apache.pig.pen.util.ExampleTuple;
-import org.apache.pig.pen.util.LineageTracer;
+import org.apache.pig.pen.Illustrator;
 
 /**
  * The package operator that packages
@@ -67,50 +64,18 @@ public class POPackage extends PhysicalOperator {
      */
     private static final long serialVersionUID = 1L;
 
-
-    public static enum PackageType { GROUP, JOIN };
-
     //The iterator of indexed Tuples
     //that is typically provided by
     //Hadoop
     transient Iterator<NullableTuple> tupIter;
 
     //The key being worked on
-    Object key;
-
-    // marker to indicate if key is a tuple
-    protected boolean isKeyTuple = false;
-    // marker to indicate if the tuple key is compound in nature
-    protected boolean isKeyCompound = false;
-    // key as a Tuple object (if the key is a tuple)
-    protected Tuple keyAsTuple;
-
-    //key's type
-    byte keyType;
+    protected Object key;
 
     //The number of inputs to this
     //co-group.  0 indicates a distinct, which means there will only be a
     //key, no value.
-    int numInputs;
-
-    // If the attaching map-reduce plan use secondary sort key
-    boolean useSecondaryKey = false;
-
-    //Denotes if inner is specified
-    //on a particular input
-    boolean[] inner;
-
-    // flag to denote whether there is a distinct
-    // leading to this package
-    protected boolean distinct = false;
-
-    // A mapping of input index to key information got from LORearrange
-    // for that index. The Key information is a pair of boolean, Map.
-    // The boolean indicates whether there is a lone project(*) in the
-    // cogroup by. If not, the Map has a mapping of column numbers in the
-    // "value" to column numbers in the "key" which contain the fields in
-    // the "value"
-    protected Map<Integer, Pair<Boolean, Map<Integer, Integer>>> keyInfo;
+    protected int numInputs;
 
     protected static final BagFactory mBagFactory = BagFactory.getInstance();
     protected static final TupleFactory mTupleFactory = TupleFactory.getInstance();
@@ -119,7 +84,9 @@ public class POPackage extends PhysicalOperator {
 
     private boolean useDefaultBag = false;
 
-    private PackageType pkgType;
+    protected Packager pkgr;
+
+    private PigNullableWritable keyWritable;
 
     public POPackage(OperatorKey k) {
         this(k, -1, null);
@@ -134,16 +101,27 @@ public class POPackage extends PhysicalOperator {
     }
 
     public POPackage(OperatorKey k, int rp, List<PhysicalOperator> inp) {
+        this(k, rp, inp, new Packager());
+    }
+
+    public POPackage(OperatorKey k, int rp, List<PhysicalOperator> inp,
+            Packager pkgr) {
         super(k, rp, inp);
         numInputs = -1;
-        keyInfo = new HashMap<Integer, Pair<Boolean, Map<Integer, Integer>>>();
+        this.pkgr = pkgr;
+    }
+
+    @Override
+    public void setIllustrator(Illustrator illustrator) {
+        super.setIllustrator(illustrator);
+        pkgr.setIllustrator(illustrator);
     }
 
     @Override
     public String name() {
-        return getAliasString() + "Package" + "["
+        return getAliasString() + "Package" + "(" + pkgr.name() + ")" + "["
                 + DataType.findTypeName(resultType) + "]" + "{"
-                + DataType.findTypeName(keyType) + "}" + " - "
+                + DataType.findTypeName(pkgr.getKeyType()) + "}" + " - "
                 + mKey.toString();
     }
 
@@ -171,16 +149,9 @@ public class POPackage extends PhysicalOperator {
     public void attachInput(PigNullableWritable k, Iterator<NullableTuple> inp) {
         try {
             tupIter = inp;
-            key = k.getValueAsPigType();
-            if (useSecondaryKey) {
-                key = ((Tuple)key).get(0);
-
-            }
-            if(isKeyTuple) {
-                // key is a tuple, cache the key as a
-                // tuple for use in the getNext()
-                keyAsTuple = (Tuple)key;
-            }
+            key = pkgr.getKey(k);
+            keyWritable = k;
+            inputAttached = true;
         } catch (Exception e) {
             throw new RuntimeException(
                     "Error attaching input for key " + k +
@@ -191,9 +162,11 @@ public class POPackage extends PhysicalOperator {
     /**
      * attachInput's better half!
      */
+    @Override
     public void detachInput() {
         tupIter = null;
         key = null;
+        inputAttached = false;
     }
 
     public int getNumInps() {
@@ -202,46 +175,38 @@ public class POPackage extends PhysicalOperator {
 
     public void setNumInps(int numInps) {
         this.numInputs = numInps;
-    }
-
-    public boolean[] getInner() {
-        return inner;
-    }
-
-    public void setInner(boolean[] inner) {
-        this.inner = inner;
+        pkgr.setNumInputs(numInps);
     }
 
     /**
-     * From the inputs, constructs the output tuple
-     * for this co-group in the required format which
-     * is (key, {bag of tuples from input 1}, {bag of tuples from input 2}, ...)
+     * From the inputs, constructs the output tuple for this co-group in the
+     * required format which is (key, {bag of tuples from input 1}, {bag of
+     * tuples from input 2}, ...)
      */
     @Override
     public Result getNextTuple() throws ExecException {
-        Tuple res;
-
         if(firstTime){
             firstTime = false;
             if (PigMapReduce.sJobConfInternal.get() != null) {
-                String bagType = PigMapReduce.sJobConfInternal.get().get("pig.cachedbag.type");
+                String bagType = PigMapReduce.sJobConfInternal.get().get(
+                        "pig.cachedbag.type");
                 if (bagType != null && bagType.equalsIgnoreCase("default")) {
                     useDefaultBag = true;
                 }
             }
         }
+        int numInputs = pkgr.getNumInputs(keyWritable.getIndex());
+        boolean[] readOnce = new boolean[numInputs];
+        for (int i = 0; i < numInputs; i++)
+            readOnce[i] = false;
 
-        if(distinct) {
-            // only set the key which has the whole
-            // tuple
-            res = mTupleFactory.newTuple(1);
-            res.set(0, key);
-        } else {
-            //Create numInputs bags
+        if (isInputAttached()) {
+            // Create numInputs bags
             DataBag[] dbs = null;
             dbs = new DataBag[numInputs];
 
             if (isAccumulative()) {
+                readOnce[numInputs - 1] = false;
                 // create bag wrapper to pull tuples in many batches
                 // all bags have reference to the sample tuples buffer
                 // which contains tuples from one batch
@@ -251,22 +216,35 @@ public class POPackage extends PhysicalOperator {
                 }
 
             } else {
+                readOnce[numInputs - 1] = true;
+                // We know the tuples will come sorted by index, so we can wrap
+                // the last input in a ReadOnceBag and let the Packager decide
+                // whether or not to read into memory
+
                 // create bag to pull all tuples out of iterator
                 for (int i = 0; i < numInputs; i++) {
-                    dbs[i] = useDefaultBag ? BagFactory.getInstance().newDefaultBag()
-                    // In a very rare case if there is a POStream after this
-                    // POPackage in the pipeline and is also blocking the pipeline;
-                    // constructor argument should be 2 * numInputs. But for one obscure
-                    // case we don't want to pay the penalty all the time.
+                    dbs[i] = useDefaultBag ? BagFactory.getInstance()
+                            .newDefaultBag()
+                            // In a very rare case if there is a POStream after this
+                            // POPackage in the pipeline and is also blocking the
+                            // pipeline;
+                            // constructor argument should be 2 * numInputs. But for one
+                            // obscure
+                            // case we don't want to pay the penalty all the time.
                             : new InternalCachedBag(numInputs);
                 }
-                //For each indexed tup in the inp, sort them
-                //into their corresponding bags based
-                //on the index
+                // For each indexed tup in the inp, sort them
+                // into their corresponding bags based
+                // on the index
                 while (tupIter.hasNext()) {
                     NullableTuple ntup = tupIter.next();
                     int index = ntup.getIndex();
-                    Tuple copy = getValueTuple(ntup, index);
+                    if (index == numInputs - 1) {
+                        dbs[index] = new PeekedBag(pkgr, ntup, tupIter,
+                                keyWritable);
+                        break;
+                    }
+                    Tuple copy = pkgr.getValueTuple(keyWritable, ntup, index);
 
                     if (numInputs == 1) {
 
@@ -278,109 +256,29 @@ public class POPackage extends PhysicalOperator {
                     } else {
                         dbs[index].add(copy);
                     }
-                    if(getReporter()!=null) {
+                    if (getReporter() != null) {
                         getReporter().progress();
                     }
                 }
             }
-
-            //Construct the output tuple by appending
-            //the key and all the above constructed bags
-            //and return it.
-            res = mTupleFactory.newTuple(numInputs+1);
-            res.set(0,key);
-            int i=-1;
-            for (DataBag bag : dbs) {
-                i++;
-                if(inner[i] && !isAccumulative()){
-                    if(bag.size()==0){
-                        detachInput();
-                        Result r = new Result();
-                        r.returnStatus = POStatus.STATUS_NULL;
-                        return r;
-                    }
-                }
-
-                res.set(i+1,bag);
-            }
+            // Construct the output tuple by appending
+            // the key and all the above constructed bags
+            // and return it.
+            pkgr.attachInput(key, dbs, readOnce);
+            detachInput();
         }
-        Result r = new Result();
-        r.returnStatus = POStatus.STATUS_OK;
-        if (!isAccumulative())
-            r.result = illustratorMarkup(null, res, 0);
-        else
-            r.result = res;
-        detachInput();
-        return r;
-    }
 
-    protected Tuple getValueTuple(NullableTuple ntup, int index) throws ExecException {
-     // Need to make a copy of the value, as hadoop uses the same ntup
-        // to represent each value.
-        Tuple val = (Tuple)ntup.getValueAsPigType();
-
-        Tuple copy = null;
-        // The "value (val)" that we just got may not
-        // be the complete "value". It may have some portions
-        // in the "key" (look in POLocalRearrange for more comments)
-        // If this is the case we need to stitch
-        // the "value" together.
-        Pair<Boolean, Map<Integer, Integer>> lrKeyInfo =
-            keyInfo.get(index);
-        boolean isProjectStar = lrKeyInfo.first;
-        Map<Integer, Integer> keyLookup = lrKeyInfo.second;
-        int keyLookupSize = keyLookup.size();
-
-        if( keyLookupSize > 0) {
-
-            // we have some fields of the "value" in the
-            // "key".
-            int finalValueSize = keyLookupSize + val.size();
-            copy = mTupleFactory.newTuple(finalValueSize);
-            int valIndex = 0; // an index for accessing elements from
-                              // the value (val) that we have currently
-            for(int i = 0; i < finalValueSize; i++) {
-                Integer keyIndex = keyLookup.get(i);
-                if(keyIndex == null) {
-                    // the field for this index is not in the
-                    // key - so just take it from the "value"
-                    // we were handed
-                    copy.set(i, val.get(valIndex));
-                    valIndex++;
-                } else {
-                    // the field for this index is in the key
-                    if(isKeyTuple && isKeyCompound) {
-                        // the key is a tuple, extract the
-                        // field out of the tuple
-                        copy.set(i, keyAsTuple.get(keyIndex));
-                    } else {
-                        copy.set(i, key);
-                    }
-                }
-            }
-            copy = illustratorMarkup2(val, copy);
-        } else if (isProjectStar) {
-
-            // the whole "value" is present in the "key"
-            copy = mTupleFactory.newTuple(keyAsTuple.getAll());
-            copy = illustratorMarkup2(keyAsTuple, copy);
-        } else {
-
-            // there is no field of the "value" in the
-            // "key" - so just make a copy of what we got
-            // as the "value"
-            copy = mTupleFactory.newTuple(val.getAll());
-            copy = illustratorMarkup2(val, copy);
-        }
-        return copy;
+        return pkgr.getNext();
     }
 
-    public byte getKeyType() {
-        return keyType;
+    public Packager getPkgr() {
+        return pkgr;
     }
 
-    public void setKeyType(byte keyType) {
-        this.keyType = keyType;
+    public void setPkgr(Packager pkgr) {
+        this.pkgr = pkgr;
+        pkgr.setParent(this);
+        pkgr.setIllustrator(illustrator);
     }
 
     /**
@@ -393,74 +291,11 @@ public class POPackage extends PhysicalOperator {
         clone.mKey = new OperatorKey(mKey.scope, NodeIdGenerator.getGenerator().getNextNodeId(mKey.scope));
         clone.requestedParallelism = requestedParallelism;
         clone.resultType = resultType;
-        clone.keyType = keyType;
         clone.numInputs = numInputs;
-        if (inner!=null)
-        {
-            clone.inner = new boolean[inner.length];
-            for (int i = 0; i < inner.length; i++) {
-                clone.inner[i] = inner[i];
-            }
-        }
-        else
-            clone.inner = null;
+        clone.pkgr = (Packager) this.pkgr.clone();
         return clone;
     }
 
-    /**
-     * @param keyInfo the keyInfo to set
-     */
-    public void setKeyInfo(Map<Integer, Pair<Boolean, Map<Integer, Integer>>> keyInfo) {
-        this.keyInfo = keyInfo;
-    }
-
-    /**
-     * @param keyTuple the keyTuple to set
-     */
-    public void setKeyTuple(boolean keyTuple) {
-        this.isKeyTuple = keyTuple;
-    }
-
-    /**
-     * @param keyCompound the keyCompound to set
-     */
-    public void setKeyCompound(boolean keyCompound) {
-        this.isKeyCompound = keyCompound;
-    }
-
-    /**
-     * @return the keyInfo
-     */
-    public Map<Integer, Pair<Boolean, Map<Integer, Integer>>> getKeyInfo() {
-        return keyInfo;
-    }
-
-    /**
-     * @return the distinct
-     */
-    public boolean isDistinct() {
-        return distinct;
-    }
-
-    /**
-     * @param distinct the distinct to set
-     */
-    public void setDistinct(boolean distinct) {
-        this.distinct = distinct;
-    }
-
-    public void setUseSecondaryKey(boolean useSecondaryKey) {
-        this.useSecondaryKey = useSecondaryKey;
-    }
-
-    public void setPackageType(PackageType type) {
-        this.pkgType = type;
-    }
-
-    public PackageType getPackageType() {
-        return pkgType;
-    }
-
     class POPackageTupleBuffer implements AccumulativeTupleBuffer {
         private List<Tuple>[] bags;
         private Iterator<NullableTuple> iter;
@@ -499,25 +334,26 @@ public class POPackage extends PhysicalOperator {
             key = currKey;
             for(int i=0; i<batchSize; i++) {
                 if (iter.hasNext()) {
-                     NullableTuple ntup = iter.next();
-                     int index = ntup.getIndex();
-                     Tuple copy = getValueTuple(ntup, index);
-                     if (numInputs == 1) {
-
-                            // this is for multi-query merge where
-                            // the numInputs is always 1, but the index
-                            // (the position of the inner plan in the
-                            // enclosed operator) may not be 1.
-                            bags[0].add(copy);
-                     } else {
-                            bags[index].add(copy);
-                     }
+                    NullableTuple ntup = iter.next();
+                    int index = ntup.getIndex();
+                    Tuple copy = pkgr.getValueTuple(keyWritable, ntup, index);
+                    if (numInputs == 1) {
+
+                        // this is for multi-query merge where
+                        // the numInputs is always 1, but the index
+                        // (the position of the inner plan in the
+                        // enclosed operator) may not be 1.
+                        bags[0].add(copy);
+                    } else {
+                        bags[index].add(copy);
+                    }
                 }else{
                     break;
                 }
             }
         }
 
+        @Override
         public void clear() {
             for(int i=0; i<bags.length; i++) {
                 bags[i].clear();
@@ -525,6 +361,7 @@ public class POPackage extends PhysicalOperator {
             iter = null;
         }
 
+        @Override
         public Iterator<Tuple> getTuples(int index) {
             return bags[index].iterator();
         }
@@ -532,76 +369,82 @@ public class POPackage extends PhysicalOperator {
         public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
             return POPackage.this.illustratorMarkup(in, out, eqClassIndex);
         }
-       };
-
-       private Tuple illustratorMarkup2(Object in, Object out) {
-           if(illustrator != null) {
-               ExampleTuple tOut = new ExampleTuple((Tuple) out);
-               illustrator.getLineage().insert(tOut);
-               tOut.synthetic = ((ExampleTuple) in).synthetic;
-               illustrator.getLineage().union(tOut, (Tuple) in);
-               return tOut;
-           } else
-               return (Tuple) out;
-       }
-
-       @Override
-       public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
-           if(illustrator != null) {
-               ExampleTuple tOut = new ExampleTuple((Tuple) out);
-               LineageTracer lineageTracer = illustrator.getLineage();
-               lineageTracer.insert(tOut);
-               Tuple tmp;
-               boolean synthetic = false;
-               if (illustrator.getEquivalenceClasses() == null) {
-                   LinkedList<IdentityHashSet<Tuple>> equivalenceClasses = new LinkedList<IdentityHashSet<Tuple>>();
-                   for (int i = 0; i < numInputs; ++i) {
-                       IdentityHashSet<Tuple> equivalenceClass = new IdentityHashSet<Tuple>();
-                       equivalenceClasses.add(equivalenceClass);
-                   }
-                   illustrator.setEquivalenceClasses(equivalenceClasses, this);
-               }
-
-               if (distinct) {
-                   int count;
-                   for (count = 0; tupIter.hasNext(); ++count) {
-                       NullableTuple ntp = tupIter.next();
-                       tmp = (Tuple) ntp.getValueAsPigType();
-                       if (!tmp.equals(tOut))
-                           lineageTracer.union(tOut, tmp);
-                   }
-                   if (count > 1) // only non-distinct tuples are inserted into the equivalence class
-                       illustrator.getEquivalenceClasses().get(eqClassIndex).add(tOut);
-                   illustrator.addData((Tuple) tOut);
-                   return (Tuple) tOut;
-               }
-               boolean outInEqClass = true;
-               try {
-                   for (int i = 1; i < numInputs+1; i++)
-                   {
-                       DataBag dbs = (DataBag) ((Tuple) out).get(i);
-                       Iterator<Tuple> iter = dbs.iterator();
-                       if (dbs.size() <= 1 && outInEqClass) // all inputs have >= 2 records
-                           outInEqClass = false;
-                       while (iter.hasNext()) {
-                           tmp = iter.next();
-                           // any of synthetic data in bags causes the output tuple to be synthetic
-                           if (!synthetic && ((ExampleTuple)tmp).synthetic)
-                               synthetic = true;
-                           lineageTracer.union(tOut, tmp);
-                       }
-                   }
-               } catch (ExecException e) {
-                 // TODO better exception handling
-                 throw new RuntimeException("Illustrator exception :"+e.getMessage());
-               }
-               if (outInEqClass)
-                   illustrator.getEquivalenceClasses().get(eqClassIndex).add(tOut);
-               tOut.synthetic = synthetic;
-               illustrator.addData((Tuple) tOut);
-               return tOut;
-           } else
-               return (Tuple) out;
-       }
+    };
+
+    @Override
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+        return pkgr.illustratorMarkup(in, out, eqClassIndex);
+    }
+
+    public int numberOfEquivalenceClasses() {
+        return pkgr.numberOfEquivalenceClasses();
+    }
+
+    // A ReadOnceBag that we've already "peeked" at
+    private static class PeekedBag extends ReadOnceBag {
+        /**
+         * 
+         */
+        private static final long serialVersionUID = 1L;
+        NullableTuple head;
+        int index;
+
+        public PeekedBag(Packager pkgr, NullableTuple head,
+                Iterator<NullableTuple> tupIter,
+                PigNullableWritable keyWritable) {
+            super(pkgr, tupIter, keyWritable);
+            this.head = head;
+            this.index = head.getIndex();
+        }
+
+        @Override
+        public Iterator<Tuple> iterator() {
+            return new Iterator<Tuple>() {
+                boolean headReturned = false;
+
+                @Override
+                public boolean hasNext() {
+                    if (!headReturned)
+                        return true;
+
+                    return tupIter.hasNext();
+                }
+
+                @Override
+                public Tuple next() {
+                    if (!headReturned) {
+                        headReturned = true;
+                        try {
+                            return pkgr.getValueTuple(keyWritable, head,
+                                    head.getIndex());
+                        } catch (ExecException e) {
+                            throw new RuntimeException(
+                                    "PeekedBag failed to get value tuple : "
+                                            + e.toString());
+                        }
+                    }
+                    NullableTuple ntup = tupIter.next();
+                    Tuple ret = null;
+                    try {
+                        ret = pkgr.getValueTuple(keyWritable, ntup, index);
+                    } catch (ExecException e) {
+                        throw new RuntimeException(
+                                "PeekedBag failed to get value tuple : "
+                                        + e.toString());
+                    }
+                    if (getReporter() != null) {
+                        getReporter().progress();
+                    }
+                    return ret;
+                }
+
+                @Override
+                public void remove() {
+                    throw new UnsupportedOperationException(
+                            "PeekedBag does not support removal");
+                }
+            };
+        }
+    }
 
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPackageLite.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPackageLite.java
deleted file mode 100644
index c200715e4..000000000
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPackageLite.java
+++ /dev/null
@@ -1,235 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * 
- */
-package org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators;
-
-import java.util.Iterator;
-import java.util.LinkedList;
-import java.util.List;
-import java.util.HashMap;
-import java.util.Map;
-import java.util.Map.Entry;
-
-import org.apache.pig.impl.util.IdentityHashSet;
-import org.apache.pig.impl.util.Pair;
-
-import org.apache.pig.PigException;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataType;
-import org.apache.pig.data.ReadOnceBag;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.impl.io.NullableTuple;
-import org.apache.pig.impl.plan.NodeIdGenerator;
-import org.apache.pig.impl.plan.OperatorKey;
-import org.apache.pig.impl.plan.VisitorException;
-import org.apache.pig.pen.util.ExampleTuple;
-import org.apache.pig.pen.util.LineageTracer;
-
-/**
- * This package operator is a specialization
- * of POPackage operator used for the specific
- * case of the order by query. See JIRA 802 
- * for more details. 
- */
-public class POPackageLite extends POPackage {
-
-    /**
-     * 
-     */
-    private static final long serialVersionUID = 1L;
-
-    public POPackageLite(OperatorKey k) {
-        super(k, -1, null);
-    }
-
-    public POPackageLite(OperatorKey k, int rp) {
-        super(k, rp, null);
-    }
-
-    public POPackageLite(OperatorKey k, List<PhysicalOperator> inp) {
-        super(k, -1, inp);
-    }
-
-    public POPackageLite(OperatorKey k, int rp, List<PhysicalOperator> inp) {
-        super(k, rp, inp);
-    }
-
-    /* (non-Javadoc)
-     * @see org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage#setNumInps(int)
-     */
-	@Override
-    public void setNumInps(int numInps) {
-		if(numInps != 1)
-		{
-			throw new RuntimeException("POPackageLite can only take 1 input");
-		}
-        this.numInputs = numInps;
-    }
-	
-    public boolean[] getInner() {
-        throw new RuntimeException("POPackageLite does not support getInner operation");
-    }
-
-    public void setInner(boolean[] inner) {
-        throw new RuntimeException("POPackageLite does not support setInner operation");
-    }
-    
-    /**
-     * Make a deep copy of this operator.  
-     * @throws CloneNotSupportedException
-     */
-    @Override
-    public POPackageLite clone() throws CloneNotSupportedException {
-        POPackageLite clone = (POPackageLite)super.clone();
-        clone.inner = null;
-        clone.keyInfo = new HashMap<Integer, Pair<Boolean,Map<Integer,Integer>>>();
-        for (Entry<Integer, Pair<Boolean, Map<Integer,Integer>>> entry: keyInfo.entrySet()) {
-            clone.keyInfo.put(entry.getKey(), entry.getValue());
-        }
-        return clone;
-    }
-    
-    /**
-     * @return the distinct
-     */
-    @Override
-    public boolean isDistinct() {
-        throw new RuntimeException("POPackageLite does not support isDistinct operation");
-    }
-
-    /**
-     * @param distinct the distinct to set
-     */
-    @Override
-    public void setDistinct(boolean distinct) {
-        throw new RuntimeException("POPackageLite does not support setDistinct operation");
-    }
-
-    /**
-     * @return the isKeyTuple
-     */
-    public boolean getKeyTuple() {
-        return isKeyTuple;
-    }
-
-    /**
-     * @return the keyAsTuple
-     */
-    public Tuple getKeyAsTuple() {
-        return keyAsTuple;
-    }
-
-    /**
-     * @return the tupIter
-     */
-    public Iterator<NullableTuple> getTupIter() {
-        return tupIter;
-    }
-
-    /**
-     * @return the key
-     */
-    public Object getKey() {
-        return key;
-    }
-
-    /**
-     * Similar to POPackage.getNext except that
-     * only one input is expected with index 0 
-     * and ReadOnceBag is used instead of 
-     * DefaultDataBag.
-     */
-    @Override
-    public Result getNextTuple() throws ExecException {
-        Tuple res;
-        //Create numInputs bags
-        ReadOnceBag db = null;
-        db = new ReadOnceBag(this, tupIter, key);
-        if(getReporter()!=null) {
-            getReporter().progress();
-        }
-        
-        //Construct the output tuple by appending
-        //the key and all the above constructed bags
-        //and return it.
-        res = mTupleFactory.newTuple(numInputs+1);
-        res.set(0,key);
-        res.set(1,db);
-        detachInput();
-        Result r = new Result();
-        r.returnStatus = POStatus.STATUS_OK;
-        r.result = illustratorMarkup(null, res, 0);
-        return r;
-    }
-    
-    /**
-     * Makes use of the superclass method, but this requires 
-     * an additional parameter key passed by ReadOnceBag.
-     * key of this instance will be set to null in detachInput 
-     * call, but an instance of ReadOnceBag may have the original 
-     * key that it uses. Therefore this extra argument is taken
-     * to temporarily set it before the call to the superclass method 
-     * and then restore it.  
-     */
-    public Tuple getValueTuple(NullableTuple ntup, int index, Object key) throws ExecException {
-        Object origKey = this.key;
-        this.key = key;
-        Tuple retTuple = super.getValueTuple(ntup, index);
-        this.key = origKey;
-        return retTuple;
-    }
-
-    @Override
-    public String name() {
-        return getAliasString() + "PackageLite" + "["
-                + DataType.findTypeName(resultType) + "]" + "{"
-                + DataType.findTypeName(keyType) + "}" + " - "
-                + mKey.toString();
-    }
-    
-    @Override
-    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
-        if(illustrator != null) {
-            ExampleTuple tOut = new ExampleTuple((Tuple) out);
-            LineageTracer lineageTracer = illustrator.getLineage();
-            lineageTracer.insert(tOut);
-            if (illustrator.getEquivalenceClasses() == null) {
-                LinkedList<IdentityHashSet<Tuple>> equivalenceClasses = new LinkedList<IdentityHashSet<Tuple>>();
-                for (int i = 0; i < numInputs; ++i) {
-                    IdentityHashSet<Tuple> equivalenceClass = new IdentityHashSet<Tuple>();
-                    equivalenceClasses.add(equivalenceClass);
-                }
-                illustrator.setEquivalenceClasses(equivalenceClasses, this);
-            }
-            illustrator.getEquivalenceClasses().get(eqClassIndex).add(tOut);
-            tOut.synthetic = false;  // not expect this to be really used
-            illustrator.addData((Tuple) tOut);
-            return tOut;
-        } else
-            return (Tuple) out;
-    }
-}
-
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPartialAgg.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPartialAgg.java
index a92e6437a..05365d429 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPartialAgg.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPartialAgg.java
@@ -229,7 +229,7 @@ public class POPartialAgg extends PhysicalOperator implements Spillable {
                 }
             }
             avgTupleSize = estTotalMem / estTuples;
-            int totalTuples = memLimits.getCacheLimit();
+            long totalTuples = memLimits.getCacheLimit();
             LOG.info("Estimated total tuples to buffer, based on " + estTuples + " tuples that took up " + estTotalMem + " bytes: " + totalTuples);
             firstTierThreshold = (int) (0.5 + totalTuples * (1f - (1f / sizeReduction)));
             secondTierThreshold = (int) (0.5 + totalTuples *  (1f / sizeReduction));
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/Packager.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/Packager.java
new file mode 100644
index 000000000..cca17b980
--- /dev/null
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/Packager.java
@@ -0,0 +1,486 @@
+package org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators;
+
+import java.io.Serializable;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.LinkedList;
+import java.util.Map;
+
+import org.apache.pig.backend.executionengine.ExecException;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
+import org.apache.pig.data.BagFactory;
+import org.apache.pig.data.DataBag;
+import org.apache.pig.data.InternalCachedBag;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.data.TupleFactory;
+import org.apache.pig.impl.io.NullableTuple;
+import org.apache.pig.impl.io.PigNullableWritable;
+import org.apache.pig.impl.util.IdentityHashSet;
+import org.apache.pig.impl.util.Pair;
+import org.apache.pig.pen.Illustrable;
+import org.apache.pig.pen.Illustrator;
+import org.apache.pig.pen.util.ExampleTuple;
+import org.apache.pig.pen.util.LineageTracer;
+
+public class Packager implements Illustrable, Serializable, Cloneable {
+
+    /**
+     * 
+     */
+    private static final long serialVersionUID = 1L;
+
+    protected boolean[] readOnce;
+
+    protected DataBag[] bags;
+
+    public static enum PackageType {
+        GROUP, JOIN
+    };
+
+    protected transient Illustrator illustrator = null;
+
+    // The key being worked on
+    Object key;
+
+    // marker to indicate if key is a tuple
+    protected boolean isKeyTuple = false;
+    // marker to indicate if the tuple key is compound in nature
+    protected boolean isKeyCompound = false;
+
+    // key's type
+    byte keyType;
+
+    // The number of inputs to this
+    // co-group. 0 indicates a distinct, which means there will only be a
+    // key, no value.
+    int numInputs;
+
+    // If the attaching map-reduce plan use secondary sort key
+    boolean useSecondaryKey = false;
+
+    // Denotes if inner is specified
+    // on a particular input
+    boolean[] inner;
+
+    // flag to denote whether there is a distinct
+    // leading to this package
+    protected boolean distinct = false;
+
+    // A mapping of input index to key information got from LORearrange
+    // for that index. The Key information is a pair of boolean, Map.
+    // The boolean indicates whether there is a lone project(*) in the
+    // cogroup by. If not, the Map has a mapping of column numbers in the
+    // "value" to column numbers in the "key" which contain the fields in
+    // the "value"
+    protected Map<Integer, Pair<Boolean, Map<Integer, Integer>>> keyInfo;
+
+    private PackageType pkgType;
+
+    boolean firstTime = true;
+    boolean useDefaultBag = false;
+
+    protected POPackage parent = null;
+
+    protected static final BagFactory mBagFactory = BagFactory.getInstance();
+    protected static final TupleFactory mTupleFactory = TupleFactory
+            .getInstance();
+
+    public Object getKey(PigNullableWritable key) throws ExecException {
+        Object keyObject = key.getValueAsPigType();
+        if (useSecondaryKey) {
+            return ((Tuple) keyObject).get(0);
+        } else {
+            return keyObject;
+        }
+    }
+
+    public void attachInput(Object key, DataBag[] bags, boolean[] readOnce)
+            throws ExecException {
+        checkBagType();
+
+        this.key = key;
+        this.bags = bags;
+        this.readOnce = readOnce;
+        // We assume that we need all bags materialized. Specialized subclasses
+        // may choose to handle this differently
+        for (int i = 0; i < bags.length; i++) {
+            if (readOnce[i]) {
+                DataBag materializedBag = getBag();
+                materializedBag.addAll(bags[i]);
+                bags[i] = materializedBag;
+            }
+        }
+    }
+
+    public Result getNext() throws ExecException {
+        if (bags == null) {
+            return new Result(POStatus.STATUS_EOP, null);
+        }
+        Tuple res;
+
+        if (isDistinct()) {
+            // only set the key which has the whole
+            // tuple
+            res = mTupleFactory.newTuple(1);
+            res.set(0, key);
+        } else {
+            // Construct the output tuple by appending
+            // the key and all the above constructed bags
+            // and return it.
+            res = mTupleFactory.newTuple(numInputs + 1);
+            res.set(0, key);
+            int i = -1;
+            for (DataBag bag : bags) {
+                i++;
+                if (inner[i]) {
+                    if (bag.size() == 0) {
+                        detachInput();
+                        Result r = new Result();
+                        r.returnStatus = POStatus.STATUS_NULL;
+                        return r;
+                    }
+                }
+
+                res.set(i + 1, bag);
+            }
+        }
+        Result r = new Result();
+        r.returnStatus = POStatus.STATUS_OK;
+        r.result = illustratorMarkup(null, res, 0);
+        detachInput();
+        return r;
+    }
+
+    public void detachInput() {
+        key = null;
+        bags = null;
+    }
+
+    protected Tuple illustratorMarkup2(Object in, Object out) {
+        if (illustrator != null) {
+            ExampleTuple tOut;
+            if (!(out instanceof ExampleTuple)) {
+                tOut = new ExampleTuple((Tuple) out);
+            } else {
+                tOut = (ExampleTuple) out;
+            }
+            illustrator.getLineage().insert(tOut);
+            tOut.synthetic = ((ExampleTuple) in).synthetic;
+            illustrator.getLineage().union(tOut, (Tuple) in);
+            return tOut;
+        } else
+            return (Tuple) out;
+    }
+
+    protected Tuple starMarkup(Tuple key, Tuple val, Tuple out){
+        if (illustrator != null){
+            Tuple copy = illustratorMarkup2(key, out);
+            // For distinct, we also need to retain lineage information from the values.
+            if (isDistinct())
+                copy = illustratorMarkup2(val, out);
+            return copy;
+        } else
+            return (Tuple) out;
+    }
+
+    public Tuple getValueTuple(PigNullableWritable keyWritable,
+            NullableTuple ntup, int index) throws ExecException {
+        Object key = getKey(keyWritable);
+        // Need to make a copy of the value, as hadoop uses the same ntup
+        // to represent each value.
+        Tuple val = (Tuple) ntup.getValueAsPigType();
+
+        Tuple copy = null;
+        // The "value (val)" that we just got may not
+        // be the complete "value". It may have some portions
+        // in the "key" (look in POLocalRearrange for more comments)
+        // If this is the case we need to stitch
+        // the "value" together.
+        Pair<Boolean, Map<Integer, Integer>> lrKeyInfo = keyInfo.get(index);
+        boolean isProjectStar = lrKeyInfo.first;
+        Map<Integer, Integer> keyLookup = lrKeyInfo.second;
+        int keyLookupSize = keyLookup.size();
+
+        if (keyLookupSize > 0) {
+
+            // we have some fields of the "value" in the
+            // "key".
+            int finalValueSize = keyLookupSize + val.size();
+            copy = mTupleFactory.newTuple(finalValueSize);
+            int valIndex = 0; // an index for accessing elements from
+            // the value (val) that we have currently
+            for (int i = 0; i < finalValueSize; i++) {
+                Integer keyIndex = keyLookup.get(i);
+                if (keyIndex == null) {
+                    // the field for this index is not in the
+                    // key - so just take it from the "value"
+                    // we were handed
+                    copy.set(i, val.get(valIndex));
+                    valIndex++;
+                } else {
+                    // the field for this index is in the key
+                    if (isKeyTuple && isKeyCompound) {
+                        // the key is a tuple, extract the
+                        // field out of the tuple
+                        copy.set(i, ((Tuple) key).get(keyIndex));
+                    } else {
+                        copy.set(i, key);
+                    }
+                }
+            }
+            copy = illustratorMarkup2(val, copy);
+        } else if (isProjectStar) {
+
+            // the whole "value" is present in the "key"
+            copy = mTupleFactory.newTuple(((Tuple) key).getAll());
+            copy = starMarkup((Tuple) key, val, copy);
+        } else {
+
+            // there is no field of the "value" in the
+            // "key" - so just make a copy of what we got
+            // as the "value"
+            copy = mTupleFactory.newTuple(val.getAll());
+            copy = illustratorMarkup2(val, copy);
+        }
+        return copy;
+    }
+
+    public byte getKeyType() {
+        return keyType;
+    }
+
+    public void setKeyType(byte keyType) {
+        this.keyType = keyType;
+    }
+
+    /**
+     * @return the isKeyTuple
+     */
+    public boolean getKeyTuple() {
+        return isKeyTuple;
+    }
+
+    /**
+     * @return the keyAsTuple
+     */
+    public Tuple getKeyAsTuple() {
+        return isKeyTuple ? (Tuple) key : null;
+    }
+
+    /**
+     * @return the key
+     */
+    public Object getKey() {
+        return key;
+    }
+
+    public boolean[] getInner() {
+        return inner;
+    }
+
+    public void setInner(boolean[] inner) {
+        this.inner = inner;
+    }
+
+    /**
+     * @param keyInfo
+     *            the keyInfo to set
+     */
+    public void setKeyInfo(
+            Map<Integer, Pair<Boolean, Map<Integer, Integer>>> keyInfo) {
+        this.keyInfo = keyInfo;
+    }
+
+    /**
+     * @param keyTuple
+     *            the keyTuple to set
+     */
+    public void setKeyTuple(boolean keyTuple) {
+        this.isKeyTuple = keyTuple;
+    }
+
+    /**
+     * @param keyCompound
+     *            the keyCompound to set
+     */
+    public void setKeyCompound(boolean keyCompound) {
+        this.isKeyCompound = keyCompound;
+    }
+
+    /**
+     * @return the keyInfo
+     */
+    public Map<Integer, Pair<Boolean, Map<Integer, Integer>>> getKeyInfo() {
+        return keyInfo;
+    }
+
+    public Illustrator getIllustrator() {
+        return illustrator;
+    }
+
+    @Override
+    public void setIllustrator(Illustrator illustrator) {
+        this.illustrator = illustrator;
+    }
+
+    /**
+     * @return the distinct
+     */
+    public boolean isDistinct() {
+        return distinct;
+    }
+
+    /**
+     * @param distinct
+     *            the distinct to set
+     */
+    public void setDistinct(boolean distinct) {
+        this.distinct = distinct;
+    }
+
+    public void setUseSecondaryKey(boolean useSecondaryKey) {
+        this.useSecondaryKey = useSecondaryKey;
+    }
+
+    public void setPackageType(PackageType type) {
+        this.pkgType = type;
+    }
+
+    public PackageType getPackageType() {
+        return pkgType;
+    }
+
+    public int getNumInputs(byte index) {
+        return numInputs;
+    }
+
+    public int getNumInputs() {
+        return numInputs;
+    }
+
+    public void setNumInputs(int numInputs) {
+        this.numInputs = numInputs;
+    }
+
+    @Override
+    public Packager clone() throws CloneNotSupportedException {
+        Packager clone = (Packager) super.clone();
+        clone.setNumInputs(numInputs);
+        clone.setPackageType(pkgType);
+        clone.setDistinct(distinct);
+        if (inner != null) {
+            clone.inner = new boolean[inner.length];
+            for (int i = 0; i < inner.length; i++) {
+                clone.inner[i] = inner[i];
+            }
+        } else
+            clone.inner = null;
+        if (keyInfo != null)
+            clone.setKeyInfo(new HashMap<Integer, Pair<Boolean, Map<Integer, Integer>>>(
+                    keyInfo));
+        clone.setKeyCompound(isKeyCompound);
+        clone.setKeyTuple(isKeyTuple);
+        clone.setKeyType(keyType);
+        clone.setUseSecondaryKey(useSecondaryKey);
+        return clone;
+    }
+
+    public String name() {
+        return this.getClass().getSimpleName();
+    }
+
+    @Override
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+        // All customized packagers are introduced during MRCompilaition.
+        // Illustrate happens before that, so we only have to focus on the basic
+        // POPackage
+        if (illustrator != null) {
+            ExampleTuple tOut = new ExampleTuple((Tuple) out);
+            LineageTracer lineageTracer = illustrator.getLineage();
+            lineageTracer.insert(tOut);
+            boolean synthetic = false;
+            if (illustrator.getEquivalenceClasses() == null) {
+                LinkedList<IdentityHashSet<Tuple>> equivalenceClasses = new LinkedList<IdentityHashSet<Tuple>>();
+                for (int i = 0; i < numInputs; ++i) {
+                    IdentityHashSet<Tuple> equivalenceClass = new IdentityHashSet<Tuple>();
+                    equivalenceClasses.add(equivalenceClass);
+                }
+                illustrator.setEquivalenceClasses(equivalenceClasses, parent);
+            }
+
+            if (isDistinct()) {
+                int count = 0;
+                for (Tuple tmp : bags[0]){
+                    count++;
+                    if (!tmp.equals(tOut))
+                        lineageTracer.union(tOut, tmp);
+                }
+                if (count > 1) // only non-distinct tuples are inserted into the
+                    // equivalence class
+                    illustrator.getEquivalenceClasses().get(eqClassIndex)
+                    .add(tOut);
+                illustrator.addData((Tuple) tOut);
+                return (Tuple) tOut;
+            }
+            boolean outInEqClass = true;
+            try {
+                for (int i = 1; i < numInputs + 1; i++) {
+                    DataBag dbs = (DataBag) ((Tuple) out).get(i);
+                    Iterator<Tuple> iter = dbs.iterator();
+                    if (dbs.size() <= 1 && outInEqClass) // all inputs have >= 2
+                        // records
+                        outInEqClass = false;
+                    while (iter.hasNext()) {
+                        Tuple tmp = iter.next();
+                        // any of synthetic data in bags causes the output tuple
+                        // to be synthetic
+                        if (!synthetic && ((ExampleTuple) tmp).synthetic)
+                            synthetic = true;
+                        lineageTracer.union(tOut, tmp);
+                    }
+                }
+            } catch (ExecException e) {
+                // TODO better exception handling
+                throw new RuntimeException("Illustrator exception :"
+                        + e.getMessage());
+            }
+            if (outInEqClass)
+                illustrator.getEquivalenceClasses().get(eqClassIndex).add(tOut);
+            tOut.synthetic = synthetic;
+            illustrator.addData((Tuple) tOut);
+            return tOut;
+        } else
+            return (Tuple) out;
+    }
+
+    public void setParent(POPackage pack) {
+        parent = pack;
+    }
+
+    public int numberOfEquivalenceClasses() {
+        return 1;
+    }
+
+    public void checkBagType() {
+        if(firstTime){
+            firstTime = false;
+            if (PigMapReduce.sJobConfInternal.get() != null) {
+                String bagType = PigMapReduce.sJobConfInternal.get().get("pig.cachedbag.type");
+                if (bagType != null && bagType.equalsIgnoreCase("default")) {
+                    useDefaultBag = true;
+                }
+            }
+        }
+    }
+
+    public DataBag getBag(){
+        return useDefaultBag ? mBagFactory.newDefaultBag()
+                // In a very rare case if there is a POStream after this
+                // POJoinPackage in the pipeline and is also blocking the pipeline;
+                // constructor argument should be 2 * numInputs. But for one obscure
+                // case we don't want to pay the penalty all the time.
+                : new InternalCachedBag(numInputs-1);
+    }
+}
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/util/PlanHelper.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/util/PlanHelper.java
index b8605210b..4b4e03d47 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/util/PlanHelper.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/util/PlanHelper.java
@@ -51,7 +51,6 @@ import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOpe
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCollectedGroup;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCombinerPackage;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCross;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODemux;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODistinct;
@@ -59,13 +58,11 @@ import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOpe
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POGlobalRearrange;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POJoinPackage;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLimit;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeCogroup;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMultiQueryPackage;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PONative;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POOptimizedForEach;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage;
@@ -208,20 +205,6 @@ public class PlanHelper {
             visit(pkg);
         }
 
-        @Override
-        public void visitCombinerPackage(POCombinerPackage pkg)
-                throws VisitorException {
-            super.visitCombinerPackage(pkg);
-            visit(pkg);
-        }
-
-        @Override
-        public void visitMultiQueryPackage(POMultiQueryPackage pkg)
-                throws VisitorException {
-            super.visitMultiQueryPackage(pkg);
-            visit(pkg);
-        }
-
         @Override
         public void visitPOForEach(POForEach nfe) throws VisitorException {
             super.visitPOForEach(nfe);
@@ -399,13 +382,6 @@ public class PlanHelper {
             visit(mapLookUp);
         }
 
-        @Override
-        public void visitJoinPackage(POJoinPackage joinPackage)
-                throws VisitorException {
-            super.visitJoinPackage(joinPackage);
-            visit(joinPackage);
-        }
-
         @Override
         public void visitCast(POCast cast) {
             super.visitCast(cast);
@@ -449,7 +425,6 @@ public class PlanHelper {
             visit(stream);
         }
 
-        @Override
         public void visitSkewedJoin(POSkewedJoin sk) throws VisitorException {
             super.visitSkewedJoin(sk);
             visit(sk);
diff --git a/src/org/apache/pig/data/ReadOnceBag.java b/src/org/apache/pig/data/ReadOnceBag.java
index e2b388717..9e8e8cd4b 100644
--- a/src/org/apache/pig/data/ReadOnceBag.java
+++ b/src/org/apache/pig/data/ReadOnceBag.java
@@ -22,30 +22,28 @@ import java.io.DataInput;
 import java.io.DataOutput;
 import java.io.IOException;
 import java.util.Iterator;
+
 import org.apache.pig.PigException;
 import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackageLite;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.Packager;
 import org.apache.pig.impl.io.NullableTuple;
+import org.apache.pig.impl.io.PigNullableWritable;
 
 /**
- * This bag is specifically created for use by POPackageLite. So it has three 
- * properties, the NullableTuple iterator, the key (Object) and the keyInfo 
- * (Map<Integer, Pair<Boolean, Map<Integer, Integer>>>) all three 
- * of which are required in the constructor call. This bag does not store 
- * the tuples in memory, but has access to an iterator typically provided by 
- * Hadoop. Use this when you already have an iterator over tuples and do not 
- * want to copy over again to a new bag.
+ * This bag does not store the tuples in memory, but has access to an iterator
+ * typically provided by Hadoop. Use this when you already have an iterator over
+ * tuples and do not want to copy over again to a new bag.
  */
 public class ReadOnceBag implements DataBag {
 
-    // The Package operator that created this
-    POPackageLite pkg;
+    // The Packager that created this
+    protected Packager pkgr;
     
     //The iterator of Tuples. Marked transient because we will never serialize this.
-    transient Iterator<NullableTuple> tupIter;
+    protected transient Iterator<NullableTuple> tupIter;
     
     // The key being worked on
-    Object key;
+    protected PigNullableWritable keyWritable;
 
     /**
      * 
@@ -60,10 +58,11 @@ public class ReadOnceBag implements DataBag {
      * @param tupIter Iterator<NullableTuple>
      * @param key Object
      */
-    public ReadOnceBag(POPackageLite pkg, Iterator<NullableTuple> tupIter, Object key) {
-        this.pkg = pkg;
+    public ReadOnceBag(Packager pkgr, Iterator<NullableTuple> tupIter,
+            PigNullableWritable keyWritable) {
+        this.pkgr = pkgr;
         this.tupIter = tupIter;
-        this.key = key;
+        this.keyWritable = keyWritable;
     }
 
     /* (non-Javadoc)
@@ -177,27 +176,23 @@ public class ReadOnceBag implements DataBag {
 
     @Override
     public boolean equals(Object other) {
-        if(other instanceof ReadOnceBag)
-        {
-            if(pkg.getKeyTuple())
-            {
-                if(tupIter == ((ReadOnceBag)other).tupIter && pkg.getKeyTuple() == ((ReadOnceBag)other).pkg.getKeyTuple() && pkg.getKeyAsTuple().equals(((ReadOnceBag)other).pkg.getKeyAsTuple()))
-                {
+        if (other instanceof ReadOnceBag) {
+            if (pkgr.getKeyTuple()) {
+                if (tupIter == ((ReadOnceBag) other).tupIter
+                        && pkgr.getKeyTuple() == ((ReadOnceBag) other).pkgr
+                                .getKeyTuple()
+                        && pkgr.getKeyAsTuple().equals(
+                                ((ReadOnceBag) other).pkgr.getKeyAsTuple())) {
                     return true;
-                }
-                else
-                {
+                } else {
                     return false;
                 }
-            }
-            else
-            {
-                if(tupIter == ((ReadOnceBag)other).tupIter && pkg.getKey().equals(((ReadOnceBag)other).pkg.getKey()))
-                {
+            } else {
+                if (tupIter == ((ReadOnceBag) other).tupIter
+                        && pkgr.getKey().equals(
+                                ((ReadOnceBag) other).pkgr.getKey())) {
                     return true;
-                }
-                else
-                {
+                } else {
                     return false;
                 }
             }
@@ -208,18 +203,18 @@ public class ReadOnceBag implements DataBag {
     @Override
     public int hashCode() {
     	int hash = 7;
-        if(pkg.getKeyTuple())
+        if (pkgr.getKeyTuple())
         {
-            hash = hash*31 + pkg.getKeyAsTuple().hashCode();
+            hash = hash * 31 + pkgr.getKeyAsTuple().hashCode();
         }
         else
         {
-        	hash = hash*31 + pkg.getKey().hashCode();
+            hash = hash * 31 + pkgr.getKey().hashCode();
         }
         return hash;
     }
 
-    class ReadOnceBagIterator implements Iterator<Tuple>
+    protected class ReadOnceBagIterator implements Iterator<Tuple>
     {
         /* (non-Javadoc)
          * @see java.util.Iterator#hasNext()
@@ -238,7 +233,7 @@ public class ReadOnceBag implements DataBag {
             int index = ntup.getIndex();
             Tuple ret = null;
             try {
-                ret = pkg.getValueTuple(ntup, index, key);
+                ret = pkgr.getValueTuple(keyWritable, ntup, index);
             } catch (ExecException e)
             {
             	throw new RuntimeException("ReadOnceBag failed to get value tuple : "+e.toString());
diff --git a/src/org/apache/pig/data/SelfSpillBag.java b/src/org/apache/pig/data/SelfSpillBag.java
index 2fcf36a8c..0dbf7db3e 100644
--- a/src/org/apache/pig/data/SelfSpillBag.java
+++ b/src/org/apache/pig/data/SelfSpillBag.java
@@ -53,10 +53,10 @@ public abstract class SelfSpillBag extends DefaultAbstractBag {
     public static class MemoryLimits {
 
         private long maxMemUsage;
-        private int cacheLimit = Integer.MAX_VALUE;
+        private long cacheLimit = Integer.MAX_VALUE;
         private long memUsage = 0;
         private long numObjsSizeChecked = 0;
-        
+
         private static float cachedMemUsage = 0.2F;
         private static long maxMem = 0;
         static {
@@ -99,11 +99,11 @@ public abstract class SelfSpillBag extends DefaultAbstractBag {
          * 
          * @return number of objects limit
          */
-        public int getCacheLimit() {
+        public long getCacheLimit() {
             if (numObjsSizeChecked > 0) {
                 long avgUsage = memUsage / numObjsSizeChecked;
                 if (avgUsage > 0) {
-                    cacheLimit = (int) (maxMemUsage / avgUsage);
+                    cacheLimit = maxMemUsage / avgUsage;
                 }
             }
             return cacheLimit;
diff --git a/src/org/apache/pig/newplan/logical/relational/LogToPhyTranslationVisitor.java b/src/org/apache/pig/newplan/logical/relational/LogToPhyTranslationVisitor.java
index 7112695f0..ac6ca76e5 100644
--- a/src/org/apache/pig/newplan/logical/relational/LogToPhyTranslationVisitor.java
+++ b/src/org/apache/pig/newplan/logical/relational/LogToPhyTranslationVisitor.java
@@ -56,7 +56,6 @@ import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOpe
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PONative;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage.PackageType;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PORank;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSkewedJoin;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSort;
@@ -64,6 +63,7 @@ import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOpe
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStream;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POUnion;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.Packager.PackageType;
 import org.apache.pig.data.DataType;
 import org.apache.pig.data.SchemaTupleClassGenerator.GenContext;
 import org.apache.pig.data.SchemaTupleFrontend;
@@ -357,7 +357,7 @@ public class LogToPhyTranslationVisitor extends LogicalRelationalNodesVisitor {
                     expressionPlans.put(i,loRank.getRankColPlans());
 
                 POPackage poPackage = compileToLR_GR_PackTrio(loRank, null, flags, expressionPlans);
-                poPackage.setPackageType(PackageType.GROUP);
+                poPackage.getPkgr().setPackageType(PackageType.GROUP);
                 translateSoftLinks(loRank);
 
                 List<Boolean> flattenLst = Arrays.asList(true, false);
@@ -366,7 +366,7 @@ public class LogToPhyTranslationVisitor extends LogicalRelationalNodesVisitor {
                 POProject feproj1 = new POProject(new OperatorKey(scope, nodeGen.getNextNodeId(scope)), -1);
                 feproj1.addOriginalLocation(loRank.getAlias(), loRank.getLocation());
                 feproj1.setColumn(0);
-                feproj1.setResultType(poPackage.getKeyType());
+                feproj1.setResultType(poPackage.getPkgr().getKeyType());
                 feproj1.setStar(false);
                 feproj1.setOverloaded(false);
                 fep1.add(feproj1);
@@ -667,14 +667,14 @@ public class LogToPhyTranslationVisitor extends LogicalRelationalNodesVisitor {
                 throw new VisitorException(msg, errCode, PigException.BUG, e);
             }
 
-            poPackage.setKeyType(DataType.TUPLE);
+            poPackage.getPkgr().setKeyType(DataType.TUPLE);
             poPackage.setResultType(DataType.TUPLE);
             poPackage.setNumInps(count);
             boolean inner[] = new boolean[count];
             for (int i=0;i<count;i++) {
                 inner[i] = true;
             }
-            poPackage.setInner(inner);
+            poPackage.getPkgr().setInner(inner);
 
             List<PhysicalPlan> fePlans = new ArrayList<PhysicalPlan>();
             List<Boolean> flattenLst = new ArrayList<Boolean>();
@@ -999,7 +999,7 @@ public class LogToPhyTranslationVisitor extends LogicalRelationalNodesVisitor {
             break;
         case REGULAR:
             POPackage poPackage = compileToLR_GR_PackTrio(cg, cg.getCustomPartitioner(), cg.getInner(), cg.getExpressionPlans());
-            poPackage.setPackageType(PackageType.GROUP);            
+            poPackage.getPkgr().setPackageType(PackageType.GROUP);
             logToPhyMap.put(cg, poPackage);
             break;
         case MERGE:
@@ -1414,7 +1414,7 @@ public class LogToPhyTranslationVisitor extends LogicalRelationalNodesVisitor {
                         e.getErrorCode(),e.getErrorSource(),e);
             }
             logToPhyMap.put(loj, fe);
-            poPackage.setPackageType(POPackage.PackageType.JOIN);
+            poPackage.getPkgr().setPackageType(PackageType.JOIN);
         }
         translateSoftLinks(loj);
     }
@@ -1485,10 +1485,10 @@ public class LogToPhyTranslationVisitor extends LogicalRelationalNodesVisitor {
             }
         }
 
-        poPackage.setKeyType(type);
+        poPackage.getPkgr().setKeyType(type);
         poPackage.setResultType(DataType.TUPLE);
         poPackage.setNumInps(count);
-        poPackage.setInner(innerFlags);
+        poPackage.getPkgr().setInner(innerFlags);
         return poPackage;
     }
 
diff --git a/src/org/apache/pig/pen/IllustratorAttacher.java b/src/org/apache/pig/pen/IllustratorAttacher.java
index db9c707d0..0ffbc3dcf 100644
--- a/src/org/apache/pig/pen/IllustratorAttacher.java
+++ b/src/org/apache/pig/pen/IllustratorAttacher.java
@@ -19,69 +19,59 @@
 package org.apache.pig.pen;
 
 import java.util.ArrayList;
+import java.util.Collection;
 import java.util.HashMap;
 import java.util.LinkedList;
-import java.util.Map;
-import java.util.Collection;
 import java.util.List;
+import java.util.Map;
 
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCounter;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PORank;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCollectedGroup;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackageLite;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCombinerPackage;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMultiQueryPackage;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POUnion;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODemux;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODistinct;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSort;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.GreaterThanExpr;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.LessThanExpr;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.EqualToExpr;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.GTOrEqualToExpr;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.GreaterThanExpr;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.LTOrEqualToExpr;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.EqualToExpr;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.LessThanExpr;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.NotEqualToExpr;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.PORegexp;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POIsNull;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POAnd;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POOr;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.PONot;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POBinCond;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POIsNull;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POMapLookUp;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.PONegative;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.PONot;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POOr;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.PORegexp;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserComparisonFunc;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POMapLookUp;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POJoinPackage;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.LitePackager;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCounter;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODemux;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODistinct;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLimit;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFRJoin;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeCogroup;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStream;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSkewedJoin;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPartitionRearrange;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POOptimizedForEach;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPreCombinerLocalRearrange;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PORank;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSort;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStream;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POUnion;
 import org.apache.pig.data.DataBag;
+import org.apache.pig.data.Tuple;
 import org.apache.pig.impl.PigContext;
+import org.apache.pig.impl.plan.DepthFirstWalker;
 import org.apache.pig.impl.plan.PlanWalker;
 import org.apache.pig.impl.plan.VisitorException;
 import org.apache.pig.impl.util.IdentityHashSet;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
-import org.apache.pig.pen.util.LineageTracer;
-import org.apache.pig.impl.plan.DepthFirstWalker;
 import org.apache.pig.newplan.logical.relational.LogicalSchema;
+import org.apache.pig.pen.util.LineageTracer;
 
 /**
  * The class used to (re)attach illustrators to physical operators
@@ -206,20 +196,7 @@ public class IllustratorAttacher extends PhyPlanVisitor {
 
     @Override
     public void visitPackage(POPackage pkg) throws VisitorException{
-        if (!(pkg instanceof POPackageLite) && pkg.isDistinct())
-            setIllustrator(pkg, 1);
-        else
-            setIllustrator(pkg, null);
-    }
-
-    @Override
-    public void visitCombinerPackage(POCombinerPackage pkg) throws VisitorException{
-        setIllustrator(pkg);
-    }
-
-    @Override
-    public void visitMultiQueryPackage(POMultiQueryPackage pkg) throws VisitorException{
-      setIllustrator(pkg);
+        setIllustrator(pkg, pkg.numberOfEquivalenceClasses());
     }
 
     @Override
@@ -230,10 +207,9 @@ public class IllustratorAttacher extends PhyPlanVisitor {
         for (PhysicalPlan innerPlan : innerPlans)
           innerPlanAttach(nfe, innerPlan);
         List<PhysicalOperator> preds = mPlan.getPredecessors(nfe);
-        if (preds != null && preds.size() == 1 &&
-            preds.get(0) instanceof POPackage &&
-            !(preds.get(0) instanceof POPackageLite) &&
-            ((POPackage) preds.get(0)).isDistinct()) {
+        if (preds != null && preds.size() == 1
+                && preds.get(0) instanceof POPackage
+                && ((POPackage) preds.get(0)).getPkgr().isDistinct()) {
             // equivalence class of POPackage for DISTINCT needs to be used
             //instead of the succeeding POForEach's equivalence class
             setIllustrator(nfe, preds.get(0).getIllustrator().getEquivalenceClasses());
@@ -390,14 +366,6 @@ public class IllustratorAttacher extends PhyPlanVisitor {
       setIllustrator(mapLookUp, 1);
     }
 
-    @Override
-    public void visitJoinPackage(POJoinPackage joinPackage) throws VisitorException{
-        if (revisit &&  joinPackage.getIllustrator() != null)
-            return;
-        setIllustrator(joinPackage);
-        joinPackage.getForEach().setIllustrator(joinPackage.getIllustrator());
-    }
-
     @Override
     public void visitCast(POCast cast) {
     }
diff --git a/src/org/apache/pig/pen/PhysicalPlanResetter.java b/src/org/apache/pig/pen/PhysicalPlanResetter.java
index f50fba74f..6bde089ae 100644
--- a/src/org/apache/pig/pen/PhysicalPlanResetter.java
+++ b/src/org/apache/pig/pen/PhysicalPlanResetter.java
@@ -17,25 +17,12 @@
  */
 package org.apache.pig.pen;
 
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.pig.PigException;
-import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOper;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POJoinPackage;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackageLite;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCombinerPackage;
 import org.apache.pig.impl.plan.DepthFirstWalker;
 import org.apache.pig.impl.plan.VisitorException;
-import org.apache.pig.impl.plan.optimizer.OptimizerException;
-import org.apache.pig.impl.util.Pair;
 
 /**
  * This visitor visits the physical plan and resets it for next MRCompilation
@@ -48,6 +35,6 @@ public class PhysicalPlanResetter extends PhyPlanVisitor {
     
     @Override
     public void visitPackage(POPackage pkg) throws VisitorException {
-        pkg.setKeyInfo(null);
+        pkg.getPkgr().setKeyInfo(null);
     }
 }
diff --git a/test/org/apache/pig/test/TestExampleGenerator.java b/test/org/apache/pig/test/TestExampleGenerator.java
index 42f757f7a..356a3a543 100644
--- a/test/org/apache/pig/test/TestExampleGenerator.java
+++ b/test/org/apache/pig/test/TestExampleGenerator.java
@@ -45,7 +45,7 @@ public class TestExampleGenerator {
     static int MAX = 100;
     static String A, B;
     static  File fileA, fileB;
-    
+
     @BeforeClass
     public static void oneTimeSetup() throws Exception {
         pigContext.connect();
@@ -55,7 +55,7 @@ public class TestExampleGenerator {
 
         writeData(fileA);
         writeData(fileB);
-     
+
 
         fileA.deleteOnExit();
         fileB.deleteOnExit();
@@ -122,7 +122,7 @@ public class TestExampleGenerator {
 
         assertNotNull(derivedData);
     }
-    
+
     @Test
     public void testFilter3() throws Exception {
 
@@ -144,7 +144,7 @@ public class TestExampleGenerator {
         assertNotNull(derivedData);
 
     }
-    
+
     @Test
     public void testForeach() throws ExecException, IOException {
         PigServer pigServer = new PigServer(pigContext);
@@ -157,7 +157,7 @@ public class TestExampleGenerator {
 
         assertNotNull(derivedData);
     }
-    
+
     //see PIG-2170
     @Test
     public void testForeachBinCondWithBooleanExp() throws ExecException, IOException {
@@ -171,7 +171,7 @@ public class TestExampleGenerator {
 
         assertNotNull(derivedData);
     }
-    
+
     @Test
     public void testForeachWithTypeCastCounter() throws ExecException, IOException {
         PigServer pigServer = new PigServer(pigContext);
@@ -245,7 +245,7 @@ public class TestExampleGenerator {
         assertNotNull(derivedData);
 
     }
-    
+
     @Test
     public void testGroup2() throws Exception {
         PigServer pigServer = new PigServer(pigContext);
@@ -270,7 +270,7 @@ public class TestExampleGenerator {
         assertNotNull(derivedData);
 
     }
-    
+
     @Test
     public void testFilterUnion() throws Exception {
         PigServer pigServer = new PigServer(pigContext);
@@ -283,7 +283,7 @@ public class TestExampleGenerator {
         assertNotNull(derivedData);
 
     }
-    
+
     @Test
     public void testForEachNestedBlock() throws Exception {
         PigServer pigServer = new PigServer(pigContext);
@@ -307,7 +307,7 @@ public class TestExampleGenerator {
         assertNotNull(derivedData);
 
     }
-    
+
     @Test
     public void testUnion() throws Exception {
         PigServer pigServer = new PigServer(pigContext);
@@ -328,7 +328,7 @@ public class TestExampleGenerator {
 
         assertNotNull(derivedData);
     }
-    
+
     @Test
     public void testCross() throws Exception {
         PigServer pigServer = new PigServer(pigContext);
@@ -339,7 +339,7 @@ public class TestExampleGenerator {
 
         assertNotNull(derivedData);
     }
-    
+
     @Test
     public void testLimit() throws Exception {
         PigServer pigServer = new PigServer(pigContext);
@@ -349,7 +349,7 @@ public class TestExampleGenerator {
 
         assertNotNull(derivedData);
     }
-    
+
     //see PIG-2275
     @Test
     public void testFilterWithIsNull() throws ExecException, IOException {
@@ -363,7 +363,7 @@ public class TestExampleGenerator {
 
         assertNotNull(derivedData);
     }
-    
+
     @Test
     public void testFilterWithUDF() throws ExecException, IOException {
         PigServer pigServer = new PigServer(pigContext);
@@ -383,7 +383,7 @@ public class TestExampleGenerator {
         File out = File.createTempFile("testFilterGroupCountStoreOutput", "");
         out.deleteOnExit();
         out.delete();
-    
+
         PigServer pigServer = new PigServer(pigContext);
         pigServer.setBatchOn();
         pigServer.registerQuery("A = load " + A.toString() + " as (x, y);");
@@ -392,16 +392,26 @@ public class TestExampleGenerator {
         pigServer.registerQuery("D = foreach C generate group as x, COUNT(B) as the_count;");
         pigServer.registerQuery("store D into '" +  Util.encodeEscape(out.getAbsolutePath()) + "';");
         Map<Operator, DataBag> derivedData = pigServer.getExamples(null);
-    
+
         assertNotNull(derivedData);
     }
-    
+
     @Test
     public void testLoaderWithContext() throws Exception {
         PigServer pigServer = new PigServer(pigContext);
         pigServer.registerQuery("A = load " + A.toString() + " using " + UDFContextTestLoaderWithSignature.class.getName() + "('a') as (x, y);");
         Map<Operator, DataBag> derivedData = pigServer.getExamples("A");
-        
+
+        assertNotNull(derivedData);
+    }
+
+    @Test
+    public void testOrderBy() throws Exception {
+        PigServer pigServer = new PigServer(pigContext);
+        pigServer.registerQuery("A = load " + A.toString() + " as (x, y);");
+        pigServer.registerQuery("B = order A by x;");
+        Map<Operator, DataBag> derivedData = pigServer.getExamples("B");
+
         assertNotNull(derivedData);
     }
 
diff --git a/test/org/apache/pig/test/TestJobSubmission.java b/test/org/apache/pig/test/TestJobSubmission.java
index 9741dc9f2..1254aeff0 100644
--- a/test/org/apache/pig/test/TestJobSubmission.java
+++ b/test/org/apache/pig/test/TestJobSubmission.java
@@ -100,7 +100,7 @@ public class TestJobSubmission {
         if (Util.WINDOWS) {
             inpDir="/"+FileLocalizer.parseCygPath(inpDir, FileLocalizer.STYLE_WINDOWS);
             golDir="/"+FileLocalizer.parseCygPath(golDir, FileLocalizer.STYLE_WINDOWS);
-	}
+        }
     }
 
     @After
@@ -112,329 +112,6 @@ public class TestJobSubmission {
         cluster.shutDown();
     }
 
-/*    private void generateInput(int numTuples) throws ExecException{
-
-        DataBag inpDb = GenRandomData.genRandSmallTupDataBag(r, numTuples, 1000);
-
-        POProject proj = new POProject(new OperatorKey("", r.nextLong()));
-        Tuple t = new DefaultTuple();
-        t.append(inpDb);
-        proj.attachInput(t);
-        proj.setColumn(0);
-        proj.setOverloaded(true);
-        proj.setResultType(DataType.TUPLE);
-
-        List<PhysicalOperator> inps = new ArrayList<PhysicalOperator>();
-        inps.add(proj);
-
-        POStore str = new POStore(new OperatorKey("", r.nextLong()));
-        str.setInputs(inps);
-
-        FileSpec fSpec = new FileSpec(ldFile, new FuncSpec(PigStorage.class.getName()));
-
-        str.setSFile(fSpec);
-        str.setPc(pc);
-        str.store();
-    }
-
-    private void setUp1(boolean gen) throws Exception {
-        ldFile = "file:" + inpDir + "jsTst1.txt";
-        expFile = ldFile;
-        stFile = "jsTst1";
-        grpName = "jobSubTst1";
-
-        if(gen){
-            generateInput(100);
-            return;
-        }
-
-        hadoopLdFile = FileLocalizer.hadoopify(ldFile, pc);
-
-        FileSpec LFSpec = new FileSpec(hadoopLdFile,new FuncSpec(PigStorage.class.getName()));
-        FileSpec SFSpec = new FileSpec(stFile, new FuncSpec(PigStorage.class.getName()));
-
-        POLoad ld = new POLoad(new OperatorKey("", r.nextLong()), true);
-        POStore st = new POStore(new OperatorKey("", r.nextLong()));
-        ld.setPc(pc);
-        ld.setLFile(LFSpec);
-        st.setPc(pc);
-        st.setSFile(SFSpec);
-
-        php.add(ld);
-        php.add(st);
-        php.connect(ld, st);
-     }
-
-//    @Test
-    public void testCompile1() throws Exception {
-        boolean gen = false;
-
-        setUp1(gen);
-
-        if(gen)
-            return;
-
-        submit();
-
-        assertEquals(true, FileLocalizer.fileExists(stFile, pc));
-
-        FileSpec fSpecExp = new FileSpec(expFile, new FuncSpec(PigStorage.class.getName()));
-        FileSpec fSpecAct = new FileSpec(stFile, new FuncSpec(PigStorage.class.getName()));
-
-        assertEquals(true, TestHelper.areFilesSame(fSpecExp, fSpecAct, pc));
-    }
-
-    private void setUp2(boolean gen) throws Exception {
-        ldFile = "file:" + inpDir + "jsTst2.txt";
-        expFile = ldFile;
-        stFile = "jsTst2";
-        grpName = "jobSubTst2";
-
-        if(gen){
-            generateInput(1000);
-            return;
-        }
-
-        hadoopLdFile = FileLocalizer.hadoopify(ldFile, pc);
-
-        FileSpec LFSpec = new FileSpec(hadoopLdFile, new FuncSpec(PigStorage.class.getName()));
-        FileSpec SFSpec = new FileSpec(stFile,new FuncSpec(PigStorage.class.getName()));
-
-        POLoad ld = new POLoad(new OperatorKey("", r.nextLong()), true);
-        POStore st = new POStore(new OperatorKey("", r.nextLong()));
-        ld.setPc(pc);
-        ld.setLFile(LFSpec);
-        st.setPc(pc);
-        st.setSFile(SFSpec);
-
-        php.add(ld);
-        php.add(st);
-        php.connect(ld, st);
-     }
-
-//    @Test
-    public void testCompile2() throws Exception {
-        boolean gen = false;
-
-        setUp2(gen);
-
-        if(gen)
-            return;
-
-        submit();
-
-        assertEquals(true, FileLocalizer.fileExists(stFile, pc));
-
-        FileSpec fSpecExp = new FileSpec(expFile,new FuncSpec(PigStorage.class.getName()));
-        FileSpec fSpecAct = new FileSpec(stFile,new FuncSpec(PigStorage.class.getName()));
-
-        assertEquals(true, TestHelper.areFilesSame(fSpecExp, fSpecAct, pc));
-    }
-
-    private void setUp3(boolean gen) throws Exception {
-        ldFile = "file:" + inpDir + "jsTst1.txt";
-        expFile = "file:" + golDir + "jsTst3";
-        stFile = "jsTst3";
-        grpName = "jobSubTst3";
-
-        if(gen){
-            generateInput(1000);
-            return;
-        }
-
-        hadoopLdFile = FileLocalizer.hadoopify(ldFile, pc);
-
-        FileSpec LFSpec = new FileSpec(hadoopLdFile, new FuncSpec(PigStorage.class.getName()));
-        FileSpec SFSpec = new FileSpec(stFile, new FuncSpec(PigStorage.class.getName()));
-
-        POLoad ld = new POLoad(new OperatorKey("", r.nextLong()), true);
-        POStore st = new POStore(new OperatorKey("", r.nextLong()));
-        ld.setPc(pc);
-        ld.setLFile(LFSpec);
-        st.setPc(pc);
-        st.setSFile(SFSpec);
-
-        int[] flds = {0,1};
-        Tuple sample = new DefaultTuple();
-        sample.append(new String("S"));
-        sample.append(new Integer("10"));
-
-        POForEach fe = GenPhyOp.topForEachOPWithPlan(flds , sample);
-
-        POFilter fl = GenPhyOp.topFilterOpWithProj(1, 500, GenPhyOp.LT);
-
-        php.add(ld);
-        php.add(fe);
-        php.connect(ld, fe);
-
-        php.add(fl);
-        php.connect(fe, fl);
-
-        php.add(st);
-        php.connect(fl, st);
-     }
-
-//    @Test
-    public void testCompile3() throws Exception {
-        boolean gen = false;
-
-        setUp3(gen);
-
-        if(gen)
-            return;
-
-        submit();
-
-        assertEquals(true, FileLocalizer.fileExists(stFile, pc));
-
-        FileSpec fSpecExp = new FileSpec(expFile, new FuncSpec(PigStorage.class.getName(), new String[]{","}));
-        FileSpec fSpecAct = new FileSpec(stFile,new FuncSpec(PigStorage.class.getName()));
-
-        assertEquals(true, TestHelper.areFilesSame(fSpecExp, fSpecAct, pc));
-    }
-
-    private void setUp4(boolean gen) throws Exception {
-        ldFile = "file:" + inpDir + "jsTst1.txt";
-        expFile = "file:" + golDir + "jsTst4";
-        stFile = "jsTst4";
-        grpName = "jobSubTst4";
-
-        if(gen){
-            generateInput(1000);
-            return;
-        }
-
-        hadoopLdFile = FileLocalizer.hadoopify(ldFile, pc);
-
-        FileSpec LFSpec = new FileSpec(hadoopLdFile,new FuncSpec(PigStorage.class.getName()));
-        FileSpec SFSpec = new FileSpec(stFile,new FuncSpec(PigStorage.class.getName()));
-
-        POLoad ld = new POLoad(new OperatorKey("", r.nextLong()), true);
-        POStore st = new POStore(new OperatorKey("", r.nextLong()));
-        ld.setPc(pc);
-        ld.setLFile(LFSpec);
-        st.setPc(pc);
-        st.setSFile(SFSpec);
-
-        POSplit spl = GenPhyOp.topSplitOp();
-        POFilter fl1 = GenPhyOp.topFilterOpWithProjWithCast(1, 200, GenPhyOp.LT);
-        POFilter fl2 = GenPhyOp.topFilterOpWithProjWithCast(1, 800, GenPhyOp.GT);
-
-        POUnion un = GenPhyOp.topUnionOp();
-
-        php.add(ld);
-        php.add(spl);
-        php.connect(ld, spl);
-
-        php.add(fl1);
-        php.connect(spl, fl1);
-
-        php.add(fl2);
-        php.connect(spl, fl2);
-
-        php.add(un);
-        php.connect(fl1, un);
-        php.connect(fl2, un);
-
-        php.add(st);
-        php.connect(un, st);
-     }
-
-//    @Test
-    public void testCompile4() throws Exception {
-        boolean gen = false;
-
-        setUp4(gen);
-
-        if(gen)
-            return;
-
-        submit();
-
-        assertEquals(true, FileLocalizer.fileExists(stFile, pc));
-
-        FileSpec fSpecExp = new FileSpec(expFile, new FuncSpec(PigStorage.class.getName(), new String[]{","}));
-        FileSpec fSpecAct = new FileSpec(stFile,new FuncSpec(PigStorage.class.getName()));
-
-        assertEquals(true, TestHelper.areFilesSame(fSpecExp, fSpecAct, pc));
-
-    }
-
-    private void setUp5(boolean gen) throws Exception {
-        ldFile = "file:" + inpDir + "jsTst5.txt";
-        expFile = ldFile;
-        stFile = "jsTst5";
-        grpName = "jobSubTst5";
-
-        if(gen){
-            generateInput(1000);
-            return;
-        }
-
-        hadoopLdFile = FileLocalizer.hadoopify(ldFile, pc);
-
-        FileSpec LFSpec = new FileSpec(hadoopLdFile, new FuncSpec(PigStorage.class.getName(), new String[]{","}));
-        FileSpec SFSpec = new FileSpec(stFile,new FuncSpec(PigStorage.class.getName()));
-
-        POLoad ld = new POLoad(new OperatorKey("", r.nextLong()), true);
-        POStore st = new POStore(new OperatorKey("", r.nextLong()));
-        ld.setPc(pc);
-        ld.setLFile(LFSpec);
-        st.setPc(pc);
-        st.setSFile(SFSpec);
-
-        Tuple sample = new DefaultTuple();
-        sample.append("S");
-        sample.append(1);
-        POLocalRearrange lr = GenPhyOp.topLocalRearrangeOPWithPlan(0, 1, sample);
-
-        POGlobalRearrange gr = GenPhyOp.topGlobalRearrangeOp();
-
-        POPackage pk = GenPhyOp.topPackageOp();
-        pk.setKeyType(DataType.INTEGER);
-        pk.setNumInps(1);
-        boolean[] inner = {false};
-        pk.setInner(inner);
-
-        POForEach fe = GenPhyOp.topForEachOPWithPlan(1);
-
-        php.add(ld);
-        php.add(lr);
-        php.connect(ld, lr);
-
-        php.add(gr);
-        php.connect(lr, gr);
-
-        php.add(pk);
-        php.connect(gr, pk);
-
-        php.add(fe);
-        php.connect(pk, fe);
-
-        php.add(st);
-        php.connect(fe, st);
-     }
-
-    @Test
-    public void testCompile5() throws Exception {
-        boolean gen = false;
-
-        setUp5(gen);
-
-        if(gen)
-            return;
-
-        submit();
-
-        assertEquals(true, FileLocalizer.fileExists(stFile, pc));
-
-        FileSpec fSpecExp = new FileSpec(expFile, new FuncSpec(PigStorage.class.getName(), new String[]{","}));
-        FileSpec fSpecAct = new FileSpec(stFile,new FuncSpec(PigStorage.class.getName()));
-
-        assertEquals(true, TestHelper.areFilesSame(fSpecExp, fSpecAct, pc));
-
-    }*/
-
     @Test
     public void testJobControlCompilerErr() throws Exception {
         String query = "a = load 'input';" + "b = order a by $0;" + "store b into 'output';";
@@ -443,17 +120,17 @@ public class TestJobSubmission {
         POStore store = GenPhyOp.dummyPigStorageOp();
         pp.addAsLeaf(store);
         MROperPlan mrPlan = Util.buildMRPlan(pp, pc);
-        
+
         for(MapReduceOper mro: mrPlan.getLeaves()) {
             if(mro.reducePlan != null) {
                 PhysicalOperator po = mro.reducePlan.getRoots().get(0);
-                if(po instanceof POPackage) {
-                    ((POPackage)po).setKeyType(DataType.BAG);
+                if (po instanceof POPackage) {
+                    ((POPackage) po).getPkgr().setKeyType(DataType.BAG);
                     mro.setGlobalSort(true);
                 }
             }
         }
-        
+
         ConfigurationValidator.validatePigProperties(pc.getProperties());
         Configuration conf = ConfigurationUtil.toConfiguration(pc.getProperties());
         JobControlCompiler jcc = new JobControlCompiler(pc, conf);
@@ -517,9 +194,9 @@ public class TestJobSubmission {
         // default_parallel is considered only at runtime, so here we only test requested parallel
         // more thorough tests can be found in TestNumberOfReducers.java
         String query = "a = load 'input';" +
-                       "b = load 'input';" +
-                       "c = join a by $0, b by $0 using 'skewed' parallel 100;" +
-                       "store c into 'output';";
+                "b = load 'input';" +
+                "c = join a by $0, b by $0 using 'skewed' parallel 100;" +
+                "store c into 'output';";
         PigServer ps = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
         PhysicalPlan pp = Util.buildPp(ps, query);
         MROperPlan mrPlan = Util.buildMRPlan(pp, pc);
@@ -551,8 +228,8 @@ public class TestJobSubmission {
         util.startMiniHBaseCluster(1, 1);
 
         String query = "a = load '/passwd';" +
-                       "b = group a by $0;" +
-                       "store b into 'output';";
+                "b = group a by $0;" +
+                "store b into 'output';";
         PigServer ps = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
         PhysicalPlan pp = Util.buildPp(ps, query);
         MROperPlan mrPlan = Util.buildMRPlan(pp, pc);
@@ -620,13 +297,13 @@ public class TestJobSubmission {
 
     @Test
     public void testReducerNumEstimationForOrderBy() throws Exception{
-       // use the estimation
+        // use the estimation
         pc.getProperties().setProperty("pig.exec.reducers.bytes.per.reducer", "100");
         pc.getProperties().setProperty("pig.exec.reducers.max", "10");
 
         String query = "a = load '/passwd';" +
-                       "b = order a by $0;" +
-                       "store b into 'output';";
+                "b = order a by $0;" +
+                "store b into 'output';";
         PigServer ps = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
         PhysicalPlan pp = Util.buildPp(ps, query);
 
@@ -714,17 +391,17 @@ public class TestJobSubmission {
         //Third job is the order, which uses the estimated number of reducers
         Util.assertParallelValues(-1, -1, reducer, reducer, jobControl.getWaitingJobs().get(0).getJobConf());
     }
-    
+
     @Test
     public void testToUri() throws Exception {
         Class<JobControlCompiler> jobControlCompilerClass = JobControlCompiler.class;
         Method toURIMethod = jobControlCompilerClass.getDeclaredMethod("toURI", Path.class);
         toURIMethod.setAccessible(true);
-        
+
         Path p1 = new Path("/tmp/temp-1510081022/tmp-1308657145#pigsample_1889145873_1351808882314");
         URI uri1 = (URI)toURIMethod.invoke(null, p1);
         Assert.assertEquals(uri1.toString(), "/tmp/temp-1510081022/tmp-1308657145#pigsample_1889145873_1351808882314");
-        
+
         Path p2 = new Path("C:/Program Files/GnuWin32/bin/head.exe#pigsample_1889145873_1351808882314");
         URI uri2 = (URI)toURIMethod.invoke(null, p2);
         Assert.assertTrue(uri2.toString().equals("C:/Program%20Files/GnuWin32/bin/head.exe#pigsample_1889145873_1351808882314")||
diff --git a/test/org/apache/pig/test/TestPackage.java b/test/org/apache/pig/test/TestPackage.java
index 6b197d136..ffab10836 100644
--- a/test/org/apache/pig/test/TestPackage.java
+++ b/test/org/apache/pig/test/TestPackage.java
@@ -80,7 +80,7 @@ public class TestPackage {
         // ITIterator iti = new TestPackage.ITIterator(db.iterator());
         POPackage pop = new POPackage(new OperatorKey("", r.nextLong()));
         pop.setNumInps(2);
-        pop.setInner(inner);
+        pop.getPkgr().setInner(inner);
         PigNullableWritable k = HDataType.getWritableComparableTypes(key, keyType);
         pop.attachInput(k, db.iterator());
         if (keyType != DataType.BAG) {
@@ -113,7 +113,7 @@ public class TestPackage {
                 new Pair<Boolean, Map<Integer, Integer>>(false, new HashMap<Integer, Integer>());
         keyInfo.put(0, p);
         keyInfo.put(1, p);
-        pop.setKeyInfo(keyInfo);
+        pop.getPkgr().setKeyInfo(keyInfo);
         Tuple t = null;
         Result res = null;
         res = pop.getNextTuple();
diff --git a/test/org/apache/pig/test/data/GoldenFiles/Cogroup.gld b/test/org/apache/pig/test/data/GoldenFiles/Cogroup.gld
index 35ed75a82..2b84cb932 100644
--- a/test/org/apache/pig/test/data/GoldenFiles/Cogroup.gld
+++ b/test/org/apache/pig/test/data/GoldenFiles/Cogroup.gld
@@ -1,6 +1,6 @@
 test-10: Store()
 |
-|---A: Package[tuple]{tuple} - scope-19
+|---A: Package(Packager)[tuple]{tuple} - scope-19
     |
     |---A: Global Rearrange[tuple] - scope-18
         |
diff --git a/test/org/apache/pig/test/data/GoldenFiles/MRC1.gld b/test/org/apache/pig/test/data/GoldenFiles/MRC1.gld
index 0a3472817..8df091510 100644
--- a/test/org/apache/pig/test/data/GoldenFiles/MRC1.gld
+++ b/test/org/apache/pig/test/data/GoldenFiles/MRC1.gld
@@ -5,7 +5,7 @@ MapReduce(-1) - -3:
 |       |   |
 |       |   Constant(true) - --3396897091865664764
 |       |
-|       |---Package[tuple]{Unknown} - --5758282087831209061
+|       |---Package(Packager)[tuple]{Unknown} - --5758282087831209061
 |   Local Rearrange[tuple]{Unknown}(false) - -3709512757404691843
 |   |
 |   |---Load(file:/tmp/temp-1456742965/tmp-1456742965:org.apache.pig.impl.io.InterStorage) - -2
@@ -13,7 +13,7 @@ MapReduce(-1) - -3:
 |---MapReduce(-1) - -0:
     |   Store(file:/tmp/temp-1456742965/tmp-1456742965:org.apache.pig.impl.io.InterStorage) - -1
     |   |
-    |   |---Package[tuple]{Unknown} - --2057425961601007773
+    |   |---Package(Packager)[tuple]{Unknown} - --2057425961601007773
     |   Local Rearrange[tuple]{Unknown}(false) - --8361563503038121624
     |   |
     |   |---Load(DummyFil:DummyLdr) - -7506868571066332964
\ No newline at end of file
diff --git a/test/org/apache/pig/test/data/GoldenFiles/MRC10.gld b/test/org/apache/pig/test/data/GoldenFiles/MRC10.gld
index b9fad0f4a..ae5e3dd70 100644
--- a/test/org/apache/pig/test/data/GoldenFiles/MRC10.gld
+++ b/test/org/apache/pig/test/data/GoldenFiles/MRC10.gld
@@ -15,7 +15,7 @@ Reduce Plan Empty
 |   |       |   |
 |   |       |   Constant(true) - --8248982303554009
 |   |       |
-|   |       |---Package[tuple]{Unknown} - -4061122832660258194
+|   |       |---Package(Packager)[tuple]{Unknown} - -4061122832660258194
 |   |   Union[tuple] - -3
 |   |   |
 |   |   |---Local Rearrange[tuple]{Unknown}(false) - -3527883492192621891
@@ -33,7 +33,7 @@ Reduce Plan Empty
 |---MapReduce(-1) - -12:
     |   Store(file:/tmp/temp-1456742965/tmp774375955:org.apache.pig.impl.io.InterStorage) - -18
     |   |
-    |   |---Package[tuple]{Unknown} - -5679595123645092366
+    |   |---Package(Packager)[tuple]{Unknown} - -5679595123645092366
     |   Union[tuple] - -13
     |   |
     |   |---Local Rearrange[tuple]{Unknown}(false) - --8216215966586363937
@@ -47,7 +47,7 @@ Reduce Plan Empty
     |---MapReduce(30) - -4:
     |   |   Store(file:/tmp/temp-1456742965/tmp2077335416:org.apache.pig.impl.io.InterStorage) - -5
     |   |   |
-    |   |   |---Package[tuple]{Unknown} - --7212359720440714287
+    |   |   |---Package(Packager)[tuple]{Unknown} - --7212359720440714287
     |   |   Local Rearrange[tuple]{Unknown}(false) - -7469509242284658386
     |   |   |
     |   |   |---Load(DummyFil:DummyLdr) - -990040854696137546
@@ -55,7 +55,7 @@ Reduce Plan Empty
     |---MapReduce(20) - -8:
         |   Store(file:/tmp/temp-1456742965/tmp-26634357:org.apache.pig.impl.io.InterStorage) - -9
         |   |
-        |   |---Package[tuple]{Unknown} - --6259721534861268730
+        |   |---Package(Packager)[tuple]{Unknown} - --6259721534861268730
         |   Local Rearrange[tuple]{Unknown}(false) - -3248199015665744565
         |   |
         |   |---Filter[tuple] - -6520791719738296531
diff --git a/test/org/apache/pig/test/data/GoldenFiles/MRC11.gld b/test/org/apache/pig/test/data/GoldenFiles/MRC11.gld
index 861608d31..6eca48a38 100644
--- a/test/org/apache/pig/test/data/GoldenFiles/MRC11.gld
+++ b/test/org/apache/pig/test/data/GoldenFiles/MRC11.gld
@@ -15,7 +15,7 @@ Reduce Plan Empty
 |   |       |   |
 |   |       |   Constant(true) - -7391599663069134339
 |   |       |
-|   |       |---Package[tuple]{Unknown} - -4024598850351165272
+|   |       |---Package(Packager)[tuple]{Unknown} - -4024598850351165272
 |   |   Union[tuple] - -3
 |   |   |
 |   |   |---Local Rearrange[tuple]{Unknown}(false) - -913150185705910016
@@ -33,7 +33,7 @@ Reduce Plan Empty
 |---MapReduce(-1) - -6:
     |   Store(file:/tmp/temp-1456742965/tmp-26634357:org.apache.pig.impl.io.InterStorage) - -14
     |   |
-    |   |---Package[tuple]{Unknown} - -3742910951635599848
+    |   |---Package(Packager)[tuple]{Unknown} - -3742910951635599848
     |   Union[tuple] - -7
     |   |
     |   |---Load(file:/tmp/temp-1456742965/tmp-1456742965:org.apache.pig.impl.io.InterStorage) - -8
@@ -49,7 +49,7 @@ Reduce Plan Empty
     |---MapReduce(-1) - -4:
         |   Store(file:/tmp/temp-1456742965/tmp-1456742965:org.apache.pig.impl.io.InterStorage) - -9
         |   |
-        |   |---Package[tuple]{Unknown} - --5733160635931065595
+        |   |---Package(Packager)[tuple]{Unknown} - --5733160635931065595
         |   Local Rearrange[tuple]{Unknown}(false) - --1115934782004129477
         |   |
         |   |---Load(DummyFil:DummyLdr) - -2833954415250116776
\ No newline at end of file
diff --git a/test/org/apache/pig/test/data/GoldenFiles/MRC12.gld b/test/org/apache/pig/test/data/GoldenFiles/MRC12.gld
index 15f1158c7..ae52a95da 100644
--- a/test/org/apache/pig/test/data/GoldenFiles/MRC12.gld
+++ b/test/org/apache/pig/test/data/GoldenFiles/MRC12.gld
@@ -1,7 +1,7 @@
 MapReduce(-1) - -6:
 |   Store(DummyFil:DummyLdr) - --1596982214453270401
 |   |
-|   |---Package[tuple]{Unknown} - -2312201225366906989
+|   |---Package(Packager)[tuple]{Unknown} - -2312201225366906989
 |   Union[tuple] - -7
 |   |
 |   |---Local Rearrange[tuple]{Unknown}(false) - --4071762447953696591
diff --git a/test/org/apache/pig/test/data/GoldenFiles/MRC13.gld b/test/org/apache/pig/test/data/GoldenFiles/MRC13.gld
index 0d4de2b82..093d8ff9f 100644
--- a/test/org/apache/pig/test/data/GoldenFiles/MRC13.gld
+++ b/test/org/apache/pig/test/data/GoldenFiles/MRC13.gld
@@ -19,7 +19,7 @@ Reduce Plan Empty
 |---MapReduce(-1) - -6:
     |   Store(file:/tmp/temp-1456742965/tmp2077335416:org.apache.pig.impl.io.InterStorage) - -8
     |   |
-    |   |---Package[tuple]{Unknown} - --1607475648664293401
+    |   |---Package(Packager)[tuple]{Unknown} - --1607475648664293401
     |   Union[tuple] - -7
     |   |
     |   |---Local Rearrange[tuple]{Unknown}(false) - --3476413267732334825
diff --git a/test/org/apache/pig/test/data/GoldenFiles/MRC14.gld b/test/org/apache/pig/test/data/GoldenFiles/MRC14.gld
index eafa50987..35dd8cf21 100644
--- a/test/org/apache/pig/test/data/GoldenFiles/MRC14.gld
+++ b/test/org/apache/pig/test/data/GoldenFiles/MRC14.gld
@@ -19,7 +19,7 @@ Reduce Plan Empty
 |---MapReduce(40) - -14:
     |   Store(file:/tmp/temp-1456742965/tmp-586682361:org.apache.pig.impl.io.InterStorage) - -16
     |   |
-    |   |---Package[tuple]{Unknown} - --7096529877081178520
+    |   |---Package(Packager)[tuple]{Unknown} - --7096529877081178520
     |   Union[tuple] - -15
     |   |
     |   |---Local Rearrange[tuple]{Unknown}(false) - --7868505214447593853
diff --git a/test/org/apache/pig/test/data/GoldenFiles/MRC15.gld b/test/org/apache/pig/test/data/GoldenFiles/MRC15.gld
index e8bd28145..47b191e32 100644
--- a/test/org/apache/pig/test/data/GoldenFiles/MRC15.gld
+++ b/test/org/apache/pig/test/data/GoldenFiles/MRC15.gld
@@ -7,7 +7,7 @@ MapReduce(1,GFCross) - -29:
 |       |   |
 |       |   |---Project[tuple][*] - -3845271178065259412
 |       |
-|       |---Package[tuple]{Unknown} - -3851605818031718348
+|       |---Package(Packager)[tuple]{Unknown} - -3851605818031718348
 |   Local Rearrange[tuple]{Unknown}(false) - --3658165997714269604
 |   |
 |   |---Load(file:/tmp/temp-1456742965/tmp-586682361:org.apache.pig.impl.io.InterStorage) - -28
@@ -21,7 +21,7 @@ MapReduce(1,GFCross) - -29:
     |       |   |
     |       |   |---Project[tuple][*] - -2975419344702132532
     |       |
-    |       |---Package[tuple]{Unknown} - --368145047166239648
+    |       |---Package(Packager)[tuple]{Unknown} - --368145047166239648
     |   Local Rearrange[tuple]{Unknown}(false) - -6541270116190953413
     |   |
     |   |---Load(file:/tmp/temp-1456742965/tmp-26634357:org.apache.pig.impl.io.InterStorage) - -25
@@ -43,7 +43,7 @@ MapReduce(1,GFCross) - -29:
         |           |   |
         |           |   Project[bag][1] - -22
         |           |
-        |           |---PackageLite[tuple]{tuple} - -21
+        |           |---Package(LitePackager)[tuple]{tuple} - -21
         |   Local Rearrange[tuple]{tuple}(false) - -20
         |   |   |
         |   |   Project[tuple][*] - -19
@@ -69,7 +69,7 @@ MapReduce(1,GFCross) - -29:
             |           |   |
             |           |   |---Project[bag][1] - -9
             |           |
-            |           |---Package[tuple]{chararray} - -8
+            |           |---Package(Packager)[tuple]{chararray} - -8
             |   Local Rearrange[tuple]{chararray}(false) - -7
             |   |   |
             |   |   Constant(all) - -6
diff --git a/test/org/apache/pig/test/data/GoldenFiles/MRC16.gld b/test/org/apache/pig/test/data/GoldenFiles/MRC16.gld
index 6151d1e57..28adeea83 100644
--- a/test/org/apache/pig/test/data/GoldenFiles/MRC16.gld
+++ b/test/org/apache/pig/test/data/GoldenFiles/MRC16.gld
@@ -5,7 +5,7 @@ MapReduce(-1) - -13:
 |       |   |
 |       |   Project[tuple][0] - -15
 |       |
-|       |---Package[tuple]{tuple} - -14
+|       |---Package(Packager)[tuple]{tuple} - -14
 |   Local Rearrange[tuple]{tuple}(true) - -10
 |   |   |
 |   |   Project[tuple][*] - -9
@@ -15,7 +15,7 @@ MapReduce(-1) - -13:
 |---MapReduce(-1) - -8:
     |   Store(file:/tmp/temp-1456742965/tmp2077335416:org.apache.pig.impl.io.InterStorage) - -11
     |   |
-    |   |---Package[tuple]{Unknown} - -3088212343542276753
+    |   |---Package(Packager)[tuple]{Unknown} - -3088212343542276753
     |   Local Rearrange[tuple]{Unknown}(false) - --881122551222328650
     |   |
     |   |---Load(file:/tmp/temp-1456742965/tmp-1456742965:org.apache.pig.impl.io.InterStorage) - -7
@@ -27,7 +27,7 @@ MapReduce(-1) - -13:
         |       |   |
         |       |   Project[tuple][0] - -4
         |       |
-        |       |---Package[tuple]{tuple} - -3
+        |       |---Package(Packager)[tuple]{tuple} - -3
         |   Local Rearrange[tuple]{tuple}(true) - -2
         |   |   |
         |   |   Project[tuple][*] - -1
diff --git a/test/org/apache/pig/test/data/GoldenFiles/MRC17.gld b/test/org/apache/pig/test/data/GoldenFiles/MRC17.gld
index cec5d5744..b945b8867 100644
--- a/test/org/apache/pig/test/data/GoldenFiles/MRC17.gld
+++ b/test/org/apache/pig/test/data/GoldenFiles/MRC17.gld
@@ -7,11 +7,11 @@ MapReduce(1) - -0:
 |           |   |
 |           |   Project[tuple][1] - -4
 |           |
-|           |---Package[tuple]{tuple} - -3
+|           |---Package(Packager)[tuple]{tuple} - -3
 |   Local Rearrange[tuple]{tuple}(false) - -2
 |   |   |
 |   |   Project[tuple][*] - -1
 |   |
 |   |---Limit - --43001471365805096
 |       |
-|       |---Load(DummyFil:DummyLdr) - -5534705358975373945
\ No newline at end of file
+|       |---Load(DummyFil:DummyLdr) - -5534705358975373945
diff --git a/test/org/apache/pig/test/data/GoldenFiles/MRC18.gld b/test/org/apache/pig/test/data/GoldenFiles/MRC18.gld
index 01cdc32b2..24c0f74d6 100644
--- a/test/org/apache/pig/test/data/GoldenFiles/MRC18.gld
+++ b/test/org/apache/pig/test/data/GoldenFiles/MRC18.gld
@@ -13,9 +13,9 @@ Reduce Plan Empty
     |       |   |
     |       |   Project[tuple][1] - scope-114
     |       |
-    |       |---Package[tuple]{tuple} - scope-113
+    |       |---Package(Packager)[tuple]{tuple} - scope-113
     |   Local Rearrange[tuple]{tuple}(false) - scope-112
     |   |   |
     |   |   Project[tuple][*] - scope-111
     |   |
-    |   |---b: Load(/tmp/input2:org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MergeJoinIndexer('org.apache.pig.builtin.PigStorage','eNqtVTtvE0EQnlzixJiQhGeDKBCv7k6iQqKABIgwHNgiaXDF5G5zPti7XXb3wpkCiQYKKKFAAomCkt+AhCiooaRC9NSUMLu248MBUZgtLO/M7jy+/b65t9+hphXsv41b6Bcm5f6yUtgLU23Kh5+PvPiIr6ZhqgkzOr3PSgkAU/dm7C9dui5U4qPEqMt8mSb+BkZ3WB77XYyFkD4rWUQRRc7yJM3pSLen0wh5iD2mfMkx1357YGvTDvprygOvA3soUGtzmfNmLgsTQk3IDKWBYyElDfpJA0oapJnkgS08uFZwk15DebZUcGKsNHvKpfRbkik0QtmMT9/pl1/DD10P6iE0slUlsktxwvRdeADTlDO7ynrawGJo0RkkofghzGUhwy1GvqWKz4JGzpmsJV2IWgiz2Q0hjNvNhrCQrYlNM55m3lnXRdVWz6r7UhLaR//UknuxYeMDAD0PpmwVZHFVuNt7Rw98GXWXfLW5L+8/HLr1aRq8VWhwgfEqRgRME3aZrmK6K3hcynPnXcz5e3X6XbJ/S1dTY4fDMuL4P2EnRCvvfAW8NCagdSQkM7CvDyadT4I1o9I8OVsOu+qawTFKc3MS4hGLqTtN7mFNRMNWW4nbLDKj2mY7sJDqgZkeI4870BBbTFmkGG0OSiUiGyhPVjBpba4XkjPdgQMUnR5kjeycOSO5m1DXBpW5IDixJxK8yHJiz8EKe7Z1Z/m78b87vLTDNmoVKGE4ScKhineGnu9ADaOoyEjRqVXysjE2R9y0ON0tSEIsbqNCzhlPdbYCjT586z3JiBjIU3R8W6CNC2Dg8PgIiNFg4JAm9c26U0PF7eFULSZsXWHElIFT43cly4firRykOHuFSqlValFEaDsfhpwThalkaEhULDd2nBhoj4cfwBj0YQzGYAx+gzFwIzGojkSqY1rZMXNxosA3HKSWV0f/KdDR27khQno1cPKPQ9deCyq6Jq1aTBYBftI6LWk5kNxyH41GKe0sWpmEa/1eKjNkBeYVM4XK1wyaQhMD+gz6fZy0NqyOqURvOFBgu7j+F62xPSXtBb+ZG5Ywte/b6zc/Hj4+49mPYG0LecEIxqXRuetFtsHUo7fPj+x+9vWJFZNLUJbj4e328F+Hp/M6+ModoDlD/S83YUI3yPIXqjl9HQ==','','b_45-1','scope','true')) - scope-102
\ No newline at end of file
+    |   |---b: Load(/tmp/input2:org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MergeJoinIndexer('org.apache.pig.builtin.PigStorage','eNqtVTtvE0EQnlzixJiQhGeDKBCv7k6iQqKABIgwHNgiaXDF5G5zPti7XXb3wpkCiQYKKKFAAomCkt+AhCiooaRC9NSUMLu248MBUZgtLO/M7jy+/b65t9+hphXsv41b6Bcm5f6yUtgLU23Kh5+PvPiIr6ZhqgkzOr3PSgkAU/dm7C9dui5U4qPEqMt8mSb+BkZ3WB77XYyFkD4rWUQRRc7yJM3pSLen0wh5iD2mfMkx1357YGvTDvprygOvA3soUGtzmfNmLgsTQk3IDKWBYyElDfpJA0oapJnkgS08uFZwk15DebZUcGKsNHvKpfRbkik0QtmMT9/pl1/DD10P6iE0slUlsktxwvRdeADTlDO7ynrawGJo0RkkofghzGUhwy1GvqWKz4JGzpmsJV2IWgiz2Q0hjNvNhrCQrYlNM55m3lnXRdVWz6r7UhLaR//UknuxYeMDAD0PpmwVZHFVuNt7Rw98GXWXfLW5L+8/HLr1aRq8VWhwgfEqRgRME3aZrmK6K3hcynPnXcz5e3X6XbJ/S1dTY4fDMuL4P2EnRCvvfAW8NCagdSQkM7CvDyadT4I1o9I8OVsOu+qawTFKc3MS4hGLqTtN7mFNRMNWW4nbLDKj2mY7sJDqgZkeI4870BBbTFmkGG0OSiUiGyhPVjBpba4XkjPdgQMUnR5kjeycOSO5m1DXBpW5IDixJxK8yHJiz8EKe7Z1Z/m78b87vLTDNmoVKGE4ScKhineGnu9ADaOoyEjRqVXysjE2R9y0ON0tSEIsbqNCzhlPdbYCjT586z3JiBjIU3R8W6CNC2Dg8PgIiNFg4JAm9c26U0PF7eFULSZsXWHElIFT43cly4firRykOHuFSqlValFEaDsfhpwThalkaEhULDd2nBhoj4cfwBj0YQzGYAx+gzFwIzGojkSqY1rZMXNxosA3HKSWV0f/KdDR27khQno1cPKPQ9deCyq6Jq1aTBYBftI6LWk5kNxyH41GKe0sWpmEa/1eKjNkBeYVM4XK1wyaQhMD+gz6fZy0NqyOqURvOFBgu7j+F62xPSXtBb+ZG5Ywte/b6zc/Hj4+49mPYG0LecEIxqXRuetFtsHUo7fPj+x+9vWJFZNLUJbj4e328F+Hp/M6+ModoDlD/S83YUI3yPIXqjl9HQ==','','b_45-1','scope','true')) - scope-102
diff --git a/test/org/apache/pig/test/data/GoldenFiles/MRC19.gld b/test/org/apache/pig/test/data/GoldenFiles/MRC19.gld
index 8688178bf..7fec28d2b 100644
--- a/test/org/apache/pig/test/data/GoldenFiles/MRC19.gld
+++ b/test/org/apache/pig/test/data/GoldenFiles/MRC19.gld
@@ -5,7 +5,7 @@ MapReduce(1) - -0:
 |       |   |
 |       |   POSortedDistinct[tuple] - -1793365165218576787
 |       |
-|       |---Package[tuple]{Unknown} - --3258087883799592471
+|       |---Package(Packager)[tuple]{Unknown} - --3258087883799592471
 |   Local Rearrange[tuple]{Unknown}(false) - --8083579488061691196
 |   |
 |   |---Load(DummyFil:DummyLdr) - -698333249886773960
\ No newline at end of file
diff --git a/test/org/apache/pig/test/data/GoldenFiles/MRC2.gld b/test/org/apache/pig/test/data/GoldenFiles/MRC2.gld
index 97f0ed05e..0de5f2fde 100644
--- a/test/org/apache/pig/test/data/GoldenFiles/MRC2.gld
+++ b/test/org/apache/pig/test/data/GoldenFiles/MRC2.gld
@@ -11,7 +11,7 @@ Reduce Plan Empty
 |---MapReduce(-1) - -0:
 |   |   Store(file:/tmp/temp-1456742965/tmp-1456742965:org.apache.pig.impl.io.InterStorage) - -4
 |   |   |
-|   |   |---Package[tuple]{Unknown} - --2829086180578157275
+|   |   |---Package(Packager)[tuple]{Unknown} - --2829086180578157275
 |   |   Local Rearrange[tuple]{Unknown}(false) - -6435233330554227045
 |   |   |
 |   |   |---Load(DummyFil:DummyLdr) - --4168060277593001906
@@ -19,7 +19,7 @@ Reduce Plan Empty
 |---MapReduce(-1) - -1:
     |   Store(file:/tmp/temp-1456742965/tmp2077335416:org.apache.pig.impl.io.InterStorage) - -6
     |   |
-    |   |---Package[tuple]{Unknown} - -7483213803049293823
+    |   |---Package(Packager)[tuple]{Unknown} - -7483213803049293823
     |   Local Rearrange[tuple]{Unknown}(false) - --5190071275724378681
     |   |
     |   |---Load(DummyFil:DummyLdr) - --6957591191795645940
\ No newline at end of file
diff --git a/test/org/apache/pig/test/data/GoldenFiles/MRC3.gld b/test/org/apache/pig/test/data/GoldenFiles/MRC3.gld
index 77a155f78..63de1c10d 100644
--- a/test/org/apache/pig/test/data/GoldenFiles/MRC3.gld
+++ b/test/org/apache/pig/test/data/GoldenFiles/MRC3.gld
@@ -17,7 +17,7 @@ Reduce Plan Empty
 |---MapReduce(-1) - -0:
 |   |   Store(file:/tmp/temp-1456742965/tmp-1456742965:org.apache.pig.impl.io.InterStorage) - -5
 |   |   |
-|   |   |---Package[tuple]{Unknown} - --3671186819751472084
+|   |   |---Package(Packager)[tuple]{Unknown} - --3671186819751472084
 |   |   Local Rearrange[tuple]{Unknown}(false) - -3737603423295312892
 |   |   |
 |   |   |---Load(DummyFil:DummyLdr) - --5123390619301085966
@@ -25,7 +25,7 @@ Reduce Plan Empty
 |---MapReduce(-1) - -2:
     |   Store(file:/tmp/temp-1456742965/tmp2077335416:org.apache.pig.impl.io.InterStorage) - -7
     |   |
-    |   |---Package[tuple]{Unknown} - --2325244147060806375
+    |   |---Package(Packager)[tuple]{Unknown} - --2325244147060806375
     |   Local Rearrange[tuple]{Unknown}(false) - --1194577301115518934
     |   |
     |   |---Load(DummyFil:DummyLdr) - --8027742474430787324
\ No newline at end of file
diff --git a/test/org/apache/pig/test/data/GoldenFiles/MRC4.gld b/test/org/apache/pig/test/data/GoldenFiles/MRC4.gld
index 7c5078edc..49bba07f8 100644
--- a/test/org/apache/pig/test/data/GoldenFiles/MRC4.gld
+++ b/test/org/apache/pig/test/data/GoldenFiles/MRC4.gld
@@ -23,7 +23,7 @@ Reduce Plan Empty
 |---MapReduce(-1) - -2:
 |   |   Store(file:/tmp/temp-1456742965/tmp-1456742965:org.apache.pig.impl.io.InterStorage) - -6
 |   |   |
-|   |   |---Package[tuple]{Unknown} - -810744320494301308
+|   |   |---Package(Packager)[tuple]{Unknown} - -810744320494301308
 |   |   Local Rearrange[tuple]{Unknown}(false) - --5111685507913827932
 |   |   |
 |   |   |---Load(DummyFil:DummyLdr) - --2426993543147308005
@@ -31,7 +31,7 @@ Reduce Plan Empty
 |---MapReduce(-1) - -3:
     |   Store(file:/tmp/temp-1456742965/tmp2077335416:org.apache.pig.impl.io.InterStorage) - -8
     |   |
-    |   |---Package[tuple]{Unknown} - -7127277012934370361
+    |   |---Package(Packager)[tuple]{Unknown} - -7127277012934370361
     |   Local Rearrange[tuple]{Unknown}(false) - --8622295867288126988
     |   |
     |   |---Load(DummyFil:DummyLdr) - -3683227376238667289
\ No newline at end of file
diff --git a/test/org/apache/pig/test/data/GoldenFiles/MRC6.gld b/test/org/apache/pig/test/data/GoldenFiles/MRC6.gld
index 45cba2d0a..872fb182b 100644
--- a/test/org/apache/pig/test/data/GoldenFiles/MRC6.gld
+++ b/test/org/apache/pig/test/data/GoldenFiles/MRC6.gld
@@ -1,7 +1,7 @@
 MapReduce(-1) - -8:
 |   Store(DummyFil:DummyLdr) - --6047015131487356012
 |   |
-|   |---Package[tuple]{Unknown} - --2771799342014688397
+|   |---Package(Packager)[tuple]{Unknown} - --2771799342014688397
 |   Union[tuple] - -9
 |   |
 |   |---Local Rearrange[tuple]{Unknown}(false) - --1524666447399813617
@@ -15,7 +15,7 @@ MapReduce(-1) - -8:
 |---MapReduce(-1) - -0:
 |   |   Store(file:/tmp/temp-1456742965/tmp-1456742965:org.apache.pig.impl.io.InterStorage) - -1
 |   |   |
-|   |   |---Package[tuple]{Unknown} - -3777104251028634198
+|   |   |---Package(Packager)[tuple]{Unknown} - -3777104251028634198
 |   |   Local Rearrange[tuple]{Unknown}(false) - --4160312061837144266
 |   |   |
 |   |   |---Load(DummyFil:DummyLdr) - -2997708366016271267
@@ -23,7 +23,7 @@ MapReduce(-1) - -8:
 |---MapReduce(-1) - -4:
     |   Store(file:/tmp/temp-1456742965/tmp2077335416:org.apache.pig.impl.io.InterStorage) - -5
     |   |
-    |   |---Package[tuple]{Unknown} - -9019572209815819418
+    |   |---Package(Packager)[tuple]{Unknown} - -9019572209815819418
     |   Local Rearrange[tuple]{Unknown}(false) - -3615014757987062850
     |   |
     |   |---Load(DummyFil:DummyLdr) - --8706893458091286727
\ No newline at end of file
diff --git a/test/org/apache/pig/test/data/GoldenFiles/MRC7.gld b/test/org/apache/pig/test/data/GoldenFiles/MRC7.gld
index 09fcaec7d..eab081fff 100644
--- a/test/org/apache/pig/test/data/GoldenFiles/MRC7.gld
+++ b/test/org/apache/pig/test/data/GoldenFiles/MRC7.gld
@@ -1,7 +1,7 @@
 MapReduce(-1) - -9:
 |   Store(DummyFil:DummyLdr) - --4867358672373443663
 |   |
-|   |---Package[tuple]{Unknown} - -31712229583931650
+|   |---Package(Packager)[tuple]{Unknown} - -31712229583931650
 |   Union[tuple] - -10
 |   |
 |   |---Filter[tuple] - --4213306726552862637
@@ -21,7 +21,7 @@ MapReduce(-1) - -9:
 |---MapReduce(-1) - -1:
 |   |   Store(file:/tmp/temp-1456742965/tmp-1456742965:org.apache.pig.impl.io.InterStorage) - -2
 |   |   |
-|   |   |---Package[tuple]{Unknown} - --8479692259657755370
+|   |   |---Package(Packager)[tuple]{Unknown} - --8479692259657755370
 |   |   Local Rearrange[tuple]{Unknown}(false) - -9193928674704944093
 |   |   |
 |   |   |---Load(DummyFil:DummyLdr) - --4238531569995320849
@@ -29,7 +29,7 @@ MapReduce(-1) - -9:
 |---MapReduce(-1) - -5:
     |   Store(file:/tmp/temp-1456742965/tmp2077335416:org.apache.pig.impl.io.InterStorage) - -6
     |   |
-    |   |---Package[tuple]{Unknown} - -8767305735755351861
+    |   |---Package(Packager)[tuple]{Unknown} - -8767305735755351861
     |   Local Rearrange[tuple]{Unknown}(false) - -5965044993061572808
     |   |
     |   |---Load(DummyFil:DummyLdr) - -8914265632748254170
\ No newline at end of file
diff --git a/test/org/apache/pig/test/data/GoldenFiles/MRC8.gld b/test/org/apache/pig/test/data/GoldenFiles/MRC8.gld
index 2d301a9c2..85235d706 100644
--- a/test/org/apache/pig/test/data/GoldenFiles/MRC8.gld
+++ b/test/org/apache/pig/test/data/GoldenFiles/MRC8.gld
@@ -1,7 +1,7 @@
 MapReduce(-1) - -10:
 |   Store(DummyFil:DummyLdr) - -7856319821130535798
 |   |
-|   |---Package[tuple]{Unknown} - -7398260302074824818
+|   |---Package(Packager)[tuple]{Unknown} - -7398260302074824818
 |   Union[tuple] - -11
 |   |
 |   |---Local Rearrange[tuple]{Unknown}(false) - --7926255547935388282
@@ -27,7 +27,7 @@ MapReduce(-1) - -10:
 |---MapReduce(-1) - -0:
 |   |   Store(file:/tmp/temp-1456742965/tmp-1456742965:org.apache.pig.impl.io.InterStorage) - -1
 |   |   |
-|   |   |---Package[tuple]{Unknown} - --5683415113785058706
+|   |   |---Package(Packager)[tuple]{Unknown} - --5683415113785058706
 |   |   Local Rearrange[tuple]{Unknown}(false) - -727770031531364881
 |   |   |
 |   |   |---Load(DummyFil:DummyLdr) - -7128285064986147947
@@ -35,7 +35,7 @@ MapReduce(-1) - -10:
 |---MapReduce(-1) - -6:
     |   Store(file:/tmp/temp-1456742965/tmp2077335416:org.apache.pig.impl.io.InterStorage) - -7
     |   |
-    |   |---Package[tuple]{Unknown} - --885269774183211482
+    |   |---Package(Packager)[tuple]{Unknown} - --885269774183211482
     |   Local Rearrange[tuple]{Unknown}(false) - --776319888013965510
     |   |
     |   |---Load(DummyFil:DummyLdr) - -7965768498188214494
\ No newline at end of file
diff --git a/test/org/apache/pig/test/data/GoldenFiles/MRC9.gld b/test/org/apache/pig/test/data/GoldenFiles/MRC9.gld
index c5d70471b..393421099 100644
--- a/test/org/apache/pig/test/data/GoldenFiles/MRC9.gld
+++ b/test/org/apache/pig/test/data/GoldenFiles/MRC9.gld
@@ -1,7 +1,7 @@
 MapReduce(-1) - -2:
 |   Store(DummyFil:DummyLdr) - -7714111612268358662
 |   |
-|   |---Package[tuple]{Unknown} - --1613182091613226659
+|   |---Package(Packager)[tuple]{Unknown} - --1613182091613226659
 |   Union[tuple] - -3
 |   |
 |   |---Filter[tuple] - -5165956429696944631
