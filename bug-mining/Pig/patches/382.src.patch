diff --git a/CHANGES.txt b/CHANGES.txt
index 38dbf5ffa..2f4807269 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -22,6 +22,10 @@ Trunk (unreleased changes)
 
 INCOMPATIBLE CHANGES
 
+PIG-1265: Change LoadMetadata and StoreMetadata to use Job instead of
+Configuraiton and add a cleanupOnFailure method to StoreFuncInterface
+(pradeepkth)
+
 PIG-1250:  Make StoreFunc an abstract class and create a mirror interface
 called StoreFuncInterface (pradeepkth)
 
diff --git a/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/JsonMetadata.java b/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/JsonMetadata.java
index 360cc9f1a..33f54544a 100644
--- a/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/JsonMetadata.java
+++ b/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/JsonMetadata.java
@@ -27,6 +27,7 @@ import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.mapreduce.Job;
 import org.apache.pig.Expression;
 import org.apache.pig.LoadMetadata;
 import org.apache.pig.StoreMetadata;
@@ -131,7 +132,7 @@ public class JsonMetadata implements LoadMetadata, StoreMetadata {
     // Implementation of LoadMetaData interface
     
     @Override
-    public String[] getPartitionKeys(String location, Configuration conf) {
+    public String[] getPartitionKeys(String location, Job job) {
         return null;
     }
 
@@ -147,7 +148,8 @@ public class JsonMetadata implements LoadMetadata, StoreMetadata {
      * TODO location and conf params are ignored in favor of initialzation data 
      */
     @Override
-    public ResourceSchema getSchema(String location, Configuration conf) throws IOException {     
+    public ResourceSchema getSchema(String location, Job job) throws IOException {     
+        Configuration conf = job.getConfiguration();
         Set<ElementDescriptor> schemaFileSet = null;
         try {
             schemaFileSet = findMetaFile(location, schemaFileName, conf);
@@ -188,7 +190,8 @@ public class JsonMetadata implements LoadMetadata, StoreMetadata {
      * @see org.apache.pig.LoadMetadata#getStatistics(String, Configuration)
      */
     @Override
-    public ResourceStatistics getStatistics(String location, Configuration conf) throws IOException {
+    public ResourceStatistics getStatistics(String location, Job job) throws IOException {
+        Configuration conf = job.getConfiguration();
         Set<ElementDescriptor> statFileSet = null;
         try {
             statFileSet = findMetaFile(location, statFileName, conf);
@@ -224,7 +227,8 @@ public class JsonMetadata implements LoadMetadata, StoreMetadata {
     // Implementation of StoreMetaData interface
     
     @Override
-    public void storeStatistics(ResourceStatistics stats, String location, Configuration conf) throws IOException {
+    public void storeStatistics(ResourceStatistics stats, String location, Job job) throws IOException {
+        Configuration conf = job.getConfiguration();
         DataStorage storage = new HDataStorage(ConfigurationUtil.toProperties(conf));
         ElementDescriptor statFilePath = storage.asElement(location, statFileName);
         if(!statFilePath.exists() && stats != null) {
@@ -241,7 +245,8 @@ public class JsonMetadata implements LoadMetadata, StoreMetadata {
     }
 
     @Override
-    public void storeSchema(ResourceSchema schema, String location, Configuration conf) throws IOException {
+    public void storeSchema(ResourceSchema schema, String location, Job job) throws IOException {
+        Configuration conf = job.getConfiguration();
         DataStorage storage = new HDataStorage(ConfigurationUtil.toProperties(conf));
         ElementDescriptor schemaFilePath = storage.asElement(location, schemaFileName);
         if(!schemaFilePath.exists() && schema != null) {
diff --git a/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/PigStorageSchema.java b/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/PigStorageSchema.java
index 81805768b..8f0e3b0fc 100644
--- a/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/PigStorageSchema.java
+++ b/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/PigStorageSchema.java
@@ -21,6 +21,7 @@ package org.apache.pig.piggybank.storage;
 import java.io.IOException;
 
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapreduce.Job;
 import org.apache.pig.Expression;
 import org.apache.pig.LoadMetadata;
 import org.apache.pig.ResourceSchema;
@@ -55,13 +56,13 @@ public class PigStorageSchema extends PigStorage implements LoadMetadata, StoreM
     
     @Override
     public ResourceSchema getSchema(String location,
-            Configuration conf) throws IOException {
-        return (new JsonMetadata()).getSchema(location, conf);
+            Job job) throws IOException {
+        return (new JsonMetadata()).getSchema(location, job);
     }
 
     @Override
     public ResourceStatistics getStatistics(String location,
-            Configuration conf) throws IOException {        
+            Job job) throws IOException {        
         return null;
     }
 
@@ -71,7 +72,7 @@ public class PigStorageSchema extends PigStorage implements LoadMetadata, StoreM
     }
     
     @Override
-    public String[] getPartitionKeys(String location, Configuration conf)
+    public String[] getPartitionKeys(String location, Job job)
             throws IOException {
         return null;
     }
@@ -81,18 +82,18 @@ public class PigStorageSchema extends PigStorage implements LoadMetadata, StoreM
 
     @Override
     public void storeSchema(ResourceSchema schema, String location,
-            Configuration conf) throws IOException {
+            Job job) throws IOException {
         JsonMetadata metadataWriter = new JsonMetadata();
         byte fieldDel = '\t';
         byte recordDel = '\n';
         metadataWriter.setFieldDel(fieldDel);
         metadataWriter.setRecordDel(recordDel);
-        metadataWriter.storeSchema(schema, location, conf);               
+        metadataWriter.storeSchema(schema, location, job);               
     }
 
     @Override
     public void storeStatistics(ResourceStatistics stats, String location,
-            Configuration conf) throws IOException {
+            Job job) throws IOException {
         
     }
 }
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/TableLoader.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/TableLoader.java
index f8b7cdeb4..6272f1714 100644
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/TableLoader.java
+++ b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/TableLoader.java
@@ -280,15 +280,14 @@ public class TableLoader extends IndexableLoadFunc implements LoadMetadata, Load
      }
 
      @Override
-     public String[] getPartitionKeys(String location, Configuration conf)
+     public String[] getPartitionKeys(String location, Job job)
      throws IOException {
          return null;
      }
 
      @Override
-     public ResourceSchema getSchema(String location, Configuration conf) throws IOException {
-         Path[] paths = getPathsFromLocation( location, conf );
-         Job job = new Job(conf);
+     public ResourceSchema getSchema(String location, Job job) throws IOException {
+         Path[] paths = getPathsFromLocation( location, job.getConfiguration());
          TableInputFormat.setInputPaths( job, paths );
 
          Schema tableSchema = null;
@@ -325,7 +324,7 @@ public class TableLoader extends IndexableLoadFunc implements LoadMetadata, Load
      }
 
      @Override
-     public ResourceStatistics getStatistics(String location, Configuration conf)
+     public ResourceStatistics getStatistics(String location, Job job)
      throws IOException {
          // Statistics is not supported.
          return null;
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/TableStorer.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/TableStorer.java
index 8b15f5790..8b9cdfa36 100644
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/TableStorer.java
+++ b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/TableStorer.java
@@ -163,10 +163,11 @@ public class TableStorer extends StoreFunc implements StoreMetadata {
     }
 
     @Override
-    public void storeSchema(ResourceSchema schema, String location, Configuration conf)
+    public void storeSchema(ResourceSchema schema, String location, Job job)
     throws IOException {
     	// no-op. We do close at cleanupJob().
-        BasicTable.Writer write = new BasicTable.Writer( new Path( location ), conf );
+        BasicTable.Writer write = new BasicTable.Writer( new Path( location ), 
+                job.getConfiguration());
         write.close();
     }
 
@@ -177,7 +178,7 @@ public class TableStorer extends StoreFunc implements StoreMetadata {
 
     @Override
     public void storeStatistics(ResourceStatistics stats, String location,
-            Configuration conf) throws IOException {
+            Job job) throws IOException {
         // no-op
     }
 
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestSimpleType.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestSimpleType.java
index bd034b05c..dc07e9da8 100644
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestSimpleType.java
+++ b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestSimpleType.java
@@ -437,15 +437,19 @@ public class TestSimpleType {
       System.out.println(RowValue);
     }
 
-    Path newPath = new Path(getCurrentMethodName());
-    ExecJob pigJob = pigServer
-        .store(
-            "records",
-            new Path(newPath, "store").toString(),
-            TableStorer.class.getCanonicalName()
-                + "('[s7, s2]; [s3, s4]')");
-    Assert.assertNotNull(pigJob.getException());
-    System.out.println(pigJob.getException());
+    try {
+        Path newPath = new Path(getCurrentMethodName());
+        ExecJob pigJob = pigServer
+            .store(
+                "records",
+                new Path(newPath, "store").toString(),
+                TableStorer.class.getCanonicalName()
+                    + "('[s7, s2]; [s3, s4]')");
+    } catch (Exception e) {
+        System.out.println(e);
+        return;
+    }
+    Assert.fail("Exception expected");
   }
 
   @Test
@@ -462,16 +466,20 @@ public class TestSimpleType {
       System.out.println(RowValue);
     }
 
-    Path newPath = new Path(getCurrentMethodName());
-    
-    ExecJob pigJob = pigServer
-          .store(
-              "records",
-              new Path(newPath, "store").toString(),
-              TableStorer.class.getCanonicalName()
-                  + "('[s1, s2]; [s1, s4]')");
-      Assert.assertNotNull(pigJob.getException());
-      System.out.println(pigJob.getException());
+    try {
+        Path newPath = new Path(getCurrentMethodName());
+        
+        ExecJob pigJob = pigServer
+              .store(
+                  "records",
+                  new Path(newPath, "store").toString(),
+                  TableStorer.class.getCanonicalName()
+                      + "('[s1, s2]; [s1, s4]')");
+    } catch(Exception e) {
+        System.out.println(e);
+        return;
+    }
+    Assert.fail("Exception expected");
   }
 
   @Test
@@ -487,17 +495,21 @@ public class TestSimpleType {
       Tuple RowValue = it.next();
       System.out.println(RowValue);
     }
+    try{
+        Path newPath = new Path(getCurrentMethodName());
+    
+        ExecJob pigJob = pigServer
+            .store(
+                "records",
+                new Path(newPath, "store").toString(),
+                TableStorer.class.getCanonicalName()
+                    + "('[s1]; [s1]')");
+    } catch(Exception e) {
+        System.out.println(e);
+        return;
+    }
+    Assert.fail("Exception expected");
 
-    Path newPath = new Path(getCurrentMethodName());
-
-    ExecJob pigJob = pigServer
-        .store(
-            "records",
-            new Path(newPath, "store").toString(),
-            TableStorer.class.getCanonicalName()
-                + "('[s1]; [s1]')");
-    Assert.assertNotNull(pigJob.getException());
-    System.out.println(pigJob.getException());
   }
 
   // @Test
@@ -541,13 +553,17 @@ public class TestSimpleType {
     // Use pig STORE to store testing data
     //
     System.out.println("path = " + path);
-    ExecJob pigJob = pigServer
-        .store(
-            "records",
-            path.toString(),
-            TableStorer.class.getCanonicalName()
-                + "('[s1, s2]; [s3, s4]')");
-    Assert.assertNotNull(pigJob.getException());
-    System.out.println("pig job exception : " + pigJob.getException());
+    try {
+        ExecJob pigJob = pigServer
+            .store(
+                "records",
+                path.toString(),
+                TableStorer.class.getCanonicalName()
+                    + "('[s1, s2]; [s3, s4]')");
+    } catch(Exception e) {
+        System.out.println(e);
+        return;
+    }
+    Assert.fail("Exception expected");
   }
 }
diff --git a/src/org/apache/pig/LoadMetadata.java b/src/org/apache/pig/LoadMetadata.java
index 9a321ff64..a4c081f44 100644
--- a/src/org/apache/pig/LoadMetadata.java
+++ b/src/org/apache/pig/LoadMetadata.java
@@ -20,6 +20,7 @@ package org.apache.pig;
 import java.io.IOException;
 
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapreduce.Job;
 
 /**
  * This interface defines how to retrieve metadata related to data to be loaded.
@@ -32,14 +33,16 @@ public interface LoadMetadata {
      * Get a schema for the data to be loaded.  
      * @param location Location as returned by 
      * {@link LoadFunc#relativeToAbsolutePath(String, org.apache.hadoop.fs.Path)}
-     * @param conf The {@link Configuration} object 
+     * @param job The {@link Job} object - this should be used only to obtain 
+     * cluster properties through {@link Job#getConfiguration()} and not to set/query
+     * any runtime job information.  
      * @return schema for the data to be loaded. This schema should represent
      * all tuples of the returned data.  If the schema is unknown or it is
      * not possible to return a schema that represents all returned data,
      * then null should be returned.
      * @throws IOException if an exception occurs while determining the schema
      */
-    ResourceSchema getSchema(String location, Configuration conf) throws 
+    ResourceSchema getSchema(String location, Job job) throws 
     IOException;
 
     /**
@@ -47,31 +50,35 @@ public interface LoadMetadata {
      * available, then null should be returned.
      * @param location Location as returned by 
      * {@link LoadFunc#relativeToAbsolutePath(String, org.apache.hadoop.fs.Path)}
-     * @param conf The {@link Configuration} object
+     * @param job The {@link Job} object - this should be used only to obtain 
+     * cluster properties through {@link Job#getConfiguration()} and not to set/query
+     * any runtime job information.  
      * @return statistics about the data to be loaded.  If no statistics are
      * available, then null should be returned.
      * @throws IOException if an exception occurs while retrieving statistics
      */
-    ResourceStatistics getStatistics(String location, Configuration conf) 
+    ResourceStatistics getStatistics(String location, Job job) 
     throws IOException;
 
     /**
      * Find what columns are partition keys for this input.
      * @param location Location as returned by 
      * {@link LoadFunc#relativeToAbsolutePath(String, org.apache.hadoop.fs.Path)}
-     * @param conf The {@link Configuration} object
+     * @param job The {@link Job} object - this should be used only to obtain 
+     * cluster properties through {@link Job#getConfiguration()} and not to set/query
+     * any runtime job information.  
      * @return array of field names of the partition keys. Implementations 
      * should return null to indicate that there are no partition keys
      * @throws IOException if an exception occurs while retrieving partition keys
      */
-    String[] getPartitionKeys(String location, Configuration conf) 
+    String[] getPartitionKeys(String location, Job job) 
     throws IOException;
 
     /**
      * Set the filter for partitioning.  It is assumed that this filter
      * will only contain references to fields given as partition keys in
      * getPartitionKeys. So if the implementation returns null in 
-     * {@link #getPartitionKeys(String, Configuration)}, then this method is not
+     * {@link #getPartitionKeys(String, Job)}, then this method is not
      * called by pig runtime. This method is also not called by the pig runtime
      * if there are no partition filter conditions. 
      * @param partitionFilter that describes filter for partitioning
diff --git a/src/org/apache/pig/PigServer.java b/src/org/apache/pig/PigServer.java
index 7f10c1fdb..f98f399c6 100644
--- a/src/org/apache/pig/PigServer.java
+++ b/src/org/apache/pig/PigServer.java
@@ -36,6 +36,7 @@ import java.util.Properties;
 import java.util.Set;
 import java.util.Stack;
 
+import org.apache.hadoop.mapreduce.Job;
 import org.apache.log4j.Level;
 import org.apache.log4j.Logger;
 import org.apache.pig.impl.plan.PlanException;
@@ -70,7 +71,9 @@ import org.apache.pig.impl.logicalLayer.parser.ParseException;
 import org.apache.pig.impl.logicalLayer.parser.QueryParser;
 import org.apache.pig.impl.logicalLayer.schema.Schema;
 import org.apache.pig.impl.logicalLayer.validators.LogicalPlanValidationExecutor;
+import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
 import org.apache.pig.impl.plan.CompilationMessageCollector;
 import org.apache.pig.impl.plan.DependencyOrderWalker;
 import org.apache.pig.impl.plan.DepthFirstWalker;
@@ -539,7 +542,7 @@ public class PigServer {
         }
 
         try {
-            LogicalPlan lp = compileLp(id);
+            LogicalPlan lp = clonePlan(id);
 
             // MRCompiler needs a store to be the leaf - hence
             // add a store to the plan to explain
@@ -557,7 +560,8 @@ public class PigServer {
                 }
             }
             
-            LogicalPlan storePlan = QueryParser.generateStorePlan(scope, lp, filename, func, leaf, leaf.getAlias(), pigContext);
+            LogicalPlan unCompiledstorePlan = QueryParser.generateStorePlan(scope, lp, filename, func, leaf, leaf.getAlias(), pigContext);
+            LogicalPlan storePlan = compileLp(unCompiledstorePlan, true);
             List<ExecJob> jobs = executeCompiledLogicalPlan(storePlan);
             if (jobs.size() < 1) {
                 throw new IOException("Couldn't retrieve job.");
@@ -813,8 +817,13 @@ public class PigServer {
         List<ExecJob> execJobs = pigContext.getExecutionEngine().execute(pp, "job_pigexec_");
         for (ExecJob execJob: execJobs) {
             if (execJob.getStatus()==ExecJob.JOB_STATUS.FAILED) {
-                FileLocalizer.triggerDeleteOnFail();
-                break;
+                POStore store = execJob.getPOStore();
+                try {
+                    store.getStoreFunc().cleanupOnFailure(store.getSFile().getFileName(),
+                            new Job(ConfigurationUtil.toConfiguration(execJob.getConfiguration())));
+                } catch (IOException e) {
+                    throw new ExecException(e);
+                }
             }
         }
         return execJobs;
@@ -841,39 +850,21 @@ public class PigServer {
             String msg = "Unable to clone plan before compiling";
             throw new FrontendException(msg, errCode, PigException.BUG, e);
         }
-        
+        return compileLp(lpClone, optimize);
+    }
+    
+    @SuppressWarnings("unchecked")
+    private  LogicalPlan compileLp(LogicalPlan lp, boolean optimize) throws
+    FrontendException {
         // Set the logical plan values correctly in all the operators
-        PlanSetter ps = new PlanSetter(lpClone);
+        PlanSetter ps = new PlanSetter(lp);
         ps.visit();
         
-        SortInfoSetter sortInfoSetter = new SortInfoSetter(lpClone);
-        sortInfoSetter.visit();
-        
         // run through validator
         CompilationMessageCollector collector = new CompilationMessageCollector() ;
-        FrontendException caught = null;
-        try {
-            LogicalPlanValidationExecutor validator = 
-                new LogicalPlanValidationExecutor(lpClone, pigContext);
-            validator.validate(lpClone, collector);
-        } catch (FrontendException fe) {
-            // Need to go through and see what the collector has in it.  But
-            // remember what we've caught so we can wrap it into what we
-            // throw.
-            caught = fe;            
-        }
-        
-        if(aggregateWarning) {
-            CompilationMessageCollector.logMessages(collector, MessageType.Warning, aggregateWarning, log);
-        } else {
-            for(Enum type: MessageType.values()) {
-                CompilationMessageCollector.logAllMessages(collector, log);
-            }
-        }
+        boolean isBeforeOptimizer = true;
+        validate(lp, collector, isBeforeOptimizer);
         
-        if (caught != null) {
-            throw caught;
-        }
 
         // optimize
         if (optimize && pigContext.getProperties().getProperty("pig.usenewlogicalplan", "false").equals("false")) {
@@ -888,11 +879,19 @@ public class PigServer {
                 throw new FrontendException(msg, errCode, PigException.BUG, ioe);
             }
 
-            LogicalOptimizer optimizer = new LogicalOptimizer(lpClone, pigContext.getExecType(), optimizerRules);
+            LogicalOptimizer optimizer = new LogicalOptimizer(lp, pigContext.getExecType(), optimizerRules);
             optimizer.optimize();
         }
 
-        return lpClone;
+        // compute whether output data is sorted or not
+        SortInfoSetter sortInfoSetter = new SortInfoSetter(lp);
+        sortInfoSetter.visit();
+        
+        // run validations to be done after optimization
+        isBeforeOptimizer = false;
+        validate(lp, collector, isBeforeOptimizer);
+        
+        return lp;
     }
 
     private PhysicalPlan compilePp(LogicalPlan lp) throws ExecException {
@@ -904,6 +903,32 @@ public class PigServer {
         return pp;
     }
 
+    private void validate(LogicalPlan lp, CompilationMessageCollector collector,
+            boolean isBeforeOptimizer) throws FrontendException {
+        FrontendException caught = null;
+        try {
+            LogicalPlanValidationExecutor validator = 
+                new LogicalPlanValidationExecutor(lp, pigContext, isBeforeOptimizer);
+            validator.validate(lp, collector);
+        } catch (FrontendException fe) {
+            // Need to go through and see what the collector has in it.  But
+            // remember what we've caught so we can wrap it into what we
+            // throw.
+            caught = fe;            
+        }
+        
+        if(aggregateWarning) {
+            CompilationMessageCollector.logMessages(collector, MessageType.Warning, aggregateWarning, log);
+        } else {
+            for(Enum type: MessageType.values()) {
+                CompilationMessageCollector.logAllMessages(collector, log);
+            }
+        }
+        
+        if (caught != null) {
+            throw caught;
+        }
+    }
     private LogicalPlan getPlanFromAlias(
             String alias,
             String operation) throws FrontendException {
@@ -1128,6 +1153,7 @@ public class PigServer {
             }
         }
 
+        @Override
         protected Graph clone() {
             // There are two choices on how we clone the logical plan
             // 1 - we really clone each operator and connect up the cloned operators
diff --git a/src/org/apache/pig/StoreFunc.java b/src/org/apache/pig/StoreFunc.java
index d94dcf6ef..b1e7cd15c 100644
--- a/src/org/apache/pig/StoreFunc.java
+++ b/src/org/apache/pig/StoreFunc.java
@@ -19,12 +19,12 @@ package org.apache.pig;
 
 import java.io.IOException;
 
+import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.mapreduce.Job;
 import org.apache.hadoop.mapreduce.OutputFormat;
 import org.apache.hadoop.mapreduce.RecordWriter;
 import org.apache.pig.data.Tuple;
-import org.apache.pig.impl.logicalLayer.FrontendException;
 import org.apache.pig.impl.util.UDFContext;
 
 
@@ -54,6 +54,7 @@ public abstract class StoreFunc implements StoreFuncInterface {
      * @throws IOException 
      * @throws IOException if the conversion is not possible
      */
+    @Override
     public String relToAbsPathForStoreLocation(String location, Path curDir) 
     throws IOException {
         return LoadFunc.getAbsolutePath(location, curDir);
@@ -71,12 +72,9 @@ public abstract class StoreFunc implements StoreFuncInterface {
     public abstract OutputFormat getOutputFormat() throws IOException;
 
     /**
-     * Communicate to the store function the location used in Pig Latin to refer 
-     * to the object(s) being stored.  That is, if the PL script is
-     * <b>store A into 'bla'</b>
-     * then 'bla' is the location.  This location should be either a file name
-     * or a URI.  If it does not have a URI scheme Pig will assume it is a 
-     * filename.  
+     * Communicate to the storer the location where the data needs to be stored.  
+     * The location string passed to the {@link StoreFunc} here is the 
+     * return value of {@link StoreFunc#relToAbsPathForStoreLocation(String, Path)} 
      * This method will be called in the frontend and backend multiple times. Implementations
      * should bear in mind that this method is called multiple times and should
      * ensure there are no inconsistent side effects due to the multiple calls.
@@ -84,7 +82,8 @@ public abstract class StoreFunc implements StoreFuncInterface {
      * {@link #setStoreLocation(String, Job)}.
      * 
 
-     * @param location Location indicated in store statement.
+     * @param location Location returned by 
+     * {@link StoreFunc#relToAbsPathForStoreLocation(String, Path)}
      * @param job The {@link Job} object
      * @throws IOException if the location is not valid.
      */
@@ -102,6 +101,7 @@ public abstract class StoreFunc implements StoreFuncInterface {
      * @throws IOException if this schema is not acceptable.  It should include
      * a detailed error message indicating what is wrong with the schema.
      */
+    @Override
     public void checkSchema(ResourceSchema s) throws IOException {
         // default implementation is a no-op
     }
@@ -131,7 +131,41 @@ public abstract class StoreFunc implements StoreFuncInterface {
      * will be called before other methods in {@link StoreFunc}.
      * @param signature a unique signature to identify this StoreFunc
      */
+    @Override
     public void setStoreFuncUDFContextSignature(String signature) {
         // default implementation is a no-op
     }
+    
+    /**
+     * This method will be called by Pig if the job which contains this store
+     * fails. Implementations can clean up output locations in this method to
+     * ensure that no incorrect/incomplete results are left in the output location.
+     * The implementation in {@link StoreFunc} deletes the output location if it
+     * is a {@link FileSystem} location.
+     * @param location Location returned by 
+     * {@link StoreFunc#relToAbsPathForStoreLocation(String, Path)}
+     * @param job The {@link Job} object - this should be used only to obtain 
+     * cluster properties through {@link Job#getConfiguration()} and not to set/query
+     * any runtime job information. 
+     */
+    @Override
+    public void cleanupOnFailure(String location, Job job) 
+    throws IOException {
+        cleanupOnFailureImpl(location, job);
+    }
+    
+    /**
+     * Implementation for {@link #cleanupOnFailure(String, Job)}
+     * @param location
+     * @param job
+     * @throws IOException
+     */
+    public static void cleanupOnFailureImpl(String location, Job job) 
+    throws IOException {
+        FileSystem fs = FileSystem.get(job.getConfiguration());
+        Path path = new Path(location);
+        if(fs.exists(path)){
+            fs.delete(path, true);
+        }    
+    }
 }
diff --git a/src/org/apache/pig/StoreFuncInterface.java b/src/org/apache/pig/StoreFuncInterface.java
index aecbc4f66..4b82171db 100644
--- a/src/org/apache/pig/StoreFuncInterface.java
+++ b/src/org/apache/pig/StoreFuncInterface.java
@@ -68,18 +68,16 @@ public interface StoreFuncInterface {
     OutputFormat getOutputFormat() throws IOException;
 
     /**
-     * Communicate to the store function the location used in Pig Latin to refer 
-     * to the object(s) being stored.  That is, if the PL script is
-     * <b>store A into 'bla'</b>
-     * then 'bla' is the location.  This location should be either a file name
-     * or a URI.  If it does not have a URI scheme Pig will assume it is a 
-     * filename.  
+     * Communicate to the storer the location where the data needs to be stored.  
+     * The location string passed to the {@link StoreFuncInterface} here is the 
+     * return value of {@link StoreFuncInterface#relToAbsPathForStoreLocation(String, Path)}
      * This method will be called in the frontend and backend multiple times. Implementations
      * should bear in mind that this method is called multiple times and should
      * ensure there are no inconsistent side effects due to the multiple calls.
      * 
 
-     * @param location Location indicated in store statement.
+     * @param location Location returned by 
+     * {@link StoreFuncInterface#relToAbsPathForStoreLocation(String, Path)}
      * @param job The {@link Job} object
      * @throws IOException if the location is not valid.
      */
@@ -124,4 +122,16 @@ public interface StoreFuncInterface {
      * @param signature a unique signature to identify this StoreFuncInterface
      */
     public void setStoreFuncUDFContextSignature(String signature);
+
+    /**
+     * This method will be called by Pig if the job which contains this store
+     * fails. Implementations can clean up output locations in this method to
+     * ensure that no incorrect/incomplete results are left in the output location
+     * @param location Location returned by 
+     * {@link StoreFuncInterface#relToAbsPathForStoreLocation(String, Path)}
+     * @param job The {@link Job} object - this should be used only to obtain 
+     * cluster properties through {@link Job#getConfiguration()} and not to set/query
+     * any runtime job information. 
+     */
+    void cleanupOnFailure(String location, Job job) throws IOException;
 }
diff --git a/src/org/apache/pig/StoreMetadata.java b/src/org/apache/pig/StoreMetadata.java
index c9370b42e..fa8b69074 100644
--- a/src/org/apache/pig/StoreMetadata.java
+++ b/src/org/apache/pig/StoreMetadata.java
@@ -21,6 +21,7 @@ package org.apache.pig;
 import java.io.IOException;
 
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapreduce.Job;
 import org.apache.pig.ResourceSchema;
 import org.apache.pig.ResourceStatistics;
 
@@ -34,15 +35,19 @@ public interface StoreMetadata {
 
     /**
      * Store statistics about the data being written.
-     * 
+     * @param job The {@link Job} object - this should be used only to obtain 
+     * cluster properties through {@link Job#getConfiguration()} and not to set/query
+     * any runtime job information.  
      * @throws IOException 
      */
-    void storeStatistics(ResourceStatistics stats, String location, Configuration conf) throws IOException;
+    void storeStatistics(ResourceStatistics stats, String location, Job job) throws IOException;
 
     /**
      * Store schema of the data being written
-     * 
+     * @param job The {@link Job} object - this should be used only to obtain 
+     * cluster properties through {@link Job#getConfiguration()} and not to set/query
+     * any runtime job information.  
      * @throws IOException 
      */
-    void storeSchema(ResourceSchema schema, String location, Configuration conf) throws IOException;
+    void storeSchema(ResourceSchema schema, String location, Job job) throws IOException;
 }
diff --git a/src/org/apache/pig/backend/executionengine/ExecJob.java b/src/org/apache/pig/backend/executionengine/ExecJob.java
index 2a398d74a..a3c342046 100644
--- a/src/org/apache/pig/backend/executionengine/ExecJob.java
+++ b/src/org/apache/pig/backend/executionengine/ExecJob.java
@@ -22,6 +22,7 @@ import java.util.Iterator;
 import java.util.Properties;
 import java.io.OutputStream;
 
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
 import org.apache.pig.data.Tuple;
 import org.apache.pig.tools.pigstats.PigStats;
 
@@ -80,6 +81,12 @@ public interface ExecJob {
      * @return statistics relevant to the execution engine
      */
     public PigStats getStatistics();
+    
+    /**
+     * 
+     * @return {@link POStore} object associated with the store 
+     */
+    public POStore getPOStore();
 
     /**
      * hook for asynchronous notification of job completion pushed from the back-end
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/HExecutionEngine.java b/src/org/apache/pig/backend/hadoop/executionengine/HExecutionEngine.java
index eba98081d..9261f9a62 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/HExecutionEngine.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/HExecutionEngine.java
@@ -306,14 +306,16 @@ public class HExecutionEngine implements ExecutionEngine {
         try {
             PigStats stats = launcher.launchPig(plan, jobName, pigContext);
 
-            for (FileSpec spec: launcher.getSucceededFiles()) {
+            for (POStore store: launcher.getSucceededFiles()) {
+                FileSpec spec = store.getSFile();
                 String alias = leafMap.containsKey(spec.toString()) ? leafMap.get(spec.toString()).getAlias() : null;
-                jobs.add(new HJob(ExecJob.JOB_STATUS.COMPLETED, pigContext, spec, alias, stats));
+                jobs.add(new HJob(ExecJob.JOB_STATUS.COMPLETED, pigContext, store, alias, stats));
             }
 
-            for (FileSpec spec: launcher.getFailedFiles()) {
+            for (POStore store: launcher.getFailedFiles()) {
+                FileSpec spec = store.getSFile();
                 String alias = leafMap.containsKey(spec.toString()) ? leafMap.get(spec.toString()).getAlias() : null;
-                HJob j = new HJob(ExecJob.JOB_STATUS.FAILED, pigContext, spec, alias, stats);
+                HJob j = new HJob(ExecJob.JOB_STATUS.FAILED, pigContext, store, alias, stats);
                 j.setException(launcher.getError(spec));
                 jobs.add(j);
             }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/HJob.java b/src/org/apache/pig/backend/hadoop/executionengine/HJob.java
index ff64caf57..41aa14887 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/HJob.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/HJob.java
@@ -19,23 +19,20 @@
 package org.apache.pig.backend.hadoop.executionengine;
 
 import java.io.OutputStream;
-import java.io.InputStream;
 import java.util.Iterator;
 import java.util.Properties;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
+import org.apache.pig.LoadFunc;
+import org.apache.pig.PigException;
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.backend.executionengine.ExecJob;
 import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
-import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
 import org.apache.pig.data.Tuple;
 import org.apache.pig.impl.PigContext;
 import org.apache.pig.impl.io.FileSpec;
-import org.apache.pig.LoadFunc;
-import org.apache.pig.PigException;
-import org.apache.pig.impl.io.FileLocalizer;
-import org.apache.pig.impl.io.BufferedPositionedInputStream;
 import org.apache.pig.impl.io.ReadToEndLoader;
 import org.apache.pig.tools.pigstats.PigStats;
 
@@ -49,26 +46,29 @@ public class HJob implements ExecJob {
     protected FileSpec outFileSpec;
     protected Exception backendException;
     protected String alias;
+    protected POStore poStore;
     private PigStats stats;
     
     public HJob(JOB_STATUS status,
                 PigContext pigContext,
-                FileSpec outFileSpec,
+                POStore store,
                 String alias) {
         this.status = status;
         this.pigContext = pigContext;
-        this.outFileSpec = outFileSpec;
+        this.poStore = store;
+        this.outFileSpec = poStore.getSFile();
         this.alias = alias;
     }
     
     public HJob(JOB_STATUS status,
             PigContext pigContext,
-            FileSpec outFileSpec,
+            POStore store,
             String alias,
             PigStats stats) {
         this.status = status;
         this.pigContext = pigContext;
-        this.outFileSpec = outFileSpec;
+        this.poStore = store;
+        this.outFileSpec = poStore.getSFile();
         this.alias = alias;
         this.stats = stats;
     }
@@ -143,8 +143,7 @@ public class HJob implements ExecJob {
     }
 
     public Properties getConfiguration() {
-        Properties props = new Properties();
-        return props;
+        return pigContext.getProperties();
     }
 
     public PigStats getStatistics() {
@@ -184,4 +183,12 @@ public class HJob implements ExecJob {
     public String getAlias() throws ExecException {
         return alias;
     }
+
+    /**
+     * @return the poStore
+     */
+    @Override
+    public POStore getPOStore() {
+        return poStore;
+    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java
index 2f853cb50..bcd6e1e0b 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java
@@ -133,7 +133,6 @@ public class JobControlCompiler{
      */
     public static final String PIG_MAP_STORES = "pig.map.stores";
     public static final String PIG_REDUCE_STORES = "pig.reduce.stores";
-    public static final String PIG_STORE_FUNC = "pig.storeFunc";
     
     // A mapping of job to pair of store locations and tmp locations for that job
     private Map<Job, Pair<List<POStore>, Path>> jobStoreMap;
@@ -460,13 +459,6 @@ public class JobControlCompiler{
                 String outputPath = st.getSFile().getFileName();
                 FuncSpec outputFuncSpec = st.getSFile().getFuncSpec();
                 
-                // serialize the store func spec using ObjectSerializer
-                // ObjectSerializer.serialize() uses default java serialization
-                // and then further encodes the output so that control characters
-                // get encoded as regular characters. Otherwise any control characters
-                // in the store funcspec would break the job.xml which is created by
-                // hadoop from the jobconf.
-                conf.set(PIG_STORE_FUNC, ObjectSerializer.serialize(outputFuncSpec.toString()));
                 conf.set("pig.streaming.log.dir", 
                             new Path(outputPath, LOG_DIR).toString());
                 conf.set("pig.streaming.task.output.dir", outputPath);
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/Launcher.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/Launcher.java
index a8413b883..88ea4af24 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/Launcher.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/Launcher.java
@@ -20,17 +20,16 @@ package org.apache.pig.backend.hadoop.executionengine.mapReduceLayer;
 import java.io.IOException;
 import java.io.PrintStream;
 import java.util.ArrayList;
+import java.util.Arrays;
 import java.util.HashSet;
-import java.util.List;
 import java.util.LinkedList;
+import java.util.List;
 import java.util.Set;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
-import java.util.Arrays;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.mapred.JobClient;
 import org.apache.hadoop.mapred.JobID;
 import org.apache.hadoop.mapred.RunningJob;
@@ -40,17 +39,13 @@ import org.apache.hadoop.mapred.jobcontrol.JobControl;
 import org.apache.pig.FuncSpec;
 import org.apache.pig.PigException;
 import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.executionengine.ExecutionEngine;
-import org.apache.pig.backend.hadoop.datastorage.HConfiguration;
-import org.apache.pig.backend.hadoop.executionengine.HExecutionEngine;
-import org.apache.pig.impl.PigContext;
-import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.MROperPlan;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
+import org.apache.pig.impl.PigContext;
+import org.apache.pig.impl.io.FileSpec;
 import org.apache.pig.impl.plan.PlanException;
 import org.apache.pig.impl.plan.VisitorException;
 import org.apache.pig.impl.util.LogUtils;
-import org.apache.pig.impl.io.FileSpec;
 import org.apache.pig.tools.pigstats.PigStats;
 
 public abstract class Launcher {
@@ -62,8 +57,8 @@ public abstract class Launcher {
     boolean outOfMemory = false;
     static final String OOM_ERR = "OutOfMemoryError";
 
-    protected List<FileSpec> succeededStores = null;
-    protected List<FileSpec> failedStores = null;
+    protected List<POStore> succeededStores = null;
+    protected List<POStore> failedStores = null;
     
     protected Launcher(){
         totalHadoopTimeSpent = 0;
@@ -75,21 +70,19 @@ public abstract class Launcher {
     }
 
     /**
-     * Returns a list of locations of results that have been
-     * successfully completed.
-     * @return A list of filspecs that corresponds to the locations of
-     * the successful stores.
+     *
+     * @return A list of {@link POStore} objects corresponding to the store
+     * statements that were successful
      */
-    public List<FileSpec> getSucceededFiles() {
+    public List<POStore> getSucceededFiles() {
         return succeededStores;
     }
 
     /**
-     * Returns a list of locations of results that have failed.
-     * @return A list of filspecs that corresponds to the locations of
-     * the failed stores.
+     * @return A list of {@link POStore} objects corresponding to the store
+     * statements that failed
      */
-    public List<FileSpec> getFailedFiles() {
+    public List<POStore> getFailedFiles() {
         return failedStores;
     }
 
@@ -97,8 +90,8 @@ public abstract class Launcher {
      * Resets the state after a launch
      */
     public void reset() {
-        succeededStores = new LinkedList<FileSpec>();
-        failedStores = new LinkedList<FileSpec>();
+        succeededStores = new LinkedList<POStore>();
+        failedStores = new LinkedList<POStore>();
     }
 
     /**
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java
index d5f44b176..6aba3ba38 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java
@@ -276,9 +276,8 @@ public class MapReduceLauncher extends Launcher{
                         finalStores++;
                         log.error("Failed to produce result in: \""+st.getSFile().getFileName()+"\"");
                     }
-                    failedStores.add(st.getSFile());
+                    failedStores.add(st);
                     failureMap.put(st.getSFile(), backendException);
-                    FileLocalizer.registerDeleteOnFail(st.getSFile().getFileName(), pc);
                     //log.error("Failed to produce result in: \""+st.getSFile().getFileName()+"\"");
                 }
             }
@@ -304,7 +303,7 @@ public class MapReduceLauncher extends Launcher{
                         storeSchema(job, st);
                     }
                     if (!st.isTmpStore()) {
-                        succeededStores.add(st.getSFile());
+                        succeededStores.add(st);
                         finalStores++;
                         log.info("Successfully stored result in: \""+st.getSFile().getFileName()+"\"");
                     }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigOutputCommitter.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigOutputCommitter.java
index 5034a1a8d..91cde9401 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigOutputCommitter.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigOutputCommitter.java
@@ -23,6 +23,7 @@ import java.util.LinkedList;
 import java.util.List;
 
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapreduce.Job;
 import org.apache.hadoop.mapreduce.JobContext;
 import org.apache.hadoop.mapreduce.OutputCommitter;
 import org.apache.hadoop.mapreduce.TaskAttemptContext;
@@ -129,9 +130,7 @@ public class PigOutputCommitter extends OutputCommitter {
         // call setLocation() on the storeFunc so that if there are any
         // side effects like setting map.output.dir on the Configuration
         // in the Context are needed by the OutputCommitter, those actions
-        // will be done before the committer is created. Also the String 
-        // version of StoreFunc for the specific store need
-        // to be set up in the context in case the committer needs them
+        // will be done before the committer is created. 
         PigOutputFormat.setLocation(contextCopy, store);
         return contextCopy;   
     }
@@ -161,14 +160,11 @@ public class PigOutputCommitter extends OutputCommitter {
             if (schema != null) {
                 ((StoreMetadata) storeFunc).storeSchema(
                         new ResourceSchema(schema, store.getSortInfo()), store.getSFile()
-                                .getFileName(), conf);
+                                .getFileName(), new Job(conf));
             }
         }
     }
     
-    /* (non-Javadoc)
-     * @see org.apache.hadoop.mapred.FileOutputCommitter#cleanupJob(org.apache.hadoop.mapred.JobContext)
-     */
     @Override
     public void cleanupJob(JobContext context) throws IOException {
         // call clean up on all map and reduce committers
@@ -188,9 +184,6 @@ public class PigOutputCommitter extends OutputCommitter {
        
     }
 
-    /* (non-Javadoc)
-     * @see org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter#abortTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
-     */
     @Override
     public void abortTask(TaskAttemptContext context) throws IOException {        
         if(context.getTaskAttemptID().isMap()) {
@@ -210,9 +203,6 @@ public class PigOutputCommitter extends OutputCommitter {
         }
     }
     
-    /* (non-Javadoc)
-     * @see org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter#commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
-     */
     @Override
     public void commitTask(TaskAttemptContext context) throws IOException {
         if(context.getTaskAttemptID().isMap()) {
@@ -232,9 +222,6 @@ public class PigOutputCommitter extends OutputCommitter {
         }
     }
     
-    /* (non-Javadoc)
-     * @see org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter#needsTaskCommit(org.apache.hadoop.mapreduce.TaskAttemptContext)
-     */
     @Override
     public boolean needsTaskCommit(TaskAttemptContext context)
             throws IOException {
@@ -260,9 +247,6 @@ public class PigOutputCommitter extends OutputCommitter {
         }
     }
     
-    /* (non-Javadoc)
-     * @see org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter#setupJob(org.apache.hadoop.mapreduce.JobContext)
-     */
     @Override
     public void setupJob(JobContext context) throws IOException {
         // call set up on all map and reduce committers
@@ -279,9 +263,6 @@ public class PigOutputCommitter extends OutputCommitter {
         }
     }
     
-    /* (non-Javadoc)
-     * @see org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter#setupTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
-     */
     @Override
     public void setupTask(TaskAttemptContext context) throws IOException {
         if(context.getTaskAttemptID().isMap()) {
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigOutputFormat.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigOutputFormat.java
index efd46a243..e955eec26 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigOutputFormat.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigOutputFormat.java
@@ -178,12 +178,6 @@ public class PigOutputFormat extends OutputFormat<WritableComparable, Tuple> {
         // supplied as input has the updates.
         ConfigurationUtil.mergeConf(jobContext.getConfiguration(), 
                 storeJob.getConfiguration());
-        
-        // Before delegating calls to underlying OutputFormat or OutputCommitter
-        // Pig needs to ensure the Configuration in the JobContext contains
-        // StoreFunc for the specific store - so set this up in the context 
-        // for this specific store
-        updateContextWithStoreInfo(jobContext, store);
     }
 
     @Override
@@ -240,21 +234,4 @@ public class PigOutputFormat extends OutputFormat<WritableComparable, Tuple> {
         // will wrap the real OutputCommitter(s) belonging to the store(s)
         return new PigOutputCommitter(taskattemptcontext);
     }
-    
-    /**
-     * Before delegating calls to underlying OutputFormat or OutputCommitter
-     * Pig needs to ensure the Configuration in the {@link JobContext} contains
-     * {@link JobControlCompiler#PIG_STORE_FUNC}. This helper method can be
-     * used to set this up
-     * @param context the job context
-     * @param store the POStore whose information is to be put into the context
-     * @throws IOException in case of failure
-     */
-    public static void updateContextWithStoreInfo(JobContext context, 
-            POStore store) throws IOException {
-        Configuration conf = context.getConfiguration();
-        conf.set(JobControlCompiler.PIG_STORE_FUNC, 
-                store.getSFile().getFuncSpec().toString());
-        
-    }
 }
diff --git a/src/org/apache/pig/builtin/BinStorage.java b/src/org/apache/pig/builtin/BinStorage.java
index aeb1a06f4..8ed5ac975 100644
--- a/src/org/apache/pig/builtin/BinStorage.java
+++ b/src/org/apache/pig/builtin/BinStorage.java
@@ -45,6 +45,7 @@ import org.apache.pig.LoadMetadata;
 import org.apache.pig.PigException;
 import org.apache.pig.PigWarning;
 import org.apache.pig.ResourceSchema;
+import org.apache.pig.StoreFunc;
 import org.apache.pig.ResourceSchema.ResourceFieldSchema;
 import org.apache.pig.ResourceStatistics;
 import org.apache.pig.StoreFuncInterface;
@@ -380,14 +381,15 @@ implements LoadCaster, StoreFuncInterface, LoadMetadata {
     }
 
     @Override
-    public String[] getPartitionKeys(String location, Configuration conf)
+    public String[] getPartitionKeys(String location, Job job)
             throws IOException {
         return null;
     }
 
     @Override
-    public ResourceSchema getSchema(String location, Configuration conf)
+    public ResourceSchema getSchema(String location, Job job)
             throws IOException {
+        Configuration conf = job.getConfiguration();
         Properties props = ConfigurationUtil.toProperties(conf);
         // since local mode now is implemented as hadoop's local mode
         // we can treat either local or hadoop mode as hadoop mode - hence
@@ -423,7 +425,7 @@ implements LoadCaster, StoreFuncInterface, LoadMetadata {
     }
 
     @Override
-    public ResourceStatistics getStatistics(String location, Configuration conf)
+    public ResourceStatistics getStatistics(String location, Job job)
             throws IOException {
         throw new UnsupportedOperationException();
     }
@@ -437,4 +439,9 @@ implements LoadCaster, StoreFuncInterface, LoadMetadata {
     public void setStoreFuncUDFContextSignature(String signature) {
     }
 
+    @Override
+    public void cleanupOnFailure(String location, Job job) throws IOException {
+        StoreFunc.cleanupOnFailureImpl(location, job);
+    }
+
 }
diff --git a/src/org/apache/pig/builtin/PigStorage.java b/src/org/apache/pig/builtin/PigStorage.java
index 71386ab39..b67ad2d1d 100644
--- a/src/org/apache/pig/builtin/PigStorage.java
+++ b/src/org/apache/pig/builtin/PigStorage.java
@@ -26,6 +26,7 @@ import java.util.Properties;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.io.WritableComparable;
@@ -45,6 +46,7 @@ import org.apache.pig.LoadFunc;
 import org.apache.pig.LoadPushDown;
 import org.apache.pig.PigException;
 import org.apache.pig.ResourceSchema;
+import org.apache.pig.StoreFunc;
 import org.apache.pig.StoreFuncInterface;
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigSplit;
@@ -135,7 +137,7 @@ LoadPushDown {
       
     }
 
-    ByteArrayOutputStream mOut = new ByteArrayOutputStream(BUFFER_SIZE);
+    protected ByteArrayOutputStream mOut = new ByteArrayOutputStream(BUFFER_SIZE);
 
     @Override
     public void putNext(Tuple f) throws IOException {
@@ -293,4 +295,10 @@ LoadPushDown {
     public void setStoreFuncUDFContextSignature(String signature) {
     }
 
+    @Override
+    public void cleanupOnFailure(String location, Job job)
+            throws IOException {
+        StoreFunc.cleanupOnFailureImpl(location, job);
+    }
+
 }
diff --git a/src/org/apache/pig/impl/logicalLayer/LOLoad.java b/src/org/apache/pig/impl/logicalLayer/LOLoad.java
index c01d709b4..5c92ac08d 100644
--- a/src/org/apache/pig/impl/logicalLayer/LOLoad.java
+++ b/src/org/apache/pig/impl/logicalLayer/LOLoad.java
@@ -47,6 +47,7 @@ import org.apache.pig.impl.util.Pair;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapreduce.Job;
 
 public class LOLoad extends RelationalOperator {
     private static final long serialVersionUID = 2L;
@@ -164,7 +165,7 @@ public class LOLoad extends RelationalOperator {
         if(LoadMetadata.class.isAssignableFrom(mLoadFunc.getClass())) {
             LoadMetadata loadMetadata = (LoadMetadata)mLoadFunc;
             ResourceSchema rSchema = loadMetadata.getSchema(
-                    mInputFileSpec.getFileName(), conf);
+                    mInputFileSpec.getFileName(), new Job(conf));
             return Schema.getPigSchema(rSchema);
         } else {
             return null;
diff --git a/src/org/apache/pig/impl/logicalLayer/optimizer/PartitionFilterOptimizer.java b/src/org/apache/pig/impl/logicalLayer/optimizer/PartitionFilterOptimizer.java
index 6b77230d4..bcb6bb22b 100644
--- a/src/org/apache/pig/impl/logicalLayer/optimizer/PartitionFilterOptimizer.java
+++ b/src/org/apache/pig/impl/logicalLayer/optimizer/PartitionFilterOptimizer.java
@@ -24,6 +24,7 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
+import org.apache.hadoop.mapreduce.Job;
 import org.apache.pig.Expression;
 import org.apache.pig.LoadFunc;
 import org.apache.pig.LoadMetadata;
@@ -128,7 +129,7 @@ public class PartitionFilterOptimizer extends
         loadMetadata = (LoadMetadata)loadFunc;
         try {
             partitionKeys = loadMetadata.getPartitionKeys(
-                    loLoad.getInputFile().getFileName(), loLoad.getConfiguration());
+                    loLoad.getInputFile().getFileName(), new Job(loLoad.getConfiguration()));
             if(partitionKeys == null || partitionKeys.length == 0) {
                 return false;
             }
diff --git a/src/org/apache/pig/impl/logicalLayer/validators/LogicalPlanValidationExecutor.java b/src/org/apache/pig/impl/logicalLayer/validators/LogicalPlanValidationExecutor.java
index 9900b84b7..9c399518d 100644
--- a/src/org/apache/pig/impl/logicalLayer/validators/LogicalPlanValidationExecutor.java
+++ b/src/org/apache/pig/impl/logicalLayer/validators/LogicalPlanValidationExecutor.java
@@ -56,19 +56,25 @@ public class LogicalPlanValidationExecutor
      */
     
     public LogicalPlanValidationExecutor(LogicalPlan plan,
-                                         PigContext pigContext) {
+                                         PigContext pigContext, 
+                                         boolean beforeOptimizer) {
    
         // Default validations
-        if (!pigContext.inExplain) {
+        if (!pigContext.inExplain && !beforeOptimizer) {
             // When running explain we don't want to check for input
-            // files.
+            // files. - run this validator after optimizer for two reasons
+            // 1) input/output may get changed (optimized away)
+            // 2) we will call checkSchema on the StoreFunc for the store(s) 
+            // in this validator and the schema should contain correct schema 
+            // after optimization
             validatorList.add(new InputOutputFileValidator(pigContext)) ;
+        } else if (beforeOptimizer) {
+            // This one has to be done before the type checker.
+            //validatorList.add(new TypeCastInserterValidator()) ;
+            validatorList.add(new TypeCheckingValidator()) ;
+            
+            validatorList.add(new SchemaAliasValidator()) ;
         }
-        // This one has to be done before the type checker.
-        //validatorList.add(new TypeCastInserterValidator()) ;
-        validatorList.add(new TypeCheckingValidator()) ;
-        
-        validatorList.add(new SchemaAliasValidator()) ;
     }    
 
     public void validate(LogicalPlan plan,
diff --git a/src/org/apache/pig/pen/ExampleGenerator.java b/src/org/apache/pig/pen/ExampleGenerator.java
index f4420f4ae..3ec3a5367 100644
--- a/src/org/apache/pig/pen/ExampleGenerator.java
+++ b/src/org/apache/pig/pen/ExampleGenerator.java
@@ -214,13 +214,19 @@ public class ExampleGenerator {
         CompilationMessageCollector collector = new CompilationMessageCollector();
         FrontendException caught = null;
         try {
+            boolean isBeforeOptimizer = true;
             LogicalPlanValidationExecutor validator = new LogicalPlanValidationExecutor(
-                    plan, pigContext);
+                    plan, pigContext, isBeforeOptimizer);
             validator.validate(plan, collector);
 
             FunctionalLogicalOptimizer optimizer = new FunctionalLogicalOptimizer(
                     plan);
             optimizer.optimize();
+            
+            isBeforeOptimizer = false;
+            validator = new LogicalPlanValidationExecutor(
+                    plan, pigContext, isBeforeOptimizer);
+            validator.validate(plan, collector);
         } catch (FrontendException fe) {
             // Need to go through and see what the collector has in it. But
             // remember what we've caught so we can wrap it into what we
diff --git a/test/org/apache/pig/test/TestInputOutputFileValidator.java b/test/org/apache/pig/test/TestInputOutputFileValidator.java
index 86c42eb7a..6d768221b 100644
--- a/test/org/apache/pig/test/TestInputOutputFileValidator.java
+++ b/test/org/apache/pig/test/TestInputOutputFileValidator.java
@@ -18,13 +18,18 @@
 package org.apache.pig.test;
 
 import java.io.* ;
+import java.util.Iterator;
 import java.util.Properties;
 
 import org.apache.pig.ExecType; 
 import org.apache.pig.FuncSpec;
+import org.apache.pig.PigServer;
 import org.apache.pig.backend.datastorage.DataStorage;
 import org.apache.pig.backend.datastorage.ElementDescriptor;
+import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
+import org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil;
+import org.apache.pig.data.Tuple;
 import org.apache.pig.impl.PigContext;
 import org.apache.pig.impl.io.FileLocalizer;
 import org.apache.pig.impl.io.FileSpec;
@@ -37,7 +42,10 @@ import org.apache.pig.impl.plan.NodeIdGenerator;
 import org.apache.pig.impl.logicalLayer.validators.* ;
 import org.apache.pig.impl.plan.CompilationMessageCollector;
 import org.apache.pig.impl.plan.CompilationMessageCollector.MessageType; 
+import org.apache.pig.impl.util.LogUtils;
 import org.junit.Test;
+
+import junit.framework.Assert;
 import junit.framework.TestCase;
 
 public class TestInputOutputFileValidator extends TestCase {
@@ -57,7 +65,8 @@ public class TestInputOutputFileValidator extends TestCase {
         LogicalPlan plan = genNewLoadStorePlan(inputfile, outputfile, ctx.getFs()) ;        
         
         CompilationMessageCollector collector = new CompilationMessageCollector() ;        
-        LogicalPlanValidationExecutor executor = new LogicalPlanValidationExecutor(plan, ctx) ;
+        boolean isBeforeOptimizer = false; // we are not optimizing in this testcase
+        LogicalPlanValidationExecutor executor = new LogicalPlanValidationExecutor(plan, ctx, isBeforeOptimizer) ;
         executor.validate(plan, collector) ;
         
         assertFalse(collector.hasError()) ;
@@ -77,7 +86,8 @@ public class TestInputOutputFileValidator extends TestCase {
         LogicalPlan plan = genNewLoadStorePlan(inputfile, outputfile, ctx.getDfs()) ;        
         
         CompilationMessageCollector collector = new CompilationMessageCollector() ;        
-        LogicalPlanValidationExecutor executor = new LogicalPlanValidationExecutor(plan, ctx) ;
+        boolean isBeforeOptimizer = false; // we are not optimizing in this testcase
+        LogicalPlanValidationExecutor executor = new LogicalPlanValidationExecutor(plan, ctx, isBeforeOptimizer) ;
         try {
             executor.validate(plan, collector) ;
             fail("Expected to fail.");
@@ -104,7 +114,8 @@ public class TestInputOutputFileValidator extends TestCase {
         LogicalPlan plan = genNewLoadStorePlan(inputfile, outputfile, ctx.getDfs()) ;                     
         
         CompilationMessageCollector collector = new CompilationMessageCollector() ;        
-        LogicalPlanValidationExecutor executor = new LogicalPlanValidationExecutor(plan, ctx) ;
+        boolean isBeforeOptimizer = false; // we are not optimizing in this testcase
+        LogicalPlanValidationExecutor executor = new LogicalPlanValidationExecutor(plan, ctx, isBeforeOptimizer) ;
         executor.validate(plan, collector) ;
             
         assertFalse(collector.hasError()) ;
@@ -123,7 +134,8 @@ public class TestInputOutputFileValidator extends TestCase {
         LogicalPlan plan = genNewLoadStorePlan(inputfile, outputfile, ctx.getDfs()) ;                     
         
         CompilationMessageCollector collector = new CompilationMessageCollector() ;        
-        LogicalPlanValidationExecutor executor = new LogicalPlanValidationExecutor(plan, ctx) ;
+        boolean isBeforeOptimizer = false; // we are not optimizing in this testcase
+        LogicalPlanValidationExecutor executor = new LogicalPlanValidationExecutor(plan, ctx, isBeforeOptimizer) ;
         try {
             executor.validate(plan, collector) ;
             fail("Excepted to fail.");
@@ -137,6 +149,93 @@ public class TestInputOutputFileValidator extends TestCase {
         }       
 
     }
+    
+    /**
+     * Testcase to ensure Input output validation allows store to a location
+     * that does not exist when using {@link PigServer#store(String, String)}
+     * @throws Exception
+     */
+    @Test
+    public void testPigServerStore() throws Exception {
+        String input = "input.txt";
+        String output= "output.txt";
+        String data[] = new String[] {"hello\tworld"};
+        ExecType[] modes = new ExecType[] {ExecType.MAPREDUCE, ExecType.LOCAL};
+        PigServer pig = null;
+        for (ExecType execType : modes) {
+            try {
+                if(execType == ExecType.MAPREDUCE) {
+                    pig = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
+                } else {
+                    Properties props = new Properties();
+                    props.put(MapRedUtil.FILE_SYSTEM_NAME, "file:///");
+                    pig = new PigServer(ExecType.LOCAL, props);
+                }
+                // reinitialize FileLocalizer for each mode
+                // this is need for the tmp file creation as part of
+                // PigServer.openIterator
+                FileLocalizer.setInitialized(false);
+                Util.deleteFile(pig.getPigContext(), input);
+                Util.deleteFile(pig.getPigContext(), output);
+                Util.createInputFile(pig.getPigContext(), input, data);
+                pig.registerQuery("a = load '" + input + "';");
+                pig.store("a", output);
+                pig.registerQuery("b = load '" + output + "';");
+                Iterator<Tuple> it = pig.openIterator("b");
+                Tuple t = it.next();
+                Assert.assertEquals("hello", t.get(0).toString());
+                Assert.assertEquals("world", t.get(1).toString());
+                Assert.assertEquals(false, it.hasNext());
+            } finally {
+                Util.deleteFile(pig.getPigContext(), input);
+                Util.deleteFile(pig.getPigContext(), output);
+            }
+        }
+    }
+    
+    /**
+     * Test case to test that Input output file validation catches the case
+     * where the output file exists when using 
+     * {@link PigServer#store(String, String)}
+     * @throws Exception
+     */
+    @Test
+    public void testPigServerStoreNeg() throws Exception {
+        String input = "input.txt";
+        String output= "output.txt";
+        String data[] = new String[] {"hello\tworld"};
+        ExecType[] modes = new ExecType[] {ExecType.MAPREDUCE, ExecType.LOCAL};
+        PigServer pig = null;
+        for (ExecType execType : modes) {
+            try {
+                boolean exceptionCaught = false;
+                if(execType == ExecType.MAPREDUCE) {
+                    pig = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
+                } else {
+                    Properties props = new Properties();
+                    props.put(MapRedUtil.FILE_SYSTEM_NAME, "file:///");
+                    pig = new PigServer(ExecType.LOCAL, props);
+                }
+                Util.deleteFile(pig.getPigContext(), input);
+                Util.deleteFile(pig.getPigContext(), output);
+                Util.createInputFile(pig.getPigContext(), input, data);
+                Util.createInputFile(pig.getPigContext(), output, data);
+                try {
+                    pig.registerQuery("a = load '" + input + "';");
+                    pig.store("a", output);
+                } catch (Exception e) {
+                    assertEquals(6000, LogUtils.getPigException(e).getErrorCode());
+                    exceptionCaught = true;
+                }
+                if(!exceptionCaught) {
+                    Assert.fail("Expected exception to be caught");
+                }
+            } finally {
+                Util.deleteFile(pig.getPigContext(), input);
+                Util.deleteFile(pig.getPigContext(), output);
+            }
+        }
+    }
         
     private LogicalPlan genNewLoadStorePlan(String inputFile,
                                             String outputFile, DataStorage dfs) 
diff --git a/test/org/apache/pig/test/TestLocalPOSplit.java b/test/org/apache/pig/test/TestLocalPOSplit.java
index e80949cd5..d74312d17 100644
--- a/test/org/apache/pig/test/TestLocalPOSplit.java
+++ b/test/org/apache/pig/test/TestLocalPOSplit.java
@@ -302,13 +302,19 @@ public class TestLocalPOSplit extends TestCase {
         CompilationMessageCollector collector = new CompilationMessageCollector();
         FrontendException caught = null;
         try {
+            boolean isBeforeOptimizer = true;
             LogicalPlanValidationExecutor validator = new LogicalPlanValidationExecutor(
-                    plan, pigContext);
+                    plan, pigContext, isBeforeOptimizer);
             validator.validate(plan, collector);
 
             FunctionalLogicalOptimizer optimizer = new FunctionalLogicalOptimizer(
                     plan);
             optimizer.optimize();
+            
+            isBeforeOptimizer = false;
+            validator = new LogicalPlanValidationExecutor(
+                    plan, pigContext, isBeforeOptimizer);
+            validator.validate(plan, collector);
         } catch (FrontendException fe) {
             // Need to go through and see what the collector has in it. But
             // remember what we've caught so we can wrap it into what we
diff --git a/test/org/apache/pig/test/TestPartitionFilterOptimization.java b/test/org/apache/pig/test/TestPartitionFilterOptimization.java
index 7011b2df3..1e1a2f4ed 100644
--- a/test/org/apache/pig/test/TestPartitionFilterOptimization.java
+++ b/test/org/apache/pig/test/TestPartitionFilterOptimization.java
@@ -505,20 +505,20 @@ public class TestPartitionFilterOptimization extends TestCase {
         }
 
         @Override
-        public String[] getPartitionKeys(String location, Configuration conf)
+        public String[] getPartitionKeys(String location, Job job)
                 throws IOException {
             return partCols;
         }
 
         @Override
-        public ResourceSchema getSchema(String location, Configuration conf)
+        public ResourceSchema getSchema(String location, Job job)
                 throws IOException {
             return new ResourceSchema(schema);
         }
 
         @Override
         public ResourceStatistics getStatistics(String location,
-                Configuration conf) throws IOException {
+                Job job) throws IOException {
             return null;
         }
 
diff --git a/test/org/apache/pig/test/TestStore.java b/test/org/apache/pig/test/TestStore.java
index c41555c00..8a538737a 100644
--- a/test/org/apache/pig/test/TestStore.java
+++ b/test/org/apache/pig/test/TestStore.java
@@ -23,6 +23,7 @@ import java.io.FileReader;
 import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
+import java.util.Properties;
 import java.util.Random;
 
 import junit.framework.Assert;
@@ -30,18 +31,12 @@ import junit.framework.Assert;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.Text;
 import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.OutputFormat;
-import org.apache.hadoop.mapreduce.RecordWriter;
-import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
-import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
 import org.apache.pig.ExecType;
 import org.apache.pig.PigException;
 import org.apache.pig.PigServer;
 import org.apache.pig.ResourceSchema;
 import org.apache.pig.ResourceStatistics;
-import org.apache.pig.StoreFunc;
 import org.apache.pig.StoreMetadata;
 import org.apache.pig.ResourceSchema.ResourceFieldSchema;
 import org.apache.pig.backend.datastorage.DataStorage;
@@ -51,6 +46,7 @@ import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLau
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
+import org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil;
 import org.apache.pig.builtin.BinStorage;
 import org.apache.pig.builtin.PigStorage;
 import org.apache.pig.data.DataBag;
@@ -329,8 +325,8 @@ public class TestStore extends junit.framework.TestCase {
             Util.registerMultiLineQuery(pig, query);
             pig.executeBatch();
             ResourceSchema rs = new BinStorage().getSchema(outputFileName, 
-                    ConfigurationUtil.toConfiguration(pig.getPigContext().
-                            getProperties()));
+                    new Job(ConfigurationUtil.toConfiguration(pig.getPigContext().
+                            getProperties())));
             Schema expectedSchema = Util.getSchemaFromString(
                     "c:chararray,i:int,d:double");
             Assert.assertTrue("Checking binstorage getSchema output", Schema.equals( 
@@ -399,7 +395,9 @@ public class TestStore extends junit.framework.TestCase {
                     Util.deleteFile(ps.getPigContext(), outputFileName);
                     Util.deleteFile(ps.getPigContext(), storeSchemaOutputFile);
                 } else {
-                    ps = new PigServer(ExecType.LOCAL);
+                    Properties props = new Properties();                                          
+                    props.setProperty(MapRedUtil.FILE_SYSTEM_NAME, "file:///");
+                    ps = new PigServer(ExecType.LOCAL, props);
                     Util.deleteFile(ps.getPigContext(), inputFileName);
                     Util.deleteFile(ps.getPigContext(), outputFileName);
                     Util.deleteFile(ps.getPigContext(), storeSchemaOutputFile);
@@ -424,61 +422,195 @@ public class TestStore extends junit.framework.TestCase {
         }
     }
     
-    public static class DummyStore extends StoreFunc implements StoreMetadata{
-
-        @Override
-        public void checkSchema(ResourceSchema s) throws IOException {
+    @Test
+    public void testCleanupOnFailure() throws Exception {
+        PigServer ps = null;
+        String cleanupSuccessFile = outputFileName + "_cleanupOnFailure_succeeded";
+        String cleanupFailFile = outputFileName + "_cleanupOnFailure_failed";
+        try {
+            ExecType[] modes = new ExecType[] { ExecType.LOCAL, ExecType.MAPREDUCE};
+            String[] inputData = new String[]{"hello\tworld", "bye\tworld"};
             
+            String script = "a = load '"+ inputFileName + "';" +
+                    "store a into '" + outputFileName + "' using " + 
+                    DummyStore.class.getName() + "('true');";
+            
+            for (ExecType execType : modes) {
+                if(execType == ExecType.MAPREDUCE) {
+                    ps = new PigServer(ExecType.MAPREDUCE, 
+                            cluster.getProperties());
+                } else {
+                    Properties props = new Properties();                                          
+                    props.setProperty(MapRedUtil.FILE_SYSTEM_NAME, "file:///");
+                    ps = new PigServer(ExecType.LOCAL, props);
+                }
+                Util.deleteFile(ps.getPigContext(), inputFileName);
+                Util.deleteFile(ps.getPigContext(), outputFileName);
+                Util.deleteFile(ps.getPigContext(), cleanupFailFile);
+                Util.deleteFile(ps.getPigContext(), cleanupSuccessFile);
+                ps.setBatchOn();
+                Util.createInputFile(ps.getPigContext(), 
+                        inputFileName, inputData);
+                Util.registerMultiLineQuery(ps, script);
+                ps.executeBatch();
+                assertEquals(
+                        "Checking if file indicating that cleanupOnFailure failed " +
+                        " does not exists in " + execType + " mode", false, 
+                        Util.exists(ps.getPigContext(), cleanupFailFile));
+                assertEquals(
+                        "Checking if file indicating that cleanupOnFailure was " +
+                        "successfully called exists in " + execType + " mode", true, 
+                        Util.exists(ps.getPigContext(), cleanupSuccessFile));
+            }
+        } catch (Exception e) {
+            e.printStackTrace();
+            Assert.fail("Exception encountered - hence failing:" + e);
+        } finally {
+            Util.deleteFile(ps.getPigContext(), inputFileName);
+            Util.deleteFile(ps.getPigContext(), outputFileName);
+            Util.deleteFile(ps.getPigContext(), cleanupFailFile);
+            Util.deleteFile(ps.getPigContext(), cleanupSuccessFile);
         }
-
-        @Override
-        public OutputFormat getOutputFormat() throws IOException {
-            // we don't really write in the test - so this is just to keep
-            // Pig/hadoop happy
-            return new TextOutputFormat<Long, Text>();
-        }
-
-        @Override
-        public void prepareToWrite(RecordWriter writer) throws IOException {
+    }
+    
+    
+    @Test
+    public void testCleanupOnFailureMultiStore() throws Exception {
+        PigServer ps = null;
+        String outputFileName1 = "/tmp/TestStore-output-" + new Random().nextLong() + ".txt";
+        String outputFileName2 = "/tmp/TestStore-output-" + new Random().nextLong() + ".txt";
+        String cleanupSuccessFile1 = outputFileName1 + "_cleanupOnFailure_succeeded1";
+        String cleanupFailFile1 = outputFileName1 + "_cleanupOnFailure_failed1";
+        String cleanupSuccessFile2 = outputFileName2 + "_cleanupOnFailure_succeeded2";
+        String cleanupFailFile2 = outputFileName2 + "_cleanupOnFailure_failed2";
+        
+        try {
+            ExecType[] modes = new ExecType[] { /*ExecType.LOCAL, */ExecType.MAPREDUCE};
+            String[] inputData = new String[]{"hello\tworld", "bye\tworld"};
             
+            // though the second store should
+            // not cause a failure, the first one does and the result should be
+            // that both stores are considered to have failed
+            String script = "a = load '"+ inputFileName + "';" +
+                    "store a into '" + outputFileName1 + "' using " + 
+                    DummyStore.class.getName() + "('true', '1');" +
+                    "store a into '" + outputFileName2 + "' using " + 
+                    DummyStore.class.getName() + "('false', '2');"; 
+            
+            for (ExecType execType : modes) {
+                if(execType == ExecType.MAPREDUCE) {
+                    ps = new PigServer(ExecType.MAPREDUCE, 
+                            cluster.getProperties());
+                } else {
+                    Properties props = new Properties();                                          
+                    props.setProperty(MapRedUtil.FILE_SYSTEM_NAME, "file:///");
+                    ps = new PigServer(ExecType.LOCAL, props);
+                }
+                Util.deleteFile(ps.getPigContext(), inputFileName);
+                Util.deleteFile(ps.getPigContext(), outputFileName1);
+                Util.deleteFile(ps.getPigContext(), outputFileName2);
+                Util.deleteFile(ps.getPigContext(), cleanupFailFile1);
+                Util.deleteFile(ps.getPigContext(), cleanupSuccessFile1);
+                Util.deleteFile(ps.getPigContext(), cleanupFailFile2);
+                Util.deleteFile(ps.getPigContext(), cleanupSuccessFile2);
+                ps.setBatchOn();
+                Util.createInputFile(ps.getPigContext(), 
+                        inputFileName, inputData);
+                Util.registerMultiLineQuery(ps, script);
+                ps.executeBatch();
+                assertEquals(
+                        "Checking if file indicating that cleanupOnFailure failed " +
+                        " does not exists in " + execType + " mode", false, 
+                        Util.exists(ps.getPigContext(), cleanupFailFile1));
+                assertEquals(
+                        "Checking if file indicating that cleanupOnFailure failed " +
+                        " does not exists in " + execType + " mode", false, 
+                        Util.exists(ps.getPigContext(), cleanupFailFile2));
+                assertEquals(
+                        "Checking if file indicating that cleanupOnFailure was " +
+                        "successfully called exists in " + execType + " mode", true, 
+                        Util.exists(ps.getPigContext(), cleanupSuccessFile1));
+                assertEquals(
+                        "Checking if file indicating that cleanupOnFailure was " +
+                        "successfully called exists in " + execType + " mode", true, 
+                        Util.exists(ps.getPigContext(), cleanupSuccessFile2));
+            }
+        } catch (Exception e) {
+            e.printStackTrace();
+            Assert.fail("Exception encountered - hence failing:" + e);
+        } finally {
+            Util.deleteFile(ps.getPigContext(), inputFileName);
+            Util.deleteFile(ps.getPigContext(), outputFileName1);
+            Util.deleteFile(ps.getPigContext(), outputFileName2);
+            Util.deleteFile(ps.getPigContext(), cleanupFailFile1);
+            Util.deleteFile(ps.getPigContext(), cleanupSuccessFile1);
+            Util.deleteFile(ps.getPigContext(), cleanupFailFile2);
+            Util.deleteFile(ps.getPigContext(), cleanupSuccessFile2);
         }
+    }
+    public static class DummyStore extends PigStorage implements StoreMetadata{
 
-        @Override
-        public void putNext(Tuple t) throws IOException {
-            // we don't really write anything out            
-        }
+        private boolean failInPutNext = false;
+        
+        private String outputFileSuffix= "";
 
-        @Override
-        public String relToAbsPathForStoreLocation(String location, Path curDir)
-                throws IOException {
-            return location;
+        public DummyStore(String failInPutNextStr) {
+            failInPutNext = Boolean.parseBoolean(failInPutNextStr);
         }
-
-        @Override
-        public void setStoreFuncUDFContextSignature(String signature) {
-            
+        
+        public DummyStore(String failInPutNextStr, String outputFileSuffix) {
+            failInPutNext = Boolean.parseBoolean(failInPutNextStr);
+            this.outputFileSuffix = outputFileSuffix;
         }
+        
+        public DummyStore() {
+        }
+
 
         @Override
-        public void setStoreLocation(String location, Job job)
-                throws IOException {
-            FileOutputFormat.setOutputPath(job, new Path(location));
+        public void putNext(Tuple t) throws IOException {
+            if(failInPutNext) {
+                throw new IOException("Failing in putNext");
+            }
+            super.putNext(t);
         }
 
         @Override
         public void storeSchema(ResourceSchema schema, String location,
-                Configuration conf) throws IOException {
-            FileSystem fs = FileSystem.get(conf);
+                Job job) throws IOException {
+            FileSystem fs = FileSystem.get(job.getConfiguration());
             // create a file to test that this method got called - if it gets called
             // multiple times, the create will throw an Exception
             fs.create(
-                    new Path(conf.get("mapred.output.dir") + "_storeSchema_test"),
+                    new Path(location + "_storeSchema_test"),
                     false);
         }
 
+        @Override
+        public void cleanupOnFailure(String location, Job job)
+                throws IOException {
+            super.cleanupOnFailure(location, job);
+            
+            // check that the output file location is not present
+            Configuration conf = job.getConfiguration();
+            FileSystem fs = FileSystem.get(conf);
+            if(fs.exists(new Path(location))) {
+                // create a file to inidicate that the cleanup did not happen
+                fs.create(new Path(location + "_cleanupOnFailure_failed" + 
+                        outputFileSuffix), false);
+            }
+            // create a file to test that this method got called successfully
+            // if it gets called multiple times, the create will throw an Exception 
+            fs.create(
+                    new Path(location + "_cleanupOnFailure_succeeded" + 
+                            outputFileSuffix), false);
+        }
+
         @Override
         public void storeStatistics(ResourceStatistics stats, String location,
-                Configuration conf) throws IOException {
+                Job job) throws IOException {
+            // TODO Auto-generated method stub
+            
         }
     }
     
