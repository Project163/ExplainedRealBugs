diff --git a/.eclipse.templates/.classpath b/.eclipse.templates/.classpath
index 1602ebb33..aa9bfd535 100644
--- a/.eclipse.templates/.classpath
+++ b/.eclipse.templates/.classpath
@@ -25,7 +25,7 @@
 	<classpathentry exported="true" kind="lib" path="build/ivy/lib/Pig/js-1.7R2.jar"/>
 	<classpathentry exported="true" kind="lib" path="build/ivy/lib/Pig/zookeeper-3.3.3.jar"/>
 	<classpathentry exported="true" kind="lib" path="build/ivy/lib/Pig/guava-r06.jar"/>
-	<classpathentry exported="true" kind="lib" path="build/ivy/lib/Pig/jython-2.5.0.jar"/>
+	<classpathentry exported="true" kind="lib" path="build/ivy/lib/Pig/jython-2.5.2.jar"/>
 	<classpathentry exported="true" kind="lib" path="build/ivy/lib/Pig/hadoop-core-0.20.2.jar"/>
 	<classpathentry exported="true" kind="lib" path="build/ivy/lib/Pig/commons-logging-1.1.1.jar"/>
 	<classpathentry exported="true" kind="lib" path="build/ivy/lib/Pig/commons-cli-1.2.jar"/>
diff --git a/CHANGES.txt b/CHANGES.txt
index 24b31c3d9..33bc32487 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -24,6 +24,8 @@ INCOMPATIBLE CHANGES
 
 IMPROVEMENTS
 
+PIG-1314: Add DateTime Support to Pig (zjshen via thejas)
+
 PIG-2785: NoClassDefFoundError after upgrading to pig 0.10.0 from 0.9.0 (matterhayes via sms)
 
 PIG-2556: CSVExcelStorage load: quoted field with newline as first character sees newline as record end (tivv via dvryaboy)
diff --git a/conf/pig.properties b/conf/pig.properties
index d26a46722..a453940c0 100644
--- a/conf/pig.properties
+++ b/conf/pig.properties
@@ -17,6 +17,10 @@
 #exectype local|mapreduce, mapreduce is default
 #exectype=local
 
+#the default timezone: if it is not set, the default timezone for this host is used.
+#the correct timezone format is the UTC offset: e.g., +08:00. 
+#pig.datetime.default.tz=
+
 #pig.logfile=
 
 #Do not spill temp files smaller than this size (bytes)
diff --git a/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/DBStorage.java b/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/DBStorage.java
index 8579a3887..53411b8b2 100644
--- a/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/DBStorage.java
+++ b/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/DBStorage.java
@@ -17,6 +17,8 @@
  */
 package org.apache.pig.piggybank.storage;
 
+import org.joda.time.DateTime;
+
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.io.NullWritable;
@@ -115,6 +117,11 @@ public class DBStorage extends StoreFunc {
             sqlPos++;
             break;
 
+          case DataType.DATETIME:
+            ps.setDate(sqlPos, ((DateTime) field).toDate());
+            sqlPos++;
+            break;
+
           case DataType.BYTEARRAY:
             byte[] b = ((DataByteArray) field).get();
             ps.setBytes(sqlPos, b);
diff --git a/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/SequenceFileLoader.java b/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/SequenceFileLoader.java
index f75f3147f..6031a40d6 100644
--- a/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/SequenceFileLoader.java
+++ b/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/SequenceFileLoader.java
@@ -21,6 +21,8 @@ import java.io.IOException;
 import java.lang.reflect.Type;
 import java.util.ArrayList;
 
+import org.joda.time.DateTime;
+
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.io.BooleanWritable;
@@ -40,6 +42,7 @@ import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
 import org.apache.hadoop.mapreduce.lib.input.SequenceFileRecordReader;
 import org.apache.pig.FileInputLoadFunc;
 import org.apache.pig.backend.BackendException;
+import org.apache.pig.backend.hadoop.DateTimeWritable;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigSplit;
 import org.apache.pig.data.DataByteArray;
 import org.apache.pig.data.DataType;
@@ -96,6 +99,7 @@ public class SequenceFileLoader extends FileInputLoadFunc {
     else if (t == DoubleWritable.class) return DataType.DOUBLE;
     else if (t == BooleanWritable.class) return DataType.BOOLEAN;
     else if (t == ByteWritable.class) return DataType.BYTE;
+    else if (t == DateTimeWritable.class) return DataType.DATETIME;
     // not doing maps or other complex types for now
     else return DataType.ERROR;
   }
@@ -110,6 +114,7 @@ public class SequenceFileLoader extends FileInputLoadFunc {
       case DataType.FLOAT: return ((FloatWritable) w).get();
       case DataType.DOUBLE: return ((DoubleWritable) w).get();
       case DataType.BYTE: return ((ByteWritable) w).get();
+      case DataType.DATETIME: return ((DateTimeWritable) w).get();
     }
     
     return null;
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/SchemaConverter.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/SchemaConverter.java
index 7860aae38..1bcdc418e 100644
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/SchemaConverter.java
+++ b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/SchemaConverter.java
@@ -49,6 +49,9 @@ class SchemaConverter {
         case DataType.BOOLEAN:
             ret = ColumnType.BOOL; 
             break;
+        case DataType.DATETIME:
+            ret = ColumnType.DATETIME; 
+            break;
         case DataType.BAG:
             ret = ColumnType.COLLECTION; 
             break;
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/DateTimeExpr.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/DateTimeExpr.java
new file mode 100644
index 000000000..c1a7e6092
--- /dev/null
+++ b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/DateTimeExpr.java
@@ -0,0 +1,53 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with this
+ * work for additional information regarding copyright ownership. The ASF
+ * licenses this file to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ * 
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+ * License for the specific language governing permissions and limitations under
+ * the License.
+ */
+package org.apache.hadoop.zebra.pig.comparator;
+
+import org.joda.time.DateTime;
+
+
+public final class DateTimeExpr extends FixedLengthPrimitive {
+  private static final int ONE_MINUTE = 60000;
+  public DateTimeExpr(int index) {
+    super(index, (Long.SIZE + Short.SIZE) / Byte.SIZE);
+  }
+
+  protected void convertValue(byte[] b, Object o) {
+    if (b.length != (Long.SIZE + Short.SIZE) / Byte.SIZE) {
+      throw new IllegalArgumentException("Incorrect buffer size");
+    }
+    long dt = ((DateTime) o).getMillis();
+    dt ^= 1L << (Long.SIZE - 1);
+    b[7] = (byte) dt;
+    b[6] = (byte) (dt >> 8);
+    b[5] = (byte) (dt >> 16);
+    b[4] = (byte) (dt >> 24);
+    b[3] = (byte) (dt >> 32);
+    b[2] = (byte) (dt >> 40);
+    b[1] = (byte) (dt >> 48);
+    b[0] = (byte) (dt >> 56);
+    
+    short dtz = (short) (((DateTime) o).getZone().getOffset((DateTime) o) / ONE_MINUTE);
+    dtz ^= 1 << (Short.SIZE - 1);
+    b[9] = (byte) dtz;
+    b[8] = (byte) (dtz >> 8);
+  }
+
+  @Override
+  protected String getType() {
+    return "DateTime";
+  }
+}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/ExprUtils.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/ExprUtils.java
index bcc9b2aec..f5072c7b4 100644
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/ExprUtils.java
+++ b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/ExprUtils.java
@@ -109,6 +109,8 @@ public class ExprUtils {
         return new IntExpr(index);
       case DataType.LONG:
         return new LongExpr(index);
+      case DataType.DATETIME:
+        return new DateTimeExpr(index);
       default:
         throw new RuntimeException("Not a prmitive PIG type");
     }
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/schema/ColumnType.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/schema/ColumnType.java
index f0f7c9df5..baf332d1d 100644
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/schema/ColumnType.java
+++ b/contrib/zebra/src/java/org/apache/hadoop/zebra/schema/ColumnType.java
@@ -78,6 +78,14 @@ public enum ColumnType {
       return DataType.BOOLEAN;
     }
   },
+  /**
+   * DateTime
+   */
+  DATETIME("datetime") {
+    public byte pigDataType() {
+      return DataType.DATETIME;
+    }
+  },
   /**
    * Collection
    */
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/schema/SchemaParser.jjt b/contrib/zebra/src/java/org/apache/hadoop/zebra/schema/SchemaParser.jjt
index 47dde117a..ca4e9b4c2 100644
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/schema/SchemaParser.jjt
+++ b/contrib/zebra/src/java/org/apache/hadoop/zebra/schema/SchemaParser.jjt
@@ -57,6 +57,7 @@ TOKEN : { <BOOL : "boolean"> }
 TOKEN : { <LONG : "long"> }
 TOKEN : { <FLOAT : "float"> }
 TOKEN : { <DOUBLE : "double"> }
+TOKEN : { <DATETIME : "datetime"> }
 TOKEN : { <STRING : "string"> }
 TOKEN : { <BYTES : "bytes"> }
 TOKEN : { <COLLECTION : "collection"> }
diff --git a/src/org/apache/pig/LoadCaster.java b/src/org/apache/pig/LoadCaster.java
index 42be23f0e..574769be1 100644
--- a/src/org/apache/pig/LoadCaster.java
+++ b/src/org/apache/pig/LoadCaster.java
@@ -19,6 +19,8 @@ package org.apache.pig;
 import java.io.IOException;
 import java.util.Map;
 
+import org.joda.time.DateTime;
+
 import org.apache.pig.ResourceSchema.ResourceFieldSchema;
 import org.apache.pig.classification.InterfaceAudience;
 import org.apache.pig.classification.InterfaceStability;
@@ -69,6 +71,14 @@ public interface LoadCaster {
      */
     public Double bytesToDouble(byte[] b) throws IOException;
 
+    /**
+     * Cast data from bytearray to datetime value.  
+     * @param b bytearray to be cast.
+     * @return datetime value.
+     * @throws IOException if the value cannot be cast.
+     */
+    public DateTime bytesToDateTime(byte[] b) throws IOException;
+
     /**
      * Cast data from bytearray to integer value.  
      * @param b bytearray to be cast.
diff --git a/src/org/apache/pig/PigWarning.java b/src/org/apache/pig/PigWarning.java
index e65a63083..5de075f0b 100644
--- a/src/org/apache/pig/PigWarning.java
+++ b/src/org/apache/pig/PigWarning.java
@@ -34,6 +34,7 @@ public enum PigWarning {
     IMPLICIT_CAST_TO_INT,
     IMPLICIT_CAST_TO_LONG,
     IMPLICIT_CAST_TO_BOOLEAN,
+    IMPLICIT_CAST_TO_DATETIME,
     IMPLICIT_CAST_TO_MAP,
     IMPLICIT_CAST_TO_TUPLE,
     TOO_LARGE_FOR_INT,
diff --git a/src/org/apache/pig/StoreCaster.java b/src/org/apache/pig/StoreCaster.java
index 6e947a072..5fe48de21 100644
--- a/src/org/apache/pig/StoreCaster.java
+++ b/src/org/apache/pig/StoreCaster.java
@@ -20,6 +20,8 @@ package org.apache.pig;
 import java.io.IOException;
 import java.util.Map;
 
+import org.joda.time.DateTime;
+
 import org.apache.pig.classification.InterfaceAudience;
 import org.apache.pig.classification.InterfaceStability;
 import org.apache.pig.data.DataBag;
@@ -48,6 +50,8 @@ public interface StoreCaster extends LoadCaster {
 
     public byte[] toBytes(Long l) throws IOException;
 
+    public byte[] toBytes(DateTime dt) throws IOException;
+
     public byte[] toBytes(Map<String, Object> m) throws IOException;
 
     public byte[] toBytes(Tuple t) throws IOException;
diff --git a/src/org/apache/pig/backend/hadoop/DateTimeWritable.java b/src/org/apache/pig/backend/hadoop/DateTimeWritable.java
new file mode 100644
index 000000000..aefea0725
--- /dev/null
+++ b/src/org/apache/pig/backend/hadoop/DateTimeWritable.java
@@ -0,0 +1,112 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.pig.backend.hadoop;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+
+import org.joda.time.DateTime;
+import org.joda.time.DateTimeZone;
+
+import org.apache.hadoop.io.WritableComparable;
+import org.apache.hadoop.io.WritableComparator;
+
+/**
+ * Writable for Double values.
+ */
+public class DateTimeWritable implements WritableComparable {
+    
+    private static final int ONE_MINUTE = 60000;
+
+    private DateTime value = null;
+
+    public DateTimeWritable() {
+
+    }
+
+    public DateTimeWritable(DateTime dt) {
+        value = dt;
+    }
+
+    public void readFields(DataInput in) throws IOException {
+        value = new DateTime(in.readLong(), DateTimeZone.forOffsetMillis(in.readShort() * ONE_MINUTE));
+    }
+
+    public void write(DataOutput out) throws IOException {
+        out.writeLong(value.getMillis());
+        out.writeShort(value.getZone().getOffset(value) / ONE_MINUTE);
+    }
+
+    public void set(DateTime dt) {
+        value = dt;
+    }
+
+    public DateTime get() {
+        return value;
+    }
+
+    /**
+     * Returns true iff <code>o</code> is a DateTimeWritable with the same
+     * value.
+     */
+    @Override
+    public boolean equals(Object o) {
+        if (!(o instanceof DateTimeWritable)) {
+            return false;
+        }
+        DateTimeWritable other = (DateTimeWritable) o;
+        return this.value.equals(other.value);
+    }
+
+    @Override
+    public int hashCode() {
+        return value.hashCode();
+    }
+
+    public int compareTo(Object o) {
+        DateTimeWritable other = (DateTimeWritable) o;
+        return value.compareTo(other.value);
+    }
+
+    @Override
+    public String toString() {
+        return value.toString();
+    }
+
+    /** 
+     * A Comparator optimized for DateTimeWritable.
+     */
+    public static class Comparator extends WritableComparator {
+        public Comparator() {
+            super(DateTimeWritable.class);
+        }
+
+        @Override
+        public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
+            DateTime thisValue = new DateTime(readLong(b1, s1));
+            DateTime thatValue = new DateTime(readLong(b2, s2));
+            return thisValue.compareTo(thatValue);
+        }
+    }
+
+    static { // register this comparator
+        WritableComparator.define(DateTimeWritable.class, new Comparator());
+    }
+}
diff --git a/src/org/apache/pig/backend/hadoop/HDataType.java b/src/org/apache/pig/backend/hadoop/HDataType.java
index 1a6690af0..84a56b89b 100644
--- a/src/org/apache/pig/backend/hadoop/HDataType.java
+++ b/src/org/apache/pig/backend/hadoop/HDataType.java
@@ -19,6 +19,8 @@ package org.apache.pig.backend.hadoop;
 
 import java.util.Map;
 
+import org.joda.time.DateTime;
+
 import org.apache.pig.PigException;
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.data.DataBag;
@@ -32,6 +34,7 @@ import org.apache.pig.impl.io.NullableDoubleWritable;
 import org.apache.pig.impl.io.NullableFloatWritable;
 import org.apache.pig.impl.io.NullableIntWritable;
 import org.apache.pig.impl.io.NullableLongWritable;
+import org.apache.pig.impl.io.NullableDateTimeWritable;
 import org.apache.pig.impl.io.NullableText;
 import org.apache.pig.impl.io.NullableTuple;
 import org.apache.pig.impl.io.PigNullableWritable;
@@ -48,6 +51,7 @@ public class HDataType {
     static NullableDoubleWritable doubleWrit = new NullableDoubleWritable();
     static NullableIntWritable intWrit = new NullableIntWritable();
     static NullableLongWritable longWrit = new NullableLongWritable();
+    static NullableDateTimeWritable dtWrit = new NullableDateTimeWritable();
     static NullableBag defDB = new NullableBag();
     static NullableTuple defTup = new NullableTuple();
     static Map<Byte, String> typeToName = null;
@@ -82,7 +86,10 @@ public class HDataType {
            
         case DataType.LONG:
             return new NullableLongWritable((Long)o);
-          
+
+        case DataType.DATETIME:
+            return new NullableDateTimeWritable((DateTime)o);
+
         case DataType.TUPLE:
             return new NullableTuple((Tuple)o);
          
@@ -126,6 +133,10 @@ public class HDataType {
                 NullableLongWritable nLongWrit = new NullableLongWritable();
                 nLongWrit.setNull(true);
                 return nLongWrit;
+            case DataType.DATETIME:
+                NullableDateTimeWritable nDateTimeWrit = new NullableDateTimeWritable();
+                nDateTimeWrit.setNull(true);
+                return nDateTimeWrit;
             case DataType.TUPLE:
                 NullableTuple ntuple = new NullableTuple();
                 ntuple.setNull(true);
@@ -178,6 +189,9 @@ public class HDataType {
         case DataType.LONG:
             wcKey = longWrit;
             break;
+        case DataType.DATETIME:
+            wcKey = dtWrit;
+            break;
         case DataType.TUPLE:
             wcKey = defTup;
             break;
@@ -212,6 +226,8 @@ public class HDataType {
             return DataType.INTEGER;
         else if (o instanceof NullableLongWritable)
             return DataType.LONG;
+        else if (o instanceof NullableDateTimeWritable)
+            return DataType.DATETIME;
         else if (o instanceof NullableBag)
             return DataType.BAG;
         else if (o instanceof NullableTuple)
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java
index 7de5cd8ee..49446db45 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java
@@ -84,6 +84,7 @@ import org.apache.pig.impl.io.NullableDoubleWritable;
 import org.apache.pig.impl.io.NullableFloatWritable;
 import org.apache.pig.impl.io.NullableIntWritable;
 import org.apache.pig.impl.io.NullableLongWritable;
+import org.apache.pig.impl.io.NullableDateTimeWritable;
 import org.apache.pig.impl.io.NullablePartitionWritable;
 import org.apache.pig.impl.io.NullableText;
 import org.apache.pig.impl.io.NullableTuple;
@@ -956,6 +957,12 @@ public class JobControlCompiler{
         }
     }
 
+    public static class PigDateTimeWritableComparator extends PigWritableComparator {
+        public PigDateTimeWritableComparator() {
+            super(NullableDateTimeWritable.class);
+        }
+    }
+
     public static class PigCharArrayWritableComparator extends PigWritableComparator {
         public PigCharArrayWritableComparator() {
             super(NullableText.class);
@@ -1012,6 +1019,12 @@ public class JobControlCompiler{
         }
     }
 
+    public static class PigGroupingDateTimeWritableComparator extends WritableComparator {
+        public PigGroupingDateTimeWritableComparator() {
+            super(NullableDateTimeWritable.class, true);
+        }
+    }
+
     public static class PigGroupingCharArrayWritableComparator extends WritableComparator {
         public PigGroupingCharArrayWritableComparator() {
             super(NullableText.class, true);
@@ -1087,6 +1100,10 @@ public class JobControlCompiler{
                 job.setSortComparatorClass(PigDoubleRawComparator.class);
                 break;
 
+            case DataType.DATETIME:
+                job.setSortComparatorClass(PigDateTimeRawComparator.class);
+                break;
+
             case DataType.CHARARRAY:
                 job.setSortComparatorClass(PigTextRawComparator.class);
                 break;
@@ -1141,6 +1158,11 @@ public class JobControlCompiler{
             job.setGroupingComparatorClass(PigGroupingDoubleWritableComparator.class);
             break;
 
+        case DataType.DATETIME:
+            job.setSortComparatorClass(PigDateTimeWritableComparator.class);
+            job.setGroupingComparatorClass(PigGroupingDateTimeWritableComparator.class);
+            break;
+
         case DataType.CHARARRAY:
             job.setSortComparatorClass(PigCharArrayWritableComparator.class);
             job.setGroupingComparatorClass(PigGroupingCharArrayWritableComparator.class);
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java.orig b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java.orig
new file mode 100644
index 000000000..7de5cd8ee
--- /dev/null
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java.orig
@@ -0,0 +1,1525 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.pig.backend.hadoop.executionengine.mapReduceLayer;
+
+import java.io.File;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.io.OutputStream;
+import java.net.URI;
+import java.net.URISyntaxException;
+import java.net.URL;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Map;
+import java.util.Properties;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.filecache.DistributedCache;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.IOUtils;
+import org.apache.hadoop.io.WritableComparable;
+import org.apache.hadoop.io.WritableComparator;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.JobPriority;
+import org.apache.hadoop.mapred.jobcontrol.Job;
+import org.apache.hadoop.mapred.jobcontrol.JobControl;
+import org.apache.pig.ComparisonFunc;
+import org.apache.pig.ExecType;
+import org.apache.pig.LoadFunc;
+import org.apache.pig.PigException;
+import org.apache.pig.StoreFuncInterface;
+import org.apache.pig.backend.executionengine.ExecException;
+import org.apache.pig.backend.hadoop.HDataType;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.SecondaryKeyPartitioner;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.SkewedPartitioner;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.WeightedRangePartitioner;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.MROperPlan;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.ConstantExpression;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFRJoin;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeCogroup;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.util.PlanHelper;
+import org.apache.pig.backend.hadoop.executionengine.shims.HadoopShims;
+import org.apache.pig.data.BagFactory;
+import org.apache.pig.data.DataType;
+import org.apache.pig.data.SchemaTupleFrontend;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.data.TupleFactory;
+import org.apache.pig.impl.PigContext;
+import org.apache.pig.impl.io.FileLocalizer;
+import org.apache.pig.impl.io.FileSpec;
+import org.apache.pig.impl.io.NullableBooleanWritable;
+import org.apache.pig.impl.io.NullableBytesWritable;
+import org.apache.pig.impl.io.NullableDoubleWritable;
+import org.apache.pig.impl.io.NullableFloatWritable;
+import org.apache.pig.impl.io.NullableIntWritable;
+import org.apache.pig.impl.io.NullableLongWritable;
+import org.apache.pig.impl.io.NullablePartitionWritable;
+import org.apache.pig.impl.io.NullableText;
+import org.apache.pig.impl.io.NullableTuple;
+import org.apache.pig.impl.io.PigNullableWritable;
+import org.apache.pig.impl.plan.DepthFirstWalker;
+import org.apache.pig.impl.plan.OperatorKey;
+import org.apache.pig.impl.plan.VisitorException;
+import org.apache.pig.impl.util.JarManager;
+import org.apache.pig.impl.util.ObjectSerializer;
+import org.apache.pig.impl.util.Pair;
+import org.apache.pig.impl.util.UDFContext;
+import org.apache.pig.impl.util.Utils;
+import org.apache.pig.tools.pigstats.ScriptState;
+
+/**
+ * This is compiler class that takes an MROperPlan and converts
+ * it into a JobControl object with the relevant dependency info
+ * maintained. The JobControl Object is made up of Jobs each of
+ * which has a JobConf. The MapReduceOper corresponds to a Job
+ * and the getJobCong method returns the JobConf that is configured
+ * as per the MapReduceOper
+ *
+ * <h2>Comparator Design</h2>
+ * <p>
+ * A few words on how comparators are chosen.  In almost all cases we use raw
+ * comparators (the one exception being when the user provides a comparison
+ * function for order by).  For order by queries the PigTYPERawComparator
+ * functions are used, where TYPE is Int, Long, etc.  These comparators are
+ * null aware and asc/desc aware.  The first byte of each of the
+ * NullableTYPEWritable classes contains info on whether the value is null.
+ * Asc/desc is written as an array into the JobConf with the key pig.sortOrder
+ * so that it can be read by each of the comparators as part of their
+ * setConf call.
+ * <p>
+ * For non-order by queries, PigTYPEWritableComparator classes are used.
+ * These are all just type specific instances of WritableComparator.
+ *
+ */
+public class JobControlCompiler{
+    MROperPlan plan;
+    Configuration conf;
+    PigContext pigContext;
+
+    private static final Log log = LogFactory.getLog(JobControlCompiler.class);
+
+    public static final String LOG_DIR = "_logs";
+
+    public static final String END_OF_INP_IN_MAP = "pig.invoke.close.in.map";
+
+    private static final String REDUCER_ESTIMATOR_KEY = "pig.exec.reducer.estimator";
+    private static final String REDUCER_ESTIMATOR_ARG_KEY =  "pig.exec.reducer.estimator.arg";
+
+    /**
+     * We will serialize the POStore(s) present in map and reduce in lists in
+     * the Hadoop Conf. In the case of Multi stores, we could deduce these from
+     * the map plan and reduce plan but in the case of single store, we remove
+     * the POStore from the plan - in either case, we serialize the POStore(s)
+     * so that PigOutputFormat and PigOutputCommiter can get the POStore(s) in
+     * the same way irrespective of whether it is multi store or single store.
+     */
+    public static final String PIG_MAP_STORES = "pig.map.stores";
+    public static final String PIG_REDUCE_STORES = "pig.reduce.stores";
+
+    // A mapping of job to pair of store locations and tmp locations for that job
+    private Map<Job, Pair<List<POStore>, Path>> jobStoreMap;
+
+    private Map<Job, MapReduceOper> jobMroMap;
+
+    public JobControlCompiler(PigContext pigContext, Configuration conf) throws IOException {
+        this.pigContext = pigContext;
+        this.conf = conf;
+        jobStoreMap = new HashMap<Job, Pair<List<POStore>, Path>>();
+        jobMroMap = new HashMap<Job, MapReduceOper>();
+    }
+
+    /**
+     * Returns all store locations of a previously compiled job
+     */
+    public List<POStore> getStores(Job job) {
+        Pair<List<POStore>, Path> pair = jobStoreMap.get(job);
+        if (pair != null && pair.first != null) {
+            return pair.first;
+        } else {
+            return new ArrayList<POStore>();
+        }
+    }
+
+    /**
+     * Resets the state
+     */
+    public void reset() {
+        jobStoreMap = new HashMap<Job, Pair<List<POStore>, Path>>();
+        jobMroMap = new HashMap<Job, MapReduceOper>();
+        UDFContext.getUDFContext().reset();
+    }
+
+    /**
+     * Gets the map of Job and the MR Operator
+     */
+    public Map<Job, MapReduceOper> getJobMroMap() {
+        return Collections.unmodifiableMap(jobMroMap);
+    }
+
+    /**
+     * Moves all the results of a collection of MR jobs to the final
+     * output directory. Some of the results may have been put into a
+     * temp location to work around restrictions with multiple output
+     * from a single map reduce job.
+     *
+     * This method should always be called after the job execution
+     * completes.
+     */
+    public void moveResults(List<Job> completedJobs) throws IOException {
+        for (Job job: completedJobs) {
+            Pair<List<POStore>, Path> pair = jobStoreMap.get(job);
+            if (pair != null && pair.second != null) {
+                Path tmp = pair.second;
+                Path abs = new Path(tmp, "abs");
+                Path rel = new Path(tmp, "rel");
+                FileSystem fs = tmp.getFileSystem(conf);
+
+                if (fs.exists(abs)) {
+                    moveResults(abs, abs.toUri().getPath(), fs);
+                }
+
+                if (fs.exists(rel)) {
+                    moveResults(rel, rel.toUri().getPath()+"/", fs);
+                }
+            }
+        }
+    }
+
+    /**
+     * Walks the temporary directory structure to move (rename) files
+     * to their final location.
+     */
+    private void moveResults(Path p, String rem, FileSystem fs) throws IOException {
+        for (FileStatus fstat: fs.listStatus(p)) {
+            Path src = fstat.getPath();
+            if (fstat.isDir()) {
+                log.info("mkdir: "+src);
+                fs.mkdirs(removePart(src, rem));
+                moveResults(fstat.getPath(), rem, fs);
+            } else {
+                Path dst = removePart(src, rem);
+                log.info("mv: "+src+" "+dst);
+                fs.rename(src,dst);
+            }
+        }
+    }
+
+    private Path removePart(Path src, String part) {
+        URI uri = src.toUri();
+        String pathStr = uri.getPath().replace(part, "");
+        return new Path(pathStr);
+    }
+
+    /**
+     * Compiles all jobs that have no dependencies removes them from
+     * the plan and returns. Should be called with the same plan until
+     * exhausted.
+     * @param plan - The MROperPlan to be compiled
+     * @param grpName - The name given to the JobControl
+     * @return JobControl object - null if no more jobs in plan
+     * @throws JobCreationException
+     */
+    public JobControl compile(MROperPlan plan, String grpName) throws JobCreationException{
+        // Assert plan.size() != 0
+        this.plan = plan;
+
+        int timeToSleep;
+        String defaultPigJobControlSleep = pigContext.getExecType() == ExecType.LOCAL ? "100" : "5000";
+        String pigJobControlSleep = conf.get("pig.jobcontrol.sleep", defaultPigJobControlSleep);
+        if (!pigJobControlSleep.equals(defaultPigJobControlSleep)) {
+          log.info("overriding default JobControl sleep (" + defaultPigJobControlSleep + ") to " + pigJobControlSleep);
+        }
+
+        try {
+          timeToSleep = Integer.parseInt(pigJobControlSleep);
+        } catch (NumberFormatException e) {
+          throw new RuntimeException("Invalid configuration " +
+              "pig.jobcontrol.sleep=" + pigJobControlSleep +
+              " should be a time in ms. default=" + defaultPigJobControlSleep, e);
+        }
+
+        JobControl jobCtrl = HadoopShims.newJobControl(grpName, timeToSleep);
+
+        try {
+            List<MapReduceOper> roots = new LinkedList<MapReduceOper>();
+            roots.addAll(plan.getRoots());
+            for (MapReduceOper mro: roots) {
+                if(mro instanceof NativeMapReduceOper) {
+                    return null;
+                }
+                Job job = getJob(plan, mro, conf, pigContext);
+                jobMroMap.put(job, mro);
+                jobCtrl.addJob(job);
+            }
+        } catch (JobCreationException jce) {
+        	throw jce;
+        } catch(Exception e) {
+            int errCode = 2017;
+            String msg = "Internal error creating job configuration.";
+            throw new JobCreationException(msg, errCode, PigException.BUG, e);
+        }
+
+        return jobCtrl;
+    }
+
+    // Update Map-Reduce plan with the execution status of the jobs. If one job
+    // completely fail (the job has only one store and that job fail), then we
+    // remove all its dependent jobs. This method will return the number of MapReduceOper
+    // removed from the Map-Reduce plan
+    public int updateMROpPlan(List<Job> completeFailedJobs)
+    {
+        int sizeBefore = plan.size();
+        for (Job job : completeFailedJobs)  // remove all subsequent jobs
+        {
+            MapReduceOper mrOper = jobMroMap.get(job);
+            plan.trimBelow(mrOper);
+            plan.remove(mrOper);
+        }
+
+        // Remove successful jobs from jobMroMap
+        for (Job job : jobMroMap.keySet())
+        {
+            if (!completeFailedJobs.contains(job))
+            {
+                MapReduceOper mro = jobMroMap.get(job);
+                plan.remove(mro);
+            }
+        }
+        jobMroMap.clear();
+        int sizeAfter = plan.size();
+        return sizeBefore-sizeAfter;
+    }
+
+    /**
+     * The method that creates the Job corresponding to a MapReduceOper.
+     * The assumption is that
+     * every MapReduceOper will have a load and a store. The JobConf removes
+     * the load operator and serializes the input filespec so that PigInputFormat can
+     * take over the creation of splits. It also removes the store operator
+     * and serializes the output filespec so that PigOutputFormat can take over
+     * record writing. The remaining portion of the map plan and reduce plans are
+     * serialized and stored for the PigMapReduce or PigMapOnly objects to take over
+     * the actual running of the plans.
+     * The Mapper &amp; Reducer classes and the required key value formats are set.
+     * Checks if this is a map only job and uses PigMapOnly class as the mapper
+     * and uses PigMapReduce otherwise.
+     * If it is a Map Reduce job, it is bound to have a package operator. Remove it from
+     * the reduce plan and serializes it so that the PigMapReduce class can use it to package
+     * the indexed tuples received by the reducer.
+     * @param mro - The MapReduceOper for which the JobConf is required
+     * @param config - the Configuration object from which JobConf is built
+     * @param pigContext - The PigContext passed on from execution engine
+     * @return Job corresponding to mro
+     * @throws JobCreationException
+     */
+    @SuppressWarnings({ "unchecked", "deprecation" })
+    private Job getJob(MROperPlan plan, MapReduceOper mro, Configuration config, PigContext pigContext) throws JobCreationException{
+        org.apache.hadoop.mapreduce.Job nwJob = null;
+
+        try{
+            nwJob = new org.apache.hadoop.mapreduce.Job(config);
+        }catch(Exception e) {
+            throw new JobCreationException(e);
+        }
+
+        Configuration conf = nwJob.getConfiguration();
+
+        ArrayList<FileSpec> inp = new ArrayList<FileSpec>();
+        ArrayList<List<OperatorKey>> inpTargets = new ArrayList<List<OperatorKey>>();
+        ArrayList<String> inpSignatureLists = new ArrayList<String>();
+        ArrayList<Long> inpLimits = new ArrayList<Long>();
+        ArrayList<POStore> storeLocations = new ArrayList<POStore>();
+        Path tmpLocation = null;
+
+        // add settings for pig statistics
+        String setScriptProp = conf.get(ScriptState.INSERT_ENABLED, "true");
+        if (setScriptProp.equalsIgnoreCase("true")) {
+            ScriptState ss = ScriptState.get();
+            ss.addSettingsToConf(mro, conf);
+        }
+
+
+        conf.set("mapred.mapper.new-api", "true");
+        conf.set("mapred.reducer.new-api", "true");
+
+        String buffPercent = conf.get("mapred.job.reduce.markreset.buffer.percent");
+        if (buffPercent == null || Double.parseDouble(buffPercent) <= 0) {
+            log.info("mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3");
+            conf.set("mapred.job.reduce.markreset.buffer.percent", "0.3");
+        }else{
+            log.info("mapred.job.reduce.markreset.buffer.percent is set to " + conf.get("mapred.job.reduce.markreset.buffer.percent"));
+        }
+
+        // Convert mapred.output.* to output.compression.*, See PIG-1791
+        if( "true".equals( conf.get( "mapred.output.compress" ) ) ) {
+            conf.set( "output.compression.enabled",  "true" );
+            String codec = conf.get( "mapred.output.compression.codec" );
+            if( codec == null ) {
+                throw new JobCreationException("'mapred.output.compress' is set but no value is specified for 'mapred.output.compression.codec'." );
+            } else {
+                conf.set( "output.compression.codec", codec );
+            }
+        }
+
+        try{
+
+            //Process the POLoads
+            List<POLoad> lds = PlanHelper.getPhysicalOperators(mro.mapPlan, POLoad.class);
+
+            if(lds!=null && lds.size()>0){
+                for (POLoad ld : lds) {
+                    LoadFunc lf = ld.getLoadFunc();
+                    lf.setLocation(ld.getLFile().getFileName(), nwJob);
+
+                    //Store the inp filespecs
+                    inp.add(ld.getLFile());
+                }
+            }
+
+            adjustNumReducers(plan, mro, nwJob);
+
+            if(lds!=null && lds.size()>0){
+              for (POLoad ld : lds) {
+                    //Store the target operators for tuples read
+                    //from this input
+                    List<PhysicalOperator> ldSucs = mro.mapPlan.getSuccessors(ld);
+                    List<OperatorKey> ldSucKeys = new ArrayList<OperatorKey>();
+                    if(ldSucs!=null){
+                        for (PhysicalOperator operator2 : ldSucs) {
+                            ldSucKeys.add(operator2.getOperatorKey());
+                        }
+                    }
+                    inpTargets.add(ldSucKeys);
+                    inpSignatureLists.add(ld.getSignature());
+                    inpLimits.add(ld.getLimit());
+                    //Remove the POLoad from the plan
+                    if (!pigContext.inIllustrator)
+                        mro.mapPlan.remove(ld);
+                }
+            }
+
+            if (!pigContext.inIllustrator && pigContext.getExecType() != ExecType.LOCAL)
+            {
+
+                // Setup the DistributedCache for this job
+                for (URL extraJar : pigContext.extraJars) {
+                    log.debug("Adding jar to DistributedCache: " + extraJar.toString());
+                    putJarOnClassPathThroughDistributedCache(pigContext, conf, extraJar);
+                }
+
+                //Create the jar of all functions and classes required
+                File submitJarFile = File.createTempFile("Job", ".jar");
+                log.info("creating jar file "+submitJarFile.getName());
+                // ensure the job jar is deleted on exit
+                submitJarFile.deleteOnExit();
+                FileOutputStream fos = new FileOutputStream(submitJarFile);
+                JarManager.createJar(fos, mro.UDFs, pigContext);
+                log.info("jar file "+submitJarFile.getName()+" created");
+                //Start setting the JobConf properties
+                conf.set("mapred.jar", submitJarFile.getPath());
+            }
+            conf.set("pig.inputs", ObjectSerializer.serialize(inp));
+            conf.set("pig.inpTargets", ObjectSerializer.serialize(inpTargets));
+            conf.set("pig.inpSignatures", ObjectSerializer.serialize(inpSignatureLists));
+            conf.set("pig.inpLimits", ObjectSerializer.serialize(inpLimits));
+            conf.set("pig.pigContext", ObjectSerializer.serialize(pigContext));
+            conf.set("udf.import.list", ObjectSerializer.serialize(PigContext.getPackageImportList()));
+            // this is for unit tests since some don't create PigServer
+
+            // if user specified the job name using -D switch, Pig won't reset the name then.
+            if (System.getProperty("mapred.job.name") == null &&
+                    pigContext.getProperties().getProperty(PigContext.JOB_NAME) != null){
+                nwJob.setJobName(pigContext.getProperties().getProperty(PigContext.JOB_NAME));
+            }
+
+            if (pigContext.getProperties().getProperty(PigContext.JOB_PRIORITY) != null) {
+                // If the job priority was set, attempt to get the corresponding enum value
+                // and set the hadoop job priority.
+                String jobPriority = pigContext.getProperties().getProperty(PigContext.JOB_PRIORITY).toUpperCase();
+                try {
+                  // Allow arbitrary case; the Hadoop job priorities are all upper case.
+                  conf.set("mapred.job.priority", JobPriority.valueOf(jobPriority).toString());
+
+                } catch (IllegalArgumentException e) {
+                  StringBuffer sb = new StringBuffer("The job priority must be one of [");
+                  JobPriority[] priorities = JobPriority.values();
+                  for (int i = 0; i < priorities.length; ++i) {
+                    if (i > 0)  sb.append(", ");
+                    sb.append(priorities[i]);
+                  }
+                  sb.append("].  You specified [" + jobPriority + "]");
+                  throw new JobCreationException(sb.toString());
+                }
+            }
+
+            setupDistributedCache(pigContext, nwJob.getConfiguration(), pigContext.getProperties(),
+                                  "pig.streaming.ship.files", true);
+            setupDistributedCache(pigContext, nwJob.getConfiguration(), pigContext.getProperties(),
+                                  "pig.streaming.cache.files", false);
+
+            nwJob.setInputFormatClass(PigInputFormat.class);
+
+            //Process POStore and remove it from the plan
+            LinkedList<POStore> mapStores = PlanHelper.getPhysicalOperators(mro.mapPlan, POStore.class);
+            LinkedList<POStore> reduceStores = PlanHelper.getPhysicalOperators(mro.reducePlan, POStore.class);
+
+            for (POStore st: mapStores) {
+                storeLocations.add(st);
+                StoreFuncInterface sFunc = st.getStoreFunc();
+                sFunc.setStoreLocation(st.getSFile().getFileName(), new org.apache.hadoop.mapreduce.Job(nwJob.getConfiguration()));
+            }
+
+            for (POStore st: reduceStores) {
+                storeLocations.add(st);
+                StoreFuncInterface sFunc = st.getStoreFunc();
+                sFunc.setStoreLocation(st.getSFile().getFileName(), new org.apache.hadoop.mapreduce.Job(nwJob.getConfiguration()));
+            }
+
+            // the OutputFormat we report to Hadoop is always PigOutputFormat
+            nwJob.setOutputFormatClass(PigOutputFormat.class);
+
+            if (mapStores.size() + reduceStores.size() == 1) { // single store case
+                log.info("Setting up single store job");
+
+                POStore st;
+                if (reduceStores.isEmpty()) {
+                    st = mapStores.get(0);
+                    if(!pigContext.inIllustrator)
+                        mro.mapPlan.remove(st);
+                }
+                else {
+                    st = reduceStores.get(0);
+                    if(!pigContext.inIllustrator)
+                        mro.reducePlan.remove(st);
+                }
+
+                // set out filespecs
+                String outputPathString = st.getSFile().getFileName();
+                if (!outputPathString.contains("://") || outputPathString.startsWith("hdfs://")) {
+                    conf.set("pig.streaming.log.dir",
+                            new Path(outputPathString, LOG_DIR).toString());
+                } else {
+                    String tmpLocationStr =  FileLocalizer
+                    .getTemporaryPath(pigContext).toString();
+                    tmpLocation = new Path(tmpLocationStr);
+                    conf.set("pig.streaming.log.dir",
+                            new Path(tmpLocation, LOG_DIR).toString());
+                }
+                conf.set("pig.streaming.task.output.dir", outputPathString);
+            }
+           else if (mapStores.size() + reduceStores.size() > 0) { // multi store case
+                log.info("Setting up multi store job");
+                String tmpLocationStr =  FileLocalizer
+                .getTemporaryPath(pigContext).toString();
+                tmpLocation = new Path(tmpLocationStr);
+
+                nwJob.setOutputFormatClass(PigOutputFormat.class);
+
+                boolean disableCounter = conf.getBoolean("pig.disable.counter", false);
+                if (disableCounter) {
+                    log.info("Disable Pig custom output counters");
+                }
+                int idx = 0;
+                for (POStore sto: storeLocations) {
+                    sto.setDisableCounter(disableCounter);
+                    sto.setMultiStore(true);
+                    sto.setIndex(idx++);
+                }
+
+                conf.set("pig.streaming.log.dir",
+                            new Path(tmpLocation, LOG_DIR).toString());
+                conf.set("pig.streaming.task.output.dir", tmpLocation.toString());
+           }
+
+            // store map key type
+            // this is needed when the key is null to create
+            // an appropriate NullableXXXWritable object
+            conf.set("pig.map.keytype", ObjectSerializer.serialize(new byte[] { mro.mapKeyType }));
+
+            // set parent plan in all operators in map and reduce plans
+            // currently the parent plan is really used only when POStream is present in the plan
+            new PhyPlanSetter(mro.mapPlan).visit();
+            new PhyPlanSetter(mro.reducePlan).visit();
+
+            // this call modifies the ReplFiles names of POFRJoin operators
+            // within the MR plans, must be called before the plans are
+            // serialized
+            setupDistributedCacheForJoin(mro, pigContext, conf);
+
+            // Search to see if we have any UDFs that need to pack things into the
+            // distrubted cache.
+            setupDistributedCacheForUdfs(mro, pigContext, conf);
+
+            SchemaTupleFrontend.copyAllGeneratedToDistributedCache(pigContext, conf);
+
+            POPackage pack = null;
+            if(mro.reducePlan.isEmpty()){
+                //MapOnly Job
+                nwJob.setMapperClass(PigMapOnly.Map.class);
+                nwJob.setNumReduceTasks(0);
+                if(!pigContext.inIllustrator)
+                    conf.set("pig.mapPlan", ObjectSerializer.serialize(mro.mapPlan));
+                if(mro.isEndOfAllInputSetInMap()) {
+                    // this is used in Map.close() to decide whether the
+                    // pipeline needs to be rerun one more time in the close()
+                    // The pipeline is rerun if there either was a stream or POMergeJoin
+                    conf.set(END_OF_INP_IN_MAP, "true");
+                }
+            }
+            else{
+                //Map Reduce Job
+                //Process the POPackage operator and remove it from the reduce plan
+                if(!mro.combinePlan.isEmpty()){
+                    POPackage combPack = (POPackage)mro.combinePlan.getRoots().get(0);
+                    mro.combinePlan.remove(combPack);
+                    nwJob.setCombinerClass(PigCombiner.Combine.class);
+                    conf.set("pig.combinePlan", ObjectSerializer.serialize(mro.combinePlan));
+                    conf.set("pig.combine.package", ObjectSerializer.serialize(combPack));
+                } else if (mro.needsDistinctCombiner()) {
+                    nwJob.setCombinerClass(DistinctCombiner.Combine.class);
+                    log.info("Setting identity combiner class.");
+                }
+                pack = (POPackage)mro.reducePlan.getRoots().get(0);
+                if(!pigContext.inIllustrator)
+                    mro.reducePlan.remove(pack);
+                nwJob.setMapperClass(PigMapReduce.Map.class);
+                nwJob.setReducerClass(PigMapReduce.Reduce.class);
+
+                if (mro.customPartitioner != null)
+                	nwJob.setPartitionerClass(PigContext.resolveClassName(mro.customPartitioner));
+
+                if(!pigContext.inIllustrator)
+                    conf.set("pig.mapPlan", ObjectSerializer.serialize(mro.mapPlan));
+                if(mro.isEndOfAllInputSetInMap()) {
+                    // this is used in Map.close() to decide whether the
+                    // pipeline needs to be rerun one more time in the close()
+                    // The pipeline is rerun only if there was a stream or merge-join.
+                    conf.set(END_OF_INP_IN_MAP, "true");
+                }
+                if(!pigContext.inIllustrator)
+                    conf.set("pig.reducePlan", ObjectSerializer.serialize(mro.reducePlan));
+                if(mro.isEndOfAllInputSetInReduce()) {
+                    // this is used in Map.close() to decide whether the
+                    // pipeline needs to be rerun one more time in the close()
+                    // The pipeline is rerun only if there was a stream
+                    conf.set("pig.stream.in.reduce", "true");
+                }
+                if (!pigContext.inIllustrator)
+                    conf.set("pig.reduce.package", ObjectSerializer.serialize(pack));
+                conf.set("pig.reduce.key.type", Byte.toString(pack.getKeyType()));
+
+                if (mro.getUseSecondaryKey()) {
+                    nwJob.setGroupingComparatorClass(PigSecondaryKeyGroupComparator.class);
+                    nwJob.setPartitionerClass(SecondaryKeyPartitioner.class);
+                    nwJob.setSortComparatorClass(PigSecondaryKeyComparator.class);
+                    nwJob.setOutputKeyClass(NullableTuple.class);
+                    conf.set("pig.secondarySortOrder",
+                            ObjectSerializer.serialize(mro.getSecondarySortOrder()));
+
+                }
+                else
+                {
+                    Class<? extends WritableComparable> keyClass = HDataType.getWritableComparableTypes(pack.getKeyType()).getClass();
+                    nwJob.setOutputKeyClass(keyClass);
+                    selectComparator(mro, pack.getKeyType(), nwJob);
+                }
+                nwJob.setOutputValueClass(NullableTuple.class);
+            }
+
+            if(mro.isGlobalSort() || mro.isLimitAfterSort()){
+                // Only set the quantiles file and sort partitioner if we're a
+                // global sort, not for limit after sort.
+                if (mro.isGlobalSort()) {
+                    String symlink = addSingleFileToDistributedCache(
+                            pigContext, conf, mro.getQuantFile(), "pigsample");
+                    conf.set("pig.quantilesFile", symlink);
+                    nwJob.setPartitionerClass(WeightedRangePartitioner.class);
+                }
+
+                if (mro.isUDFComparatorUsed) {
+                    boolean usercomparator = false;
+                    for (String compFuncSpec : mro.UDFs) {
+                        Class comparator = PigContext.resolveClassName(compFuncSpec);
+                        if(ComparisonFunc.class.isAssignableFrom(comparator)) {
+                            nwJob.setMapperClass(PigMapReduce.MapWithComparator.class);
+                            nwJob.setReducerClass(PigMapReduce.ReduceWithComparator.class);
+                            conf.set("pig.reduce.package", ObjectSerializer.serialize(pack));
+                            conf.set("pig.usercomparator", "true");
+                            nwJob.setOutputKeyClass(NullableTuple.class);
+                            nwJob.setSortComparatorClass(comparator);
+                            usercomparator = true;
+                            break;
+                        }
+                    }
+                    if (!usercomparator) {
+                        String msg = "Internal error. Can't find the UDF comparator";
+                        throw new IOException (msg);
+                    }
+
+                } else {
+                    conf.set("pig.sortOrder",
+                        ObjectSerializer.serialize(mro.getSortOrder()));
+                }
+            }
+
+            if (mro.isSkewedJoin()) {
+                String symlink = addSingleFileToDistributedCache(pigContext,
+                        conf, mro.getSkewedJoinPartitionFile(), "pigdistkey");
+                conf.set("pig.keyDistFile", symlink);
+                nwJob.setPartitionerClass(SkewedPartitioner.class);
+                nwJob.setMapperClass(PigMapReduce.MapWithPartitionIndex.class);
+                nwJob.setMapOutputKeyClass(NullablePartitionWritable.class);
+                nwJob.setGroupingComparatorClass(PigGroupingPartitionWritableComparator.class);
+            }
+
+            if (!pigContext.inIllustrator)
+            {
+                // unset inputs for POStore, otherwise, map/reduce plan will be unnecessarily deserialized
+                for (POStore st: mapStores) { st.setInputs(null); st.setParentPlan(null);}
+                for (POStore st: reduceStores) { st.setInputs(null); st.setParentPlan(null);}
+                conf.set(PIG_MAP_STORES, ObjectSerializer.serialize(mapStores));
+                conf.set(PIG_REDUCE_STORES, ObjectSerializer.serialize(reduceStores));
+            }
+
+            // tmp file compression setups
+            if (Utils.tmpFileCompression(pigContext)) {
+                conf.setBoolean("pig.tmpfilecompression", true);
+                conf.set("pig.tmpfilecompression.codec", Utils.tmpFileCompressionCodec(pigContext));
+            }
+
+            String tmp;
+            long maxCombinedSplitSize = 0;
+            if (!mro.combineSmallSplits() || pigContext.getProperties().getProperty("pig.splitCombination", "true").equals("false"))
+                conf.setBoolean("pig.noSplitCombination", true);
+            else if ((tmp = pigContext.getProperties().getProperty("pig.maxCombinedSplitSize", null)) != null) {
+                try {
+                    maxCombinedSplitSize = Long.parseLong(tmp);
+                } catch (NumberFormatException e) {
+                    log.warn("Invalid numeric format for pig.maxCombinedSplitSize; use the default maximum combined split size");
+                }
+            }
+            if (maxCombinedSplitSize > 0)
+                conf.setLong("pig.maxCombinedSplitSize", maxCombinedSplitSize);
+
+            // It's a hack to set distributed cache file for hadoop 23. Once MiniMRCluster do not require local
+            // jar on fixed location, this can be removed
+            if (pigContext.getExecType() == ExecType.MAPREDUCE) {
+                String newfiles = conf.get("alternative.mapreduce.job.cache.files");
+                if (newfiles!=null) {
+                    String files = conf.get("mapreduce.job.cache.files");
+                    conf.set("mapreduce.job.cache.files",
+                        files == null ? newfiles.toString() : files + "," + newfiles);
+                }
+            }
+            // Serialize the UDF specific context info.
+            UDFContext.getUDFContext().serialize(conf);
+            Job cjob = new Job(new JobConf(nwJob.getConfiguration()), new ArrayList());
+            jobStoreMap.put(cjob,new Pair<List<POStore>, Path>(storeLocations, tmpLocation));
+            return cjob;
+
+        } catch (JobCreationException jce) {
+            throw jce;
+        } catch(Exception e) {
+            int errCode = 2017;
+            String msg = "Internal error creating job configuration.";
+            throw new JobCreationException(msg, errCode, PigException.BUG, e);
+        }
+    }
+
+    /**
+     * Adjust the number of reducers based on the default_parallel, requested parallel and estimated
+     * parallel. For sampler jobs, we also adjust the next job in advance to get its runtime parallel as
+     * the number of partitions used in the sampler.
+     * @param plan the MR plan
+     * @param mro the MR operator
+     * @param nwJob the current job
+     * @throws IOException
+     */
+    public void adjustNumReducers(MROperPlan plan, MapReduceOper mro,
+            org.apache.hadoop.mapreduce.Job nwJob) throws IOException {
+        int jobParallelism = calculateRuntimeReducers(mro, nwJob);
+
+        if (mro.isSampler()) {
+            // We need to calculate the final number of reducers of the next job (order-by or skew-join)
+            // to generate the quantfile.
+            MapReduceOper nextMro = plan.getSuccessors(mro).get(0);
+
+            // Here we use the same conf and Job to calculate the runtime #reducers of the next job
+            // which is fine as the statistics comes from the nextMro's POLoads
+            int nPartitions = calculateRuntimeReducers(nextMro, nwJob);
+
+            // set the runtime #reducer of the next job as the #partition
+            ParallelConstantVisitor visitor =
+              new ParallelConstantVisitor(mro.reducePlan, nPartitions);
+            visitor.visit();
+        }
+        log.info("Setting Parallelism to " + jobParallelism);
+
+        Configuration conf = nwJob.getConfiguration();
+
+        // set various parallelism into the job conf for later analysis, PIG-2779
+        conf.setInt("pig.info.reducers.default.parallel", pigContext.defaultParallel);
+        conf.setInt("pig.info.reducers.requested.parallel", mro.requestedParallelism);
+        conf.setInt("pig.info.reducers.estimated.parallel", mro.estimatedParallelism);
+
+        // this is for backward compatibility, and we encourage to use runtimeParallelism at runtime
+        mro.requestedParallelism = jobParallelism;
+
+        // finally set the number of reducers
+        conf.setInt("mapred.reduce.tasks", jobParallelism);
+    }
+
+    /**
+     * Calculate the runtime #reducers based on the default_parallel, requested parallel and estimated
+     * parallel, and save it to MapReduceOper's runtimeParallelism.
+     * @return the runtimeParallelism
+     * @throws IOException
+     */
+    private int calculateRuntimeReducers(MapReduceOper mro,
+            org.apache.hadoop.mapreduce.Job nwJob) throws IOException{
+        // we don't recalculate for the same job
+        if (mro.runtimeParallelism != -1) {
+            return mro.runtimeParallelism;
+        }
+
+        int jobParallelism = -1;
+
+        if (mro.requestedParallelism > 0) {
+            jobParallelism = mro.requestedParallelism;
+        } else if (pigContext.defaultParallel > 0) {
+            jobParallelism = pigContext.defaultParallel;
+        } else {
+            mro.estimatedParallelism = estimateNumberOfReducers(nwJob, mro);
+            if (mro.estimatedParallelism > 0) {
+                jobParallelism = mro.estimatedParallelism;
+            } else {
+                // reducer estimation could return -1 if it couldn't estimate
+                log.info("Could not estimate number of reducers and no requested or default " +
+                         "parallelism set. Defaulting to 1 reducer.");
+                jobParallelism = 1;
+            }
+        }
+
+        // save it
+        mro.runtimeParallelism = jobParallelism;
+        return jobParallelism;
+    }
+
+    /**
+     * Looks up the estimator from REDUCER_ESTIMATOR_KEY and invokes it to find the number of
+     * reducers to use. If REDUCER_ESTIMATOR_KEY isn't set, defaults to InputSizeReducerEstimator.
+     * @param job
+     * @param mapReducerOper
+     * @throws IOException
+     */
+    public static int estimateNumberOfReducers(org.apache.hadoop.mapreduce.Job job,
+                                               MapReduceOper mapReducerOper) throws IOException {
+        Configuration conf = job.getConfiguration();
+
+        PigReducerEstimator estimator = conf.get(REDUCER_ESTIMATOR_KEY) == null ?
+          new InputSizeReducerEstimator() :
+          PigContext.instantiateObjectFromParams(conf,
+                  REDUCER_ESTIMATOR_KEY, REDUCER_ESTIMATOR_ARG_KEY, PigReducerEstimator.class);
+
+        log.info("Using reducer estimator: " + estimator.getClass().getName());
+        int numberOfReducers = estimator.estimateNumberOfReducers(job, mapReducerOper);
+        return numberOfReducers;
+    }
+
+    public static class PigSecondaryKeyGroupComparator extends WritableComparator {
+        public PigSecondaryKeyGroupComparator() {
+//            super(TupleFactory.getInstance().tupleClass(), true);
+            super(NullableTuple.class, true);
+        }
+
+        @SuppressWarnings("unchecked")
+		@Override
+        public int compare(WritableComparable a, WritableComparable b)
+        {
+            PigNullableWritable wa = (PigNullableWritable)a;
+            PigNullableWritable wb = (PigNullableWritable)b;
+            if ((wa.getIndex() & PigNullableWritable.mqFlag) != 0) { // this is a multi-query index
+                if ((wa.getIndex() & PigNullableWritable.idxSpace) < (wb.getIndex() & PigNullableWritable.idxSpace)) return -1;
+                else if ((wa.getIndex() & PigNullableWritable.idxSpace) > (wb.getIndex() & PigNullableWritable.idxSpace)) return 1;
+                // If equal, we fall through
+            }
+
+            // wa and wb are guaranteed to be not null, POLocalRearrange will create a tuple anyway even if main key and secondary key
+            // are both null; however, main key can be null, we need to check for that using the same logic we have in PigNullableWritable
+            Object valuea = null;
+            Object valueb = null;
+            try {
+                // Get the main key from compound key
+                valuea = ((Tuple)wa.getValueAsPigType()).get(0);
+                valueb = ((Tuple)wb.getValueAsPigType()).get(0);
+            } catch (ExecException e) {
+                throw new RuntimeException("Unable to access tuple field", e);
+            }
+            if (!wa.isNull() && !wb.isNull()) {
+
+                int result = DataType.compare(valuea, valueb);
+
+                // If any of the field inside tuple is null, then we do not merge keys
+                // See PIG-927
+                if (result == 0 && valuea instanceof Tuple && valueb instanceof Tuple)
+                {
+                    try {
+                        for (int i=0;i<((Tuple)valuea).size();i++)
+                            if (((Tuple)valueb).get(i)==null)
+                                return (wa.getIndex()&PigNullableWritable.idxSpace) - (wb.getIndex()&PigNullableWritable.idxSpace);
+                    } catch (ExecException e) {
+                        throw new RuntimeException("Unable to access tuple field", e);
+                    }
+                }
+                return result;
+            } else if (valuea==null && valueb==null) {
+                // If they're both null, compare the indicies
+                if ((wa.getIndex() & PigNullableWritable.idxSpace) < (wb.getIndex() & PigNullableWritable.idxSpace)) return -1;
+                else if ((wa.getIndex() & PigNullableWritable.idxSpace) > (wb.getIndex() & PigNullableWritable.idxSpace)) return 1;
+                else return 0;
+            }
+            else if (valuea==null) return -1;
+            else return 1;
+        }
+    }
+
+    public static class PigWritableComparator extends WritableComparator {
+        @SuppressWarnings("unchecked")
+        protected PigWritableComparator(Class c) {
+            super(c);
+        }
+
+        @Override
+        public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2){
+            return WritableComparator.compareBytes(b1, s1, l1, b2, s2, l2);
+        }
+    }
+
+    public static class PigBooleanWritableComparator extends PigWritableComparator {
+        public PigBooleanWritableComparator() {
+            super(NullableBooleanWritable.class);
+        }
+    }
+
+    public static class PigIntWritableComparator extends PigWritableComparator {
+        public PigIntWritableComparator() {
+            super(NullableIntWritable.class);
+        }
+    }
+
+    public static class PigLongWritableComparator extends PigWritableComparator {
+        public PigLongWritableComparator() {
+            super(NullableLongWritable.class);
+        }
+    }
+
+    public static class PigFloatWritableComparator extends PigWritableComparator {
+        public PigFloatWritableComparator() {
+            super(NullableFloatWritable.class);
+        }
+    }
+
+    public static class PigDoubleWritableComparator extends PigWritableComparator {
+        public PigDoubleWritableComparator() {
+            super(NullableDoubleWritable.class);
+        }
+    }
+
+    public static class PigCharArrayWritableComparator extends PigWritableComparator {
+        public PigCharArrayWritableComparator() {
+            super(NullableText.class);
+        }
+    }
+
+    public static class PigDBAWritableComparator extends PigWritableComparator {
+        public PigDBAWritableComparator() {
+            super(NullableBytesWritable.class);
+        }
+    }
+
+    public static class PigTupleWritableComparator extends PigWritableComparator {
+        public PigTupleWritableComparator() {
+            super(TupleFactory.getInstance().tupleClass());
+        }
+    }
+
+    public static class PigBagWritableComparator extends PigWritableComparator {
+        public PigBagWritableComparator() {
+            super(BagFactory.getInstance().newDefaultBag().getClass());
+        }
+    }
+
+    // XXX hadoop 20 new API integration: we need to explicitly set the Grouping
+    // Comparator
+    public static class PigGroupingBooleanWritableComparator extends WritableComparator {
+        public PigGroupingBooleanWritableComparator() {
+            super(NullableBooleanWritable.class, true);
+        }
+    }
+
+    public static class PigGroupingIntWritableComparator extends WritableComparator {
+        public PigGroupingIntWritableComparator() {
+            super(NullableIntWritable.class, true);
+        }
+    }
+
+    public static class PigGroupingLongWritableComparator extends WritableComparator {
+        public PigGroupingLongWritableComparator() {
+            super(NullableLongWritable.class, true);
+        }
+    }
+
+    public static class PigGroupingFloatWritableComparator extends WritableComparator {
+        public PigGroupingFloatWritableComparator() {
+            super(NullableFloatWritable.class, true);
+        }
+    }
+
+    public static class PigGroupingDoubleWritableComparator extends WritableComparator {
+        public PigGroupingDoubleWritableComparator() {
+            super(NullableDoubleWritable.class, true);
+        }
+    }
+
+    public static class PigGroupingCharArrayWritableComparator extends WritableComparator {
+        public PigGroupingCharArrayWritableComparator() {
+            super(NullableText.class, true);
+        }
+    }
+
+    public static class PigGroupingDBAWritableComparator extends WritableComparator {
+        public PigGroupingDBAWritableComparator() {
+            super(NullableBytesWritable.class, true);
+        }
+    }
+
+    public static class PigGroupingTupleWritableComparator extends WritableComparator {
+        public PigGroupingTupleWritableComparator() {
+            super(NullableTuple.class, true);
+        }
+    }
+
+    public static class PigGroupingPartitionWritableComparator extends WritableComparator {
+        public PigGroupingPartitionWritableComparator() {
+            super(NullablePartitionWritable.class, true);
+        }
+    }
+
+    public static class PigGroupingBagWritableComparator extends WritableComparator {
+        public PigGroupingBagWritableComparator() {
+            super(BagFactory.getInstance().newDefaultBag().getClass(), true);
+        }
+    }
+
+    private void selectComparator(
+            MapReduceOper mro,
+            byte keyType,
+            org.apache.hadoop.mapreduce.Job job) throws JobCreationException {
+        // If this operator is involved in an order by, use the pig specific raw
+        // comparators.  If it has a cogroup, we need to set the comparator class
+        // to the raw comparator and the grouping comparator class to pig specific
+        // raw comparators (which skip the index).  Otherwise use the hadoop provided
+        // raw comparator.
+
+        // An operator has an order by if global sort is set or if it's successor has
+        // global sort set (because in that case it's the sampling job) or if
+        // it's a limit after a sort.
+        boolean hasOrderBy = false;
+        if (mro.isGlobalSort() || mro.isLimitAfterSort() || mro.usingTypedComparator()) {
+            hasOrderBy = true;
+        } else {
+            List<MapReduceOper> succs = plan.getSuccessors(mro);
+            if (succs != null) {
+                MapReduceOper succ = succs.get(0);
+                if (succ.isGlobalSort()) hasOrderBy = true;
+            }
+        }
+        if (hasOrderBy) {
+            switch (keyType) {
+            case DataType.BOOLEAN:
+                job.setSortComparatorClass(PigBooleanRawComparator.class);
+                break;
+
+            case DataType.INTEGER:
+                job.setSortComparatorClass(PigIntRawComparator.class);
+                break;
+
+            case DataType.LONG:
+                job.setSortComparatorClass(PigLongRawComparator.class);
+                break;
+
+            case DataType.FLOAT:
+                job.setSortComparatorClass(PigFloatRawComparator.class);
+                break;
+
+            case DataType.DOUBLE:
+                job.setSortComparatorClass(PigDoubleRawComparator.class);
+                break;
+
+            case DataType.CHARARRAY:
+                job.setSortComparatorClass(PigTextRawComparator.class);
+                break;
+
+            case DataType.BYTEARRAY:
+                job.setSortComparatorClass(PigBytesRawComparator.class);
+                break;
+
+            case DataType.MAP:
+                int errCode = 1068;
+                String msg = "Using Map as key not supported.";
+                throw new JobCreationException(msg, errCode, PigException.INPUT);
+
+            case DataType.TUPLE:
+                job.setSortComparatorClass(PigTupleSortComparator.class);
+                break;
+
+            case DataType.BAG:
+                errCode = 1068;
+                msg = "Using Bag as key not supported.";
+                throw new JobCreationException(msg, errCode, PigException.INPUT);
+
+            default:
+                break;
+            }
+            return;
+        }
+
+        switch (keyType) {
+        case DataType.BOOLEAN:
+            job.setSortComparatorClass(PigBooleanWritableComparator.class);
+            job.setGroupingComparatorClass(PigGroupingBooleanWritableComparator.class);
+            break;
+
+        case DataType.INTEGER:
+            job.setSortComparatorClass(PigIntWritableComparator.class);
+            job.setGroupingComparatorClass(PigGroupingIntWritableComparator.class);
+            break;
+
+        case DataType.LONG:
+            job.setSortComparatorClass(PigLongWritableComparator.class);
+            job.setGroupingComparatorClass(PigGroupingLongWritableComparator.class);
+            break;
+
+        case DataType.FLOAT:
+            job.setSortComparatorClass(PigFloatWritableComparator.class);
+            job.setGroupingComparatorClass(PigGroupingFloatWritableComparator.class);
+            break;
+
+        case DataType.DOUBLE:
+            job.setSortComparatorClass(PigDoubleWritableComparator.class);
+            job.setGroupingComparatorClass(PigGroupingDoubleWritableComparator.class);
+            break;
+
+        case DataType.CHARARRAY:
+            job.setSortComparatorClass(PigCharArrayWritableComparator.class);
+            job.setGroupingComparatorClass(PigGroupingCharArrayWritableComparator.class);
+            break;
+
+        case DataType.BYTEARRAY:
+            job.setSortComparatorClass(PigDBAWritableComparator.class);
+            job.setGroupingComparatorClass(PigGroupingDBAWritableComparator.class);
+            break;
+
+        case DataType.MAP:
+            int errCode = 1068;
+            String msg = "Using Map as key not supported.";
+            throw new JobCreationException(msg, errCode, PigException.INPUT);
+
+        case DataType.TUPLE:
+            job.setSortComparatorClass(PigTupleWritableComparator.class);
+            job.setGroupingComparatorClass(PigGroupingTupleWritableComparator.class);
+            break;
+
+        case DataType.BAG:
+            errCode = 1068;
+            msg = "Using Bag as key not supported.";
+            throw new JobCreationException(msg, errCode, PigException.INPUT);
+
+        default:
+            errCode = 2036;
+            msg = "Unhandled key type " + DataType.findTypeName(keyType);
+            throw new JobCreationException(msg, errCode, PigException.BUG);
+        }
+    }
+
+    private void setupDistributedCacheForJoin(MapReduceOper mro,
+            PigContext pigContext, Configuration conf) throws IOException {
+
+        new JoinDistributedCacheVisitor(mro.mapPlan, pigContext, conf)
+                .visit();
+
+        new JoinDistributedCacheVisitor(mro.reducePlan, pigContext, conf)
+                .visit();
+    }
+
+    private void setupDistributedCacheForUdfs(MapReduceOper mro,
+                                              PigContext pigContext,
+                                              Configuration conf) throws IOException {
+        new UdfDistributedCacheVisitor(mro.mapPlan, pigContext, conf).visit();
+        new UdfDistributedCacheVisitor(mro.reducePlan, pigContext, conf).visit();
+    }
+
+    private static void setupDistributedCache(PigContext pigContext,
+                                              Configuration conf,
+                                              Properties properties, String key,
+                                              boolean shipToCluster)
+    throws IOException {
+        // Set up the DistributedCache for this job
+        String fileNames = properties.getProperty(key);
+
+        if (fileNames != null) {
+            String[] paths = fileNames.split(",");
+            setupDistributedCache(pigContext, conf, paths, shipToCluster);
+        }
+    }
+
+    private static void setupDistributedCache(PigContext pigContext,
+            Configuration conf, String[] paths, boolean shipToCluster) throws IOException {
+        // Turn on the symlink feature
+        DistributedCache.createSymlink(conf);
+
+        for (String path : paths) {
+            path = path.trim();
+            if (path.length() != 0) {
+                Path src = new Path(path);
+
+                // Ensure that 'src' is a valid URI
+                URI srcURI = toURI(src);
+
+                // Ship it to the cluster if necessary and add to the
+                // DistributedCache
+                if (shipToCluster) {
+                    Path dst =
+                        new Path(FileLocalizer.getTemporaryPath(pigContext).toString());
+                    FileSystem fs = dst.getFileSystem(conf);
+                    fs.copyFromLocalFile(src, dst);
+
+                    // Construct the dst#srcName uri for DistributedCache
+                    URI dstURI = null;
+                    try {
+                        dstURI = new URI(dst.toString() + "#" + src.getName());
+                    } catch (URISyntaxException ue) {
+                        byte errSrc = pigContext.getErrorSource();
+                        int errCode = 0;
+                        switch(errSrc) {
+                        case PigException.REMOTE_ENVIRONMENT:
+                            errCode = 6004;
+                            break;
+                        case PigException.USER_ENVIRONMENT:
+                            errCode = 4004;
+                            break;
+                        default:
+                            errCode = 2037;
+                            break;
+                        }
+                        String msg = "Invalid ship specification. " +
+                        "File doesn't exist: " + dst;
+                        throw new ExecException(msg, errCode, errSrc);
+                    }
+                    DistributedCache.addCacheFile(dstURI, conf);
+                } else {
+                    DistributedCache.addCacheFile(srcURI, conf);
+                }
+            }
+        }
+    }
+
+    private static String addSingleFileToDistributedCache(
+            PigContext pigContext, Configuration conf, String filename,
+            String prefix) throws IOException {
+
+        if (!pigContext.inIllustrator && !FileLocalizer.fileExists(filename, pigContext)) {
+            throw new IOException(
+                    "Internal error: skew join partition file "
+                    + filename + " does not exist");
+        }
+
+        String symlink = filename;
+
+        // XXX Hadoop currently doesn't support distributed cache in local mode.
+        // This line will be removed after the support is added by Hadoop team.
+        if (pigContext.getExecType() != ExecType.LOCAL) {
+            symlink = prefix + "_"
+                    + Integer.toString(System.identityHashCode(filename)) + "_"
+                    + Long.toString(System.currentTimeMillis());
+            filename = filename + "#" + symlink;
+            setupDistributedCache(pigContext, conf, new String[] { filename },
+                    false);
+        }
+
+        return symlink;
+    }
+
+
+    /**
+     * Ensure that 'src' is a valid URI
+     * @param src the source Path
+     * @return a URI for this path
+     * @throws ExecException
+     */
+    private static URI toURI(Path src) throws ExecException {
+        try {
+            return new URI(src.toString());
+        } catch (URISyntaxException ue) {
+            int errCode = 6003;
+            String msg = "Invalid cache specification. " +
+                    "File doesn't exist: " + src;
+            throw new ExecException(msg, errCode, PigException.USER_ENVIRONMENT);
+        }
+    }
+
+    /**
+     * if url is not in HDFS will copy the path to HDFS from local before adding to distributed cache
+     * @param pigContext the pigContext
+     * @param conf the job conf
+     * @param url the url to be added to distributed cache
+     * @return the path as seen on distributed cache
+     * @throws IOException
+     */
+    private static void putJarOnClassPathThroughDistributedCache(
+            PigContext pigContext,
+            Configuration conf,
+            URL url) throws IOException {
+
+        // Turn on the symlink feature
+        DistributedCache.createSymlink(conf);
+
+        // REGISTER always copies locally the jar file. see PigServer.registerJar()
+        Path pathInHDFS = shipToHDFS(pigContext, conf, url);
+        // and add to the DistributedCache
+        DistributedCache.addFileToClassPath(pathInHDFS, conf);
+        pigContext.skipJars.add(url.getPath());
+    }
+
+    /**
+     * copy the file to hdfs in a temporary path
+     * @param pigContext the pig context
+     * @param conf the job conf
+     * @param url the url to ship to hdfs
+     * @return the location where it was shipped
+     * @throws IOException
+     */
+    private static Path shipToHDFS(
+            PigContext pigContext,
+            Configuration conf,
+            URL url) throws IOException {
+
+        String path = url.getPath();
+        int slash = path.lastIndexOf("/");
+        String suffix = slash == -1 ? path : path.substring(slash+1);
+
+        Path dst = new Path(FileLocalizer.getTemporaryPath(pigContext).toUri().getPath(), suffix);
+        FileSystem fs = dst.getFileSystem(conf);
+        OutputStream os = fs.create(dst);
+        try {
+            IOUtils.copyBytes(url.openStream(), os, 4096, true);
+        } finally {
+            // IOUtils can not close both the input and the output properly in a finally
+            // as we can get an exception in between opening the stream and calling the method
+            os.close();
+        }
+        return dst;
+    }
+
+
+    private static class JoinDistributedCacheVisitor extends PhyPlanVisitor {
+
+        private PigContext pigContext = null;
+
+        private Configuration conf = null;
+
+        public JoinDistributedCacheVisitor(PhysicalPlan plan,
+                PigContext pigContext, Configuration conf) {
+            super(plan, new DepthFirstWalker<PhysicalOperator, PhysicalPlan>(
+                    plan));
+            this.pigContext = pigContext;
+            this.conf = conf;
+        }
+
+        @Override
+        public void visitFRJoin(POFRJoin join) throws VisitorException {
+
+            // XXX Hadoop currently doesn't support distributed cache in local mode.
+            // This line will be removed after the support is added
+            if (pigContext.getExecType() == ExecType.LOCAL) return;
+
+            // set up distributed cache for the replicated files
+            FileSpec[] replFiles = join.getReplFiles();
+            ArrayList<String> replicatedPath = new ArrayList<String>();
+
+            FileSpec[] newReplFiles = new FileSpec[replFiles.length];
+
+            // the first input is not replicated
+            for (int i = 0; i < replFiles.length; i++) {
+                // ignore fragmented file
+                String symlink = "";
+                if (i != join.getFragment()) {
+                    symlink = "pigrepl_" + join.getOperatorKey().toString() + "_"
+                            + Integer.toString(System.identityHashCode(replFiles[i].getFileName()))
+                            + "_" + Long.toString(System.currentTimeMillis())
+                            + "_" + i;
+                    replicatedPath.add(replFiles[i].getFileName() + "#"
+                            + symlink);
+                }
+                newReplFiles[i] = new FileSpec(symlink,
+                        (replFiles[i] == null ? null : replFiles[i].getFuncSpec()));
+            }
+
+            join.setReplFiles(newReplFiles);
+
+            try {
+                setupDistributedCache(pigContext, conf, replicatedPath
+                        .toArray(new String[0]), false);
+            } catch (IOException e) {
+                String msg = "Internal error. Distributed cache could not " +
+                        "be set up for the replicated files";
+                throw new VisitorException(msg, e);
+            }
+        }
+
+        @Override
+        public void visitMergeJoin(POMergeJoin join) throws VisitorException {
+
+            // XXX Hadoop currently doesn't support distributed cache in local mode.
+            // This line will be removed after the support is added
+            if (pigContext.getExecType() == ExecType.LOCAL) return;
+
+            String indexFile = join.getIndexFile();
+
+            // merge join may not use an index file
+            if (indexFile == null) return;
+
+            try {
+                String symlink = addSingleFileToDistributedCache(pigContext,
+                        conf, indexFile, "indexfile_");
+                join.setIndexFile(symlink);
+            } catch (IOException e) {
+                String msg = "Internal error. Distributed cache could not " +
+                        "be set up for merge join index file";
+                throw new VisitorException(msg, e);
+            }
+        }
+
+        @Override
+        public void visitMergeCoGroup(POMergeCogroup mergeCoGrp)
+                throws VisitorException {
+
+            // XXX Hadoop currently doesn't support distributed cache in local mode.
+            // This line will be removed after the support is added
+            if (pigContext.getExecType() == ExecType.LOCAL) return;
+
+            String indexFile = mergeCoGrp.getIndexFileName();
+
+            if (indexFile == null) throw new VisitorException("No index file");
+
+            try {
+                String symlink = addSingleFileToDistributedCache(pigContext,
+                        conf, indexFile, "indexfile_mergecogrp_");
+                mergeCoGrp.setIndexFileName(symlink);
+            } catch (IOException e) {
+                String msg = "Internal error. Distributed cache could not " +
+                        "be set up for merge cogrp index file";
+                throw new VisitorException(msg, e);
+            }
+        }
+    }
+
+    private static class UdfDistributedCacheVisitor extends PhyPlanVisitor {
+
+        private PigContext pigContext = null;
+        private Configuration conf = null;
+
+        public UdfDistributedCacheVisitor(PhysicalPlan plan,
+                PigContext pigContext,
+                Configuration conf) {
+            super(plan, new DepthFirstWalker<PhysicalOperator, PhysicalPlan>(
+                    plan));
+            this.pigContext = pigContext;
+            this.conf = conf;
+        }
+
+        @Override
+        public void visitUserFunc(POUserFunc func) throws VisitorException {
+
+            // XXX Hadoop currently doesn't support distributed cache in local mode.
+            // This line will be removed after the support is added
+            if (pigContext.getExecType() == ExecType.LOCAL) return;
+
+            // set up distributed cache for files indicated by the UDF
+            String[] files = func.getCacheFiles();
+            if (files == null) return;
+
+            try {
+                setupDistributedCache(pigContext, conf, files, false);
+            } catch (IOException e) {
+                String msg = "Internal error. Distributed cache could not " +
+                        "be set up for the requested files";
+                throw new VisitorException(msg, e);
+            }
+        }
+    }
+
+    private static class ParallelConstantVisitor extends PhyPlanVisitor {
+
+        private int rp;
+
+        private boolean replaced = false;
+
+        public ParallelConstantVisitor(PhysicalPlan plan, int rp) {
+            super(plan, new DepthFirstWalker<PhysicalOperator, PhysicalPlan>(
+                    plan));
+            this.rp = rp;
+}
+
+        @Override
+        public void visitConstant(ConstantExpression cnst) throws VisitorException {
+            if (cnst.getRequestedParallelism() == -1) {
+                Object obj = cnst.getValue();
+                if (obj instanceof Integer) {
+                    if (replaced) {
+                        // sample job should have only one ConstantExpression
+                        throw new VisitorException("Invalid reduce plan: more " +
+                                       "than one ConstantExpression found in sampling job");
+                    }
+                    cnst.setValue(rp);
+                    cnst.setRequestedParallelism(rp);
+                    replaced = true;
+                }
+            }
+        }
+
+        boolean isReplaced() { return replaced; }
+    }
+
+}
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigDateTimeRawComparator.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigDateTimeRawComparator.java
new file mode 100644
index 000000000..2f2a0ffea
--- /dev/null
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigDateTimeRawComparator.java
@@ -0,0 +1,102 @@
+package org.apache.pig.backend.hadoop.executionengine.mapReduceLayer;
+
+import java.io.IOException;
+
+import org.joda.time.DateTime;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configurable;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.io.WritableComparator;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.pig.backend.hadoop.DateTimeWritable;
+import org.apache.pig.impl.io.NullableDateTimeWritable;
+import org.apache.pig.impl.util.ObjectSerializer;
+
+public class PigDateTimeRawComparator extends WritableComparator implements
+        Configurable {
+
+    private final Log mLog = LogFactory.getLog(getClass());
+    private boolean[] mAsc;
+    private DateTimeWritable.Comparator mWrappedComp;
+
+    public PigDateTimeRawComparator() {
+        super(NullableDateTimeWritable.class);
+        mWrappedComp = new DateTimeWritable.Comparator();
+    }
+
+    public void setConf(Configuration conf) {
+        if (!(conf instanceof JobConf)) {
+            mLog.warn("Expected jobconf in setConf, got "
+                    + conf.getClass().getName());
+            return;
+        }
+        JobConf jconf = (JobConf) conf;
+        try {
+            mAsc = (boolean[]) ObjectSerializer.deserialize(jconf
+                    .get("pig.sortOrder"));
+        } catch (IOException ioe) {
+            mLog.error("Unable to deserialize pig.sortOrder "
+                    + ioe.getMessage());
+            throw new RuntimeException(ioe);
+        }
+        if (mAsc == null) {
+            mAsc = new boolean[1];
+            mAsc[0] = true;
+        }
+    }
+
+    public Configuration getConf() {
+        return null;
+    }
+
+    /**
+     * Compare two NullableIntWritables as raw bytes. If neither are null, then
+     * IntWritable.compare() is used. If both are null then the indices are
+     * compared. Otherwise the null one is defined to be less.
+     */
+    public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
+        int rc = 0;
+
+        // If either are null, handle differently.
+        if (b1[s1] == 0 && b2[s2] == 0) {
+            rc = mWrappedComp.compare(b1, s1 + 1, l1 - 2, b2, s2 + 1, l2 - 2);
+        } else {
+            // For sorting purposes two nulls are equal.
+            if (b1[s1] != 0 && b2[s2] != 0)
+                rc = 0;
+            else if (b1[s1] != 0)
+                rc = -1;
+            else
+                rc = 1;
+        }
+        if (!mAsc[0])
+            rc *= -1;
+        return rc;
+    }
+
+    public int compare(Object o1, Object o2) {
+        NullableDateTimeWritable ndtw1 = (NullableDateTimeWritable) o1;
+        NullableDateTimeWritable ndtw2 = (NullableDateTimeWritable) o2;
+        int rc = 0;
+
+        // If either are null, handle differently.
+        if (!ndtw1.isNull() && !ndtw2.isNull()) {
+            rc = ((DateTime) ndtw1.getValueAsPigType())
+                    .compareTo((DateTime) ndtw2.getValueAsPigType());
+        } else {
+            // For sorting purposes two nulls are equal.
+            if (ndtw1.isNull() && ndtw2.isNull())
+                rc = 0;
+            else if (ndtw1.isNull())
+                rc = -1;
+            else
+                rc = 1;
+        }
+        if (!mAsc[0])
+            rc *= -1;
+        return rc;
+    }
+
+}
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigGenericMapBase.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigGenericMapBase.java
index ff2087bac..6ab33a4c2 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigGenericMapBase.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigGenericMapBase.java
@@ -22,6 +22,8 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 
+import org.joda.time.DateTimeZone;
+
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
@@ -211,6 +213,12 @@ public abstract class PigGenericMapBase extends Mapper<Text, Tuple, PigNullableW
         PigStatusReporter.setContext(context);
  
         log.info("Aliases being processed per job phase (AliasName[line,offset]): " + job.get("pig.alias.location"));
+        
+        String dtzStr = PigMapReduce.sJobConfInternal.get().get("pig.datetime.default.tz");
+        if (dtzStr != null && dtzStr.length() > 0) {
+            // ensure that the internal timezone is uniformly in UTC offset style
+            DateTimeZone.setDefault(DateTimeZone.forOffsetMillis(DateTimeZone.forID(dtzStr).getOffset(null)));
+        }
     }
     
     /**
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigGenericMapReduce.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigGenericMapReduce.java
index 74076bb3d..2e7046255 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigGenericMapReduce.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigGenericMapReduce.java
@@ -23,6 +23,8 @@ import java.util.ArrayList;
 import java.util.Iterator;
 import java.util.List;
 
+import org.joda.time.DateTimeZone;
+
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
@@ -345,6 +347,12 @@ public class PigGenericMapReduce {
                 throw new RuntimeException(msg, ioe);
             }
             log.info("Aliases being processed per job phase (AliasName[line,offset]): " + jConf.get("pig.alias.location"));
+            
+            String dtzStr = PigMapReduce.sJobConfInternal.get().get("pig.datetime.default.tz");
+            if (dtzStr != null && dtzStr.length() > 0) {
+                // ensure that the internal timezone is uniformly in UTC offset style
+                DateTimeZone.setDefault(DateTimeZone.forOffsetMillis(DateTimeZone.forID(dtzStr).getOffset(null)));
+            }
         }
         
         /**
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/partitioners/WeightedRangePartitioner.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/partitioners/WeightedRangePartitioner.java
index 5455e1cf1..d7504aedf 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/partitioners/WeightedRangePartitioner.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/partitioners/WeightedRangePartitioner.java
@@ -47,6 +47,7 @@ import org.apache.pig.impl.io.NullableDoubleWritable;
 import org.apache.pig.impl.io.NullableFloatWritable;
 import org.apache.pig.impl.io.NullableIntWritable;
 import org.apache.pig.impl.io.NullableLongWritable;
+import org.apache.pig.impl.io.NullableDateTimeWritable;
 import org.apache.pig.impl.io.NullableText;
 import org.apache.pig.impl.io.NullableTuple;
 import org.apache.pig.impl.io.PigNullableWritable;
@@ -211,6 +212,8 @@ public class WeightedRangePartitioner extends Partitioner<PigNullableWritable, W
             quantiles = quantilesList.toArray(new NullableIntWritable[0]);
         } else if (quantilesList.get(0).getClass().equals(NullableLongWritable.class)) {
             quantiles = quantilesList.toArray(new NullableLongWritable[0]);
+        } else if (quantilesList.get(0).getClass().equals(NullableDateTimeWritable.class)) {
+            quantiles = quantilesList.toArray(new NullableDateTimeWritable[0]);
         } else if (quantilesList.get(0).getClass().equals(NullableText.class)) {
             quantiles = quantilesList.toArray(new NullableText[0]);
         } else {
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/PhysicalOperator.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/PhysicalOperator.java
index 578ab9dd1..26d5389be 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/PhysicalOperator.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/PhysicalOperator.java
@@ -23,6 +23,8 @@ import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 
+import org.joda.time.DateTime;
+
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.pig.backend.executionengine.ExecException;
@@ -123,6 +125,8 @@ public abstract class PhysicalOperator extends Operator<PhyPlanVisitor> implemen
 
     static final protected Boolean dummyBool = null;
 
+    static final protected DateTime dummyDateTime = null;
+
     static final protected Tuple dummyTuple = null;
 
     static final protected DataBag dummyBag = null;
@@ -343,6 +347,8 @@ public abstract class PhysicalOperator extends Operator<PhyPlanVisitor> implemen
             return getNext((Integer) obj);
         case DataType.LONG:
             return getNext((Long) obj);
+        case DataType.DATETIME:
+            return getNext((DateTime) obj);
         case DataType.MAP:
             return getNext((Map) obj);
         case DataType.TUPLE:
@@ -370,6 +376,8 @@ public abstract class PhysicalOperator extends Operator<PhyPlanVisitor> implemen
             return dummyFloat;
         case DataType.LONG:
             return dummyLong;
+        case DataType.DATETIME:
+            return dummyDateTime;
         case DataType.MAP:
             return dummyMap;
         case DataType.TUPLE:
@@ -394,6 +402,10 @@ public abstract class PhysicalOperator extends Operator<PhyPlanVisitor> implemen
     public Result getNext(Float f) throws ExecException {
         return res;
     }
+    
+    public Result getNext(DateTime dt) throws ExecException {
+        return res;
+    }
 
     public Result getNext(String s) throws ExecException {
         return res;
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/ComparisonOperator.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/ComparisonOperator.java
index 1773ff9d1..51d9f34ab 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/ComparisonOperator.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/ComparisonOperator.java
@@ -19,6 +19,8 @@ package org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOp
 
 import java.util.Map;
 
+import org.joda.time.DateTime;
+
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.data.DataBag;
 import org.apache.pig.data.DataByteArray;
@@ -65,6 +67,8 @@ public interface ComparisonOperator {
 
     public Result getNext(Boolean b) throws ExecException;
 
+    public Result getNext(DateTime dt) throws ExecException;
+
     public Result getNext(Tuple t) throws ExecException;
 
     public Result getNext(DataBag db) throws ExecException;
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/ConstantExpression.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/ConstantExpression.java
index 75d64d770..db3840f84 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/ConstantExpression.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/ConstantExpression.java
@@ -20,6 +20,8 @@ package org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOp
 import java.util.List;
 import java.util.Map;
 
+import org.joda.time.DateTime;
+
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
@@ -138,6 +140,12 @@ public class ConstantExpression extends ExpressionOperator {
 
     }
 
+    @Override
+    public Result getNext(DateTime dt) throws ExecException {
+        return genericGetNext(dt, DataType.DATETIME);
+
+    }
+
     @Override
     public Result getNext(String s) throws ExecException {
         return genericGetNext(s, DataType.CHARARRAY);
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/EqualToExpr.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/EqualToExpr.java
index d3c432f4c..661ad6283 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/EqualToExpr.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/EqualToExpr.java
@@ -70,6 +70,7 @@ public class EqualToExpr extends BinaryComparisonOperator {
         case DataType.BOOLEAN:
         case DataType.INTEGER:
         case DataType.LONG:
+        case DataType.DATETIME:
         case DataType.CHARARRAY:
         case DataType.TUPLE:
         case DataType.MAP: {
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/ExpressionOperator.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/ExpressionOperator.java
index e72c56ccb..b026a8884 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/ExpressionOperator.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/ExpressionOperator.java
@@ -21,6 +21,8 @@ package org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOp
 import java.util.List;
 import java.util.Map;
 
+import org.joda.time.DateTime;
+
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.pig.backend.executionengine.ExecException;
@@ -180,6 +182,14 @@ public abstract class ExpressionOperator extends PhysicalOperator {
 
     }
 
+    /**
+     * Drive all the UDFs in accumulative mode
+     */
+    protected Result accumChild(List<ExpressionOperator> child, DateTime dt) throws ExecException {
+        return accumChild(child, dt, DataType.DATETIME);
+
+    }
+
     /**
      * Drive all the UDFs in accumulative mode
      */
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/GTOrEqualToExpr.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/GTOrEqualToExpr.java
index d79c7d585..d64a08064 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/GTOrEqualToExpr.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/GTOrEqualToExpr.java
@@ -67,6 +67,7 @@ public class GTOrEqualToExpr extends BinaryComparisonOperator {
         case DataType.FLOAT:
         case DataType.INTEGER:
         case DataType.LONG:
+        case DataType.DATETIME:
         case DataType.CHARARRAY: {
             Object dummy = getDummy(operandType);
             Result r = accumChild(null, dummy, operandType);
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/GreaterThanExpr.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/GreaterThanExpr.java
index 6ac84d734..704d0b802 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/GreaterThanExpr.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/GreaterThanExpr.java
@@ -63,6 +63,7 @@ public class GreaterThanExpr extends BinaryComparisonOperator {
         case DataType.FLOAT:
         case DataType.INTEGER:
         case DataType.LONG:
+        case DataType.DATETIME:
         case DataType.CHARARRAY: {
             Object dummy = getDummy(operandType);
             Result r = accumChild(null, dummy, operandType);
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/LTOrEqualToExpr.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/LTOrEqualToExpr.java
index 8d9d91d9e..9dc929e89 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/LTOrEqualToExpr.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/LTOrEqualToExpr.java
@@ -67,6 +67,7 @@ public class LTOrEqualToExpr extends BinaryComparisonOperator {
         case DataType.FLOAT:
         case DataType.INTEGER:
         case DataType.LONG:
+        case DataType.DATETIME:
         case DataType.CHARARRAY: {
             Object dummy = getDummy(operandType);
             Result r = accumChild(null, dummy, operandType);
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/LessThanExpr.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/LessThanExpr.java
index 1eeacc73b..03206984b 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/LessThanExpr.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/LessThanExpr.java
@@ -67,6 +67,7 @@ public class LessThanExpr extends BinaryComparisonOperator {
         case DataType.FLOAT:
         case DataType.INTEGER:
         case DataType.LONG:
+        case DataType.DATETIME:
         case DataType.CHARARRAY: {
             Object dummy = getDummy(operandType);
             Result r = accumChild(null, dummy, operandType);
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/NotEqualToExpr.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/NotEqualToExpr.java
index 5ed12c010..79a446162 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/NotEqualToExpr.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/NotEqualToExpr.java
@@ -70,6 +70,7 @@ public class NotEqualToExpr extends BinaryComparisonOperator {
         case DataType.BOOLEAN:
         case DataType.INTEGER:
         case DataType.LONG:
+        case DataType.DATETIME:
         case DataType.CHARARRAY:
         case DataType.TUPLE:
         case DataType.MAP: {
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POBinCond.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POBinCond.java
index 0e4fe1864..08544d527 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POBinCond.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POBinCond.java
@@ -21,6 +21,8 @@ import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
 
+import org.joda.time.DateTime;
+
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
@@ -125,6 +127,11 @@ public class POBinCond extends ExpressionOperator {
         return genericGetNext(l, DataType.LONG);
     }
 
+    @Override
+    public Result getNext(DateTime dt) throws ExecException {
+        return genericGetNext(dt, DataType.DATETIME);
+    }
+
     @Override
     public Result getNext(Map m) throws ExecException {
         return genericGetNext(m, DataType.MAP);
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POCast.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POCast.java
index cde340c3a..e8c2f2cfb 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POCast.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POCast.java
@@ -24,6 +24,9 @@ import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 
+import org.joda.time.DateTime;
+import org.joda.time.DateTimeZone;
+
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.pig.FuncSpec;
@@ -39,6 +42,7 @@ import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
+import org.apache.pig.builtin.ToDate;
 import org.apache.pig.data.DataBag;
 import org.apache.pig.data.DataByteArray;
 import org.apache.pig.data.DataType;
@@ -139,6 +143,12 @@ public class POCast extends ExpressionOperator {
             res.returnStatus = POStatus.STATUS_ERR;
             return res;
         }
+
+        case DataType.DATETIME: {
+            Result res = new Result();
+            res.returnStatus = POStatus.STATUS_ERR;
+            return res;
+        }
         
         case DataType.BYTEARRAY: {
             DataByteArray dba = null;
@@ -338,6 +348,15 @@ public class POCast extends ExpressionOperator {
             return res;
         }
 
+        case DataType.DATETIME: {
+            DateTime dt = null;
+            Result res = in.getNext(dt);
+            if (res.returnStatus == POStatus.STATUS_OK && res.result != null) {
+                res.result = Integer.valueOf(Long.valueOf(((DateTime) res.result).getMillis()).intValue());
+            }
+            return res;
+        }
+
         case DataType.CHARARRAY: {
             String str = null;
             Result res = in.getNext(str);
@@ -460,6 +479,15 @@ public class POCast extends ExpressionOperator {
             return res;
         }
 
+        case DataType.DATETIME: {
+            DateTime dt = null;
+            Result res = in.getNext(dt);
+            if (res.returnStatus == POStatus.STATUS_OK && res.result != null) {
+                res.result = Long.valueOf(((DateTime) res.result).getMillis());
+            }
+            return res;
+        }
+
         case DataType.CHARARRAY: {
             String str = null;
             Result res = in.getNext(str);
@@ -581,6 +609,15 @@ public class POCast extends ExpressionOperator {
             return res;
         }
 
+        case DataType.DATETIME: {
+            DateTime dt = null;
+            Result res = in.getNext(dt);
+            if (res.returnStatus == POStatus.STATUS_OK && res.result != null) {
+                res.result = new Double(Long.valueOf(((DateTime) res.result).getMillis()).doubleValue());
+            }
+            return res;
+        }
+
         case DataType.CHARARRAY: {
             String str = null;
             Result res = in.getNext(str);
@@ -704,6 +741,15 @@ public class POCast extends ExpressionOperator {
             return res;
         }
 
+        case DataType.DATETIME: {
+            DateTime dt = null;
+            Result res = in.getNext(dt);
+            if (res.returnStatus == POStatus.STATUS_OK && res.result != null) {
+                res.result = new Float(Long.valueOf(((DateTime) res.result).getMillis()).floatValue());
+            }
+            return res;
+        }
+
         case DataType.CHARARRAY: {
             String str = null;
             Result res = in.getNext(str);
@@ -720,6 +766,130 @@ public class POCast extends ExpressionOperator {
         return res;
     }
 
+    @Override
+    public Result getNext(DateTime dt) throws ExecException {
+        PhysicalOperator in = inputs.get(0);
+        Byte resultType = in.getResultType();
+        switch (resultType) {
+        case DataType.BAG: {
+            Result res = new Result();
+            res.returnStatus = POStatus.STATUS_ERR;
+            return res;
+        }
+
+        case DataType.TUPLE: {
+            Result res = new Result();
+            res.returnStatus = POStatus.STATUS_ERR;
+            return res;
+        }
+
+        case DataType.MAP: {
+            Result res = new Result();
+            res.returnStatus = POStatus.STATUS_ERR;
+            return res;
+        }
+
+        case DataType.BYTEARRAY: {
+            DataByteArray dba = null;
+            Result res = in.getNext(dba);
+            if (res.returnStatus == POStatus.STATUS_OK && res.result != null) {
+                try {
+                    dba = (DataByteArray) res.result;
+                } catch (ClassCastException e) {
+                    // res.result is not of type ByteArray. But it can be one of the types from which cast is still possible.
+                    if (realType == null) {
+                        // Find the type in first call and cache it.
+                        realType = DataType.findType(res.result);
+                    }
+                    try {
+                        res.result = DataType.toDateTime(res.result, realType);
+                    } catch (ClassCastException cce) {
+                        // Type has changed. Need to find type again and try casting it again.
+                        realType = DataType.findType(res.result);
+                        res.result = DataType.toDateTime(res.result, realType);
+                    }
+                    return res;
+                }
+                try {
+                    if (null != caster) {
+                        res.result = caster.bytesToDateTime(dba.get());
+                    } else {
+                        int errCode = 1075;
+                        String msg = "Received a bytearray from the UDF. Cannot determine how to convert the bytearray to datetime.";
+                        throw new ExecException(msg, errCode, PigException.INPUT);
+                    }
+                } catch (ExecException ee) {
+                    throw ee;
+                } catch (IOException e) {
+                    log.error("Error while casting from ByteArray to DateTime");
+                }
+            }
+            return res;
+        }
+
+        case DataType.INTEGER: {
+            Integer dummyI = null;
+            Result res = in.getNext(dummyI);
+            if (res.returnStatus == POStatus.STATUS_OK && res.result != null) {
+                res.result = new DateTime(((Integer) res.result).longValue());
+            }
+            return res;
+        }
+
+        case DataType.DOUBLE: {
+            Double d = null;
+            Result res = in.getNext(d);
+            if (res.returnStatus == POStatus.STATUS_OK && res.result != null) {
+                res.result = new DateTime(((Double) res.result).longValue());
+            }
+            return res;
+        }
+
+        case DataType.LONG: {
+            Long l = null;
+            Result res = in.getNext(l);
+            if (res.returnStatus == POStatus.STATUS_OK && res.result != null) {
+                res.result = new DateTime(((Long) res.result).longValue());
+            }
+            return res;
+        }
+
+        case DataType.FLOAT: {
+            Float f = null;
+            Result res = in.getNext(f);
+            if (res.returnStatus == POStatus.STATUS_OK && res.result != null) {
+                res.result = new DateTime(((Float) res.result).longValue());
+            }
+            return res;
+        }
+
+        case DataType.DATETIME: {
+            Result res = in.getNext(dt);
+            
+            return res;
+        }
+
+        case DataType.CHARARRAY: {
+            String str = null;
+            Result res = in.getNext(str);
+            if (res.returnStatus == POStatus.STATUS_OK && res.result != null) {
+                DateTimeZone dtz = ToDate.extractDateTimeZone((String) res.result);
+                if (dtz == null) {
+                    res.result = new DateTime((String) res.result);
+                } else {
+                    res.result = new DateTime((String) res.result, dtz);
+                }
+            }
+            return res;
+        }
+
+        }
+
+        Result res = new Result();
+        res.returnStatus = POStatus.STATUS_ERR;
+        return res;
+    }
+
     @Override
     public Result getNext(String str) throws ExecException {
         PhysicalOperator in = inputs.get(0);
@@ -832,6 +1002,15 @@ public class POCast extends ExpressionOperator {
             return res;
         }
 
+        case DataType.DATETIME: {
+            DateTime dt = null;
+            Result res = in.getNext(dt);
+            if (res.returnStatus == POStatus.STATUS_OK && res.result != null) {
+                res.result = ((DateTime) res.result).toString();
+            }
+            return res;
+        }
+
         case DataType.CHARARRAY: {
             Result res = in.getNext(str);
 
@@ -931,7 +1110,9 @@ public class POCast extends ExpressionOperator {
 
         case DataType.CHARARRAY:
 
-        case DataType.BOOLEAN: {
+        case DataType.BOOLEAN:
+
+        case DataType.DATETIME: {
             Result res = new Result();
             res.returnStatus = POStatus.STATUS_ERR;
             return res;
@@ -1109,6 +1290,9 @@ public class POCast extends ExpressionOperator {
             case DataType.FLOAT:
                 result = Integer.valueOf(((Float)obj).intValue());
                 break;
+            case DataType.DATETIME:
+                result = Integer.valueOf(Long.valueOf(((DateTime)obj).getMillis()).intValue());
+                break;
             case DataType.CHARARRAY:
                 result = CastUtils.stringToInteger((String)obj);
                 break;
@@ -1145,6 +1329,9 @@ public class POCast extends ExpressionOperator {
             case DataType.FLOAT:
                 result = new Double(((Float)obj).doubleValue());
                 break;
+            case DataType.DATETIME:
+                result = new Double(Long.valueOf(((DateTime)obj).getMillis()).doubleValue());
+                break;
             case DataType.CHARARRAY:
                 result = CastUtils.stringToDouble((String)obj);
                 break;
@@ -1181,6 +1368,9 @@ public class POCast extends ExpressionOperator {
             case DataType.FLOAT:
                 result = Long.valueOf(((Float)obj).longValue());
                 break;
+            case DataType.DATETIME:
+                result = Long.valueOf(((DateTime)obj).getMillis());
+                break;
             case DataType.CHARARRAY:
                 result = CastUtils.stringToLong((String)obj);
                 break;
@@ -1217,6 +1407,9 @@ public class POCast extends ExpressionOperator {
             case DataType.FLOAT:
                 result = obj;
                 break;
+            case DataType.DATETIME:
+                result = new Float(Long.valueOf(((DateTime)obj).getMillis()).floatValue());
+                break;
             case DataType.CHARARRAY:
                 result = CastUtils.stringToFloat((String)obj);
                 break;
@@ -1224,6 +1417,44 @@ public class POCast extends ExpressionOperator {
                 throw new ExecException("Cannot convert "+ obj + " to " + fs, 1120, PigException.INPUT);
             }
             break;
+        case DataType.DATETIME:
+            switch (DataType.findType(obj)) {
+            case DataType.BYTEARRAY:
+                if (null != caster) {
+                    result = caster.bytesToDateTime(((DataByteArray)obj).get());
+                } else {
+                    int errCode = 1075;
+                    String msg = "Received a bytearray from the UDF. Cannot determine how to convert the bytearray to datetime.";
+                    throw new ExecException(msg, errCode, PigException.INPUT);
+                }
+                break;
+            case DataType.INTEGER:
+                result = new DateTime(((Integer)obj).longValue());
+                break;
+            case DataType.DOUBLE:
+                result = new DateTime(((Double)obj).longValue());
+                break;
+            case DataType.LONG:
+                result = new DateTime(((Long)obj).longValue());
+                break;
+            case DataType.FLOAT:
+                result = new DateTime(((Float)obj).longValue());
+                break;
+            case DataType.DATETIME:
+                result = (DateTime)obj;
+                break;
+            case DataType.CHARARRAY:
+                DateTimeZone dtz = ToDate.extractDateTimeZone((String) obj);
+                if (dtz == null) {
+                    result = new DateTime((String) obj);
+                } else {
+                    result = new DateTime((String) obj, dtz);
+                }
+                break;
+            default:
+                throw new ExecException("Cannot convert "+ obj + " to " + fs, 1120, PigException.INPUT);
+            }
+            break;
         case DataType.CHARARRAY:
             switch (DataType.findType(obj)) {
             case DataType.BYTEARRAY:
@@ -1255,6 +1486,9 @@ public class POCast extends ExpressionOperator {
             case DataType.FLOAT:
                 result = ((Float) obj).toString();
                 break;
+            case DataType.DATETIME:
+                result = ((DateTime)obj).toString();
+                break;
             case DataType.CHARARRAY:
                 result = obj;
                 break;
@@ -1352,6 +1586,8 @@ public class POCast extends ExpressionOperator {
 
         case DataType.FLOAT:
 
+        case DataType.DATETIME:
+
         case DataType.CHARARRAY:
 
         case DataType.BOOLEAN: {
@@ -1457,6 +1693,8 @@ public class POCast extends ExpressionOperator {
 
         case DataType.LONG:
 
+        case DataType.DATETIME:
+
         case DataType.FLOAT:
 
         case DataType.CHARARRAY:
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POIsNull.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POIsNull.java
index 1c7aca172..f20b8399c 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POIsNull.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POIsNull.java
@@ -69,6 +69,7 @@ public class POIsNull extends UnaryComparisonOperator {
         case DataType.BOOLEAN:
         case DataType.LONG:
         case DataType.FLOAT:
+        case DataType.DATETIME:
         case DataType.MAP:
         case DataType.TUPLE:
         case DataType.BAG:
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POMapLookUp.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POMapLookUp.java
index b91d77626..fd5573f78 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POMapLookUp.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POMapLookUp.java
@@ -20,6 +20,8 @@ package org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOp
 import java.util.List;
 import java.util.Map;
 
+import org.joda.time.DateTime;
+
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
@@ -137,6 +139,11 @@ public class POMapLookUp extends ExpressionOperator {
         return getNext();
     }
 
+    @Override
+    public Result getNext(DateTime dt) throws ExecException {
+        return getNext();
+    }
+
     @Override
     public Result getNext(Map m) throws ExecException {
         return getNext();
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POProject.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POProject.java
index 826b4b2f2..5195deef6 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POProject.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POProject.java
@@ -22,6 +22,8 @@ import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 
+import org.joda.time.DateTime;
+
 import org.apache.pig.PigException;
 import org.apache.pig.PigWarning;
 import org.apache.pig.backend.executionengine.ExecException;
@@ -367,6 +369,11 @@ public class POProject extends ExpressionOperator {
         return getNext();
     }
 
+    @Override
+    public Result getNext(DateTime dt) throws ExecException {
+        return getNext();
+    }
+
     @Override
     public Result getNext(Map m) throws ExecException {
         return getNext();
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POUserComparisonFunc.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POUserComparisonFunc.java
index 3a7620531..fcaf9b00d 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POUserComparisonFunc.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POUserComparisonFunc.java
@@ -22,6 +22,8 @@ import java.io.ObjectInputStream;
 import java.util.List;
 import java.util.Map;
 
+import org.joda.time.DateTime;
+
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.pig.ComparisonFunc;
@@ -128,6 +130,11 @@ public class POUserComparisonFunc extends ExpressionOperator {
         return getNext();
     }
 
+    @Override
+    public Result getNext(DateTime dt) throws ExecException {
+        return getNext();
+    }
+
     @Override
     public Result getNext(Map m) throws ExecException {
         return getNext();
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POUserFunc.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POUserFunc.java
index 112093d0c..df1af28cd 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POUserFunc.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POUserFunc.java
@@ -27,6 +27,8 @@ import java.util.List;
 import java.util.Map;
 import java.util.Properties;
 
+import org.joda.time.DateTime;
+
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
@@ -414,6 +416,12 @@ public class POUserFunc extends ExpressionOperator {
         return getNext();
     }
 
+    @Override
+    public Result getNext(DateTime dt) throws ExecException {
+
+        return getNext();
+    }
+ 
     @Override
     public Result getNext(Map m) throws ExecException {
 
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POForEach.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POForEach.java
index 4c6c8f71f..4e6876ace 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POForEach.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POForEach.java
@@ -349,6 +349,7 @@ public class POForEach extends PhysicalOperator {
                 case DataType.DOUBLE :
                 case DataType.LONG :
                 case DataType.FLOAT :
+                case DataType.DATETIME :
                 case DataType.CHARARRAY :
                     inputData = planLeafOps[i].getNext(getDummy(resultTypes[i]), resultTypes[i]);
                     break;
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POLocalRearrange.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POLocalRearrange.java
index 00fad495c..ddb25f146 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POLocalRearrange.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POLocalRearrange.java
@@ -296,6 +296,7 @@ public class POLocalRearrange extends PhysicalOperator {
                 case DataType.FLOAT:
                 case DataType.INTEGER:
                 case DataType.LONG:
+                case DataType.DATETIME:
                 case DataType.MAP:
                 case DataType.TUPLE:
                     res = op.getNext(getDummy(op.getResultType()), op.getResultType());
@@ -326,6 +327,7 @@ public class POLocalRearrange extends PhysicalOperator {
                     case DataType.FLOAT:
                     case DataType.INTEGER:
                     case DataType.LONG:
+                    case DataType.DATETIME:
                     case DataType.MAP:
                     case DataType.TUPLE:
                         res = op.getNext(getDummy(op.getResultType()), op.getResultType());
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPartialAgg.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPartialAgg.java
index b9f47a376..615122dd3 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPartialAgg.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPartialAgg.java
@@ -469,6 +469,7 @@ public class POPartialAgg extends PhysicalOperator {
         case DataType.FLOAT:
         case DataType.INTEGER:
         case DataType.LONG:
+        case DataType.DATETIME:
         case DataType.MAP:
         case DataType.TUPLE:
             res = op.getNext(getDummy(op.getResultType()), op.getResultType());
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPreCombinerLocalRearrange.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPreCombinerLocalRearrange.java
index adf3c4511..52401ebe6 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPreCombinerLocalRearrange.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPreCombinerLocalRearrange.java
@@ -146,6 +146,7 @@ public class POPreCombinerLocalRearrange extends PhysicalOperator {
                 case DataType.FLOAT:
                 case DataType.INTEGER:
                 case DataType.LONG:
+                case DataType.DATETIME:
                 case DataType.MAP:
                 case DataType.TUPLE:
                     res = op.getNext(getDummy(op.getResultType()), op.getResultType());
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POSort.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POSort.java
index 05698bda0..702a01b66 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POSort.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POSort.java
@@ -192,6 +192,7 @@ public class POSort extends PhysicalOperator {
             case DataType.BOOLEAN:
             case DataType.INTEGER:
             case DataType.LONG:
+            case DataType.DATETIME:
             case DataType.TUPLE:
                 res = Op.getNext(getDummy(resultType), resultType);
                 break;
diff --git a/src/org/apache/pig/backend/hadoop/hbase/HBaseBinaryConverter.java b/src/org/apache/pig/backend/hadoop/hbase/HBaseBinaryConverter.java
index 50f62a591..60a589964 100644
--- a/src/org/apache/pig/backend/hadoop/hbase/HBaseBinaryConverter.java
+++ b/src/org/apache/pig/backend/hadoop/hbase/HBaseBinaryConverter.java
@@ -20,6 +20,8 @@ package org.apache.pig.backend.hadoop.hbase;
 import java.io.IOException;
 import java.util.Map;
 
+import org.joda.time.DateTime;
+
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.pig.LoadStoreCaster;
 import org.apache.pig.ResourceSchema.ResourceFieldSchema;
@@ -80,6 +82,14 @@ public class HBaseBinaryConverter implements LoadStoreCaster {
         }
     }
 
+    /**
+     * NOT IMPLEMENTED
+     */
+    @Override
+    public DateTime bytesToDateTime(byte[] b) throws IOException {
+        throw new ExecException("Can't generate a DateTime from byte[]");
+    }
+
     @Override
     public Map<String, Object> bytesToMap(byte[] b) throws IOException {
         return bytesToMap(b, null);
@@ -146,6 +156,14 @@ public class HBaseBinaryConverter implements LoadStoreCaster {
         return Bytes.toBytes(b);
     }
 
+    /**
+     * NOT IMPLEMENTED
+     */
+    @Override
+    public byte[] toBytes(DateTime dt) throws IOException {
+        throw new IOException("Can't generate bytes from DateTime");
+    }
+
     /**
      * NOT IMPLEMENTED
      */
diff --git a/src/org/apache/pig/backend/hadoop/hbase/HBaseStorage.java b/src/org/apache/pig/backend/hadoop/hbase/HBaseStorage.java
index 66d4469bf..c0613f498 100644
--- a/src/org/apache/pig/backend/hadoop/hbase/HBaseStorage.java
+++ b/src/org/apache/pig/backend/hadoop/hbase/HBaseStorage.java
@@ -30,6 +30,8 @@ import java.util.NavigableMap;
 import java.util.HashMap;
 import java.util.Properties;
 
+import org.joda.time.DateTime;
+
 import org.apache.commons.cli.CommandLine;
 import org.apache.commons.cli.CommandLineParser;
 import org.apache.commons.cli.GnuParser;
@@ -731,7 +733,8 @@ public class HBaseStorage extends LoadFunc implements StoreFuncInterface, LoadPu
         case DataType.INTEGER: return caster.toBytes((Integer) o);
         case DataType.LONG: return caster.toBytes((Long) o);
         case DataType.BOOLEAN: return caster.toBytes((Boolean) o);
-        
+        case DataType.DATETIME: return caster.toBytes((DateTime) o);
+
         // The type conversion here is unchecked. 
         // Relying on DataType.findType to do the right thing.
         case DataType.MAP: return caster.toBytes((Map<String, Object>) o);
diff --git a/src/org/apache/pig/builtin/AddDuration.java b/src/org/apache/pig/builtin/AddDuration.java
new file mode 100644
index 000000000..7874fc062
--- /dev/null
+++ b/src/org/apache/pig/builtin/AddDuration.java
@@ -0,0 +1,77 @@
+package org.apache.pig.builtin;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.pig.EvalFunc;
+import org.apache.pig.FuncSpec;
+import org.apache.pig.data.DataType;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.impl.logicalLayer.FrontendException;
+import org.apache.pig.impl.logicalLayer.schema.Schema;
+import org.joda.time.DateTime;
+import org.joda.time.Period;
+
+/**
+ * <p>AddDuration returns the result of a DateTime object plus a Duration object</p>
+ *
+ * <ul>
+ * <li>Jodatime: http://joda-time.sourceforge.net/</li>
+ * <li>ISO8601 Duration Format: http://en.wikipedia.org/wiki/ISO_8601#Durations</li>
+ * </ul>
+ * <br />
+ * <pre>
+ * Example usage:
+ *
+ * ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:datetime, dr:chararray);
+ *
+ * DESCRIBE ISOin;
+ * ISOin: {dt: datetime,dr: chararray}
+ *
+ * DUMP ISOin;
+ *
+ * (2009-01-07T01:07:01.000Z,PT1S)
+ * (2008-02-06T02:06:02.000Z,PT1M)
+ * (2007-03-05T03:05:03.000Z,P1D)
+ * ...
+ *
+ * dtadd = FOREACH ISOin GENERATE AddDuration(dt, dr) AS dt1;
+ *
+ * DESCRIBE dtadd;
+ * dtadd: {dt1: datetime}
+ *
+ * DUMP dtadd;
+ *
+ * (2009-01-07T01:07:02.000Z)
+ * (2008-02-06T02:07:02.000Z)
+ * (2007-03-06T03:05:03.000Z)
+ *
+ * </pre>
+ */
+public class AddDuration extends EvalFunc<DateTime> {
+
+    @Override
+    public DateTime exec(Tuple input) throws IOException {
+        if (input == null || input.size() < 2) {
+            return null;
+        }
+        
+        return ((DateTime) input.get(0)).plus(new Period((String) input.get(1)));
+    }
+    
+    @Override
+    public Schema outputSchema(Schema input) {
+        return new Schema(new Schema.FieldSchema(getSchemaName(this.getClass().getName().toLowerCase(), input), DataType.DATETIME));
+    }
+
+    @Override
+    public List<FuncSpec> getArgToFuncMapping() throws FrontendException {
+        List<FuncSpec> funcList = new ArrayList<FuncSpec>();
+        Schema s = new Schema();
+        s.add(new Schema.FieldSchema(null, DataType.DATETIME));
+        s.add(new Schema.FieldSchema(null, DataType.CHARARRAY));
+        funcList.add(new FuncSpec(this.getClass().getName(), s));
+        return funcList;
+    }
+}
diff --git a/src/org/apache/pig/builtin/BinStorage.java b/src/org/apache/pig/builtin/BinStorage.java
index 506c9f3ff..06120bf6e 100644
--- a/src/org/apache/pig/builtin/BinStorage.java
+++ b/src/org/apache/pig/builtin/BinStorage.java
@@ -28,6 +28,8 @@ import java.util.Iterator;
 import java.util.Map;
 import java.util.Properties;
 
+import org.joda.time.DateTime;
+
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
@@ -114,6 +116,11 @@ implements StoreFuncInterface, LoadMetadata {
             throw new ExecException(unImplementedErrorMessage, 1118);
         }
 
+        @Override
+        public DateTime bytesToDateTime(byte[] b) throws IOException {
+            throw new ExecException(unImplementedErrorMessage, 1118);
+        }
+
         @Override
         public Map<String, Object> bytesToMap(byte[] b) throws IOException {
             return bytesToMap(b, null);
@@ -262,6 +269,19 @@ implements StoreFuncInterface, LoadMetadata {
         return baos.toByteArray();
     }
 
+    public byte[] toBytes(DateTime dt) throws IOException {
+        ByteArrayOutputStream baos = new ByteArrayOutputStream();
+        DataOutputStream dos = new DataOutputStream(baos);
+        try {
+            DataReaderWriter.writeDatum(dos, dt);
+        } catch (Exception ee) {
+            int errCode = 2105;
+            String msg = "Error while converting datetime to bytes.";
+            throw new ExecException(msg, errCode, PigException.BUG, ee);
+        }
+        return baos.toByteArray();
+    }
+
     public byte[] toBytes(Map<String, Object> m) throws IOException {
         ByteArrayOutputStream baos = new ByteArrayOutputStream();
         DataOutputStream dos = new DataOutputStream(baos);
diff --git a/src/org/apache/pig/builtin/CurrentTime.java b/src/org/apache/pig/builtin/CurrentTime.java
new file mode 100644
index 000000000..9d2d5d316
--- /dev/null
+++ b/src/org/apache/pig/builtin/CurrentTime.java
@@ -0,0 +1,54 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.pig.builtin;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.joda.time.DateTime;
+
+import org.apache.pig.EvalFunc;
+import org.apache.pig.FuncSpec;
+import org.apache.pig.data.DataType;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.impl.logicalLayer.FrontendException;
+import org.apache.pig.impl.logicalLayer.schema.Schema;
+
+/**
+ * 
+ *  <p>CURRENT_TIME generates the DateTime object of the current time.</p>
+ *
+ */
+public class CurrentTime extends EvalFunc<DateTime> {
+
+    public DateTime exec(Tuple input) throws IOException {
+        return new DateTime();
+    }
+
+    @Override
+    public Schema outputSchema(Schema input) {
+        return new Schema(new Schema.FieldSchema(getSchemaName(this.getClass().getName().toLowerCase(), input), DataType.DATETIME));
+    }
+
+    @Override
+    public List<FuncSpec> getArgToFuncMapping() throws FrontendException {
+        return new ArrayList<FuncSpec>();
+    }
+    
+}
diff --git a/src/org/apache/pig/builtin/DaysBetween.java b/src/org/apache/pig/builtin/DaysBetween.java
new file mode 100644
index 000000000..40c2aaea1
--- /dev/null
+++ b/src/org/apache/pig/builtin/DaysBetween.java
@@ -0,0 +1,108 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.pig.builtin;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.pig.EvalFunc;
+import org.apache.pig.FuncSpec;
+import org.apache.pig.data.DataType;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.impl.logicalLayer.FrontendException;
+import org.apache.pig.impl.logicalLayer.schema.Schema;
+import org.joda.time.DateTime;
+import org.joda.time.Days;
+
+/**
+ * <p>DaysBetween returns the number of days between two DateTime objects</p>
+ *
+ * <ul>
+ * <li>Jodatime: http://joda-time.sourceforge.net/</li>
+ * <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li>
+ * </ul>
+ * <br />
+ * <pre>
+ * Example usage:
+ *
+ * ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (datetime, dt2:datetime);
+ *
+ * DESCRIBE ISOin;
+ * ISOin: {dt: datetime,dt2: datetime}
+ *
+ * DUMP ISOin;
+ *
+ * (2009-01-07T01:07:01.000Z,2008-02-01T00:00:00.000Z)
+ * (2008-02-06T02:06:02.000Z,2008-02-01T00:00:00.000Z)
+ * (2007-03-05T03:05:03.000Z,2008-02-01T00:00:00.000Z)
+ * ...
+ *
+ * diffs = FOREACH ISOin GENERATE YearsBetween(dt, dt2) AS years,
+ * MonthsBetween(dt, dt2) AS months,
+ * WeeksBetween(dt, dt2) AS weeks,
+ * DaysBetween(dt, dt2) AS days,
+ * HoursBetween(dt, dt2) AS hours,
+ * MinutesBetween(dt, dt2) AS mins,
+ * SecondsBetween(dt, dt2) AS secs;
+ * MilliSecondsBetween(dt, dt2) AS millis;
+ *
+ * DESCRIBE diffs;
+ * diffs: {years: long,months: long,weeks: long,days: long,hours: long,mins: long,secs: long,millis: long}
+ *
+ * DUMP diffs;
+ *
+ * (0L,11L,48L,341L,8185L,491107L,29466421L,29466421000L)
+ * (0L,0L,0L,5L,122L,7326L,439562L,439562000L)
+ * (0L,-10L,-47L,-332L,-7988L,-479334L,-28760097L,-28760097000L)
+ *
+ * </pre>
+ */
+public class DaysBetween extends EvalFunc<Long> {
+
+    @Override
+    public Long exec(Tuple input) throws IOException
+    {
+        if (input == null || input.size() < 2) {
+            return null;
+        }
+
+        DateTime startDate = (DateTime) input.get(0);
+        DateTime endDate = (DateTime) input.get(1);
+
+        // Larger date first
+        // Subtraction may overflow
+        return (startDate.getMillis() - endDate.getMillis()) / 86400000L;
+
+    }
+
+	@Override
+	public Schema outputSchema(Schema input) {
+        return new Schema(new Schema.FieldSchema(getSchemaName(this.getClass().getName().toLowerCase(), input), DataType.LONG));
+	}
+
+    @Override
+    public List<FuncSpec> getArgToFuncMapping() throws FrontendException {
+        List<FuncSpec> funcList = new ArrayList<FuncSpec>();
+        Schema s = new Schema();
+        s.add(new Schema.FieldSchema(null, DataType.DATETIME));
+        s.add(new Schema.FieldSchema(null, DataType.DATETIME));
+        funcList.add(new FuncSpec(this.getClass().getName(), s));
+        return funcList;
+    }
+}
diff --git a/src/org/apache/pig/builtin/GetDay.java b/src/org/apache/pig/builtin/GetDay.java
new file mode 100644
index 000000000..63ba6247d
--- /dev/null
+++ b/src/org/apache/pig/builtin/GetDay.java
@@ -0,0 +1,96 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.pig.builtin;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.pig.EvalFunc;
+import org.apache.pig.FuncSpec;
+import org.apache.pig.data.DataType;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.impl.logicalLayer.FrontendException;
+import org.apache.pig.impl.logicalLayer.schema.Schema;
+import org.joda.time.DateTime;
+
+/**
+ * GetDay extracts the day of a month from a DateTime object.
+ * <ul>
+ * <li>Jodatime: http://joda-time.sourceforge.net/</li>
+ * <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li>
+ * </ul>
+ * <br />
+ * <pre>
+ * Example usage:
+ * 
+ * ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:datetime);
+ *
+ * DESCRIBE ISOin;
+ * ISOin: {dt: datetime}
+ *
+ * DUMP ISOin;
+ *
+ * (2009-01-07T01:07:01.071Z)
+ * (2008-02-06T02:06:02.062Z)
+ * (2007-03-05T03:05:03.053Z)
+ * ...
+ * 
+ * truncated = FOREACH ISO in GENERATE GetYear(dt) AS year,
+ *     GetMonth(dt) as month,
+ *     GetWeek(dt) as week,
+ *     GetWeekYear(dt) as weekyear,
+ *     GetDay(dt) AS day,
+ *     GetHour(dt) AS hour,
+ *     GetMinute(dt) AS min,
+ *     GetSecond(dt) AS sec;
+ *     GetMillSecond(dt) AS milli;
+ *
+ * DESCRIBE truncated;
+ * truncated: {year: int,month: int,week: int,weekyear: int,day: int,hour: int,min: int,sec: int,milli: int}
+ *
+ * DUMP truncated;
+ * (2009,1,2,2009,7,1,7,1,71)
+ * (2008,2,6,2008,6,2,6,2,62)
+ * (2007,3,10,2007,5,3,5,3,53)
+ * </pre>
+ */
+public class GetDay extends EvalFunc<Integer> {
+    
+    @Override
+    public Integer exec(Tuple input) throws IOException {
+        if (input == null || input.size() < 1) {
+            return null;
+        }
+        
+        return ((DateTime) input.get(0)).getDayOfMonth();
+    }
+
+    @Override
+    public Schema outputSchema(Schema input) {
+        return new Schema(new Schema.FieldSchema(getSchemaName(this.getClass().getName().toLowerCase(), input), DataType.INTEGER));
+    }
+
+    @Override
+    public List<FuncSpec> getArgToFuncMapping() throws FrontendException {
+        List<FuncSpec> funcList = new ArrayList<FuncSpec>();
+        funcList.add(new FuncSpec(this.getClass().getName(), new Schema(new Schema.FieldSchema(null, DataType.DATETIME))));
+
+        return funcList;
+    }
+}
diff --git a/src/org/apache/pig/builtin/GetHour.java b/src/org/apache/pig/builtin/GetHour.java
new file mode 100644
index 000000000..9af555ed9
--- /dev/null
+++ b/src/org/apache/pig/builtin/GetHour.java
@@ -0,0 +1,96 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.pig.builtin;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.pig.EvalFunc;
+import org.apache.pig.FuncSpec;
+import org.apache.pig.data.DataType;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.impl.logicalLayer.FrontendException;
+import org.apache.pig.impl.logicalLayer.schema.Schema;
+import org.joda.time.DateTime;
+
+/**
+ * GetHour extracts the hour of a day from a DateTime object.
+ * <ul>
+ * <li>Jodatime: http://joda-time.sourceforge.net/</li>
+ * <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li>
+ * </ul>
+ * <br />
+ * <pre>
+ * Example usage:
+ * 
+ * ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:datetime);
+ *
+ * DESCRIBE ISOin;
+ * ISOin: {dt: datetime}
+ *
+ * DUMP ISOin;
+ *
+ * (2009-01-07T01:07:01.071Z)
+ * (2008-02-06T02:06:02.062Z)
+ * (2007-03-05T03:05:03.053Z)
+ * ...
+ * 
+ * truncated = FOREACH ISO in GENERATE GetYear(dt) AS year,
+ *     GetMonth(dt) as month,
+ *     GetWeek(dt) as week,
+ *     GetWeekYear(dt) as weekyear,
+ *     GetDay(dt) AS day,
+ *     GetHour(dt) AS hour,
+ *     GetMinute(dt) AS min,
+ *     GetSecond(dt) AS sec;
+ *     GetMillSecond(dt) AS milli;
+ *
+ * DESCRIBE truncated;
+ * truncated: {year: int,month: int,week: int,weekyear: int,day: int,hour: int,min: int,sec: int,milli: int}
+ *
+ * DUMP truncated;
+ * (2009,1,2,2009,7,1,7,1,71)
+ * (2008,2,6,2008,6,2,6,2,62)
+ * (2007,3,10,2007,5,3,5,3,53)
+ * </pre>
+ */
+public class GetHour extends EvalFunc<Integer> {
+
+    @Override
+    public Integer exec(Tuple input) throws IOException {
+        if (input == null || input.size() < 1) {
+            return null;
+        }        
+
+        return ((DateTime) input.get(0)).getHourOfDay();
+    }
+
+    @Override
+    public Schema outputSchema(Schema input) {
+        return new Schema(new Schema.FieldSchema(getSchemaName(this.getClass().getName().toLowerCase(), input), DataType.INTEGER));
+    }
+
+    @Override
+    public List<FuncSpec> getArgToFuncMapping() throws FrontendException {
+        List<FuncSpec> funcList = new ArrayList<FuncSpec>();
+        funcList.add(new FuncSpec(this.getClass().getName(), new Schema(new Schema.FieldSchema(null, DataType.DATETIME))));
+
+        return funcList;
+    }
+}
diff --git a/src/org/apache/pig/builtin/GetMilliSecond.java b/src/org/apache/pig/builtin/GetMilliSecond.java
new file mode 100644
index 000000000..585fc7f03
--- /dev/null
+++ b/src/org/apache/pig/builtin/GetMilliSecond.java
@@ -0,0 +1,96 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.pig.builtin;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.pig.EvalFunc;
+import org.apache.pig.FuncSpec;
+import org.apache.pig.data.DataType;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.impl.logicalLayer.FrontendException;
+import org.apache.pig.impl.logicalLayer.schema.Schema;
+import org.joda.time.DateTime;
+
+/**
+ * GetSecond extracts the millisecond of a second from a DateTime object.
+ * <ul>
+ * <li>Jodatime: http://joda-time.sourceforge.net/</li>
+ * <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li>
+ * </ul>
+ * <br />
+ * <pre>
+ * Example usage:
+ * 
+ * ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:datetime);
+ *
+ * DESCRIBE ISOin;
+ * ISOin: {dt: datetime}
+ *
+ * DUMP ISOin;
+ *
+ * (2009-01-07T01:07:01.071Z)
+ * (2008-02-06T02:06:02.062Z)
+ * (2007-03-05T03:05:03.053Z)
+ * ...
+ * 
+ * truncated = FOREACH ISO in GENERATE GetYear(dt) AS year,
+ *     GetMonth(dt) as month,
+ *     GetWeek(dt) as week,
+ *     GetWeekYear(dt) as weekyear,
+ *     GetDay(dt) AS day,
+ *     GetHour(dt) AS hour,
+ *     GetMinute(dt) AS min,
+ *     GetSecond(dt) AS sec;
+ *     GetMillSecond(dt) AS milli;
+ *
+ * DESCRIBE truncated;
+ * truncated: {year: int,month: int,week: int,weekyear: int,day: int,hour: int,min: int,sec: int,milli: int}
+ *
+ * DUMP truncated;
+ * (2009,1,2,2009,7,1,7,1,71)
+ * (2008,2,6,2008,6,2,6,2,62)
+ * (2007,3,10,2007,5,3,5,3,53)
+ * </pre>
+ */
+public class GetMilliSecond extends EvalFunc<Integer> {
+
+    @Override
+    public Integer exec(Tuple input) throws IOException {
+        if (input == null || input.size() < 1) {
+            return null;
+        }
+
+        return ((DateTime) input.get(0)).getMillisOfSecond();
+    }
+
+    @Override
+    public Schema outputSchema(Schema input) {
+        return new Schema(new Schema.FieldSchema(getSchemaName(this.getClass().getName().toLowerCase(), input), DataType.INTEGER));
+    }
+
+    @Override
+    public List<FuncSpec> getArgToFuncMapping() throws FrontendException {
+        List<FuncSpec> funcList = new ArrayList<FuncSpec>();
+        funcList.add(new FuncSpec(this.getClass().getName(), new Schema(new Schema.FieldSchema(null, DataType.DATETIME))));
+
+        return funcList;
+    }
+}
diff --git a/src/org/apache/pig/builtin/GetMinute.java b/src/org/apache/pig/builtin/GetMinute.java
new file mode 100644
index 000000000..e4d492a35
--- /dev/null
+++ b/src/org/apache/pig/builtin/GetMinute.java
@@ -0,0 +1,96 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.pig.builtin;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.pig.EvalFunc;
+import org.apache.pig.FuncSpec;
+import org.apache.pig.data.DataType;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.impl.logicalLayer.FrontendException;
+import org.apache.pig.impl.logicalLayer.schema.Schema;
+import org.joda.time.DateTime;
+
+/**
+ * GetMinute extracts the minute of an hour from a DateTime object.
+ * <ul>
+ * <li>Jodatime: http://joda-time.sourceforge.net/</li>
+ * <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li>
+ * </ul>
+ * <br />
+ * <pre>
+ * Example usage:
+ * 
+ * ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:datetime);
+ *
+ * DESCRIBE ISOin;
+ * ISOin: {dt: datetime}
+ *
+ * DUMP ISOin;
+ *
+ * (2009-01-07T01:07:01.071Z)
+ * (2008-02-06T02:06:02.062Z)
+ * (2007-03-05T03:05:03.053Z)
+ * ...
+ * 
+ * truncated = FOREACH ISO in GENERATE GetYear(dt) AS year,
+ *     GetMonth(dt) as month,
+ *     GetWeek(dt) as week,
+ *     GetWeekYear(dt) as weekyear,
+ *     GetDay(dt) AS day,
+ *     GetHour(dt) AS hour,
+ *     GetMinute(dt) AS min,
+ *     GetSecond(dt) AS sec;
+ *     GetMillSecond(dt) AS milli;
+ *
+ * DESCRIBE truncated;
+ * truncated: {year: int,month: int,week: int,weekyear: int,day: int,hour: int,min: int,sec: int,milli: int}
+ *
+ * DUMP truncated;
+ * (2009,1,2,2009,7,1,7,1,71)
+ * (2008,2,6,2008,6,2,6,2,62)
+ * (2007,3,10,2007,5,3,5,3,53)
+ * </pre>
+ */
+public class GetMinute extends EvalFunc<Integer> {
+
+    @Override
+    public Integer exec(Tuple input) throws IOException {
+        if (input == null || input.size() < 1) {
+            return null;
+        }
+
+        return ((DateTime) input.get(0)).getMinuteOfHour();
+    }
+
+    @Override
+    public Schema outputSchema(Schema input) {
+        return new Schema(new Schema.FieldSchema(getSchemaName(this.getClass().getName().toLowerCase(), input), DataType.INTEGER));
+    }
+
+    @Override
+    public List<FuncSpec> getArgToFuncMapping() throws FrontendException {
+        List<FuncSpec> funcList = new ArrayList<FuncSpec>();
+        funcList.add(new FuncSpec(this.getClass().getName(), new Schema(new Schema.FieldSchema(null, DataType.DATETIME))));
+
+        return funcList;
+    }
+}
diff --git a/src/org/apache/pig/builtin/GetMonth.java b/src/org/apache/pig/builtin/GetMonth.java
new file mode 100644
index 000000000..4d0d4f816
--- /dev/null
+++ b/src/org/apache/pig/builtin/GetMonth.java
@@ -0,0 +1,96 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.pig.builtin;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.pig.EvalFunc;
+import org.apache.pig.FuncSpec;
+import org.apache.pig.data.DataType;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.impl.logicalLayer.FrontendException;
+import org.apache.pig.impl.logicalLayer.schema.Schema;
+import org.joda.time.DateTime;
+
+/**
+ * GetMonth extracts the month of a year from a DateTime object.
+ * <ul>
+ * <li>Jodatime: http://joda-time.sourceforge.net/</li>
+ * <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li>
+ * </ul>
+ * <br />
+ * <pre>
+ * Example usage:
+ * 
+ * ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:datetime);
+ *
+ * DESCRIBE ISOin;
+ * ISOin: {dt: datetime}
+ *
+ * DUMP ISOin;
+ *
+ * (2009-01-07T01:07:01.071Z)
+ * (2008-02-06T02:06:02.062Z)
+ * (2007-03-05T03:05:03.053Z)
+ * ...
+ * 
+ * truncated = FOREACH ISO in GENERATE GetYear(dt) AS year,
+ *     GetMonth(dt) as month,
+ *     GetWeek(dt) as week,
+ *     GetWeekYear(dt) as weekyear,
+ *     GetDay(dt) AS day,
+ *     GetHour(dt) AS hour,
+ *     GetMinute(dt) AS min,
+ *     GetSecond(dt) AS sec;
+ *     GetMillSecond(dt) AS milli;
+ *
+ * DESCRIBE truncated;
+ * truncated: {year: int,month: int,week: int,weekyear: int,day: int,hour: int,min: int,sec: int,milli: int}
+ *
+ * DUMP truncated;
+ * (2009,1,2,2009,7,1,7,1,71)
+ * (2008,2,6,2008,6,2,6,2,62)
+ * (2007,3,10,2007,5,3,5,3,53)
+ * </pre>
+ */
+public class GetMonth extends EvalFunc<Integer> {
+
+    @Override
+    public Integer exec(Tuple input) throws IOException {
+        if (input == null || input.size() < 1) {
+            return null;
+        }
+
+        return ((DateTime) input.get(0)).getMonthOfYear();
+    }
+
+    @Override
+    public Schema outputSchema(Schema input) {
+        return new Schema(new Schema.FieldSchema(getSchemaName(this.getClass().getName().toLowerCase(), input), DataType.INTEGER));
+    }
+
+    @Override
+    public List<FuncSpec> getArgToFuncMapping() throws FrontendException {
+        List<FuncSpec> funcList = new ArrayList<FuncSpec>();
+        funcList.add(new FuncSpec(this.getClass().getName(), new Schema(new Schema.FieldSchema(null, DataType.DATETIME))));
+
+        return funcList;
+    }
+}
diff --git a/src/org/apache/pig/builtin/GetSecond.java b/src/org/apache/pig/builtin/GetSecond.java
new file mode 100644
index 000000000..b8fca088c
--- /dev/null
+++ b/src/org/apache/pig/builtin/GetSecond.java
@@ -0,0 +1,96 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.pig.builtin;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.pig.EvalFunc;
+import org.apache.pig.FuncSpec;
+import org.apache.pig.data.DataType;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.impl.logicalLayer.FrontendException;
+import org.apache.pig.impl.logicalLayer.schema.Schema;
+import org.joda.time.DateTime;
+
+/**
+ * GetSecond extracts the second of a minute from a DateTime object.
+ * <ul>
+ * <li>Jodatime: http://joda-time.sourceforge.net/</li>
+ * <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li>
+ * </ul>
+ * <br />
+ * <pre>
+ * Example usage:
+ * 
+ * ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:datetime);
+ *
+ * DESCRIBE ISOin;
+ * ISOin: {dt: datetime}
+ *
+ * DUMP ISOin;
+ *
+ * (2009-01-07T01:07:01.071Z)
+ * (2008-02-06T02:06:02.062Z)
+ * (2007-03-05T03:05:03.053Z)
+ * ...
+ * 
+ * truncated = FOREACH ISO in GENERATE GetYear(dt) AS year,
+ *     GetMonth(dt) as month,
+ *     GetWeek(dt) as week,
+ *     GetWeekYear(dt) as weekyear,
+ *     GetDay(dt) AS day,
+ *     GetHour(dt) AS hour,
+ *     GetMinute(dt) AS min,
+ *     GetSecond(dt) AS sec;
+ *     GetMillSecond(dt) AS milli;
+ *
+ * DESCRIBE truncated;
+ * truncated: {year: int,month: int,week: int,weekyear: int,day: int,hour: int,min: int,sec: int,milli: int}
+ *
+ * DUMP truncated;
+ * (2009,1,2,2009,7,1,7,1,71)
+ * (2008,2,6,2008,6,2,6,2,62)
+ * (2007,3,10,2007,5,3,5,3,53)
+ * </pre>
+ */
+public class GetSecond extends EvalFunc<Integer> {
+
+    @Override
+    public Integer exec(Tuple input) throws IOException {
+        if (input == null || input.size() < 1) {
+            return null;
+        }
+
+        return ((DateTime) input.get(0)).getSecondOfMinute();
+    }
+
+    @Override
+    public Schema outputSchema(Schema input) {
+        return new Schema(new Schema.FieldSchema(getSchemaName(this.getClass().getName().toLowerCase(), input), DataType.INTEGER));
+    }
+
+    @Override
+    public List<FuncSpec> getArgToFuncMapping() throws FrontendException {
+        List<FuncSpec> funcList = new ArrayList<FuncSpec>();
+        funcList.add(new FuncSpec(this.getClass().getName(), new Schema(new Schema.FieldSchema(null, DataType.DATETIME))));
+
+        return funcList;
+    }
+}
diff --git a/src/org/apache/pig/builtin/GetWeek.java b/src/org/apache/pig/builtin/GetWeek.java
new file mode 100644
index 000000000..456e56e4f
--- /dev/null
+++ b/src/org/apache/pig/builtin/GetWeek.java
@@ -0,0 +1,96 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.pig.builtin;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.pig.EvalFunc;
+import org.apache.pig.FuncSpec;
+import org.apache.pig.data.DataType;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.impl.logicalLayer.FrontendException;
+import org.apache.pig.impl.logicalLayer.schema.Schema;
+import org.joda.time.DateTime;
+
+/**
+ * GetMonth extracts the week of a week year from a DateTime object.
+ * <ul>
+ * <li>Jodatime: http://joda-time.sourceforge.net/</li>
+ * <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li>
+ * </ul>
+ * <br />
+ * <pre>
+ * Example usage:
+ * 
+ * ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:datetime);
+ *
+ * DESCRIBE ISOin;
+ * ISOin: {dt: datetime}
+ *
+ * DUMP ISOin;
+ *
+ * (2009-01-07T01:07:01.071Z)
+ * (2008-02-06T02:06:02.062Z)
+ * (2007-03-05T03:05:03.053Z)
+ * ...
+ * 
+ * truncated = FOREACH ISO in GENERATE GetYear(dt) AS year,
+ *     GetMonth(dt) as month,
+ *     GetWeek(dt) as week,
+ *     GetWeekYear(dt) as weekyear,
+ *     GetDay(dt) AS day,
+ *     GetHour(dt) AS hour,
+ *     GetMinute(dt) AS min,
+ *     GetSecond(dt) AS sec;
+ *     GetMillSecond(dt) AS milli;
+ *
+ * DESCRIBE truncated;
+ * truncated: {year: int,month: int,week: int,weekyear: int,day: int,hour: int,min: int,sec: int,milli: int}
+ *
+ * DUMP truncated;
+ * (2009,1,2,2009,7,1,7,1,71)
+ * (2008,2,6,2008,6,2,6,2,62)
+ * (2007,3,10,2007,5,3,5,3,53)
+ * </pre>
+ */
+public class GetWeek extends EvalFunc<Integer> {
+
+    @Override
+    public Integer exec(Tuple input) throws IOException {
+        if (input == null || input.size() < 1) {
+            return null;
+        }
+
+        return ((DateTime) input.get(0)).getWeekOfWeekyear();
+    }
+
+    @Override
+    public Schema outputSchema(Schema input) {
+        return new Schema(new Schema.FieldSchema(getSchemaName(this.getClass().getName().toLowerCase(), input), DataType.INTEGER));
+    }
+
+    @Override
+    public List<FuncSpec> getArgToFuncMapping() throws FrontendException {
+        List<FuncSpec> funcList = new ArrayList<FuncSpec>();
+        funcList.add(new FuncSpec(this.getClass().getName(), new Schema(new Schema.FieldSchema(null, DataType.DATETIME))));
+
+        return funcList;
+    }
+}
diff --git a/src/org/apache/pig/builtin/GetWeekYear.java b/src/org/apache/pig/builtin/GetWeekYear.java
new file mode 100644
index 000000000..571daea25
--- /dev/null
+++ b/src/org/apache/pig/builtin/GetWeekYear.java
@@ -0,0 +1,96 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.pig.builtin;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.pig.EvalFunc;
+import org.apache.pig.FuncSpec;
+import org.apache.pig.data.DataType;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.impl.logicalLayer.FrontendException;
+import org.apache.pig.impl.logicalLayer.schema.Schema;
+import org.joda.time.DateTime;
+
+/**
+ * GetMonth extracts the week year from a DateTime object.
+ * <ul>
+ * <li>Jodatime: http://joda-time.sourceforge.net/</li>
+ * <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li>
+ * </ul>
+ * <br />
+ * <pre>
+ * Example usage:
+ * 
+ * ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:datetime);
+ *
+ * DESCRIBE ISOin;
+ * ISOin: {dt: datetime}
+ *
+ * DUMP ISOin;
+ *
+ * (2009-01-07T01:07:01.071Z)
+ * (2008-02-06T02:06:02.062Z)
+ * (2007-03-05T03:05:03.053Z)
+ * ...
+ * 
+ * truncated = FOREACH ISO in GENERATE GetYear(dt) AS year,
+ *     GetMonth(dt) as month,
+ *     GetWeek(dt) as week,
+ *     GetWeekYear(dt) as weekyear,
+ *     GetDay(dt) AS day,
+ *     GetHour(dt) AS hour,
+ *     GetMinute(dt) AS min,
+ *     GetSecond(dt) AS sec;
+ *     GetMillSecond(dt) AS milli;
+ *
+ * DESCRIBE truncated;
+ * truncated: {year: int,month: int,week: int,weekyear: int,day: int,hour: int,min: int,sec: int,milli: int}
+ *
+ * DUMP truncated;
+ * (2009,1,2,2009,7,1,7,1,71)
+ * (2008,2,6,2008,6,2,6,2,62)
+ * (2007,3,10,2007,5,3,5,3,53)
+ * </pre>
+ */
+public class GetWeekYear extends EvalFunc<Integer> {
+
+    @Override
+    public Integer exec(Tuple input) throws IOException {
+        if (input == null || input.size() < 1) {
+            return null;
+        }
+
+        return ((DateTime) input.get(0)).getWeekyear();
+    }
+
+    @Override
+    public Schema outputSchema(Schema input) {
+        return new Schema(new Schema.FieldSchema(getSchemaName(this.getClass().getName().toLowerCase(), input), DataType.INTEGER));
+    }
+
+    @Override
+    public List<FuncSpec> getArgToFuncMapping() throws FrontendException {
+        List<FuncSpec> funcList = new ArrayList<FuncSpec>();
+        funcList.add(new FuncSpec(this.getClass().getName(), new Schema(new Schema.FieldSchema(null, DataType.DATETIME))));
+
+        return funcList;
+    }
+}
diff --git a/src/org/apache/pig/builtin/GetYear.java b/src/org/apache/pig/builtin/GetYear.java
new file mode 100644
index 000000000..9fb580c23
--- /dev/null
+++ b/src/org/apache/pig/builtin/GetYear.java
@@ -0,0 +1,98 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.pig.builtin;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.pig.EvalFunc;
+import org.apache.pig.FuncSpec;
+import org.apache.pig.data.DataType;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.impl.logicalLayer.FrontendException;
+import org.apache.pig.impl.logicalLayer.schema.Schema;
+import org.joda.time.DateTime;
+
+/**
+ * GetYear extracts the year from a DateTime object.
+ * <ul>
+ * <li>Jodatime: http://joda-time.sourceforge.net/</li>
+ * <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li>
+ * </ul>
+ * <br />
+ * <pre>
+ * Example usage:
+ * 
+ * ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:datetime);
+ *
+ * DESCRIBE ISOin;
+ * ISOin: {dt: datetime}
+ *
+ * DUMP ISOin;
+ *
+ * (2009-01-07T01:07:01.071Z)
+ * (2008-02-06T02:06:02.062Z)
+ * (2007-03-05T03:05:03.053Z)
+ * ...
+ * 
+ * truncated = FOREACH ISO in GENERATE GetYear(dt) AS year,
+ *     GetMonth(dt) as month,
+ *     GetWeek(dt) as week,
+ *     GetWeekYear(dt) as weekyear,
+ *     GetDay(dt) AS day,
+ *     GetHour(dt) AS hour,
+ *     GetMinute(dt) AS min,
+ *     GetSecond(dt) AS sec;
+ *     GetMillSecond(dt) AS milli;
+ *
+ * DESCRIBE truncated;
+ * truncated: {year: int,month: int,week: int,weekyear: int,day: int,hour: int,min: int,sec: int,milli: int}
+ *
+ * DUMP truncated;
+ * (2009,1,2,2009,7,1,7,1,71)
+ * (2008,2,6,2008,6,2,6,2,62)
+ * (2007,3,10,2007,5,3,5,3,53)
+ * </pre>
+ */
+public class GetYear extends EvalFunc<Integer> {
+
+    @Override
+    public Integer exec(Tuple input) throws IOException {
+        if (input == null || input.size() < 1) {
+            return null;
+        }
+
+        return ((DateTime) input.get(0)).getYear();
+    }
+
+    @Override
+    public Schema outputSchema(Schema input) {
+        return new Schema(new Schema.FieldSchema(getSchemaName(this.getClass().getName().toLowerCase(), input), DataType.INTEGER));
+    }
+
+    @Override
+    public List<FuncSpec> getArgToFuncMapping() throws FrontendException {
+        List<FuncSpec> funcList = new ArrayList<FuncSpec>();
+        funcList.add(new FuncSpec(this.getClass().getName(), new Schema(new Schema.FieldSchema(null, DataType.DATETIME))));
+
+        return funcList;
+    }
+
+}
+
diff --git a/src/org/apache/pig/builtin/HoursBetween.java b/src/org/apache/pig/builtin/HoursBetween.java
new file mode 100644
index 000000000..f15c8ea65
--- /dev/null
+++ b/src/org/apache/pig/builtin/HoursBetween.java
@@ -0,0 +1,106 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.pig.builtin;
+
+import org.apache.pig.EvalFunc;
+import org.apache.pig.FuncSpec;
+import org.apache.pig.data.DataType;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.impl.logicalLayer.FrontendException;
+import org.apache.pig.impl.logicalLayer.schema.Schema;
+import org.joda.time.*;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+/**
+ * <p>HoursBetween returns the number of hours between two DateTime objects</p>
+ *
+ * <ul>
+ * <li>Jodatime: http://joda-time.sourceforge.net/</li>
+ * <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li>
+ * </ul>
+ * <br />
+ * <pre>
+ * Example usage:
+ *
+ * ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (datetime, dt2:datetime);
+ *
+ * DESCRIBE ISOin;
+ * ISOin: {dt: datetime,dt2: datetime}
+ *
+ * DUMP ISOin;
+ *
+ * (2009-01-07T01:07:01.000Z,2008-02-01T00:00:00.000Z)
+ * (2008-02-06T02:06:02.000Z,2008-02-01T00:00:00.000Z)
+ * (2007-03-05T03:05:03.000Z,2008-02-01T00:00:00.000Z)
+ * ...
+ *
+ * diffs = FOREACH ISOin GENERATE YearsBetween(dt, dt2) AS years,
+ * MonthsBetween(dt, dt2) AS months,
+ * WeeksBetween(dt, dt2) AS weeks,
+ * DaysBetween(dt, dt2) AS days,
+ * HoursBetween(dt, dt2) AS hours,
+ * MinutesBetween(dt, dt2) AS mins,
+ * SecondsBetween(dt, dt2) AS secs;
+ * MilliSecondsBetween(dt, dt2) AS millis;
+ *
+ * DESCRIBE diffs;
+ * diffs: {years: long,months: long,weeks: long,days: long,hours: long,mins: long,secs: long,millis: long}
+ *
+ * DUMP diffs;
+ *
+ * (0L,11L,48L,341L,8185L,491107L,29466421L,29466421000L)
+ * (0L,0L,0L,5L,122L,7326L,439562L,439562000L)
+ * (0L,-10L,-47L,-332L,-7988L,-479334L,-28760097L,-28760097000L)
+ *
+ * </pre>
+ */
+public class HoursBetween extends EvalFunc<Long> {
+
+    @Override
+    public Long exec(Tuple input) throws IOException
+    {
+        if (input == null || input.size() < 2) {
+            return null;
+        }
+
+        DateTime startDate = (DateTime) input.get(0);
+        DateTime endDate = (DateTime) input.get(1);
+
+        // Larger date first
+        return (startDate.getMillis() - endDate.getMillis()) / 3600000L;
+
+    }
+
+	@Override
+	public Schema outputSchema(Schema input) {
+        return new Schema(new Schema.FieldSchema(getSchemaName(this.getClass().getName().toLowerCase(), input), DataType.LONG));
+	}
+
+    @Override
+    public List<FuncSpec> getArgToFuncMapping() throws FrontendException {
+        List<FuncSpec> funcList = new ArrayList<FuncSpec>();
+        Schema s = new Schema();
+        s.add(new Schema.FieldSchema(null, DataType.DATETIME));
+        s.add(new Schema.FieldSchema(null, DataType.DATETIME));
+        funcList.add(new FuncSpec(this.getClass().getName(), s));
+        return funcList;
+    }
+}
diff --git a/src/org/apache/pig/builtin/MilliSecondsBetween.java b/src/org/apache/pig/builtin/MilliSecondsBetween.java
new file mode 100644
index 000000000..6ffb65583
--- /dev/null
+++ b/src/org/apache/pig/builtin/MilliSecondsBetween.java
@@ -0,0 +1,106 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.pig.builtin;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.pig.EvalFunc;
+import org.apache.pig.FuncSpec;
+import org.apache.pig.data.DataType;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.impl.logicalLayer.FrontendException;
+import org.apache.pig.impl.logicalLayer.schema.Schema;
+import org.joda.time.DateTime;
+
+/**
+ * <p>MilliSecondsBetween returns the number of milliseconds between two DateTime objects</p>
+ *
+ * <ul>
+ * <li>Jodatime: http://joda-time.sourceforge.net/</li>
+ * <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li>
+ * </ul>
+ * <br />
+ * <pre>
+ * Example usage:
+ *
+ * ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (datetime, dt2:datetime);
+ *
+ * DESCRIBE ISOin;
+ * ISOin: {dt: datetime,dt2: datetime}
+ *
+ * DUMP ISOin;
+ *
+ * (2009-01-07T01:07:01.000Z,2008-02-01T00:00:00.000Z)
+ * (2008-02-06T02:06:02.000Z,2008-02-01T00:00:00.000Z)
+ * (2007-03-05T03:05:03.000Z,2008-02-01T00:00:00.000Z)
+ * ...
+ *
+ * diffs = FOREACH ISOin GENERATE YearsBetween(dt, dt2) AS years,
+ * MonthsBetween(dt, dt2) AS months,
+ * WeeksBetween(dt, dt2) AS weeks,
+ * DaysBetween(dt, dt2) AS days,
+ * HoursBetween(dt, dt2) AS hours,
+ * MinutesBetween(dt, dt2) AS mins,
+ * SecondsBetween(dt, dt2) AS secs;
+ * MilliSecondsBetween(dt, dt2) AS millis;
+ *
+ * DESCRIBE diffs;
+ * diffs: {years: long,months: long,weeks: long,days: long,hours: long,mins: long,secs: long,millis: long}
+ *
+ * DUMP diffs;
+ *
+ * (0L,11L,48L,341L,8185L,491107L,29466421L,29466421000L)
+ * (0L,0L,0L,5L,122L,7326L,439562L,439562000L)
+ * (0L,-10L,-47L,-332L,-7988L,-479334L,-28760097L,-28760097000L)
+ *
+ * </pre>
+ */
+public class MilliSecondsBetween extends EvalFunc<Long> {
+
+    @Override
+    public Long exec(Tuple input) throws IOException
+    {
+        if (input == null || input.size() < 2) {
+            return null;
+        }
+
+        DateTime startDate = (DateTime) input.get(0);
+        DateTime endDate = (DateTime) input.get(1);
+
+        // Larger date first
+        // Subtraction may overflow
+        return startDate.getMillis() - endDate.getMillis();
+    }
+
+    @Override
+    public Schema outputSchema(Schema input) {
+        return new Schema(new Schema.FieldSchema(getSchemaName(this.getClass().getName().toLowerCase(), input), DataType.LONG));
+    }
+
+    @Override
+    public List<FuncSpec> getArgToFuncMapping() throws FrontendException {
+        List<FuncSpec> funcList = new ArrayList<FuncSpec>();
+        Schema s = new Schema();
+        s.add(new Schema.FieldSchema(null, DataType.DATETIME));
+        s.add(new Schema.FieldSchema(null, DataType.DATETIME));
+        funcList.add(new FuncSpec(this.getClass().getName(), s));
+        return funcList;
+    }
+}
diff --git a/src/org/apache/pig/builtin/MinutesBetween.java b/src/org/apache/pig/builtin/MinutesBetween.java
new file mode 100644
index 000000000..e7975c61d
--- /dev/null
+++ b/src/org/apache/pig/builtin/MinutesBetween.java
@@ -0,0 +1,107 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.pig.builtin;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.pig.EvalFunc;
+import org.apache.pig.FuncSpec;
+import org.apache.pig.data.DataType;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.impl.logicalLayer.FrontendException;
+import org.apache.pig.impl.logicalLayer.schema.Schema;
+import org.joda.time.DateTime;
+import org.joda.time.Minutes;
+
+/**
+ * <p>MinutesBetween returns the number of minutes between two DateTime objects</p>
+ *
+ * <ul>
+ * <li>Jodatime: http://joda-time.sourceforge.net/</li>
+ * <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li>
+ * </ul>
+ * <br />
+ * <pre>
+ * Example usage:
+ *
+ * ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (datetime, dt2:datetime);
+ *
+ * DESCRIBE ISOin;
+ * ISOin: {dt: datetime,dt2: datetime}
+ *
+ * DUMP ISOin;
+ *
+ * (2009-01-07T01:07:01.000Z,2008-02-01T00:00:00.000Z)
+ * (2008-02-06T02:06:02.000Z,2008-02-01T00:00:00.000Z)
+ * (2007-03-05T03:05:03.000Z,2008-02-01T00:00:00.000Z)
+ * ...
+ *
+ * diffs = FOREACH ISOin GENERATE YearsBetween(dt, dt2) AS years,
+ * MonthsBetween(dt, dt2) AS months,
+ * WeeksBetween(dt, dt2) AS weeks,
+ * DaysBetween(dt, dt2) AS days,
+ * HoursBetween(dt, dt2) AS hours,
+ * MinutesBetween(dt, dt2) AS mins,
+ * SecondsBetween(dt, dt2) AS secs;
+ * MilliSecondsBetween(dt, dt2) AS millis;
+ *
+ * DESCRIBE diffs;
+ * diffs: {years: long,months: long,weeks: long,days: long,hours: long,mins: long,secs: long,millis: long}
+ *
+ * DUMP diffs;
+ *
+ * (0L,11L,48L,341L,8185L,491107L,29466421L,29466421000L)
+ * (0L,0L,0L,5L,122L,7326L,439562L,439562000L)
+ * (0L,-10L,-47L,-332L,-7988L,-479334L,-28760097L,-28760097000L)
+ *
+ * </pre>
+ */
+public class MinutesBetween extends EvalFunc<Long> {
+
+    @Override
+    public Long exec(Tuple input) throws IOException
+    {
+        if (input == null || input.size() < 2) {
+            return null;
+        }
+
+        DateTime startDate = (DateTime) input.get(0);
+        DateTime endDate = (DateTime) input.get(1);
+
+        // Larger date first
+        // Subtraction may overflow
+        return (startDate.getMillis() - endDate.getMillis()) / 60000L;
+    }
+
+	@Override
+	public Schema outputSchema(Schema input) {
+        return new Schema(new Schema.FieldSchema(getSchemaName(this.getClass().getName().toLowerCase(), input), DataType.LONG));
+	}
+
+    @Override
+    public List<FuncSpec> getArgToFuncMapping() throws FrontendException {
+        List<FuncSpec> funcList = new ArrayList<FuncSpec>();
+        Schema s = new Schema();
+        s.add(new Schema.FieldSchema(null, DataType.DATETIME));
+        s.add(new Schema.FieldSchema(null, DataType.DATETIME));
+        funcList.add(new FuncSpec(this.getClass().getName(), s));
+        return funcList;
+    }
+}
diff --git a/src/org/apache/pig/builtin/MonthsBetween.java b/src/org/apache/pig/builtin/MonthsBetween.java
new file mode 100644
index 000000000..687e82d3e
--- /dev/null
+++ b/src/org/apache/pig/builtin/MonthsBetween.java
@@ -0,0 +1,109 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.pig.builtin;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.pig.EvalFunc;
+import org.apache.pig.FuncSpec;
+import org.apache.pig.data.DataType;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.impl.logicalLayer.FrontendException;
+import org.apache.pig.impl.logicalLayer.schema.Schema;
+import org.joda.time.DateTime;
+import org.joda.time.Months;
+
+/**
+ * <p>MonthsBetween returns the number of months between two DateTime objects</p>
+ *
+ * <ul>
+ * <li>Jodatime: http://joda-time.sourceforge.net/</li>
+ * <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li>
+ * </ul>
+ * <br />
+ * <pre>
+ * Example usage:
+ *
+ * ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (datetime, dt2:datetime);
+ *
+ * DESCRIBE ISOin;
+ * ISOin: {dt: datetime,dt2: datetime}
+ *
+ * DUMP ISOin;
+ *
+ * (2009-01-07T01:07:01.000Z,2008-02-01T00:00:00.000Z)
+ * (2008-02-06T02:06:02.000Z,2008-02-01T00:00:00.000Z)
+ * (2007-03-05T03:05:03.000Z,2008-02-01T00:00:00.000Z)
+ * ...
+ *
+ * diffs = FOREACH ISOin GENERATE YearsBetween(dt, dt2) AS years,
+ * MonthsBetween(dt, dt2) AS months,
+ * WeeksBetween(dt, dt2) AS weeks,
+ * DaysBetween(dt, dt2) AS days,
+ * HoursBetween(dt, dt2) AS hours,
+ * MinutesBetween(dt, dt2) AS mins,
+ * SecondsBetween(dt, dt2) AS secs;
+ * MilliSecondsBetween(dt, dt2) AS millis;
+ *
+ * DESCRIBE diffs;
+ * diffs: {years: long,months: long,weeks: long,days: long,hours: long,mins: long,secs: long,millis: long}
+ *
+ * DUMP diffs;
+ *
+ * (0L,11L,48L,341L,8185L,491107L,29466421L,29466421000L)
+ * (0L,0L,0L,5L,122L,7326L,439562L,439562000L)
+ * (0L,-10L,-47L,-332L,-7988L,-479334L,-28760097L,-28760097000L)
+ *
+ * </pre>
+ */
+public class MonthsBetween extends EvalFunc<Long> {
+
+    @Override
+    public Long exec(Tuple input) throws IOException
+    {
+        if (input == null || input.size() < 2) {
+            return null;
+        }
+
+        DateTime startDate = (DateTime) input.get(0);
+        DateTime endDate = (DateTime) input.get(1);
+
+        // Larger value first
+        Months m = Months.monthsBetween(endDate, startDate);
+        // joda limitation, only integer range, at the risk of overflow, need to be improved
+        return (long) m.getMonths();
+
+    }
+
+	@Override
+	public Schema outputSchema(Schema input) {
+        return new Schema(new Schema.FieldSchema(getSchemaName(this.getClass().getName().toLowerCase(), input), DataType.LONG));
+	}
+
+    @Override
+    public List<FuncSpec> getArgToFuncMapping() throws FrontendException {
+        List<FuncSpec> funcList = new ArrayList<FuncSpec>();
+        Schema s = new Schema();
+        s.add(new Schema.FieldSchema(null, DataType.DATETIME));
+        s.add(new Schema.FieldSchema(null, DataType.DATETIME));
+        funcList.add(new FuncSpec(this.getClass().getName(), s));
+        return funcList;
+    }
+}
diff --git a/src/org/apache/pig/builtin/SecondsBetween.java b/src/org/apache/pig/builtin/SecondsBetween.java
new file mode 100644
index 000000000..b4ede47b7
--- /dev/null
+++ b/src/org/apache/pig/builtin/SecondsBetween.java
@@ -0,0 +1,107 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.pig.builtin;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.pig.EvalFunc;
+import org.apache.pig.FuncSpec;
+import org.apache.pig.data.DataType;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.impl.logicalLayer.FrontendException;
+import org.apache.pig.impl.logicalLayer.schema.Schema;
+import org.joda.time.DateTime;
+import org.joda.time.Seconds;
+
+/**
+ * <p>SecondsBetween returns the number of seconds between two DateTime objects</p>
+ *
+ * <ul>
+ * <li>Jodatime: http://joda-time.sourceforge.net/</li>
+ * <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li>
+ * </ul>
+ * <br />
+ * <pre>
+ * Example usage:
+ *
+ * ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (datetime, dt2:datetime);
+ *
+ * DESCRIBE ISOin;
+ * ISOin: {dt: datetime,dt2: datetime}
+ *
+ * DUMP ISOin;
+ *
+ * (2009-01-07T01:07:01.000Z,2008-02-01T00:00:00.000Z)
+ * (2008-02-06T02:06:02.000Z,2008-02-01T00:00:00.000Z)
+ * (2007-03-05T03:05:03.000Z,2008-02-01T00:00:00.000Z)
+ * ...
+ *
+ * diffs = FOREACH ISOin GENERATE YearsBetween(dt, dt2) AS years,
+ * MonthsBetween(dt, dt2) AS months,
+ * WeeksBetween(dt, dt2) AS weeks,
+ * DaysBetween(dt, dt2) AS days,
+ * HoursBetween(dt, dt2) AS hours,
+ * MinutesBetween(dt, dt2) AS mins,
+ * SecondsBetween(dt, dt2) AS secs;
+ * MilliSecondsBetween(dt, dt2) AS millis;
+ *
+ * DESCRIBE diffs;
+ * diffs: {years: long,months: long,weeks: long,days: long,hours: long,mins: long,secs: long,millis: long}
+ *
+ * DUMP diffs;
+ *
+ * (0L,11L,48L,341L,8185L,491107L,29466421L,29466421000L)
+ * (0L,0L,0L,5L,122L,7326L,439562L,439562000L)
+ * (0L,-10L,-47L,-332L,-7988L,-479334L,-28760097L,-28760097000L)
+ *
+ * </pre>
+ */
+public class SecondsBetween extends EvalFunc<Long> {
+
+    @Override
+    public Long exec(Tuple input) throws IOException
+    {
+        if (input == null || input.size() < 2) {
+            return null;
+        }
+
+        DateTime startDate = (DateTime) input.get(0);
+        DateTime endDate = (DateTime) input.get(1);
+
+        // Larger date first
+        // Subtraction may overflow
+        return (startDate.getMillis() - endDate.getMillis()) / 1000L;
+    }
+
+	@Override
+	public Schema outputSchema(Schema input) {
+        return new Schema(new Schema.FieldSchema(getSchemaName(this.getClass().getName().toLowerCase(), input), DataType.LONG));
+	}
+
+    @Override
+    public List<FuncSpec> getArgToFuncMapping() throws FrontendException {
+        List<FuncSpec> funcList = new ArrayList<FuncSpec>();
+        Schema s = new Schema();
+        s.add(new Schema.FieldSchema(null, DataType.DATETIME));
+        s.add(new Schema.FieldSchema(null, DataType.DATETIME));
+        funcList.add(new FuncSpec(this.getClass().getName(), s));
+        return funcList;
+    }
+}
diff --git a/src/org/apache/pig/builtin/SubtractDuration.java b/src/org/apache/pig/builtin/SubtractDuration.java
new file mode 100644
index 000000000..5c2292a78
--- /dev/null
+++ b/src/org/apache/pig/builtin/SubtractDuration.java
@@ -0,0 +1,77 @@
+package org.apache.pig.builtin;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.pig.EvalFunc;
+import org.apache.pig.FuncSpec;
+import org.apache.pig.data.DataType;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.impl.logicalLayer.FrontendException;
+import org.apache.pig.impl.logicalLayer.schema.Schema;
+import org.joda.time.DateTime;
+import org.joda.time.Period;
+
+/**
+ * <p>SubtractDuration returns the result of a DateTime object plus a Duration object</p>
+ *
+ * <ul>
+ * <li>Jodatime: http://joda-time.sourceforge.net/</li>
+ * <li>ISO8601 Duration Format: http://en.wikipedia.org/wiki/ISO_8601#Durations</li>
+ * </ul>
+ * <br />
+ * <pre>
+ * Example usage:
+ *
+ * ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:datetime, dr:chararray);
+ *
+ * DESCRIBE ISOin;
+ * ISOin: {dt: datetime,dr: chararray}
+ *
+ * DUMP ISOin;
+ *
+ * (2009-01-07T01:07:01.000Z,PT1S)
+ * (2008-02-06T02:06:02.000Z,PT1M)
+ * (2007-03-05T03:05:03.000Z,P1D)
+ * ...
+ *
+ * dtsubtract = FOREACH ISOin GENERATE SubtractDuration(dt, dr) AS dt1;
+ *
+ * DESCRIBE dtsubtract;
+ * dtsubtract: {dt1: datetime}
+ *
+ * DUMP dtsubtract;
+ *
+ * (2009-01-07T01:07:00.000Z)
+ * (2008-02-06T02:05:02.000Z)
+ * (2007-03-04T03:05:03.000Z)
+ *
+ * </pre>
+ */
+public class SubtractDuration extends EvalFunc<DateTime> {
+
+    @Override
+    public DateTime exec(Tuple input) throws IOException {
+        if (input == null || input.size() < 2) {
+            return null;
+        }
+        
+        return ((DateTime) input.get(0)).minus(new Period((String) input.get(1)));
+    }
+    
+    @Override
+    public Schema outputSchema(Schema input) {
+        return new Schema(new Schema.FieldSchema(getSchemaName(this.getClass().getName().toLowerCase(), input), DataType.DATETIME));
+    }
+
+    @Override
+    public List<FuncSpec> getArgToFuncMapping() throws FrontendException {
+        List<FuncSpec> funcList = new ArrayList<FuncSpec>();
+        Schema s = new Schema();
+        s.add(new Schema.FieldSchema(null, DataType.DATETIME));
+        s.add(new Schema.FieldSchema(null, DataType.CHARARRAY));
+        funcList.add(new FuncSpec(this.getClass().getName(), s));
+        return funcList;
+    }
+}
diff --git a/src/org/apache/pig/builtin/TextLoader.java b/src/org/apache/pig/builtin/TextLoader.java
index bcd62348e..d5bcf0207 100644
--- a/src/org/apache/pig/builtin/TextLoader.java
+++ b/src/org/apache/pig/builtin/TextLoader.java
@@ -20,6 +20,8 @@ package org.apache.pig.builtin;
 import java.io.IOException;
 import java.util.Map;
 
+import org.joda.time.DateTime;
+
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.mapreduce.InputFormat;
 import org.apache.hadoop.mapreduce.Job;
@@ -120,6 +122,17 @@ public class TextLoader extends LoadFunc implements LoadCaster {
         String msg = "TextLoader does not support conversion to Double.";
         throw new ExecException(msg, errCode, PigException.BUG);
     }
+    
+    /**
+     * TextLoader does not support conversion to DateTime
+     * @throws IOException if the value cannot be cast.
+     */
+    @Override
+    public DateTime bytesToDateTime(byte[] b) throws IOException {
+        int errCode = 2109;
+        String msg = "TextLoader does not support conversion to DateTime.";
+        throw new ExecException(msg, errCode, PigException.BUG);
+    }
 
     /**
      * Cast data from bytes to chararray value.  
@@ -209,6 +222,12 @@ public class TextLoader extends LoadFunc implements LoadCaster {
         throw new ExecException(msg, errCode, PigException.BUG);
     }
 
+    public byte[] toBytes(DateTime dt) throws IOException {
+        int errCode = 2109;
+        String msg = "TextLoader does not support conversion from DateTime.";
+        throw new ExecException(msg, errCode, PigException.BUG);
+    }
+
     public byte[] toBytes(Map<String, Object> m) throws IOException {
         int errCode = 2109;
         String msg = "TextLoader does not support conversion from Map.";
diff --git a/src/org/apache/pig/builtin/ToDate.java b/src/org/apache/pig/builtin/ToDate.java
new file mode 100644
index 000000000..2cb391605
--- /dev/null
+++ b/src/org/apache/pig/builtin/ToDate.java
@@ -0,0 +1,112 @@
+package org.apache.pig.builtin;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+import org.apache.pig.EvalFunc;
+import org.apache.pig.FuncSpec;
+import org.apache.pig.backend.executionengine.ExecException;
+import org.apache.pig.data.DataType;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.impl.logicalLayer.FrontendException;
+import org.apache.pig.impl.logicalLayer.schema.Schema;
+import org.joda.time.DateTime;
+import org.joda.time.DateTimeZone;
+import org.joda.time.format.DateTimeFormat;
+import org.joda.time.format.DateTimeFormatter;
+
+/**
+ * 
+ * <p>ToDate converts the ISO or the customized string or the Unix timestamp to the DateTime object.</p>
+ * <p>ToDate is overloaded.</p>
+ * 
+ * <dl>
+ * <dt><b>Syntax:</b></dt>
+ * <dd><code>DateTime ToDate(Long millis)</code>.</dd>
+ * <dt><b>Input:</b></dt>
+ * <dd><code>the milliseconds</code>.</dd>
+ * <dt><b>Output:</b></dt>
+ * <dd><code>the DateTime object</code>.</dd>
+ * </dl>
+ *
+ * <dl>
+ * <dt><b>Syntax:</b></dt>
+ * <dd><code>DateTime ToDate(String dtStr)</code>.</dd>
+ * <dt><b>Input:</b></dt>
+ * <dd><code>the ISO format date time string</code>.</dd>
+ * <dt><b>Output:</b></dt>
+ * <dd><code>the DateTime object</code>.</dd>
+ * </dl>
+ * 
+ * <dl>
+ * <dt><b>Syntax:</b></dt>
+ * <dd><code>DateTime ToDate(String dtStr, String format)</code>.</dd>
+ * <dt><b>Input:</b></dt>
+ * <dd><code>dtStr: the string that represents a date time</code>.</dd>
+ * <dd><code>format: the format string</code>.</dd>
+ * <dt><b>Output:</b></dt>
+ * <dd><code>the DateTime object</code>.</dd>
+ * </dl>
+ *
+ * <dl>
+ * <dt><b>Syntax:</b></dt>
+ * <dd><code>DateTime ToDate(String dtStr, String format, String timezone)</code>.</dd>
+ * <dt><b>Input:</b></dt>
+ * <dd><code>dtStr: the string that represents a date time</code>.</dd>
+ * <dd><code>format: the format string</code>.</dd>
+ * <dd><code>timezone: the timezone string</code>.</dd>
+ * <dt><b>Output:</b></dt>
+ * <dd><code>the DateTime object</code>.</dd>
+ * </dl>
+ */
+public class ToDate extends EvalFunc<DateTime> {
+
+    public DateTime exec(Tuple input) throws IOException {
+        return new DateTime(DataType.toLong(input.get(0)));
+    }
+
+    @Override
+    public Schema outputSchema(Schema input) {
+        return new Schema(new Schema.FieldSchema(getSchemaName(this.getClass()
+                .getName().toLowerCase(), input), DataType.DATETIME));
+    }
+
+    @Override
+    public List<FuncSpec> getArgToFuncMapping() throws FrontendException {
+        List<FuncSpec> funcList = new ArrayList<FuncSpec>();
+        Schema s = new Schema();
+        s.add(new Schema.FieldSchema(null, DataType.LONG));
+        funcList.add(new FuncSpec(this.getClass().getName(), s));
+        s = new Schema();
+        s.add(new Schema.FieldSchema(null, DataType.CHARARRAY));
+        funcList.add(new FuncSpec(ToDateISO.class.getClass().getName(), s));
+        s = new Schema();
+        s.add(new Schema.FieldSchema(null, DataType.CHARARRAY));
+        s.add(new Schema.FieldSchema(null, DataType.CHARARRAY));
+        funcList.add(new FuncSpec(ToDate2ARGS.class.getClass().getName(), s));
+        s = new Schema();
+        s.add(new Schema.FieldSchema(null, DataType.CHARARRAY));
+        s.add(new Schema.FieldSchema(null, DataType.CHARARRAY));
+        s.add(new Schema.FieldSchema(null, DataType.CHARARRAY));
+        funcList.add(new FuncSpec(ToDate3ARGS.class.getClass().getName(), s));
+        return funcList;
+    }
+    
+    public static DateTimeZone extractDateTimeZone(String dtStr) {
+        Pattern pattern = Pattern.compile("(Z|((\\+|-)\\d{2}(:?\\d{2})?))$");
+        Matcher matcher = pattern.matcher(dtStr);
+        if (matcher.find()) {
+            String dtzStr = matcher.group();
+            if (dtzStr.equals("Z")) {
+                return DateTimeZone.forOffsetMillis(DateTimeZone.UTC.getOffset(null));
+            } else {
+                return DateTimeZone.forOffsetMillis(DateTimeZone.forID(dtzStr).getOffset(null));
+            }
+        } else {
+            return null;
+        }
+    }
+}
diff --git a/src/org/apache/pig/builtin/ToDate2ARGS.java b/src/org/apache/pig/builtin/ToDate2ARGS.java
new file mode 100644
index 000000000..ca3c8b3c3
--- /dev/null
+++ b/src/org/apache/pig/builtin/ToDate2ARGS.java
@@ -0,0 +1,30 @@
+package org.apache.pig.builtin;
+
+import java.io.IOException;
+
+import org.apache.pig.EvalFunc;
+import org.apache.pig.data.DataType;
+import org.apache.pig.data.Tuple;
+import org.joda.time.DateTime;
+import org.joda.time.format.DateTimeFormat;
+import org.joda.time.format.DateTimeFormatter;
+
+/**
+ * This method should never be used directly, use {@link ToDate}.
+ */
+public class ToDate2ARGS extends EvalFunc<DateTime> {
+
+    public DateTime exec(Tuple input) throws IOException {
+        String dtStr = DataType.toString(input.get(0));
+        //DateTimeZone dtz = extractDateTimeZone(dtStr);
+        //The timezone in the customized format is not predictable
+        DateTimeFormatter dtf = DateTimeFormat.forPattern(DataType
+                .toString(input.get(1)));
+        //if (dtz == null) {
+            return dtf.parseDateTime(dtStr);
+        //} else {
+        //    return dtf.withZone(dtz).parseDateTime(dtStr);
+        //}
+    }
+
+}
diff --git a/src/org/apache/pig/builtin/ToDate3ARGS.java b/src/org/apache/pig/builtin/ToDate3ARGS.java
new file mode 100644
index 000000000..5066a626a
--- /dev/null
+++ b/src/org/apache/pig/builtin/ToDate3ARGS.java
@@ -0,0 +1,26 @@
+package org.apache.pig.builtin;
+
+import java.io.IOException;
+
+import org.apache.pig.EvalFunc;
+import org.apache.pig.data.DataType;
+import org.apache.pig.data.Tuple;
+import org.joda.time.DateTime;
+import org.joda.time.DateTimeZone;
+import org.joda.time.format.DateTimeFormat;
+import org.joda.time.format.DateTimeFormatter;
+
+/**
+ * This method should never be used directly, use {@link ToDate}.
+ */
+public class ToDate3ARGS extends EvalFunc<DateTime> {
+
+    public DateTime exec(Tuple input) throws IOException {
+        DateTimeFormatter dtf = DateTimeFormat.forPattern(DataType
+                .toString(input.get(1)));
+        DateTimeZone dtz = DateTimeZone.forOffsetMillis(DateTimeZone.forID(
+                DataType.toString(input.get(2))).getOffset(null));
+        return dtf.withZone(dtz).parseDateTime(DataType.toString(input.get(0)));
+    }
+
+}
diff --git a/src/org/apache/pig/builtin/ToDateISO.java b/src/org/apache/pig/builtin/ToDateISO.java
new file mode 100644
index 000000000..5a7a1401f
--- /dev/null
+++ b/src/org/apache/pig/builtin/ToDateISO.java
@@ -0,0 +1,26 @@
+package org.apache.pig.builtin;
+
+import java.io.IOException;
+
+import org.apache.pig.EvalFunc;
+import org.apache.pig.data.DataType;
+import org.apache.pig.data.Tuple;
+import org.joda.time.DateTime;
+import org.joda.time.DateTimeZone;
+
+/**
+ * This method should never be used directly, use {@link ToDate}.
+ */
+public class ToDateISO extends EvalFunc<DateTime> {
+
+    public DateTime exec(Tuple input) throws IOException {
+        String dtStr = DataType.toString(input.get(0));
+        DateTimeZone dtz = ToDate.extractDateTimeZone(dtStr);
+        if (dtz == null) {
+            return new DateTime(dtStr);
+        } else {
+            return new DateTime(dtStr, dtz);
+        }
+    }
+
+}
diff --git a/src/org/apache/pig/builtin/ToMilliSeconds.java b/src/org/apache/pig/builtin/ToMilliSeconds.java
new file mode 100644
index 000000000..f1d3c6b21
--- /dev/null
+++ b/src/org/apache/pig/builtin/ToMilliSeconds.java
@@ -0,0 +1,97 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.pig.builtin;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.pig.EvalFunc;
+import org.apache.pig.FuncSpec;
+import org.apache.pig.data.DataType;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.impl.logicalLayer.FrontendException;
+import org.apache.pig.impl.logicalLayer.schema.Schema;
+import org.joda.time.DateTime;
+
+/**
+ * <p>
+ * ToUnixTime converts the DateTime to the number of milliseconds that have passed
+ * since January 1, 1970 00:00:00.000 GMT.
+ * </p>
+ * <ul>
+ * <li>Jodatime: http://joda-time.sourceforge.net/</li>
+ * <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li>
+ * <li>Unix Time: http://en.wikipedia.org/wiki/Unix_time</li>
+ * </ul>
+ * <br />
+ * <pre>
+ * Example usage:
+ *
+ * ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:datetime, dt2:datetime);
+ *
+ * DESCRIBE ISOin;
+ * ISOin: {dt: datetime,dt2: datetime}
+ *
+ * DUMP ISOin;
+ *
+ * (2009-01-07T01:07:01.000Z,2008-02-01T00:00:00.000Z)
+ * (2008-02-06T02:06:02.000Z,2008-02-01T00:00:00.000Z)
+ * (2007-03-05T03:05:03.000Z,2008-02-01T00:00:00.000Z)
+ * ...
+ *
+ * toMilliSeconds = FOREACH ISOin GENERATE ToMilliSeconds(dt) AS unixTime:long;
+ *
+ * DESCRIBE toMilliSeconds;
+ * toMilliSeconds: {unixTime: long}
+ *
+ * DUMP toMilliSeconds;
+ *
+ * (1231290421000L)
+ * (1202263562000L)
+ * (1173063903000L)
+ * ...
+ *</pre>
+ */
+public class ToMilliSeconds extends EvalFunc<Long> {
+
+    @Override
+    public Long exec(Tuple input) throws IOException
+    {
+        if (input == null || input.size() < 1) {
+            return null;
+        }
+
+        DateTime result = (DateTime) input.get(0);
+
+        return result.getMillis();
+    }
+
+    @Override
+    public Schema outputSchema(Schema input) {
+        return new Schema(new Schema.FieldSchema(getSchemaName(this.getClass().getName().toLowerCase(), input), DataType.LONG));
+    }
+
+    @Override
+    public List<FuncSpec> getArgToFuncMapping() throws FrontendException {
+        List<FuncSpec> funcList = new ArrayList<FuncSpec>();
+        funcList.add(new FuncSpec(this.getClass().getName(), new Schema(new Schema.FieldSchema(null, DataType.DATETIME))));
+
+        return funcList;
+    }
+}
diff --git a/src/org/apache/pig/builtin/ToString.java b/src/org/apache/pig/builtin/ToString.java
new file mode 100644
index 000000000..4ad1ea7a9
--- /dev/null
+++ b/src/org/apache/pig/builtin/ToString.java
@@ -0,0 +1,52 @@
+package org.apache.pig.builtin;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.pig.EvalFunc;
+import org.apache.pig.FuncSpec;
+import org.apache.pig.data.DataType;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.impl.logicalLayer.FrontendException;
+import org.apache.pig.impl.logicalLayer.schema.Schema;
+import org.joda.time.format.DateTimeFormat;
+import org.joda.time.format.DateTimeFormatter;
+
+/**
+ * 
+ * <p>ToString converts the DateTime object of the ISO or the customized string.</p>
+ *
+ */
+public class ToString extends EvalFunc<String> {
+
+    public String exec(Tuple input) throws IOException {
+        if (input == null) {
+            return null;
+        }
+        if (input.size() == 1) {
+            return DataType.toDateTime(input.get(0)).toString();
+        } else if (input.size() == 2) {
+            DateTimeFormatter dtf = DateTimeFormat.forPattern(DataType.toString(input.get(1)));
+            return DataType.toDateTime(input.get(0)).toString(dtf);
+        } else {
+            return null;
+        }
+    }
+
+    @Override
+    public Schema outputSchema(Schema input) {
+        return new Schema(new Schema.FieldSchema(getSchemaName(this.getClass().getName().toLowerCase(), input), DataType.CHARARRAY));
+    }
+
+    @Override
+    public List<FuncSpec> getArgToFuncMapping() throws FrontendException {
+        List<FuncSpec> funcList = new ArrayList<FuncSpec>();
+        Schema s = new Schema();
+        s.add(new Schema.FieldSchema(null, DataType.DATETIME));
+        s.add(new Schema.FieldSchema(null, DataType.CHARARRAY));
+        funcList.add(new FuncSpec(this.getClass().getName(), s));
+        return funcList;
+    }
+    
+}
diff --git a/src/org/apache/pig/builtin/ToUnixTime.java b/src/org/apache/pig/builtin/ToUnixTime.java
new file mode 100644
index 000000000..2b92c8ba7
--- /dev/null
+++ b/src/org/apache/pig/builtin/ToUnixTime.java
@@ -0,0 +1,95 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.pig.builtin;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.pig.EvalFunc;
+import org.apache.pig.FuncSpec;
+import org.apache.pig.data.DataType;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.impl.logicalLayer.FrontendException;
+import org.apache.pig.impl.logicalLayer.schema.Schema;
+import org.joda.time.DateTime;
+
+/**
+ * <p>ToUnixTime converts the DateTime to the Unix Time Long</p>
+ * <ul>
+ * <li>Jodatime: http://joda-time.sourceforge.net/</li>
+ * <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li>
+ * <li>Unix Time: http://en.wikipedia.org/wiki/Unix_time</li>
+ * </ul>
+ * <br />
+ * <pre>
+ * Example usage:
+ *
+ * ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:datetime, dt2:datetime);
+ *
+ * DESCRIBE ISOin;
+ * ISOin: {dt: datetime,dt2: datetime}
+ *
+ * DUMP ISOin;
+ *
+ * (2009-01-07T01:07:01.000Z,2008-02-01T00:00:00.000Z)
+ * (2008-02-06T02:06:02.000Z,2008-02-01T00:00:00.000Z)
+ * (2007-03-05T03:05:03.000Z,2008-02-01T00:00:00.000Z)
+ * ...
+ *
+ * toUnix = FOREACH ISOin GENERATE ToUnixTime(dt) AS unixTime:long;
+ *
+ * DESCRIBE toUnix;
+ * toUnix: {unixTime: long}
+ *
+ * DUMP toUnix;
+ *
+ * (1231290421L)
+ * (1202263562L)
+ * (1173063903L)
+ * ...
+ *</pre>
+ */
+public class ToUnixTime extends EvalFunc<Long> {
+
+    @Override
+    public Long exec(Tuple input) throws IOException
+    {
+        
+        if (input == null || input.size() < 1) {
+            return null;
+        }
+
+        DateTime result = (DateTime) input.get(0);
+
+        return result.getMillis() / 1000;
+    }
+
+	@Override
+	public Schema outputSchema(Schema input) {
+        return new Schema(new Schema.FieldSchema(getSchemaName(this.getClass().getName().toLowerCase(), input), DataType.LONG));
+	}
+
+    @Override
+    public List<FuncSpec> getArgToFuncMapping() throws FrontendException {
+        List<FuncSpec> funcList = new ArrayList<FuncSpec>();
+        funcList.add(new FuncSpec(this.getClass().getName(), new Schema(new Schema.FieldSchema(null, DataType.DATETIME))));
+
+        return funcList;
+    }
+}
diff --git a/src/org/apache/pig/builtin/Utf8StorageConverter.java b/src/org/apache/pig/builtin/Utf8StorageConverter.java
index 5965cb5f7..1defb3fa8 100644
--- a/src/org/apache/pig/builtin/Utf8StorageConverter.java
+++ b/src/org/apache/pig/builtin/Utf8StorageConverter.java
@@ -27,6 +27,9 @@ import java.util.Map;
 import java.util.Stack;
 import java.util.EmptyStackException;
 
+import org.joda.time.DateTime;
+import org.joda.time.DateTimeZone;
+
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.pig.LoadStoreCaster;
@@ -309,6 +312,9 @@ public class Utf8StorageConverter implements LoadStoreCaster {
         case DataType.BOOLEAN:
             field = bytesToBoolean(b);
             break;
+        case DataType.DATETIME:
+            field = bytesToDateTime(b);
+            break;
         default:
             throw new IOException("Unknown simple data type");
         }
@@ -463,6 +469,28 @@ public class Utf8StorageConverter implements LoadStoreCaster {
         }
     }
 
+    @Override
+    public DateTime bytesToDateTime(byte[] b) throws IOException {
+        if (b == null) {
+            return null;
+        }
+        try {
+            String dtStr = new String(b);
+            DateTimeZone dtz = ToDate.extractDateTimeZone(dtStr);
+            if (dtz == null) {
+                return new DateTime(dtStr);
+            } else {
+                return new DateTime(dtStr, dtz);
+            }
+        } catch (IllegalArgumentException e) {
+            LogUtils.warn(this, "Unable to interpret value " + Arrays.toString(b) + " in field being " +
+                    "converted to datetime, caught IllegalArgumentException <" +
+                    e.getMessage() + "> field discarded", 
+                    PigWarning.FIELD_DISCARDED_TYPE_CONVERSION_FAILED, mLog);
+            return null;
+        }
+    }
+
     @Override
     @SuppressWarnings("unchecked")
     public Map<String, Object> bytesToMap(byte[] b, ResourceFieldSchema fieldSchema) throws IOException {
@@ -546,6 +574,11 @@ public class Utf8StorageConverter implements LoadStoreCaster {
         return b.toString().getBytes();
     }
 
+    @Override
+    public byte[] toBytes(DateTime dt) throws IOException {
+        return dt.toString().getBytes();
+    }
+
     @Override
     public byte[] toBytes(Map<String, Object> m) throws IOException {
         return DataType.mapToString(m).getBytes();
diff --git a/src/org/apache/pig/builtin/WeeksBetween.java b/src/org/apache/pig/builtin/WeeksBetween.java
new file mode 100644
index 000000000..6dcb81481
--- /dev/null
+++ b/src/org/apache/pig/builtin/WeeksBetween.java
@@ -0,0 +1,109 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.pig.builtin;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.pig.EvalFunc;
+import org.apache.pig.FuncSpec;
+import org.apache.pig.data.DataType;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.impl.logicalLayer.FrontendException;
+import org.apache.pig.impl.logicalLayer.schema.Schema;
+import org.joda.time.DateTime;
+import org.joda.time.Weeks;
+
+/**
+ * <p>WeeksBetween returns the number of weeks between two DateTime objects</p>
+ *
+ * <ul>
+ * <li>Jodatime: http://joda-time.sourceforge.net/</li>
+ * <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li>
+ * </ul>
+ * <br />
+ * <pre>
+ * Example usage:
+ *
+ * ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (datetime, dt2:datetime);
+ *
+ * DESCRIBE ISOin;
+ * ISOin: {dt: datetime,dt2: datetime}
+ *
+ * DUMP ISOin;
+ *
+ * (2009-01-07T01:07:01.000Z,2008-02-01T00:00:00.000Z)
+ * (2008-02-06T02:06:02.000Z,2008-02-01T00:00:00.000Z)
+ * (2007-03-05T03:05:03.000Z,2008-02-01T00:00:00.000Z)
+ * ...
+ *
+ * diffs = FOREACH ISOin GENERATE YearsBetween(dt, dt2) AS years,
+ * MonthsBetween(dt, dt2) AS months,
+ * WeeksBetween(dt, dt2) AS weeks,
+ * DaysBetween(dt, dt2) AS days,
+ * HoursBetween(dt, dt2) AS hours,
+ * MinutesBetween(dt, dt2) AS mins,
+ * SecondsBetween(dt, dt2) AS secs;
+ * MilliSecondsBetween(dt, dt2) AS millis;
+ *
+ * DESCRIBE diffs;
+ * diffs: {years: long,months: long,weeks: long,days: long,hours: long,mins: long,secs: long,millis: long}
+ *
+ * DUMP diffs;
+ *
+ * (0L,11L,48L,341L,8185L,491107L,29466421L,29466421000L)
+ * (0L,0L,0L,5L,122L,7326L,439562L,439562000L)
+ * (0L,-10L,-47L,-332L,-7988L,-479334L,-28760097L,-28760097000L)
+ *
+ * </pre>
+ */
+public class WeeksBetween extends EvalFunc<Long> {
+
+    @Override
+    public Long exec(Tuple input) throws IOException
+    {
+        if (input == null || input.size() < 2) {
+            return null;
+        }
+
+        DateTime startDate = (DateTime) input.get(0);
+        DateTime endDate = (DateTime) input.get(1);
+
+        // Larger date first
+        // Subtraction may overflow
+        return (startDate.getMillis() - endDate.getMillis()) / 604800000L;
+
+    }
+
+    @Override
+    public Schema outputSchema(Schema input) {
+        return new Schema(new Schema.FieldSchema(getSchemaName(this.getClass().getName().toLowerCase(), input), DataType.LONG));
+    }
+
+    @Override
+    public List<FuncSpec> getArgToFuncMapping() throws FrontendException {
+        List<FuncSpec> funcList = new ArrayList<FuncSpec>();
+        Schema s = new Schema();
+        s.add(new Schema.FieldSchema(null, DataType.DATETIME));
+        s.add(new Schema.FieldSchema(null, DataType.DATETIME));
+        funcList.add(new FuncSpec(this.getClass().getName(), s));
+        return funcList;
+    }
+}
+
diff --git a/src/org/apache/pig/builtin/YearsBetween.java b/src/org/apache/pig/builtin/YearsBetween.java
new file mode 100644
index 000000000..542840097
--- /dev/null
+++ b/src/org/apache/pig/builtin/YearsBetween.java
@@ -0,0 +1,109 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.pig.builtin;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.pig.EvalFunc;
+import org.apache.pig.FuncSpec;
+import org.apache.pig.data.DataType;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.impl.logicalLayer.FrontendException;
+import org.apache.pig.impl.logicalLayer.schema.Schema;
+import org.joda.time.DateTime;
+import org.joda.time.Years;
+
+/**
+ * <p>YearsBetween returns the number of years between two DateTime objects</p>
+ *
+ * <ul>
+ * <li>Jodatime: http://joda-time.sourceforge.net/</li>
+ * <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li>
+ * </ul>
+ * <br />
+ * <pre>
+ * Example usage:
+ *
+ * ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (datetime, dt2:datetime);
+ *
+ * DESCRIBE ISOin;
+ * ISOin: {dt: datetime,dt2: datetime}
+ *
+ * DUMP ISOin;
+ *
+ * (2009-01-07T01:07:01.000Z,2008-02-01T00:00:00.000Z)
+ * (2008-02-06T02:06:02.000Z,2008-02-01T00:00:00.000Z)
+ * (2007-03-05T03:05:03.000Z,2008-02-01T00:00:00.000Z)
+ * ...
+ *
+ * diffs = FOREACH ISOin GENERATE YearsBetween(dt, dt2) AS years,
+ * MonthsBetween(dt, dt2) AS months,
+ * WeeksBetween(dt, dt2) AS weeks,
+ * DaysBetween(dt, dt2) AS days,
+ * HoursBetween(dt, dt2) AS hours,
+ * MinutesBetween(dt, dt2) AS mins,
+ * SecondsBetween(dt, dt2) AS secs;
+ * MilliSecondsBetween(dt, dt2) AS millis;
+ *
+ * DESCRIBE diffs;
+ * diffs: {years: long,months: long,weeks: long,days: long,hours: long,mins: long,secs: long,millis: long}
+ *
+ * DUMP diffs;
+ *
+ * (0L,11L,48L,341L,8185L,491107L,29466421L,29466421000L)
+ * (0L,0L,0L,5L,122L,7326L,439562L,439562000L)
+ * (0L,-10L,-47L,-332L,-7988L,-479334L,-28760097L,-28760097000L)
+ *
+ * </pre>
+ */
+public class YearsBetween extends EvalFunc<Long> {
+
+    @Override
+    public Long exec(Tuple input) throws IOException
+    {
+        if (input == null || input.size() < 2) {
+            return null;
+        }
+
+        DateTime startDate = (DateTime) input.get(0);
+        DateTime endDate = (DateTime) input.get(1);
+
+        // Larger value first
+        Years y = Years.yearsBetween(endDate, startDate);
+        // joda limitation, only integer range, at the risk of overflow, need to be improved
+        return (long) y.getYears();
+
+    }
+
+	@Override
+	public Schema outputSchema(Schema input) {
+        return new Schema(new Schema.FieldSchema(getSchemaName(this.getClass().getName().toLowerCase(), input), DataType.LONG));
+	}
+
+    @Override
+    public List<FuncSpec> getArgToFuncMapping() throws FrontendException {
+        List<FuncSpec> funcList = new ArrayList<FuncSpec>();
+        Schema s = new Schema();
+        s.add(new Schema.FieldSchema(null, DataType.DATETIME));
+        s.add(new Schema.FieldSchema(null, DataType.DATETIME));
+        funcList.add(new FuncSpec(this.getClass().getName(), s));
+        return funcList;
+    }
+}
diff --git a/src/org/apache/pig/data/BinInterSedes.java b/src/org/apache/pig/data/BinInterSedes.java
index c82e933ca..a7ba39b59 100644
--- a/src/org/apache/pig/data/BinInterSedes.java
+++ b/src/org/apache/pig/data/BinInterSedes.java
@@ -28,6 +28,9 @@ import java.util.HashMap;
 import java.util.Iterator;
 import java.util.Map;
 
+import org.joda.time.DateTime;
+import org.joda.time.DateTimeZone;
+
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
@@ -51,6 +54,8 @@ import org.apache.pig.impl.util.ObjectSerializer;
 @InterfaceAudience.Private
 @InterfaceStability.Stable
 public class BinInterSedes implements InterSedes {
+    
+    private static final int ONE_MINUTE = 60000;
 
     public static final byte BOOLEAN_TRUE = 0;
     public static final byte BOOLEAN_FALSE = 1;
@@ -103,6 +108,8 @@ public class BinInterSedes implements InterSedes {
     public static final byte LONG_0 = 34;
     public static final byte LONG_1 = 35;
 
+    public static final byte DATETIME = 50;
+
     public static final byte TUPLE_0 = 36;
     public static final byte TUPLE_1 = 37;
     public static final byte TUPLE_2 = 38;
@@ -375,6 +382,9 @@ public class BinInterSedes implements InterSedes {
         case LONG:
             return Long.valueOf(in.readLong());
 
+        case DATETIME:
+            return new DateTime(in.readLong(), DateTimeZone.forOffsetMillis(in.readShort() * ONE_MINUTE));
+
         case FLOAT:
             return Float.valueOf(in.readFloat());
 
@@ -496,6 +506,12 @@ public class BinInterSedes implements InterSedes {
             }
             break;
 
+        case DataType.DATETIME:
+            out.writeByte(DATETIME);
+            out.writeLong(((DateTime) val).getMillis());
+            out.writeShort(((DateTime) val).getZone().getOffset((DateTime) val) / ONE_MINUTE);
+            break;
+            
         case DataType.FLOAT:
             out.writeByte(FLOAT);
             out.writeFloat((Float) val);
@@ -809,6 +825,18 @@ public class BinInterSedes implements InterSedes {
                 }
                 break;
             }
+            case BinInterSedes.DATETIME: {
+                type1 = DataType.DATETIME;
+                type2 = getGeneralizedDataType(dt2);
+                if (type1 == type2) {
+                    long lv1 = bb1.getLong();
+                    bb1.position(bb1.position() + 2); // move cursor forward without read the timezone bytes
+                    long lv2 = bb2.getLong();
+                    bb2.position(bb2.position() + 2);
+                    rc = (lv1 < lv2 ? -1 : (lv1 == lv2 ? 0 : 1));
+                }
+                break;
+            }
             case BinInterSedes.FLOAT: {
                 type1 = DataType.FLOAT;
                 type2 = getGeneralizedDataType(dt2);
@@ -1108,6 +1136,8 @@ public class BinInterSedes implements InterSedes {
             case BinInterSedes.LONG_ININT:
             case BinInterSedes.LONG:
                 return DataType.LONG;
+            case BinInterSedes.DATETIME:
+                return DataType.DATETIME;
             case BinInterSedes.FLOAT:
                 return DataType.FLOAT;
             case BinInterSedes.DOUBLE:
diff --git a/src/org/apache/pig/data/DataReaderWriter.java b/src/org/apache/pig/data/DataReaderWriter.java
index add65fc67..37a162a55 100644
--- a/src/org/apache/pig/data/DataReaderWriter.java
+++ b/src/org/apache/pig/data/DataReaderWriter.java
@@ -25,6 +25,9 @@ import java.util.HashMap;
 import java.util.Iterator;
 import java.util.Map;
 
+import org.joda.time.DateTime;
+import org.joda.time.DateTimeZone;
+
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 
@@ -47,6 +50,7 @@ public class DataReaderWriter {
     private static BagFactory mBagFactory = BagFactory.getInstance();
     static final int UNSIGNED_SHORT_MAX = 65535;
     public static final String UTF8 = "UTF-8";
+    private static final int ONE_MINUTE = 60000;
 
     public static Tuple bytesToTuple(DataInput in) throws IOException {
         // Don't use Tuple.readFields, because it requires you to
@@ -178,6 +182,9 @@ public class DataReaderWriter {
 
             case DataType.BYTE:
                 return Byte.valueOf(in.readByte());
+                
+            case DataType.DATETIME:
+                return new DateTime(in.readLong(), DateTimeZone.forOffsetMillis(in.readShort() * ONE_MINUTE));
 
             case DataType.BYTEARRAY: {
                 int size = in.readInt();
@@ -290,6 +297,11 @@ public class DataReaderWriter {
                 out.writeByte((Byte)val);
                 break;
 
+            case DataType.DATETIME:
+                out.writeByte(DataType.DATETIME);
+                out.writeLong(((DateTime)val).getMillis());
+                out.writeShort(((DateTime)val).getZone().getOffset((DateTime)val) / 60000);
+
             case DataType.BYTEARRAY: {
                 out.writeByte(DataType.BYTEARRAY);
                 DataByteArray bytes = (DataByteArray)val;
diff --git a/src/org/apache/pig/data/DataType.java b/src/org/apache/pig/data/DataType.java
index 77c9a82da..e4c7b9854 100644
--- a/src/org/apache/pig/data/DataType.java
+++ b/src/org/apache/pig/data/DataType.java
@@ -26,6 +26,9 @@ import java.util.Iterator;
 import java.util.Map;
 import java.util.TreeMap;
 
+import org.joda.time.DateTime;
+import org.joda.time.DateTimeZone;
+
 import org.apache.hadoop.io.WritableComparable;
 
 import org.apache.pig.classification.InterfaceAudience;
@@ -33,6 +36,7 @@ import org.apache.pig.classification.InterfaceStability;
 import org.apache.pig.PigException;
 import org.apache.pig.ResourceSchema;
 import org.apache.pig.backend.executionengine.ExecException;
+import org.apache.pig.builtin.ToDate;
 import org.apache.pig.impl.logicalLayer.FrontendException;
 import org.apache.pig.impl.logicalLayer.schema.Schema;
 import org.apache.pig.impl.logicalLayer.schema.SchemaMergeException;
@@ -63,6 +67,7 @@ public class DataType {
     public static final byte LONG      =  15;
     public static final byte FLOAT     =  20;
     public static final byte DOUBLE    =  25;
+    public static final byte DATETIME  =  30;
     public static final byte BYTEARRAY =  50;
     public static final byte CHARARRAY =  55;
     /**
@@ -119,6 +124,8 @@ public class DataType {
             return DOUBLE;
         } else if (o instanceof Boolean) {
             return BOOLEAN;
+        } else if (o instanceof DateTime) {
+            return DATETIME;
         } else if (o instanceof Byte) {
             return BYTE;
         } else if (o instanceof WritableComparable) {
@@ -154,6 +161,8 @@ public class DataType {
             return BOOLEAN;
         } else if (t == Byte.class) {
             return BYTE;
+        } else if (t == DateTime.class) {
+            return DATETIME;
         } else if (t == InternalMap.class) {
             return INTERNALMAP;
         } else {
@@ -218,7 +227,7 @@ public class DataType {
      */
     public static byte[] genAllTypes(){
         byte[] types = { DataType.BAG, DataType.BIGCHARARRAY, DataType.BOOLEAN, DataType.BYTE, DataType.BYTEARRAY,
-                DataType.CHARARRAY, DataType.DOUBLE, DataType.FLOAT,
+                DataType.CHARARRAY, DataType.DOUBLE, DataType.FLOAT, DataType.DATETIME,
                 DataType.GENERIC_WRITABLECOMPARABLE,
                 DataType.INTEGER, DataType.INTERNALMAP,
                 DataType.LONG, DataType.MAP, DataType.TUPLE};
@@ -227,7 +236,7 @@ public class DataType {
 
     private static String[] genAllTypeNames(){
         String[] names = { "BAG", "BIGCHARARRAY", "BOOLEAN", "BYTE", "BYTEARRAY",
-                "CHARARRAY", "DOUBLE", "FLOAT",
+                "CHARARRAY", "DOUBLE", "FLOAT", "DATETIME",
                 "GENERIC_WRITABLECOMPARABLE",
                 "INTEGER","INTERNALMAP",
                 "LONG", "MAP", "TUPLE" };
@@ -285,6 +294,7 @@ public class DataType {
         case LONG:      return "long";
         case FLOAT:     return "float";
         case DOUBLE:    return "double";
+        case DATETIME:  return "datetime";
         case BYTEARRAY: return "bytearray";
         case BIGCHARARRAY: return "bigchararray";
         case CHARARRAY: return "chararray";
@@ -332,6 +342,7 @@ public class DataType {
                 (dataType == DOUBLE) ||
                 (dataType == BOOLEAN) ||
                 (dataType == BYTE) ||
+                (dataType == DATETIME) ||
                 (dataType == GENERIC_WRITABLECOMPARABLE));
     }
 
@@ -369,7 +380,7 @@ public class DataType {
      * because there's no super class that implements compareTo.  This
      * function provides an (arbitrary) ordering of objects of different
      * types as follows:  NULL &lt; BOOLEAN &lt; BYTE &lt; INTEGER &lt; LONG &lt;
-     * FLOAT &lt; DOUBLE * &lt; BYTEARRAY &lt; STRING &lt; MAP &lt;
+     * FLOAT &lt; DOUBLE &lt; DATETIME &lt; BYTEARRAY &lt; STRING &lt; MAP &lt;
      * TUPLE &lt; BAG.  No other functions should implement this cross
      * object logic.  They should call this function for it instead.
      * @param o1 First object
@@ -420,6 +431,9 @@ public class DataType {
             case DOUBLE:
                 return ((Double)o1).compareTo((Double)o2);
 
+            case DATETIME:
+                return ((DateTime)o1).compareTo((DateTime)o2);
+
             case BYTEARRAY:
                 return ((DataByteArray)o1).compareTo(o2);
 
@@ -504,6 +518,9 @@ public class DataType {
         case LONG:
             return ((Number) o).toString().getBytes();
 
+        case DATETIME:
+            return ((DateTime) o).toString().getBytes();
+
         case CHARARRAY:
             return ((String) o).getBytes();
         case MAP:
@@ -574,6 +591,7 @@ public class DataType {
                 } else {
                     return null;
                 }
+            case DATETIME:
             case MAP:
             case INTERNALMAP:
             case TUPLE:
@@ -649,6 +667,9 @@ public class DataType {
 			case NULL:
 			    return null;
 
+			case DATETIME:
+			    return Integer.valueOf(Long.valueOf(((DateTime)o).getMillis()).intValue());
+
 			case MAP:
 			case INTERNALMAP:
 			case TUPLE:
@@ -738,6 +759,8 @@ public class DataType {
 			case NULL:
 			    return null;
 
+			case DATETIME:
+			    return Long.valueOf(((DateTime)o).getMillis());
 			case MAP:
 			case INTERNALMAP:
 			case TUPLE:
@@ -812,6 +835,9 @@ public class DataType {
 			case DOUBLE:
 			    return new Float(((Double)o).floatValue());
 
+	         case DATETIME:
+	             return new Float(Long.valueOf(((DateTime)o).getMillis()).floatValue());
+
 			case BYTEARRAY:
 			    return Float.valueOf(((DataByteArray)o).toString());
 
@@ -895,7 +921,10 @@ public class DataType {
 			case DOUBLE:
 			    return (Double)o;
 
-			case BYTEARRAY:
+            case DATETIME:
+                return new Double(Long.valueOf(((DateTime)o).getMillis()).doubleValue());
+
+            case BYTEARRAY:
 			    return Double.valueOf(((DataByteArray)o).toString());
 
 			case CHARARRAY:
@@ -930,6 +959,78 @@ public class DataType {
 			throw new ExecException(msg, errCode, PigException.BUG);
 		}
     }
+    
+    /**
+     * Force a data object to a DateTime, if possible. Only CharArray, ByteArray
+     * can be forced to a DateTime. Numeric types and complex types
+     * cannot be forced to a DateTime. This isn't particularly efficient, so if
+     * you already <b>know</b> that the object you have is a DateTime you should
+     * just cast it.
+     * 
+     * @param o
+     *            object to cast
+     * @param type
+     *            of the object you are casting
+     * @return The object as a Boolean.
+     * @throws ExecException
+     *             if the type can't be forced to a Boolean.
+     */
+    public static DateTime toDateTime(Object o, byte type) throws ExecException {
+        try {
+            switch (type) {
+            case NULL:
+                return null;
+            case BYTEARRAY:
+                return new DateTime(((DataByteArray) o).toString());
+            case CHARARRAY:
+                // the string can contain just date part or date part plus time part
+                DateTimeZone dtz = ToDate.extractDateTimeZone((String) o);
+                if (dtz == null) {
+                    return new DateTime((String) o);
+                } else {
+                    return new DateTime((String) o, dtz);
+                }
+            case INTEGER:
+                return new DateTime(((Integer) o).longValue());
+            case LONG:
+                return new DateTime(((Long) o).longValue());
+            case FLOAT:
+                return new DateTime(((Float) o).longValue());
+            case DOUBLE:
+                return new DateTime(((Double) o).longValue());
+            case DATETIME:
+                return (DateTime) o;
+
+            case BOOLEAN:
+            case BYTE:
+            case MAP:
+            case INTERNALMAP:
+            case TUPLE:
+            case BAG:
+            case UNKNOWN:
+            default:
+                int errCode = 1071;
+                String msg = "Cannot convert a " + findTypeName(o) + " to a Boolean";
+                throw new ExecException(msg, errCode, PigException.INPUT);
+            }
+        } catch (ClassCastException cce) {
+            throw cce;
+        } catch (ExecException ee) {
+            throw ee;
+        } catch (NumberFormatException nfe) {
+            int errCode = 1074;
+            String msg = "Problem with formatting. Could not convert " + o + " to Float.";
+            throw new ExecException(msg, errCode, PigException.INPUT, nfe);
+        } catch (Exception e) {
+            int errCode = 2054;
+            String msg = "Internal error. Could not convert " + o + " to Float.";
+            throw new ExecException(msg, errCode, PigException.BUG);
+        }
+    }
+    
+    public static DateTime toDateTime(Object o) throws ExecException {
+        return toDateTime(o, findType(o));
+    }
 
     /**
      * Force a data object to a Double, if possible.  Any numeric type
@@ -973,6 +1074,9 @@ public class DataType {
 
 			case DOUBLE:
 			    return ((Double)o).toString();
+    
+			case DATETIME:
+			    return ((DateTime)o).toString();
 
 			case BYTEARRAY:
 			    return ((DataByteArray)o).toString();
@@ -1315,6 +1419,7 @@ public class DataType {
         case LONG:
         case FLOAT:
         case DOUBLE:
+        case DATETIME:
         case BYTEARRAY:
         case CHARARRAY:
         case MAP:
diff --git a/src/org/apache/pig/data/DefaultTuple.java b/src/org/apache/pig/data/DefaultTuple.java
index 6038ce437..e182b06cd 100644
--- a/src/org/apache/pig/data/DefaultTuple.java
+++ b/src/org/apache/pig/data/DefaultTuple.java
@@ -323,6 +323,13 @@ public class DefaultTuple extends AbstractTuple {
                             double dv2 = bb2.getDouble();
                             rc = Double.compare(dv1, dv2);
                             break;
+                        case DataType.DATETIME:
+                            long dtv1 = bb1.getLong();
+                            bb1.position(bb1.position() + 2); // move cursor forward without read the timezone bytes
+                            long dtv2 = bb2.getLong();
+                            bb2.position(bb2.position() + 2);
+                            rc = (dtv1 < dtv2 ? -1 : (dtv1 == dtv2 ? 0 : 1));
+                            break;
                         case DataType.BYTEARRAY:
                             int basz1 = bb1.getInt();
                             int basz2 = bb2.getInt();
diff --git a/src/org/apache/pig/data/SchemaTuple.java b/src/org/apache/pig/data/SchemaTuple.java
index 765c3dd6f..4f0483457 100644
--- a/src/org/apache/pig/data/SchemaTuple.java
+++ b/src/org/apache/pig/data/SchemaTuple.java
@@ -24,6 +24,9 @@ import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 
+import org.joda.time.DateTime;
+import org.joda.time.DateTimeZone;
+
 import org.apache.commons.codec.binary.Base64;
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.classification.InterfaceAudience;
@@ -51,6 +54,7 @@ import com.google.common.collect.Lists;
 public abstract class SchemaTuple<T extends SchemaTuple<T>> extends AbstractTuple implements TypeAwareTuple {
 
     private static final long serialVersionUID = 1L;
+    private static final int ONE_MINUTE = 60000;
     private static final BinInterSedes bis = new BinInterSedes();
 
     @NotImplemented
@@ -173,6 +177,11 @@ public abstract class SchemaTuple<T extends SchemaTuple<T>> extends AbstractTupl
         out.writeDouble(v);
     }
 
+    protected static void write(DataOutput out, DateTime v) throws IOException {
+        out.writeLong(v.getMillis());
+        out.writeShort(v.getZone().getOffset(v) / ONE_MINUTE);
+    }
+
     protected static void write(DataOutput out, byte[] v) throws IOException {
         SedesHelper.writeBytes(out, v);
     }
@@ -210,6 +219,10 @@ public abstract class SchemaTuple<T extends SchemaTuple<T>> extends AbstractTupl
         return in.readDouble();
     }
 
+    protected static DateTime read(DataInput in, DateTime v) throws IOException {
+        return new DateTime(in.readLong(), DateTimeZone.forOffsetMillis(in.readShort() * ONE_MINUTE));
+    }
+
     protected static String read(DataInput in, String v) throws IOException {
         return SedesHelper.readChararray(in, in.readByte());
     }
@@ -373,6 +386,10 @@ public abstract class SchemaTuple<T extends SchemaTuple<T>> extends AbstractTupl
         return unbox((Boolean)v);
     }
 
+    protected DateTime unbox(Object v, DateTime t) {
+        return (DateTime)v;
+    }
+
     protected String unbox(Object v, String t) {
         return (String)v;
     }
@@ -416,6 +433,10 @@ public abstract class SchemaTuple<T extends SchemaTuple<T>> extends AbstractTupl
         return v.booleanValue();
     }
 
+    protected DateTime unbox(DateTime v) {
+        return v;
+    }
+
     protected DataBag box(DataBag v) {
         return v;
     }
@@ -459,6 +480,10 @@ public abstract class SchemaTuple<T extends SchemaTuple<T>> extends AbstractTupl
         return new Boolean(v);
     }
 
+    protected DateTime box(DateTime v) {
+        return v;
+    }
+
     protected int hashCodePiece(int hash, int v, boolean isNull) {
         return isNull ? hash : 31 * hash + v;
     }
@@ -480,6 +505,10 @@ public abstract class SchemaTuple<T extends SchemaTuple<T>> extends AbstractTupl
         return isNull ? hash : 31 * hash + (v ? 1231 : 1237);
     }
 
+    protected int hashCodePiece(int hash, DateTime v, boolean isNull) {
+        return isNull ? hash : 31 * hash + v.hashCode();
+    }
+
     protected int hashCodePiece(int hash, byte[] v, boolean isNull) {
         return isNull ? hash : 31 * hash + DataByteArray.hashCode(v);
     }
@@ -578,6 +607,13 @@ public abstract class SchemaTuple<T extends SchemaTuple<T>> extends AbstractTupl
 
     protected abstract void generatedCodeSetBoolean(int fieldNum, boolean val) throws ExecException;
 
+    @Override
+    public void setDateTime(int fieldNum, DateTime val) throws ExecException {
+         generatedCodeSetDateTime(fieldNum, val);
+    }
+
+    protected abstract void generatedCodeSetDateTime(int fieldNum, DateTime val) throws ExecException;
+
     @Override
     public void setString(int fieldNum, String val) throws ExecException {
          generatedCodeSetString(fieldNum, val);
@@ -644,6 +680,11 @@ public abstract class SchemaTuple<T extends SchemaTuple<T>> extends AbstractTupl
         return val;
     }
 
+    protected DateTime returnUnlessNull(boolean isNull, DateTime val) throws FieldIsNullException {
+        errorIfNull(isNull, "DateTime");
+        return val;
+    }
+
     protected Tuple returnUnlessNull(boolean isNull, Tuple val) throws FieldIsNullException {
         errorIfNull(isNull, "Tuple");
         return val;
@@ -722,6 +763,17 @@ public abstract class SchemaTuple<T extends SchemaTuple<T>> extends AbstractTupl
 
     public boolean unboxBoolean(Object val) {
         return ((Boolean)val).booleanValue();
+    }    
+
+    @Override
+    public DateTime getDateTime(int fieldNum) throws ExecException {
+        return generatedCodeGetDateTime(fieldNum);
+    }
+
+    protected abstract DateTime generatedCodeGetDateTime(int fieldNum) throws ExecException;
+
+    public DateTime unboxDateTime(Object val) {
+        return (DateTime)val;
     }
 
     @Override
@@ -1027,6 +1079,33 @@ public abstract class SchemaTuple<T extends SchemaTuple<T>> extends AbstractTupl
         return compare(isNull, val, themNull, themVal);
     }
 
+    protected int compare(boolean usNull, DateTime usVal, boolean themNull, DateTime themVal) {
+        if (usNull && themNull) {
+            return 0;
+        } else if (themNull) {
+            return 1;
+        } else if (usNull) {
+            return -1;
+        }
+        return compare(usVal, themVal);
+    }
+
+    protected int compare(DateTime val, DateTime themVal) {
+        return val.compareTo(themVal);
+    }
+
+    protected int compareWithElementAtPos(boolean isNull, DateTime val, SchemaTuple<?> t, int pos) {
+        DateTime themVal;
+        boolean themNull;
+        try {
+            themVal = t.getDateTime(pos);
+            themNull = t.isNull(pos);
+        } catch (ExecException e) {
+            throw new RuntimeException("Unable to retrieve String field " + pos + " in given Tuple: " + t, e);
+        }
+        return compare(isNull, val, themNull, themVal);
+    }
+
     protected int compare(boolean usNull, String usVal, boolean themNull, String themVal) {
         if (usNull && themNull) {
             return 0;
@@ -1158,4 +1237,4 @@ public abstract class SchemaTuple<T extends SchemaTuple<T>> extends AbstractTupl
     protected abstract void generatedCodeReadFields(DataInput in, boolean[] nulls) throws IOException;
 
     protected abstract boolean[] generatedCodeNullsArray() throws IOException;
-}
\ No newline at end of file
+}
diff --git a/src/org/apache/pig/data/SchemaTupleClassGenerator.java b/src/org/apache/pig/data/SchemaTupleClassGenerator.java
index 090a1aaf7..ef1531813 100644
--- a/src/org/apache/pig/data/SchemaTupleClassGenerator.java
+++ b/src/org/apache/pig/data/SchemaTupleClassGenerator.java
@@ -729,6 +729,8 @@ public class SchemaTupleClassGenerator {
                 if (booleans++ % 8 == 0) {
                     size++; //accounts for the byte used to store boolean values
                 }
+            } else if (isDateTime()) {
+                size += 10; // 8 for long and 2 for short
             } else if (isBag()) {
                 size += 8; //the ptr
                 s += "(pos_"+fieldPos+" == null ? 0 : pos_"+fieldPos+".getMemorySize()) + ";
@@ -764,6 +766,7 @@ public class SchemaTupleClassGenerator {
             case (DataType.FLOAT): add("    return 0.0f;"); break;
             case (DataType.DOUBLE): add("    return 0.0;"); break;
             case (DataType.BOOLEAN): add("    return true;"); break;
+            case (DataType.DATETIME): add("    return new DateTime();"); break;
             case (DataType.BYTEARRAY): add("    return (byte[])null;"); break;
             case (DataType.CHARARRAY): add("    return (String)null;"); break;
             case (DataType.TUPLE): add("    return (Tuple)null;"); break;
@@ -1032,6 +1035,7 @@ public class SchemaTupleClassGenerator {
             listOfFutureMethods.add(new TypeAwareSetString(DataType.BYTEARRAY));
             listOfFutureMethods.add(new TypeAwareSetString(DataType.CHARARRAY));
             listOfFutureMethods.add(new TypeAwareSetString(DataType.BOOLEAN));
+            listOfFutureMethods.add(new TypeAwareSetString(DataType.DATETIME));
             listOfFutureMethods.add(new TypeAwareSetString(DataType.TUPLE));
             listOfFutureMethods.add(new TypeAwareSetString(DataType.BAG));
             listOfFutureMethods.add(new TypeAwareSetString(DataType.MAP));
@@ -1042,6 +1046,7 @@ public class SchemaTupleClassGenerator {
             listOfFutureMethods.add(new TypeAwareGetString(DataType.BYTEARRAY));
             listOfFutureMethods.add(new TypeAwareGetString(DataType.CHARARRAY));
             listOfFutureMethods.add(new TypeAwareGetString(DataType.BOOLEAN));
+            listOfFutureMethods.add(new TypeAwareGetString(DataType.DATETIME));
             listOfFutureMethods.add(new TypeAwareGetString(DataType.TUPLE));
             listOfFutureMethods.add(new TypeAwareGetString(DataType.BAG));
             listOfFutureMethods.add(new TypeAwareGetString(DataType.MAP));
@@ -1069,6 +1074,8 @@ public class SchemaTupleClassGenerator {
                     .append("\n")
                     .append("import com.google.common.collect.Lists;\n")
                     .append("\n")
+                    .append("import org.joda.time.DateTime;")
+                    .append("\n")
                     .append("import org.apache.pig.data.DataType;\n")
                     .append("import org.apache.pig.data.DataBag;\n")
                     .append("import org.apache.pig.data.Tuple;\n")
@@ -1187,6 +1194,10 @@ public class SchemaTupleClassGenerator {
             return type == DataType.DOUBLE;
         }
 
+        public boolean isDateTime() {
+            return type == DataType.DATETIME;
+        }
+
         public boolean isPrimitive() {
             return isInt() || isLong() || isFloat() || isDouble() || isBoolean();
         }
@@ -1232,6 +1243,7 @@ public class SchemaTupleClassGenerator {
                 case (DataType.BYTEARRAY): return "byte[]";
                 case (DataType.CHARARRAY): return "String";
                 case (DataType.BOOLEAN): return "boolean";
+                case (DataType.DATETIME): return "DateTime";
                 case (DataType.TUPLE): return "Tuple";
                 case (DataType.BAG): return "DataBag";
                 case (DataType.MAP): return "Map";
@@ -1247,4 +1259,4 @@ public class SchemaTupleClassGenerator {
             }
         }
     }
-}
\ No newline at end of file
+}
diff --git a/src/org/apache/pig/data/SizeUtil.java b/src/org/apache/pig/data/SizeUtil.java
index a99bdc0b7..90e5d94b2 100644
--- a/src/org/apache/pig/data/SizeUtil.java
+++ b/src/org/apache/pig/data/SizeUtil.java
@@ -67,6 +67,9 @@ public class SizeUtil {
         case DataType.LONG:
             return 8 + 8;
 
+        case DataType.DATETIME:
+            return 8 + 2 + 8 + 6 /* one long (8) + one short (2) + 6 to round to 8 bytes */;
+
         case DataType.MAP: {
             @SuppressWarnings("unchecked")
             Map<String, Object> m = (Map<String, Object>) o;
diff --git a/src/org/apache/pig/data/TypeAwareTuple.java b/src/org/apache/pig/data/TypeAwareTuple.java
index 4490c515f..cf78d0aa3 100644
--- a/src/org/apache/pig/data/TypeAwareTuple.java
+++ b/src/org/apache/pig/data/TypeAwareTuple.java
@@ -19,6 +19,8 @@ package org.apache.pig.data;
 
 import java.util.Map;
 
+import org.joda.time.DateTime;
+
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.impl.logicalLayer.schema.Schema;
 
@@ -34,6 +36,7 @@ public interface TypeAwareTuple extends Tuple {
     public void setTuple(int idx, Tuple val) throws ExecException;
     public void setDataBag(int idx, DataBag val) throws ExecException;
     public void setMap(int idx, Map<String,Object> val) throws ExecException;
+    public void setDateTime(int idx, DateTime val) throws ExecException;
 
     public int getInt(int idx) throws ExecException, FieldIsNullException;
     public float getFloat(int idx) throws ExecException, FieldIsNullException;
@@ -45,6 +48,7 @@ public interface TypeAwareTuple extends Tuple {
     public Tuple getTuple(int idx) throws ExecException;
     public DataBag getDataBag(int idx) throws ExecException, FieldIsNullException;
     public Map<String,Object> getMap(int idx) throws ExecException, FieldIsNullException;
+    public DateTime getDateTime(int idx) throws ExecException, FieldIsNullException;
 
     public Schema getSchema();
 
diff --git a/src/org/apache/pig/impl/io/NullableDateTimeWritable.java b/src/org/apache/pig/impl/io/NullableDateTimeWritable.java
new file mode 100644
index 000000000..135e2a2b6
--- /dev/null
+++ b/src/org/apache/pig/impl/io/NullableDateTimeWritable.java
@@ -0,0 +1,39 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.pig.impl.io;
+
+import org.joda.time.DateTime;
+
+import org.apache.pig.backend.hadoop.DateTimeWritable; 
+
+public class NullableDateTimeWritable extends PigNullableWritable {
+
+    public NullableDateTimeWritable() {
+        mValue = new DateTimeWritable();
+    }
+
+    public NullableDateTimeWritable(DateTime dt) {
+        mValue = new DateTimeWritable(dt);
+    }
+
+    public Object getValueAsPigType() {
+        return isNull() ? null : ((DateTimeWritable)mValue).get();
+    }
+
+}
diff --git a/src/org/apache/pig/impl/logicalLayer/schema/Schema.java b/src/org/apache/pig/impl/logicalLayer/schema/Schema.java
index 5294285a7..0d65a80c8 100644
--- a/src/org/apache/pig/impl/logicalLayer/schema/Schema.java
+++ b/src/org/apache/pig/impl/logicalLayer/schema/Schema.java
@@ -256,11 +256,16 @@ public class Schema implements Serializable, Cloneable {
                 }
                 else if (DataType.isNumberType(inputType) && (castType == DataType.CHARARRAY
                         || castType == DataType.BYTEARRAY || DataType.isNumberType(castType)
-                        || castType == DataType.BOOLEAN)) {
+                        || castType == DataType.BOOLEAN || castType == DataType.DATETIME)) {
+                    // good
+                }
+                else if (inputType == DataType.DATETIME && (castType == DataType.CHARARRAY
+                        || castType == DataType.BYTEARRAY || DataType.isNumberType(castType))) {
                     // good
                 }
                 else if (inputType == DataType.CHARARRAY && (castType == DataType.BYTEARRAY
-                        || DataType.isNumberType(castType) || castType == DataType.BOOLEAN)) {
+                        || DataType.isNumberType(castType) || castType == DataType.BOOLEAN
+                        || castType == DataType.DATETIME)) {
                     // good
                 } 
                 else if (inputType == DataType.BYTEARRAY) {
diff --git a/src/org/apache/pig/impl/logicalLayer/schema/SchemaUtil.java b/src/org/apache/pig/impl/logicalLayer/schema/SchemaUtil.java
index eae60f428..c257adab8 100644
--- a/src/org/apache/pig/impl/logicalLayer/schema/SchemaUtil.java
+++ b/src/org/apache/pig/impl/logicalLayer/schema/SchemaUtil.java
@@ -50,6 +50,7 @@ public class SchemaUtil {
         SUPPORTED_TYPE_SET.add(DataType.BYTEARRAY);
         SUPPORTED_TYPE_SET.add(DataType.DOUBLE);
         SUPPORTED_TYPE_SET.add(DataType.FLOAT);
+        SUPPORTED_TYPE_SET.add(DataType.DATETIME);
         SUPPORTED_TYPE_SET.add(DataType.MAP);
     }
 
diff --git a/src/org/apache/pig/impl/util/CastUtils.java b/src/org/apache/pig/impl/util/CastUtils.java
index b38f2632d..309130a24 100644
--- a/src/org/apache/pig/impl/util/CastUtils.java
+++ b/src/org/apache/pig/impl/util/CastUtils.java
@@ -56,6 +56,7 @@ public class CastUtils {
 	    case (DataType.INTEGER): return caster.bytesToInteger(bytes);
 	    case (DataType.LONG): return caster.bytesToLong(bytes);
 	    case (DataType.BOOLEAN): return caster.bytesToBoolean(bytes);
+	    case (DataType.DATETIME): return caster.bytesToDateTime(bytes);
 	    case (DataType.MAP): return caster.bytesToMap(bytes);
 	    case (DataType.TUPLE): return caster.bytesToTuple(bytes, fieldSchema);
 	    default: throw new IOException("Unknown type " + dataType);
diff --git a/src/org/apache/pig/impl/util/NumValCarrier.java b/src/org/apache/pig/impl/util/NumValCarrier.java
index 619ee8a43..af519fda8 100644
--- a/src/org/apache/pig/impl/util/NumValCarrier.java
+++ b/src/org/apache/pig/impl/util/NumValCarrier.java
@@ -41,6 +41,7 @@ public class NumValCarrier {
         byteToStr.put(DataType.INTEGER,valCarrier);
         byteToStr.put(DataType.LONG,valCarrier);
         byteToStr.put(DataType.BOOLEAN,valCarrier);
+        byteToStr.put(DataType.DATETIME,valCarrier);
         byteToStr.put(DataType.MAP,mapCarrier);
         byteToStr.put(DataType.TUPLE,tupleCarrier);
         byteToStr.put(DataType.NULL,nullCarrier);
diff --git a/src/org/apache/pig/impl/util/StorageUtil.java b/src/org/apache/pig/impl/util/StorageUtil.java
index 0c76ddbd5..087651a76 100644
--- a/src/org/apache/pig/impl/util/StorageUtil.java
+++ b/src/org/apache/pig/impl/util/StorageUtil.java
@@ -23,6 +23,8 @@ import java.util.ArrayList;
 import java.util.Iterator;
 import java.util.Map;
 
+import org.joda.time.DateTime;
+
 import org.apache.hadoop.io.Text;
 import org.apache.pig.PigException;
 import org.apache.pig.backend.executionengine.ExecException;
@@ -135,6 +137,10 @@ public final class StorageUtil {
             out.write(((Double)field).toString().getBytes());
             break;
 
+        case DataType.DATETIME:
+            out.write(((DateTime)field).toString().getBytes());
+            break;
+
         case DataType.BYTEARRAY: 
             byte[] b = ((DataByteArray)field).get();
             out.write(b, 0, b.length);
diff --git a/src/org/apache/pig/newplan/logical/visitor/TypeCheckingExpVisitor.java b/src/org/apache/pig/newplan/logical/visitor/TypeCheckingExpVisitor.java
index 52fe5be58..d7a1370f2 100644
--- a/src/org/apache/pig/newplan/logical/visitor/TypeCheckingExpVisitor.java
+++ b/src/org/apache/pig/newplan/logical/visitor/TypeCheckingExpVisitor.java
@@ -353,6 +353,10 @@ public class TypeCheckingExpVisitor extends LogicalExpressionVisitor{
                 insertCast(binOp, biggerType, binOp.getRhs());
             }
         }
+        else if ( (lhsType == DataType.DATETIME) &&
+                (rhsType == DataType.DATETIME) ) {
+            // good
+        }
         else if ( (lhsType == DataType.CHARARRAY) &&
                 (rhsType == DataType.CHARARRAY) ) {
             // good
@@ -362,13 +366,13 @@ public class TypeCheckingExpVisitor extends LogicalExpressionVisitor{
             // good
         }
         else if ( (lhsType == DataType.BYTEARRAY) &&
-                ( (rhsType == DataType.CHARARRAY) || (DataType.isNumberType(rhsType)) || (rhsType == DataType.BOOLEAN))
+                ( (rhsType == DataType.CHARARRAY) || (DataType.isNumberType(rhsType)) || (rhsType == DataType.BOOLEAN) || (rhsType == DataType.DATETIME))
         ) {
             // Cast byte array to the type on rhs
             insertCast(binOp, rhsType, binOp.getLhs());
         }
         else if ( (rhsType == DataType.BYTEARRAY) &&
-                ( (lhsType == DataType.CHARARRAY) || (DataType.isNumberType(lhsType)) || (lhsType == DataType.BOOLEAN))
+                ( (lhsType == DataType.CHARARRAY) || (DataType.isNumberType(lhsType)) || (lhsType == DataType.BOOLEAN) || (lhsType == DataType.DATETIME))
         ) {
             // Cast byte array to the type on lhs
             insertCast(binOp, lhsType, binOp.getRhs());
@@ -559,12 +563,12 @@ public class TypeCheckingExpVisitor extends LogicalExpressionVisitor{
         } 
         else if ((lhsType == DataType.BYTEARRAY)
                 && ((rhsType == DataType.CHARARRAY) || (DataType
-                        .isNumberType(rhsType)))) {
+                        .isNumberType(rhsType))) || (rhsType == DataType.DATETIME)) { // need to add boolean as well
             // Cast byte array to the type on rhs
             insertCast(binCond, rhsType, binCond.getLhs());
         } else if ((rhsType == DataType.BYTEARRAY)
                 && ((lhsType == DataType.CHARARRAY) || (DataType
-                        .isNumberType(lhsType)))) {
+                        .isNumberType(lhsType)) || (rhsType == DataType.DATETIME))) { // need to add boolean as well
             // Cast byte array to the type on lhs
             insertCast(binCond, lhsType, binCond.getRhs());
         }
@@ -1197,7 +1201,7 @@ public class TypeCheckingExpVisitor extends LogicalExpressionVisitor{
         //Ordering here decides the score for the best fit function.
         //Do not change the order. Conversions to a smaller type is preferred
         //over conversion to a bigger type where ordering of types is:
-        //INTEGER, LONG, FLOAT, DOUBLE, CHARARRAY, TUPLE, BAG, MAP
+        //INTEGER, LONG, FLOAT, DOUBLE, DATETIME, CHARARRAY, TUPLE, BAG, MAP
         //from small to big
         
         List<Byte> boolToTypes = Arrays.asList(
@@ -1233,6 +1237,7 @@ public class TypeCheckingExpVisitor extends LogicalExpressionVisitor{
                 DataType.LONG,
                 DataType.FLOAT,
                 DataType.DOUBLE,
+                DataType.DATETIME,
                 DataType.CHARARRAY,
                 DataType.TUPLE,
                 DataType.BAG,
@@ -1353,6 +1358,9 @@ public class TypeCheckingExpVisitor extends LogicalExpressionVisitor{
         case DataType.BOOLEAN:
             kind = PigWarning.IMPLICIT_CAST_TO_BOOLEAN;
             break;
+        case DataType.DATETIME:
+            kind = PigWarning.IMPLICIT_CAST_TO_DATETIME;
+            break;
         case DataType.MAP:
             kind = PigWarning.IMPLICIT_CAST_TO_MAP;
             break;
diff --git a/src/org/apache/pig/parser/AliasMasker.g b/src/org/apache/pig/parser/AliasMasker.g
index 4155cb956..1c4e568f5 100644
--- a/src/org/apache/pig/parser/AliasMasker.g
+++ b/src/org/apache/pig/parser/AliasMasker.g
@@ -210,7 +210,7 @@ type : simple_type | tuple_type | bag_type | map_type
 ;
 
 simple_type 
-    : BOOLEAN | INT | LONG | FLOAT | DOUBLE | CHARARRAY | BYTEARRAY 
+    : BOOLEAN | INT | LONG | FLOAT | DOUBLE | DATETIME | CHARARRAY | BYTEARRAY 
 ;
 
 tuple_type 
@@ -636,6 +636,7 @@ eid : rel_str_op
     | LONG
     | FLOAT
     | DOUBLE
+    | DATETIME
     | CHARARRAY
     | BYTEARRAY
     | BAG
diff --git a/src/org/apache/pig/parser/AstPrinter.g b/src/org/apache/pig/parser/AstPrinter.g
index 950ee577d..3e788cefd 100644
--- a/src/org/apache/pig/parser/AstPrinter.g
+++ b/src/org/apache/pig/parser/AstPrinter.g
@@ -176,6 +176,7 @@ simple_type
     | LONG { sb.append($LONG.text); }
     | FLOAT { sb.append($FLOAT.text); }
     | DOUBLE { sb.append($DOUBLE.text); }
+    | DATETIME { sb.append($DATETIME.text); }
     | CHARARRAY { sb.append($CHARARRAY.text); }
     | BYTEARRAY { sb.append($BYTEARRAY.text); }
 ;
@@ -618,6 +619,7 @@ eid : rel_str_op
     | LONG      { sb.append($LONG.text); }
     | FLOAT     { sb.append($FLOAT.text); }
     | DOUBLE    { sb.append($DOUBLE.text); }
+    | DATETIME  { sb.append($DATETIME.text); }
     | CHARARRAY { sb.append($CHARARRAY.text); }
     | BYTEARRAY { sb.append($BYTEARRAY.text); }
     | BAG       { sb.append($BAG.text); }
diff --git a/src/org/apache/pig/parser/AstValidator.g b/src/org/apache/pig/parser/AstValidator.g
index a219457e6..e16e233da 100644
--- a/src/org/apache/pig/parser/AstValidator.g
+++ b/src/org/apache/pig/parser/AstValidator.g
@@ -230,6 +230,7 @@ simple_type returns [byte typev]
   | LONG { $typev = DataType.LONG; }
   | FLOAT { $typev = DataType.FLOAT; }
   | DOUBLE { $typev = DataType.DOUBLE; }
+  | DATETIME { $typev = DataType.DATETIME; }
   | CHARARRAY { $typev = DataType.CHARARRAY; }
   | BYTEARRAY { $typev = DataType.BYTEARRAY; }
 ;
@@ -628,6 +629,7 @@ eid : rel_str_op
     | LONG
     | FLOAT
     | DOUBLE
+    | DATETIME
     | CHARARRAY
     | BYTEARRAY
     | BAG
diff --git a/src/org/apache/pig/parser/LogicalPlanGenerator.g b/src/org/apache/pig/parser/LogicalPlanGenerator.g
index dc1207694..72dcbc1c1 100644
--- a/src/org/apache/pig/parser/LogicalPlanGenerator.g
+++ b/src/org/apache/pig/parser/LogicalPlanGenerator.g
@@ -386,6 +386,7 @@ simple_type returns[byte datatype]
  | LONG { $datatype = DataType.LONG; }
  | FLOAT { $datatype = DataType.FLOAT; }
  | DOUBLE { $datatype = DataType.DOUBLE; }
+ | DATETIME { $datatype = DataType.DATETIME; }
  | CHARARRAY { $datatype = DataType.CHARARRAY; }
  | BYTEARRAY { $datatype = DataType.BYTEARRAY; }
 ;
@@ -1733,6 +1734,7 @@ eid returns[String id] : rel_str_op { $id = $rel_str_op.id; }
     | LONG { $id = $LONG.text; }
     | FLOAT { $id = $FLOAT.text; }
     | DOUBLE { $id = $DOUBLE.text; }
+    | DATETIME { $id = $DATETIME.text; }
     | CHARARRAY { $id = $CHARARRAY.text; }
     | BYTEARRAY { $id = $BYTEARRAY.text; }
     | BAG { $id = $BAG.text; }
diff --git a/src/org/apache/pig/parser/QueryLexer.g b/src/org/apache/pig/parser/QueryLexer.g
index f795a0f79..be35e981a 100644
--- a/src/org/apache/pig/parser/QueryLexer.g
+++ b/src/org/apache/pig/parser/QueryLexer.g
@@ -180,6 +180,9 @@ FLOAT : 'FLOAT'
 DOUBLE : 'DOUBLE'
 ;
 
+DATETIME : 'DATETIME'
+;
+
 CHARARRAY : 'CHARARRAY'
 ;
 
diff --git a/src/org/apache/pig/parser/QueryParser.g b/src/org/apache/pig/parser/QueryParser.g
index 5b2f2ec54..9c0964572 100644
--- a/src/org/apache/pig/parser/QueryParser.g
+++ b/src/org/apache/pig/parser/QueryParser.g
@@ -317,7 +317,7 @@ field_def_list : field_def ( COMMA field_def )*
 type : simple_type | tuple_type | bag_type | map_type
 ;
 
-simple_type : BOOLEAN | INT | LONG | FLOAT | DOUBLE | CHARARRAY | BYTEARRAY
+simple_type : BOOLEAN | INT | LONG | FLOAT | DOUBLE | DATETIME | CHARARRAY | BYTEARRAY
 ;
 
 tuple_type : TUPLE? LEFT_PAREN field_def_list? RIGHT_PAREN
@@ -391,16 +391,13 @@ or_cond : and_cond  ( OR^ and_cond )*
 and_cond : unary_cond ( AND^ unary_cond )*
 ;
 
-unary_cond : expr rel_op^ expr
-           | LEFT_PAREN! cond RIGHT_PAREN!
-           | not_cond           
+unary_cond : LEFT_PAREN! cond RIGHT_PAREN!
+           | not_cond
+           | expr rel_op^ expr
            | func_eval
            | null_check_cond
-           | bool_cond           
 ;
 
-bool_cond: expr -> ^(BOOL_COND expr);
-
 not_cond : NOT^ unary_cond
 ;
 
@@ -767,6 +764,7 @@ eid : rel_str_op
     | LONG
     | FLOAT
     | DOUBLE
+    | DATETIME
     | CHARARRAY
     | BYTEARRAY
     | BAG
@@ -794,7 +792,6 @@ eid : rel_str_op
     | TRUE
     | FALSE
     | REALIAS
-    | BOOL_COND
 ;
 
 // relational operator
diff --git a/src/org/apache/pig/pen/AugmentBaseDataVisitor.java b/src/org/apache/pig/pen/AugmentBaseDataVisitor.java
index 84e58cf63..57dcb14bf 100644
--- a/src/org/apache/pig/pen/AugmentBaseDataVisitor.java
+++ b/src/org/apache/pig/pen/AugmentBaseDataVisitor.java
@@ -28,6 +28,8 @@ import java.util.HashSet;
 import java.util.Set;
 import java.util.Collection;
 
+import org.joda.time.DateTime;
+
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.pig.backend.executionengine.ExecException;
@@ -1212,6 +1214,19 @@ public class AugmentBaseDataVisitor extends LogicalRelationalNodesVisitor {
             return Float.valueOf((Float) v - 1);
         case DataType.DOUBLE:
             return Double.valueOf((Double) v - 1);
+        case DataType.DATETIME:
+            DateTime dt = (DateTime) v;
+            if (dt.getMillisOfSecond() != 0) {
+                return dt.minusMillis(1);
+            } else if (dt.getSecondOfMinute() != 0) {
+                return dt.minusSeconds(1);
+            } else if (dt.getMinuteOfHour() != 0) {
+                return dt.minusMinutes(1);
+            } else if (dt.getHourOfDay() != 0) {
+                return dt.minusHours(1);
+            } else {
+                return dt.minusDays(1);
+            }
         default:
             return null;
         }
@@ -1240,6 +1255,19 @@ public class AugmentBaseDataVisitor extends LogicalRelationalNodesVisitor {
             return Float.valueOf((Float) v + 1);
         case DataType.DOUBLE:
             return Double.valueOf((Double) v + 1);
+        case DataType.DATETIME:
+            DateTime dt = (DateTime) v;
+            if (dt.getMillisOfSecond() != 0) {
+                return dt.plusMillis(1);
+            } else if (dt.getSecondOfMinute() != 0) {
+                return dt.plusSeconds(1);
+            } else if (dt.getMinuteOfHour() != 0) {
+                return dt.plusMinutes(1);
+            } else if (dt.getHourOfDay() != 0) {
+                return dt.plusHours(1);
+            } else {
+                return dt.plusDays(1);
+            }
         default:
             return null;
         }
@@ -1265,6 +1293,8 @@ public class AugmentBaseDataVisitor extends LogicalRelationalNodesVisitor {
             return Integer.valueOf(data);
         case DataType.LONG:
             return Long.valueOf(data);
+        case DataType.DATETIME:
+            return new DateTime(data);
         case DataType.CHARARRAY:
             return data;
         default:
diff --git a/src/org/apache/pig/scripting/jruby/JrubyScriptEngine.java b/src/org/apache/pig/scripting/jruby/JrubyScriptEngine.java
index fff99d0b5..3e59e4e9f 100644
--- a/src/org/apache/pig/scripting/jruby/JrubyScriptEngine.java
+++ b/src/org/apache/pig/scripting/jruby/JrubyScriptEngine.java
@@ -171,6 +171,7 @@ public class JrubyScriptEngine extends ScriptEngine {
                 case DataType.FLOAT: canonicalName += "Float"; break;
                 case DataType.INTEGER: canonicalName += "Integer"; break;
                 case DataType.LONG: canonicalName += "Long"; break;
+                case DataType.DATETIME: canonicalName += "DateTime"; break;
                 case DataType.MAP: canonicalName += "Map"; break;
                 case DataType.BYTEARRAY: canonicalName += "DataByteArray"; break;
                 default: throw new ExecException("Unable to instantiate Algebraic EvalFunc " + method + " as schema type is invalid");
diff --git a/src/org/apache/pig/scripting/jruby/RubySchema.java b/src/org/apache/pig/scripting/jruby/RubySchema.java
index 460fcee51..5255bc7d0 100644
--- a/src/org/apache/pig/scripting/jruby/RubySchema.java
+++ b/src/org/apache/pig/scripting/jruby/RubySchema.java
@@ -294,6 +294,22 @@ public class RubySchema extends RubyObject {
        return makeNullAliasRubySchema(context, DataType.FLOAT);
     }
 
+    /**
+     * This is a static helper method to create a null aliased datetime Schema.
+     * This is useful in cases where you do not want the output to have an explicit
+     * name, which {@link Utils#getSchemaFromString} will assign.
+     *
+     * @param context the context the method is being executed in
+     * @param self    an instance of the RubyClass with metadata on
+     *                the Ruby class object this method is being
+     *                statically invoked against
+     * @return        a null-aliased bytearray schema
+     */
+    @JRubyMethod(meta = true, name = {"dt", "datetime"})
+    public static RubySchema nullDateTime(ThreadContext context, IRubyObject self) {
+       return makeNullAliasRubySchema(context, DataType.DATETIME);
+    }
+
     /**
      * This is a static helper method to create a null aliased tuple Schema.
      * This is useful in cases where you do not want the output to have an explicit
diff --git a/test/e2e/pig/udfs/java/org/apache/pig/test/udf/storefunc/PigPerformanceLoader.java b/test/e2e/pig/udfs/java/org/apache/pig/test/udf/storefunc/PigPerformanceLoader.java
index 6d30bc69a..f42653f2b 100644
--- a/test/e2e/pig/udfs/java/org/apache/pig/test/udf/storefunc/PigPerformanceLoader.java
+++ b/test/e2e/pig/udfs/java/org/apache/pig/test/udf/storefunc/PigPerformanceLoader.java
@@ -21,6 +21,8 @@ import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
 
+import org.joda.time.DateTime;
+
 import org.apache.pig.LoadCaster;
 import org.apache.pig.ResourceSchema.ResourceFieldSchema;
 import org.apache.pig.backend.executionengine.ExecException;
@@ -160,6 +162,11 @@ public class PigPerformanceLoader extends PigStorage {
             return helper.bytesToBoolean(arg0);
         }
 
+        @Override
+        public DateTime bytesToDateTime(byte[] arg0) throws IOException {
+            return helper.bytesToDateTime(arg0);
+        }
+
         @Override
         public Tuple bytesToTuple(byte[] arg0, ResourceFieldSchema fs) throws IOException {
             return helper.bytesToTuple(arg0, fs);
diff --git a/test/org/apache/pig/data/TestSchemaTuple.java b/test/org/apache/pig/data/TestSchemaTuple.java
index 3172c692f..24142c2e1 100644
--- a/test/org/apache/pig/data/TestSchemaTuple.java
+++ b/test/org/apache/pig/data/TestSchemaTuple.java
@@ -41,6 +41,8 @@ import java.util.Map;
 import java.util.Properties;
 import java.util.Random;
 
+import org.joda.time.DateTime;
+
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.mapreduce.InputSplit;
@@ -106,9 +108,9 @@ public class TestSchemaTuple {
         udfSchema = Utils.getSchemaFromString("((a:int,b:int),(a:int,b:int),(a:int,b:int)),((a:int,b:int),(a:int,b:int),(a:int,b:int))");
         SchemaTupleFrontend.registerToGenerateIfPossible(udfSchema, isAppendable, context);
 
-        udfSchema = Utils.getSchemaFromString("a:int, b:long, c:chararray, d:boolean, e:bytearray, f:float, g:double,"
-                +"(a:int, b:long, c:chararray, d:boolean, e:bytearray, f:float, g:double,"
-                +"(a:int, b:long, c:chararray, d:boolean, e:bytearray, f:float, g:double))");
+        udfSchema = Utils.getSchemaFromString("a:int, b:long, c:chararray, d:boolean, e:bytearray, f:float, g:double, dt:datetime"
+                +"(a:int, b:long, c:chararray, d:boolean, e:bytearray, f:float, g:double, dt:datetime"
+                +"(a:int, b:long, c:chararray, d:boolean, e:bytearray, f:float, g:double, dt:datetime))");
         SchemaTupleFrontend.registerToGenerateIfPossible(udfSchema, isAppendable, context);
 
         udfSchema = Utils.getSchemaFromString("boolean, boolean, boolean, boolean, boolean, boolean"
@@ -126,45 +128,45 @@ public class TestSchemaTuple {
                 + "boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean");
         SchemaTupleFrontend.registerToGenerateIfPossible(udfSchema, isAppendable, context);
 
-        udfSchema = Utils.getSchemaFromString("int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double))"
-                +"int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double))"
-                +"int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double))"
-                +"int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double))"
-                +"int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double))"
-                +"int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double))"
-                +"int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double))"
-                +"int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double))"
-                +"int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double))"
-                +"int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double))"
-                +"int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double))"
-                +"int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double))"
-                +"int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double))");
+        udfSchema = Utils.getSchemaFromString("int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime))"
+                +"int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime))"
+                +"int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime))"
+                +"int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime))"
+                +"int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime))"
+                +"int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime))"
+                +"int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime))"
+                +"int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime))"
+                +"int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime))"
+                +"int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime))"
+                +"int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime))"
+                +"int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime))"
+                +"int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime))");
         SchemaTupleFrontend.registerToGenerateIfPossible(udfSchema, isAppendable, context);
 
         isAppendable = true;
@@ -223,9 +225,9 @@ public class TestSchemaTuple {
         tf = SchemaTupleFactory.getInstance(udfSchema, isAppendable, context);
         putThroughPaces(tf, udfSchema, isAppendable);
 
-        udfSchema = Utils.getSchemaFromString("a:int, b:long, c:chararray, d:boolean, e:bytearray, f:float, g:double,"
-                +"(a:int, b:long, c:chararray, d:boolean, e:bytearray, f:float, g:double,"
-                +"(a:int, b:long, c:chararray, d:boolean, e:bytearray, f:float, g:double))");
+        udfSchema = Utils.getSchemaFromString("a:int, b:long, c:chararray, d:boolean, e:bytearray, f:float, g:double, dt: datetime"
+                +"(a:int, b:long, c:chararray, d:boolean, e:bytearray, f:float, g:double, dt: datetime,"
+                +"(a:int, b:long, c:chararray, d:boolean, e:bytearray, f:float, g:double, dt: datetime))");
         tf = SchemaTupleFactory.getInstance(udfSchema, isAppendable, context);
         putThroughPaces(tf, udfSchema, isAppendable);
 
@@ -245,45 +247,45 @@ public class TestSchemaTuple {
         tf = SchemaTupleFactory.getInstance(udfSchema, isAppendable, context);
         putThroughPaces(tf, udfSchema, isAppendable);
 
-        udfSchema = Utils.getSchemaFromString("int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double))"
-                +"int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double))"
-                +"int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double))"
-                +"int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double))"
-                +"int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double))"
-                +"int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double))"
-                +"int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double))"
-                +"int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double))"
-                +"int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double))"
-                +"int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double))"
-                +"int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double))"
-                +"int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double))"
-                +"int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double,"
-                +"(int, long, chararray, boolean, bytearray, float, double))");
+        udfSchema = Utils.getSchemaFromString("int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime))"
+                +"int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime))"
+                +"int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime))"
+                +"int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime))"
+                +"int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime))"
+                +"int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime))"
+                +"int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime))"
+                +"int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime))"
+                +"int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime))"
+                +"int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime))"
+                +"int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime))"
+                +"int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime))"
+                +"int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime,"
+                +"(int, long, chararray, boolean, bytearray, float, double, datetime))");
         tf = SchemaTupleFactory.getInstance(udfSchema, isAppendable, context);
         putThroughPaces(tf, udfSchema, isAppendable);
 
@@ -399,6 +401,7 @@ public class TestSchemaTuple {
         case DataType.LONG: return r.nextLong();
         case DataType.FLOAT: return r.nextFloat();
         case DataType.DOUBLE: return r.nextDouble();
+        case DataType.DATETIME: return new DateTime(r.nextLong());
         case DataType.BAG:
             DataBag db = BagFactory.getInstance().newDefaultBag();
             int sz = r.nextInt(100);
@@ -450,6 +453,7 @@ public class TestSchemaTuple {
                 case DataType.LONG: st.getLong(i); break;
                 case DataType.FLOAT: st.getFloat(i); break;
                 case DataType.DOUBLE: st.getDouble(i); break;
+                case DataType.DATETIME: st.getDateTime(i); break;
                 case DataType.TUPLE: st.getTuple(i); break;
                 case DataType.BAG: st.getDataBag(i); break;
                 case DataType.MAP: st.getMap(i); break;
diff --git a/test/org/apache/pig/test/TestAdd.java b/test/org/apache/pig/test/TestAdd.java
index 598630241..cdeabb3d7 100644
--- a/test/org/apache/pig/test/TestAdd.java
+++ b/test/org/apache/pig/test/TestAdd.java
@@ -23,6 +23,8 @@ import java.util.Random;
 
 import junit.framework.TestCase;
 
+import org.joda.time.DateTime;
+
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.data.DataBag;
 import org.apache.pig.data.DataByteArray;
@@ -54,7 +56,7 @@ public class TestAdd extends TestCase{
     public void testOperator() throws ExecException{
         //int TRIALS = 10;
         byte[] types = { DataType.BAG, DataType.BOOLEAN, DataType.BYTEARRAY, DataType.CHARARRAY, 
-                DataType.DOUBLE, DataType.FLOAT, DataType.INTEGER, DataType.LONG, DataType.MAP, DataType.TUPLE};
+                DataType.DOUBLE, DataType.FLOAT, DataType.INTEGER, DataType.LONG, DataType.DATETIME, DataType.MAP, DataType.TUPLE};
         //Map<Byte,String> map = GenRandomData.genTypeToNameMap();
         System.out.println("Testing Add operator");
         for(byte type : types) {
@@ -230,6 +232,25 @@ public class TestAdd extends TestCase{
                 assertEquals(null, (Long)resl.result);
                 break;
             }
+            case DataType.DATETIME:
+                DateTime inpdt1 = new DateTime(r.nextLong());
+                DateTime inpdt2 = new DateTime(r.nextLong());
+                lt.setValue(inpdt1);
+                rt.setValue(inpdt2);
+                Result resdt = op.getNext(inpdt1);
+                assertEquals(resdt.returnStatus, POStatus.STATUS_ERR);
+                
+                // test with null in lhs
+                lt.setValue(null);
+                rt.setValue(inpdt2);
+                resdt = op.getNext(inpdt1);
+                assertEquals(resdt.returnStatus, POStatus.STATUS_ERR);
+                // test with null in rhs
+                lt.setValue(inpdt1);
+                rt.setValue(null);
+                resdt = op.getNext(inpdt1);
+                assertEquals(resdt.returnStatus, POStatus.STATUS_ERR);
+                break;
             case DataType.MAP: {
                 Map<String,Object> inpm1 = GenRandomData.genRandMap(r, 10);
                 Map<String,Object> inpm2 = GenRandomData.genRandMap(r, 10);
diff --git a/test/org/apache/pig/test/TestBuiltin.java b/test/org/apache/pig/test/TestBuiltin.java
index 64b7b087a..8513c9245 100644
--- a/test/org/apache/pig/test/TestBuiltin.java
+++ b/test/org/apache/pig/test/TestBuiltin.java
@@ -32,7 +32,6 @@ import java.util.Iterator;
 import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
-import java.util.Properties;
 import java.util.Random;
 import java.util.Set;
 import java.util.StringTokenizer;
@@ -45,14 +44,27 @@ import org.apache.pig.PigServer;
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
 import org.apache.pig.builtin.ARITY;
+import org.apache.pig.builtin.AddDuration;
 import org.apache.pig.builtin.BagSize;
 import org.apache.pig.builtin.CONCAT;
 import org.apache.pig.builtin.COR;
 import org.apache.pig.builtin.COUNT;
 import org.apache.pig.builtin.COUNT_STAR;
 import org.apache.pig.builtin.COV;
+import org.apache.pig.builtin.CurrentTime;
 import org.apache.pig.builtin.DIFF;
+import org.apache.pig.builtin.DaysBetween;
 import org.apache.pig.builtin.Distinct;
+import org.apache.pig.builtin.GetDay;
+import org.apache.pig.builtin.GetHour;
+import org.apache.pig.builtin.GetMilliSecond;
+import org.apache.pig.builtin.GetMinute;
+import org.apache.pig.builtin.GetMonth;
+import org.apache.pig.builtin.GetSecond;
+import org.apache.pig.builtin.GetWeek;
+import org.apache.pig.builtin.GetWeekYear;
+import org.apache.pig.builtin.GetYear;
+import org.apache.pig.builtin.HoursBetween;
 import org.apache.pig.builtin.INDEXOF;
 import org.apache.pig.builtin.INVERSEMAP;
 import org.apache.pig.builtin.KEYSET;
@@ -60,6 +72,9 @@ import org.apache.pig.builtin.LAST_INDEX_OF;
 import org.apache.pig.builtin.LCFIRST;
 import org.apache.pig.builtin.LOWER;
 import org.apache.pig.builtin.MapSize;
+import org.apache.pig.builtin.MilliSecondsBetween;
+import org.apache.pig.builtin.MinutesBetween;
+import org.apache.pig.builtin.MonthsBetween;
 import org.apache.pig.builtin.PigStorage;
 import org.apache.pig.builtin.REGEX_EXTRACT;
 import org.apache.pig.builtin.REGEX_EXTRACT_ALL;
@@ -67,8 +82,10 @@ import org.apache.pig.builtin.REPLACE;
 import org.apache.pig.builtin.SIZE;
 import org.apache.pig.builtin.STRSPLIT;
 import org.apache.pig.builtin.SUBSTRING;
+import org.apache.pig.builtin.SecondsBetween;
 import org.apache.pig.builtin.StringConcat;
 import org.apache.pig.builtin.StringSize;
+import org.apache.pig.builtin.SubtractDuration;
 import org.apache.pig.builtin.TOBAG;
 import org.apache.pig.builtin.TOKENIZE;
 import org.apache.pig.builtin.TOMAP;
@@ -76,11 +93,20 @@ import org.apache.pig.builtin.TOP;
 import org.apache.pig.builtin.TOTUPLE;
 import org.apache.pig.builtin.TRIM;
 import org.apache.pig.builtin.TextLoader;
+import org.apache.pig.builtin.ToDate;
+import org.apache.pig.builtin.ToDate2ARGS;
+import org.apache.pig.builtin.ToDate3ARGS;
+import org.apache.pig.builtin.ToDateISO;
+import org.apache.pig.builtin.ToMilliSeconds;
+import org.apache.pig.builtin.ToString;
+import org.apache.pig.builtin.ToUnixTime;
 import org.apache.pig.builtin.TupleSize;
 import org.apache.pig.builtin.UCFIRST;
 import org.apache.pig.builtin.UPPER;
 import org.apache.pig.builtin.VALUELIST;
 import org.apache.pig.builtin.VALUESET;
+import org.apache.pig.builtin.WeeksBetween;
+import org.apache.pig.builtin.YearsBetween;
 import org.apache.pig.data.BagFactory;
 import org.apache.pig.data.DataBag;
 import org.apache.pig.data.DataByteArray;
@@ -94,6 +120,8 @@ import org.apache.pig.impl.logicalLayer.FrontendException;
 import org.apache.pig.impl.logicalLayer.schema.Schema;
 import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;
 import org.apache.pig.impl.logicalLayer.validators.TypeCheckerException;
+import org.joda.time.DateTime;
+import org.joda.time.DateTimeZone;
 import org.junit.AfterClass;
 import org.junit.Assert;
 import org.junit.Before;
@@ -290,6 +318,9 @@ public class TestBuiltin {
         }catch(ExecException e) {
             e.printStackTrace();
         }
+
+
+        DateTimeZone.setDefault(DateTimeZone.forOffsetMillis(DateTimeZone.UTC.getOffset(null)));
     }
 
     @AfterClass
@@ -315,6 +346,117 @@ public class TestBuiltin {
         }
     }
 
+    @Test
+    public void testAddSubtractDuration() throws Exception {
+        AddDuration func1 = new AddDuration();
+        SubtractDuration func2 = new SubtractDuration();
+
+        Tuple t1 = TupleFactory.getInstance().newTuple(2);
+        t1.set(0, new DateTime("2009-01-07T01:07:01.000Z"));
+        t1.set(1, "PT1S");
+        Tuple t2 = TupleFactory.getInstance().newTuple(2);
+        t2.set(0, new DateTime("2008-02-06T02:06:02.000Z"));
+        t2.set(1, "PT1M");
+        Tuple t3 = TupleFactory.getInstance().newTuple(2);
+        t3.set(0, new DateTime("2007-03-05T03:05:03.000Z"));
+        t3.set(1, "P1D");
+        
+        assertEquals(func1.exec(t1), new DateTime("2009-01-07T01:07:02.000Z"));
+        assertEquals(func1.exec(t2), new DateTime("2008-02-06T02:07:02.000Z"));
+        assertEquals(func1.exec(t3), new DateTime("2007-03-06T03:05:03.000Z"));
+        assertEquals(func2.exec(t1), new DateTime("2009-01-07T01:07:00.000Z"));
+        assertEquals(func2.exec(t2), new DateTime("2008-02-06T02:05:02.000Z"));
+        assertEquals(func2.exec(t3), new DateTime("2007-03-04T03:05:03.000Z"));
+    }
+
+    @Test
+    public void testConversionBetweenDateTimeAndString() throws Exception {
+        ToDate func1 = new ToDate();
+        Tuple t1 = TupleFactory.getInstance().newTuple(1);
+        t1.set(0, 1231290421000L);
+        DateTime dt1 = func1.exec(t1);
+        assertEquals(dt1, new DateTime("2009-01-07T01:07:01.000Z"));
+
+        ToDateISO func2 = new ToDateISO();
+        Tuple t2 = TupleFactory.getInstance().newTuple(1);
+        t2.set(0, "2009-01-07T01:07:01.000Z");
+        DateTime dt2 = func2.exec(t2);
+        assertEquals(dt2, new DateTime("2009-01-07T01:07:01.000Z"));
+
+        Tuple t3 = TupleFactory.getInstance().newTuple(1);
+        t3.set(0, "2009-01-07T01:07:01.000+08:00");
+        DateTime dt3 = func2.exec(t3);
+        assertEquals(dt3, new DateTime("2009-01-07T01:07:01.000+08:00", DateTimeZone.forID("+08:00")));
+
+        ToDate2ARGS func3 = new ToDate2ARGS();        
+        Tuple t4 = TupleFactory.getInstance().newTuple(2);
+        t4.set(0, "2009.01.07 AD at 01:07:01");
+        t4.set(1, "yyyy.MM.dd G 'at' HH:mm:ss");
+        DateTime dt4 = func3.exec(t4);
+        assertEquals(dt4, new DateTime("2009-01-07T01:07:01.000Z"));
+
+        Tuple t5 = TupleFactory.getInstance().newTuple(2);
+        t5.set(0, "2009.01.07 AD at 01:07:01 +0800");
+        t5.set(1, "yyyy.MM.dd G 'at' HH:mm:ss Z");
+        DateTime dt5 = func3.exec(t5);
+        assertEquals(dt5, new DateTime("2009-01-07T01:07:01.000+08:00"));
+        
+        ToDate3ARGS func4 = new ToDate3ARGS();        
+        Tuple t6 = TupleFactory.getInstance().newTuple(3);
+        t6.set(0, "2009.01.07 AD at 01:07:01");
+        t6.set(1, "yyyy.MM.dd G 'at' HH:mm:ss");
+        t6.set(2, "+00:00");
+        DateTime dt6 = func4.exec(t6);
+        assertEquals(dt6, new DateTime("2009-01-07T01:07:01.000Z", DateTimeZone.forID("+00:00")));
+
+        Tuple t7 = TupleFactory.getInstance().newTuple(3);
+        t7.set(0, "2009.01.07 AD at 01:07:01 +0800");
+        t7.set(1, "yyyy.MM.dd G 'at' HH:mm:ss Z");
+        t7.set(2, "asia/singapore");
+        DateTime dt7 = func4.exec(t7);
+        assertEquals(dt7, new DateTime("2009-01-07T01:07:01.000+08:00", DateTimeZone.forID("+08:00")));
+
+        ToUnixTime func5 = new ToUnixTime();
+        Tuple t8 = TupleFactory.getInstance().newTuple(1);
+        t8.set(0, new DateTime(1231290421000L));
+        Long ut1 = func5.exec(t8);
+        assertEquals(ut1.longValue(), 1231290421L);
+
+        ToString func6 = new ToString();
+
+        Tuple t9 = TupleFactory.getInstance().newTuple(1);
+        t9.set(0, new DateTime("2009-01-07T01:07:01.000Z"));
+        String dtStr1 = func6.exec(t9);
+        assertEquals(dtStr1, "2009-01-07T01:07:01.000Z");
+
+        Tuple t10 = TupleFactory.getInstance().newTuple(1);
+        t10.set(0, new DateTime("2009-01-07T09:07:01.000+08:00"));
+        String dtStr2 = func6.exec(t10);
+        assertEquals(dtStr2, "2009-01-07T01:07:01.000Z");
+
+        Tuple t11 = TupleFactory.getInstance().newTuple(2);
+        t11.set(0, new DateTime("2009-01-07T01:07:01.000Z"));
+        t11.set(1, "yyyy.MM.dd G 'at' HH:mm:ss");
+        String dtStr3 = func6.exec(t11);
+        assertEquals(dtStr3, "2009.01.07 AD at 01:07:01");
+
+        Tuple t12 = TupleFactory.getInstance().newTuple(2);
+        t12.set(0, new DateTime("2009-01-07T01:07:01.000+08:00", DateTimeZone.forID("+08:00")));
+        t12.set(1, "yyyy.MM.dd G 'at' HH:mm:ss Z");
+        String dtStr4 = func6.exec(t12);
+        assertEquals(dtStr4, "2009.01.07 AD at 01:07:01 +0800");
+        
+        ToMilliSeconds func7 = new ToMilliSeconds();
+        Tuple t13 = TupleFactory.getInstance().newTuple(1);
+        t13.set(0, new DateTime(1231290421000L));
+        Long ut2 = func7.exec(t11);
+        assertEquals(ut2.longValue(), 1231290421000L);
+        
+        CurrentTime func8 = new CurrentTime();
+        DateTime dt11 = func8.exec(null);
+        Assert.assertNotNull(dt11);
+    }
+
     /**
      * Test the case where the combiner is not called - so initial is called
      * and then final is called
@@ -2538,4 +2680,113 @@ public class TestBuiltin {
         assertEquals((String)resultList.get(1), "source");
     }
 
+    @Test
+    public void testDiffDateTime() throws Exception {
+        Tuple t = TupleFactory.getInstance().newTuple(2);
+        t.set(0, new DateTime("2009-01-07T00:00:00.000Z"));
+        t.set(1, new DateTime("2002-01-01T00:00:00.000Z"));
+
+        YearsBetween func1 = new YearsBetween();
+        Long years = func1.exec(t);
+        System.out.println("Years: " + years.toString());
+        Assert.assertEquals(years.longValue(), 7L);
+        
+        MonthsBetween func2 = new MonthsBetween();
+        Long months = func2.exec(t);
+        System.out.println("Months: " + months.toString());
+        Assert.assertEquals(months.longValue(),84L);
+        
+        WeeksBetween func3 = new WeeksBetween();
+        Long weeks = func3.exec(t);
+        System.out.println("Weeks: " + weeks.toString());
+        Assert.assertEquals(weeks.longValue(), 366L);
+
+        DaysBetween func4 = new DaysBetween();
+        Long days = func4.exec(t);
+        System.out.println("Days: " + days.toString());
+        Assert.assertEquals(days.longValue(), 2563L);
+
+        HoursBetween func5 = new HoursBetween();
+        Long hours = func5.exec(t);
+        System.out.println("Hours: " + hours.toString());
+        Assert.assertEquals(hours.longValue(), 61512L);
+
+        MinutesBetween func6 = new MinutesBetween();
+        Long mins = func6.exec(t);
+        System.out.println("Minutes: " + mins.toString());
+        Assert.assertEquals(mins.longValue(), 3690720L);
+
+        SecondsBetween func7 = new SecondsBetween();
+        Long secs = func7.exec(t);
+        System.out.println("Seconds: " + secs.toString());
+        Assert.assertEquals(secs.longValue(), 221443200L);
+
+        MilliSecondsBetween func8 = new MilliSecondsBetween();
+        Long millis = func8.exec(t);
+        System.out.println("MilliSeconds: " + millis.toString());
+        Assert.assertEquals(millis.longValue(), 221443200000L);
+    }
+
+    @Test
+    public void testGetDateTimeField() throws Exception {
+        Tuple t1 = TupleFactory.getInstance().newTuple(1);
+        t1.set(0, new DateTime("2010-04-15T08:11:33.020Z"));
+        Tuple t2 = TupleFactory.getInstance().newTuple(1);
+        t2.set(0, new DateTime("2010-04-15T08:11:33.020+08:00"));
+        
+        GetYear func1 = new GetYear();
+        Integer year = func1.exec(t1);
+        assertEquals(year.intValue(), 2010);
+        year = func1.exec(t2);
+        assertEquals(year.intValue(), 2010);
+
+        GetMonth func2 = new GetMonth();
+        Integer month = func2.exec(t1);
+        assertEquals(month.intValue(), 4);
+        month = func2.exec(t2);
+        assertEquals(month.intValue(), 4);
+        
+        GetDay func3 = new GetDay();
+        Integer day = func3.exec(t1);
+        assertEquals(day.intValue(), 15);
+        day = func3.exec(t2);
+        assertEquals(day.intValue(), 15);
+        
+        GetHour func4 = new GetHour();
+        Integer hour = func4.exec(t1);
+        assertEquals(hour.intValue(), 8);
+        hour = func4.exec(t2);
+        assertEquals(hour.intValue(), 0);
+        
+        GetMinute func5 = new GetMinute();
+        Integer minute = func5.exec(t1);
+        assertEquals(minute.intValue(), 11);
+        minute = func5.exec(t2);
+        assertEquals(minute.intValue(), 11);
+        
+        GetSecond func6 = new GetSecond();
+        Integer second = func6.exec(t1);
+        assertEquals(second.intValue(), 33);
+        second = func6.exec(t2);
+        assertEquals(second.intValue(), 33);
+        
+        GetMilliSecond func7 = new GetMilliSecond();
+        Integer milli = func7.exec(t1);
+        assertEquals(milli.intValue(), 20);
+        milli = func7.exec(t2);
+        assertEquals(milli.intValue(), 20);
+
+        GetWeekYear func8 = new GetWeekYear();
+        Integer weekyear = func8.exec(t1);
+        assertEquals(weekyear.intValue(), 2010);
+        weekyear = func8.exec(t2);
+        assertEquals(weekyear.intValue(), 2010);
+        
+        GetWeek func9 = new GetWeek();
+        Integer week = func9.exec(t1);
+        assertEquals(week.intValue(), 15);
+        week = func9.exec(t2);
+        assertEquals(week.intValue(), 15);
+    }
+
 }
diff --git a/test/org/apache/pig/test/TestConversions.java b/test/org/apache/pig/test/TestConversions.java
index e238b1e8c..152ad5cc1 100644
--- a/test/org/apache/pig/test/TestConversions.java
+++ b/test/org/apache/pig/test/TestConversions.java
@@ -22,6 +22,9 @@ import java.util.Random;
 import java.util.Map;
 import java.io.IOException;
 
+import org.joda.time.DateTime;
+import org.joda.time.DateTimeZone;
+
 import org.apache.pig.ResourceSchema;
 import org.apache.pig.ResourceSchema.ResourceFieldSchema;
 import org.apache.pig.builtin.PigStorage;
@@ -52,6 +55,11 @@ public class TestConversions extends TestCase {
 	Random r = new Random();
 	final int MAX = 10;
 
+	@Override
+	public void setUp() {
+	    DateTimeZone.setDefault(DateTimeZone.forOffsetMillis(DateTimeZone.UTC.getOffset(null)));
+	}
+
     @Test
     public void testBytesToBoolean() throws IOException {
         // valid booleans
@@ -245,6 +253,12 @@ public class TestConversions extends TestCase {
         Double d = r.nextDouble();
         assertTrue(DataType.equalByteArrays(d.toString().getBytes(), ((Utf8StorageConverter)ps.getLoadCaster()).toBytes(d)));
     }
+
+    @Test
+    public void testDateTimeToBytes() throws IOException {
+        DateTime dt = new DateTime(r.nextLong());
+        assertTrue(DataType.equalByteArrays(dt.toString().getBytes(), ((Utf8StorageConverter)ps.getLoadCaster()).toBytes(dt)));
+    }
         
     @Test
     public void testCharArrayToBytes() throws IOException {
diff --git a/test/org/apache/pig/test/TestDivide.java b/test/org/apache/pig/test/TestDivide.java
index cc9df70bf..be7c6a382 100644
--- a/test/org/apache/pig/test/TestDivide.java
+++ b/test/org/apache/pig/test/TestDivide.java
@@ -23,6 +23,8 @@ import java.util.Random;
 
 import junit.framework.TestCase;
 
+import org.joda.time.DateTime;
+
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.data.DataBag;
 import org.apache.pig.data.DataByteArray;
@@ -54,7 +56,7 @@ public class TestDivide extends TestCase{
     public void testOperator() throws ExecException{
         //int TRIALS = 10;
         byte[] types = { DataType.BAG, DataType.BOOLEAN, DataType.BYTEARRAY, DataType.CHARARRAY, 
-                DataType.DOUBLE, DataType.FLOAT, DataType.INTEGER, DataType.LONG, DataType.MAP, DataType.TUPLE};
+                DataType.DOUBLE, DataType.FLOAT, DataType.INTEGER, DataType.LONG, DataType.DATETIME, DataType.MAP, DataType.TUPLE};
         //Map<Byte,String> map = GenRandomData.genTypeToNameMap();
         System.out.println("Testing DIVIDE operator");
         for(byte type : types) {
@@ -250,6 +252,25 @@ public class TestDivide extends TestCase{
                 assertEquals(null, (Long)resl.result);
                 break;
             }
+            case DataType.DATETIME:
+                DateTime inpdt1 = new DateTime(r.nextLong());
+                DateTime inpdt2 = new DateTime(r.nextLong());
+                lt.setValue(inpdt1);
+                rt.setValue(inpdt2);
+                Result resdt = op.getNext(inpdt1);
+                assertEquals(resdt.returnStatus, POStatus.STATUS_ERR);
+                
+                // test with null in lhs
+                lt.setValue(null);
+                rt.setValue(inpdt2);
+                resdt = op.getNext(inpdt1);
+                assertEquals(resdt.returnStatus, POStatus.STATUS_ERR);
+                // test with null in rhs
+                lt.setValue(inpdt1);
+                rt.setValue(null);
+                resdt = op.getNext(inpdt1);
+                assertEquals(resdt.returnStatus, POStatus.STATUS_ERR);
+                break;
             case DataType.MAP: {
                 Map<String,Object> inpm1 = GenRandomData.genRandMap(r, 10);
                 Map<String,Object> inpm2 = GenRandomData.genRandMap(r, 10);
diff --git a/test/org/apache/pig/test/TestEqualTo.java b/test/org/apache/pig/test/TestEqualTo.java
index 894bdc600..edefa3544 100644
--- a/test/org/apache/pig/test/TestEqualTo.java
+++ b/test/org/apache/pig/test/TestEqualTo.java
@@ -21,6 +21,8 @@ import java.util.HashMap;
 import java.util.Map;
 import java.util.Random;
 
+import org.joda.time.DateTime;
+
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.data.DataBag;
 import org.apache.pig.data.DataByteArray;
@@ -239,6 +241,44 @@ public class TestEqualTo extends junit.framework.TestCase {
 	    
 	}
 
+    @Test
+    public void testDateTimeNe() throws Exception {
+        ConstantExpression lt = GenPhyOp.exprConst();
+        lt.setValue(new DateTime(1L));
+        ConstantExpression rt = GenPhyOp.exprConst();
+        rt.setValue(new DateTime(0L));
+        EqualToExpr g = GenPhyOp.compEqualToExpr();
+        g.setLhs(lt);
+        g.setRhs(rt);
+        g.setOperandType(DataType.DATETIME);
+        Result r = g.getNext(new Boolean(true));
+        assertEquals(POStatus.STATUS_OK, r.returnStatus);
+        assertFalse((Boolean)r.result);
+    }
+
+    @Test
+    public void testDateTimeEq() throws Exception {
+        ConstantExpression lt = GenPhyOp.exprConst();
+        lt.setValue(new DateTime(1L));
+        ConstantExpression rt = GenPhyOp.exprConst();
+        rt.setValue(new DateTime(1L));
+        EqualToExpr g = GenPhyOp.compEqualToExpr();
+        g.setLhs(lt);
+        g.setRhs(rt);
+        g.setOperandType(DataType.DATETIME);
+        Result r = g.getNext(new Boolean(true));
+        assertEquals(POStatus.STATUS_OK, r.returnStatus);
+        assertTrue((Boolean)r.result);
+    }
+
+    
+    @Test
+    public void testDateTimeAndNullValues() throws Exception {
+        
+        checkNullValues(  DataType.DATETIME,  new DateTime(1L) );
+
+    }
+
     @Test
     public void testStringNe() throws Exception {
         ConstantExpression lt = GenPhyOp.exprConst();
diff --git a/test/org/apache/pig/test/TestGTOrEqual.java b/test/org/apache/pig/test/TestGTOrEqual.java
index fb0d4da84..a35bc7498 100644
--- a/test/org/apache/pig/test/TestGTOrEqual.java
+++ b/test/org/apache/pig/test/TestGTOrEqual.java
@@ -20,6 +20,8 @@ package org.apache.pig.test;
 import java.util.Map;
 import java.util.Random;
 
+import org.joda.time.DateTime;
+
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.data.DataBag;
 import org.apache.pig.data.DataByteArray;
@@ -253,6 +255,56 @@ public class TestGTOrEqual extends junit.framework.TestCase {
     	checkNullValues( DataType.DOUBLE,  new Double(1.0) );
     }
 
+    @Test
+    public void testDateTimeGt() throws Exception {
+        ConstantExpression lt = GenPhyOp.exprConst();
+        lt.setValue(new DateTime(1L));
+        ConstantExpression rt = GenPhyOp.exprConst();
+        rt.setValue(new DateTime(0L));
+        GTOrEqualToExpr g = GenPhyOp.compGTOrEqualToExpr();
+        g.setLhs(lt);
+        g.setRhs(rt);
+        g.setOperandType(DataType.DATETIME);
+        Result r = g.getNext(new Boolean(true));
+        assertEquals(POStatus.STATUS_OK, r.returnStatus);
+        assertTrue((Boolean)r.result);
+    }
+
+    @Test
+    public void testDateTimeLt() throws Exception {
+        ConstantExpression lt = GenPhyOp.exprConst();
+        lt.setValue(new DateTime(0L));
+        ConstantExpression rt = GenPhyOp.exprConst();
+        rt.setValue(new DateTime(1L));
+        GTOrEqualToExpr g = GenPhyOp.compGTOrEqualToExpr();
+        g.setLhs(lt);
+        g.setRhs(rt);
+        g.setOperandType(DataType.DATETIME);
+        Result r = g.getNext(new Boolean(true));
+        assertEquals(POStatus.STATUS_OK, r.returnStatus);
+        assertFalse((Boolean)r.result);
+    }
+
+    @Test
+    public void testDateTimeEq() throws Exception {
+        ConstantExpression lt = GenPhyOp.exprConst();
+        lt.setValue(new DateTime(1L));
+        ConstantExpression rt = GenPhyOp.exprConst();
+        rt.setValue(new DateTime(1L));
+        GTOrEqualToExpr g = GenPhyOp.compGTOrEqualToExpr();
+        g.setLhs(lt);
+        g.setRhs(rt);
+        g.setOperandType(DataType.DATETIME);
+        Result r = g.getNext(new Boolean(true));
+        assertEquals(POStatus.STATUS_OK, r.returnStatus);
+        assertTrue((Boolean)r.result);
+    }
+
+    @Test
+    public void testDateTimeAndNullValues() throws Exception {
+        
+        checkNullValues( DataType.DATETIME,  new DateTime(1L) );
+    }
 
     @Test
     public void testStringGt() throws Exception {
diff --git a/test/org/apache/pig/test/TestGreaterThan.java b/test/org/apache/pig/test/TestGreaterThan.java
index 897368b38..d5229755f 100644
--- a/test/org/apache/pig/test/TestGreaterThan.java
+++ b/test/org/apache/pig/test/TestGreaterThan.java
@@ -20,6 +20,8 @@ package org.apache.pig.test;
 import java.util.Map;
 import java.util.Random;
 
+import org.joda.time.DateTime;
+
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.data.DataBag;
 import org.apache.pig.data.DataByteArray;
@@ -264,7 +266,63 @@ public class TestGreaterThan extends junit.framework.TestCase {
 	    checkNullValues(   DataType.DOUBLE,  new Double(1.0) );
 	    
 	}
+
+    @Test
+    public void testDateTimeGt() throws Exception {
+        ConstantExpression lt = GenPhyOp.exprConst();
+        lt.setValue(new DateTime(1L));
+        ConstantExpression rt = GenPhyOp.exprConst();
+        rt.setValue(new DateTime(0L));
+        GreaterThanExpr g = GenPhyOp.compGreaterThanExpr();
+        g.setLhs(lt);
+        g.setRhs(rt);
+        g.setOperandType(DataType.DATETIME);
+        Result r = g.getNext(new Boolean(true));
+        assertEquals(POStatus.STATUS_OK, r.returnStatus);
+        assertTrue((Boolean)r.result);
+    }
+
+
+    @Test
+    public void testDateTimeLt() throws Exception {
+        ConstantExpression lt = GenPhyOp.exprConst();
+        lt.setValue(new DateTime(0L));
+        ConstantExpression rt = GenPhyOp.exprConst();
+        rt.setValue(new DateTime(1L));
+        GreaterThanExpr g = GenPhyOp.compGreaterThanExpr();
+        g.setLhs(lt);
+        g.setRhs(rt);
+        g.setOperandType(DataType.DATETIME);
+        Result r = g.getNext(new Boolean(true));
+        assertEquals(POStatus.STATUS_OK, r.returnStatus);
+        assertFalse((Boolean)r.result);
+    }
+
+  
     
+    @Test
+    public void testDateTimeEq() throws Exception {
+        ConstantExpression lt = GenPhyOp.exprConst();
+        lt.setValue(new DateTime(1L));
+        ConstantExpression rt = GenPhyOp.exprConst();
+        rt.setValue(new DateTime(1L));
+        GreaterThanExpr g = GenPhyOp.compGreaterThanExpr();
+        g.setLhs(lt);
+        g.setRhs(rt);
+        g.setOperandType(DataType.DATETIME);
+        Result r = g.getNext(new Boolean(true));
+        assertEquals(POStatus.STATUS_OK, r.returnStatus);
+        assertFalse((Boolean)r.result);
+    }
+
+
+    @Test
+    public void testDateTimeAndNullValues() throws Exception {
+        
+        checkNullValues(  DataType.DATETIME,  new DateTime(1L) );
+
+    }
+
 	@Test
     public void testStringGt() throws Exception {
         ConstantExpression lt = GenPhyOp.exprConst();
diff --git a/test/org/apache/pig/test/TestLTOrEqual.java b/test/org/apache/pig/test/TestLTOrEqual.java
index 1f05f209c..6b7c7955c 100644
--- a/test/org/apache/pig/test/TestLTOrEqual.java
+++ b/test/org/apache/pig/test/TestLTOrEqual.java
@@ -20,6 +20,8 @@ package org.apache.pig.test;
 import java.util.Map;
 import java.util.Random;
 
+import org.joda.time.DateTime;
+
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.data.DataBag;
 import org.apache.pig.data.DataByteArray;
@@ -250,6 +252,56 @@ public class TestLTOrEqual extends junit.framework.TestCase {
 		checkNullValues(  DataType.DOUBLE, new Double(1.0) );		    
 	}
 
+    @Test
+    public void testDateTimeGt() throws Exception {
+        ConstantExpression lt = GenPhyOp.exprConst();
+        lt.setValue(new DateTime(1L));
+        ConstantExpression rt = GenPhyOp.exprConst();
+        rt.setValue(new DateTime(0L));
+        LTOrEqualToExpr g = GenPhyOp.compLTOrEqualToExpr();
+        g.setLhs(lt);
+        g.setRhs(rt);
+        g.setOperandType(DataType.DATETIME);
+        Result r = g.getNext(new Boolean(true));
+        assertEquals(POStatus.STATUS_OK, r.returnStatus);
+        assertFalse((Boolean)r.result);
+    }
+
+    @Test
+    public void testDateTimeLt() throws Exception {
+        ConstantExpression lt = GenPhyOp.exprConst();
+        lt.setValue(new DateTime(0L));
+        ConstantExpression rt = GenPhyOp.exprConst();
+        rt.setValue(new DateTime(1L));
+        LTOrEqualToExpr g = GenPhyOp.compLTOrEqualToExpr();
+        g.setLhs(lt);
+        g.setRhs(rt);
+        g.setOperandType(DataType.DATETIME);
+        Result r = g.getNext(new Boolean(true));
+        assertEquals(POStatus.STATUS_OK, r.returnStatus);
+        assertTrue((Boolean)r.result);
+    }
+
+    @Test
+    public void testDateTimeEq() throws Exception {
+        ConstantExpression lt = GenPhyOp.exprConst();
+        lt.setValue(new DateTime(1L));
+        ConstantExpression rt = GenPhyOp.exprConst();
+        rt.setValue(new DateTime(1L));
+        LTOrEqualToExpr g = GenPhyOp.compLTOrEqualToExpr();
+        g.setLhs(lt);
+        g.setRhs(rt);
+        g.setOperandType(DataType.DATETIME);
+        Result r = g.getNext(new Boolean(true));
+        assertEquals(POStatus.STATUS_OK, r.returnStatus);
+        assertTrue((Boolean)r.result);
+    }
+
+    @Test
+    public void testDateTimeAndNullValues() throws Exception {
+        checkNullValues(  DataType.DATETIME, new DateTime(1L) );            
+    }
+
     @Test
     public void testStringGt() throws Exception {
         ConstantExpression lt = GenPhyOp.exprConst();
diff --git a/test/org/apache/pig/test/TestLessThan.java b/test/org/apache/pig/test/TestLessThan.java
index 1c6cfd7ab..7df1dc137 100644
--- a/test/org/apache/pig/test/TestLessThan.java
+++ b/test/org/apache/pig/test/TestLessThan.java
@@ -20,6 +20,8 @@ package org.apache.pig.test;
 import java.util.Map;
 import java.util.Random;
 
+import org.joda.time.DateTime;
+
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.data.DataBag;
 import org.apache.pig.data.DataByteArray;
@@ -254,6 +256,58 @@ public class TestLessThan extends junit.framework.TestCase {
     	checkNullValues( DataType.DOUBLE, new Double(1.0) );
     }  
 
+    @Test
+    public void testDateTimeGt() throws Exception {
+        ConstantExpression lt = GenPhyOp.exprConst();
+        lt.setValue(new DateTime(1L));
+        ConstantExpression rt = GenPhyOp.exprConst();
+        rt.setValue(new DateTime(0L));
+        LessThanExpr g = GenPhyOp.compLessThanExpr();
+        g.setLhs(lt);
+        g.setRhs(rt);
+        g.setOperandType(DataType.DATETIME);
+        Result r = g.getNext(new Boolean(true));
+        assertEquals(POStatus.STATUS_OK, r.returnStatus);
+        assertFalse((Boolean)r.result);
+    }
+
+    @Test
+    public void testDateTimeLt() throws Exception {
+        ConstantExpression lt = GenPhyOp.exprConst();
+        lt.setValue(new DateTime(0L));
+        ConstantExpression rt = GenPhyOp.exprConst();
+        rt.setValue(new DateTime(1L));
+        LessThanExpr g = GenPhyOp.compLessThanExpr();
+        g.setLhs(lt);
+        g.setRhs(rt);
+        g.setOperandType(DataType.DATETIME);
+        Result r = g.getNext(new Boolean(true));
+        assertEquals(POStatus.STATUS_OK, r.returnStatus);
+        assertTrue((Boolean)r.result);
+    }
+
+    @Test
+    public void testDateTimeEq() throws Exception {
+        ConstantExpression lt = GenPhyOp.exprConst();
+        lt.setValue(new DateTime(1L));
+        ConstantExpression rt = GenPhyOp.exprConst();
+        rt.setValue(new DateTime(1L));
+        LessThanExpr g = GenPhyOp.compLessThanExpr();
+        g.setLhs(lt);
+        g.setRhs(rt);
+        g.setOperandType(DataType.DATETIME);
+        Result r = g.getNext(new Boolean(true));
+        assertEquals(POStatus.STATUS_OK, r.returnStatus);
+        assertFalse((Boolean)r.result);
+    }
+
+    
+    @Test
+    public void testDateTimeAndNullValues() throws Exception {
+        
+        checkNullValues( DataType.DATETIME,  new DateTime(1L) );
+    }  
+
     @Test
     public void testStringGt() throws Exception {
         ConstantExpression lt = GenPhyOp.exprConst();
diff --git a/test/org/apache/pig/test/TestMod.java b/test/org/apache/pig/test/TestMod.java
index 031c0e15a..523238ca1 100644
--- a/test/org/apache/pig/test/TestMod.java
+++ b/test/org/apache/pig/test/TestMod.java
@@ -23,6 +23,8 @@ import java.util.Random;
 
 import junit.framework.TestCase;
 
+import org.joda.time.DateTime;
+
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.data.DataBag;
 import org.apache.pig.data.DataByteArray;
@@ -54,7 +56,7 @@ public class TestMod extends TestCase{
     public void testOperator() throws ExecException{
         //int TRIALS = 10;
         byte[] types = { DataType.BAG, DataType.BOOLEAN, DataType.BYTEARRAY, DataType.CHARARRAY, 
-                DataType.DOUBLE, DataType.FLOAT, DataType.INTEGER, DataType.LONG, DataType.MAP, DataType.TUPLE};
+                DataType.DOUBLE, DataType.FLOAT, DataType.INTEGER, DataType.LONG, DataType.DATETIME, DataType.MAP, DataType.TUPLE};
         //Map<Byte,String> map = GenRandomData.genTypeToNameMap();
         System.out.println("Testing Mod operator");
         for(byte type : types) {
@@ -228,6 +230,25 @@ public class TestMod extends TestCase{
                 assertEquals(null, (Long)resl.result);
                 break;
             }
+            case DataType.DATETIME:
+                DateTime inpdt1 = new DateTime(r.nextLong());
+                DateTime inpdt2 = new DateTime(r.nextLong());
+                lt.setValue(inpdt1);
+                rt.setValue(inpdt2);
+                Result resdt = op.getNext(inpdt1);
+                assertEquals(resdt.returnStatus, POStatus.STATUS_ERR);
+                
+                // test with null in lhs
+                lt.setValue(null);
+                rt.setValue(inpdt2);
+                resb = op.getNext(inpdt1);
+                assertEquals(resb.returnStatus, POStatus.STATUS_ERR);
+                // test with null in rhs
+                lt.setValue(inpdt1);
+                rt.setValue(null);
+                resb = op.getNext(inpdt1);
+                assertEquals(resb.returnStatus, POStatus.STATUS_ERR);
+                break;
             case DataType.MAP: {
                 Map<String,Object> inpm1 = GenRandomData.genRandMap(r, 10);
                 Map<String,Object> inpm2 = GenRandomData.genRandMap(r, 10);
diff --git a/test/org/apache/pig/test/TestMultiply.java b/test/org/apache/pig/test/TestMultiply.java
index fd19cb9a3..7cb1d41cd 100644
--- a/test/org/apache/pig/test/TestMultiply.java
+++ b/test/org/apache/pig/test/TestMultiply.java
@@ -23,6 +23,8 @@ import java.util.Random;
 
 import junit.framework.TestCase;
 
+import org.joda.time.DateTime;
+
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.data.DataBag;
 import org.apache.pig.data.DataByteArray;
@@ -54,7 +56,7 @@ public class TestMultiply extends TestCase{
     public void testOperator() throws ExecException{
         //int TRIALS = 10;
         byte[] types = { DataType.BAG, DataType.BOOLEAN, DataType.BYTEARRAY, DataType.CHARARRAY, 
-                DataType.DOUBLE, DataType.FLOAT, DataType.INTEGER, DataType.LONG, DataType.MAP, DataType.TUPLE};
+                DataType.DOUBLE, DataType.FLOAT, DataType.INTEGER, DataType.LONG, DataType.DATETIME, DataType.MAP, DataType.TUPLE};
         //Map<Byte,String> map = GenRandomData.genTypeToNameMap();
         System.out.println("Testing Multiply operator");
         for(byte type : types) {
@@ -230,6 +232,26 @@ public class TestMultiply extends TestCase{
                 assertEquals(null, (Long)resl.result);
                 break;
             }
+            case DataType.DATETIME: {
+                DateTime inpdt1 = new DateTime(r.nextLong());
+                DateTime inpdt2 = new DateTime(r.nextLong());
+                lt.setValue(inpdt1);
+                rt.setValue(inpdt2);
+                Result resdt = op.getNext(inpdt1);
+                assertEquals(resdt.returnStatus, POStatus.STATUS_ERR);
+                
+                // test with null in lhs
+                lt.setValue(null);
+                rt.setValue(inpdt2);
+                resb = op.getNext(inpdt1);
+                assertEquals(resb.returnStatus, POStatus.STATUS_ERR);
+                // test with null in rhs
+                lt.setValue(inpdt1);
+                rt.setValue(null);
+                resb = op.getNext(inpdt1);
+                assertEquals(resb.returnStatus, POStatus.STATUS_ERR);
+                break;
+            }
             case DataType.MAP: {
                 Map<String,Object> inpm1 = GenRandomData.genRandMap(r, 10);
                 Map<String,Object> inpm2 = GenRandomData.genRandMap(r, 10);
diff --git a/test/org/apache/pig/test/TestNotEqualTo.java b/test/org/apache/pig/test/TestNotEqualTo.java
index d526a5500..e6c3bcaa6 100644
--- a/test/org/apache/pig/test/TestNotEqualTo.java
+++ b/test/org/apache/pig/test/TestNotEqualTo.java
@@ -21,6 +21,8 @@ import java.util.HashMap;
 import java.util.Map;
 import java.util.Random;
 
+import org.joda.time.DateTime;
+
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.data.DataBag;
 import org.apache.pig.data.DataByteArray;
@@ -233,6 +235,43 @@ public class TestNotEqualTo extends junit.framework.TestCase {
     	checkNullValues( DataType.DOUBLE,  new Double(1.0) );
     }
 
+    @Test
+    public void testDateTimeNe() throws Exception {
+        ConstantExpression lt = GenPhyOp.exprConst();
+        lt.setValue(new DateTime(1L));
+        ConstantExpression rt = GenPhyOp.exprConst();
+        rt.setValue(new DateTime(0L));
+        NotEqualToExpr g = GenPhyOp.compNotEqualToExpr();
+        g.setLhs(lt);
+        g.setRhs(rt);
+        g.setOperandType(DataType.DATETIME);
+        Result r = g.getNext(new Boolean(true));
+        assertEquals(POStatus.STATUS_OK, r.returnStatus);
+        assertTrue((Boolean)r.result);
+    }
+
+    @Test
+    public void testDateTimeEq() throws Exception {
+        ConstantExpression lt = GenPhyOp.exprConst();
+        lt.setValue(new DateTime(1L));
+        ConstantExpression rt = GenPhyOp.exprConst();
+        rt.setValue(new DateTime(1L));
+        NotEqualToExpr g = GenPhyOp.compNotEqualToExpr();
+        g.setLhs(lt);
+        g.setRhs(rt);
+        g.setOperandType(DataType.DATETIME);
+        Result r = g.getNext(new Boolean(true));
+        assertEquals(POStatus.STATUS_OK, r.returnStatus);
+        assertFalse((Boolean)r.result);
+    }
+
+
+    @Test
+    public void testDateTimeAndNullValues() throws Exception {
+        
+        checkNullValues( DataType.DATETIME,  new DateTime(1L) );
+    }
+
     @Test
     public void testStringNe() throws Exception {
         ConstantExpression lt = GenPhyOp.exprConst();
diff --git a/test/org/apache/pig/test/TestNull.java b/test/org/apache/pig/test/TestNull.java
index b12c253a5..64deaaf66 100644
--- a/test/org/apache/pig/test/TestNull.java
+++ b/test/org/apache/pig/test/TestNull.java
@@ -22,6 +22,8 @@ import static org.junit.Assert.*;
 import java.util.Map;
 import java.util.Random;
 
+import org.joda.time.DateTime;
+
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.data.DataBag;
 import org.apache.pig.data.DataByteArray;
@@ -193,6 +195,22 @@ public class TestNull extends junit.framework.TestCase {
             if (!res.result.equals(true))
                 return false;
             return true;
+        case DataType.DATETIME:
+            inp1 = new DateTime(r.nextLong());
+            res = isNullExpr.getNext((Boolean) null);
+            if ((Boolean) res.result != true)
+                return false;
+            lt.setValue(inp1);
+            res = isNullExpr.getNext((Boolean) null);
+            ret = (DataType.compare(inp1, null) == 0);
+            if (!(res.result.equals(ret)))
+                return false;
+            // set the input to null and test
+            lt.setValue((DateTime)null);
+            res = isNullExpr.getNext((Boolean) null);
+            if (!res.result.equals(true))
+                return false;
+            return true;
         case DataType.MAP:
             inp1 = GenRandomData.genRandMap(r, 10);
             res = isNullExpr.getNext((Boolean) null);
diff --git a/test/org/apache/pig/test/TestOrderBy.java b/test/org/apache/pig/test/TestOrderBy.java
index d0c185af9..62d876bbe 100644
--- a/test/org/apache/pig/test/TestOrderBy.java
+++ b/test/org/apache/pig/test/TestOrderBy.java
@@ -28,6 +28,9 @@ import java.util.List;
 
 import junit.framework.TestCase;
 
+import org.joda.time.DateTime;
+import org.joda.time.DateTimeZone;
+
 import org.apache.pig.ExecType;
 import org.apache.pig.PigServer;
 import org.apache.pig.data.DataType;
@@ -65,6 +68,8 @@ public class TestOrderBy extends TestCase {
             ps.println("1\t" + DATA[1][i] + "\t" + DATA[0][i]);
         }
         ps.close();
+        
+        DateTimeZone.setDefault(DateTimeZone.forOffsetMillis(DateTimeZone.UTC.getOffset(null)));
     }
     
     @After
@@ -303,4 +308,52 @@ public class TestOrderBy extends TestCase {
         return fp1;
     }
 
+    @Test
+    public void testOrderByDateTimeColumn() throws Exception {
+        File tmpFile = genDataSetFileForOrderByDateTimeColumn();
+        List<Tuple> expectedResults = new ArrayList<Tuple>();
+        expectedResults.add(Util.buildTuple("value3", null));
+        expectedResults.add(Util.buildTuple("value4", null));
+        expectedResults.add(Util.buildTuple("value10", null));
+        expectedResults.add(Util.buildTuple("value2", new DateTime("1970-01-01T00:00:00.000Z")));
+        expectedResults.add(Util.buildTuple("value6", new DateTime("1970-01-01T00:00:01.000Z")));
+        expectedResults.add(Util.buildTuple("value7", new DateTime("1970-01-01T00:00:01.000Z")));
+        expectedResults.add(Util.buildTuple("value1", new DateTime("1970-01-01T00:01:00.000Z")));
+        expectedResults.add(Util.buildTuple("value5", new DateTime("1970-01-01T01:00:00.000Z")));
+        expectedResults.add(Util.buildTuple("value8", new DateTime("1970-01-02T00:00:00.000Z")));
+        expectedResults.add(Util.buildTuple("value9", new DateTime("1970-02-01T00:00:00.000Z")));
+        
+        pig.registerQuery("blah = load '"
+                + Util.generateURI(tmpFile.toString(), pig.getPigContext())
+                + "' as (data:chararray, test:datetime);");
+        pig.registerQuery("ordered = order blah by test;");
+        Iterator<Tuple> expectedItr = expectedResults.iterator();
+        Iterator<Tuple> actualItr = pig.openIterator("ordered");
+        while (expectedItr.hasNext() && actualItr.hasNext()) {
+            Tuple expectedTuple = expectedItr.next();
+            Tuple actualTuple = actualItr.next();
+            assertEquals(expectedTuple, actualTuple);
+        }
+        assertEquals(expectedItr.hasNext(), actualItr.hasNext());
+    }
+
+    private File genDataSetFileForOrderByDateTimeColumn() throws IOException {
+
+        File fp1 = File.createTempFile("order_by_datetime", "txt");
+        PrintStream ps = new PrintStream(new FileOutputStream(fp1));
+        ps.println("value1\t1970-01-01T00:01:00.000Z");
+        ps.println("value2\t1970-01-01T00:00:00.000Z");
+        ps.println("value3\t");
+        ps.println("value4\t");
+        ps.println("value5\t1970-01-01T01:00:00.000Z");
+        ps.println("value6\t1970-01-01T00:00:01.000Z");
+        ps.println("value7\t1970-01-01T00:00:01.000Z");
+        ps.println("value8\t1970-01-02T00:00:00.000Z");
+        ps.println("value9\t1970-02-01T00:00:00.000Z");
+        ps.println("value10\t");
+
+        ps.close();
+
+        return fp1;
+    }
 }
diff --git a/test/org/apache/pig/test/TestPOBinCond.java b/test/org/apache/pig/test/TestPOBinCond.java
index 642a38482..20bd73459 100644
--- a/test/org/apache/pig/test/TestPOBinCond.java
+++ b/test/org/apache/pig/test/TestPOBinCond.java
@@ -39,6 +39,7 @@ import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOpe
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.EqualToExpr;
 import org.apache.pig.impl.plan.PlanException;
 import org.apache.pig.test.utils.GenPhyOp;
+import org.joda.time.DateTime;
 import org.junit.Before;
 
 import junit.framework.TestCase;
@@ -275,7 +276,23 @@ public class TestPOBinCond extends TestCase {
 	    }
 
     }
-   
+
+    public void testPOBinCondWithDateTime() throws  ExecException, PlanException {
+        bag= getBag(DataType.DATETIME);
+        TestPoBinCondHelper testHelper= new TestPoBinCondHelper(DataType.DATETIME, new DateTime(1L) );
+    
+        for(Iterator<Tuple> it = bag.iterator(); it.hasNext(); ) {
+               Tuple t = it.next();
+               testHelper.getPlan().attachInput(t);
+               DateTime value = (DateTime) t.get(0);
+               int expected = value.equals(new DateTime(1L))? 1:0 ;
+               Integer dummy = new Integer(0);
+               Integer result=(Integer)testHelper.getOperator().getNext(dummy).result;
+               int actual = result.intValue();
+               assertEquals( expected, actual );
+        }
+    }
+
     public void testPOBinCondIntWithNull() throws  ExecException, PlanException {
    	
     	bag= getBagWithNulls(DataType.INTEGER);
@@ -364,6 +381,33 @@ public class TestPOBinCond extends TestCase {
 	       }
 
 	}
+
+    public void testPOBinCondDateTimeWithNull() throws  ExecException, PlanException {
+        
+        bag= getBagWithNulls(DataType.DATETIME);
+        TestPoBinCondHelper testHelper= new TestPoBinCondHelper(DataType.DATETIME, new DateTime(1L) );
+
+        for(Iterator<Tuple> it = bag.iterator(); it.hasNext(); ) {
+               Tuple t = it.next();
+               testHelper.getPlan().attachInput(t);
+               
+               DateTime value=null;
+               if ( t.get(0)!=null){
+                   value = (DateTime) t.get(0);
+               }
+               Integer dummy = new Integer(0);
+               Integer result=(Integer)testHelper.getOperator().getNext(dummy).result;                  
+               int expected;
+               int actual;
+               if ( value!=null ) {
+                   expected=value.equals(new DateTime(1L))? 1:0 ;
+                   actual  = result.intValue();
+                   assertEquals( expected, actual );
+               } else {
+                   assertEquals( null, result );
+               }
+           }
+    }
    
     protected class TestPoBinCondHelper {
     	
@@ -451,7 +495,10 @@ public class TestPOBinCond extends TestCase {
                     break;
                 case DataType.DOUBLE:
                     t.append((i % 2 == 0 ? 1.0 : 0.0));
-                    break;                
+                    break;
+                case DataType.DATETIME:
+                    t.append(new DateTime(r.nextLong() % 2L));
+                    break;
             }
             t.append(1);
             t.append(0);
@@ -483,6 +530,9 @@ public class TestPOBinCond extends TestCase {
                     case DataType.DOUBLE:
                         t.append( (i % 2 == 0 ? 1.0 : 0.0));
                         break;
+                    case DataType.DATETIME:
+                        t.append(new DateTime(r.nextLong() % 2L));
+                        break;
                 }
             }            
             t.append(1);
diff --git a/test/org/apache/pig/test/TestPOCast.java b/test/org/apache/pig/test/TestPOCast.java
index 98d559fe5..ad19bea54 100644
--- a/test/org/apache/pig/test/TestPOCast.java
+++ b/test/org/apache/pig/test/TestPOCast.java
@@ -25,6 +25,8 @@ import java.util.Random;
 
 import junit.framework.TestCase;
 
+import org.joda.time.DateTime;
+
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.mapreduce.InputFormat;
 import org.apache.hadoop.mapreduce.Job;
@@ -167,7 +169,15 @@ public class TestPOCast extends TestCase {
                 assertEquals(d, res.result);
             }
         }
-        
+
+        for(Iterator<Tuple> it = bag.iterator(); it.hasNext(); ) {
+            Tuple t = it.next();
+            plan.attachInput(t);
+            DateTime dt = null;
+            Result res = op.getNext(dt);
+            assertEquals(POStatus.STATUS_ERR, res.returnStatus);
+        }
+
         for(Iterator<Tuple> it = bag.iterator(); it.hasNext(); ) {
             Tuple t = it.next();
             plan.attachInput(t);
@@ -345,6 +355,21 @@ public class TestPOCast extends TestCase {
 				assertEquals(d, res.result);
 			}
 		}
+
+        for(Iterator<Tuple> it = bag.iterator(); it.hasNext(); ) {
+            Tuple t = it.next();
+            plan.attachInput(t);
+            DateTime dt = new DateTime(((Integer)t.get(0)).longValue());
+            Result res = op.getNext(dt);
+            if(res.returnStatus == POStatus.STATUS_OK) {
+                assertEquals(dt, res.result);
+            }
+            planToTestBACasts.attachInput(t);
+            res = opWithInputTypeAsBA.getNext(dt);
+            if(res.returnStatus == POStatus.STATUS_OK) {
+                assertEquals(dt, res.result);
+            }
+        }
 		
 		for(Iterator<Tuple> it = bag.iterator(); it.hasNext(); ) {
 			Tuple t = it.next();
@@ -531,7 +556,23 @@ public class TestPOCast extends TestCase {
 			if(res.returnStatus == POStatus.STATUS_OK)
 				assertEquals(d, res.result);
 		}
-		
+
+		for(Iterator<Tuple> it = bag.iterator(); it.hasNext(); ) {
+            Tuple t = it.next();
+            plan.attachInput(t);
+            DateTime dt = new DateTime((Long)t.get(0));
+            Result res = op.getNext(dt);
+            if(res.returnStatus == POStatus.STATUS_OK) {
+                //System.out.println(res.result + " : " + l);
+                assertEquals(dt, res.result);
+            }
+            
+            planToTestBACasts.attachInput(t);
+            res = opWithInputTypeAsBA.getNext(dt);
+            if(res.returnStatus == POStatus.STATUS_OK)
+                assertEquals(dt, res.result);
+        }
+
 		for(Iterator<Tuple> it = bag.iterator(); it.hasNext(); ) {
 			Tuple t = it.next();
 			plan.attachInput(t);
@@ -712,7 +753,23 @@ public class TestPOCast extends TestCase {
 			if(res.returnStatus == POStatus.STATUS_OK)
 				assertEquals(d, res.result);
 		}
-		
+
+        for(Iterator<Tuple> it = bag.iterator(); it.hasNext(); ) {
+            Tuple t = it.next();
+            plan.attachInput(t);
+            DateTime dt = new DateTime(((Float)t.get(0)).longValue());
+            Result res = op.getNext(dt);
+            if(res.returnStatus == POStatus.STATUS_OK) {
+                //System.out.println(res.result + " : " + dt);
+                assertEquals(dt, res.result);
+            }
+            
+            planToTestBACasts.attachInput(t);
+            res = opWithInputTypeAsBA.getNext(dt);
+            if(res.returnStatus == POStatus.STATUS_OK)
+                assertEquals(dt, res.result);
+        }   
+
 		for(Iterator<Tuple> it = bag.iterator(); it.hasNext(); ) {
 			Tuple t = it.next();
 			plan.attachInput(t);
@@ -902,7 +959,23 @@ public class TestPOCast extends TestCase {
 			if(res.returnStatus == POStatus.STATUS_OK)
 				assertEquals(d, res.result);
 		}
-		
+
+        for(Iterator<Tuple> it = bag.iterator(); it.hasNext(); ) {
+            Tuple t = it.next();
+            plan.attachInput(t);
+            DateTime dt = new DateTime(((Double)t.get(0)).longValue());
+            Result res = op.getNext(dt);
+            if(res.returnStatus == POStatus.STATUS_OK) {
+                //System.out.println(res.result + " : " + dt);
+                assertEquals(dt, res.result);
+            }
+            
+            planToTestBACasts.attachInput(t);
+            res = opWithInputTypeAsBA.getNext(dt);
+            if(res.returnStatus == POStatus.STATUS_OK)
+                assertEquals(dt, res.result);
+        }		
+
 		for(Iterator<Tuple> it = bag.iterator(); it.hasNext(); ) {
 			Tuple t = it.next();
 			plan.attachInput(t);
@@ -980,7 +1053,202 @@ public class TestPOCast extends TestCase {
             }
         }
 	}
-	
+
+    @Test
+    public void testDateTimeToOther() throws IOException {
+        //Create data
+        DataBag bag = BagFactory.getInstance().newDefaultBag();
+        for(int i = 0; i < MAX; i++) {
+            Tuple t = TupleFactory.getInstance().newTuple();
+            t.append(i == 0 ? new DateTime(0L) : new DateTime(r.nextLong()));
+            bag.add(t);
+        }
+        
+        POCast op = new POCast(new OperatorKey("", r.nextLong()), -1);
+        LoadFunc load = new TestLoader();
+        op.setFuncSpec(new FuncSpec(load.getClass().getName()));
+        POProject prj = new POProject(new OperatorKey("", r.nextLong()), -1, 0);
+        PhysicalPlan plan = new PhysicalPlan();
+        plan.add(prj);
+        plan.add(op);
+        plan.connect(prj, op);
+        
+        prj.setResultType(DataType.DATETIME);
+        
+        // Plan to test when result type is ByteArray and casting is requested
+        // for example casting of values coming out of map lookup.
+        POCast opWithInputTypeAsBA = new POCast(new OperatorKey("", r.nextLong()), -1);
+        PhysicalPlan planToTestBACasts = constructPlan(opWithInputTypeAsBA);
+
+        for (Iterator<Tuple> it = bag.iterator(); it.hasNext();) {
+            Tuple t = it.next();
+            plan.attachInput(t);
+            Boolean b = null;
+            Result res = op.getNext(b);
+            assertEquals(POStatus.STATUS_ERR, res.returnStatus);
+        }
+
+        for(Iterator<Tuple> it = bag.iterator(); it.hasNext(); ) {
+            Tuple t = it.next();
+            plan.attachInput(t);
+            Integer i = new Long(((DateTime) t.get(0)).getMillis()).intValue();
+            Result res = op.getNext(i);
+            if(res.returnStatus == POStatus.STATUS_OK) {
+                //System.out.println(res.result + " : " + i);
+                assertEquals(i, res.result);
+            }
+            
+            planToTestBACasts.attachInput(t);
+            res = opWithInputTypeAsBA.getNext(i);
+            if(res.returnStatus == POStatus.STATUS_OK)
+                assertEquals(i, res.result);
+            
+        }
+        
+        for(Iterator<Tuple> it = bag.iterator(); it.hasNext(); ) {
+            Tuple t = it.next();
+            plan.attachInput(t);
+            Float f = new Float(Long.valueOf(((DateTime) t.get(0)).getMillis()).floatValue());
+            Result res = op.getNext(f);
+            if(res.returnStatus == POStatus.STATUS_OK) {
+                //System.out.println(res.result + " : " + f);
+                assertEquals(f, res.result);
+            }
+            
+            planToTestBACasts.attachInput(t);
+            res = opWithInputTypeAsBA.getNext(f);
+            if(res.returnStatus == POStatus.STATUS_OK)
+                assertEquals(f, res.result);
+        }
+        
+        for(Iterator<Tuple> it = bag.iterator(); it.hasNext(); ) {
+            Tuple t = it.next();
+            plan.attachInput(t);
+            Long l = new Long(((DateTime)t.get(0)).getMillis());
+            Result res = op.getNext(l);
+            if(res.returnStatus == POStatus.STATUS_OK) {
+                //System.out.println(res.result + " : " + l);
+                assertEquals(l, res.result);
+            }
+            
+            planToTestBACasts.attachInput(t);
+            res = opWithInputTypeAsBA.getNext(l);
+            if(res.returnStatus == POStatus.STATUS_OK)
+                assertEquals(l, res.result);
+        }
+        
+        for(Iterator<Tuple> it = bag.iterator(); it.hasNext(); ) {
+            Tuple t = it.next();
+            plan.attachInput(t);
+            Double d = new Double(Long.valueOf(((DateTime) t.get(0)).getMillis()).doubleValue());
+            Result res = op.getNext(d);
+            if(res.returnStatus == POStatus.STATUS_OK) {
+                //System.out.println(res.result + " : " + f);
+                assertEquals(d, res.result);
+            }
+            
+            planToTestBACasts.attachInput(t);
+            res = opWithInputTypeAsBA.getNext(d);
+            if(res.returnStatus == POStatus.STATUS_OK)
+                assertEquals(d, res.result);
+        }
+
+        for(Iterator<Tuple> it = bag.iterator(); it.hasNext(); ) {
+            Tuple t = it.next();
+            plan.attachInput(t);
+            DateTime dt = (DateTime)t.get(0);
+            Result res = op.getNext(dt);
+            if(res.returnStatus == POStatus.STATUS_OK) {
+                //System.out.println(res.result + " : " + l);
+                assertEquals(dt, res.result);
+            }
+            
+            planToTestBACasts.attachInput(t);
+            res = opWithInputTypeAsBA.getNext(dt);
+            if(res.returnStatus == POStatus.STATUS_OK)
+                assertEquals(dt, res.result);
+        }
+
+        for(Iterator<Tuple> it = bag.iterator(); it.hasNext(); ) {
+            Tuple t = it.next();
+            plan.attachInput(t);
+            String str = ((DateTime)t.get(0)).toString();
+            Result res = op.getNext(str);
+            if(res.returnStatus == POStatus.STATUS_OK) {
+                //System.out.println(res.result + " : " + str);
+                assertEquals(str, res.result);
+            }
+            
+            planToTestBACasts.attachInput(t);
+            res = opWithInputTypeAsBA.getNext(str);
+            if(res.returnStatus == POStatus.STATUS_OK)
+                assertEquals(str, res.result);
+        }
+        
+        for(Iterator<Tuple> it = bag.iterator(); it.hasNext(); ) {
+            Tuple t = it.next();
+            plan.attachInput(t);
+            DataByteArray dba = new DataByteArray(((DateTime)t.get(0)).toString().getBytes());
+            Result res = op.getNext(dba);
+            if(res.returnStatus == POStatus.STATUS_OK) {
+                //System.out.println(res.result + " : " + dba);
+                assertEquals(dba, res.result);
+            }
+            
+            planToTestBACasts.attachInput(t);
+            res = opWithInputTypeAsBA.getNext(dba);
+            if(res.returnStatus == POStatus.STATUS_OK)
+                assertEquals(dba, res.result);
+        }
+        
+        for(Iterator<Tuple> it = bag.iterator(); it.hasNext(); ) {
+            Tuple t = it.next();
+            plan.attachInput(t);
+            Map map = null;
+            Result res = op.getNext(map);
+            assertEquals(POStatus.STATUS_ERR, res.returnStatus);
+        }
+        
+        for(Iterator<Tuple> it = bag.iterator(); it.hasNext(); ) {
+            Tuple t = it.next();
+            plan.attachInput(t);
+            Result res = op.getNext(t);
+            assertEquals(POStatus.STATUS_ERR, res.returnStatus);
+        }
+        
+        for(Iterator<Tuple> it = bag.iterator(); it.hasNext(); ) {
+            Tuple t = it.next();
+            plan.attachInput(t);
+            DataBag b = null;
+            Result res = op.getNext(b);
+            assertEquals(POStatus.STATUS_ERR, res.returnStatus);
+        }
+        {
+            planToTestBACasts.attachInput(dummyTuple);
+            try{
+                opWithInputTypeAsBA.getNext(dummyMap);
+            }catch (Exception e) {
+                assertEquals(ExecException.class, e.getClass());
+            }
+        }
+        {
+            planToTestBACasts.attachInput(dummyTuple);
+            try{
+                opWithInputTypeAsBA.getNext(dummyTuple);
+            }catch (Exception e) {
+                assertEquals(ExecException.class, e.getClass());
+            }
+        }
+        {
+            planToTestBACasts.attachInput(dummyTuple);
+            try{
+                opWithInputTypeAsBA.getNext(dummyBag);
+            }catch (Exception e) {
+                assertEquals(ExecException.class, e.getClass());
+            }
+        }
+    }
+
 	@Test
 	public void testStringToOther() throws IOException {
 		POCast op = new POCast(new OperatorKey("", r.nextLong()), -1);
@@ -1099,7 +1367,23 @@ public class TestPOCast extends TestCase {
 			if(res.returnStatus == POStatus.STATUS_OK)
 				assertEquals(i, res.result);
 		}
-		
+
+        {
+            Tuple t = tf.newTuple();
+            t.append((new DateTime(r.nextLong())).toString());
+            plan.attachInput(t);
+            DateTime i = new DateTime(((String) t.get(0)));
+            Result res = op.getNext(i);
+            if(res.returnStatus == POStatus.STATUS_OK) {
+                //System.out.println(res.result + " : " + i);
+                assertEquals(i, res.result);
+            }
+            planToTestBACasts.attachInput(t);
+            res = opWithInputTypeAsBA.getNext(i);
+            if(res.returnStatus == POStatus.STATUS_OK)
+                assertEquals(i, res.result);
+        }
+
 		{
 			Tuple t = tf.newTuple();
 			t.append(GenRandomData.genRandString(r));
@@ -1237,6 +1521,10 @@ public class TestPOCast extends TestCase {
             return new Long(Long.valueOf(new DataByteArray(b).toString()));
         }
 
+        public DateTime bytesToDateTime(byte[] b) throws IOException {
+            return new DateTime(new DataByteArray(b).toString());
+        }
+
         public Map<String, Object> bytesToMap(byte[] b) throws IOException {
           return null;
         }
@@ -1272,6 +1560,10 @@ public class TestPOCast extends TestCase {
         public byte[] toBytes(Long l) throws IOException {
             return l.toString().getBytes();
         }
+        
+        public byte[] toBytes(DateTime dt) throws IOException {
+            return dt.toString().getBytes();
+        }
 
         public byte[] toBytes(Boolean b) throws IOException {
             return b.toString().getBytes();
@@ -1423,7 +1715,23 @@ public class TestPOCast extends TestCase {
 			if(res.returnStatus == POStatus.STATUS_OK)
 				assertEquals(i, res.result);
 		}
-		
+
+		{
+            Tuple t = tf.newTuple();
+            t.append(new DataByteArray((new DateTime(r.nextLong())).toString().getBytes()));
+            plan.attachInput(t);
+            DateTime i = new DateTime(((DataByteArray) t.get(0)).toString());
+            Result res = op.getNext(i);
+            if(res.returnStatus == POStatus.STATUS_OK) {
+                //System.out.println(res.result + " : " + i);
+                assertEquals(i, res.result);
+            }
+            planToTestBACasts.attachInput(t);
+            res = opWithInputTypeAsBA.getNext(i);
+            if(res.returnStatus == POStatus.STATUS_OK)
+                assertEquals(i, res.result);
+        }
+
 		{
 			Tuple t = tf.newTuple();
 			t.append(new DataByteArray(GenRandomData.genRandString(r).getBytes()));
@@ -1622,7 +1930,24 @@ public class TestPOCast extends TestCase {
                 assertEquals(input, res.result);
             }
         }
-        
+
+        {
+            // create a new POCast each time since we 
+            // maintain a state variable per POCast object
+            // indicating if cast is really required
+            POCast newOp = new POCast(new OperatorKey("", r.nextLong()), -1);
+            plan = constructPlan(newOp);
+            Tuple t = tf.newTuple();
+            DateTime input = new DateTime(r.nextLong());
+            t.append(input);
+            plan.attachInput(t);
+            Result res = newOp.getNext(new DateTime(0L));
+            if(res.returnStatus == POStatus.STATUS_OK) {
+                //System.out.println(res.result + " : " + i);
+                assertEquals(input, res.result);
+            }
+        }
+
         {
             // create a new POCast each time since we 
             // maintain a state variable per POCast object
@@ -1777,7 +2102,18 @@ public class TestPOCast extends TestCase {
 			Result res = op.getNext(i);
 			assertEquals(POStatus.STATUS_ERR, res.returnStatus);
 		}
-	      
+
+		{
+            Tuple t = tf.newTuple();
+            t.append(GenRandomData.genRandString(r));
+            Tuple tNew = tf.newTuple();
+            tNew.append(t);
+            plan.attachInput(tNew);
+            DateTime dt = null;
+            Result res = op.getNext(dt);
+            assertEquals(POStatus.STATUS_ERR, res.returnStatus);
+        }
+		
 		{
 			Tuple t = tf.newTuple();
 			t.append(GenRandomData.genRandString(r));
@@ -1942,7 +2278,16 @@ public class TestPOCast extends TestCase {
 			Result res = op.getNext(i);
 			assertEquals(POStatus.STATUS_ERR, res.returnStatus);
 		}
-		
+
+        {
+            Tuple t = tf.newTuple();
+            t.append(GenRandomData.genRandSmallTupDataBag(r, 1, 100));
+            plan.attachInput(t);
+            DateTime dt = null;
+            Result res = op.getNext(dt);
+            assertEquals(POStatus.STATUS_ERR, res.returnStatus);
+        }
+
 		{
 			Tuple t = tf.newTuple();
 			t.append(GenRandomData.genRandSmallTupDataBag(r, 1, 100));
@@ -2102,7 +2447,15 @@ public class TestPOCast extends TestCase {
 			Result res = op.getNext(i);
 			assertEquals(POStatus.STATUS_ERR, res.returnStatus);
 		}
-		
+
+        {     
+            Tuple t = tf.newTuple();
+            t.append(GenRandomData.genRandMap(r, 10));
+            DateTime dt = null;
+            Result res = op.getNext(dt);
+            assertEquals(POStatus.STATUS_ERR, res.returnStatus);
+        }
+
 		{
 			Tuple t = tf.newTuple();
 			t.append(GenRandomData.genRandMap(r, 10));
@@ -2203,7 +2556,21 @@ public class TestPOCast extends TestCase {
 
 			}
 		}
-		
+
+        prj.setResultType(DataType.DATETIME); 
+        
+        for(Iterator<Tuple> it = bag.iterator(); it.hasNext(); ) {
+            Tuple t = it.next();
+            plan.attachInput(t);
+            if(t.get(0) == null) {
+                
+                DateTime result  = (DateTime)op.getNext((DateTime)null).result;
+                assertEquals( null, result);
+
+            } 
+            
+        }	
+
 		prj.setResultType(DataType.CHARARRAY);
 		
 		for(Iterator<Tuple> it = bag.iterator(); it.hasNext(); ) {
@@ -2249,16 +2616,18 @@ public class TestPOCast extends TestCase {
 		// Create a bag having tuples having values of different types.
 		for(int i = 0; i < MAX; i++) {
 			Tuple t = TupleFactory.getInstance().newTuple();
-			if (i % 5 == 0)
+			if (i % 6 == 0)
 			    t.append(r.nextBoolean());
-			if(i % 5 == 1)
+			if(i % 6 == 1)
 				t.append(r.nextInt());
-			if(i % 5 == 2)
+			if(i % 6 == 2)
 				t.append(r.nextLong());
-			if(i % 5 == 3)
+			if(i % 6 == 3)
 				t.append(r.nextDouble());
-			if(i % 5 == 4)
+			if(i % 6 == 4)
 				t.append(r.nextFloat());
+			if(i % 6 == 5)
+			    t.append(r.nextLong());
 			bag.add(t);
 		}
 
@@ -2294,6 +2663,14 @@ public class TestPOCast extends TestCase {
 				assertEquals(d, res.result);
 			}
 
+			if (!(toCast instanceof Boolean)) {
+                DateTime dt = DataType.toDateTime(toCast);
+                res = opWithInputTypeAsBA.getNext(dt);
+                if(res.returnStatus == POStatus.STATUS_OK) {
+                    assertEquals(dt, res.result);
+                }
+			}
+
 			String s = DataType.toString(toCast);
 			res = opWithInputTypeAsBA.getNext(s);
 			if(res.returnStatus == POStatus.STATUS_OK) {
diff --git a/test/org/apache/pig/test/TestPackage.java b/test/org/apache/pig/test/TestPackage.java
index a5c6e2fe4..7bc6c5c1a 100644
--- a/test/org/apache/pig/test/TestPackage.java
+++ b/test/org/apache/pig/test/TestPackage.java
@@ -27,6 +27,8 @@ import java.util.List;
 import java.util.Map;
 import java.util.Random;
 
+import org.joda.time.DateTime;
+
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.backend.hadoop.HDataType;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
@@ -148,6 +150,9 @@ public class TestPackage extends junit.framework.TestCase {
         case DataType.LONG:
             runTest(r.nextLong(),inner, DataType.LONG);
             break;
+        case DataType.DATETIME:
+            runTest(new DateTime(r.nextLong()),inner, DataType.DATETIME);
+            break;
         case DataType.MAP:
         case DataType.INTERNALMAP:
         case DataType.BYTE:
diff --git a/test/org/apache/pig/test/TestPigTupleRawComparator.java b/test/org/apache/pig/test/TestPigTupleRawComparator.java
index 0fa9f2eab..ae97bf3eb 100644
--- a/test/org/apache/pig/test/TestPigTupleRawComparator.java
+++ b/test/org/apache/pig/test/TestPigTupleRawComparator.java
@@ -29,6 +29,8 @@ import java.util.Collections;
 import java.util.List;
 import java.util.Random;
 
+import org.joda.time.DateTime;
+
 import org.apache.hadoop.io.RawComparator;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigTupleDefaultRawComparator;
@@ -65,7 +67,7 @@ public class TestPigTupleRawComparator {
         oldComparator.setConf(jobConf);
         list = Arrays.<Object> asList(1f, 2, 3.0, 4l, (byte) 5, true,
                 new DataByteArray(new byte[] { 0x10, 0x2a, 0x5e }), "hello world!",
-                tf.newTuple(Arrays.<Object> asList(8.0, 9f, 10l, 11)));
+                tf.newTuple(Arrays.<Object> asList(8.0, 9f, 10l, 11)), new DateTime(12L));
         prototype = new NullableTuple(tf.newTuple(list));
         baos1.reset();
         baos2.reset();
@@ -358,6 +360,15 @@ public class TestPigTupleRawComparator {
         assertTrue(res < 0);
     }
 
+    @Test
+    public void testCompareDateTime() throws IOException {
+        list.set(9, ((DateTime) list.get(9)).plus(1L));
+        NullableTuple t = new NullableTuple(tf.newTuple(list));
+        int res = compareHelper(prototype, t, comparator);
+        assertEquals(Math.signum(prototype.compareTo(t)), Math.signum(res), 0);
+        assertTrue(res < 0);
+    }
+
     @Test
     public void testCompareDiffertTypes() throws IOException {
         // DataType.INTEGER < DataType.LONG
@@ -442,6 +453,10 @@ public class TestPigTupleRawComparator {
         case 8:
             length = rand.nextInt(6);
             t.set(pos, getRandomTuple(rand));
+            break;
+        case 9:
+            t.set(pos, new DateTime(rand.nextLong()));
+            break;
         default:
         }
         return t;
diff --git a/test/org/apache/pig/test/TestResourceSchema.java b/test/org/apache/pig/test/TestResourceSchema.java
index c8394fba3..dd206b422 100644
--- a/test/org/apache/pig/test/TestResourceSchema.java
+++ b/test/org/apache/pig/test/TestResourceSchema.java
@@ -280,16 +280,16 @@ public class TestResourceSchema {
     @Test
     public void testToStringAndParse() throws Exception {
         ResourceSchema rs = new ResourceSchema();
-        ResourceFieldSchema[] fields = new ResourceFieldSchema[12];
+        ResourceFieldSchema[] fields = new ResourceFieldSchema[13];
 
         byte[] types = {DataType.INTEGER, DataType.LONG, DataType.FLOAT,
             DataType.DOUBLE, DataType.BYTEARRAY, DataType.CHARARRAY,
             DataType.MAP, DataType.TUPLE, DataType.TUPLE, DataType.BAG,
-            DataType.BAG, DataType.BOOLEAN};
+            DataType.BAG, DataType.BOOLEAN, DataType.DATETIME};
         String[] names = {"i", "l", "f",
             "d", "b", "s",
             "m", "tschema", "tnull", "bschema",
-            "bnull", "bb"};
+            "bnull", "bb", "dt"};
         
         for (int i = 0; i < fields.length; i++) {
             fields[i] = new ResourceFieldSchema();
@@ -334,14 +334,14 @@ public class TestResourceSchema {
 
         assertEquals("i:int,l:long,f:float,d:double,b:bytearray,s:" +
             "chararray,m:[],tschema:(i:int,l:long,f:float)," +
-            "tnull:(),bschema:{t:(d:double,b:bytearray,s:chararray)},bnull:{},bb:boolean",
+            "tnull:(),bschema:{t:(d:double,b:bytearray,s:chararray)},bnull:{},bb:boolean,dt:datetime",
             strSchema);
 
         ResourceSchema after =
             new ResourceSchema(Utils.getSchemaFromString(strSchema));
         ResourceFieldSchema[] afterFields = after.getFields();
 
-        assertEquals(12, afterFields.length);
+        assertEquals(13, afterFields.length);
         assertEquals("i", afterFields[0].getName());
         assertEquals(DataType.INTEGER, afterFields[0].getType());
         assertEquals("l", afterFields[1].getName());
@@ -366,6 +366,9 @@ public class TestResourceSchema {
         assertEquals(DataType.BAG, afterFields[10].getType());
         assertEquals("bb", afterFields[11].getName());
         assertEquals(DataType.BOOLEAN, afterFields[11].getType());
+        assertEquals("dt", afterFields[12].getName());
+        assertEquals(DataType.DATETIME, afterFields[12].getType());
+        
 
         assertNotNull(afterFields[7].getSchema());
         ResourceFieldSchema[] tAfterFields =
diff --git a/test/org/apache/pig/test/TestSchema.java b/test/org/apache/pig/test/TestSchema.java
index fe5f8517e..45846e9ac 100644
--- a/test/org/apache/pig/test/TestSchema.java
+++ b/test/org/apache/pig/test/TestSchema.java
@@ -871,10 +871,10 @@ public class TestSchema {
             "float,float,float,float,float,float,float,float,float,float",
             "double,double,double,double,double,double,double,double,double,double",
             "boolean,boolean,boolean,boolean,boolean,boolean,boolean,boolean,boolean,boolean",
-            "(),(),(),(),(),(),(),(),(),()",
+            "datetime,datetime,datetime,datetime,datetime,datetime,datetime,datetime,datetime,datetime",
             "{},{},{},{},{},{},{},{},{},{}",
             "map[],map[],map[],map[],map[],map[],map[],map[],map[],map[]",
-            "int,int,long,long,float,float,double,double,boolean,boolean,(int,long,float,double,boolean),{(int,long,float,double,boolean)},map[(int,long,float,double,boolean)]"
+            "int,int,long,long,float,float,double,double,boolean,boolean,datetime,datetime(int,long,float,double,boolean,datetime),{(int,long,float,double,boolean,datetime)},map[(int,long,float,double,boolean,datetime)]"
         };
         for (String schemaString : schemaStrings) {
             Schema s1 = Utils.getSchemaFromString(schemaString);
diff --git a/test/org/apache/pig/test/TestStore.java b/test/org/apache/pig/test/TestStore.java
index e7e1ca1ab..822cdf7e6 100644
--- a/test/org/apache/pig/test/TestStore.java
+++ b/test/org/apache/pig/test/TestStore.java
@@ -70,6 +70,7 @@ import org.apache.pig.parser.ParserException;
 import org.apache.pig.parser.QueryParserDriver;
 import org.apache.pig.test.utils.GenRandomData;
 import org.apache.pig.test.utils.TestHelper;
+import org.joda.time.DateTimeZone;
 import org.junit.After;
 import org.junit.AfterClass;
 import org.junit.Before;
@@ -103,6 +104,8 @@ public class TestStore extends junit.framework.TestCase {
         pc = pig.getPigContext();
         inputFileName = "/tmp/TestStore-" + new Random().nextLong() + ".txt";
         outputFileName = "/tmp/TestStore-output-" + new Random().nextLong() + ".txt";
+        
+        DateTimeZone.setDefault(DateTimeZone.forOffsetMillis(DateTimeZone.UTC.getOffset(null)));
     }
 
     @Override
@@ -116,7 +119,7 @@ public class TestStore extends junit.framework.TestCase {
     private void storeAndCopyLocally(DataBag inpDB) throws Exception {
         setUpInputFileOnCluster(inpDB);
         String script = "a = load '" + inputFileName + "'; " +
-                "store a into '" + outputFileName + "' using PigStorage(':');" +
+                "store a into '" + outputFileName + "' using PigStorage('\t');" +
                 "fs -ls /tmp";
         pig.setBatchOn();
         Util.registerMultiLineQuery(pig, script);
@@ -180,7 +183,7 @@ public class TestStore extends junit.framework.TestCase {
         int size = 0;
         BufferedReader br = new BufferedReader(new FileReader(outputFileName));
         for(String line=br.readLine();line!=null;line=br.readLine()){
-            String[] flds = line.split(":",-1);
+            String[] flds = line.split("\t",-1);
             Tuple t = new DefaultTuple();
             t.append(flds[0].compareTo("")!=0 ? flds[0] : null);
             t.append(flds[1].compareTo("")!=0 ? Integer.parseInt(flds[1]) : null);
@@ -235,11 +238,11 @@ public class TestStore extends junit.framework.TestCase {
     public void testStoreComplexData() throws Exception {
         inpDB = GenRandomData.genRandFullTupTextDataBag(new Random(), 10, 100);
         storeAndCopyLocally(inpDB);
-        PigStorage ps = new PigStorage(":");
+        PigStorage ps = new PigStorage("\t");
         int size = 0;
         BufferedReader br = new BufferedReader(new FileReader(outputFileName));
         for(String line=br.readLine();line!=null;line=br.readLine()){
-            String[] flds = line.split(":",-1);
+            String[] flds = line.split("\t",-1);
             Tuple t = new DefaultTuple();
             
             ResourceFieldSchema bagfs = GenRandomData.getSmallTupDataBagFieldSchema();
@@ -255,6 +258,7 @@ public class TestStore extends junit.framework.TestCase {
             t.append(flds[7].compareTo("")!=0 ? ps.getLoadCaster().bytesToMap(flds[7].getBytes()) : null);
             t.append(flds[8].compareTo("")!=0 ? ps.getLoadCaster().bytesToTuple(flds[8].getBytes(), tuplefs) : null);
             t.append(flds[9].compareTo("")!=0 ? ps.getLoadCaster().bytesToBoolean(flds[9].getBytes()) : null);
+            t.append(flds[10].compareTo("")!=0 ? ps.getLoadCaster().bytesToDateTime(flds[10].getBytes()) : null);
             assertEquals(true, TestHelper.bagContains(inpDB, t));
             ++size;
         }
@@ -267,13 +271,13 @@ public class TestStore extends junit.framework.TestCase {
         inpDB = DefaultBagFactory.getInstance().newDefaultBag();
         inpDB.add(inputTuple);
         storeAndCopyLocally(inpDB);
-        PigStorage ps = new PigStorage(":");
+        PigStorage ps = new PigStorage("\t");
         int size = 0;
         BufferedReader br = new BufferedReader(new FileReader(outputFileName));
         for(String line=br.readLine();line!=null;line=br.readLine()){
             System.err.println("Complex data: ");
             System.err.println(line);
-            String[] flds = line.split(":",-1);
+            String[] flds = line.split("\t",-1);
             Tuple t = new DefaultTuple();
             
             ResourceFieldSchema stringfs = new ResourceFieldSchema();
@@ -303,7 +307,8 @@ public class TestStore extends junit.framework.TestCase {
             t.append(flds[7].compareTo("")!=0 ? ps.getLoadCaster().bytesToMap(flds[7].getBytes()) : null);
             t.append(flds[8].compareTo("")!=0 ? ps.getLoadCaster().bytesToTuple(flds[8].getBytes(), tuplefs) : null);
             t.append(flds[9].compareTo("")!=0 ? ps.getLoadCaster().bytesToBoolean(flds[9].getBytes()) : null);
-            t.append(flds[10].compareTo("")!=0 ? ps.getLoadCaster().bytesToCharArray(flds[10].getBytes()) : null);
+            t.append(flds[10].compareTo("")!=0 ? ps.getLoadCaster().bytesToDateTime(flds[10].getBytes()) : null);
+            t.append(flds[11].compareTo("")!=0 ? ps.getLoadCaster().bytesToCharArray(flds[10].getBytes()) : null);
             assertTrue(TestHelper.tupleEquals(inputTuple, t));
             ++size;
         }
diff --git a/test/org/apache/pig/test/TestSubtract.java b/test/org/apache/pig/test/TestSubtract.java
index c302f3a23..25900d0ee 100644
--- a/test/org/apache/pig/test/TestSubtract.java
+++ b/test/org/apache/pig/test/TestSubtract.java
@@ -23,6 +23,8 @@ import java.util.Random;
 
 import junit.framework.TestCase;
 
+import org.joda.time.DateTime;
+
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.data.DataBag;
 import org.apache.pig.data.DataByteArray;
@@ -54,7 +56,7 @@ public class TestSubtract extends TestCase{
     public void testOperator() throws ExecException{
         //int TRIALS = 10;
         byte[] types = { DataType.BAG, DataType.BOOLEAN, DataType.BYTEARRAY, DataType.CHARARRAY, 
-                DataType.DOUBLE, DataType.FLOAT, DataType.INTEGER, DataType.LONG, DataType.MAP, DataType.TUPLE};
+                DataType.DOUBLE, DataType.FLOAT, DataType.INTEGER, DataType.LONG, DataType.DATETIME, DataType.MAP, DataType.TUPLE};
         //Map<Byte,String> map = GenRandomData.genTypeToNameMap();
         System.out.println("Testing Subtract operator");
         for(byte type : types) {
@@ -230,6 +232,26 @@ public class TestSubtract extends TestCase{
                 assertEquals(null, (Long)resl.result);
                 break;
             }
+            case DataType.DATETIME: {
+                DateTime inpdt1 = new DateTime(r.nextLong());
+                DateTime inpdt2 = new DateTime(r.nextLong());
+                lt.setValue(inpdt1);
+                rt.setValue(inpdt2);
+                Result resdt = op.getNext(inpdt1);
+                assertEquals(resdt.returnStatus, POStatus.STATUS_ERR);
+                
+                // test with null in lhs
+                lt.setValue(null);
+                rt.setValue(inpdt2);
+                resdt = op.getNext(inpdt1);
+                assertEquals(resdt.returnStatus, POStatus.STATUS_ERR);
+                // test with null in rhs
+                lt.setValue(inpdt1);
+                rt.setValue(null);
+                resdt = op.getNext(inpdt1);
+                assertEquals(resdt.returnStatus, POStatus.STATUS_ERR);
+                break;
+            }
             case DataType.MAP: {
                 Map<String,Object> inpm1 = GenRandomData.genRandMap(r, 10);
                 Map<String,Object> inpm2 = GenRandomData.genRandMap(r, 10);
diff --git a/test/org/apache/pig/test/TestTextDataParser.java b/test/org/apache/pig/test/TestTextDataParser.java
index 23b957544..db221ffc1 100644
--- a/test/org/apache/pig/test/TestTextDataParser.java
+++ b/test/org/apache/pig/test/TestTextDataParser.java
@@ -20,6 +20,9 @@ package org.apache.pig.test;
 import java.io.IOException;
 import java.util.Map;
 
+import org.joda.time.DateTime;
+import org.joda.time.DateTimeZone;
+
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.pig.ResourceSchema;
@@ -109,6 +112,13 @@ public class TestTextDataParser extends junit.framework.TestCase {
         Double d = ps.getLoadCaster().bytesToDouble(myDouble.getBytes());
         assertTrue(d.equals(0.1));
     }
+
+    @Test
+    public void testDateTime() throws Exception{
+        String myDateTime = "1970-01-01T00:00:00.000Z";
+        DateTime d = ps.getLoadCaster().bytesToDateTime(myDateTime.getBytes());
+        assertTrue(d.equals(new DateTime(myDateTime, DateTimeZone.forID("+00:00"))));
+    }
     
     @Test
     public void testString() throws Exception{
diff --git a/test/org/apache/pig/test/TestTypeCheckingValidatorNewLP.java b/test/org/apache/pig/test/TestTypeCheckingValidatorNewLP.java
index 719cc078f..26d01ff91 100644
--- a/test/org/apache/pig/test/TestTypeCheckingValidatorNewLP.java
+++ b/test/org/apache/pig/test/TestTypeCheckingValidatorNewLP.java
@@ -44,6 +44,8 @@ import java.util.Properties;
 
 import junit.framework.Assert;
 
+import org.joda.time.DateTime;
+
 import org.apache.hadoop.conf.Configuration;
 import org.apache.pig.EvalFunc;
 import org.apache.pig.ExecType;
@@ -82,6 +84,8 @@ import org.apache.pig.newplan.logical.expression.DereferenceExpression;
 import org.apache.pig.newplan.logical.expression.DivideExpression;
 import org.apache.pig.newplan.logical.expression.EqualExpression;
 import org.apache.pig.newplan.logical.expression.GreaterThanExpression;
+import org.apache.pig.newplan.logical.expression.GreaterThanEqualExpression;
+import org.apache.pig.newplan.logical.expression.LessThanExpression;
 import org.apache.pig.newplan.logical.expression.LessThanEqualExpression;
 import org.apache.pig.newplan.logical.expression.LogicalExpression;
 import org.apache.pig.newplan.logical.expression.LogicalExpressionPlan;
@@ -699,6 +703,78 @@ public class TestTypeCheckingValidatorNewLP {
         }
     }
 
+    @Test
+    public void testExpressionTypeChecking11() throws Throwable {
+        // test whether conditional operators can accept two datetime operands
+        LogicalExpressionPlan plan = new LogicalExpressionPlan();
+        ConstantExpression constant0 = new ConstantExpression(plan, new DateTime(0L));
+        ConstantExpression constant1 = new ConstantExpression(plan, new DateTime("1970-01-01T00:00:00.000Z"));
+        ConstantExpression constant2 = new ConstantExpression(plan, new DateTime(1L));
+        ConstantExpression constant3 = new ConstantExpression(plan, new DateTime(2L));
+        ConstantExpression constant4 = new ConstantExpression(plan, new DataByteArray("1970-01-01T00:00:00.003Z"));
+
+        LessThanExpression lt1 = new LessThanExpression(plan, constant1, constant2);
+        LessThanEqualExpression lte1 = new LessThanEqualExpression(plan, constant1, constant2);
+        GreaterThanExpression gt1 = new GreaterThanExpression(plan, constant3, constant4);
+        GreaterThanEqualExpression gte1 = new GreaterThanEqualExpression(plan, constant3, constant4);
+        EqualExpression eq1 = new EqualExpression(plan, constant0, constant1);
+        NotEqualExpression neq1 = new NotEqualExpression(plan, constant0, constant2);
+
+        CompilationMessageCollector collector = new CompilationMessageCollector();
+        TypeCheckingExpVisitor expTypeChecker = new TypeCheckingExpVisitor(
+                plan, collector, null);
+        expTypeChecker.visit();
+
+        plan.explain(System.out, "text", true);
+
+        printMessageCollector(collector);
+        // printTypeGraph(plan) ;
+
+        if (collector.hasError()) {
+            throw new Exception("Error during type checking");
+        }
+
+        // Induction check
+        assertEquals(DataType.BOOLEAN, lt1.getType());
+        assertEquals(DataType.BOOLEAN, lte1.getType());
+        assertEquals(DataType.BOOLEAN, gt1.getType());
+        assertEquals(DataType.BOOLEAN, gte1.getType());
+        assertEquals(DataType.BOOLEAN, eq1.getType());
+        assertEquals(DataType.BOOLEAN, neq1.getType());
+
+        // Cast insertion check
+        assertEquals(DataType.DATETIME, gt1.getRhs().getType());
+        assertEquals(DataType.DATETIME, gte1.getRhs().getType());
+    }
+    
+    @Test
+    public void testExpressionTypeCheckingFail11() throws Throwable {
+        // test whether conditional operators will reject the operation of one
+        // value of datetime and one of other type
+        LogicalExpressionPlan plan = new LogicalExpressionPlan();
+        ConstantExpression constant0 = new ConstantExpression(plan, new DateTime(0L));
+        ConstantExpression constant1 = new ConstantExpression(plan, new DataByteArray("1970-01-01T00:00:00.000Z"));
+        CastExpression cast1 = new CastExpression(plan,  constant1, createFS(DataType.BYTEARRAY)) ;
+        EqualExpression eq1 = new EqualExpression(plan, constant0, cast1);
+
+        CompilationMessageCollector collector = new CompilationMessageCollector();
+        TypeCheckingExpVisitor expTypeChecker = new TypeCheckingExpVisitor(
+                plan, collector, null);
+
+        try {
+            expTypeChecker.visit();
+            fail("Exception expected");
+        } catch (TypeCheckerException pve) {
+            // good
+        }
+        printMessageCollector(collector);
+        // printTypeGraph(plan) ;
+
+        if (!collector.hasError()) {
+            throw new Exception("Error during type checking");
+        }
+    }
+
     @Test
     public void testArithmeticOpCastInsert1() throws Throwable {
         LogicalExpressionPlan plan = new LogicalExpressionPlan() ;
diff --git a/test/org/apache/pig/test/utils/GenRandomData.java b/test/org/apache/pig/test/utils/GenRandomData.java
index 8cc718b73..f50159b74 100644
--- a/test/org/apache/pig/test/utils/GenRandomData.java
+++ b/test/org/apache/pig/test/utils/GenRandomData.java
@@ -22,6 +22,8 @@ import java.util.HashMap;
 import java.util.Map;
 import java.util.Random;
 
+import org.joda.time.DateTime;
+
 import org.apache.pig.ResourceSchema;
 import org.apache.pig.ResourceSchema.ResourceFieldSchema;
 import org.apache.pig.data.DataBag;
@@ -188,6 +190,7 @@ public class GenRandomData {
         t.append(genRandMap(r, num));
         t.append(genRandSmallTuple(r, 100));
         t.append(new Boolean(r.nextBoolean()));
+        t.append(new DateTime(r.nextLong()));
         return t;
     }
     
@@ -219,10 +222,13 @@ public class GenRandomData {
 
         ResourceFieldSchema boolfs = new ResourceFieldSchema();
         boolfs.setType(DataType.BOOLEAN);
+
+        ResourceFieldSchema dtfs = new ResourceFieldSchema();
+        dtfs.setType(DataType.DATETIME);
         
         ResourceSchema outSchema = new ResourceSchema();
         outSchema.setFields(new ResourceFieldSchema[]{bagfs, dbafs, stringfs, doublefs, floatfs,
-                intfs, longfs, mapfs, tuplefs, boolfs});
+                intfs, longfs, mapfs, tuplefs, boolfs, dtfs});
         ResourceFieldSchema outfs = new ResourceFieldSchema();
         outfs.setSchema(outSchema);
         outfs.setType(DataType.TUPLE);
@@ -251,6 +257,7 @@ public class GenRandomData {
         t.append(genRandMap(r, num));
         t.append(genRandSmallTuple(r, 100));
         t.append(new Boolean(r.nextBoolean()));
+        t.append(new DateTime(r.nextLong()));
         return t;
     }
     
@@ -313,6 +320,7 @@ public class GenRandomData {
         t.append(genRandMap(r, num));
         t.append(genRandSmallTuple(r, 100));
         t.append(new Boolean(r.nextBoolean()));
+        t.append(new DateTime(r.nextLong()));
         t.append(null);
         return t;
     }
@@ -338,6 +346,7 @@ public class GenRandomData {
         t.append(genRandMap(r, num));
         t.append(genRandSmallTuple(r, 100));
         t.append(new Boolean(r.nextBoolean()));
+        t.append(new DateTime(r.nextLong()));
         t.append(null);
         return t;
     }
@@ -395,6 +404,7 @@ public class GenRandomData {
             t.append("true");
         else
             t.append("false");
+        t.append(new DateTime(r.nextLong()));
         return t;
     }
     
@@ -411,9 +421,11 @@ public class GenRandomData {
         doublefs.setType(DataType.DOUBLE);
         ResourceFieldSchema boolfs = new ResourceFieldSchema();
         boolfs.setType(DataType.BOOLEAN);
+        ResourceFieldSchema dtfs = new ResourceFieldSchema();
+        dtfs.setType(DataType.DATETIME);
         
         ResourceSchema tupleSchema = new ResourceSchema();
-        tupleSchema.setFields(new ResourceFieldSchema[]{stringfs, longfs, intfs, doublefs, floatfs, stringfs, intfs, doublefs, floatfs, boolfs});
+        tupleSchema.setFields(new ResourceFieldSchema[]{stringfs, longfs, intfs, doublefs, floatfs, stringfs, intfs, doublefs, floatfs, boolfs, dtfs});
         ResourceFieldSchema tuplefs = new ResourceFieldSchema();
         tuplefs.setSchema(tupleSchema);
         tuplefs.setType(DataType.TUPLE);
