diff --git a/CHANGES.txt b/CHANGES.txt
index 73b69ffb2..3feb68879 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -24,6 +24,8 @@ INCOMPATIBLE CHANGES
 
 IMPROVEMENTS
 
+PIG-1712: ILLUSTRATE rework (yanz)
+
 PIG-1758: Deep cast of complex type (daijy)
 
 PIG-1728: doc updates (chandec via olgan)
diff --git a/src/org/apache/pig/PigServer.java b/src/org/apache/pig/PigServer.java
index 07eab38bc..231a90264 100644
--- a/src/org/apache/pig/PigServer.java
+++ b/src/org/apache/pig/PigServer.java
@@ -1125,23 +1125,32 @@ public class PigServer {
         return currDAG.getAliasOp().keySet();
     }
 
-    public Map<LogicalOperator, DataBag> getExamples(String alias) {
+    public Map<LogicalOperator, DataBag> getExamples(String alias) throws IOException {
         LogicalPlan plan = null;
-
         try {        
-            if (currDAG.isBatchOn()) {
+            if (currDAG.isBatchOn() && alias != null) {
                 currDAG.execute();
             }
-            
-            plan = getClonedGraph().getPlan(alias);
+            Graph g = getClonedGraph();
+            plan = g.getPlan(alias);
+            plan = compileLp(plan, g, false);
         } catch (IOException e) {
             //Since the original script is parsed anyway, there should not be an
             //error in this parsing. The only reason there can be an error is when
             //the files being loaded in load don't exist anymore.
             e.printStackTrace();
         }
+
         ExampleGenerator exgen = new ExampleGenerator(plan, pigContext);
-        return exgen.getExamples();
+        try {
+            return exgen.getExamples();
+        } catch (ExecException e) {
+            e.printStackTrace(System.out);
+            throw new IOException("ExecException : "+ e.getMessage());
+        } catch (Exception e) {
+            e.printStackTrace(System.out);
+            throw new IOException("Exception : "+ e.getMessage());
+        }
     }
 
     private LogicalPlan getStorePlan(String alias) throws IOException {
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/HExecutionEngine.java b/src/org/apache/pig/backend/hadoop/executionengine/HExecutionEngine.java
index 5f3a3861d..2c4a014e7 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/HExecutionEngine.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/HExecutionEngine.java
@@ -57,6 +57,8 @@ import org.apache.pig.impl.PigContext;
 import org.apache.pig.impl.io.FileLocalizer;
 import org.apache.pig.impl.io.FileSpec;
 import org.apache.pig.impl.logicalLayer.FrontendException;
+import org.apache.pig.impl.logicalLayer.LOForEach;
+import org.apache.pig.impl.logicalLayer.LogicalOperator;
 import org.apache.pig.impl.logicalLayer.LogicalPlan;
 import org.apache.pig.impl.plan.NodeIdGenerator;
 import org.apache.pig.impl.plan.OperatorKey;
@@ -69,17 +71,17 @@ import org.apache.pig.newplan.logical.LogicalPlanMigrationVistor;
 import org.apache.pig.newplan.logical.expression.ConstantExpression;
 import org.apache.pig.newplan.logical.expression.LogicalExpressionPlan;
 import org.apache.pig.newplan.logical.optimizer.SchemaResetter;
-import org.apache.pig.newplan.logical.optimizer.ProjectionPatcher.ProjectionFinder;
 import org.apache.pig.newplan.logical.relational.LOLimit;
 import org.apache.pig.newplan.logical.relational.LOSort;
 import org.apache.pig.newplan.logical.relational.LOSplit;
 import org.apache.pig.newplan.logical.relational.LOSplitOutput;
 import org.apache.pig.newplan.logical.relational.LOStore;
 import org.apache.pig.newplan.logical.relational.LogicalRelationalNodesVisitor;
+import org.apache.pig.newplan.logical.relational.LogicalRelationalOperator;
 import org.apache.pig.newplan.logical.rules.InputOutputFileValidator;
-import org.apache.pig.newplan.optimizer.Rule;
 import org.apache.pig.tools.pigstats.OutputStats;
 import org.apache.pig.tools.pigstats.PigStats;
+import org.apache.pig.pen.POOptimizeDisabler;
 
 public class HExecutionEngine {
     
@@ -105,6 +107,11 @@ public class HExecutionEngine {
     // map from LOGICAL key to into about the execution
     protected Map<OperatorKey, MapRedResult> materializedResults;
     
+    protected Map<LogicalOperator, PhysicalOperator> logToPhyMap;
+    protected Map<LogicalOperator, LogicalRelationalOperator> opsMap;
+    protected Map<Operator, PhysicalOperator> newLogToPhyMap;
+    private Map<LOForEach, Map<LogicalOperator, LogicalRelationalOperator>> forEachInnerOpMap;
+    
     public HExecutionEngine(PigContext pigContext) {
         this.pigContext = pigContext;
         this.logicalToPhysicalKeys = new HashMap<OperatorKey, OperatorKey>();      
@@ -255,8 +262,16 @@ public class HExecutionEngine {
                 // translate old logical plan to new plan
                 LogicalPlanMigrationVistor visitor = new LogicalPlanMigrationVistor(plan);
                 visitor.visit();
+                opsMap = visitor.getOldToNewLOOpMap();
+                forEachInnerOpMap = visitor.getForEachInnerMap();
                 org.apache.pig.newplan.logical.relational.LogicalPlan newPlan = visitor.getNewLogicalPlan();
                 
+                if (pigContext.inIllustrator) {
+                    // disable all PO-specific optimizations
+                    POOptimizeDisabler pod = new POOptimizeDisabler(newPlan);
+                    pod.visit();
+                }
+                
                 SchemaResetter schemaResetter = new SchemaResetter(newPlan);
                 schemaResetter.visit();
                 
@@ -271,6 +286,22 @@ public class HExecutionEngine {
                     throw new FrontendException(msg, errCode, PigException.BUG, ioe);
                 }
                 
+                if (pigContext.inIllustrator) {
+                    // disable MergeForEach in illustrator
+                    if (optimizerRules == null)
+                        optimizerRules = new HashSet<String>();
+                    optimizerRules.add("MergeForEach");
+                    optimizerRules.add("PartitionFilterOptimizer");
+                    optimizerRules.add("LimitOptimizer");
+                    optimizerRules.add("SplitFilter");
+                    optimizerRules.add("PushUpFilter");
+                    optimizerRules.add("MergeFilter");
+                    optimizerRules.add("PushDownForEachFlatten");
+                    optimizerRules.add("ColumnMapKeyPrune");
+                    optimizerRules.add("AddForEach");
+                    optimizerRules.add("GroupByConstParallelSetter");
+                }
+                
                 // run optimizer
                 org.apache.pig.newplan.logical.optimizer.LogicalPlanOptimizer optimizer = 
                     new org.apache.pig.newplan.logical.optimizer.LogicalPlanOptimizer(newPlan, 100, optimizerRules);
@@ -294,6 +325,7 @@ public class HExecutionEngine {
                 
                 translator.setPigContext(pigContext);
                 translator.visit();
+                newLogToPhyMap = translator.getLogToPhyMap();
                 return translator.getPhysicalPlan();
                 
             }else{       
@@ -303,13 +335,40 @@ public class HExecutionEngine {
                 translator.visit();
                 return translator.getPhysicalPlan();
             }
-        } catch (Exception ve) {
+        } catch (ExecException ve) {
             int errCode = 2042;
             String msg = "Error in new logical plan. Try -Dpig.usenewlogicalplan=false.";
             throw new FrontendException(msg, errCode, PigException.BUG, ve);
         }
     }
     
+    public Map<LogicalOperator, PhysicalOperator> getLogToPhyMap() {
+        if (logToPhyMap != null)
+            return logToPhyMap;
+        else if (newLogToPhyMap != null) {
+            Map<LogicalOperator, PhysicalOperator> result = new HashMap<LogicalOperator, PhysicalOperator>();
+            for (LogicalOperator lo: opsMap.keySet()) {
+                result.put(lo, newLogToPhyMap.get(opsMap.get(lo))); 
+            }
+            return result;
+        } else
+            return null;
+    }
+    
+    public Map<LOForEach, Map<LogicalOperator, PhysicalOperator>> getForEachInnerLogToPhyMap() {
+        Map<LOForEach, Map<LogicalOperator, PhysicalOperator>> result =
+            new HashMap<LOForEach, Map<LogicalOperator, PhysicalOperator>>();
+        for (Map.Entry<LOForEach, Map<LogicalOperator, LogicalRelationalOperator>> entry :
+            forEachInnerOpMap.entrySet()) {
+            Map<LogicalOperator, PhysicalOperator> innerOpMap = new HashMap<LogicalOperator, PhysicalOperator>();
+            for (Map.Entry<LogicalOperator, LogicalRelationalOperator> innerEntry : entry.getValue().entrySet()) {
+                innerOpMap.put(innerEntry.getKey(), newLogToPhyMap.get(innerEntry.getValue()));
+            }
+            result.put(entry.getKey(), innerOpMap);
+        }
+        return result;
+    }
+    
     public static class SortInfoSetter extends LogicalRelationalNodesVisitor {
 
         public SortInfoSetter(OperatorPlan plan) throws FrontendException {
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java
index 3a348e0d5..88efe8262 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java
@@ -174,7 +174,10 @@ public class JobControlCompiler{
         UDFContext.getUDFContext().reset();
     }
 
-    Map<Job, MapReduceOper> getJobMroMap() {
+    /**
+     * Gets the map of Job and the MR Operator
+     */
+    public Map<Job, MapReduceOper> getJobMroMap() {
         return Collections.unmodifiableMap(jobMroMap);
     }
     
@@ -378,19 +381,23 @@ public class JobControlCompiler{
                     inpTargets.add(ldSucKeys);
                     inpSignatureLists.add(ld.getSignature());
                     //Remove the POLoad from the plan
-                    mro.mapPlan.remove(ld);
+                    if (!pigContext.inIllustrator)
+                        mro.mapPlan.remove(ld);
                 }
             }
 
-            //Create the jar of all functions and classes required
-            File submitJarFile = File.createTempFile("Job", ".jar");
-            // ensure the job jar is deleted on exit
-            submitJarFile.deleteOnExit();
-            FileOutputStream fos = new FileOutputStream(submitJarFile);
-            JarManager.createJar(fos, mro.UDFs, pigContext);
+            if (!pigContext.inIllustrator) 
+            {
+                //Create the jar of all functions and classes required
+                File submitJarFile = File.createTempFile("Job", ".jar");
+                // ensure the job jar is deleted on exit
+                submitJarFile.deleteOnExit();
+                FileOutputStream fos = new FileOutputStream(submitJarFile);
+                JarManager.createJar(fos, mro.UDFs, pigContext);
             
-            //Start setting the JobConf properties
-            conf.set("mapred.jar", submitJarFile.getPath());
+                //Start setting the JobConf properties
+                conf.set("mapred.jar", submitJarFile.getPath());
+            }
             conf.set("pig.inputs", ObjectSerializer.serialize(inp));
             conf.set("pig.inpTargets", ObjectSerializer.serialize(inpTargets));
             conf.set("pig.inpSignatures", ObjectSerializer.serialize(inpSignatureLists));
@@ -457,22 +464,23 @@ public class JobControlCompiler{
                 POStore st;
                 if (reduceStores.isEmpty()) {
                     st = mapStores.get(0);
-                    mro.mapPlan.remove(st);
+                    if(!pigContext.inIllustrator)
+                        mro.mapPlan.remove(st);
                 }
                 else {
                     st = reduceStores.get(0);
-                    mro.reducePlan.remove(st);
+                    if(!pigContext.inIllustrator)
+                        mro.reducePlan.remove(st);
                 }
 
                 // set out filespecs
                 String outputPath = st.getSFile().getFileName();
-                FuncSpec outputFuncSpec = st.getSFile().getFuncSpec();
                 
                 conf.set("pig.streaming.log.dir", 
                             new Path(outputPath, LOG_DIR).toString());
                 conf.set("pig.streaming.task.output.dir", outputPath);
             } 
-           else { // multi store case
+           else if (mapStores.size() + reduceStores.size() > 0) { // multi store case
                 log.info("Setting up multi store job");
                 String tmpLocationStr =  FileLocalizer
                 .getTemporaryPath(pigContext).toString();
@@ -513,7 +521,8 @@ public class JobControlCompiler{
                 //MapOnly Job
                 nwJob.setMapperClass(PigMapOnly.Map.class);
                 nwJob.setNumReduceTasks(0);
-                conf.set("pig.mapPlan", ObjectSerializer.serialize(mro.mapPlan));
+                if(!pigContext.inIllustrator)
+                    conf.set("pig.mapPlan", ObjectSerializer.serialize(mro.mapPlan));
                 if(mro.isEndOfAllInputSetInMap()) {
                     // this is used in Map.close() to decide whether the
                     // pipeline needs to be rerun one more time in the close()
@@ -535,7 +544,8 @@ public class JobControlCompiler{
                     log.info("Setting identity combiner class.");
                 }
                 pack = (POPackage)mro.reducePlan.getRoots().get(0);
-                mro.reducePlan.remove(pack);
+                if(!pigContext.inIllustrator)
+                    mro.reducePlan.remove(pack);
                 nwJob.setMapperClass(PigMapReduce.Map.class);
                 nwJob.setReducerClass(PigMapReduce.Reduce.class);
                 
@@ -550,21 +560,24 @@ public class JobControlCompiler{
                 if (mro.customPartitioner != null)
                 	nwJob.setPartitionerClass(PigContext.resolveClassName(mro.customPartitioner));
 
-                conf.set("pig.mapPlan", ObjectSerializer.serialize(mro.mapPlan));
+                if(!pigContext.inIllustrator)
+                    conf.set("pig.mapPlan", ObjectSerializer.serialize(mro.mapPlan));
                 if(mro.isEndOfAllInputSetInMap()) {
                     // this is used in Map.close() to decide whether the
                     // pipeline needs to be rerun one more time in the close()
                     // The pipeline is rerun only if there was a stream or merge-join.
                     conf.set(END_OF_INP_IN_MAP, "true");
                 }
-                conf.set("pig.reducePlan", ObjectSerializer.serialize(mro.reducePlan));
+                if(!pigContext.inIllustrator)
+                    conf.set("pig.reducePlan", ObjectSerializer.serialize(mro.reducePlan));
                 if(mro.isEndOfAllInputSetInReduce()) {
                     // this is used in Map.close() to decide whether the
                     // pipeline needs to be rerun one more time in the close()
                     // The pipeline is rerun only if there was a stream
                     conf.set("pig.stream.in.reduce", "true");
                 }
-                conf.set("pig.reduce.package", ObjectSerializer.serialize(pack));
+                if (!pigContext.inIllustrator)
+                    conf.set("pig.reduce.package", ObjectSerializer.serialize(pack));
                 conf.set("pig.reduce.key.type", Byte.toString(pack.getKeyType())); 
                 
                 if (mro.getUseSecondaryKey()) {
@@ -631,9 +644,14 @@ public class JobControlCompiler{
                 nwJob.setGroupingComparatorClass(PigGroupingPartitionWritableComparator.class);
             }
             
-            // unset inputs for POStore, otherwise, map/reduce plan will be unnecessarily deserialized 
-            for (POStore st: mapStores) { st.setInputs(null); st.setParentPlan(null);}
-            for (POStore st: reduceStores) { st.setInputs(null); st.setParentPlan(null);}
+            if (!pigContext.inIllustrator)
+            {
+                // unset inputs for POStore, otherwise, map/reduce plan will be unnecessarily deserialized 
+                for (POStore st: mapStores) { st.setInputs(null); st.setParentPlan(null);}
+                for (POStore st: reduceStores) { st.setInputs(null); st.setParentPlan(null);}
+                conf.set(PIG_MAP_STORES, ObjectSerializer.serialize(mapStores));
+                conf.set(PIG_REDUCE_STORES, ObjectSerializer.serialize(reduceStores));
+            }
 
             // tmp file compression setups
             if (Utils.tmpFileCompression(pigContext)) {
@@ -641,8 +659,6 @@ public class JobControlCompiler{
                 conf.set("pig.tmpfilecompression.codec", Utils.tmpFileCompressionCodec(pigContext));
             }
 
-            conf.set(PIG_MAP_STORES, ObjectSerializer.serialize(mapStores));
-            conf.set(PIG_REDUCE_STORES, ObjectSerializer.serialize(reduceStores));
             String tmp;
             long maxCombinedSplitSize = 0;
             if (!mro.combineSmallSplits() || pigContext.getProperties().getProperty("pig.splitCombination", "true").equals("false"))
@@ -661,7 +677,6 @@ public class JobControlCompiler{
             UDFContext.getUDFContext().serialize(conf);
             Job cjob = new Job(new JobConf(nwJob.getConfiguration()), new ArrayList());
             jobStoreMap.put(cjob,new Pair<List<POStore>, Path>(storeLocations, tmpLocation));
-            
             return cjob;
             
         } catch (JobCreationException jce) {
@@ -1142,7 +1157,7 @@ public class JobControlCompiler{
             PigContext pigContext, Configuration conf, String filename,
             String prefix) throws IOException {
 
-        if (!FileLocalizer.fileExists(filename, pigContext)) {
+        if (!pigContext.inIllustrator && !FileLocalizer.fileExists(filename, pigContext)) {
             throw new IOException(
                     "Internal error: skew join partition file "
                     + filename + " does not exist");
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRCompiler.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRCompiler.java
index 4acf46399..72ff51dd8 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRCompiler.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRCompiler.java
@@ -109,7 +109,6 @@ import org.apache.pig.impl.util.ObjectSerializer;
 import org.apache.pig.impl.util.Pair;
 import org.apache.pig.impl.util.UriUtil;
 import org.apache.pig.impl.util.Utils;
-import org.mortbay.util.URIUtil;
 
 /**
  * The compiler that compiles a given physical plan
@@ -311,6 +310,7 @@ public class MRCompiler extends PhyPlanVisitor {
     public MROperPlan compile() throws IOException, PlanException, VisitorException {
         List<PhysicalOperator> leaves = plan.getLeaves();
 
+        if (!pigContext.inIllustrator)
         for (PhysicalOperator op : leaves) {
             if (!(op instanceof POStore)) {
                 int errCode = 2025;
@@ -324,8 +324,14 @@ public class MRCompiler extends PhyPlanVisitor {
         // and compile their plans
         List<POStore> stores = PlanHelper.getStores(plan);
         List<PONative> nativeMRs= PlanHelper.getNativeMRs(plan);
-        List<PhysicalOperator> ops = new ArrayList<PhysicalOperator>(stores.size() + nativeMRs.size());
-        ops.addAll(stores);
+        List<PhysicalOperator> ops;
+        if (!pigContext.inIllustrator) {
+            ops = new ArrayList<PhysicalOperator>(stores.size() + nativeMRs.size());
+            ops.addAll(stores);
+        } else {
+            ops = new ArrayList<PhysicalOperator>(leaves.size() + nativeMRs.size());
+            ops.addAll(leaves);
+        }
         ops.addAll(nativeMRs);
         Collections.sort(ops);
         
@@ -1005,16 +1011,23 @@ public class MRCompiler extends PhyPlanVisitor {
             if (!mro.isMapDone()) {
             	// if map plan is open, add a limit for optimization, eventually we
             	// will add another limit to reduce plan
-                mro.mapPlan.addAsLeaf(op);
-                mro.setMapDone(true);
+                if (!pigContext.inIllustrator)
+                {
+                    mro.mapPlan.addAsLeaf(op);
+                    mro.setMapDone(true);
+                }
                 
                 if (mro.reducePlan.isEmpty())
                 {
                     simpleConnectMapToReduce(mro);
                     mro.requestedParallelism = 1;
-                    POLimit pLimit2 = new POLimit(new OperatorKey(scope,nig.getNextNodeId(scope)));
-                    pLimit2.setLimit(op.getLimit());
-                    mro.reducePlan.addAsLeaf(pLimit2);
+                    if (!pigContext.inIllustrator) {
+                        POLimit pLimit2 = new POLimit(new OperatorKey(scope,nig.getNextNodeId(scope)));
+                        pLimit2.setLimit(op.getLimit());
+                        mro.reducePlan.addAsLeaf(pLimit2);
+                    } else {
+                        mro.reducePlan.addAsLeaf(op);
+                    }
                 }
                 else
                 {
@@ -1848,6 +1861,7 @@ public class MRCompiler extends PhyPlanVisitor {
             curMROp.reducePlan.addAsLeaf(nfe1);
             curMROp.setNeedsDistinctCombiner(true);
             phyToMROpMap.put(op, curMROp);
+            curMROp.phyToMRMap.put(op, nfe1);
         }catch(Exception e){
             int errCode = 2034;
             String msg = "Error compiling operator " + op.getClass().getSimpleName();
@@ -2221,12 +2235,13 @@ public class MRCompiler extends PhyPlanVisitor {
         POForEach nfe1 = new POForEach(new OperatorKey(scope,nig.getNextNodeId(scope)),-1,eps2,flattened);
         mro.reducePlan.add(nfe1);
         mro.reducePlan.connect(pkg, nfe1);
-        
+        mro.phyToMRMap.put(sort, nfe1);
         if (limit!=-1)
         {
 	        POLimit pLimit2 = new POLimit(new OperatorKey(scope,nig.getNextNodeId(scope)));
 	    	pLimit2.setLimit(limit);
 	    	mro.reducePlan.addAsLeaf(pLimit2);
+	    	mro.phyToMRMap.put(sort, pLimit2);
         }
 
 //        ep1.add(innGen);
@@ -2649,7 +2664,7 @@ public class MRCompiler extends PhyPlanVisitor {
             POPackage pack = (POPackage)op;
             
             List<PhysicalOperator> sucs = mr.reducePlan.getSuccessors(pack);
-            if (sucs.size()!=1) {
+            if (sucs == null || sucs.size()!=1) {
                 return;
             }
             
@@ -2739,12 +2754,12 @@ public class MRCompiler extends PhyPlanVisitor {
                 {
                     // Now we can optimize the map-reduce plan
                     // Replace POPackage->POForeach to POJoinPackage
-                    replaceWithPOJoinPackage(mr.reducePlan, pack, forEach, chunkSize);
+                    replaceWithPOJoinPackage(mr.reducePlan, mr, pack, forEach, chunkSize);
                 }
             }
         }
 
-        public static void replaceWithPOJoinPackage(PhysicalPlan plan,
+        public static void replaceWithPOJoinPackage(PhysicalPlan plan, MapReduceOper mr,
                 POPackage pack, POForEach forEach, String chunkSize) throws VisitorException {
             String scope = pack.getOperatorKey().scope;
             NodeIdGenerator nig = NodeIdGenerator.getGenerator();
@@ -2772,7 +2787,7 @@ public class MRCompiler extends PhyPlanVisitor {
                 String msg = "Error rewriting POJoinPackage.";
                 throw new MRCompilerException(msg, errCode, PigException.BUG, e);
             }
-            
+            mr.phyToMRMap.put(forEach, joinPackage);
             LogFactory.
             getLog(LastInputStreamingOptimizer.class).info("Rewrite: POPackage->POForEach to POJoinPackage");
         }
@@ -2800,6 +2815,7 @@ public class MRCompiler extends PhyPlanVisitor {
                 throw new MRCompilerException(msg, errCode, PigException.BUG);
             }
             PhysicalOperator mpLeaf = mpLeaves.get(0);
+            if (!pigContext.inIllustrator)
             if (!(mpLeaf instanceof POStore)) {
                 int errCode = 2025;
                 String msg = "Expected leaf of reduce plan to " +
@@ -2885,6 +2901,7 @@ public class MRCompiler extends PhyPlanVisitor {
                     throw new MRCompilerException(msg, errCode, PigException.BUG);
                 }
                 PhysicalOperator mpLeaf = mpLeaves.get(0);
+                if (!pigContext.inIllustrator)
                 if (!(mpLeaf instanceof POStore)) {
                     int errCode = 2025;
                     String msg = "Expected leaf of reduce plan to " +
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java
index ca4cf59d2..626be25f8 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java
@@ -461,7 +461,7 @@ public class MapReduceLauncher extends Launcher{
         }
     }
 
-    private MROperPlan compile(
+    public MROperPlan compile(
             PhysicalPlan php,
             PigContext pc) throws PlanException, IOException, VisitorException {
         MRCompiler comp = new MRCompiler(php, pc);
@@ -479,7 +479,7 @@ public class MapReduceLauncher extends Launcher{
         
         //String prop = System.getProperty("pig.exec.nocombiner");
         String prop = pc.getProperties().getProperty("pig.exec.nocombiner");
-        if (!("true".equals(prop)))  {
+        if (!pc.inIllustrator && !("true".equals(prop)))  {
             CombinerOptimizer co = new CombinerOptimizer(plan, lastInputChunkSize);
             co.visit();
             //display the warning message(s) from the CombinerOptimizer
@@ -493,7 +493,7 @@ public class MapReduceLauncher extends Launcher{
         
         // Optimize to use secondary sort key if possible
         prop = pc.getProperties().getProperty("pig.exec.nosecondarykey");
-        if (!("true".equals(prop)))  {
+        if (!pc.inIllustrator && !("true".equals(prop)))  {
             SecondaryKeyOptimizer skOptimizer = new SecondaryKeyOptimizer(plan);
             skOptimizer.visit();
         }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceOper.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceOper.java
index 871716b77..6fb1b0858 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceOper.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceOper.java
@@ -20,6 +20,8 @@ package org.apache.pig.backend.hadoop.executionengine.mapReduceLayer;
 import java.io.ByteArrayOutputStream;
 import java.util.HashSet;
 import java.util.Set;
+import java.util.Map;
+import java.util.HashMap;
 
 import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.impl.plan.NodeIdGenerator;
@@ -146,6 +148,10 @@ public class MapReduceOper extends Operator<MROpPlanVisitor> {
 	// are NOT combinable for correctness.
 	private boolean combineSmallSplits = true;
 	
+	// Map of the physical operator in physical plan to the one in MR plan: only needed
+	// if the physical operator is changed/replaced in MR compilation due to, e.g., optimization
+	public Map<PhysicalOperator, PhysicalOperator> phyToMRMap;
+	
 	private static enum OPER_FEATURE {
 	    NONE,
 	    // Indicate if this job is a sampling job
@@ -169,6 +175,7 @@ public class MapReduceOper extends Operator<MROpPlanVisitor> {
         scalars = new HashSet<PhysicalOperator>();
         nig = NodeIdGenerator.getGenerator();
         scope = k.getScope();
+        phyToMRMap = new HashMap<PhysicalOperator, PhysicalOperator>();
     }
 
     /*@Override
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PhyPlanSetter.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PhyPlanSetter.java
index 6d179b0b1..a6a1e8dd8 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PhyPlanSetter.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PhyPlanSetter.java
@@ -276,13 +276,6 @@ public class PhyPlanSetter extends PhyPlanVisitor {
         stream.setParentPlan(parent);
     }
 
-    @Override
-    public void visitLocalRearrangeForIllustrate(
-            POLocalRearrangeForIllustrate lrfi) throws VisitorException {
-        super.visitLocalRearrangeForIllustrate(lrfi);
-        lrfi.setParentPlan(parent);
-    }
-
 /*
     @Override
     public void visitPartitionRearrange(POPartitionRearrange lrfi) throws VisitorException {
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapBase.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapBase.java
index bcd54904d..0302ec81c 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapBase.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapBase.java
@@ -21,6 +21,7 @@ import java.io.ByteArrayOutputStream;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
+import java.util.Iterator;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
@@ -28,6 +29,8 @@ import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.mapreduce.Mapper;
+import org.apache.hadoop.mapreduce.InputSplit;
+import org.apache.hadoop.mapreduce.TaskAttemptID;
 import org.apache.log4j.PropertyConfigurator;
 import org.apache.pig.PigException;
 import org.apache.pig.backend.executionengine.ExecException;
@@ -40,6 +43,7 @@ import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOpe
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.util.PlanHelper;
 import org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil;
 import org.apache.pig.data.Tuple;
+import org.apache.pig.data.DataBag;
 import org.apache.pig.data.TupleFactory;
 import org.apache.pig.impl.PigContext;
 import org.apache.pig.impl.io.PigNullableWritable;
@@ -48,6 +52,7 @@ import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.impl.plan.VisitorException;
 import org.apache.pig.impl.util.ObjectSerializer;
 import org.apache.pig.impl.util.SpillableMemoryManager;
+import org.apache.pig.impl.util.Pair;
 import org.apache.pig.tools.pigstats.PigStatusReporter;
 
 public abstract class PigMapBase extends Mapper<Text, Tuple, PigNullableWritable, Writable> {
@@ -58,13 +63,15 @@ public abstract class PigMapBase extends Mapper<Text, Tuple, PigNullableWritable
     protected byte keyType;
         
     //Map Plan
-    protected PhysicalPlan mp;
+    protected PhysicalPlan mp = null;
 
     // Store operators
     protected List<POStore> stores;
 
     protected TupleFactory tf = TupleFactory.getInstance();
     
+    boolean inIllustrator = false;
+    
     Context outputCollector;
     
     // Reporter that will be used by operators
@@ -80,6 +87,14 @@ public abstract class PigMapBase extends Mapper<Text, Tuple, PigNullableWritable
     PigContext pigContext = null;
     private volatile boolean initialized = false;
     
+    /**
+     * for local map/reduce simulation
+     * @param plan the map plan
+     */
+    public void setMapPlan(PhysicalPlan plan) {
+        mp = plan;
+    }
+    
     /**
      * Will be called when all the tuples in the input
      * are done. So reporter thread should be closed.
@@ -142,14 +157,16 @@ public abstract class PigMapBase extends Mapper<Text, Tuple, PigNullableWritable
         SpillableMemoryManager.configure(ConfigurationUtil.toProperties(job));
         PigMapReduce.sJobContext = context;
         PigMapReduce.sJobConf = context.getConfiguration();
+        inIllustrator = (context instanceof IllustratorContext);
         
         PigContext.setPackageImportList((ArrayList<String>)ObjectSerializer.deserialize(job.get("udf.import.list")));
         pigContext = (PigContext)ObjectSerializer.deserialize(job.get("pig.pigContext"));
         if (pigContext.getLog4jProperties()!=null)
             PropertyConfigurator.configure(pigContext.getLog4jProperties());
         
-        mp = (PhysicalPlan) ObjectSerializer.deserialize(
-            job.get("pig.mapPlan"));
+        if (mp == null)
+            mp = (PhysicalPlan) ObjectSerializer.deserialize(
+                job.get("pig.mapPlan"));
         stores = PlanHelper.getStores(mp);
         
         // To be removed
@@ -207,7 +224,8 @@ public abstract class PigMapBase extends Mapper<Text, Tuple, PigNullableWritable
                 MapReducePOStoreImpl impl 
                     = new MapReducePOStoreImpl(context);
                 store.setStoreImpl(impl);
-                store.setUp();
+                if (!pigContext.inIllustrator)
+                    store.setUp();
             }
             
             boolean aggregateWarning = "true".equalsIgnoreCase(pigContext.getProperties().getProperty("aggregate.warning"));
@@ -225,7 +243,13 @@ public abstract class PigMapBase extends Mapper<Text, Tuple, PigNullableWritable
         }
         
         for (PhysicalOperator root : roots) {
-            root.attachInput(tf.newTupleNoCopy(inpTuple.getAll()));
+            if (inIllustrator) {
+                if (root != null) {
+                    root.attachInput(inpTuple);
+                }
+            } else {
+                root.attachInput(tf.newTupleNoCopy(inpTuple.getAll()));
+            }
         }
             
         runPipeline(leaf);
@@ -284,4 +308,76 @@ public abstract class PigMapBase extends Mapper<Text, Tuple, PigNullableWritable
         this.keyType = keyType;
     }
     
+    /**
+     * 
+     * Get mapper's illustrator context
+     * 
+     * @param conf  Configuration
+     * @param input Input bag to serve as data source
+     * @param output Map output buffer
+     * @param split the split
+     * @return Illustrator's context
+     * @throws IOException
+     * @throws InterruptedException
+     */
+    public Context getIllustratorContext(Configuration conf, DataBag input,
+          List<Pair<PigNullableWritable, Writable>> output, InputSplit split)
+          throws IOException, InterruptedException {
+        return new IllustratorContext(conf, input, output, split);
+    }
+    
+    public class IllustratorContext extends Context {
+        private DataBag input;
+        List<Pair<PigNullableWritable, Writable>> output;
+        private Iterator<Tuple> it = null;
+        private Tuple value = null;
+        private boolean init  = false;
+
+        public IllustratorContext(Configuration conf, DataBag input,
+              List<Pair<PigNullableWritable, Writable>> output,
+              InputSplit split) throws IOException, InterruptedException {
+              super(conf, new TaskAttemptID(), null, null, null, null, split);
+              if (output == null)
+                  throw new IOException("Null output can not be used");
+              this.input = input; this.output = output;
+        }
+        
+        @Override
+        public boolean nextKeyValue() throws IOException, InterruptedException {
+            if (input == null) {
+                if (!init) {
+                    init = true;
+                    return true;
+                }
+                return false;
+            }
+            if (it == null)
+                it = input.iterator();
+            if (!it.hasNext())
+                return false;
+            value = it.next();
+            return true;
+        }
+        
+        @Override
+        public Text getCurrentKey() {
+          return null;
+        }
+        
+        @Override
+        public Tuple getCurrentValue() {
+          return value;
+        }
+        
+        @Override
+        public void write(PigNullableWritable key, Writable value) 
+            throws IOException, InterruptedException {
+            output.add(new Pair<PigNullableWritable, Writable>(key, value));
+        }
+        
+        @Override
+        public void progress() {
+          
+        }
+    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapReduce.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapReduce.java
index 6d219b650..c5bf58a24 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapReduce.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigMapReduce.java
@@ -18,18 +18,24 @@
 package org.apache.pig.backend.hadoop.executionengine.mapReduceLayer;
 
 import java.io.ByteArrayOutputStream;
+import java.io.DataOutputStream;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Iterator;
 import java.util.List;
+import java.util.Collections;
+import java.util.Comparator;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.io.RawComparator;
 import org.apache.hadoop.mapreduce.JobContext;
+import org.apache.hadoop.mapred.jobcontrol.Job;
 import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.log4j.PropertyConfigurator;
+import org.apache.hadoop.mapreduce.TaskAttemptID;
+
 import org.apache.pig.PigException;
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.backend.hadoop.HDataType;
@@ -43,6 +49,7 @@ import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOpe
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.util.PlanHelper;
 import org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil;
+import org.apache.pig.pen.FakeRawKeyValueIterator;
 import org.apache.pig.data.DataBag;
 import org.apache.pig.data.DataType;
 import org.apache.pig.data.Tuple;
@@ -55,6 +62,7 @@ import org.apache.pig.impl.plan.VisitorException;
 import org.apache.pig.impl.util.ObjectSerializer;
 import org.apache.pig.impl.util.SpillableMemoryManager;
 import org.apache.pig.impl.util.UDFContext;
+import org.apache.pig.impl.util.Pair;
 import org.apache.pig.tools.pigstats.PigStatusReporter;
 
 /**
@@ -111,8 +119,8 @@ public class PigMapReduce {
             // value.  The value needs it so that POPackage can properly
             // assign the tuple to its slot in the projection.
             key.setIndex(index);
-            val.setIndex(index);         	
-            	
+            val.setIndex(index);
+
             oc.write(key, val);
         }
     }
@@ -194,7 +202,6 @@ public class PigMapReduce {
             // set the partition
             wrappedKey.setPartition(partitionIndex);
             val.setIndex(index);
-
             oc.write(wrappedKey, val);
         }
 
@@ -254,7 +261,7 @@ public class PigMapReduce {
         protected final Log log = LogFactory.getLog(getClass());
         
         //The reduce plan
-        protected PhysicalPlan rp;
+        protected PhysicalPlan rp = null;
 
         // Store operators
         protected List<POStore> stores;
@@ -279,6 +286,16 @@ public class PigMapReduce {
         PigContext pigContext = null;
         protected volatile boolean initialized = false;
         
+        private boolean inIllustrator = false;
+        
+        /**
+         * Set the reduce plan: to be used by local runner for illustrator
+         * @param plan Reduce plan
+         */
+        public void setReducePlan(PhysicalPlan plan) {
+            rp = plan;
+        }
+
         /**
          * Configures the Reduce plan, the POPackage operator
          * and the reporter thread
@@ -287,7 +304,9 @@ public class PigMapReduce {
         @Override
         protected void setup(Context context) throws IOException, InterruptedException {
             super.setup(context);
-            
+            inIllustrator = (context instanceof IllustratorContext);
+            if (inIllustrator)
+                pack = ((IllustratorContext) context).pack;
             Configuration jConf = context.getConfiguration();
             SpillableMemoryManager.configure(ConfigurationUtil.toProperties(jConf));
             sJobContext = context;
@@ -296,11 +315,13 @@ public class PigMapReduce {
                 PigContext.setPackageImportList((ArrayList<String>)ObjectSerializer.deserialize(jConf.get("udf.import.list")));
                 pigContext = (PigContext)ObjectSerializer.deserialize(jConf.get("pig.pigContext"));
                 
-                rp = (PhysicalPlan) ObjectSerializer.deserialize(jConf
-                        .get("pig.reducePlan"));
+                if (rp == null)
+                    rp = (PhysicalPlan) ObjectSerializer.deserialize(jConf
+                            .get("pig.reducePlan"));
                 stores = PlanHelper.getStores(rp);
 
-                pack = (POPackage)ObjectSerializer.deserialize(jConf.get("pig.reduce.package"));
+                if (!inIllustrator)
+                    pack = (POPackage)ObjectSerializer.deserialize(jConf.get("pig.reduce.package"));
                 // To be removed
                 if(rp.isEmpty())
                     log.debug("Reduce Plan empty!");
@@ -352,12 +373,13 @@ public class PigMapReduce {
                 
                 PhysicalOperator.setPigLogger(pigHadoopLogger);
 
-                for (POStore store: stores) {
-                    MapReducePOStoreImpl impl 
-                        = new MapReducePOStoreImpl(context);
-                    store.setStoreImpl(impl);
-                    store.setUp();
-                }
+                if (!inIllustrator)
+                    for (POStore store: stores) {
+                        MapReducePOStoreImpl impl 
+                            = new MapReducePOStoreImpl(context);
+                        store.setStoreImpl(impl);
+                        store.setUp();
+                    }
             }
           
             // In the case we optimize the join, we combine
@@ -512,6 +534,127 @@ public class PigMapReduce {
             PhysicalOperator.setReporter(null);
             initialized = false;
         }
+        
+        /**
+         * Get reducer's illustrator context
+         * 
+         * @param input Input buffer as output by maps
+         * @param pkg package
+         * @return reducer's illustrator context
+         * @throws IOException
+         * @throws InterruptedException
+         */
+        public Context getIllustratorContext(Job job,
+               List<Pair<PigNullableWritable, Writable>> input, POPackage pkg) throws IOException, InterruptedException {
+            return new IllustratorContext(job, input, pkg);
+        }
+        
+        @SuppressWarnings("unchecked")
+        public class IllustratorContext extends Context {
+            private PigNullableWritable currentKey = null, nextKey = null;
+            private NullableTuple nextValue = null;
+            private List<NullableTuple> currentValues = null;
+            private Iterator<Pair<PigNullableWritable, Writable>> it;
+            private final ByteArrayOutputStream bos;
+            private final DataOutputStream dos;
+            private final RawComparator sortComparator, groupingComparator;
+            POPackage pack = null;
+
+            public IllustratorContext(Job job,
+                  List<Pair<PigNullableWritable, Writable>> input,
+                  POPackage pkg
+                  ) throws IOException, InterruptedException {
+                super(job.getJobConf(), new TaskAttemptID(), new FakeRawKeyValueIterator(input.iterator().hasNext()),
+                    null, null, null, null, null, null, PigNullableWritable.class, NullableTuple.class);
+                bos = new ByteArrayOutputStream();
+                dos = new DataOutputStream(bos);
+                org.apache.hadoop.mapreduce.Job nwJob = new org.apache.hadoop.mapreduce.Job(job.getJobConf());
+                sortComparator = nwJob.getSortComparator();
+                groupingComparator = nwJob.getGroupingComparator();
+                
+                Collections.sort(input, new Comparator<Pair<PigNullableWritable, Writable>>() {
+                        @Override
+                        public int compare(Pair<PigNullableWritable, Writable> o1,
+                                           Pair<PigNullableWritable, Writable> o2) {
+                            try {
+                                o1.first.write(dos);
+                                int l1 = bos.size();
+                                o2.first.write(dos);
+                                int l2 = bos.size();
+                                byte[] bytes = bos.toByteArray();
+                                bos.reset();
+                                return sortComparator.compare(bytes, 0, l1, bytes, l1, l2-l1);
+                            } catch (IOException e) {
+                                throw new RuntimeException("Serialization exception in sort:"+e.getMessage());
+                            }
+                        }
+                    }
+                );
+                currentValues = new ArrayList<NullableTuple>();
+                it = input.iterator();
+                if (it.hasNext()) {
+                    Pair<PigNullableWritable, Writable> entry = it.next();
+                    nextKey = entry.first;
+                    nextValue = (NullableTuple) entry.second;
+                }
+                pack = pkg;
+            }
+            
+            @Override
+            public PigNullableWritable getCurrentKey() {
+                return currentKey;
+            }
+            
+            @Override
+            public boolean nextKey() {
+                if (nextKey == null)
+                    return false;
+                currentKey = nextKey;
+                currentValues.clear();
+                currentValues.add(nextValue);
+                nextKey = null;
+                for(; it.hasNext(); ) {
+                    Pair<PigNullableWritable, Writable> entry = it.next();
+                    /* Why can't raw comparison be used?
+                    byte[] bytes;
+                    int l1, l2;
+                    try {
+                        currentKey.write(dos);
+                        l1 = bos.size();
+                        entry.first.write(dos);
+                        l2 = bos.size();
+                        bytes = bos.toByteArray();
+                    } catch (IOException e) {
+                        throw new RuntimeException("nextKey exception : "+e.getMessage());
+                    }
+                    bos.reset();
+                    if (groupingComparator.compare(bytes, 0, l1, bytes, l1, l2-l1) == 0)
+                    */
+                    if (groupingComparator.compare(currentKey, entry.first) == 0)
+                    {
+                        currentValues.add((NullableTuple)entry.second);
+                    } else {
+                        nextKey = entry.first;
+                        nextValue = (NullableTuple) entry.second;
+                        break;
+                    }
+                }
+                return true;
+            }
+            
+            @Override
+            public Iterable<NullableTuple> getValues() {
+                return currentValues;
+            }
+            
+            @Override
+            public void write(PigNullableWritable k, Writable t) {
+            }
+            
+            @Override
+            public void progress() { 
+            }
+        }
     }
     
     /**
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/LogToPhyTranslationVisitor.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/LogToPhyTranslationVisitor.java
index d89e5c49e..ef8d50a50 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/LogToPhyTranslationVisitor.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/LogToPhyTranslationVisitor.java
@@ -91,6 +91,10 @@ public class LogToPhyTranslationVisitor extends LOVisitor {
         return currentPlan;
     }
 
+    public Map<LogicalOperator, PhysicalOperator> getLogToPhyMap() {
+        return logToPhyMap;
+    }
+    
     @Override
     protected void visit(LOGreaterThan op) throws VisitorException {
         String scope = op.getOperatorKey().scope;
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/PhysicalOperator.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/PhysicalOperator.java
index 4846dafd7..ac099e167 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/PhysicalOperator.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/PhysicalOperator.java
@@ -35,6 +35,8 @@ import org.apache.pig.impl.plan.Operator;
 import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.impl.plan.VisitorException;
 import org.apache.pig.pen.util.LineageTracer;
+import org.apache.pig.pen.Illustrator;
+import org.apache.pig.pen.Illustrable;
 
 /**
  * 
@@ -58,7 +60,7 @@ import org.apache.pig.pen.util.LineageTracer;
  * only those types that are supported.
  *
  */
-public abstract class PhysicalOperator extends Operator<PhyPlanVisitor> implements Cloneable {
+public abstract class PhysicalOperator extends Operator<PhyPlanVisitor> implements Illustrable, Cloneable {
 
     private Log log = LogFactory.getLog(getClass());
 
@@ -125,8 +127,14 @@ public abstract class PhysicalOperator extends Operator<PhyPlanVisitor> implemen
 
     static final protected Map dummyMap = null;
     
+    // TODO: This is not needed. But a lot of tests check serialized physical plans
+    // that are sensitive to the serialized image of the contained physical operators.
+    // So for now, just keep it. Later it'll be cleansed along with those test golden
+    // files
     protected LineageTracer lineageTracer;
 
+    protected transient Illustrator illustrator = null;
+
     private boolean accum;
     private transient boolean accumStart;
 
@@ -149,8 +157,13 @@ public abstract class PhysicalOperator extends Operator<PhyPlanVisitor> implemen
         res = new Result();
     }
 
-    public void setLineageTracer(LineageTracer lineage) {
-	this.lineageTracer = lineage;
+    @Override
+    public void setIllustrator(Illustrator illustrator) {
+	      this.illustrator = illustrator;
+    }
+    
+    public Illustrator getIllustrator() {
+        return illustrator;
     }
     
     public int getRequestedParallelism() {
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/BinaryComparisonOperator.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/BinaryComparisonOperator.java
index c6a4312bf..3e434f34b 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/BinaryComparisonOperator.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/BinaryComparisonOperator.java
@@ -17,6 +17,7 @@
  */
 package org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators;
 
+import org.apache.pig.data.Tuple;
 import org.apache.pig.impl.plan.OperatorKey;
 
 /**
@@ -66,4 +67,11 @@ public abstract class BinaryComparisonOperator extends BinaryExpressionOperator
         operandType = op.operandType;
         super.cloneHelper(op);
     }
+    
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+        if (illustrator != null) {
+            illustrator.setSubExpResult(eqClassIndex == 0);
+        }
+        return null;
+    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/BinaryExpressionOperator.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/BinaryExpressionOperator.java
index 944f529bf..7d22e18c9 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/BinaryExpressionOperator.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/BinaryExpressionOperator.java
@@ -20,7 +20,9 @@ package org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOp
 import java.util.ArrayList;
 import java.util.List;
 
+import org.apache.pig.data.Tuple;
 import org.apache.pig.impl.plan.OperatorKey;
+import org.apache.pig.impl.util.IdentityHashSet;
 
 /**
  * A base class for all Binary expression operators.
@@ -84,4 +86,9 @@ public abstract class BinaryExpressionOperator extends ExpressionOperator {
         rhs = op.rhs;
         super.cloneHelper(op);
     }
-}
+    
+    @Override
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+        return null;
+    }
+    }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/ConstantExpression.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/ConstantExpression.java
index e5293bcf3..930a36f3f 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/ConstantExpression.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/ConstantExpression.java
@@ -204,4 +204,9 @@ public class ConstantExpression extends ExpressionOperator {
     public List<ExpressionOperator> getChildExpressions() {		
         return null;
     }
+    
+    @Override
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+        return (Tuple) out;
+    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/EqualToExpr.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/EqualToExpr.java
index 27ed86a2c..2731aa797 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/EqualToExpr.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/EqualToExpr.java
@@ -185,7 +185,7 @@ public class EqualToExpr extends BinaryComparisonOperator {
         }else{
             throw new ExecException("The left side and right side has the different types");
         }
-        
+        illustratorMarkup(null, left.result, (Boolean) left.result ? 0 : 1);
         return left;
     }
 
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/ExpressionOperator.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/ExpressionOperator.java
index e318db6f3..19bf7fe6c 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/ExpressionOperator.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/ExpressionOperator.java
@@ -35,6 +35,7 @@ import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
 import org.apache.pig.impl.plan.VisitorException;
+import org.apache.pig.pen.Illustrator;
 
 /**
  * A base class for all types of expressions. All expression
@@ -54,6 +55,11 @@ public abstract class ExpressionOperator extends PhysicalOperator {
         super(k, rp);
     }
     
+    @Override
+    public void setIllustrator(Illustrator illustrator) {
+        this.illustrator = illustrator;
+    }
+    
     @Override
     public boolean supportsMultipleOutputs() {
         return false;
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/GTOrEqualToExpr.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/GTOrEqualToExpr.java
index 752b932c2..18e32a2c1 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/GTOrEqualToExpr.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/GTOrEqualToExpr.java
@@ -154,6 +154,7 @@ public class GTOrEqualToExpr extends BinaryComparisonOperator {
         } else {
             left.result = falseRef;
         }
+        illustratorMarkup(null, left.result, (Boolean) left.result ? 0 : 1);
         return left;
     }
 
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/GreaterThanExpr.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/GreaterThanExpr.java
index d844dfbc0..711e184a6 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/GreaterThanExpr.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/GreaterThanExpr.java
@@ -17,15 +17,11 @@
  */
 package org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators;
 
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
 import org.apache.pig.PigException;
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
-import org.apache.pig.data.DataByteArray;
 import org.apache.pig.data.DataType;
 import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.impl.plan.NodeIdGenerator;
@@ -37,7 +33,6 @@ public class GreaterThanExpr extends BinaryComparisonOperator {
      * 
      */
     private static final long serialVersionUID = 1L;
-    transient private final Log log = LogFactory.getLog(getClass());
 
     public GreaterThanExpr(OperatorKey k) {
         this(k, -1);
@@ -60,7 +55,6 @@ public class GreaterThanExpr extends BinaryComparisonOperator {
 
     @Override
     public Result getNext(Boolean bool) throws ExecException {
-        byte status;
         Result left, right;
 
         switch (operandType) {
@@ -154,6 +148,7 @@ public class GreaterThanExpr extends BinaryComparisonOperator {
         } else {
             left.result = falseRef;
         }
+        illustratorMarkup(null, left.result, (Boolean) left.result ? 0 : 1);
         return left;
     }
 
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/LTOrEqualToExpr.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/LTOrEqualToExpr.java
index fe10b2686..445756037 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/LTOrEqualToExpr.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/LTOrEqualToExpr.java
@@ -154,6 +154,7 @@ public class LTOrEqualToExpr extends BinaryComparisonOperator {
         } else {
             left.result = falseRef;
         }
+        illustratorMarkup(null, left.result, (Boolean) left.result ? 0 : 1);
         return left;
     }
 
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/LessThanExpr.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/LessThanExpr.java
index 1a0f70d55..f8a2e27dc 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/LessThanExpr.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/LessThanExpr.java
@@ -154,6 +154,7 @@ public class LessThanExpr extends BinaryComparisonOperator {
         } else {
             left.result = falseRef;
         }
+        illustratorMarkup(null, left.result, (Boolean) left.result ? 0 : 1);
         return left;
     }
 
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/NotEqualToExpr.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/NotEqualToExpr.java
index 0d349722a..6fd8026d4 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/NotEqualToExpr.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/NotEqualToExpr.java
@@ -184,6 +184,7 @@ public class NotEqualToExpr extends BinaryComparisonOperator {
         }else{
             throw new ExecException("The left side and right side has the different types");
         }
+        illustratorMarkup(null, left.result, (Boolean) left.result ? 0 : 1);
         return left;
     }
 
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POAnd.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POAnd.java
index 84db68603..f24c2acc3 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POAnd.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POAnd.java
@@ -78,9 +78,20 @@ public class POAnd extends BinaryComparisonOperator {
         // 3) f    f f f
         
         // Short circuit - if lhs is false, return false; ROW 3 above is handled with this
-        if (left.result != null && !(((Boolean)left.result).booleanValue())) return left;
+        boolean returnLeft = false;
+        if (left.result != null && !(((Boolean)left.result).booleanValue())) {
+          if (illustrator == null) {
+              return left;
+          }
+          illustratorMarkup(null, left.result, 1);
+          returnLeft = true;
+        }
         
         Result right = rhs.getNext(dummyBool);
+        if (returnLeft) {
+            return left;
+        }
+        
         // pass on ERROR and EOP 
         if(right.returnStatus != POStatus.STATUS_OK && right.returnStatus != POStatus.STATUS_NULL) {
             return right;
@@ -94,6 +105,8 @@ public class POAnd extends BinaryComparisonOperator {
         
         // No matter what, what we get from the right side is what we'll
         // return, null, true, or false.
+        if (right.result != null)
+            illustratorMarkup(null, right.result, (Boolean) right.result ? 0 : 1);
         return right;
     }
 
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POBinCond.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POBinCond.java
index 4f92eb9d9..b16109f33 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POBinCond.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POBinCond.java
@@ -32,6 +32,7 @@ import org.apache.pig.data.Tuple;
 import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.impl.plan.NodeIdGenerator;
 import org.apache.pig.impl.plan.VisitorException;
+import org.apache.pig.impl.util.IdentityHashSet;
 
 public class POBinCond extends ExpressionOperator {
     
@@ -65,7 +66,9 @@ public class POBinCond extends ExpressionOperator {
         
         Result res = cond.getNext(b);
         if (res.result==null || res.returnStatus != POStatus.STATUS_OK) return res;
-        return ((Boolean)res.result) == true ? lhs.getNext(b) : rhs.getNext(b);
+        Result result = ((Boolean)res.result) == true ? lhs.getNext(b) : rhs.getNext(b);
+        illustratorMarkup(null, result.result, ((Boolean)res.result) ? 0 : 1);
+        return result;
         
     }
 
@@ -88,7 +91,9 @@ public class POBinCond extends ExpressionOperator {
                         
         Result res = cond.getNext(dummyBool);
         if (res.result==null || res.returnStatus != POStatus.STATUS_OK) return res;
-        return ((Boolean)res.result) == true ? lhs.getNext(db) : rhs.getNext(db);
+        Result result = ((Boolean)res.result) == true ? lhs.getNext(db) : rhs.getNext(db);
+        illustratorMarkup(null, result.result, ((Boolean)res.result) ? 0 : 1);
+        return result;
     }
 
     @Override
@@ -109,7 +114,9 @@ public class POBinCond extends ExpressionOperator {
         }
         Result res = cond.getNext(dummyBool);
         if (res.result==null || res.returnStatus != POStatus.STATUS_OK) return res;
-        return ((Boolean)res.result) == true ? lhs.getNext(ba) : rhs.getNext(ba);
+        Result result = ((Boolean)res.result) == true ? lhs.getNext(ba) : rhs.getNext(ba);
+        illustratorMarkup(null, result.result, ((Boolean)res.result) ? 0 : 1);
+        return result;
     }
 
     @Override
@@ -130,7 +137,9 @@ public class POBinCond extends ExpressionOperator {
         }
         Result res = cond.getNext(dummyBool);
         if (res.result==null || res.returnStatus != POStatus.STATUS_OK) return res;
-        return ((Boolean)res.result) == true ? lhs.getNext(d) : rhs.getNext(d);
+        Result result = ((Boolean)res.result) == true ? lhs.getNext(d) : rhs.getNext(d);
+        illustratorMarkup(null, result.result, ((Boolean)res.result) ? 0 : 1);
+        return result;
     }
 
     @Override
@@ -151,7 +160,9 @@ public class POBinCond extends ExpressionOperator {
         }
         Result res = cond.getNext(dummyBool);
         if (res.result==null || res.returnStatus != POStatus.STATUS_OK) return res;
-        return ((Boolean)res.result) == true ? lhs.getNext(f) : rhs.getNext(f);
+        Result result = ((Boolean)res.result) == true ? lhs.getNext(f) : rhs.getNext(f);
+        illustratorMarkup(null, result.result, ((Boolean)res.result) ? 0 : 1);
+        return result;
     }
 
     @Override
@@ -171,7 +182,9 @@ public class POBinCond extends ExpressionOperator {
         }
         Result res = cond.getNext(dummyBool);
         if (res.result==null || res.returnStatus != POStatus.STATUS_OK) return res;
-        return ((Boolean)res.result) == true ? lhs.getNext(i) : rhs.getNext(i);
+        Result result = ((Boolean)res.result) == true ? lhs.getNext(i) : rhs.getNext(i);
+        illustratorMarkup(null, result.result, ((Boolean)res.result) ? 0 : 1);
+        return result;
     }
 
     @Override
@@ -192,7 +205,9 @@ public class POBinCond extends ExpressionOperator {
         }
         Result res = cond.getNext(dummyBool);
         if (res.result==null || res.returnStatus != POStatus.STATUS_OK) return res;
-        return ((Boolean)res.result) == true ? lhs.getNext(l) : rhs.getNext(l);
+        Result result = ((Boolean)res.result) == true ? lhs.getNext(l) : rhs.getNext(l);
+        illustratorMarkup(null, result.result, ((Boolean)res.result) ? 0 : 1);
+        return result;
     }
 
     @Override
@@ -213,7 +228,9 @@ public class POBinCond extends ExpressionOperator {
         }
         Result res = cond.getNext(dummyBool);
         if (res.result==null || res.returnStatus != POStatus.STATUS_OK) return res;
-        return ((Boolean)res.result) == true ? lhs.getNext(m) : rhs.getNext(m);
+        Result result = ((Boolean)res.result) == true ? lhs.getNext(m) : rhs.getNext(m);
+        illustratorMarkup(null, result.result, ((Boolean)res.result) ? 0 : 1);
+        return result;
     }
 
     @Override
@@ -234,7 +251,9 @@ public class POBinCond extends ExpressionOperator {
         }
         Result res = cond.getNext(dummyBool);
         if (res.result==null || res.returnStatus != POStatus.STATUS_OK) return res;
-        return ((Boolean)res.result) == true ? lhs.getNext(s) : rhs.getNext(s);
+        Result result = ((Boolean)res.result) == true ? lhs.getNext(s) : rhs.getNext(s);
+        illustratorMarkup(null, result.result, ((Boolean)res.result) ? 0 : 1);
+        return result;
     }
 
     @Override
@@ -255,7 +274,9 @@ public class POBinCond extends ExpressionOperator {
         }
         Result res = cond.getNext(dummyBool);
         if (res.result==null || res.returnStatus != POStatus.STATUS_OK) return res;
-        return ((Boolean)res.result) == true ? lhs.getNext(t) : rhs.getNext(t);
+        Result result = ((Boolean)res.result) == true ? lhs.getNext(t) : rhs.getNext(t);
+        illustratorMarkup(null, result.result, ((Boolean)res.result) ? 0 : 1);
+        return result;
     }
 
     @Override
@@ -338,4 +359,11 @@ public class POBinCond extends ExpressionOperator {
         return child;
     }
 
+    @Override
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+        if(illustrator != null) {
+            
+        }
+        return null;
+    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POCast.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POCast.java
index e2f9f2661..47bd9f38d 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POCast.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POCast.java
@@ -1327,4 +1327,8 @@ public class POCast extends ExpressionOperator {
         return funcSpec;
     }
 
+    @Override
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+      return (Tuple) out;
+    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POIsNull.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POIsNull.java
index ebe7041a3..2267a759f 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POIsNull.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POIsNull.java
@@ -76,6 +76,7 @@ public class POIsNull extends UnaryComparisonOperator {
                 } else {
                     res.result = false;
                 }
+                illustratorMarkup(null, res.result, (Boolean) res.result ? 0 : 1);
             }
             return res;
         case DataType.DOUBLE:
@@ -86,6 +87,7 @@ public class POIsNull extends UnaryComparisonOperator {
                 } else {
                     res.result = false;
                 }
+                illustratorMarkup(null, res.result, (Boolean) res.result ? 0 : 1);
             }
             return res;
         case DataType.INTEGER:
@@ -96,6 +98,7 @@ public class POIsNull extends UnaryComparisonOperator {
                 } else {
                     res.result = false;
                 }
+                illustratorMarkup(null, res.result, (Boolean) res.result ? 0 : 1);
             }
             return res;
         case DataType.CHARARRAY:
@@ -106,6 +109,7 @@ public class POIsNull extends UnaryComparisonOperator {
                 } else {
                     res.result = false;
                 }
+                illustratorMarkup(null, res.result, (Boolean) res.result ? 0 : 1);
             }
             return res;
         case DataType.BOOLEAN:
@@ -116,6 +120,7 @@ public class POIsNull extends UnaryComparisonOperator {
                 } else {
                     res.result = false;
                 }
+                illustratorMarkup(null, res.result, (Boolean) res.result ? 0 : 1);
             }
             return res;
         case DataType.LONG:
@@ -126,6 +131,7 @@ public class POIsNull extends UnaryComparisonOperator {
                 } else {
                     res.result = false;
                 }
+                illustratorMarkup(null, res.result, (Boolean) res.result ? 0 : 1);
             }
             return res;
         case DataType.FLOAT:
@@ -136,6 +142,7 @@ public class POIsNull extends UnaryComparisonOperator {
                 } else {
                     res.result = false;
                 }
+                illustratorMarkup(null, res.result, (Boolean) res.result ? 0 : 1);
             }
             return res;
         case DataType.MAP:
@@ -146,6 +153,7 @@ public class POIsNull extends UnaryComparisonOperator {
                 } else {
                     res.result = false;
                 }
+                illustratorMarkup(null, res.result, (Boolean) res.result ? 0 : 1);
             }
             return res;
         case DataType.TUPLE:
@@ -156,6 +164,7 @@ public class POIsNull extends UnaryComparisonOperator {
                 } else {
                     res.result = false;
                 }
+                illustratorMarkup(null, res.result, (Boolean) res.result ? 0 : 1);
             }
             return res;
         case DataType.BAG:
@@ -166,6 +175,7 @@ public class POIsNull extends UnaryComparisonOperator {
                 } else {
                     res.result = false;
                 }
+                illustratorMarkup(null, res.result, (Boolean) res.result ? 0 : 1);
             }
             return res;        
         default: {
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POMapLookUp.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POMapLookUp.java
index 23fa00b03..b91d77626 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POMapLookUp.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POMapLookUp.java
@@ -165,6 +165,8 @@ public class POMapLookUp extends ExpressionOperator {
         return null;
     }
     
-    
-
+    @Override
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+        return (Tuple) out;
+    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/PONegative.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/PONegative.java
index d371f40ea..c076ae71a 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/PONegative.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/PONegative.java
@@ -22,6 +22,7 @@ import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
 import org.apache.pig.data.DataType;
+import org.apache.pig.data.Tuple;
 import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.impl.plan.NodeIdGenerator;
 import org.apache.pig.impl.plan.VisitorException;
@@ -101,4 +102,9 @@ public class PONegative extends UnaryExpressionOperator {
         clone.cloneHelper(this);
         return clone;
     }
+    
+    @Override
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+        return (Tuple) out;
+    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/PONot.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/PONot.java
index 375814033..973dfc525 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/PONot.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/PONot.java
@@ -72,8 +72,14 @@ public class PONot extends UnaryComparisonOperator {
         if(res.returnStatus != POStatus.STATUS_OK || res.result == null) {
             return res;
         }
-        if (((Boolean)res.result).booleanValue()) return falseRes;
-        else return trueRes;
+        if (((Boolean)res.result).booleanValue()) {
+          illustratorMarkup(null, falseRes.result, 1);
+          return falseRes;
+        }
+        else {
+          illustratorMarkup(null, trueRes.result, 0);
+          return trueRes;
+        }
     }
 
     @Override
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POOr.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POOr.java
index dce87ab60..498eb12eb 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POOr.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POOr.java
@@ -78,9 +78,19 @@ public class POOr extends BinaryComparisonOperator {
         // 3) f    t n f
         
         // Short circuit. if lhs is true, return true - ROW 1 above is handled with this
-        if (left.result != null && ((Boolean)left.result).booleanValue()) return left;
+        boolean returnLeft = false;
+        if (left.result != null && ((Boolean)left.result).booleanValue()) {
+          if (illustrator == null)
+              return left;
+          
+          illustratorMarkup(null, left.result, 0);
+          returnLeft = true;;
+        }
         
         Result right = rhs.getNext(dummyBool);
+        if (returnLeft)
+            return left;
+
         // pass on ERROR and EOP 
         if(right.returnStatus != POStatus.STATUS_OK && right.returnStatus != POStatus.STATUS_NULL) {
             return right;
@@ -94,6 +104,8 @@ public class POOr extends BinaryComparisonOperator {
         
         // No matter what, what we get from the right side is what we'll
         // return, null, true, or false.
+        if (right.result != null)
+            illustratorMarkup(null, right.result, (Boolean) right.result ? 0 : 1);
         return right;
     }
 
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POProject.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POProject.java
index 8411b2570..d67976c0d 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POProject.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POProject.java
@@ -19,6 +19,7 @@ package org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOp
 
 import java.util.ArrayList;
 import java.util.Iterator;
+import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
 
@@ -38,6 +39,8 @@ import org.apache.pig.data.TupleFactory;
 import org.apache.pig.impl.plan.NodeIdGenerator;
 import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.impl.plan.VisitorException;
+import org.apache.pig.impl.util.IdentityHashSet;
+import org.apache.pig.pen.util.ExampleTuple;
 
 /**
  * Implements the overloaded form of the project operator.
@@ -143,6 +146,7 @@ public class POProject extends ExpressionOperator {
             return res;
         }
         if (star) {
+            illustratorMarkup(inpValue, res.result, -1);
             return res;
         } else if(columns.size() == 1) {
             try {
@@ -188,6 +192,7 @@ public class POProject extends ExpressionOperator {
             ret = tupleFactory.newTuple(objList);
         }
         res.result = ret;
+        illustratorMarkup(inpValue, res.result, -1);
         return res;
     }
 
@@ -494,4 +499,11 @@ public class POProject extends ExpressionOperator {
         return null;
     }
 
+    @Override
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+        if(illustrator != null) {
+
+        }
+        return null;
+    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/PORegexp.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/PORegexp.java
index f83726446..6634915ce 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/PORegexp.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/PORegexp.java
@@ -86,6 +86,7 @@ public class PORegexp extends BinaryComparisonOperator {
         } else {
             left.result = falseRef;
         }
+        illustratorMarkup(null, left.result, (Boolean) left.result ? 0 : 1);
         return left;
     }    
 
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POUserComparisonFunc.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POUserComparisonFunc.java
index 57dd42b65..bcb16c083 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POUserComparisonFunc.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POUserComparisonFunc.java
@@ -38,6 +38,7 @@ import org.apache.pig.impl.PigContext;
 import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.impl.plan.NodeIdGenerator;
 import org.apache.pig.impl.plan.VisitorException;
+import org.apache.pig.impl.util.IdentityHashSet;
 
 //We intentionally skip type checking in backend for performance reasons
 @SuppressWarnings("unchecked")
@@ -84,6 +85,9 @@ public class POUserComparisonFunc extends ExpressionOperator {
         // the two attached tuples are used up now. So we set the
         // inputAttached flag to false
         inputAttached = false;
+        if (result.returnStatus == POStatus.STATUS_OK)
+            illustratorMarkup(null, result.result, 
+                (Integer) result.result == 0 ? 0 : (Integer) result.result > 0 ? 1 : 2);
         return result;
 
     }
@@ -192,4 +196,14 @@ public class POUserComparisonFunc extends ExpressionOperator {
         return null;
     }
 
+    @Override
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+        if(illustrator != null) {
+            illustrator.getInputs().add(t1);
+            illustrator.getEquivalenceClasses().get(eqClassIndex).add(t1);
+            illustrator.getInputs().add(t2);
+            illustrator.getEquivalenceClasses().get(eqClassIndex).add(t2);
+        }
+        return null;
+    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POUserFunc.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POUserFunc.java
index 9743d8021..c2a4ed8b6 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POUserFunc.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/POUserFunc.java
@@ -480,4 +480,13 @@ public class POUserFunc extends ExpressionOperator {
     public void setResultType(byte resultType) {
         this.resultType = resultType;
     }
+    
+    @Override
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+        return (Tuple) out;
+    }
+    
+    public EvalFunc getFunc() {
+        return func;
+    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/UnaryComparisonOperator.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/UnaryComparisonOperator.java
index c8f73fcf7..664587434 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/UnaryComparisonOperator.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/expressionOperators/UnaryComparisonOperator.java
@@ -17,7 +17,9 @@
  */
 package org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators;
 
+import org.apache.pig.data.Tuple;
 import org.apache.pig.impl.plan.OperatorKey;
+import org.apache.pig.impl.util.IdentityHashSet;
 
 /**
  * This is a base class for all unary comparison operators. Supports the
@@ -49,4 +51,12 @@ public abstract class UnaryComparisonOperator extends UnaryExpressionOperator
     public void setOperandType(byte operandType) {
         this.operandType = operandType;
     }
+    
+    @Override
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+        if(illustrator != null) {
+
+        }
+        return null;
+    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/plans/PhyPlanVisitor.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/plans/PhyPlanVisitor.java
index 71de49612..0f3008d81 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/plans/PhyPlanVisitor.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/plans/PhyPlanVisitor.java
@@ -130,18 +130,6 @@ public class PhyPlanVisitor extends PlanVisitor<PhysicalOperator,PhysicalPlan> {
 	public void visitDistinct(PODistinct distinct) throws VisitorException {
         //do nothing		
 	}
-	
-    public void visitPenCross(org.apache.pig.pen.physicalOperators.POCross cross) throws VisitorException {
-        //do nothing
-    }
-    
-    public void visitPenCogroup(org.apache.pig.pen.physicalOperators.POCogroup cogroup) throws VisitorException {
-        //do nothing
-    }
-    
-    public void visitPenSplit(org.apache.pig.pen.physicalOperators.POSplit split) throws VisitorException {
-        //do nothing
-    }
 
 	public void visitRead(PORead read) throws VisitorException {
         //do nothing		
@@ -296,25 +284,10 @@ public class PhyPlanVisitor extends PlanVisitor<PhysicalOperator,PhysicalPlan> {
         }
 	}
 
-    /**
-     * @param lrfi
-     * @throws VisitorException 
-     */
-    public void visitLocalRearrangeForIllustrate(
-            POLocalRearrangeForIllustrate lrfi) throws VisitorException {
-        List<PhysicalPlan> inpPlans = lrfi.getPlans();
-        for (PhysicalPlan plan : inpPlans) {
-            pushWalker(mCurrentWalker.spawnChildWalker(plan));
-            visit();
-            popWalker();
-        }
-        
-    }
-
     /**
      * @param optimizedForEach
      */
-    public void visitPOOptimizedForEach(POOptimizedForEach optimizedForEach) {
+    public void visitPOOptimizedForEach(POOptimizedForEach optimizedForEach) throws VisitorException {
         // TODO Auto-generated method stub
         
     }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POCollectedGroup.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POCollectedGroup.java
index aa2886bd6..74753de1c 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POCollectedGroup.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POCollectedGroup.java
@@ -19,6 +19,7 @@ package org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOp
 
 import java.util.ArrayList;
 import java.util.List;
+import java.util.Iterator;
 
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce;
@@ -37,6 +38,9 @@ import org.apache.pig.data.TupleFactory;
 import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.impl.plan.PlanException;
 import org.apache.pig.impl.plan.VisitorException;
+import org.apache.pig.pen.util.ExampleTuple;
+import org.apache.pig.pen.util.LineageTracer;
+import org.apache.pig.impl.util.IdentityHashSet;
 
 /**
  * The collected group operator is a special operator used when users give
@@ -247,7 +251,7 @@ public class POCollectedGroup extends PhysicalOperator {
             outputBag = useDefaultBag ? BagFactory.getInstance().newDefaultBag() 
                     : new InternalCachedBag(1);
             outputBag.add((Tuple)tup.get(1));
-
+            illustratorMarkup(null, tup2, 0);
             return res;
         }
 
@@ -298,4 +302,33 @@ public class POCollectedGroup extends PhysicalOperator {
             leafOps.add(leaf);
         }            
    }
+    
+    private void setIllustratorEquivalenceClasses(Tuple tin) {
+        if (illustrator != null) {
+          illustrator.getEquivalenceClasses().get(0).add(tin);
+        }
+    }
+    
+    @Override
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+        if (illustrator != null) {
+            ExampleTuple tOut = new ExampleTuple(out);
+            LineageTracer lineage = illustrator.getLineage();
+            lineage.insert(tOut);
+            DataBag bag;
+            try {
+                bag = (DataBag) tOut.get(1);
+            } catch (ExecException e) {
+                throw new RuntimeException("Illustrator markup exception" + e.getMessage());
+            }
+            boolean synthetic = false;
+            while (!synthetic && bag.iterator().hasNext()) {
+                synthetic |= ((ExampleTuple) bag.iterator().next()).synthetic;
+            }
+            tOut.synthetic = synthetic;
+            illustrator.addData((Tuple) out);
+            return tOut;
+        } else
+          return (Tuple) out;
+    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/PODemux.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/PODemux.java
index 841c11346..897aa7c55 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/PODemux.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/PODemux.java
@@ -341,5 +341,11 @@ public class PODemux extends PhysicalOperator {
     public boolean isInCombiner() {
         return inCombiner;
     }
+    
+    @Override
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+        // nothing need to be done here
+        return null;
+    }
         
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/PODistinct.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/PODistinct.java
index 9422cd5f1..55ffe0d51 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/PODistinct.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/PODistinct.java
@@ -19,7 +19,9 @@
 package org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators;
 
 import java.util.Iterator;
+import java.util.LinkedList;
 import java.util.List;
+import java.util.HashSet;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
@@ -32,12 +34,17 @@ import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlan
 import org.apache.pig.data.BagFactory;
 import org.apache.pig.data.DataBag;
 import org.apache.pig.data.DataType;
+import org.apache.pig.data.DistinctDataBag;
 import org.apache.pig.data.InternalDistinctBag;
 import org.apache.pig.data.InternalSortedBag;
 import org.apache.pig.data.Tuple;
 import org.apache.pig.impl.plan.NodeIdGenerator;
 import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.impl.plan.VisitorException;
+import org.apache.pig.impl.util.IdentityHashSet;
+import org.apache.pig.pen.util.ExampleTuple;
+import org.apache.pig.pen.util.LineageTracer;
+import org.apache.pig.impl.util.IdentityHashSet;
 
 /**
  * Find the distinct set of tuples in a bag.
@@ -105,6 +112,7 @@ public class PODistinct extends PhysicalOperator implements Cloneable {
                     continue;
                 }
                 distinctBag.add((Tuple) in.result);
+                illustratorMarkup(in.result, in.result, 0);
                 in = processInput();
             }
             inputsAccumulated = true;
@@ -159,4 +167,12 @@ public class PODistinct extends PhysicalOperator implements Cloneable {
         return new PODistinct(new OperatorKey(this.mKey.scope, NodeIdGenerator.getGenerator().getNextNodeId(this.mKey.scope)), this.requestedParallelism, this.inputs);
     }
 
+    @Override
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+        if(illustrator != null) {
+            illustrator.getEquivalenceClasses().get(eqClassIndex).add((Tuple) out);
+            illustrator.addData((Tuple) out);
+        }
+        return null;
+    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POFRJoin.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POFRJoin.java
index 3a340d917..e2fe8e905 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POFRJoin.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POFRJoin.java
@@ -445,4 +445,10 @@ public class POFRJoin extends PhysicalOperator {
     public void setReplFiles(FileSpec[] replFiles) {
         this.replFiles = replFiles;
     }
+    
+    @Override
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+        // no op: all handled by the preceding POForEach
+        return null;
+    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POFilter.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POFilter.java
index a830becf2..c78857579 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POFilter.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POFilter.java
@@ -18,6 +18,7 @@
 package org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators;
 
 import java.util.List;
+import java.util.LinkedList;
 
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.data.DataType;
@@ -25,12 +26,13 @@ import org.apache.pig.data.Tuple;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.ComparisonOperator;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
 import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.impl.plan.VisitorException;
+import org.apache.pig.impl.util.IdentityHashSet;
 import org.apache.pig.pen.util.ExampleTuple;
+import org.apache.pig.pen.util.LineageTracer;
 
 /**
  * This is an implementation of the Filter operator. It has an Expression Plan
@@ -151,13 +153,10 @@ public class POFilter extends PhysicalOperator {
                     && res.returnStatus != POStatus.STATUS_NULL) 
                 return res;
 
-            if (res.result != null && (Boolean) res.result == true) {
-        	if(lineageTracer != null) {
-        	    ExampleTuple tIn = (ExampleTuple) inp.result;
-        	    lineageTracer.insert(tIn);
-        	    lineageTracer.union(tIn, tIn);
-        	}
-                return inp;
+            if (res.result != null) {
+                illustratorMarkup(inp.result, inp.result, (Boolean) res.result ? 0 : 1);
+                if ((Boolean) res.result)
+                  return inp;
             }
         }
         return inp;
@@ -194,4 +193,20 @@ public class POFilter extends PhysicalOperator {
     public PhysicalPlan getPlan() {
         return plan;
     }
+    
+    @Override
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+      if (illustrator != null) {
+          int index = 0;
+          for (int i = 0; i < illustrator.getSubExpResults().size(); ++i) {
+              if (!illustrator.getSubExpResults().get(i)[0])
+                  index += (1 << i);
+          }
+          if (index < illustrator.getEquivalenceClasses().size())
+              illustrator.getEquivalenceClasses().get(index).add((Tuple) in);
+          if (eqClassIndex == 0) // add only qualified record
+              illustrator.addData((Tuple) out);
+      }
+      return (Tuple) out;
+    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POForEach.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POForEach.java
index a094c2795..b49691962 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POForEach.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POForEach.java
@@ -47,7 +47,9 @@ import org.apache.pig.impl.plan.DependencyOrderWalker;
 import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.impl.plan.NodeIdGenerator;
 import org.apache.pig.impl.plan.VisitorException;
+import org.apache.pig.impl.util.IdentityHashSet;
 import org.apache.pig.pen.util.ExampleTuple;
+import org.apache.pig.pen.util.LineageTracer;
 
 //We intentionally skip type checking in backend for performance reasons
 @SuppressWarnings("unchecked")
@@ -93,6 +95,8 @@ public class POForEach extends PhysicalOperator {
     
     protected transient AccumulativeTupleBuffer buffer;
     
+    protected Tuple inpTuple;
+    
     public POForEach(OperatorKey k) {
         this(k,-1,null,null);
     }
@@ -209,13 +213,6 @@ public class POForEach extends PhysicalOperator {
                 res = processPlan();               
                 
                 if(res.returnStatus==POStatus.STATUS_OK) {
-                    if(lineageTracer !=  null && res.result != null) {
-                    ExampleTuple tOut = new ExampleTuple((Tuple) res.result);
-                    tOut.synthetic = tIn.synthetic;
-                    lineageTracer.insert(tOut);
-                    lineageTracer.union(tOut, tIn);
-                    res.result = tOut;
-                    }
                     return res;
                 }
                 if(res.returnStatus==POStatus.STATUS_EOP) {
@@ -247,18 +244,18 @@ public class POForEach extends PhysicalOperator {
             }
                        
             attachInputToPlans((Tuple) inp.result);
-            Tuple tuple = (Tuple)inp.result;
+            inpTuple = (Tuple)inp.result;
             
             for (PhysicalOperator po : opsToBeReset) {
                 po.reset();
             }
             
             if (isAccumulative()) {            	
-                for(int i=0; i<tuple.size(); i++) {            		
-                    if (tuple.getType(i) == DataType.BAG) {
+                for(int i=0; i<inpTuple.size(); i++) {            		
+                    if (inpTuple.getType(i) == DataType.BAG) {
                         // we only need to check one bag, because all the bags
                         // share the same buffer
-                        buffer = ((AccumulativeBag)tuple.get(i)).getTuplebuffer();
+                        buffer = ((AccumulativeBag)inpTuple.get(i)).getTuplebuffer();
                         break;
                     }
                 }
@@ -272,8 +269,9 @@ public class POForEach extends PhysicalOperator {
                         }
                         
                         setAccumStart();                		
-                    }else{                
-                        buffer.clear();
+                    }else{
+                        inpTuple = ((POPackage.POPackageTupleBuffer) buffer).illustratorMarkup(null, inpTuple, 0);
+ //                       buffer.clear();
                         setAccumEnd();                		
                     }
                     
@@ -292,16 +290,6 @@ public class POForEach extends PhysicalOperator {
             }
             
             processingPlan = true;
-
-            if(lineageTracer != null && res.result != null) {
-            //we check for res.result since that can also be null in the case of flatten
-            tIn = (ExampleTuple) inp.result;
-            ExampleTuple tOut = new ExampleTuple((Tuple) res.result);
-            tOut.synthetic = tIn.synthetic;
-            lineageTracer.insert(tOut);
-            lineageTracer.union(tOut, tIn);
-            res.result = tOut;
-            }
             
             return res;
         }
@@ -482,12 +470,10 @@ public class POForEach extends PhysicalOperator {
             } else
                 out.append(in);
         }
-        
-        if(lineageTracer != null) {
-            ExampleTuple tOut = new ExampleTuple();
-            tOut.reference(out);
-        }
-        return out;
+        if (inpTuple != null)
+            return illustratorMarkup(inpTuple, out, 0);
+        else
+            return illustratorMarkup2(data, out);
     }
 
     
@@ -699,4 +685,43 @@ public class POForEach extends PhysicalOperator {
     public void setOpsToBeReset(List<PhysicalOperator> opsToBeReset) {
         this.opsToBeReset = opsToBeReset;
     }
+    
+    private Tuple illustratorMarkup2(Object[] in, Object out) {
+      if(illustrator != null) {
+          ExampleTuple tOut = new ExampleTuple((Tuple) out);
+          illustrator.getLineage().insert(tOut);
+          boolean synthetic = false;
+          for (Object tIn : in)
+          {
+              synthetic |= ((ExampleTuple) tIn).synthetic;
+              illustrator.getLineage().union(tOut, (Tuple) tIn);
+          }
+          illustrator.addData(tOut);
+          int i;
+          for (i = 0; i < noItems; ++i)
+              if (((DataBag)bags[i]).size() < 2)
+                  break;
+          if (i >= noItems && !illustrator.getEqClassesShared())
+              illustrator.getEquivalenceClasses().get(0).add(tOut);
+          tOut.synthetic = synthetic;
+          return tOut;
+      } else
+          return (Tuple) out;
+    }
+
+    @Override
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+        if(illustrator != null) {
+            ExampleTuple tOut = new ExampleTuple((Tuple) out);
+            illustrator.addData(tOut);
+            if (!illustrator.getEqClassesShared())
+                illustrator.getEquivalenceClasses().get(0).add(tOut);
+            LineageTracer lineageTracer = illustrator.getLineage();
+            lineageTracer.insert(tOut);
+            tOut.synthetic = ((ExampleTuple) in).synthetic;
+            lineageTracer.union((ExampleTuple) in , tOut);
+            return tOut;
+        } else
+          return (Tuple) out;
+    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POGlobalRearrange.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POGlobalRearrange.java
index 0273f21c3..5fac709b3 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POGlobalRearrange.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POGlobalRearrange.java
@@ -105,4 +105,9 @@ public class POGlobalRearrange extends PhysicalOperator {
         // TODO Auto-generated method stub
         return super.getNext(t);
     }
+    
+    @Override
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+      return null;
+    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POLimit.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POLimit.java
index ad73564f4..0b611eb6d 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POLimit.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POLimit.java
@@ -17,6 +17,7 @@
  */
 package org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators;
 
+import java.util.LinkedList;
 import java.util.List;
 
 import org.apache.commons.logging.Log;
@@ -32,6 +33,9 @@ import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlan
 import org.apache.pig.impl.plan.NodeIdGenerator;
 import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.impl.plan.VisitorException;
+import org.apache.pig.impl.util.IdentityHashSet;
+import org.apache.pig.pen.util.ExampleTuple;
+import org.apache.pig.pen.util.LineageTracer;
 
 public class POLimit extends PhysicalOperator {
 	   /**
@@ -87,7 +91,9 @@ public class POLimit extends PhysicalOperator {
                     || inp.returnStatus == POStatus.STATUS_ERR)
                 break;
             
-            if (soFar>=mLimit)
+            illustratorMarkup(inp.result, null, 0);
+            // illustrator ignore LIMIT before the post processing
+            if ((illustrator == null || illustrator.getOriginalLimit() != -1) && soFar>=mLimit)
             	inp.returnStatus = POStatus.STATUS_EOP;
             
             soFar++;
@@ -131,4 +137,14 @@ public class POLimit extends PhysicalOperator {
         newLimit.setAlias(alias);
         return newLimit;
     }
+    
+    @Override
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+        if(illustrator != null) {
+            ExampleTuple tIn = (ExampleTuple) in;
+            illustrator.getEquivalenceClasses().get(eqClassIndex).add(tIn);
+            illustrator.addData((Tuple) in);
+        }
+        return (Tuple) in;
+    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POLoad.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POLoad.java
index f1ccbc2f7..1718d9a3f 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POLoad.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POLoad.java
@@ -135,10 +135,9 @@ public class POLoad extends PhysicalOperator {
             }
             else
                 res.returnStatus = POStatus.STATUS_OK;
-            if(lineageTracer != null) {
-        	ExampleTuple tOut = new ExampleTuple((Tuple) res.result);
-        	res.result = tOut;
-            }
+
+            if (res.returnStatus == POStatus.STATUS_OK)
+                res.result = illustratorMarkup(res, res.result, 0);
         } catch (IOException e) {
             log.error("Received error from loader function: " + e);
             return res;
@@ -199,4 +198,37 @@ public class POLoad extends PhysicalOperator {
     public LoadFunc getLoadFunc(){
         return this.loader;
     }
+    
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+        if(illustrator != null) {
+          if (!illustrator.ceilingCheck()) {
+              ((Result) in).returnStatus = POStatus.STATUS_EOP;
+              return null;
+          }
+          if (illustrator.getSchema() == null || illustrator.getSchema().size() <= ((Tuple) out).size()) {
+              boolean hasNull = false;
+              for (int i = 0; i < ((Tuple) out).size(); ++i)
+                  try {
+                      if (((Tuple) out).get(i) == null)
+                      {
+                          hasNull = true;
+                          break;
+                      }
+                  } catch (ExecException e) {
+                      hasNull = true;
+                      break;
+                  }
+              if (!hasNull) {
+                  ExampleTuple tOut = new ExampleTuple((Tuple) out);
+                  illustrator.getLineage().insert(tOut);
+                  illustrator.addData((Tuple) tOut);
+                  illustrator.getEquivalenceClasses().get(eqClassIndex).add(tOut);
+                  return tOut;
+              } else
+                  return (Tuple) out;
+          } else
+              return (Tuple) out;
+        } else
+          return (Tuple) out;
+    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POLocalRearrange.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POLocalRearrange.java
index 2dfe345e8..9f70ac373 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POLocalRearrange.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POLocalRearrange.java
@@ -41,6 +41,7 @@ import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.impl.plan.NodeIdGenerator;
 import org.apache.pig.impl.plan.VisitorException;
 import org.apache.pig.impl.plan.PlanException;
+import org.apache.pig.pen.util.ExampleTuple;
 
 /**
  * The local rearrange operator is a part of the co-group
@@ -380,6 +381,7 @@ public class POLocalRearrange extends PhysicalOperator {
             if(secondaryPlans != null)
                 detachPlans(secondaryPlans);
             
+            res.result = illustratorMarkup(inp.result, res.result, 0);
             return res;
         }
         return inp;
@@ -445,7 +447,10 @@ public class POLocalRearrange extends PhysicalOperator {
             //Put the key and the indexed tuple
             //in a tuple and return
             lrOutput.set(1, key);
-            lrOutput.set(2, mFakeTuple);
+            if (illustrator != null)
+                lrOutput.set(2, key);
+            else
+                lrOutput.set(2, mFakeTuple);
             return lrOutput;
         } else if(isCross){
         
@@ -486,6 +491,7 @@ public class POLocalRearrange extends PhysicalOperator {
                             minimalValue.append(value.get(i));
                         }
                     }
+                    minimalValue = illustratorMarkup(value, minimalValue, -1);
                 } else {
                     // for the project star case
                     // we would send out an empty tuple as
@@ -799,4 +805,19 @@ public class POLocalRearrange extends PhysicalOperator {
         this.stripKeyFromValue = stripKeyFromValue;
     }
 
+    @Override
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+        if (illustrator != null) {
+            if (!(out instanceof ExampleTuple))
+            {
+                ExampleTuple tOut = new ExampleTuple((Tuple) out);
+                illustrator.getLineage().insert(tOut);
+                illustrator.addData(tOut);
+                illustrator.getLineage().union(tOut, (Tuple) in);
+                tOut.synthetic = ((ExampleTuple) in).synthetic;
+                return tOut;
+            }
+        }
+        return (Tuple) out;
+    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POLocalRearrangeForIllustrate.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POLocalRearrangeForIllustrate.java
deleted file mode 100644
index 741cba32a..000000000
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POLocalRearrangeForIllustrate.java
+++ /dev/null
@@ -1,148 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators;
-
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DataType;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.ExpressionOperator;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
-import org.apache.pig.impl.plan.OperatorKey;
-import org.apache.pig.impl.plan.NodeIdGenerator;
-import org.apache.pig.impl.plan.PlanException;
-import org.apache.pig.impl.plan.VisitorException;
-
-/**
- * The local rearrange operator is a part of the co-group
- * implementation. It has an embedded physical plan that
- * generates tuples of the form (grpKey,(indxed inp Tuple)).
- *
- */
-public class POLocalRearrangeForIllustrate extends POLocalRearrange {
-
-    /**
-     * 
-     */
-    private static final long serialVersionUID = 1L;
-
-    public POLocalRearrangeForIllustrate(OperatorKey k) {
-        this(k, -1, null);
-    }
-
-    public POLocalRearrangeForIllustrate(OperatorKey k, int rp) {
-        this(k, rp, null);
-    }
-
-    public POLocalRearrangeForIllustrate(OperatorKey k, List<PhysicalOperator> inp) {
-        this(k, -1, inp);
-    }
-
-    public POLocalRearrangeForIllustrate(OperatorKey k, int rp, List<PhysicalOperator> inp) {
-        super(k, rp, inp);
-        index = -1;
-        leafOps = new ArrayList<ExpressionOperator>();
-    }
-
-    @Override
-    public void visit(PhyPlanVisitor v) throws VisitorException {
-        v.visitLocalRearrangeForIllustrate(this);
-    }
-
-    @Override
-    public String name() {
-        return getAliasString() + "Local Rearrange For Illustrate" + "["
-                + DataType.findTypeName(resultType) + "]" + "{"
-                + DataType.findTypeName(keyType) + "}" + "(" + mIsDistinct
-                + ") - " + mKey.toString();
-    }
-
-    protected Tuple constructLROutput(List<Result> resLst, List<Result> secondaryResLst, Tuple value) throws ExecException{
-        //Construct key
-        Object key;
-        if(resLst.size()>1){
-            Tuple t = mTupleFactory.newTuple(resLst.size());
-            int i=-1;
-            for(Result res : resLst)
-                t.set(++i, res.result);
-            key = t;
-        }
-        else{
-            key = resLst.get(0).result;
-        }
-        
-        Tuple output = mTupleFactory.newTuple(3);
-        if (mIsDistinct) {
-
-            //Put the key and the indexed tuple
-            //in a tuple and return
-            output.set(0, Byte.valueOf((byte)0));
-            output.set(1, key);
-            output.set(2, mFakeTuple);
-            return output;
-        } else {
-            if(isCross){
-                for(int i=0;i<plans.size();i++)
-                    value.getAll().remove(0);
-            }
-
-            //Put the index, key, and value
-            //in a tuple and return
-            output.set(0, Byte.valueOf(index));
-            output.set(1, key);
-            output.set(2, value);
-            return output;
-        }
-    }
-
-    /**
-     * Make a deep copy of this operator.  
-     * @throws CloneNotSupportedException
-     */
-    @Override
-    public POLocalRearrangeForIllustrate clone() throws CloneNotSupportedException {
-        List<PhysicalPlan> clonePlans = new
-            ArrayList<PhysicalPlan>(plans.size());
-        for (PhysicalPlan plan : plans) {
-            clonePlans.add(plan.clone());
-        }
-        POLocalRearrangeForIllustrate clone = new POLocalRearrangeForIllustrate(new OperatorKey(
-            mKey.scope, 
-            NodeIdGenerator.getGenerator().getNextNodeId(mKey.scope)),
-            requestedParallelism);
-        try {
-            clone.setPlans(clonePlans);
-        } catch (PlanException pe) {
-            CloneNotSupportedException cnse = new CloneNotSupportedException("Problem with setting plans of " + this.getClass().getSimpleName());
-            cnse.initCause(pe);
-            throw cnse;
-        }
-        clone.keyType = keyType;
-        clone.index = index;
-        // Needs to be called as setDistinct so that the fake index tuple gets
-        // created.
-        clone.setDistinct(mIsDistinct);
-        return clone;
-    }
-
-}
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POMergeCogroup.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POMergeCogroup.java
index b91f8520b..a36a89df5 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POMergeCogroup.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POMergeCogroup.java
@@ -22,6 +22,8 @@ import java.io.IOException;
 import java.io.ObjectInputStream;
 import java.util.ArrayList;
 import java.util.Comparator;
+import java.util.Iterator;
+import java.util.LinkedList;
 import java.util.List;
 import java.util.PriorityQueue;
 import java.util.Properties;
@@ -53,7 +55,10 @@ import org.apache.pig.impl.io.FileSpec;
 import org.apache.pig.impl.plan.NodeIdGenerator;
 import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.impl.plan.VisitorException;
+import org.apache.pig.impl.util.IdentityHashSet;
 import org.apache.pig.impl.util.Pair;
+import org.apache.pig.pen.util.ExampleTuple;
+import org.apache.pig.pen.util.LineageTracer;
 
 public class POMergeCogroup extends PhysicalOperator {
 
@@ -279,7 +284,7 @@ public class POMergeCogroup extends PhysicalOperator {
         for(int i=0; i < relationCnt; i++)
             out.set(i+1,(outBags[i]));
 
-        return new Result(POStatus.STATUS_OK, out);        
+        return new Result(POStatus.STATUS_OK, illustratorMarkup(null, out, -1));        
     }
 
 
@@ -592,4 +597,38 @@ public class POMergeCogroup extends PhysicalOperator {
             System.out.println(i+++"th item in heap: "+ copy.poll());
         }
     }
+    
+    @Override
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+        if(illustrator != null) {
+            ExampleTuple tOut = new ExampleTuple((Tuple) out);
+            LineageTracer lineageTracer = illustrator.getLineage();
+            lineageTracer.insert((Tuple) out);
+            Tuple tmp;
+            boolean synthetic = false;
+            try {
+                for (int i = 1; i < relationCnt; i++)
+                {
+                    DataBag dbs = (DataBag) ((Tuple) out).get(i);
+                    Iterator<Tuple> iter = dbs.iterator();
+                    while (iter.hasNext()) {
+                        tmp = iter.next();
+                        // any of synthetic data in bags causes the output tuple to be synthetic
+                        if (!synthetic && ((ExampleTuple)tmp).synthetic)
+                            synthetic = true;
+                        lineageTracer.union(tOut, tmp);
+                        // TODO constraint of >=2 tuples per eq. class
+                        illustrator.getEquivalenceClasses().get(i-1).add(tmp);
+                    }
+                }
+            } catch (ExecException e) {
+              // TODO better exception handling
+              throw new RuntimeException("Illustrator exception :"+e.getMessage());
+            }
+            tOut.synthetic = synthetic;
+            illustrator.addData((Tuple) tOut);
+            return tOut;
+        } else
+            return (Tuple) out;
+    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POMergeJoin.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POMergeJoin.java
index f81903bd0..70ab5598a 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POMergeJoin.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POMergeJoin.java
@@ -20,6 +20,8 @@ package org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOp
 import java.io.IOException;
 import java.io.ObjectInputStream;
 import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.LinkedList;
 import java.util.List;
 
 import org.apache.commons.logging.Log;
@@ -37,6 +39,7 @@ import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOpera
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
+import org.apache.pig.data.DataBag;
 import org.apache.pig.data.DataType;
 import org.apache.pig.data.Tuple;
 import org.apache.pig.data.TupleFactory;
@@ -47,7 +50,10 @@ import org.apache.pig.impl.plan.NodeIdGenerator;
 import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.impl.plan.PlanException;
 import org.apache.pig.impl.plan.VisitorException;
+import org.apache.pig.impl.util.IdentityHashSet;
 import org.apache.pig.impl.util.MultiMap;
+import org.apache.pig.pen.util.ExampleTuple;
+import org.apache.pig.pen.util.LineageTracer;
 
 /** This operator implements merge join algorithm to do map side joins. 
  *  Currently, only two-way joins are supported. One input of join is identified as left
@@ -204,6 +210,7 @@ public class POMergeJoin extends PhysicalOperator {
                 for(int i=0; i < rightTupSize; i++)
                     joinedTup.set(i+leftTupSize, curJoiningRightTup.get(i));
 
+                joinedTup = illustratorMarkup(null, joinedTup, 0);
                 return new Result(POStatus.STATUS_OK, joinedTup);
             }
             // Join with current right input has ended. But bag of left tuples
@@ -555,4 +562,28 @@ public class POMergeJoin extends PhysicalOperator {
     public String getIndexFile() {
         return indexFile;
     }
+    
+    @Override
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+        if(illustrator != null) {
+            ExampleTuple tOut = new ExampleTuple((Tuple) out);
+            tOut.synthetic = ((ExampleTuple) out).synthetic;
+            LineageTracer lineageTracer = illustrator.getLineage();
+            lineageTracer.insert(tOut);
+            try {
+                for (int i = 0; i < leftTupSize+rightTupSize; i++)
+                {
+                    lineageTracer.union(tOut,  (Tuple) tOut.get(i));
+                    illustrator.getEquivalenceClasses().get(i).add((Tuple)tOut);
+                    // TODO constraint of >=2 tuples per eq. class
+                }
+            } catch (ExecException e) {
+              // TODO better exception handling
+              throw new RuntimeException("Illustrator exception :"+e.getMessage());
+            }
+            illustrator.addData((Tuple) tOut);
+            return tOut;
+        }
+        return (Tuple) out;
+    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POMultiQueryPackage.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POMultiQueryPackage.java
index ac500eef3..9cb9d45e9 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POMultiQueryPackage.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POMultiQueryPackage.java
@@ -270,7 +270,7 @@ public class POMultiQueryPackage extends POPackage {
             myObj.setIndex(origIndex);
             tuple.set(0, myObj);
         }
-        
+        // illustrator markup has been handled by "pack"
         return res;
     }
 
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/PONative.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/PONative.java
index 24979c3e8..a174807bd 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/PONative.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/PONative.java
@@ -22,6 +22,7 @@ import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlan
 import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.impl.plan.VisitorException;
 import org.apache.pig.impl.util.Utils;
+import org.apache.pig.data.Tuple;
 
 public class PONative extends PhysicalOperator {
     
@@ -72,4 +73,8 @@ public class PONative extends PhysicalOperator {
         return false;
     }
 
+    @Override
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+        return null;
+    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POOptimizedForEach.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POOptimizedForEach.java
index 8520be07c..91b3f0036 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POOptimizedForEach.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POOptimizedForEach.java
@@ -19,6 +19,7 @@ package org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOp
 
 import java.util.ArrayList;
 import java.util.List;
+import java.util.LinkedList;
 
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.data.DataType;
@@ -31,6 +32,8 @@ import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.impl.plan.NodeIdGenerator;
 import org.apache.pig.impl.plan.VisitorException;
 import org.apache.pig.pen.util.ExampleTuple;
+import org.apache.pig.pen.util.LineageTracer;
+import org.apache.pig.impl.util.IdentityHashSet;
 
 /**
  * A specialized version of POForeach with the difference
@@ -97,13 +100,6 @@ public class POOptimizedForEach extends POForEach {
             while(true) {
                 res = processPlan();
                 if(res.returnStatus==POStatus.STATUS_OK) {
-                    if(lineageTracer !=  null && res.result != null) {
-                	ExampleTuple tOut = new ExampleTuple((Tuple) res.result);
-                	tOut.synthetic = tIn.synthetic;
-                	lineageTracer.insert(tOut);
-                	lineageTracer.union(tOut, tIn);
-                	res.result = tOut;
-                    }
                     return res;
                 }
                 if(res.returnStatus==POStatus.STATUS_EOP) {
@@ -132,16 +128,6 @@ public class POOptimizedForEach extends POForEach {
             res = processPlan();
             
             processingPlan = true;
-
-            if(lineageTracer != null && res.result != null) {
-        	//we check for res.result since that can also be null in the case of flatten
-        	tIn = (ExampleTuple) input;
-        	ExampleTuple tOut = new ExampleTuple((Tuple) res.result);
-        	tOut.synthetic = tIn.synthetic;
-        	lineageTracer.insert(tOut);
-        	lineageTracer.union(tOut, tIn);
-        	res.result = tOut;
-            }
             
             return res;
         }
@@ -171,6 +157,4 @@ public class POOptimizedForEach extends POForEach {
             NodeIdGenerator.getGenerator().getNextNodeId(mKey.scope)),
             requestedParallelism, plans, flattens);
     }
-
-    
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPackage.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPackage.java
index adf3c87b2..b9ba4ca5e 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPackage.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPackage.java
@@ -21,11 +21,10 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.Iterator;
+import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
 
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.data.AccumulativeBag;
 import org.apache.pig.data.BagFactory;
@@ -44,7 +43,11 @@ import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
 import org.apache.pig.impl.plan.VisitorException;
+import org.apache.pig.impl.util.IdentityHashSet;
 import org.apache.pig.impl.util.Pair;
+import org.apache.pig.pen.util.ExampleTuple;
+import org.apache.pig.pen.util.LineageTracer;
+
 /**
  * The package operator that packages
  * the globally rearranged tuples into
@@ -112,8 +115,6 @@ public class POPackage extends PhysicalOperator {
     // "value" to column numbers in the "key" which contain the fields in
     // the "value"
     protected Map<Integer, Pair<Boolean, Map<Integer, Integer>>> keyInfo;
-    
-    transient private final Log log = LogFactory.getLog(getClass());
 
     protected static final BagFactory mBagFactory = BagFactory.getInstance();
     protected static final TupleFactory mTupleFactory = TupleFactory.getInstance();
@@ -304,10 +305,13 @@ public class POPackage extends PhysicalOperator {
                 res.set(i+1,bag);
             }
         }
-        detachInput();
         Result r = new Result();
-        r.result = res;
         r.returnStatus = POStatus.STATUS_OK;
+        if (!isAccumulative())
+            r.result = illustratorMarkup(null, res, 0);
+        else
+            r.result = res;
+        detachInput();
         return r;
     }
 
@@ -355,19 +359,19 @@ public class POPackage extends PhysicalOperator {
                     }
                 }
             }
-            
+            copy = illustratorMarkup2(val, copy);
         } else if (isProjectStar) {
             
             // the whole "value" is present in the "key"
             copy = mTupleFactory.newTuple(keyAsTuple.getAll());
-            
+            copy = illustratorMarkup2(keyAsTuple, copy);
         } else {
             
             // there is no field of the "value" in the
             // "key" - so just make a copy of what we got
             // as the "value"
             copy = mTupleFactory.newTuple(val.getAll());
-            
+            copy = illustratorMarkup2(val, copy);
         }
         return copy;
     }
@@ -462,7 +466,7 @@ public class POPackage extends PhysicalOperator {
         return pkgType;
     }
     
-    private class POPackageTupleBuffer implements AccumulativeTupleBuffer {
+    class POPackageTupleBuffer implements AccumulativeTupleBuffer {
         private List<Tuple>[] bags;
         private Iterator<NullableTuple> iter;
         private int batchSize;
@@ -529,5 +533,79 @@ public class POPackage extends PhysicalOperator {
         public Iterator<Tuple> getTuples(int index) {			
             return bags[index].iterator();
         }
+        
+        public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+            return POPackage.this.illustratorMarkup(in, out, eqClassIndex);
+        }
        };
+       
+       private Tuple illustratorMarkup2(Object in, Object out) {
+           if(illustrator != null) {
+               ExampleTuple tOut = new ExampleTuple((Tuple) out);
+               illustrator.getLineage().insert(tOut);
+               tOut.synthetic = ((ExampleTuple) in).synthetic;
+               illustrator.getLineage().union(tOut, (Tuple) in);
+               return tOut;
+           } else
+               return (Tuple) out;
+       }
+       
+       @Override
+       public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+           if(illustrator != null) {
+               ExampleTuple tOut = new ExampleTuple((Tuple) out);
+               LineageTracer lineageTracer = illustrator.getLineage();
+               lineageTracer.insert(tOut);
+               Tuple tmp;
+               boolean synthetic = false;
+               if (illustrator.getEquivalenceClasses() == null) {
+                   LinkedList<IdentityHashSet<Tuple>> equivalenceClasses = new LinkedList<IdentityHashSet<Tuple>>();
+                   for (int i = 0; i < numInputs; ++i) {
+                       IdentityHashSet<Tuple> equivalenceClass = new IdentityHashSet<Tuple>();
+                       equivalenceClasses.add(equivalenceClass);
+                   }
+                   illustrator.setEquivalenceClasses(equivalenceClasses, this);
+               }
+
+               if (distinct) {
+                   int count;
+                   for (count = 0; tupIter.hasNext(); ++count) {
+                       NullableTuple ntp = tupIter.next();
+                       tmp = (Tuple) ntp.getValueAsPigType();
+                       if (!tmp.equals(tOut))
+                           lineageTracer.union(tOut, tmp);
+                   }
+                   if (count > 1) // only non-distinct tuples are inserted into the equivalence class
+                       illustrator.getEquivalenceClasses().get(eqClassIndex).add(tOut);
+                   illustrator.addData((Tuple) tOut);
+                   return (Tuple) tOut;
+               }
+               boolean outInEqClass = true;
+               try {
+                   for (int i = 1; i < numInputs+1; i++)
+                   {
+                       DataBag dbs = (DataBag) ((Tuple) out).get(i);
+                       Iterator<Tuple> iter = dbs.iterator();
+                       if (dbs.size() <= 1 && outInEqClass) // all inputs have >= 2 records
+                           outInEqClass = false;
+                       while (iter.hasNext()) {
+                           tmp = iter.next();
+                           // any of synthetic data in bags causes the output tuple to be synthetic
+                           if (!synthetic && ((ExampleTuple)tmp).synthetic)
+                               synthetic = true;
+                           lineageTracer.union(tOut, tmp);
+                       }
+                   }
+               } catch (ExecException e) {
+                 // TODO better exception handling
+                 throw new RuntimeException("Illustrator exception :"+e.getMessage());
+               }
+               if (outInEqClass)
+                   illustrator.getEquivalenceClasses().get(eqClassIndex).add(tOut);
+               tOut.synthetic = synthetic;
+               illustrator.addData((Tuple) tOut);
+               return tOut;
+           } else
+               return (Tuple) out;
+       }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPackageLite.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPackageLite.java
index a8e2aa90b..8ab351d10 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPackageLite.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPackageLite.java
@@ -22,10 +22,13 @@
 package org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators;
 
 import java.util.Iterator;
+import java.util.LinkedList;
 import java.util.List;
 import java.util.HashMap;
 import java.util.Map;
 import java.util.Map.Entry;
+
+import org.apache.pig.impl.util.IdentityHashSet;
 import org.apache.pig.impl.util.Pair;
 
 import org.apache.pig.PigException;
@@ -42,6 +45,8 @@ import org.apache.pig.impl.io.NullableTuple;
 import org.apache.pig.impl.plan.NodeIdGenerator;
 import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.impl.plan.VisitorException;
+import org.apache.pig.pen.util.ExampleTuple;
+import org.apache.pig.pen.util.LineageTracer;
 
 /**
  * This package operator is a specialization
@@ -173,8 +178,8 @@ public class POPackageLite extends POPackage {
         res.set(1,db);
         detachInput();
         Result r = new Result();
-        r.result = res;
         r.returnStatus = POStatus.STATUS_OK;
+        r.result = illustratorMarkup(null, res, 0);
         return r;
     }
     
@@ -202,5 +207,27 @@ public class POPackageLite extends POPackage {
                 + DataType.findTypeName(keyType) + "}" + " - "
                 + mKey.toString();
     }
+    
+    @Override
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+        if(illustrator != null) {
+            ExampleTuple tOut = new ExampleTuple((Tuple) out);
+            LineageTracer lineageTracer = illustrator.getLineage();
+            lineageTracer.insert(tOut);
+            if (illustrator.getEquivalenceClasses() == null) {
+                LinkedList<IdentityHashSet<Tuple>> equivalenceClasses = new LinkedList<IdentityHashSet<Tuple>>();
+                for (int i = 0; i < numInputs; ++i) {
+                    IdentityHashSet<Tuple> equivalenceClass = new IdentityHashSet<Tuple>();
+                    equivalenceClasses.add(equivalenceClass);
+                }
+                illustrator.setEquivalenceClasses(equivalenceClasses, this);
+            }
+            illustrator.getEquivalenceClasses().get(eqClassIndex).add(tOut);
+            tOut.synthetic = false;  // not expect this to be really used
+            illustrator.addData((Tuple) tOut);
+            return tOut;
+        } else
+            return (Tuple) out;
+    }
 }
 
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPreCombinerLocalRearrange.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPreCombinerLocalRearrange.java
index deed3c8f9..eb051a0fc 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPreCombinerLocalRearrange.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPreCombinerLocalRearrange.java
@@ -234,4 +234,8 @@ public class POPreCombinerLocalRearrange extends PhysicalOperator {
         }            
     }
 
+    @Override
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+        return null;
+    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/PORead.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/PORead.java
index 9b8a36203..78b9c6a49 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/PORead.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/PORead.java
@@ -103,4 +103,8 @@ public class PORead extends PhysicalOperator {
         v.visitRead(this);
     }
 
+    @Override
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+        return null;
+    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POSkewedJoin.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POSkewedJoin.java
index 9de112263..53d2017e9 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POSkewedJoin.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POSkewedJoin.java
@@ -27,6 +27,7 @@ import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOpera
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
 import org.apache.pig.data.DataType;
+import org.apache.pig.data.Tuple;
 import org.apache.pig.impl.logicalLayer.schema.Schema;
 import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.impl.plan.VisitorException;
@@ -121,4 +122,8 @@ public class POSkewedJoin extends PhysicalOperator  {
 		return inputSchema.get(i);
 	}
 
+	@Override
+  public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+	    return null;
+	}
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POSort.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POSort.java
index 2d363ef7f..0faae99b3 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POSort.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POSort.java
@@ -21,6 +21,7 @@ import java.io.Serializable;
 import java.util.ArrayList;
 import java.util.Comparator;
 import java.util.Iterator;
+import java.util.LinkedList;
 import java.util.List;
 
 import org.apache.commons.logging.Log;
@@ -47,6 +48,8 @@ import org.apache.pig.data.TupleFactory;
 import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.impl.plan.NodeIdGenerator;
 import org.apache.pig.impl.plan.VisitorException;
+import org.apache.pig.impl.util.IdentityHashSet;
+import org.apache.pig.pen.util.LineageTracer;
 
 /**
  * This implementation is applicable for both the physical plan and for the
@@ -301,10 +304,7 @@ public class POSort extends PhysicalOperator {
         }
         if (it.hasNext()) {
             res.result = it.next();
-            if(lineageTracer != null) {
-                lineageTracer.insert((Tuple) res.result);
-                lineageTracer.union((Tuple)res.result, (Tuple)res.result);
-            }
+            illustratorMarkup(res.result, res.result, 0);
             res.returnStatus = POStatus.STATUS_OK;
         } else {
             res.returnStatus = POStatus.STATUS_EOP;
@@ -407,4 +407,12 @@ public class POSort extends PhysicalOperator {
     public SortInfo getSortInfo() {
         return sortInfo;
     }
+    
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+        if(illustrator != null) {
+          illustrator.getEquivalenceClasses().get(eqClassIndex).add((Tuple) in);
+            illustrator.addData((Tuple) out);
+        }
+        return (Tuple) out;
+    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POSplit.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POSplit.java
index c1a45443a..220488ca7 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POSplit.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POSplit.java
@@ -19,6 +19,7 @@ package org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOp
 
 import java.util.ArrayList;
 import java.util.BitSet;
+import java.util.LinkedList;
 import java.util.List;
 
 import org.apache.commons.logging.Log;
@@ -33,6 +34,9 @@ import org.apache.pig.data.Tuple;
 import org.apache.pig.impl.io.FileSpec;
 import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.impl.plan.VisitorException;
+import org.apache.pig.impl.util.IdentityHashSet;
+import org.apache.pig.pen.util.ExampleTuple;
+import org.apache.pig.pen.util.LineageTracer;
 
 /**
  * The MapReduce Split operator.
@@ -314,5 +318,11 @@ public class POSplit extends PhysicalOperator {
         return res;
                 
     }
+    
+    @Override
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+      // no op  
+      return null;
+    }
         
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POStore.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POStore.java
index 253ae49b3..b0115d1ee 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POStore.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POStore.java
@@ -19,6 +19,7 @@ package org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOp
 
 import java.io.IOException;
 import java.util.List;
+import java.util.LinkedList;
 
 import org.apache.hadoop.mapreduce.Counter;
 import org.apache.pig.PigException;
@@ -36,6 +37,9 @@ import org.apache.pig.impl.io.FileSpec;
 import org.apache.pig.impl.logicalLayer.schema.Schema;
 import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.impl.plan.VisitorException;
+import org.apache.pig.impl.util.IdentityHashSet;
+import org.apache.pig.pen.util.ExampleTuple;
+import org.apache.pig.pen.util.LineageTracer;
 
 /**
  * The store operator which is used in two ways:
@@ -132,8 +136,12 @@ public class POStore extends PhysicalOperator {
         try {
             switch (res.returnStatus) {
             case POStatus.STATUS_OK:
-                storer.putNext((Tuple)res.result);
+                if (illustrator == null) {
+                    storer.putNext((Tuple)res.result);
+                } else
+                    illustratorMarkup(res.result, res.result, 0);
                 res = empty;
+
                 if (outputRecordCounter != null) {
                     outputRecordCounter.increment(1);
                 }
@@ -250,4 +258,17 @@ public class POStore extends PhysicalOperator {
     public boolean isMultiStore() {
         return isMultiStore;
     }
+    
+    @Override
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+        if(illustrator != null) {
+            ExampleTuple tIn = (ExampleTuple) in;
+            LineageTracer lineage = illustrator.getLineage();
+            lineage.insert(tIn);
+            if (!isTmpStore)
+                illustrator.getEquivalenceClasses().get(eqClassIndex).add(tIn);
+            illustrator.addData((Tuple) out);
+        }
+        return (Tuple) out;
+    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POStream.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POStream.java
index bd35470fd..e3379c80c 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POStream.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POStream.java
@@ -20,6 +20,7 @@ package org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOp
 
 import java.io.IOException;
 import java.util.Iterator;
+import java.util.LinkedList;
 import java.util.List;
 import java.util.Properties;
 import java.util.concurrent.ArrayBlockingQueue;
@@ -31,6 +32,9 @@ import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.impl.plan.VisitorException;
 import org.apache.pig.impl.streaming.ExecutableManager;
 import org.apache.pig.impl.streaming.StreamingCommand;
+import org.apache.pig.impl.util.IdentityHashSet;
+import org.apache.pig.pen.util.ExampleTuple;
+import org.apache.pig.pen.util.LineageTracer;
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
@@ -158,7 +162,8 @@ public class POStream extends PhysicalOperator {
                     // we don't need to set any flag noting we saw all output
                     // from binary
                     r = EOP_RESULT;
-                }
+                } else if (r.returnStatus == POStatus.STATUS_OK)
+                    illustratorMarkup(r.result, r.result, 0);
                 return(r);
             }
             
@@ -207,7 +212,8 @@ public class POStream extends PhysicalOperator {
                     // should never be called. So we don't need to set any 
                     // flag noting we saw all output from binary
                     r = EOP_RESULT;
-                }
+                } else if (r.returnStatus == POStatus.STATUS_OK)
+                  illustratorMarkup(r.result, r.result, 0);
                 return r;
             } else {
                 // we are not being called from close() - so
@@ -222,7 +228,8 @@ public class POStream extends PhysicalOperator {
                     // for future calls
                     r = EOP_RESULT;
                     allOutputFromBinaryProcessed  = true;
-                }
+                } else if (r.returnStatus == POStatus.STATUS_OK)
+                    illustratorMarkup(r.result, r.result, 0);
                 return r;
             }
             
@@ -348,4 +355,14 @@ public class POStream extends PhysicalOperator {
     public BlockingQueue<Result> getBinaryOutputQueue() {
         return binaryOutputQueue;
     }
+    
+    @Override
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+      if(illustrator != null) {
+          ExampleTuple tIn = (ExampleTuple) in;
+          illustrator.getEquivalenceClasses().get(eqClassIndex).add(tIn);
+            illustrator.addData((Tuple) out);
+      }
+      return (Tuple) out;
+    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POUnion.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POUnion.java
index ce23c8bfd..b29c481c2 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POUnion.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POUnion.java
@@ -18,6 +18,7 @@
 package org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators;
 
 import java.util.BitSet;
+import java.util.LinkedList;
 import java.util.List;
 
 import org.apache.pig.backend.executionengine.ExecException;
@@ -29,6 +30,7 @@ import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
 import org.apache.pig.impl.plan.VisitorException;
+import org.apache.pig.impl.util.IdentityHashSet;
 import org.apache.pig.pen.util.ExampleTuple;
 
 /**
@@ -165,6 +167,7 @@ public class POUnion extends PhysicalOperator {
 
                     if(res.returnStatus == POStatus.STATUS_OK || 
                             res.returnStatus == POStatus.STATUS_NULL || res.returnStatus == POStatus.STATUS_ERR) {
+                        illustratorMarkup(res.result, res.result, ind);
                         return res;
                     }
 
@@ -181,14 +184,29 @@ public class POUnion extends PhysicalOperator {
             res.returnStatus = POStatus.STATUS_OK;
             detachInput();
             nextReturnEOP = true ;
-            if(lineageTracer != null) {
-        	ExampleTuple tOut = (ExampleTuple) res.result;
-        	lineageTracer.insert(tOut);
-        	lineageTracer.union(tOut, tOut);
-            }
+            illustratorMarkup(res.result, res.result, 0);
             return res;
         }
 
 
     }
+    
+    @Override
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+        if(illustrator != null) {
+            if (illustrator.getEquivalenceClasses() == null) {
+                int size = (inputs == null ? 1 : inputs.size());
+                LinkedList<IdentityHashSet<Tuple>> equivalenceClasses = new LinkedList<IdentityHashSet<Tuple>>();
+                for (int i = 0; i < size; ++i) {
+                    IdentityHashSet<Tuple> equivalenceClass = new IdentityHashSet<Tuple>();
+                    equivalenceClasses.add(equivalenceClass);
+                }
+                illustrator.setEquivalenceClasses(equivalenceClasses, this);
+            }
+            ExampleTuple tIn = (ExampleTuple) in;
+            illustrator.getEquivalenceClasses().get(eqClassIndex).add(tIn);
+            illustrator.addData((Tuple) out);
+        }
+        return null;
+    }
 }
diff --git a/src/org/apache/pig/data/AccumulativeBag.java b/src/org/apache/pig/data/AccumulativeBag.java
index fac278610..74bb3252d 100644
--- a/src/org/apache/pig/data/AccumulativeBag.java
+++ b/src/org/apache/pig/data/AccumulativeBag.java
@@ -68,7 +68,9 @@ public class AccumulativeBag implements DataBag {
     }
 
     public long size() {		
-        throw new RuntimeException("AccumulativeBag does not support size() operation");
+        int size = 0;
+        for (Iterator<Tuple> it = iterator(); it.hasNext(); it.next(), ++size);
+        return size;
     }
 
     public long getMemorySize() {	
diff --git a/src/org/apache/pig/data/TupleFactory.java b/src/org/apache/pig/data/TupleFactory.java
index f429a6cbb..079767187 100644
--- a/src/org/apache/pig/data/TupleFactory.java
+++ b/src/org/apache/pig/data/TupleFactory.java
@@ -75,6 +75,24 @@ public abstract class TupleFactory {
                     throw new RuntimeException("Unable to instantiate "
                         + "tuple factory " + factoryName, e);
                 }
+            } else if (factoryName != null) {
+                try {
+                    Class c = Class.forName(factoryName);
+                    Object o = c.newInstance();
+                    if (!(o instanceof TupleFactory)) {
+                        throw new RuntimeException("Provided factory " +
+                            factoryName + " does not extend TupleFactory!");
+                    }
+                    gSelf = (TupleFactory)o;
+                } catch (Exception e) {
+                    if (e instanceof RuntimeException) {
+                      // We just threw this
+                      RuntimeException re = (RuntimeException)e;
+                      throw re;
+                    }
+                    throw new RuntimeException("Unable to instantiate "
+                        + "tuple factory " + factoryName, e);
+                }
             } else {
                 gSelf = new BinSedesTupleFactory();
             }
diff --git a/src/org/apache/pig/impl/PigContext.java b/src/org/apache/pig/impl/PigContext.java
index a58d62a60..b668a42b6 100644
--- a/src/org/apache/pig/impl/PigContext.java
+++ b/src/org/apache/pig/impl/PigContext.java
@@ -115,11 +115,14 @@ public class PigContext implements Serializable {
     
     public int defaultParallel = -1;
 
-    // Says, wether we're processing an explain right now. Explain
+    // Says, whether we're processing an explain right now. Explain
     // might skip some check in the logical plan validation (file
     // existence checks, etc).
     public boolean inExplain = false;
     
+    // whether we're processing an ILLUSTRATE right now.
+    public boolean inIllustrator = false;
+    
     private String last_alias = null;
 
     // List of paths skipped for automatic shipping
diff --git a/src/org/apache/pig/impl/builtin/ReadScalars.java b/src/org/apache/pig/impl/builtin/ReadScalars.java
index a487a0d70..880dcba40 100644
--- a/src/org/apache/pig/impl/builtin/ReadScalars.java
+++ b/src/org/apache/pig/impl/builtin/ReadScalars.java
@@ -18,16 +18,17 @@
 package org.apache.pig.impl.builtin;
 
 import java.io.IOException;
+import java.util.Map;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.pig.EvalFunc;
 import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce;
 import org.apache.pig.data.DataType;
 import org.apache.pig.data.Tuple;
 import org.apache.pig.impl.io.InterStorage;
 import org.apache.pig.impl.io.ReadToEndLoader;
 import org.apache.pig.impl.util.UDFContext;
+import org.apache.pig.data.DataBag;
 
 /**
  * ReadScalars reads a line from a file and returns it as its value. The
@@ -39,6 +40,9 @@ public class ReadScalars extends EvalFunc<Object> {
     private String scalarfilename = null;
   //  private String charset = "UTF-8";
     private Object value = null;
+    
+    // in-core input : used by illustrator
+    private Map<String, DataBag> inputBuffer = null;
 
     /**
      * Java level API
@@ -54,6 +58,24 @@ public class ReadScalars extends EvalFunc<Object> {
                 return null;
 
             int pos;
+            if (inputBuffer != null)
+            {
+                pos = DataType.toInteger(input.get(0));
+                scalarfilename = DataType.toString(input.get(1));
+                DataBag inputBag = inputBuffer.get(scalarfilename);
+                if (inputBag == null || inputBag.size() ==0)
+                {
+                    log.warn("No scalar field to read, returning null");
+                    return null;
+                } else if (inputBag.size() > 1) {
+                    String msg = "Scalar has more than one row in the output.";
+                    throw new ExecException(msg);
+                }
+                Tuple t1 = inputBag.iterator().next();
+                value = t1.get(pos);
+                return value;
+            }
+            
             ReadToEndLoader loader;
             try {
                 pos = DataType.toInteger(input.get(0));
@@ -92,4 +114,8 @@ public class ReadScalars extends EvalFunc<Object> {
         return value;
     }
 
+    public void setOutputBuffer(Map<String, DataBag> inputBuffer) {
+        this.inputBuffer = inputBuffer;
+        value = null;
+    }
 }
diff --git a/src/org/apache/pig/newplan/logical/ForeachInnerPlanVisitor.java b/src/org/apache/pig/newplan/logical/ForeachInnerPlanVisitor.java
index 23d9c42c4..0c666d326 100644
--- a/src/org/apache/pig/newplan/logical/ForeachInnerPlanVisitor.java
+++ b/src/org/apache/pig/newplan/logical/ForeachInnerPlanVisitor.java
@@ -105,6 +105,10 @@ public class ForeachInnerPlanVisitor extends LogicalExpPlanMigrationVistor {
         return childPlanVisitor.exprPlan;
     }
     
+    public Map<LogicalOperator, LogicalRelationalOperator> getInnerOpMap() {
+        return innerOpsMap;
+    }
+    
     public void visit(LOProject project) throws VisitorException {
         LogicalOperator op = project.getExpression();
         
diff --git a/src/org/apache/pig/newplan/logical/LogicalPlanMigrationVistor.java b/src/org/apache/pig/newplan/logical/LogicalPlanMigrationVistor.java
index 761b471d2..65db4c5c7 100644
--- a/src/org/apache/pig/newplan/logical/LogicalPlanMigrationVistor.java
+++ b/src/org/apache/pig/newplan/logical/LogicalPlanMigrationVistor.java
@@ -60,13 +60,23 @@ import org.apache.pig.newplan.logical.relational.LogicalSchema;
 public class LogicalPlanMigrationVistor extends LOVisitor { 
     private org.apache.pig.newplan.logical.relational.LogicalPlan logicalPlan;
     private Map<LogicalOperator, LogicalRelationalOperator> opsMap;
+    private Map<LOForEach, Map<LogicalOperator, LogicalRelationalOperator>> forEachInnerMap;
    
     public LogicalPlanMigrationVistor(LogicalPlan plan) {
         super(plan, new DependencyOrderWalker<LogicalOperator, LogicalPlan>(plan));
         logicalPlan = new org.apache.pig.newplan.logical.relational.LogicalPlan();
         opsMap = new HashMap<LogicalOperator, LogicalRelationalOperator>();
+        forEachInnerMap = new HashMap<LOForEach, Map<LogicalOperator, LogicalRelationalOperator>>();
     }    
     
+    public Map<LogicalOperator, LogicalRelationalOperator> getOldToNewLOOpMap() {
+        return opsMap;
+    }
+    
+    public Map<LOForEach, Map<LogicalOperator, LogicalRelationalOperator>> getForEachInnerMap() {
+        return forEachInnerMap;
+    }
+    
     private void translateConnection(LogicalOperator oldOp, org.apache.pig.newplan.Operator newOp) {       
         List<LogicalOperator> preds = mPlan.getPredecessors(oldOp); 
         
@@ -227,12 +237,14 @@ public class LogicalPlanMigrationVistor extends LOVisitor {
         innerPlan.add(gen);                
         
         List<LogicalPlan> ll = forEach.getForEachPlans();
+        Map<LogicalOperator, LogicalRelationalOperator> innerMap = new HashMap<LogicalOperator, LogicalRelationalOperator>();
+        forEachInnerMap.put(forEach, innerMap);
         try {
             for(int i=0; i<ll.size(); i++) {
                 LogicalPlan lp = ll.get(i);
                 ForeachInnerPlanVisitor v = new ForeachInnerPlanVisitor(newForeach, forEach, lp, mPlan, opsMap);
                 v.visit();
-                
+                innerMap.putAll(v.getInnerOpMap());
                 expPlans.add(v.exprPlan);
             }
         } catch (FrontendException e) {
diff --git a/src/org/apache/pig/newplan/logical/relational/LOCogroup.java b/src/org/apache/pig/newplan/logical/relational/LOCogroup.java
index 09615e8be..a3fc84445 100644
--- a/src/org/apache/pig/newplan/logical/relational/LOCogroup.java
+++ b/src/org/apache/pig/newplan/logical/relational/LOCogroup.java
@@ -267,6 +267,10 @@ public class LOCogroup extends LogicalRelationalOperator {
         return mGroupType;
     }
     
+    public void resetGroupType() {
+        mGroupType = GROUPTYPE.REGULAR;
+    }
+    
     /**
      * Returns an Unmodifiable Map of Input Number to Uid 
      * @return Unmodifiable Map<Integer,Long>
diff --git a/src/org/apache/pig/newplan/logical/relational/LOJoin.java b/src/org/apache/pig/newplan/logical/relational/LOJoin.java
index 4040974e0..50e6c4382 100644
--- a/src/org/apache/pig/newplan/logical/relational/LOJoin.java
+++ b/src/org/apache/pig/newplan/logical/relational/LOJoin.java
@@ -82,6 +82,10 @@ public class LOJoin extends LogicalRelationalOperator {
         return mJoinType;
     }
     
+    public void resetJoinType() {
+        mJoinType = JOINTYPE.HASH;
+    }
+    
     public Collection<LogicalExpressionPlan> getJoinPlan(int inputIndex) {
         return mJoinPlans.get(inputIndex);
     }
diff --git a/src/org/apache/pig/newplan/logical/relational/LogToPhyTranslationVisitor.java b/src/org/apache/pig/newplan/logical/relational/LogToPhyTranslationVisitor.java
index f028c247c..0a89f01c1 100644
--- a/src/org/apache/pig/newplan/logical/relational/LogToPhyTranslationVisitor.java
+++ b/src/org/apache/pig/newplan/logical/relational/LogToPhyTranslationVisitor.java
@@ -105,6 +105,10 @@ public class LogToPhyTranslationVisitor extends LogicalRelationalNodesVisitor {
         this.pc = pc;
     }
 
+    public Map<Operator, PhysicalOperator> getLogToPhyMap() {
+        return logToPhyMap;
+    }
+    
     public PhysicalPlan getPhysicalPlan() {
         return currentPlan;
     }
diff --git a/src/org/apache/pig/newplan/logical/rules/MergeForEach.java b/src/org/apache/pig/newplan/logical/rules/MergeForEach.java
index 8b947ed19..af3396082 100644
--- a/src/org/apache/pig/newplan/logical/rules/MergeForEach.java
+++ b/src/org/apache/pig/newplan/logical/rules/MergeForEach.java
@@ -28,7 +28,6 @@ import java.util.Set;
 import org.apache.pig.newplan.Operator;
 import org.apache.pig.newplan.OperatorPlan;
 import org.apache.pig.newplan.OperatorSubPlan;
-import org.apache.pig.newplan.logical.expression.LogicalExpression;
 import org.apache.pig.newplan.logical.expression.LogicalExpressionPlan;
 import org.apache.pig.newplan.logical.expression.ProjectExpression;
 import org.apache.pig.newplan.logical.relational.LOForEach;
diff --git a/src/org/apache/pig/newplan/logical/rules/TypeCastInserter.java b/src/org/apache/pig/newplan/logical/rules/TypeCastInserter.java
index db6270043..81026d8f8 100644
--- a/src/org/apache/pig/newplan/logical/rules/TypeCastInserter.java
+++ b/src/org/apache/pig/newplan/logical/rules/TypeCastInserter.java
@@ -119,6 +119,8 @@ public abstract class TypeCastInserter extends Rule {
             foreach.setAlias(op.getAlias());
             
             // Insert the foreach into the plan and patch up the plan.
+            if (currentPlan.getSuccessors(op) == null)
+                return;
             Operator next = currentPlan.getSuccessors(op).get(0);
             currentPlan.insertBetween(op, foreach, next);
             
diff --git a/src/org/apache/pig/pen/AugmentBaseDataVisitor.java b/src/org/apache/pig/pen/AugmentBaseDataVisitor.java
index fa8fa4be3..0bff8c85f 100644
--- a/src/org/apache/pig/pen/AugmentBaseDataVisitor.java
+++ b/src/org/apache/pig/pen/AugmentBaseDataVisitor.java
@@ -24,10 +24,13 @@ import java.util.Iterator;
 import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
+import java.util.HashSet;
+import java.util.Set;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.pig.backend.executionengine.ExecException;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
 import org.apache.pig.data.BagFactory;
 import org.apache.pig.data.DataBag;
 import org.apache.pig.data.DataByteArray;
@@ -41,6 +44,7 @@ import org.apache.pig.impl.logicalLayer.LOAdd;
 import org.apache.pig.impl.logicalLayer.LOAnd;
 import org.apache.pig.impl.logicalLayer.LOCast;
 import org.apache.pig.impl.logicalLayer.LOCogroup;
+import org.apache.pig.impl.logicalLayer.LOLimit;
 import org.apache.pig.impl.logicalLayer.LOConst;
 import org.apache.pig.impl.logicalLayer.LOCross;
 import org.apache.pig.impl.logicalLayer.LODistinct;
@@ -69,10 +73,10 @@ import org.apache.pig.impl.logicalLayer.LOVisitor;
 import org.apache.pig.impl.logicalLayer.LogicalOperator;
 import org.apache.pig.impl.logicalLayer.LogicalPlan;
 import org.apache.pig.impl.logicalLayer.schema.Schema;
-import org.apache.pig.impl.plan.DepthFirstWalker;
 import org.apache.pig.impl.plan.VisitorException;
 import org.apache.pig.pen.util.ExampleTuple;
 import org.apache.pig.pen.util.PreOrderDepthFirstWalker;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLimit;
 
 //This is used to generate synthetic data
 //Synthetic data generation is done by making constraint tuples for each operator as we traverse the plan
@@ -83,6 +87,9 @@ public class AugmentBaseDataVisitor extends LOVisitor {
     Map<LOLoad, DataBag> baseData = null;
     Map<LOLoad, DataBag> newBaseData = new HashMap<LOLoad, DataBag>();
     Map<LogicalOperator, DataBag> derivedData = null;
+    private boolean limit = false;
+    private final Map<LogicalOperator, PhysicalOperator> logToPhysMap;
+    private Map<LOLimit, Long> oriLimitMap;
 
     Map<LogicalOperator, DataBag> outputConstraintsMap = new HashMap<LogicalOperator, DataBag>();
 
@@ -91,15 +98,20 @@ public class AugmentBaseDataVisitor extends LOVisitor {
     // Augmentation moves from the leaves to root and hence needs a
     // depthfirstwalker
     public AugmentBaseDataVisitor(LogicalPlan plan,
+            Map<LogicalOperator, PhysicalOperator> logToPhysMap,
             Map<LOLoad, DataBag> baseData,
             Map<LogicalOperator, DataBag> derivedData) {
         super(plan, new PreOrderDepthFirstWalker<LogicalOperator, LogicalPlan>(
                 plan));
         this.baseData = baseData;
         this.derivedData = derivedData;
-
+        this.logToPhysMap = logToPhysMap;
     }
 
+    public void setLimit() {
+        limit = true;
+    }
+    
     public Map<LOLoad, DataBag> getNewBaseData() {
         for (Map.Entry<LOLoad, DataBag> e : baseData.entrySet()) {
             DataBag bag = newBaseData.get(e.getKey());
@@ -112,8 +124,14 @@ public class AugmentBaseDataVisitor extends LOVisitor {
         return newBaseData;
     }
 
+    public Map<LOLimit, Long> getOriLimitMap() {
+        return oriLimitMap;
+    }
+    
     @Override
     protected void visit(LOCogroup cg) throws VisitorException {
+        if (limit && !((PreOrderDepthFirstWalker<LogicalOperator, LogicalPlan>) mCurrentWalker).getBranchFlag())
+            return;
         // we first get the outputconstraints for the current cogroup
         DataBag outputConstraints = outputConstraintsMap.get(cg);
         outputConstraintsMap.remove(cg);
@@ -234,11 +252,58 @@ public class AugmentBaseDataVisitor extends LOVisitor {
 
     @Override
     protected void visit(LODistinct dt) throws VisitorException {
+        if (limit && !((PreOrderDepthFirstWalker<LogicalOperator, LogicalPlan>) mCurrentWalker).getBranchFlag())
+            return;
+    
+        DataBag outputConstraints = outputConstraintsMap.get(dt);
+        outputConstraintsMap.remove(dt);
 
+        DataBag inputConstraints = outputConstraintsMap.get(dt.getInput());
+        if (inputConstraints == null) {
+            inputConstraints = BagFactory.getInstance().newDefaultBag();
+            outputConstraintsMap.put(dt.getInput(), inputConstraints);
+        }
+    
+        if (outputConstraints != null && outputConstraints.size() > 0) {
+            for (Iterator<Tuple> it = outputConstraints.iterator(); it.hasNext();)
+            {
+                inputConstraints.add(it.next());
+            }
+        }
+        
+        boolean emptyInputConstraints = inputConstraints.size() == 0;
+        if (emptyInputConstraints) {
+            DataBag inputData = derivedData.get(dt.getInput());
+            for (Iterator<Tuple> it = inputData.iterator(); it.hasNext();)
+            {
+                inputConstraints.add(it.next());
+            }
+        }
+        Set<Tuple> distinctSet = new HashSet<Tuple>();
+        Iterator<Tuple> it;
+        for (it = inputConstraints.iterator(); it.hasNext();) {
+            if (!distinctSet.add(it.next()))
+                break;
+        }
+        if (!it.hasNext())
+        {
+            // no duplicates found: generate one
+            if (inputConstraints.size()> 0) {
+                Tuple src = ((ExampleTuple)inputConstraints.iterator().next()).toTuple(),
+                      tgt = TupleFactory.getInstance().newTuple(src.getAll());
+                ExampleTuple inputConstraint = new ExampleTuple(tgt);
+                inputConstraint.synthetic = true;
+                inputConstraints.add(inputConstraint);
+            } else if (emptyInputConstraints)
+                inputConstraints.clear();
+        }
     }
 
     @Override
     protected void visit(LOFilter filter) throws VisitorException {
+        if (limit && !((PreOrderDepthFirstWalker<LogicalOperator, LogicalPlan>) mCurrentWalker).getBranchFlag())
+            return;
+        
         DataBag outputConstraints = outputConstraintsMap.get(filter);
         outputConstraintsMap.remove(filter);
 
@@ -308,6 +373,8 @@ public class AugmentBaseDataVisitor extends LOVisitor {
 
     @Override
     protected void visit(LOForEach forEach) throws VisitorException {
+        if (limit && !((PreOrderDepthFirstWalker<LogicalOperator, LogicalPlan>) mCurrentWalker).getBranchFlag())
+            return;
         DataBag outputConstraints = outputConstraintsMap.get(forEach);
         outputConstraintsMap.remove(forEach);
         List<LogicalPlan> plans = forEach.getForEachPlans();
@@ -388,8 +455,8 @@ public class AugmentBaseDataVisitor extends LOVisitor {
         outputConstraintsMap.remove(load);
         // check if the inputData exists
         if (inputData == null || inputData.size() == 0) {
-            log.error("No input data found!");
-            throw new RuntimeException("No input data found!");
+            log.error("No (valid) input data found!");
+            throw new RuntimeException("No (valid) input data found!");
         }
 
         // first of all, we are required to guarantee that there is at least one
@@ -403,11 +470,13 @@ public class AugmentBaseDataVisitor extends LOVisitor {
         // create example tuple to steal values from when we encounter
         // "don't care" fields (i.e. null fields)
         Tuple exampleTuple = inputData.iterator().next();
+        System.out.println(exampleTuple.toString());
 
         // run through output constraints; for each one synthesize a tuple and
         // add it to the base data
         // (while synthesizing individual fields, try to match fields that exist
         // in the real data)
+        boolean newInput = false;
         for (Iterator<Tuple> it = outputConstraints.iterator(); it.hasNext();) {
             Tuple outputConstraint = it.next();
 
@@ -423,10 +492,15 @@ public class AugmentBaseDataVisitor extends LOVisitor {
             try {
                 for (int i = 0; i < inputTuple.size(); i++) {
                     Object d = outputConstraint.get(i);
-                    if (d == null)
+                    if (d == null && i < exampleTuple.size())
                         d = exampleTuple.get(i);
                     inputTuple.set(i, d);
                 }
+                if (outputConstraint instanceof ExampleTuple)
+                    inputTuple.synthetic = ((ExampleTuple) outputConstraint).synthetic;
+                else
+                    // raw tuple should have been synthesized
+                    inputTuple.synthetic = true;
             } catch (ExecException e) {
                 log
                         .error("Error visiting Load during Augmentation phase of Example Generator! "
@@ -436,15 +510,55 @@ public class AugmentBaseDataVisitor extends LOVisitor {
                                 + e.getMessage());
 
             }
-            if (!inputTuple.equals(exampleTuple))
-                inputTuple.synthetic = true;
-
-            newInputData.add(inputTuple);
+            try {
+                if (inputTuple.synthetic || !inInput(inputTuple, inputData, schema))
+                {
+                    inputTuple.synthetic = true;
+
+                    newInputData.add(inputTuple);
+                    
+                    if (!newInput)
+                        newInput = true;
+                }
+            } catch (ExecException e) {
+                throw new VisitorException(
+                  "Error visiting Load during Augmentation phase of Example Generator! "
+                          + e.getMessage());
+            }
+        }
+        
+        if (newInput) {
+            for (Map.Entry<LOLoad, DataBag> entry : newBaseData.entrySet()) {
+                LOLoad otherLoad = entry.getKey();
+                if (otherLoad != load && otherLoad.getInputFile().equals(load.getInputFile())) {
+                    // different load sharing the same input file
+                    entry.getValue().addAll(newInputData);
+                }
+            }
         }
     }
 
+    private boolean inInput(Tuple newTuple, DataBag input, Schema schema) throws ExecException {
+        boolean result;
+        for (Iterator<Tuple> iter = input.iterator(); iter.hasNext();) {
+            result = true;
+            Tuple tmp = iter.next();
+            for (int i = 0; i < schema.size(); ++i)
+                if (!newTuple.get(i).equals(tmp.get(i)))
+                {
+                    result = false;
+                    break;
+                }
+            if (result)
+                return true;
+        }
+        return false;
+    }
+    
     @Override
     protected void visit(LOSort s) throws VisitorException {
+        if (limit && !((PreOrderDepthFirstWalker<LogicalOperator, LogicalPlan>) mCurrentWalker).getBranchFlag())
+            return;
         DataBag outputConstraints = outputConstraintsMap.get(s);
         outputConstraintsMap.remove(s);
 
@@ -457,11 +571,14 @@ public class AugmentBaseDataVisitor extends LOVisitor {
 
     @Override
     protected void visit(LOSplit split) throws VisitorException {
-
+        if (limit && !((PreOrderDepthFirstWalker<LogicalOperator, LogicalPlan>) mCurrentWalker).getBranchFlag())
+          return;
     }
 
     @Override
     protected void visit(LOStore store) throws VisitorException {
+        if (limit && !((PreOrderDepthFirstWalker<LogicalOperator, LogicalPlan>) mCurrentWalker).getBranchFlag())
+            return;
         DataBag outputConstraints = outputConstraintsMap.get(store);
         if (outputConstraints == null) {
             outputConstraintsMap.put(store.getPlan().getPredecessors(store)
@@ -475,6 +592,8 @@ public class AugmentBaseDataVisitor extends LOVisitor {
 
     @Override
     protected void visit(LOUnion u) throws VisitorException {
+        if (limit && !((PreOrderDepthFirstWalker<LogicalOperator, LogicalPlan>) mCurrentWalker).getBranchFlag())
+            return;
         DataBag outputConstraints = outputConstraintsMap.get(u);
         outputConstraintsMap.remove(u);
         if (outputConstraints == null || outputConstraints.size() == 0) {
@@ -506,6 +625,61 @@ public class AugmentBaseDataVisitor extends LOVisitor {
 
     }
 
+    @Override
+    protected void visit(LOLimit lm) throws VisitorException {
+        if (!limit) // not augment for LIMIT in this traversal
+            return;
+        
+        if (oriLimitMap == null)
+            oriLimitMap = new HashMap<LOLimit, Long>();
+        
+        DataBag outputConstraints = outputConstraintsMap.get(lm);
+        outputConstraintsMap.remove(lm);
+
+        DataBag inputConstraints = outputConstraintsMap.get(lm.getInput());
+        if (inputConstraints == null) {
+            inputConstraints = BagFactory.getInstance().newDefaultBag();
+            outputConstraintsMap.put(lm.getInput(), inputConstraints);
+        }
+
+        DataBag inputData = derivedData.get(lm.getInput());
+        
+        if (outputConstraints != null && outputConstraints.size() > 0) { // there
+            // 's
+            // one
+            // or
+            // more
+            // output
+            // constraints
+            // ;
+            // generate
+            // corresponding
+            // input
+            // constraints
+            for (Iterator<Tuple> it = outputConstraints.iterator(); it
+                  .hasNext();) {
+                inputConstraints.add(it.next());
+             // ... plus one more if only one
+             if (inputConstraints.size() == 1) {
+                inputConstraints.add(inputData.iterator().next());
+                ((PreOrderDepthFirstWalker<LogicalOperator, LogicalPlan>) mCurrentWalker).setBranchFlag();
+             }
+          }
+        } else if (inputConstraints.size() == 0){
+            // add all input to input constraints ...
+            inputConstraints.addAll(inputData);
+            // ... plus one more if only one
+            if (inputConstraints.size() == 1) {
+                inputConstraints.add(inputData.iterator().next());
+                ((PreOrderDepthFirstWalker<LogicalOperator, LogicalPlan>) mCurrentWalker).setBranchFlag();
+            }
+        }
+        POLimit poLimit = (POLimit) logToPhysMap.get(lm);
+        oriLimitMap.put(lm, Long.valueOf(poLimit.getLimit()));
+        poLimit.setLimit(inputConstraints.size()-1);
+        lm.setLimit(poLimit.getLimit());
+    }
+    
     Tuple GetGroupByInput(Object groupLabel, List<Integer> groupCols,
             int numFields) throws ExecException {
         Tuple t = TupleFactory.getInstance().newTuple(numFields);
diff --git a/src/org/apache/pig/pen/DerivedDataVisitor.java b/src/org/apache/pig/pen/DerivedDataVisitor.java
index c2811102c..69cd8bea6 100644
--- a/src/org/apache/pig/pen/DerivedDataVisitor.java
+++ b/src/org/apache/pig/pen/DerivedDataVisitor.java
@@ -67,7 +67,7 @@ import org.apache.pig.pen.util.LineageTracer;
 
 
 //This class is used to pass data through the entire plan and save the intermediates results.
-public class DerivedDataVisitor extends LOVisitor {
+public class DerivedDataVisitor {
 
     Map<LogicalOperator, DataBag> derivedData = new HashMap<LogicalOperator, DataBag>();
     PhysicalPlan physPlan = null;
@@ -83,16 +83,13 @@ public class DerivedDataVisitor extends LOVisitor {
 
     public DerivedDataVisitor(LogicalPlan plan, PigContext pc,
             Map<LOLoad, DataBag> baseData,
-            Map<LogicalOperator, PhysicalOperator> logToPhyMap,
             PhysicalPlan physPlan) {
-        super(plan, new DependencyOrderWalker<LogicalOperator, LogicalPlan>(
-                plan));
+
         this.baseData = baseData;
 
         OpToEqClasses = new HashMap<LogicalOperator, Collection<IdentityHashSet<Tuple>>>();
         EqClasses = new LinkedList<IdentityHashSet<Tuple>>();
 
-        LogToPhyMap = logToPhyMap;
         this.physPlan = physPlan;
         // if(logToPhyMap == null)
         // compilePlan(plan);
@@ -105,9 +102,6 @@ public class DerivedDataVisitor extends LOVisitor {
             Map<LOLoad, DataBag> baseData,
             Map<LogicalOperator, PhysicalOperator> logToPhyMap,
             PhysicalPlan physPlan) {
-        super(op.getPlan(),
-                new DependencyOrderLimitedWalker<LogicalOperator, LogicalPlan>(
-                        op, op.getPlan()));
         this.baseData = baseData;
 
         OpToEqClasses = new HashMap<LogicalOperator, Collection<IdentityHashSet<Tuple>>>();
@@ -115,364 +109,5 @@ public class DerivedDataVisitor extends LOVisitor {
 
         LogToPhyMap = logToPhyMap;
         this.physPlan = physPlan;
-        // if(logToPhyMap == null)
-        // compilePlan(op.getPlan());
-        // else
-        // LogToPhyMap = logToPhyMap;
-    }
-
-    public void setOperatorToEvaluate(LogicalOperator op) {
-        mCurrentWalker = new DependencyOrderLimitedWalker<LogicalOperator, LogicalPlan>(
-                op, op.getPlan());
-    }
-
-    @Override
-    protected void visit(LOCogroup cg) throws VisitorException {
-        // evaluateOperator(cg);
-        // there is a slightly different code path for cogroup because of the
-        // local rearranges
-        PhysicalOperator physOp = LogToPhyMap.get(cg);
-        Random r = new Random();
-        // get the list of original inputs
-
-        // List<PhysicalOperator> inputs = physOp.getInputs();
-        List<PhysicalOperator> inputs = new ArrayList<PhysicalOperator>();
-        PhysicalPlan phy = new PhysicalPlan();
-        phy.add(physOp);
-
-        // for(PhysicalOperator input : physOp.getInputs()) {
-        for (PhysicalOperator input : physPlan.getPredecessors(physOp)) {
-            inputs.add(input.getInputs().get(0));
-            // input.setInputs(null);
-            phy.add(input);
-            try {
-                phy.connect(input, physOp);
-            } catch (PlanException e) {
-                // TODO Auto-generated catch block
-                e.printStackTrace();
-                log.error("Error connecting " + input.name() + " to "
-                        + physOp.name());
-            }
-        }
-
-        physOp.setLineageTracer(lineage);
-
-        // replace the original inputs by POReads
-        for (int i = 0; i < inputs.size(); i++) {
-            DataBag bag = derivedData.get(cg.getInputs().get(i));
-            PORead por = new PORead(new OperatorKey("", r.nextLong()), bag);
-            phy.add(por);
-            try {
-                phy.connect(por, physOp.getInputs().get(i));
-            } catch (PlanException e) {
-                // TODO Auto-generated catch block
-                e.printStackTrace();
-                log.error("Error connecting " + por.name() + " to "
-                        + physOp.name());
-            }
-        }
-
-        DataBag output = BagFactory.getInstance().newDefaultBag();
-        Tuple t = null;
-        try {
-            for (Result res = physOp.getNext(t); res.returnStatus != POStatus.STATUS_EOP; res = physOp
-                    .getNext(t)) {
-                output.add((Tuple) res.result);
-            }
-        } catch (ExecException e) {
-            log.error("Error evaluating operator : " + physOp.name());
-        }
-        derivedData.put(cg, output);
-
-        try {
-            Collection<IdentityHashSet<Tuple>> eq = EquivalenceClasses
-                    .getEquivalenceClasses(cg, derivedData);
-            EqClasses.addAll(eq);
-            OpToEqClasses.put(cg, eq);
-        } catch (ExecException e) {
-            // TODO Auto-generated catch block
-            e.printStackTrace();
-            log
-                    .error("Error updating equivalence classes while evaluating operators. \n"
-                            + e.getMessage());
-        }
-
-        // re-attach the original operators
-        // for(int i = 0; i < inputs.size(); i++) {
-        // try {
-        // physPlan.connect(inputs.get(i), physOp.getInputs().get(i));
-        //		
-        // } catch (PlanException e) {
-        // // TODO Auto-generated catch block
-        // e.printStackTrace();
-        // log.error("Error connecting " + inputs.get(i).name() + " to " +
-        // physOp.getInputs().get(i).name());
-        // }
-        // }
-        physOp.setLineageTracer(null);
     }
-
-    @Override
-    protected void visit(LOCross cs) throws VisitorException {
-        evaluateOperator(cs);
-    }
-
-    @Override
-    protected void visit(LODistinct dt) throws VisitorException {
-        evaluateOperator(dt);
-    }
-
-    @Override
-    protected void visit(LOFilter filter) throws VisitorException {
-        evaluateOperator(filter);
-    }
-
-    @Override
-    protected void visit(LOForEach forEach) throws VisitorException {
-        evaluateOperator(forEach);
-    }
-
-    @Override
-    protected void visit(LOLoad load) throws VisitorException {
-        derivedData.put(load, baseData.get(load));
-
-        Collection<IdentityHashSet<Tuple>> eq = EquivalenceClasses
-                .getEquivalenceClasses(load, derivedData);
-        EqClasses.addAll(eq);
-        OpToEqClasses.put(load, eq);
-
-        for (Iterator<Tuple> it = derivedData.get(load).iterator(); it
-                .hasNext();) {
-            lineage.insert(it.next());
-        }
-
-    }
-
-    @Override
-    protected void visit(LOSplit split) throws VisitorException {
-        evaluateOperator(split);
-    }
-
-    @Override
-    protected void visit(LOStore store) throws VisitorException {
-        derivedData.put(store, derivedData.get(store.getPlan().getPredecessors(
-                store).get(0)));
-    }
-
-    @Override
-    protected void visit(LOUnion u) throws VisitorException {
-        evaluateOperator(u);
-    }
-
-    @Override
-    protected void visit(LOLimit l) throws VisitorException {
-        evaluateOperator(l);
-    }
-    
-    @Override
-    protected void visit(LOSort sort) throws VisitorException {
-        evaluateOperator(sort);
-    }
-
-    // private void compilePlan(LogicalPlan plan) {
-    //	
-    // plan = refineLogicalPlan(plan);
-    //	
-    // LocalLogToPhyTranslationVisitor visitor = new
-    // LocalLogToPhyTranslationVisitor(plan);
-    // visitor.setPigContext(pc);
-    // try {
-    // visitor.visit();
-    // } catch (VisitorException e) {
-    // // TODO Auto-generated catch block
-    // e.printStackTrace();
-    // log.error("Error visiting the logical plan in ExampleGenerator");
-    // }
-    // physPlan = visitor.getPhysicalPlan();
-    // LogToPhyMap = visitor.getLogToPhyMap();
-    // }
-    //    
-    // private LogicalPlan refineLogicalPlan(LogicalPlan plan) {
-    // PlanSetter ps = new PlanSetter(plan);
-    // try {
-    // ps.visit();
-    //	    
-    // } catch (VisitorException e) {
-    // // TODO Auto-generated catch block
-    // e.printStackTrace();
-    // }
-    //        
-    // // run through validator
-    // CompilationMessageCollector collector = new CompilationMessageCollector()
-    // ;
-    // FrontendException caught = null;
-    // try {
-    // LogicalPlanValidationExecutor validator =
-    // new LogicalPlanValidationExecutor(plan, pc);
-    // validator.validate(plan, collector);
-    // } catch (FrontendException fe) {
-    // // Need to go through and see what the collector has in it. But
-    // // remember what we've caught so we can wrap it into what we
-    // // throw.
-    // caught = fe;
-    // }
-    //        
-    //        
-    // return plan;
-    //
-    // }
-
-    private void evaluateOperator(LogicalOperator op) {
-        PhysicalOperator physOp = LogToPhyMap.get(op);
-        Random r = new Random();
-        // get the list of original inputs
-
-        List<PhysicalOperator> inputs = physOp.getInputs();
-        physOp.setInputs(null);
-        physOp.setLineageTracer(lineage);
-        PhysicalPlan phy = new PhysicalPlan();
-        phy.add(physOp);
-
-        // replace the original inputs by POReads
-        for (LogicalOperator l : op.getPlan().getPredecessors(op)) {
-            DataBag bag = derivedData.get(l);
-            PORead por = new PORead(new OperatorKey("", r.nextLong()), bag);
-            phy.add(por);
-            try {
-                phy.connect(por, physOp);
-            } catch (PlanException e) {
-                // TODO Auto-generated catch block
-                e.printStackTrace();
-                log.error("Error connecting " + por.name() + " to "
-                        + physOp.name());
-            }
-        }
-
-        DataBag output = BagFactory.getInstance().newDefaultBag();
-        Tuple t = null;
-        try {
-            for (Result res = physOp.getNext(t); res.returnStatus != POStatus.STATUS_EOP; res = physOp
-                    .getNext(t)) {
-                output.add((Tuple) res.result);
-            }
-        } catch (ExecException e) {
-            log.error("Error evaluating operator : " + physOp.name());
-        }
-        derivedData.put(op, output);
-
-        try {
-            Collection<IdentityHashSet<Tuple>> eq = EquivalenceClasses
-                    .getEquivalenceClasses(op, derivedData);
-            EqClasses.addAll(eq);
-            OpToEqClasses.put(op, eq);
-        } catch (ExecException e) {
-            // TODO Auto-generated catch block
-            e.printStackTrace();
-            log
-                    .error("Error updating equivalence classes while evaluating operators. \n"
-                            + e.getMessage());
-        }
-
-        // re-attach the original operators
-        physOp.setInputs(inputs);
-        physOp.setLineageTracer(null);
-    }
-
-    public DataBag evaluateIsolatedOperator(LOCogroup op,
-            List<DataBag> inputBags) {
-        if (op.getPlan().getPredecessors(op).size() > inputBags.size())
-            return null;
-
-        int count = 0;
-        for (LogicalOperator inputs : op.getPlan().getPredecessors(op)) {
-            derivedData.put(inputs, inputBags.get(count++));
-        }
-
-        return evaluateIsolatedOperator(op);
-
-    }
-
-    public DataBag evaluateIsolatedOperator(LOCogroup op) {
-        // return null if the inputs are not already evaluated
-        for (LogicalOperator in : op.getPlan().getPredecessors(op)) {
-            if (derivedData.get(in) == null)
-                return null;
-        }
-
-        LineageTracer oldLineage = this.lineage;
-        this.lineage = new LineageTracer();
-
-        PhysicalOperator physOp = LogToPhyMap.get(op);
-        Random r = new Random();
-        // get the list of original inputs
-        // List<PhysicalOperator> inputs = physOp.getInputs();
-        List<PhysicalOperator> inputs = new ArrayList<PhysicalOperator>();
-        PhysicalPlan phy = new PhysicalPlan();
-        phy.add(physOp);
-
-        for (PhysicalOperator input : physOp.getInputs()) {
-            inputs.add(input.getInputs().get(0));
-            input.setInputs(null);
-            phy.add(input);
-            try {
-                phy.connect(input, physOp);
-            } catch (PlanException e) {
-                // TODO Auto-generated catch block
-                e.printStackTrace();
-                log.error("Error connecting " + input.name() + " to "
-                        + physOp.name());
-            }
-        }
-        physOp.setLineageTracer(lineage);
-
-        physOp.setLineageTracer(null);
-
-        // replace the original inputs by POReads
-        for (int i = 0; i < inputs.size(); i++) {
-            DataBag bag = derivedData.get(op.getInputs().get(i));
-            PORead por = new PORead(new OperatorKey("", r.nextLong()), bag);
-            phy.add(por);
-            try {
-                phy.connect(por, physOp.getInputs().get(i));
-            } catch (PlanException e) {
-                // TODO Auto-generated catch block
-                e.printStackTrace();
-                log.error("Error connecting " + por.name() + " to "
-                        + physOp.name());
-            }
-        }
-
-        // replace the original inputs by POReads
-        // for(LogicalOperator l : op.getPlan().getPredecessors(op)) {
-        // DataBag bag = derivedData.get(l);
-        // PORead por = new PORead(new OperatorKey("", r.nextLong()), bag);
-        // phy.add(por);
-        // try {
-        // phy.connect(por, physOp);
-        // } catch (PlanException e) {
-        // // TODO Auto-generated catch block
-        // e.printStackTrace();
-        // log.error("Error connecting " + por.name() + " to " + physOp.name());
-        // }
-        // }
-
-        DataBag output = BagFactory.getInstance().newDefaultBag();
-        Tuple t = null;
-        try {
-            for (Result res = physOp.getNext(t); res.returnStatus != POStatus.STATUS_EOP; res = physOp
-                    .getNext(t)) {
-                output.add((Tuple) res.result);
-            }
-        } catch (ExecException e) {
-            log.error("Error evaluating operator : " + physOp.name());
-        }
-
-        this.lineage = oldLineage;
-
-        physOp.setInputs(inputs);
-        physOp.setLineageTracer(null);
-
-        return output;
-    }
-
 }
diff --git a/src/org/apache/pig/pen/EquivalenceClasses.java b/src/org/apache/pig/pen/EquivalenceClasses.java
index 186851855..a43aed0b9 100644
--- a/src/org/apache/pig/pen/EquivalenceClasses.java
+++ b/src/org/apache/pig/pen/EquivalenceClasses.java
@@ -19,174 +19,105 @@
 package org.apache.pig.pen;
 
 import java.util.Collection;
-import java.util.Iterator;
 import java.util.LinkedList;
 import java.util.Map;
+import java.util.HashMap;
+import java.util.List;
+import java.util.HashSet;
+import java.util.Iterator;
 
-import org.apache.pig.backend.executionengine.ExecException;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
 import org.apache.pig.data.DataBag;
 import org.apache.pig.data.Tuple;
-import org.apache.pig.impl.logicalLayer.LOCogroup;
-import org.apache.pig.impl.logicalLayer.LOFilter;
 import org.apache.pig.impl.logicalLayer.LOForEach;
-import org.apache.pig.impl.logicalLayer.LOLoad;
-import org.apache.pig.impl.logicalLayer.LOSort;
-import org.apache.pig.impl.logicalLayer.LOSplit;
-import org.apache.pig.impl.logicalLayer.LOUnion;
+import org.apache.pig.impl.logicalLayer.LOCross;
 import org.apache.pig.impl.logicalLayer.LogicalOperator;
+import org.apache.pig.impl.logicalLayer.LogicalPlan;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
 import org.apache.pig.impl.util.IdentityHashSet;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.*;
+import org.apache.pig.impl.plan.VisitorException;
 
 
 //These methods are used to generate equivalence classes given the operator name and the output from the operator
 //For example, it gives out 2 eq. classes for filter, one that passes the filter and one that doesn't
 public class EquivalenceClasses {
-    public static Collection<IdentityHashSet<Tuple>> getEquivalenceClasses(
-            LogicalOperator op, Map<LogicalOperator, DataBag> derivedData)
-            throws ExecException {
-        if (op instanceof LOCogroup)
-            return getEquivalenceClasses((LOCogroup) op, derivedData);
-        else if (op instanceof LOForEach)
-            return getEquivalenceClasses((LOForEach) op, derivedData);
-        else if (op instanceof LOFilter)
-            return getEquivalenceClasses((LOFilter) op, derivedData);
-        else if (op instanceof LOSort)
-            return getEquivalenceClasses((LOSort) op, derivedData);
-        else if (op instanceof LOSplit)
-            return getEquivalenceClasses((LOSplit) op, derivedData);
-        else if (op instanceof LOUnion)
-            return getEquivalenceClasses((LOUnion) op, derivedData);
-        else if (op instanceof LOLoad)
-            return getEquivalenceClasses((LOLoad) op, derivedData);
-            throw new RuntimeException("Unrecognized logical operator.");
-    }
-
-    static Collection<IdentityHashSet<Tuple>> getEquivalenceClasses(LOLoad op,
-            Map<LogicalOperator, DataBag> derivedData) {
-        // Since its a load, all the tuples belong to a single equivalence class
-        Collection<IdentityHashSet<Tuple>> equivClasses = new LinkedList<IdentityHashSet<Tuple>>();
-        IdentityHashSet<Tuple> input = new IdentityHashSet<Tuple>();
-
-        equivClasses.add(input);
-
-        DataBag output = derivedData.get(op);
-
-        for (Iterator<Tuple> it = output.iterator(); it.hasNext();) {
-            Tuple t = it.next();
-
-            input.add(t);
+    
+    public static Map<LogicalOperator, Collection<IdentityHashSet<Tuple>>> getLoToEqClassMap(PhysicalPlan plan,
+        LogicalPlan lp, Map<LogicalOperator, PhysicalOperator> logToPhyMap,
+        Map<LogicalOperator, DataBag> logToDataMap,
+        Map<LOForEach, Map<LogicalOperator, PhysicalOperator>> forEachInnerLogToPhyMap,
+        final HashMap<PhysicalOperator, Collection<IdentityHashSet<Tuple>>> poToEqclassesMap)
+        throws VisitorException {
+        Map<LogicalOperator, Collection<IdentityHashSet<Tuple>>> ret =
+          new HashMap<LogicalOperator, Collection<IdentityHashSet<Tuple>>>();
+        List<LogicalOperator> roots = lp.getRoots();
+        HashSet<LogicalOperator> seen = new HashSet<LogicalOperator>();
+        for(LogicalOperator lo: roots) {
+            getEqClasses(plan, lo, lp, logToPhyMap, ret, poToEqclassesMap, logToDataMap, forEachInnerLogToPhyMap, seen);
         }
-
-        return equivClasses;
+        return ret;
     }
-
-    static Collection<IdentityHashSet<Tuple>> getEquivalenceClasses(
-            LOCogroup op, Map<LogicalOperator, DataBag> derivedData)
-            throws ExecException {
-        Collection<IdentityHashSet<Tuple>> equivClasses = new LinkedList<IdentityHashSet<Tuple>>();
-        IdentityHashSet<Tuple> acceptableGroups = new IdentityHashSet<Tuple>();
-
-        equivClasses.add(acceptableGroups);
-
-        for (Iterator<Tuple> it = derivedData.get(op).iterator(); it.hasNext();) {
-            Tuple t = it.next();
-
-            boolean isAcceptable;
-
-            if (t.size() == 2) {
-                isAcceptable = (((DataBag) (t.get(1))).size() >= 2);
+    
+    private static void getEqClasses(PhysicalPlan plan, LogicalOperator parent, LogicalPlan lp,
+        Map<LogicalOperator, PhysicalOperator> logToPhyMap, Map<LogicalOperator,
+        Collection<IdentityHashSet<Tuple>>> result,
+        final HashMap<PhysicalOperator, Collection<IdentityHashSet<Tuple>>> poToEqclassesMap,
+        Map<LogicalOperator, DataBag> logToDataMap,
+        Map<LOForEach, Map<LogicalOperator, PhysicalOperator>> forEachInnerLogToPhyMap,
+        HashSet<LogicalOperator> seen) throws VisitorException {
+        if (parent instanceof LOForEach) {
+            if (poToEqclassesMap.get(logToPhyMap.get(parent)) != null) {
+                LinkedList<IdentityHashSet<Tuple>> eqClasses = new LinkedList<IdentityHashSet<Tuple>>();
+                eqClasses.addAll(poToEqclassesMap.get(logToPhyMap.get(parent)));
+                for (Map.Entry<LogicalOperator, PhysicalOperator> entry : forEachInnerLogToPhyMap.get(parent).entrySet()) {
+                    if (poToEqclassesMap.get(entry.getValue()) != null)
+                        eqClasses.addAll(poToEqclassesMap.get(entry.getValue()));
+                }
+                result.put(parent, eqClasses);
+            }
+        } else if (parent instanceof LOCross) {
+            boolean ok = true; 
+            for (LogicalOperator input : ((LOCross) parent).getInputs()) {
+                if (logToDataMap.get(input).size() < 2) {
+                    // only if all inputs have at least more than two tuples will all outputs be added to the eq. class
+                    ok = false;
+                    break;
+                }
+            }
+            if (ok) {
+                LinkedList<IdentityHashSet<Tuple>> eqClasses = new LinkedList<IdentityHashSet<Tuple>>();
+                IdentityHashSet<Tuple> eqClass = new IdentityHashSet<Tuple>();
+                for (Iterator<Tuple> it = logToDataMap.get(parent).iterator(); it.hasNext();) {
+                    eqClass.add(it.next());
+                }
+                eqClasses.add(eqClass);
+                result.put(parent, eqClasses);
             } else {
-                isAcceptable = true;
-                for (int field = 1; field < t.size(); field++) {
-                    DataBag bag = (DataBag) t.get(field);
-                    if (bag.size() == 0) {
-                        isAcceptable = false;
-                        break;
-                    }
+                LinkedList<IdentityHashSet<Tuple>> eqClasses = new LinkedList<IdentityHashSet<Tuple>>();
+                IdentityHashSet<Tuple> eqClass = new IdentityHashSet<Tuple>();
+                eqClasses.add(eqClass);
+                result.put(parent, eqClasses);
+            }
+        } else {
+            Collection<IdentityHashSet<Tuple>> eqClasses = poToEqclassesMap.get(logToPhyMap.get(parent));
+            if (eqClasses == null) {
+                eqClasses = new LinkedList<IdentityHashSet<Tuple>>();
+                int size = ((POPackage)logToPhyMap.get(parent)).getNumInps();
+                for (int i = 0; i < size; i++) {
+                    eqClasses.add(new IdentityHashSet<Tuple>());
                 }
             }
-
-            if (isAcceptable)
-                acceptableGroups.add(t);
-
+            result.put(parent, eqClasses);
         }
-        return equivClasses;
-    }
-
-    static Collection<IdentityHashSet<Tuple>> getEquivalenceClasses(
-            LOForEach op, Map<LogicalOperator, DataBag> derivedData) {
-        Collection<IdentityHashSet<Tuple>> equivClasses = new LinkedList<IdentityHashSet<Tuple>>();
-
-        IdentityHashSet<Tuple> equivClass = new IdentityHashSet<Tuple>();
-        equivClasses.add(equivClass);
-
-        for (Iterator<Tuple> it = derivedData.get(op).iterator(); it.hasNext();) {
-            equivClass.add(it.next());
-        }
-
-        return equivClasses;
-    }
-
-    static Collection<IdentityHashSet<Tuple>> getEquivalenceClasses(
-            LOFilter op, Map<LogicalOperator, DataBag> derivedData) {
-        Collection<IdentityHashSet<Tuple>> equivClasses = new LinkedList<IdentityHashSet<Tuple>>();
-
-        IdentityHashSet<Tuple> pass = new IdentityHashSet<Tuple>();
-        IdentityHashSet<Tuple> fail = new IdentityHashSet<Tuple>();
-
-        for (Iterator<Tuple> it = derivedData.get(op).iterator(); it.hasNext();) {
-            pass.add(it.next());
-        }
-
-        LogicalOperator input = op.getInput();
-
-        for (Iterator<Tuple> it = derivedData.get(input).iterator(); it
-                .hasNext();) {
-            Tuple t = it.next();
-            if (pass.contains(t))
-                continue;
-            else
-                fail.add(t);
-        }
-
-        equivClasses.add(pass);
-        equivClasses.add(fail);
-
-        return equivClasses;
-
-    }
-
-    static Collection<IdentityHashSet<Tuple>> getEquivalenceClasses(LOSort op,
-            Map<LogicalOperator, DataBag> derivedData) {
-        //We don't create any eq. class for sort
-        IdentityHashSet<Tuple> temp = new IdentityHashSet<Tuple>();
-        Collection<IdentityHashSet<Tuple>> output = new LinkedList<IdentityHashSet<Tuple>>();
-        output.add(temp);
-        return output;
-    }
-
-    static Collection<IdentityHashSet<Tuple>> getEquivalenceClasses(LOSplit op,
-            Map<LogicalOperator, DataBag> derivedData) {
-        throw new RuntimeException(
-                "LOSplit not supported yet in example generator.");
-    }
-
-    static Collection<IdentityHashSet<Tuple>> getEquivalenceClasses(LOUnion op,
-            Map<LogicalOperator, DataBag> derivedData) {
-
-        // make one equivalence class per input relation
-
-        Collection<IdentityHashSet<Tuple>> equivClasses = new LinkedList<IdentityHashSet<Tuple>>();
-
-        for (LogicalOperator input : op.getInputs()) {
-            IdentityHashSet<Tuple> equivClass = new IdentityHashSet<Tuple>();
-
-            for (Iterator<Tuple> it = derivedData.get(input).iterator(); it
-                    .hasNext();) {
-                equivClass.add(it.next());
+        // result.put(parent, getEquivalenceClasses(plan, parent, lp, logToPhyMap, poToEqclassesMap));
+        if (lp.getSuccessors(parent) != null) {
+            for (LogicalOperator lo : lp.getSuccessors(parent)) {
+                if (!seen.contains(lo)) {
+                    seen.add(lo);
+                    getEqClasses(plan, lo, lp, logToPhyMap, result, poToEqclassesMap, logToDataMap, forEachInnerLogToPhyMap, seen);
+                }
             }
-            equivClasses.add(equivClass);
         }
-
-        return equivClasses;
     }
 }
diff --git a/src/org/apache/pig/pen/ExampleGenerator.java b/src/org/apache/pig/pen/ExampleGenerator.java
index 3ec3a5367..04c045669 100644
--- a/src/org/apache/pig/pen/ExampleGenerator.java
+++ b/src/org/apache/pig/pen/ExampleGenerator.java
@@ -19,54 +19,77 @@
 package org.apache.pig.pen;
 
 import java.util.HashMap;
+import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
+import java.util.Collection;
+import java.util.Iterator;
+import java.io.IOException;
+import org.apache.pig.impl.util.IdentityHashSet;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
-import org.apache.pig.ExecType;
 import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad;
-import org.apache.pig.data.BagFactory;
+import org.apache.pig.backend.hadoop.executionengine.HExecutionEngine;
+
 import org.apache.pig.data.DataBag;
 import org.apache.pig.data.Tuple;
 import org.apache.pig.impl.PigContext;
+import org.apache.pig.PigException;
+import org.apache.pig.impl.io.FileLocalizer;
 import org.apache.pig.impl.logicalLayer.FrontendException;
+import org.apache.pig.impl.logicalLayer.LOForEach;
 import org.apache.pig.impl.logicalLayer.LOLoad;
 import org.apache.pig.impl.logicalLayer.LogicalOperator;
 import org.apache.pig.impl.logicalLayer.LogicalPlan;
-import org.apache.pig.impl.logicalLayer.PlanSetter;
-import org.apache.pig.impl.logicalLayer.optimizer.LogicalOptimizer;
 import org.apache.pig.impl.logicalLayer.schema.Schema;
-import org.apache.pig.impl.logicalLayer.validators.LogicalPlanValidationExecutor;
-import org.apache.pig.impl.plan.CompilationMessageCollector;
+import org.apache.pig.impl.logicalLayer.LOSort;
+import org.apache.pig.impl.logicalLayer.LOLimit;
 import org.apache.pig.impl.plan.VisitorException;
 import org.apache.pig.pen.util.DisplayExamples;
-import org.apache.pig.pen.util.FunctionalLogicalOptimizer;
 import org.apache.pig.pen.util.LineageTracer;
 
+/**
+ *   This class is used to generate example tuples for the ILLUSTRATE purpose 
+ * 
+ *
+ */
 public class ExampleGenerator {
 
     LogicalPlan plan;
-    Map<LOLoad, DataBag> baseData;
+    Map<LOLoad, DataBag> baseData = null;
     PigContext pigContext;
 
-    Map<LogicalOperator, PhysicalOperator> LogToPhyMap;
     PhysicalPlan physPlan;
+    PhysicalPlanResetter physPlanReseter;
+    private HExecutionEngine execEngine;
+    private LocalMapReduceSimulator localMRRunner;
 
     Log log = LogFactory.getLog(getClass());
 
     private int MAX_RECORDS = 10000;
+    
+    private Map<LogicalOperator, PhysicalOperator> logToPhyMap;
+    private Map<PhysicalOperator, LogicalOperator> poLoadToLogMap;
+    private Map<PhysicalOperator, LogicalOperator> poToLogMap;
+    private HashMap<PhysicalOperator, Collection<IdentityHashSet<Tuple>>> poToEqclassesMap;
+    private LineageTracer lineage;
+    private Map<LogicalOperator, DataBag> logToDataMap = null;
+    private Map<LOForEach, Map<LogicalOperator, DataBag>> forEachInnerLogToDataMap;
+    Map<LOForEach, Map<LogicalOperator, PhysicalOperator>> forEachInnerLogToPhyMap;
+    Map<LOLimit, Long> oriLimitMap = null;
+    Map<POLoad, Schema> poLoadToSchemaMap;
 
     public ExampleGenerator(LogicalPlan plan, PigContext hadoopPigContext) {
         this.plan = plan;
 //        pigContext = new PigContext(ExecType.LOCAL, hadoopPigContext
 //                .getProperties());
         pigContext = hadoopPigContext;
+        // pigContext.setExecType(ExecType.LOCAL);
+        FileLocalizer.setInitialized(false);
         try {
             pigContext.connect();
         } catch (ExecException e) {
@@ -74,167 +97,283 @@ public class ExampleGenerator {
                     + e.getLocalizedMessage());
 
         }
-
+        execEngine = new HExecutionEngine(pigContext);
+        localMRRunner = new LocalMapReduceSimulator();
+        poLoadToSchemaMap = new HashMap<POLoad, Schema>();
     }
 
+    public LineageTracer getLineage() {
+      return lineage;
+    }
+    
+    public Map<LogicalOperator, PhysicalOperator> getLogToPhyMap() {
+        return logToPhyMap;
+    }
+    
     public void setMaxRecords(int max) {
         MAX_RECORDS = max;
     }
 
-    public Map<LogicalOperator, DataBag> getExamples() {
-
-        compilePlan(plan);
-
+    public Map<LogicalOperator, DataBag> getExamples() throws IOException, InterruptedException {
+        if (pigContext.getProperties().getProperty("pig.usenewlogicalplan", "true").equals("false"))
+            throw new ExecException("ILLUSTRATE must use the new logical plan!");
+        pigContext.inIllustrator = true;
+        physPlan = compilePlan(plan);
+        physPlanReseter = new PhysicalPlanResetter(physPlan);
         List<LogicalOperator> loads = plan.getRoots();
+        List<PhysicalOperator> pRoots = physPlan.getRoots();
+        if (loads.size() != pRoots.size())
+            throw new ExecException("Logical and Physical plans have different number of roots");
+        logToPhyMap = execEngine.getLogToPhyMap();
+        forEachInnerLogToPhyMap = execEngine.getForEachInnerLogToPhyMap();
+        poLoadToLogMap = new HashMap<PhysicalOperator, LogicalOperator>();
+        logToDataMap = new HashMap<LogicalOperator, DataBag>();
+        poToLogMap = new HashMap<PhysicalOperator, LogicalOperator>();
+        
+        // set up foreach inner data map
+        forEachInnerLogToDataMap = new HashMap<LOForEach, Map<LogicalOperator, DataBag>>();
+        for (Map.Entry<LOForEach, Map<LogicalOperator, PhysicalOperator>> entry : forEachInnerLogToPhyMap.entrySet()) {
+            Map<LogicalOperator, DataBag> innerMap = new HashMap<LogicalOperator, DataBag>();
+            forEachInnerLogToDataMap.put(entry.getKey(), innerMap);
+        }
+
+        for (LogicalOperator load : loads)
+        {
+            poLoadToLogMap.put(logToPhyMap.get(load), load);
+        }
 
+        boolean hasLimit = false;
+        for (LogicalOperator lo : logToPhyMap.keySet()) {
+            poToLogMap.put(logToPhyMap.get(lo), lo);
+            if (!hasLimit && lo instanceof LOLimit)
+                hasLimit = true;
+        }
+        
         try {
             readBaseData(loads);
         } catch (ExecException e) {
-            // TODO Auto-generated catch block
             log.error("Error reading data. " + e.getMessage());
-            throw new RuntimeException(e.getMessage());
+            throw e;
         } catch (FrontendException e) {
-            // TODO Auto-generated catch block
             log.error("Error reading data. " + e.getMessage());
+            throw new RuntimeException(e.getMessage());
         }
 
-        DerivedDataVisitor derivedData = null;
+        Map<LogicalOperator, DataBag> derivedData = null;
         try {
 
             // create derived data and trim base data
             LineageTrimmingVisitor trimmer = new LineageTrimmingVisitor(plan,
-                    baseData, LogToPhyMap, physPlan, pigContext);
+                    baseData, this, logToPhyMap, physPlan, pigContext);
             trimmer.visit();
             // System.out.println(
             // "Obtained the first level derived and trimmed data");
             // create new derived data from trimmed basedata
-            derivedData = new DerivedDataVisitor(plan, null, baseData,
-                    trimmer.LogToPhyMap, physPlan);
-            derivedData.visit();
+            derivedData = getData(physPlan);
 
             // System.out.println(
             // "Got new derived data from the trimmed base data");
             // augment base data
             AugmentBaseDataVisitor augment = new AugmentBaseDataVisitor(plan,
-                    baseData, derivedData.derivedData);
+                    logToPhyMap, baseData, derivedData);
             augment.visit();
             this.baseData = augment.getNewBaseData();
             // System.out.println("Obtained augmented base data");
             // create new derived data and trim the base data after augmenting
             // base data with synthetic tuples
-            trimmer = new LineageTrimmingVisitor(plan, baseData,
-                    derivedData.LogToPhyMap, physPlan, pigContext);
+            trimmer = new LineageTrimmingVisitor(plan, baseData, this,
+                    logToPhyMap, physPlan, pigContext);
             trimmer.visit();
             // System.out.println("Final trimming");
             // create the final version of derivedData to give to the output
-            derivedData = new DerivedDataVisitor(plan, null, baseData,
-                    trimmer.LogToPhyMap, physPlan);
-            derivedData.visit();
+            derivedData = getData(physPlan);
             // System.out.println("Obtaining final derived data for output");
+            
+            if (hasLimit)
+            {
+                augment.setLimit();
+                augment.visit();
+                this.baseData = augment.getNewBaseData();
+                oriLimitMap = augment.getOriLimitMap();
+                derivedData = getData();
+            }
 
         } catch (VisitorException e) {
-            // TODO Auto-generated catch block
+            e.printStackTrace(System.out);
             log.error("Visitor exception while creating example data "
                     + e.getMessage());
+            throw new RuntimeException(e.getMessage());
         }
 
         // DisplayExamples.printSimple(plan.getLeaves().get(0),
         // derivedData.derivedData);
         System.out.println(DisplayExamples.printTabular(plan,
-                derivedData.derivedData));
-        return derivedData.derivedData;
+                derivedData, forEachInnerLogToDataMap));
+        pigContext.inIllustrator = false;
+        return derivedData;
     }
 
-    private void readBaseData(List<LogicalOperator> loads) throws ExecException, FrontendException {
-        baseData = new HashMap<LOLoad, DataBag>();
+    private void readBaseData(List<LogicalOperator> loads) throws IOException, InterruptedException, FrontendException, ExecException {
+        PhysicalPlan thisPhyPlan = new PhysicalPlan();
         for (LogicalOperator op : loads) {
             Schema schema = op.getSchema();
             if(schema == null) {
                 throw new ExecException("Example Generator requires a schema. Please provide a schema while loading data.");
             }
-            
-            DataBag opBaseData = BagFactory.getInstance().newDefaultBag();
-
-            POLoad poLoad = (POLoad) LogToPhyMap.get(op);
-//            PigContext oldPC = poLoad.getPc();
-//            poLoad.setPc(pigContext);
-
-            poLoad.setLineageTracer(new LineageTracer());
-
-            Tuple t = null;
-            int count = 0;
-            for (Result res = poLoad.getNext(t); res.returnStatus != POStatus.STATUS_EOP
-                    && count < MAX_RECORDS; res = poLoad.getNext(t)) {
-                if (res.returnStatus == POStatus.STATUS_NULL)
-                    continue;
-                if (res.returnStatus == POStatus.STATUS_ERR) {
-                    log.error("Error reading Tuple");
-                } else {
-                    opBaseData.add((Tuple) res.result);
-                    count++;
-                }
-
+            poLoadToSchemaMap.put((POLoad)logToPhyMap.get(op), schema);
+            thisPhyPlan.add(logToPhyMap.get(op));
+        }
+        baseData = null;
+        Map<LogicalOperator, DataBag> result = getData(thisPhyPlan);
+        baseData = new HashMap<LOLoad, DataBag>();
+        for (LogicalOperator lo : result.keySet()) {
+            if (lo instanceof LOLoad) {
+                baseData.put((LOLoad) lo, result.get(lo));
             }
-            baseData.put((LOLoad) op, opBaseData);
-            // poLoad.setPc(oldPC);
-            poLoad.setLineageTracer(null);
         }
-
     }
 
-    private void compilePlan(LogicalPlan plan) {
-
-        plan = refineLogicalPlan(plan);
-
-        LocalLogToPhyTranslationVisitor visitor = new LocalLogToPhyTranslationVisitor(
-                plan);
-        visitor.setPigContext(pigContext);
-        try {
-            visitor.visit();
-        } catch (VisitorException e) {
-            // TODO Auto-generated catch block
-            e.printStackTrace();
-            log.error("Error visiting the logical plan in ExampleGenerator");
+    PhysicalPlan compilePlan(LogicalPlan plan) throws ExecException, FrontendException {
+        return execEngine.compile(plan, null);
+    }
+    
+    public Map<LogicalOperator, DataBag> getData() throws IOException, InterruptedException {
+      return getData(physPlan);
+    }
+    
+    private Map<LogicalOperator, DataBag> getData(PhysicalPlan plan) throws PigException, IOException, InterruptedException
+    {
+        // get data on a physical plan possibly trimmed of one branch 
+        lineage = new LineageTracer();
+        IllustratorAttacher attacher = new IllustratorAttacher(plan, lineage, MAX_RECORDS, poLoadToSchemaMap, pigContext);
+        attacher.visit();
+        if (oriLimitMap != null) {
+            for (Map.Entry<LOLimit, Long> entry : oriLimitMap.entrySet()) {
+                logToPhyMap.get(entry.getKey()).getIllustrator().setOriginalLimit(entry.getValue());
+            }
         }
-        physPlan = visitor.getPhysicalPlan();
-        LogToPhyMap = visitor.getLogToPhyMap();
+        getLogToDataMap(attacher.getDataMap());
+        if (baseData != null ) {
+            setLoadDataMap();
+            physPlanReseter.visit();
+        }
+        localMRRunner.launchPig(plan, baseData, poLoadToLogMap, lineage, attacher, this, pigContext);
+        if (baseData == null)
+            poToEqclassesMap = attacher.poToEqclassesMap;
+        else {
+            for (Map.Entry<PhysicalOperator, Collection<IdentityHashSet<Tuple>>> entry : attacher.poToEqclassesMap.entrySet()) {
+                if(!(entry.getKey() instanceof POLoad))
+                  poToEqclassesMap.put(entry.getKey(), entry.getValue());
+            }
+        }
+        if (baseData != null)
+            // only for non derived data generation
+            phyToMRTransform(plan, attacher.getDataMap());
+        return logToDataMap;
     }
-
-    private LogicalPlan refineLogicalPlan(LogicalPlan plan) {
-        PlanSetter ps = new PlanSetter(plan);
-        try {
-            ps.visit();
-
-        } catch (VisitorException e) {
-            // TODO Auto-generated catch block
-            e.printStackTrace();
+    
+    public Map<LogicalOperator, DataBag> getData(Map<LOLoad, DataBag> newBaseData) throws Exception 
+    {
+        baseData = newBaseData;
+        return getData(physPlan);
+    }
+    
+    private void phyToMRTransform(PhysicalPlan plan, Map<PhysicalOperator, DataBag> phyToDataMap) {
+        // remap the LO to PO as result of the MR compilation may have changed PO in the MR plans
+        Map<PhysicalOperator, PhysicalOperator> phyToMRMap = localMRRunner.getPhyToMRMap();
+        for (Map.Entry<PhysicalOperator, LogicalOperator> entry : poToLogMap.entrySet()) {
+            if (phyToMRMap.get(entry.getKey()) != null) {
+                PhysicalOperator poInMR = phyToMRMap.get(entry.getKey());
+                logToDataMap.put(entry.getValue(), phyToDataMap.get(poInMR));
+                poToEqclassesMap.put(entry.getKey(), poToEqclassesMap.get(poInMR));
+            }
         }
-
-        // run through validator
-        CompilationMessageCollector collector = new CompilationMessageCollector();
-        FrontendException caught = null;
-        try {
-            boolean isBeforeOptimizer = true;
-            LogicalPlanValidationExecutor validator = new LogicalPlanValidationExecutor(
-                    plan, pigContext, isBeforeOptimizer);
-            validator.validate(plan, collector);
-
-            FunctionalLogicalOptimizer optimizer = new FunctionalLogicalOptimizer(
-                    plan);
-            optimizer.optimize();
-            
-            isBeforeOptimizer = false;
-            validator = new LogicalPlanValidationExecutor(
-                    plan, pigContext, isBeforeOptimizer);
-            validator.validate(plan, collector);
-        } catch (FrontendException fe) {
-            // Need to go through and see what the collector has in it. But
-            // remember what we've caught so we can wrap it into what we
-            // throw.
-            caught = fe;
+    }
+    
+    private void getLogToDataMap(Map<PhysicalOperator, DataBag> phyToDataMap) {
+        logToDataMap.clear();
+        for (LogicalOperator lo : logToPhyMap.keySet()) {
+            if (logToPhyMap.get(lo) != null)
+                logToDataMap.put(lo, phyToDataMap.get(logToPhyMap.get(lo)));
         }
+        
+        // set the LO-to-Data mapping for the ForEach inner plans
+        for (Map.Entry<LOForEach, Map<LogicalOperator, DataBag>> entry : forEachInnerLogToDataMap.entrySet()) {
+            entry.getValue().clear();
+            for (Map.Entry<LogicalOperator, PhysicalOperator>  innerEntry : forEachInnerLogToPhyMap.get(entry.getKey()).entrySet()) {
+                entry.getValue().put(innerEntry.getKey(), phyToDataMap.get(innerEntry.getValue()));
+            }
+        }
+    }
+    
+    private void setLoadDataMap() {
+        // This function sets up the LO-TO-Data map, eq. class, and lineage for the base data used in the coming runner
+        // this must be called after logToDataMap has been properly (re)set and before the runner is started
+        if (baseData != null) {
+            if (poToEqclassesMap == null)
+                poToEqclassesMap = new HashMap<PhysicalOperator, Collection<IdentityHashSet<Tuple>>>();
+            else
+                poToEqclassesMap.clear();
+            for (LOLoad lo : baseData.keySet()) {
+                logToDataMap.get(lo).addAll(baseData.get(lo));
+                LinkedList<IdentityHashSet<Tuple>> equivalenceClasses = new LinkedList<IdentityHashSet<Tuple>>();
+                IdentityHashSet<Tuple> equivalenceClass = new IdentityHashSet<Tuple>();
+                equivalenceClasses.add(equivalenceClass);
+                for (Tuple t : baseData.get(lo)) {
+                    lineage.insert(t);
+                    equivalenceClass.add(t);
+                }
+                poToEqclassesMap.put(logToPhyMap.get(lo), equivalenceClasses);
+            }
+        }
+    }
+    
+    public Collection<IdentityHashSet<Tuple>> getEqClasses() throws VisitorException {
+        Map<LogicalOperator, Collection<IdentityHashSet<Tuple>>> logToEqclassesMap = getLoToEqClassMap();
+        LinkedList<IdentityHashSet<Tuple>> ret = new LinkedList<IdentityHashSet<Tuple>>();
+        for (Map.Entry<LogicalOperator, Collection<IdentityHashSet<Tuple>>> entry :
+            logToEqclassesMap.entrySet()) {
+            if (entry.getValue() != null)
+                ret.addAll(entry.getValue());
+        }
+        return ret;
+    }
 
-        return plan;
-
+    public Map<LogicalOperator, Collection<IdentityHashSet<Tuple>>> getLoToEqClassMap() throws VisitorException {
+        Map<LogicalOperator, Collection<IdentityHashSet<Tuple>>> ret =
+          EquivalenceClasses.getLoToEqClassMap(physPlan, plan, logToPhyMap, logToDataMap, forEachInnerLogToPhyMap, poToEqclassesMap);
+        // eq classes adjustments based upon logical operators
+        
+        for (Map.Entry<LogicalOperator, Collection<IdentityHashSet<Tuple>>> entry :ret.entrySet())
+        {
+            if (entry.getKey() instanceof LOSort) {
+                Collection<IdentityHashSet<Tuple>> eqClasses = entry.getValue();
+                for (Iterator<IdentityHashSet<Tuple>> it = eqClasses.iterator(); it.hasNext(); ) {
+                    Object t = null;
+                    IdentityHashSet<Tuple> eqClass = it.next();
+                    if (eqClass.size() == 1) {
+                        eqClass.clear();
+                        continue;
+                    }
+                    boolean first = true, allIdentical = true;
+                    for (Iterator<Tuple> it1 = eqClass.iterator(); it1.hasNext();)
+                    {
+                        if (first) {
+                            first = false;
+                            t = it1.next();
+                        } else {
+                            if (!it1.next().equals(t)) {
+                                allIdentical = false;
+                                break;
+                            }
+                        }
+                    }
+                    if (allIdentical)
+                        eqClass.clear();
+                }
+            }
+        }
+        
+        return ret;
     }
 }
diff --git a/src/org/apache/pig/pen/FakeRawKeyValueIterator.java b/src/org/apache/pig/pen/FakeRawKeyValueIterator.java
new file mode 100644
index 000000000..55bcf2254
--- /dev/null
+++ b/src/org/apache/pig/pen/FakeRawKeyValueIterator.java
@@ -0,0 +1,56 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.pig.pen;
+
+import org.apache.hadoop.mapred.RawKeyValueIterator;
+import org.apache.hadoop.io.DataInputBuffer;
+import org.apache.hadoop.util.Progress;
+
+public class FakeRawKeyValueIterator implements RawKeyValueIterator {
+    private boolean hasData;
+    
+    public FakeRawKeyValueIterator(boolean hasData) {
+        this.hasData = hasData;
+    }
+    
+    @Override
+    public DataInputBuffer getKey() {
+        return null; 
+    }
+    
+    @Override
+    public void close() {
+      
+    }
+    
+    @Override
+    public Progress getProgress() {
+        return null;
+    }
+    
+    @Override
+    public DataInputBuffer getValue() {
+        return null;
+    }
+    
+    @Override
+    public boolean next() {
+        return hasData;
+    }
+}
diff --git a/src/org/apache/pig/pen/Illustrable.java b/src/org/apache/pig/pen/Illustrable.java
new file mode 100644
index 000000000..977cf4802
--- /dev/null
+++ b/src/org/apache/pig/pen/Illustrable.java
@@ -0,0 +1,34 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.pig.pen;
+
+import org.apache.pig.data.Tuple;
+
+public interface Illustrable {
+    public void setIllustrator(Illustrator illustrator);
+    /**
+     * input tuple mark up to be illustrate-able
+     * @param in             input tuple
+     * @param out            output tuple before wrapped in ExampleTuple
+     * @param eqClassIndex   index into equivalence classes in illustrator
+     * 
+     * @return               tuple
+     */
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex);
+}
diff --git a/src/org/apache/pig/pen/Illustrator.java b/src/org/apache/pig/pen/Illustrator.java
new file mode 100644
index 000000000..5a2147825
--- /dev/null
+++ b/src/org/apache/pig/pen/Illustrator.java
@@ -0,0 +1,142 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.pig.pen;
+
+import java.util.LinkedList;
+import java.util.ArrayList;
+
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
+
+import org.apache.pig.data.BagFactory;
+import org.apache.pig.data.DataBag;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.impl.PigContext;
+import org.apache.pig.pen.util.LineageTracer;
+import org.apache.pig.impl.util.IdentityHashSet;
+import org.apache.pig.impl.logicalLayer.schema.Schema;
+
+/**
+ * Class used by physical operators to generate example tuples for the ILLUSTRATE
+ * purpose
+ */
+
+public class Illustrator {
+
+    private LineageTracer lineage;
+    private LinkedList<IdentityHashSet<Tuple>> equivalenceClasses;
+    // all input tuples for an expression
+    private IdentityHashSet<Tuple> inputs = null;
+    
+    private DataBag data;
+    private int maxRecords = -1;
+    private int recCounter = 0;
+    private IllustratorAttacher attacher;
+    private ArrayList<Boolean[]> subExpResults;
+    private Boolean[] subExpResult;
+    private boolean eqClassesShared;
+    private long oriLimit = -1;
+    private Schema schema;
+
+    public Illustrator(LineageTracer lineage, LinkedList<IdentityHashSet<Tuple>> equivalenceClasses, IllustratorAttacher attacher, PigContext hadoopPigContext) {
+        this.lineage = lineage;
+        this.equivalenceClasses = equivalenceClasses;
+        data = BagFactory.getInstance().newDefaultBag();
+        this.attacher = attacher;
+        subExpResults = new ArrayList<Boolean[]>();
+        subExpResult = new Boolean[1];
+        schema = null;
+    }
+    
+    public Illustrator(LineageTracer lineage, LinkedList<IdentityHashSet<Tuple>> equivalenceClasses, int maxRecords, IllustratorAttacher attacher,
+        Schema schema, PigContext hadoopPigContext) {
+        this(lineage, equivalenceClasses, attacher, hadoopPigContext);
+        this.maxRecords = maxRecords;
+        this.schema = schema;
+    }
+    
+    public ArrayList<Boolean[]> getSubExpResults() {
+        return subExpResults;
+    }
+    
+    Boolean[] getSubExpResult() {
+        return subExpResult;
+    }
+    
+    public LineageTracer getLineage() {
+        return lineage;
+    }
+    
+    public LinkedList<IdentityHashSet<Tuple>> getEquivalenceClasses() {
+        return equivalenceClasses;
+    }
+    
+    public void setSubExpResult(boolean result) {
+        subExpResult[0] = result;
+    }
+    
+    public void setEquivalenceClasses(LinkedList<IdentityHashSet<Tuple>> eqClasses, PhysicalOperator po) {
+        equivalenceClasses = eqClasses;
+        attacher.poToEqclassesMap.put(po, eqClasses);
+    }
+    
+    public boolean ceilingCheck() {
+        if (maxRecords != -1 && ++recCounter > maxRecords)
+            return false;
+        else
+          return true;
+    }
+    
+    public IdentityHashSet<Tuple> getInputs() {
+        return inputs;      
+    }
+    
+    public void addInputs(IdentityHashSet<Tuple> inputs) {
+        if (this.inputs == null)
+          this.inputs = new IdentityHashSet<Tuple>();
+        this.inputs.addAll(inputs);
+    }
+    
+    public void addData(Tuple t) {
+      data.add(t);
+    }
+    
+    public DataBag getData() {
+      return data;
+    }
+    
+    public long getOriginalLimit() {
+        return oriLimit;
+    }
+    
+    public void setOriginalLimit(long oriLimit) {
+        this.oriLimit = oriLimit;
+    }
+    
+    public void setEqClassesShared() {
+        eqClassesShared = true;
+    }
+    
+    public boolean getEqClassesShared() {
+        return eqClassesShared;
+    }
+    
+    public Schema getSchema() {
+        return schema;
+    }
+}
diff --git a/src/org/apache/pig/pen/IllustratorAttacher.java b/src/org/apache/pig/pen/IllustratorAttacher.java
new file mode 100644
index 000000000..9b269f60c
--- /dev/null
+++ b/src/org/apache/pig/pen/IllustratorAttacher.java
@@ -0,0 +1,470 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.pig.pen;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.LinkedList;
+import java.util.Map;
+import java.util.Collection;
+import java.util.List;
+
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCollectedGroup;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackageLite;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCombinerPackage;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMultiQueryPackage;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POUnion;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODemux;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODistinct;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSort;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.GreaterThanExpr;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.LessThanExpr;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.GTOrEqualToExpr;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.LTOrEqualToExpr;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.EqualToExpr;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.NotEqualToExpr;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.PORegexp;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POIsNull;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POAnd;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POOr;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.PONot;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POBinCond;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.PONegative;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserComparisonFunc;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POMapLookUp;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POJoinPackage;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLimit;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFRJoin;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeCogroup;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStream;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSkewedJoin;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPartitionRearrange;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POOptimizedForEach;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPreCombinerLocalRearrange;
+import org.apache.pig.data.DataBag;
+import org.apache.pig.impl.PigContext;
+import org.apache.pig.impl.plan.PlanWalker;
+import org.apache.pig.impl.plan.VisitorException;
+import org.apache.pig.impl.util.IdentityHashSet;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
+import org.apache.pig.pen.util.LineageTracer;
+import org.apache.pig.impl.plan.DepthFirstWalker;
+import org.apache.pig.impl.logicalLayer.schema.Schema;
+
+/**
+ * The class used to (re)attach illustrators to physical operators
+ * 
+ *
+ */
+public class IllustratorAttacher extends PhyPlanVisitor {
+
+    PigContext pigContext;
+
+    LineageTracer lineage;
+
+    HashMap<PhysicalOperator, Collection<IdentityHashSet<Tuple>>> poToEqclassesMap;
+    
+    private HashMap<PhysicalOperator, DataBag> poToDataMap;
+    private int maxRecords;
+    private boolean revisit = false;
+    private ArrayList<Boolean[]> subExpResults = null;
+    private final Map<POLoad, Schema> poloadToSchemaMap;
+    
+    public IllustratorAttacher(PhysicalPlan plan, LineageTracer lineage, int maxRecords,
+        Map<POLoad, Schema> poLoadToSchemaMap, PigContext hadoopPigContext) throws VisitorException {
+        super(plan, new DepthFirstWalker<PhysicalOperator, PhysicalPlan>(plan));
+        pigContext = hadoopPigContext;
+        this.lineage = lineage;
+        poToEqclassesMap = new HashMap<PhysicalOperator, Collection<IdentityHashSet<Tuple>>>();
+        poToDataMap = new HashMap<PhysicalOperator, DataBag>();
+        this.maxRecords = maxRecords;
+        this.poloadToSchemaMap = poLoadToSchemaMap;
+    }
+
+    /**
+     * revisit an enhanced physical plan from MR compilation
+     * @param plan a physical plan to be traversed
+     */
+    public void revisit(PhysicalPlan plan) throws VisitorException {
+        pushWalker(new DepthFirstWalker<PhysicalOperator, PhysicalPlan>(plan));
+        revisit = true;
+        PhysicalPlan oriPlan = mPlan;
+        mPlan = plan;
+        visit();
+        mPlan = oriPlan;
+        popWalker();
+    }
+    
+    private void setIllustrator(PhysicalOperator po, int nEqClasses) {
+        if (revisit && po.getIllustrator() != null)
+            return;
+        LinkedList<IdentityHashSet<Tuple>> eqClasses = new LinkedList<IdentityHashSet<Tuple>>();
+        poToEqclassesMap.put(po, eqClasses);
+        for (int i = 0; i < nEqClasses; ++i)
+        {
+            IdentityHashSet<Tuple> eqClass = new IdentityHashSet<Tuple>();
+            eqClasses.add(eqClass);
+        }
+        Illustrator illustrator = new Illustrator(lineage, eqClasses, this, pigContext);
+        po.setIllustrator(illustrator);
+        poToDataMap.put(po, illustrator.getData());
+    }
+    
+    private void setIllustrator(PhysicalOperator po, LinkedList<IdentityHashSet<Tuple>> eqClasses) {
+        if (revisit && po.getIllustrator() != null)
+            return;
+        Illustrator illustrator = new Illustrator(lineage, eqClasses, this, pigContext);
+        po.setIllustrator(illustrator);
+        if (eqClasses != null)
+            poToEqclassesMap.put(po, eqClasses);
+        poToDataMap.put(po, illustrator.getData());
+    }
+
+    void setIllustrator(PhysicalOperator po) {
+        if (revisit && po.getIllustrator() != null)
+            return;
+        LinkedList<IdentityHashSet<Tuple>> eqClasses = new LinkedList<IdentityHashSet<Tuple>>();
+        IdentityHashSet<Tuple> eqClass = new IdentityHashSet<Tuple>();
+        eqClasses.add(eqClass);
+        Illustrator illustrator = new Illustrator(lineage, eqClasses, this, pigContext);
+        po.setIllustrator(illustrator);
+        poToEqclassesMap.put(po, eqClasses);
+        poToDataMap.put(po, illustrator.getData());
+    }
+    
+    public Map<PhysicalOperator, DataBag> getDataMap() {
+        return poToDataMap;
+    }
+    
+    @Override
+    public void visitLoad(POLoad ld) throws VisitorException{
+        // LOAD from temporary files need no illustrator 
+        if (revisit)
+            return;
+        
+        LinkedList<IdentityHashSet<Tuple>> eqClasses = new LinkedList<IdentityHashSet<Tuple>>();
+        poToEqclassesMap.put(ld, eqClasses);
+        
+        IdentityHashSet<Tuple> eqClass = new IdentityHashSet<Tuple>();
+        eqClasses.add(eqClass);
+        Illustrator illustrator;
+        illustrator = new Illustrator(lineage, eqClasses, maxRecords, this, poloadToSchemaMap.get(ld), pigContext);
+        ld.setIllustrator(illustrator);
+        poToDataMap.put(ld, illustrator.getData());
+    }
+    
+    @Override
+    public void visitStore(POStore st) throws VisitorException{
+        setIllustrator(st, 1);
+    }
+    
+    @Override
+    public void visitFilter(POFilter fl) throws VisitorException{
+        setIllustrator(fl, 0);
+        subExpResults = fl.getIllustrator().getSubExpResults();
+        innerPlanAttach(fl, fl.getPlan());
+        subExpResults = null;
+    }
+ 
+    @Override
+    public void visitCollectedGroup(POCollectedGroup mg) throws VisitorException{
+        setIllustrator(mg, 1);
+    }
+    
+    @Override
+    public void visitLocalRearrange(POLocalRearrange lr) throws VisitorException{
+      super.visitLocalRearrange(lr);
+      setIllustrator(lr);
+    }
+    
+    @Override
+    public void visitPackage(POPackage pkg) throws VisitorException{
+        if (!(pkg instanceof POPackageLite) && pkg.isDistinct())
+            setIllustrator(pkg, 1);
+        else
+            setIllustrator(pkg, null);
+    }
+    
+    @Override
+    public void visitCombinerPackage(POCombinerPackage pkg) throws VisitorException{
+        setIllustrator(pkg);
+    }
+ 
+    @Override
+    public void visitMultiQueryPackage(POMultiQueryPackage pkg) throws VisitorException{
+      setIllustrator(pkg);
+    }
+    
+    @Override
+    public void visitPOForEach(POForEach nfe) throws VisitorException {
+        if (revisit && nfe.getIllustrator() != null)
+            return;
+        List<PhysicalPlan> innerPlans = nfe.getInputPlans();
+        for (PhysicalPlan innerPlan : innerPlans)
+          innerPlanAttach(nfe, innerPlan);
+        List<PhysicalOperator> preds = mPlan.getPredecessors(nfe);
+        if (preds != null && preds.size() == 1 &&
+            preds.get(0) instanceof POPackage &&
+            !(preds.get(0) instanceof POPackageLite) &&
+            ((POPackage) preds.get(0)).isDistinct()) {
+            // equivalence class of POPackage for DISTINCT needs to be used
+            //instead of the succeeding POForEach's equivalence class
+            setIllustrator(nfe, preds.get(0).getIllustrator().getEquivalenceClasses());
+            nfe.getIllustrator().setEqClassesShared();
+        } else
+            setIllustrator(nfe, 1);
+    }
+    
+    @Override
+    public void visitUnion(POUnion un) throws VisitorException{
+        if (revisit && un.getIllustrator() != null)
+          return;
+        setIllustrator(un, null);
+    }
+    
+    @Override
+    public void visitSplit(POSplit spl) throws VisitorException{
+        if (revisit && spl.getIllustrator() != null)
+            return;
+        for (PhysicalPlan poPlan : spl.getPlans())
+          innerPlanAttach(spl, poPlan);
+        setIllustrator(spl);
+    }
+
+    @Override
+    public void visitDemux(PODemux demux) throws VisitorException{
+      if (revisit && demux.getIllustrator() != null)
+          return;
+      List<PhysicalPlan> innerPlans = demux.getPlans();
+      for (PhysicalPlan innerPlan : innerPlans)
+        innerPlanAttach(demux, innerPlan);
+      setIllustrator(demux);
+    }
+    
+    @Override
+	public void visitDistinct(PODistinct distinct) throws VisitorException {
+        setIllustrator(distinct, 1);
+	}
+    
+    @Override
+	public void visitSort(POSort sort) throws VisitorException {
+        setIllustrator(sort, 1);
+	}
+    
+    @Override
+    public void visitProject(POProject proj) throws VisitorException{
+    }
+    
+    @Override
+    public void visitGreaterThan(GreaterThanExpr grt) throws VisitorException{
+        setIllustrator(grt, 0);
+        if (!revisit)
+            subExpResults.add(grt.getIllustrator().getSubExpResult());
+    }
+    
+    @Override
+    public void visitLessThan(LessThanExpr lt) throws VisitorException{
+        setIllustrator(lt, 0);
+        if (!revisit)
+            subExpResults.add(lt.getIllustrator().getSubExpResult());
+    }
+    
+    @Override
+    public void visitGTOrEqual(GTOrEqualToExpr gte) throws VisitorException{
+        setIllustrator(gte, 0);
+        if (!revisit)
+            subExpResults.add(gte.getIllustrator().getSubExpResult());
+    }
+    
+    @Override
+    public void visitLTOrEqual(LTOrEqualToExpr lte) throws VisitorException{
+        setIllustrator(lte, 0);
+        if (!revisit)
+            subExpResults.add(lte.getIllustrator().getSubExpResult());
+    }
+    
+    @Override
+    public void visitEqualTo(EqualToExpr eq) throws VisitorException{
+        setIllustrator(eq, 0);
+        if (!revisit)
+            subExpResults.add(eq.getIllustrator().getSubExpResult());
+    }
+    
+    @Override
+    public void visitNotEqualTo(NotEqualToExpr eq) throws VisitorException{
+        setIllustrator(eq, 0);
+        if (!revisit)
+            subExpResults.add(eq.getIllustrator().getSubExpResult());
+    }
+    
+    @Override
+    public void visitRegexp(PORegexp re) throws VisitorException{
+        setIllustrator(re, 0);
+        if (!revisit)
+            subExpResults.add(re.getIllustrator().getSubExpResult());
+    }
+
+    @Override
+    public void visitIsNull(POIsNull isNull) throws VisitorException {
+        setIllustrator(isNull, 0);
+        if (!revisit)
+            subExpResults.add(isNull.getIllustrator().getSubExpResult());
+    }
+    
+    @Override
+    public void visitAnd(POAnd and) throws VisitorException {
+        setIllustrator(and, 0);
+    }
+    
+    @Override
+    public void visitOr(POOr or) throws VisitorException {
+        setIllustrator(or, 0);
+    }
+
+    @Override
+    public void visitNot(PONot not) throws VisitorException {
+        setIllustrator(not, 0);
+        if (!revisit)
+            subExpResults.add(not.getIllustrator().getSubExpResult());
+    }
+
+    @Override
+    public void visitBinCond(POBinCond binCond) {
+        setIllustrator(binCond, 0);
+        if (!revisit)
+            subExpResults.add(binCond.getIllustrator().getSubExpResult());
+    }
+
+    @Override
+    public void visitNegative(PONegative negative) {
+      setIllustrator(negative, 1);
+    }
+    
+    @Override
+    public void visitUserFunc(POUserFunc userFunc) throws VisitorException {
+    }
+    
+    @Override
+    public void visitComparisonFunc(POUserComparisonFunc compFunc) throws VisitorException {
+        // one each for >, ==, and <
+        setIllustrator(compFunc, 3);
+    }
+
+    @Override
+    public void visitMapLookUp(POMapLookUp mapLookUp) {
+      setIllustrator(mapLookUp, 1);
+    }
+    
+    @Override
+    public void visitJoinPackage(POJoinPackage joinPackage) throws VisitorException{
+        if (revisit &&  joinPackage.getIllustrator() != null)
+            return;
+        setIllustrator(joinPackage);
+        joinPackage.getForEach().setIllustrator(joinPackage.getIllustrator());
+    }
+
+    @Override
+    public void visitCast(POCast cast) {
+    }
+    
+    @Override
+    public void visitLimit(POLimit lim) throws VisitorException {
+        setIllustrator(lim, 1);
+    }
+    
+    @Override
+    public void visitFRJoin(POFRJoin join) throws VisitorException {
+        // one eq. class per input
+        setIllustrator(join, join.getInputs().size());
+    }
+    
+    @Override
+    public void visitMergeJoin(POMergeJoin join) throws VisitorException {
+        // one eq. class per input
+        setIllustrator(join, join.getInputs().size());
+    }
+    
+    @Override
+    public void visitMergeCoGroup(POMergeCogroup mergeCoGrp) throws VisitorException{
+        // one eq. class per input
+        setIllustrator(mergeCoGrp, mergeCoGrp.getInputs().size());
+    }
+
+    @Override
+    public void visitStream(POStream stream) throws VisitorException {
+        setIllustrator(stream, 1);
+    }
+
+    @Override
+	public void visitSkewedJoin(POSkewedJoin sk) throws VisitorException {
+        // one eq. class per input
+        // do not go to inner plans as they are not booleans so no use of eq. classes
+        setIllustrator(sk, sk.getInputs().size());
+	}
+
+    @Override
+	public void visitPartitionRearrange(POPartitionRearrange pr) throws VisitorException {
+	}
+
+    /**
+     * @param optimizedForEach
+     */
+    @Override
+    public void visitPOOptimizedForEach(POOptimizedForEach optimizedForEach) throws VisitorException {
+        visitPOForEach(optimizedForEach);
+    }
+
+    /**
+     * @param preCombinerLocalRearrange
+     */
+    @Override
+    public void visitPreCombinerLocalRearrange(
+            POPreCombinerLocalRearrange preCombinerLocalRearrange) {
+    }
+    
+    private void innerPlanAttach(PhysicalOperator po, PhysicalPlan plan) throws VisitorException {
+        PlanWalker<PhysicalOperator, PhysicalPlan> childWalker =
+              mCurrentWalker.spawnChildWalker(plan);
+        pushWalker(childWalker);
+        childWalker.walk(this);
+        popWalker();
+        LinkedList<IdentityHashSet<Tuple>> eqClasses = new LinkedList<IdentityHashSet<Tuple>>();
+        if (subExpResults != null && !revisit) {
+            int size = 1 << subExpResults.size();
+            for (int i = 0; i < size; ++i) {
+                eqClasses.add(new IdentityHashSet<Tuple>());
+            }
+            po.getIllustrator().setEquivalenceClasses(eqClasses, po);
+        }
+    }
+}
diff --git a/src/org/apache/pig/pen/LineageTrimmingVisitor.java b/src/org/apache/pig/pen/LineageTrimmingVisitor.java
index 2ada82a96..1245c7e20 100644
--- a/src/org/apache/pig/pen/LineageTrimmingVisitor.java
+++ b/src/org/apache/pig/pen/LineageTrimmingVisitor.java
@@ -19,6 +19,7 @@
 package org.apache.pig.pen;
 
 import java.util.Collection;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.IdentityHashMap;
@@ -27,10 +28,11 @@ import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
+import java.util.Comparator;
+import java.io.IOException;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
-import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
 import org.apache.pig.data.BagFactory;
@@ -38,6 +40,7 @@ import org.apache.pig.data.DataBag;
 import org.apache.pig.data.Tuple;
 import org.apache.pig.impl.PigContext;
 import org.apache.pig.impl.logicalLayer.LOCogroup;
+import org.apache.pig.impl.logicalLayer.LOJoin;
 import org.apache.pig.impl.logicalLayer.LOCross;
 import org.apache.pig.impl.logicalLayer.LODistinct;
 import org.apache.pig.impl.logicalLayer.LOFilter;
@@ -47,34 +50,41 @@ import org.apache.pig.impl.logicalLayer.LOLoad;
 import org.apache.pig.impl.logicalLayer.LOSort;
 import org.apache.pig.impl.logicalLayer.LOSplit;
 import org.apache.pig.impl.logicalLayer.LOUnion;
+import org.apache.pig.impl.logicalLayer.LOStore;
 import org.apache.pig.impl.logicalLayer.LOVisitor;
 import org.apache.pig.impl.logicalLayer.LogicalOperator;
 import org.apache.pig.impl.logicalLayer.LogicalPlan;
 import org.apache.pig.impl.plan.VisitorException;
 import org.apache.pig.impl.util.IdentityHashSet;
+import org.apache.pig.impl.util.Pair;
+import org.apache.pig.impl.io.FileSpec;
 import org.apache.pig.pen.util.LineageTracer;
 import org.apache.pig.pen.util.MetricEvaluation;
 import org.apache.pig.pen.util.PreOrderDepthFirstWalker;
+import org.apache.pig.pen.util.ExampleTuple;
 
 public class LineageTrimmingVisitor extends LOVisitor {
 
     LogicalPlan plan = null;
-    Map<LOLoad, DataBag> baseData = new HashMap<LOLoad, DataBag>();
+    Map<LOLoad, DataBag> baseData;
+    Map<FileSpec, DataBag> inputToDataMap;
     Map<LogicalOperator, PhysicalOperator> LogToPhyMap = null;
     PhysicalPlan physPlan = null;
     double completeness = 100.0;
     Log log = LogFactory.getLog(getClass());
 
-    Map<LogicalOperator, Map<IdentityHashSet<Tuple>, Integer>> AffinityGroups = new HashMap<LogicalOperator, Map<IdentityHashSet<Tuple>, Integer>>();
+    Map<LogicalOperator, Collection<IdentityHashSet<Tuple>>> AffinityGroups = new HashMap<LogicalOperator, Collection<IdentityHashSet<Tuple>>>();
     Map<LogicalOperator, LineageTracer> Lineage = new HashMap<LogicalOperator, LineageTracer>();
 
     boolean continueTrimming;
     PigContext pc;
+    private ExampleGenerator eg;
 
     public LineageTrimmingVisitor(LogicalPlan plan,
             Map<LOLoad, DataBag> baseData,
+            ExampleGenerator eg,
             Map<LogicalOperator, PhysicalOperator> LogToPhyMap,
-            PhysicalPlan physPlan, PigContext pc) {
+            PhysicalPlan physPlan, PigContext pc) throws IOException, InterruptedException {
         super(plan, new PreOrderDepthFirstWalker<LogicalOperator, LogicalPlan>(
                 plan));
         // this.baseData.putAll(baseData);
@@ -83,148 +93,65 @@ public class LineageTrimmingVisitor extends LOVisitor {
         this.LogToPhyMap = LogToPhyMap;
         this.pc = pc;
         this.physPlan = physPlan;
+        this.eg = eg;
+        this.inputToDataMap = new HashMap<FileSpec, DataBag>();
         init();
     }
 
-    public void init() {
+    public void init() throws IOException, InterruptedException {
 
-        DerivedDataVisitor visitor = new DerivedDataVisitor(plan, pc, baseData,
-                LogToPhyMap, physPlan);
-        try {
-            visitor.visit();
-        } catch (VisitorException e) {
-            log.error(e.getMessage());
-        }
+        Map<LogicalOperator, DataBag> data = eg.getData();
 
-        LineageTracer lineage = visitor.lineage;
-        Lineage.put(plan.getLeaves().get(0), lineage);
-        Map<LogicalOperator, Collection<IdentityHashSet<Tuple>>> OpToEqClasses = visitor.OpToEqClasses;
-        Collection<IdentityHashSet<Tuple>> EqClasses = visitor.EqClasses;
-        Map<IdentityHashSet<Tuple>, Integer> affinityGroup = new HashMap<IdentityHashSet<Tuple>, Integer>();
-        for (IdentityHashSet<Tuple> set : EqClasses) {
-            affinityGroup.put(set, 1);
+        LineageTracer lineage = eg.getLineage();
+        Map<LogicalOperator, Collection<IdentityHashSet<Tuple>>> OpToEqClasses = eg.getLoToEqClassMap();
+        for (LogicalOperator leaf : plan.getLeaves()) {
+            Lineage.put(leaf, lineage);
+            AffinityGroups.put(leaf, eg.getEqClasses());
         }
-        AffinityGroups.put(plan.getLeaves().get(0), affinityGroup);
         completeness = MetricEvaluation.getCompleteness(null,
-                visitor.derivedData, OpToEqClasses, true);
-        LogToPhyMap = visitor.LogToPhyMap;
+                data, OpToEqClasses, true);
+        LogToPhyMap = eg.getLogToPhyMap();
         continueTrimming = true;
 
     }
 
     @Override
     protected void visit(LOCogroup cg) throws VisitorException {
+        // can't separate CoGroup from succeeding ForEach
+        if (plan.getSuccessors(cg) != null && plan.getSuccessors(cg).get(0) instanceof LOForEach)
+            return;
+        
         if (continueTrimming) {
-            Map<IdentityHashSet<Tuple>, Integer> affinityGroups = null;
-
-            continueTrimming = checkCompleteness(cg);
-            
-            DerivedDataVisitor visitor = null;
-            LineageTracer lineage = null;
-            // create affinity groups
-            if (cg.getInputs().size() == 1) {
-                affinityGroups = new HashMap<IdentityHashSet<Tuple>, Integer>();
-                LogicalOperator childOp = cg.getInputs().get(0);
-                visitor = new DerivedDataVisitor(childOp, null, baseData,
-                        LogToPhyMap, physPlan);
-                try {
-                    visitor.visit();
-                } catch (VisitorException e) {
-                    log.error(e.getMessage());
-                }
-
-                lineage = visitor.lineage;
-
-                DataBag bag = visitor.evaluateIsolatedOperator(cg);
-                for (Iterator<Tuple> it = bag.iterator(); it.hasNext();) {
-                    DataBag field;
-                    try {
-                        field = (DataBag) it.next().get(1);
-                    } catch (ExecException e) {
-                        // TODO Auto-generated catch block
-                        e.printStackTrace();
-                        log.error(e.getMessage());
-                        throw new VisitorException(
-                                "Error trimming operator COGROUP operator "
-                                        + cg.getAlias()
-                                        + "in example generator");
-                    }
-                    IdentityHashSet<Tuple> set = new IdentityHashSet<Tuple>();
-                    affinityGroups.put(set, 2);
-                    for (Iterator<Tuple> it1 = field.iterator(); it1.hasNext();) {
-                        set.add(it1.next());
+            try {
+
+                continueTrimming = checkCompleteness(cg);
+                
+                LineageTracer lineage = null;
+                // create affinity groups
+                if (cg.getInputs().size() == 1) {
+                    lineage = eg.getLineage();
+                    AffinityGroups.put(cg.getInputs().get(0), eg.getEqClasses());
+                    Lineage.put(cg.getInputs().get(0), lineage);
+
+                } else {
+                    for (LogicalOperator input : cg.getInputs()) {
+                        Lineage.put(input, eg.getLineage());
+                        AffinityGroups.put(input, eg.getEqClasses());
                     }
                 }
-
-                // add the equivalence classes obtained from derived data
-                // creation
-                for (IdentityHashSet<Tuple> set : visitor.EqClasses) {
-                    affinityGroups.put(set, 1);
-                }
-                AffinityGroups.put(cg.getInputs().get(0), affinityGroups);
-                Lineage.put(cg.getInputs().get(0), lineage);
-
-            } else {
-                List<DataBag> inputs = new LinkedList<DataBag>();
-                visitor = new DerivedDataVisitor(cg, null, baseData,
-                        LogToPhyMap, physPlan);
-                affinityGroups = new HashMap<IdentityHashSet<Tuple>, Integer>();
-                for (int i = 0; i < cg.getInputs().size(); i++) {
-                    // affinityGroups = new HashMap<IdentityHashSet<Tuple>,
-                    // Integer>();
-                    LogicalOperator childOp = cg.getInputs().get(i);
-                    // visitor = new DerivedDataVisitor(cg.getInputs().get(i),
-                    // null, baseData, LogToPhyMap, physPlan);
-                    visitor.setOperatorToEvaluate(childOp);
-                    try {
-                        visitor.visit();
-                    } catch (VisitorException e) {
-                        log.error(e.getMessage());
-                    }
-                    // Lineage.put(childOp, visitor.lineage);
-                    inputs.add(visitor.derivedData.get(childOp));
-
-                    for (IdentityHashSet<Tuple> set : visitor.EqClasses)
-                        affinityGroups.put(set, 1);
-
-                    // AffinityGroups.put(cg.getInputs().get(i),
-                    // affinityGroups);
-                }
-                for (LogicalOperator input : cg.getInputs()) {
-                    Lineage.put(input, visitor.lineage);
-                    AffinityGroups.put(input, affinityGroups);
-                }
-
-                visitor = new DerivedDataVisitor(cg, null, baseData,
-                        LogToPhyMap, physPlan);
-                DataBag output = visitor.evaluateIsolatedOperator(cg, inputs);
-
-                for (int i = 1; i <= cg.getInputs().size(); i++) {
-                    affinityGroups = new HashMap<IdentityHashSet<Tuple>, Integer>();
-                    for (Iterator<Tuple> it = output.iterator(); it.hasNext();) {
-                        DataBag bag = null;
-                        try {
-                            bag = (DataBag) it.next().get(i);
-                        } catch (ExecException e) {
-                            // TODO Auto-generated catch block
-                            log.error(e.getMessage());
-                        }
-                        IdentityHashSet<Tuple> set = new IdentityHashSet<Tuple>();
-                        affinityGroups.put(set, 1);
-                        for (Iterator<Tuple> it1 = bag.iterator(); it1
-                                .hasNext();) {
-                            set.add(it1.next());
-                        }
-                    }
-                    AffinityGroups.get(cg.getInputs().get(i - 1)).putAll(
-                            affinityGroups);
-
-                }
-                AffinityGroups = AffinityGroups;
+            } catch (Exception e) {
+                throw new VisitorException("Exception : "+e.getMessage());
             }
         }
     }
 
+    @Override
+    protected void visit(LOJoin join) throws VisitorException {
+        if (continueTrimming) {
+          processOperator(join);
+        }
+    }
+
     @Override
     protected void visit(LOCross cs) throws VisitorException {
         if(continueTrimming)
@@ -244,6 +171,12 @@ public class LineageTrimmingVisitor extends LOVisitor {
         if (continueTrimming)
             processOperator(filter);
     }
+    
+    @Override
+    protected void visit(LOStore store) throws VisitorException {
+        if (continueTrimming)
+            processOperator(store);
+    }
 
     @Override
     protected void visit(LOForEach forEach) throws VisitorException {
@@ -286,9 +219,9 @@ public class LineageTrimmingVisitor extends LOVisitor {
     }
 
     private Map<LOLoad, DataBag> PruneBaseDataConstrainedCoverage(
-            Map<LOLoad, DataBag> baseData, DataBag rootOutput,
+            Map<LOLoad, DataBag> baseData,
             LineageTracer lineage,
-            Map<IdentityHashSet<Tuple>, Integer> equivalenceClasses) {
+            Collection<IdentityHashSet<Tuple>> equivalenceClasses) {
 
         IdentityHashMap<Tuple, Collection<Tuple>> membershipMap = lineage
                 .getMembershipMap();
@@ -300,10 +233,9 @@ public class LineageTrimmingVisitor extends LOVisitor {
         // IdentityHashMap<Tuple, Set<Integer>> lineageGroupToEquivClasses = new
         // IdentityHashMap<Tuple, Set<Integer>>();
         IdentityHashMap<Tuple, Set<IdentityHashSet<Tuple>>> lineageGroupToEquivClasses = new IdentityHashMap<Tuple, Set<IdentityHashSet<Tuple>>>();
-        int equivClassId = 0;
-        for (IdentityHashSet<Tuple> equivClass : equivalenceClasses.keySet()) {
-            for (Tuple t : equivClass) {
-                Tuple lineageGroup = lineage.getRepresentative(t);
+        for (IdentityHashSet<Tuple> equivClass : equivalenceClasses) {
+            for (Object t : equivClass) {
+                Tuple lineageGroup = lineage.getRepresentative((Tuple) t);
                 // Set<Integer> entry =
                 // lineageGroupToEquivClasses.get(lineageGroup);
                 Set<IdentityHashSet<Tuple>> entry = lineageGroupToEquivClasses
@@ -316,8 +248,6 @@ public class LineageTrimmingVisitor extends LOVisitor {
                 // entry.add(equivClassId);
                 entry.add(equivClass);
             }
-
-            equivClassId++;
         }
 
         // select lineage groups such that we cover all equivalence classes
@@ -325,18 +255,19 @@ public class LineageTrimmingVisitor extends LOVisitor {
         while (!lineageGroupToEquivClasses.isEmpty()) {
             // greedily find the lineage group with the best "score", where
             // score = # equiv classes covered / group weight
-            double bestScore = -1;
+            double bestWeight = -1;
             Tuple bestLineageGroup = null;
             Set<IdentityHashSet<Tuple>> bestEquivClassesCovered = null;
+            int bestNumEquivClassesCovered = 0;
             for (Tuple lineageGroup : lineageGroupToEquivClasses.keySet()) {
                 double weight = lineageGroupWeights.get(lineageGroup);
 
                 Set<IdentityHashSet<Tuple>> equivClassesCovered = lineageGroupToEquivClasses
                         .get(lineageGroup);
                 int numEquivClassesCovered = equivClassesCovered.size();
-                double score = ((double) numEquivClassesCovered) / weight;
 
-                if (score > bestScore) {
+                if ((numEquivClassesCovered > bestNumEquivClassesCovered) ||
+                    (numEquivClassesCovered == bestNumEquivClassesCovered && weight < bestWeight)) {
 
                     if (selectedLineageGroups.contains(lineageGroup)) {
                         bestLineageGroup = lineageGroup;
@@ -344,8 +275,9 @@ public class LineageTrimmingVisitor extends LOVisitor {
                         continue;
                     }
 
-                    bestScore = score;
+                    bestWeight = weight;
                     bestLineageGroup = lineageGroup;
+                    bestNumEquivClassesCovered = numEquivClassesCovered;
                     bestEquivClassesCovered = equivClassesCovered;
                 }
             }
@@ -371,10 +303,7 @@ public class LineageTrimmingVisitor extends LOVisitor {
                         .iterator(); it.hasNext();) {
                     IdentityHashSet<Tuple> equivClass = it.next();
                     if (bestEquivClassesCovered.contains(equivClass)) {
-                        if ((equivalenceClasses.get(equivClass) - 1) <= 0) {
-                            // equivClasses.remove(equivClass);
-                            it.remove();
-                        }
+                        it.remove();
                     }
                 }
                 if (equivClasses.size() == 0)
@@ -383,11 +312,6 @@ public class LineageTrimmingVisitor extends LOVisitor {
             }
             for (Tuple removeMe : toRemove)
                 lineageGroupToEquivClasses.remove(removeMe);
-
-            for (IdentityHashSet<Tuple> equivClass : bestEquivClassesCovered) {
-                equivalenceClasses.put(equivClass, equivalenceClasses
-                        .get(equivClass) - 1);
-            }
         }
 
         // revise baseData to only contain the tuples that are part of
@@ -414,65 +338,150 @@ public class LineageTrimmingVisitor extends LOVisitor {
         return newBaseData;
     }
 
-    private void processOperator(LogicalOperator op) {
-        if (op instanceof LOLoad) return;
+    private void processLoad(LOLoad ld) throws VisitorException {
+        // prune base records
+        if (inputToDataMap.get(ld.getInputFile()) != null) {
+            baseData.put(ld, inputToDataMap.get(ld.getInputFile()));
+            return;
+        }
         
-        continueTrimming = checkCompleteness(op);
-
-        if (continueTrimming == false)
+        DataBag data = baseData.get(ld);
+        if (data == null || data.size() < 2)
             return;
-
-        LogicalOperator childOp = plan.getPredecessors(op).get(0);
-
-        DerivedDataVisitor visitor = new DerivedDataVisitor(childOp, null,
-                baseData, LogToPhyMap, physPlan);
-        try {
-            visitor.visit();
-        } catch (VisitorException e) {
-            log.error(e.getMessage());
+        Set<Tuple> realData = new HashSet<Tuple>(), syntheticData = new HashSet<Tuple>();
+
+        for (Iterator<Tuple> it = data.iterator(); it.hasNext(); ) {
+            Tuple t = it.next();
+            if (((ExampleTuple)t).synthetic)
+                syntheticData.add(t);
+            else
+              realData.add(t);
         }
+        
+        Map<LOLoad, DataBag> newBaseData = new HashMap<LOLoad, DataBag>();
+        DataBag newData = BagFactory.getInstance().newDefaultBag();
+        newBaseData.put(ld, newData);
+        for (Map.Entry<LOLoad, DataBag> entry : baseData.entrySet()) {
+            if (entry.getKey() != ld) {
+                if (!entry.getKey().getInputFile().equals(ld.getInputFile()))
+                    newBaseData.put(entry.getKey(), entry.getValue());
+                else
+                    newBaseData.put(entry.getKey(), newData);
+            }
+        }
+        
+        if (checkNewBaseData(newData, newBaseData, realData))
+            checkNewBaseData(newData, newBaseData, syntheticData);
+        
+        inputToDataMap.put(ld.getInputFile(), baseData.get(ld));
+    }
+    
+    private boolean checkNewBaseData(DataBag data, Map<LOLoad, DataBag> newBaseData, Set<Tuple> loadData) throws VisitorException {
+        List<Pair<Tuple, Double>> sortedBase = new LinkedList<Pair<Tuple, Double>>();
+        DataBag oldData = BagFactory.getInstance().newDefaultBag();
+        oldData.addAll(data);
+        double tmpCompleteness = completeness;
+        for (Tuple t : loadData) {
+            data.add(t);
+            // obtain the derived data 
+            Map<LogicalOperator, DataBag> derivedData;
+            try {
+                derivedData = eg.getData(newBaseData);
+            } catch (Exception e) {
+                throw new VisitorException("Exception: "+e.getMessage());
+            }
+            double newCompleteness = MetricEvaluation.getCompleteness(null,
+                    derivedData, eg.getLoToEqClassMap(), true);
 
-        DataBag bag = visitor.derivedData.get(childOp);
-        Map<IdentityHashSet<Tuple>, Integer> affinityGroups = new HashMap<IdentityHashSet<Tuple>, Integer>();
-
-        for (Iterator<Tuple> it = bag.iterator(); it.hasNext();) {
-            IdentityHashSet<Tuple> set = new IdentityHashSet<Tuple>();
-            affinityGroups.put(set, 1);
-            set.add(it.next());
+            sortedBase.add(new Pair<Tuple, Double>(t, Double.valueOf(newCompleteness)));
+            if (newCompleteness >= tmpCompleteness)
+                break;
+        }
+        
+        Collections.sort(sortedBase, new Comparator<Pair<Tuple, Double>>() {
+            @Override
+            public int compare(Pair<Tuple, Double> o1,
+                               Pair<Tuple, Double> o2) {
+                return o1.second > o2.second ? -1 : o1.second == o2.second ? 0 : 1;
+            }
         }
+        );
+
+        data.clear();
+        data.addAll(oldData);
+        for (Pair<Tuple, Double> p : sortedBase) {
+            data.add(p.first);
+            // obtain the derived data 
+            Map<LogicalOperator, DataBag> derivedData;
+            try {
+                derivedData = eg.getData(newBaseData);
+            } catch (Exception e) {
+                throw new VisitorException("Exception: "+e.getMessage());
+            }
+            double newCompleteness = MetricEvaluation.getCompleteness(null,
+                    derivedData, eg.getLoToEqClassMap(), true);
 
-        for (IdentityHashSet<Tuple> set : visitor.EqClasses) {
-            // newEquivalenceClasses.put(set, 1);
-            affinityGroups.put(set, 1);
+            if (newCompleteness >= completeness) {
+                completeness = newCompleteness;
+                baseData.putAll(newBaseData);
+                return false;
+            }
         }
+        return true;
+    }
+    
+    private void processOperator(LogicalOperator op) throws VisitorException {
+        
+        try {
+            if (op instanceof LOLoad) {
+                processLoad((LOLoad) op);
+                return;
+            }
+            
+            continueTrimming = checkCompleteness(op);
 
-        AffinityGroups.put(childOp, affinityGroups);
-        Lineage.put(childOp, visitor.lineage);
+            if (plan.getPredecessors(op) == null)
+                return;
+            
+            if (continueTrimming == false)
+                return;
 
+            LogicalOperator childOp = plan.getPredecessors(op).get(0);
+            if (op instanceof LOForEach && childOp instanceof LOCogroup)
+            {
+                LOCogroup cg = (LOCogroup) childOp;
+                for (LogicalOperator input : cg.getInputs()) {
+                    AffinityGroups.put(input, eg.getEqClasses());
+                    Lineage.put(input, eg.getLineage());
+                }
+            } else {
+                List<LogicalOperator> childOps = plan.getPredecessors(op);
+                for (LogicalOperator lo : childOps) {
+                    AffinityGroups.put(lo, eg.getEqClasses());
+                    Lineage.put(lo, eg.getLineage());
+                }
+            }
+        } catch (Exception e) {
+          e.printStackTrace(System.out);
+          throw new VisitorException("Exception: "+e.getMessage());
+        }
     }
 
-    private boolean checkCompleteness(LogicalOperator op) {
+    private boolean checkCompleteness(LogicalOperator op) throws Exception, VisitorException {
         LineageTracer lineage = Lineage.get(op);
         Lineage.remove(op);
 
-        Map<IdentityHashSet<Tuple>, Integer> affinityGroups = AffinityGroups
+        Collection<IdentityHashSet<Tuple>> affinityGroups = AffinityGroups
                 .get(op);
         AffinityGroups.remove(op);
 
         Map<LOLoad, DataBag> newBaseData = PruneBaseDataConstrainedCoverage(
-                baseData, null, lineage, affinityGroups);
+                baseData, lineage, affinityGroups);
 
         // obtain the derived data
-        DerivedDataVisitor visitor = new DerivedDataVisitor(plan, null,
-                newBaseData, LogToPhyMap, physPlan);
-        try {
-            visitor.visit();
-        } catch (VisitorException e) {
-            log.error(e.getMessage());
-        }
-
+        Map<LogicalOperator, DataBag> derivedData = eg.getData(newBaseData);
         double newCompleteness = MetricEvaluation.getCompleteness(null,
-                visitor.derivedData, visitor.OpToEqClasses, true);
+                derivedData, eg.getLoToEqClassMap(), true);
 
         if (newCompleteness >= completeness) {
             completeness = newCompleteness;
diff --git a/src/org/apache/pig/pen/LocalLogToPhyTranslationVisitor.java b/src/org/apache/pig/pen/LocalLogToPhyTranslationVisitor.java
deleted file mode 100644
index a9f9be76c..000000000
--- a/src/org/apache/pig/pen/LocalLogToPhyTranslationVisitor.java
+++ /dev/null
@@ -1,383 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.pig.pen;
-
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.pig.PigException;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.LogToPhyTranslationVisitor;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.LogicalToPhysicalTranslatorException;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrangeForIllustrate;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
-import org.apache.pig.pen.physicalOperators.POCounter;
-import org.apache.pig.pen.physicalOperators.POCogroup;
-import org.apache.pig.pen.physicalOperators.POCross;
-import org.apache.pig.pen.physicalOperators.POSplit;
-import org.apache.pig.pen.physicalOperators.POSplitOutput;
-import org.apache.pig.pen.physicalOperators.POStreamLocal;
-import org.apache.pig.data.DataType;
-import org.apache.pig.impl.logicalLayer.FrontendException;
-import org.apache.pig.impl.logicalLayer.LOCogroup;
-import org.apache.pig.impl.logicalLayer.LOCross;
-import org.apache.pig.impl.logicalLayer.LOJoin;
-import org.apache.pig.impl.logicalLayer.LOSplit;
-import org.apache.pig.impl.logicalLayer.LOSplitOutput;
-import org.apache.pig.impl.logicalLayer.LOStore;
-import org.apache.pig.impl.logicalLayer.LOStream;
-import org.apache.pig.impl.logicalLayer.LogicalOperator;
-import org.apache.pig.impl.logicalLayer.LogicalPlan;
-import org.apache.pig.impl.logicalLayer.schema.Schema;
-import org.apache.pig.impl.plan.DependencyOrderWalkerWOSeenChk;
-import org.apache.pig.impl.plan.OperatorKey;
-import org.apache.pig.impl.plan.PlanException;
-import org.apache.pig.impl.plan.PlanWalker;
-import org.apache.pig.impl.plan.VisitorException;
-
-
-public class LocalLogToPhyTranslationVisitor extends LogToPhyTranslationVisitor {
-
-    private Log log = LogFactory.getLog(getClass());
-    
-    public LocalLogToPhyTranslationVisitor(LogicalPlan plan) {
-	super(plan);
-	// TODO Auto-generated constructor stub
-    }
-    
-    public Map<LogicalOperator, PhysicalOperator> getLogToPhyMap() {
-	return logToPhyMap;
-    }
-    
-    @Override
-    public void visit(LOCogroup cg) throws VisitorException {
-	String scope = cg.getOperatorKey().scope;
-        List<LogicalOperator> inputs = cg.getInputs();
-        
-        POCogroup poc = new POCogroup(new OperatorKey(scope, nodeGen.getNextNodeId(scope)), cg.getRequestedParallelism());
-        poc.setInner(cg.getInner());
-        currentPlan.add(poc);
-        
-        int count = 0;
-        Byte type = null;
-        for(LogicalOperator lo : inputs) {
-            List<LogicalPlan> plans = (List<LogicalPlan>) cg.getGroupByPlans().get(lo);
-            
-            POLocalRearrangeForIllustrate physOp = new POLocalRearrangeForIllustrate(new OperatorKey(
-                    scope, nodeGen.getNextNodeId(scope)), cg
-                    .getRequestedParallelism());
-            List<PhysicalPlan> exprPlans = new ArrayList<PhysicalPlan>();
-            currentPlans.push(currentPlan);
-            for (LogicalPlan lp : plans) {
-                currentPlan = new PhysicalPlan();
-                PlanWalker<LogicalOperator, LogicalPlan> childWalker = mCurrentWalker
-                        .spawnChildWalker(lp);
-                pushWalker(childWalker);
-                mCurrentWalker.walk(this);
-                exprPlans.add(currentPlan);
-                popWalker();
-
-            }
-            currentPlan = currentPlans.pop();
-            try {
-                physOp.setPlans(exprPlans);
-            } catch (PlanException pe) {
-                throw new VisitorException(pe);
-            }
-            try {
-                physOp.setIndex(count++);
-            } catch (ExecException e1) {
-                throw new VisitorException(e1);
-            }
-            if (plans.size() > 1) {
-                type = DataType.TUPLE;
-                physOp.setKeyType(type);
-            } else {
-                type = exprPlans.get(0).getLeaves().get(0).getResultType();
-                physOp.setKeyType(type);
-            }
-            physOp.setResultType(DataType.TUPLE);
-
-            currentPlan.add(physOp);
-
-            try {
-                currentPlan.connect(logToPhyMap.get(lo), physOp);
-                currentPlan.connect(physOp, poc);
-            } catch (PlanException e) {
-                log.error("Invalid physical operators in the physical plan"
-                        + e.getMessage());
-                throw new VisitorException(e);
-            }
-            
-        }
-        logToPhyMap.put(cg, poc);
-    }
-    
-    @Override
-    public void visit(LOJoin join) throws VisitorException {
-        String scope = join.getOperatorKey().scope;
-        List<LogicalOperator> inputs = join.getInputs();
-        boolean[] innerFlags = join.getInnerFlags();
-
-        // In local mode, LOJoin is achieved by POCogroup followed by a POForEach with flatten
-        // Insert a POCogroup in the place of LOJoin
-        POCogroup poc = new POCogroup(new OperatorKey(scope, nodeGen.getNextNodeId(scope)), join.getRequestedParallelism());
-        poc.setInner(innerFlags);
-        
-        currentPlan.add(poc);
-        
-        // Add innner plans to POCogroup
-        int count = 0;
-        Byte type = null;
-        for(LogicalOperator lo : inputs) {
-            List<LogicalPlan> plans = (List<LogicalPlan>) join.getJoinPlans().get(lo);
-            
-            POLocalRearrangeForIllustrate physOp = new POLocalRearrangeForIllustrate(new OperatorKey(
-                    scope, nodeGen.getNextNodeId(scope)), join
-                    .getRequestedParallelism());
-            List<PhysicalPlan> exprPlans = new ArrayList<PhysicalPlan>();
-            currentPlans.push(currentPlan);
-            for (LogicalPlan lp : plans) {
-                currentPlan = new PhysicalPlan();
-                PlanWalker<LogicalOperator, LogicalPlan> childWalker = mCurrentWalker
-                        .spawnChildWalker(lp);
-                pushWalker(childWalker);
-                mCurrentWalker.walk(this);
-                exprPlans.add(currentPlan);
-                popWalker();
-
-            }
-            currentPlan = currentPlans.pop();
-            try {
-                physOp.setPlans(exprPlans);
-            } catch (PlanException pe) {
-                throw new VisitorException(pe);
-            }
-            try {
-                physOp.setIndex(count++);
-            } catch (ExecException e1) {
-                throw new VisitorException(e1);
-            }
-            if (plans.size() > 1) {
-                type = DataType.TUPLE;
-                physOp.setKeyType(type);
-            } else {
-                type = exprPlans.get(0).getLeaves().get(0).getResultType();
-                physOp.setKeyType(type);
-            }
-            physOp.setResultType(DataType.TUPLE);
-
-            currentPlan.add(physOp);
-
-            try {
-                currentPlan.connect(logToPhyMap.get(lo), physOp);
-                currentPlan.connect(physOp, poc);
-            } catch (PlanException e) {
-                log.error("Invalid physical operators in the physical plan"
-                        + e.getMessage());
-                throw new VisitorException(e);
-            }
-            
-        }
-        
-        // Append POForEach after POCogroup
-        List<Boolean> flattened = new ArrayList<Boolean>();
-        List<PhysicalPlan> eps = new ArrayList<PhysicalPlan>();
-        
-        for (int i=0;i<join.getInputs().size();i++)
-        {
-            PhysicalPlan ep = new PhysicalPlan();
-            POProject prj = new POProject(new OperatorKey(scope,nodeGen.getNextNodeId(scope)));
-            prj.setResultType(DataType.BAG);
-            prj.setColumn(i+1);
-            prj.setOverloaded(false);
-            prj.setStar(false);
-            ep.add(prj);
-            eps.add(ep);
-            // the parser would have marked the side
-            // where we need to keep empty bags on
-            // non matched as outer (innerFlags[i] would be
-            // false)
-            if(!(innerFlags[i])) {
-                LogicalOperator joinInput = inputs.get(i);
-                // for outer join add a bincond
-                // which will project nulls when bag is
-                // empty
-                try {
-                    updateWithEmptyBagCheck(ep, joinInput);
-                } catch (PlanException e) {
-                    throw new VisitorException(e);
-                }
-            }
-            flattened.add(true);
-        }
-        
-        POForEach fe = new POForEach(new OperatorKey(scope,nodeGen.getNextNodeId(scope)),-1,eps,flattened);
-        
-        fe.setResultType(DataType.BAG);
-
-        currentPlan.add(fe);
-        logToPhyMap.put(join, fe);
-        try {
-            currentPlan.connect(poc, fe);
-        } catch (PlanException e) {
-            int errCode = 2015;
-            String msg = "Invalid physical operators in the physical plan" ;
-            throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
-        }
-    }
-    
-    @Override
-    public void visit(LOSplit split) throws VisitorException {
-	String scope = split.getOperatorKey().scope;
-        PhysicalOperator physOp = new POSplit(new OperatorKey(scope, nodeGen
-                .getNextNodeId(scope)), split.getRequestedParallelism());
-        
-        logToPhyMap.put(split, physOp);
-
-        currentPlan.add(physOp);
-        PhysicalOperator from = logToPhyMap.get(split.getPlan()
-                .getPredecessors(split).get(0));
-        try {
-            currentPlan.connect(from, physOp);
-        } catch (PlanException e) {
-            log.error("Invalid physical operator in the plan" + e.getMessage());
-            throw new VisitorException(e);
-        }
-    }
-    
-    @Override
-    public void visit(LOSplitOutput split) throws VisitorException {
-	String scope = split.getOperatorKey().scope;
-        PhysicalOperator physOp = new POSplitOutput(new OperatorKey(scope, nodeGen
-                .getNextNodeId(scope)), split.getRequestedParallelism());
-        logToPhyMap.put(split, physOp);
-
-        currentPlan.add(physOp);
-        currentPlans.push(currentPlan);
-        currentPlan = new PhysicalPlan();
-        PlanWalker<LogicalOperator, LogicalPlan> childWalker = mCurrentWalker
-                .spawnChildWalker(split.getConditionPlan());
-        pushWalker(childWalker);
-        mCurrentWalker.walk(this);
-        popWalker();
-
-        ((POSplitOutput) physOp).setPlan(currentPlan);
-        currentPlan = currentPlans.pop();
-        currentPlan.add(physOp);
-        PhysicalOperator from = logToPhyMap.get(split.getPlan()
-                .getPredecessors(split).get(0));
-        try {
-            currentPlan.connect(from, physOp);
-        } catch (PlanException e) {
-            log.error("Invalid physical operator in the plan" + e.getMessage());
-            throw new VisitorException(e);
-        }
-    }
-    
-    @Override
-    public void visit(LOStream stream) throws VisitorException {
-        String scope = stream.getOperatorKey().scope;
-        POStreamLocal poStream = new POStreamLocal(new OperatorKey(scope, nodeGen
-                .getNextNodeId(scope)), stream.getExecutableManager(), 
-                stream.getStreamingCommand(), pc.getProperties());
-        currentPlan.add(poStream);
-        logToPhyMap.put(stream, poStream);
-        
-        List<LogicalOperator> op = stream.getPlan().getPredecessors(stream);
-
-        PhysicalOperator from = logToPhyMap.get(op.get(0));
-        try {
-            currentPlan.connect(from, poStream);
-        } catch (PlanException e) {
-            log.error("Invalid physical operators in the physical plan"
-                    + e.getMessage());
-            throw new VisitorException(e);
-        }
-    }
-    
-    @Override
-    public void visit(LOCross cross) throws VisitorException {
-        String scope = cross.getOperatorKey().scope;
-        
-        POCross pocross = new POCross(new OperatorKey(scope, nodeGen.getNextNodeId(scope)));
-        logToPhyMap.put(cross, pocross);
-        currentPlan.add(pocross);
-        
-        
-        for(LogicalOperator in : cross.getInputs()) {
-            PhysicalOperator from = logToPhyMap.get(in);
-            try {
-                currentPlan.connect(from, pocross);
-            } catch (PlanException e) {
-                log.error("Invalid physical operators in the physical plan"
-                        + e.getMessage());
-                throw new VisitorException(e);
-            }
-        }
-        //currentPlan.explain(System.out);
-    }
-    
-    @Override
-    public void visit(LOStore loStore) throws VisitorException {
-        String scope = loStore.getOperatorKey().scope;
-        POStore store = new POStore(new OperatorKey(scope, nodeGen
-                .getNextNodeId(scope)));
-        store.setSFile(loStore.getOutputFile());
-        store.setInputSpec(loStore.getInputSpec());
-        try {
-            // create a new schema for ourselves so that when
-            // we serialize we are not serializing objects that
-            // contain the schema - apparently Java tries to
-            // serialize the object containing the schema if
-            // we are trying to serialize the schema reference in
-            // the containing object. The schema here will be serialized
-            // in JobControlCompiler
-            store.setSchema(new Schema(loStore.getSchema()));
-        } catch (FrontendException e1) {
-            int errorCode = 1060;
-            String message = "Cannot resolve Store output schema";  
-            throw new VisitorException(message, errorCode, PigException.BUG, e1);    
-        }
-        //store.setPc(pc);
-        currentPlan.add(store);
-        PhysicalOperator from = logToPhyMap.get(loStore
-                .getPlan().getPredecessors(loStore).get(0));
-        
-        POCounter counter = new POCounter(new OperatorKey(scope, nodeGen.getNextNodeId(scope)));
-        currentPlan.add(counter);
-        try {
-            currentPlan.connect(from, counter);
-            currentPlan.connect(counter, store);
-        } catch (PlanException e) {
-            int errCode = 2015;
-            String msg = "Invalid physical operators in the physical plan" ;
-            throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
-        }
-        logToPhyMap.put(loStore, store);
-        
-    }
-
-}
diff --git a/src/org/apache/pig/pen/LocalMapReduceSimulator.java b/src/org/apache/pig/pen/LocalMapReduceSimulator.java
new file mode 100644
index 000000000..d432918dd
--- /dev/null
+++ b/src/org/apache/pig/pen/LocalMapReduceSimulator.java
@@ -0,0 +1,257 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.pig.pen;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.Map;
+import java.io.IOException;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.Writable;
+import org.apache.hadoop.mapred.jobcontrol.Job;
+import org.apache.hadoop.mapred.jobcontrol.JobControl;
+import org.apache.hadoop.mapreduce.Mapper;
+import org.apache.hadoop.mapreduce.Reducer;
+import org.apache.pig.backend.executionengine.ExecException;
+import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
+import org.apache.pig.backend.hadoop.executionengine.HExecutionEngine;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOper;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigSplit;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.MROperPlan;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.util.PlanHelper;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
+import org.apache.pig.data.DataBag;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.impl.PigContext;
+import org.apache.pig.impl.io.FileLocalizer;
+import org.apache.pig.impl.io.NullableTuple;
+import org.apache.pig.impl.io.PigNullableWritable;
+import org.apache.pig.impl.logicalLayer.LOLoad;
+import org.apache.pig.impl.logicalLayer.LogicalOperator;
+import org.apache.pig.impl.plan.DepthFirstWalker;
+import org.apache.pig.impl.plan.OperatorKey;
+import org.apache.pig.impl.plan.VisitorException;
+import org.apache.pig.impl.util.ConfigurationValidator;
+import org.apache.pig.impl.util.ObjectSerializer;
+import org.apache.pig.impl.util.Pair;
+import org.apache.pig.impl.builtin.ReadScalars;
+import org.apache.pig.pen.util.LineageTracer;
+import org.apache.pig.PigException;
+
+
+
+/**
+ * Main class that launches pig for Map Reduce
+ *
+ */
+public class LocalMapReduceSimulator {
+    
+    private MapReduceLauncher launcher = new MapReduceLauncher();
+    
+    private Map<PhysicalOperator, PhysicalOperator> phyToMRMap = new HashMap<PhysicalOperator, PhysicalOperator>();;
+
+    @SuppressWarnings("unchecked")
+    public void launchPig(PhysicalPlan php, Map<LOLoad, DataBag> baseData,
+                              Map<PhysicalOperator, LogicalOperator> poLoadToLogMap,
+                              LineageTracer lineage,
+                              IllustratorAttacher attacher,
+                              ExampleGenerator eg,
+                              PigContext pc) throws PigException, IOException, InterruptedException {
+        phyToMRMap.clear();
+        MROperPlan mrp = launcher.compile(php, pc);
+                
+        HExecutionEngine exe = pc.getExecutionEngine();
+        ConfigurationValidator.validatePigProperties(exe.getConfiguration());
+        Configuration conf = ConfigurationUtil.toConfiguration(exe.getConfiguration());
+        
+        JobControlCompiler jcc = new JobControlCompiler(pc, conf);
+        
+        JobControl jc;
+        int numMRJobsCompl = 0;
+        DataBag input;
+        List<Pair<PigNullableWritable, Writable>> intermediateData = new ArrayList<Pair<PigNullableWritable, Writable>>();
+
+        Map<Job, MapReduceOper> jobToMroMap = jcc.getJobMroMap();
+        HashMap<String, DataBag> output = new HashMap<String, DataBag>();
+        Configuration jobConf;
+        // jc is null only when mrp.size == 0
+        boolean needFileInput;
+        final ArrayList<OperatorKey> emptyInpTargets = new ArrayList<OperatorKey>();
+        while(mrp.size() != 0) {
+            jc = jcc.compile(mrp, "Illustrator");
+            if(jc == null) {
+                throw new ExecException("Native execution is not supported");
+            }
+            List<Job> jobs = jc.getWaitingJobs();
+            for (Job job : jobs) {
+                jobConf = job.getJobConf();
+                FileLocalizer.setInitialized(false);
+                ArrayList<ArrayList<OperatorKey>> inpTargets =
+                    (ArrayList<ArrayList<OperatorKey>>)
+                      ObjectSerializer.deserialize(jobConf.get("pig.inpTargets"));
+                intermediateData.clear();
+                MapReduceOper mro = jobToMroMap.get(job);
+                PigSplit split = null;
+                List<POStore> stores = null;
+                PhysicalOperator pack = null;
+                // revisit as there are new physical operators from MR compilation 
+                if (!mro.mapPlan.isEmpty())
+                    attacher.revisit(mro.mapPlan);
+                if (!mro.reducePlan.isEmpty()) {
+                    attacher.revisit(mro.reducePlan);
+                    pack = mro.reducePlan.getRoots().get(0);
+                }
+                
+                List<POLoad> lds = PlanHelper.getLoads(mro.mapPlan);
+                if (!mro.mapPlan.isEmpty()) {
+                    stores = PlanHelper.getStores(mro.mapPlan);
+                }
+                if (!mro.reducePlan.isEmpty()) {
+                    if (stores == null)
+                        stores = PlanHelper.getStores(mro.reducePlan);
+                    else
+                        stores.addAll(PlanHelper.getStores(mro.reducePlan));
+                }
+
+                for (POStore store : stores) {
+                    output.put(store.getSFile().getFileName(), attacher.getDataMap().get(store));
+                }
+               
+                OutputAttacher oa = new OutputAttacher(mro.mapPlan, output);
+                oa.visit();
+                
+                if (!mro.reducePlan.isEmpty()) {
+                    oa = new OutputAttacher(mro.reducePlan, output);
+                    oa.visit();
+                }
+                int index = 0;
+                for (POLoad ld : lds) {
+                    input = output.get(ld.getLFile().getFileName());
+                    if (input == null && baseData != null) {
+                        for (LogicalOperator lo : baseData.keySet()) {
+                            if (((LOLoad) lo).getSchemaFile().equals(ld.getLFile().getFileName()))
+                            {
+                                 input = baseData.get(lo);
+                                 break;
+                            }
+                        }
+                    }
+                    if (input != null)
+                        mro.mapPlan.remove(ld);
+                }
+                for (POLoad ld : lds) {
+                    // check newly generated data first
+                    input = output.get(ld.getLFile().getFileName());
+                    if (input == null && baseData != null) {
+                        if (input == null && baseData != null) {
+                            for (LogicalOperator lo : baseData.keySet()) {
+                                if (((LOLoad) lo).getSchemaFile().equals(ld.getLFile().getFileName()))
+                                {
+                                     input = baseData.get(lo);
+                                     break;
+                                }
+                            }
+                        } 
+                    }
+                    needFileInput = (input == null);
+                    split = new PigSplit(null, index, needFileInput ? emptyInpTargets : inpTargets.get(index), 0);
+                    ++index;
+                    Mapper<Text, Tuple, PigNullableWritable, Writable> map;
+                    
+                    if (mro.reducePlan.isEmpty()) {
+                        // map-only
+                        map = new PigMapOnly.Map();
+                        ((PigMapBase) map).setMapPlan(mro.mapPlan);
+                        Mapper<Text, Tuple, PigNullableWritable, Writable>.Context context = ((PigMapOnly.Map) map)
+                          .getIllustratorContext(jobConf, input, intermediateData, split);
+                        map.run(context);
+                    } else {
+                        if ("true".equals(jobConf.get("pig.usercomparator")))
+                            map = new PigMapReduce.MapWithComparator();
+                        else if (!"".equals(jobConf.get("pig.keyDistFile", "")))
+                            map = new PigMapReduce.MapWithPartitionIndex();
+                        else
+                            map = new PigMapReduce.Map();
+                        Mapper<Text, Tuple, PigNullableWritable, Writable>.Context context = ((PigMapBase) map)
+                          .getIllustratorContext(jobConf, input, intermediateData, split);
+                        ((PigMapBase) map).setMapPlan(mro.mapPlan);
+                        map.run(context);
+                    }
+                }
+                
+                if (!mro.reducePlan.isEmpty())
+                {
+                    if (pack instanceof POPackage)
+                        mro.reducePlan.remove(pack);
+                    // reducer run
+                    PigMapReduce.Reduce reduce;
+                    if ("true".equals(jobConf.get("pig.usercomparator")))
+                        reduce = new PigMapReduce.ReduceWithComparator();
+                    else
+                        reduce = new PigMapReduce.Reduce();
+                    reduce.setReducePlan(mro.reducePlan);
+                    Reducer<PigNullableWritable, NullableTuple, PigNullableWritable, Writable>.Context
+                        context = reduce.getIllustratorContext(job, intermediateData, (POPackage) pack);
+                    reduce.run(context);
+                }
+                phyToMRMap.putAll(mro.phyToMRMap);
+            }
+            
+            
+            int removedMROp = jcc.updateMROpPlan(new LinkedList<Job>());
+            
+            numMRJobsCompl += removedMROp;
+        }
+                
+        jcc.reset();
+    }
+
+    private class OutputAttacher extends PhyPlanVisitor {
+        private Map<String, DataBag> outputBuffer;
+        OutputAttacher(PhysicalPlan plan, Map<String, DataBag> output) {
+            super(plan, new DepthFirstWalker<PhysicalOperator, PhysicalPlan>(
+                    plan));
+            this.outputBuffer = output;
+        }
+        
+        @Override
+        public void visitUserFunc(POUserFunc userFunc) throws VisitorException {
+            if (userFunc.getFunc() != null && userFunc.getFunc() instanceof ReadScalars) {
+                ((ReadScalars) userFunc.getFunc()).setOutputBuffer(outputBuffer);
+            }
+        }
+    }
+    public Map<PhysicalOperator, PhysicalOperator> getPhyToMRMap() {
+        return phyToMRMap;
+    }
+}
diff --git a/src/org/apache/pig/pen/POOptimizeDisabler.java b/src/org/apache/pig/pen/POOptimizeDisabler.java
new file mode 100644
index 000000000..c3f3e2e02
--- /dev/null
+++ b/src/org/apache/pig/pen/POOptimizeDisabler.java
@@ -0,0 +1,27 @@
+package org.apache.pig.pen;
+
+import org.apache.pig.impl.logicalLayer.FrontendException;
+import org.apache.pig.newplan.DependencyOrderWalker;
+import org.apache.pig.newplan.OperatorPlan;
+import org.apache.pig.newplan.logical.relational.LOCogroup;
+import org.apache.pig.newplan.logical.relational.LOJoin;
+import org.apache.pig.newplan.logical.relational.LogicalRelationalNodesVisitor;
+
+public class POOptimizeDisabler extends LogicalRelationalNodesVisitor {
+
+    public POOptimizeDisabler(OperatorPlan plan) throws FrontendException {
+        super(plan, new DependencyOrderWalker(plan));
+    }
+
+    @Override
+    public void visit(LOJoin join) throws FrontendException {
+        // use HASH join only
+        join.resetJoinType();
+    }
+
+    @Override
+    public void visit(LOCogroup cg) throws FrontendException {
+        // use regular group only
+        cg.resetGroupType();
+    }
+}
diff --git a/src/org/apache/pig/pen/PhysicalPlanResetter.java b/src/org/apache/pig/pen/PhysicalPlanResetter.java
new file mode 100644
index 000000000..f50fba74f
--- /dev/null
+++ b/src/org/apache/pig/pen/PhysicalPlanResetter.java
@@ -0,0 +1,53 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.pig.pen;
+
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.pig.PigException;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOper;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POJoinPackage;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackageLite;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCombinerPackage;
+import org.apache.pig.impl.plan.DepthFirstWalker;
+import org.apache.pig.impl.plan.VisitorException;
+import org.apache.pig.impl.plan.optimizer.OptimizerException;
+import org.apache.pig.impl.util.Pair;
+
+/**
+ * This visitor visits the physical plan and resets it for next MRCompilation
+ */
+public class PhysicalPlanResetter extends PhyPlanVisitor {
+    
+    public PhysicalPlanResetter(PhysicalPlan plan) {
+        super(plan, new DepthFirstWalker<PhysicalOperator, PhysicalPlan>(plan));
+    }
+    
+    @Override
+    public void visitPackage(POPackage pkg) throws VisitorException {
+        pkg.setKeyInfo(null);
+    }
+}
diff --git a/src/org/apache/pig/pen/physicalOperators/POCogroup.java b/src/org/apache/pig/pen/physicalOperators/POCogroup.java
deleted file mode 100644
index a6d70cb0c..000000000
--- a/src/org/apache/pig/pen/physicalOperators/POCogroup.java
+++ /dev/null
@@ -1,270 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.pig.pen.physicalOperators;
-
-import java.util.Comparator;
-import java.util.Iterator;
-import java.util.List;
-
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
-import org.apache.pig.data.BagFactory;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataType;
-import org.apache.pig.data.SortedDataBag;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.data.TupleFactory;
-import org.apache.pig.impl.plan.OperatorKey;
-import org.apache.pig.impl.plan.VisitorException;
-import org.apache.pig.pen.util.ExampleTuple;
-
-/** This is a local implementation of Cogroup.
- * The inputs need to be connected to LocalRearranges possibly by the
- * logical to physical translator.
- * 
- * This is a blocking operator. The outputs of LRs are put into
- * SortedDataBags. They are sorted on the keys. We then start pulling
- * tuple out of these bags and start constructing output.
- * 
- *
- */
-
-//We intentionally skip type checking in backend for performance reasons
-@SuppressWarnings("unchecked")
-public class POCogroup extends PhysicalOperator {
-    private static final long serialVersionUID = 1L;    
-    Tuple[] data = null;
-    transient Iterator<Tuple>[] its = null;
-    boolean[] inner;
-
-    public POCogroup(OperatorKey k) {
-        super(k);
-    }
-
-    public POCogroup(OperatorKey k, int rp) {
-        super(k, rp);
-    }
-
-    public POCogroup(OperatorKey k, List<PhysicalOperator> inp) {
-        super(k, inp);
-    }
-
-    public POCogroup(OperatorKey k, int rp, List<PhysicalOperator> inp) {
-        super(k, rp, inp);
-    }
-    
-    public void setInner(boolean[] inner) {
-        this.inner = inner;
-    }
-
-    @Override
-    public void visit(PhyPlanVisitor v) throws VisitorException {
-        v.visitPenCogroup(this);
-    }
-
-    @Override
-    public String name() {
-        return getAliasString() + "POCogroup"+ ": POCogroup" + "["
-                + DataType.findTypeName(resultType) + "]" + " - "
-                + mKey.toString();
-    }
-    
-    @Override
-    public Result getNext(Tuple t) throws ExecException{
-	if(its == null) {
-	    accumulateData();
-	}
-	
-	boolean done = true;
-	Result res = new Result();
-	for(int i = 0; i < data.length; i++) {
-	    done &= (data[i] == null);
-	}
-	if(done) {
-	    res.returnStatus = POStatus.STATUS_EOP;
-	    its = null;
-	    return res;
-	}
-	
-	Tuple smallestTuple = getSmallest(data);
-	Comparator<Tuple> comp = new groupComparator();
-	
-	int size = data.length;
-	
-	Tuple output = TupleFactory.getInstance().newTuple(size + 1);
-	
-	output.set(0, smallestTuple.get(1));
-	for(int i = 1; i < size + 1; i++) {
-	    output.set(i, BagFactory.getInstance().newDefaultBag());
-	}
-	ExampleTuple tOut = null;
-	if(lineageTracer != null) {
-	    tOut = new ExampleTuple(output);
-	    lineageTracer.insert(tOut);
-	}
-	
-	boolean loop = true;
-	
-	while(loop) {
-	    loop = false;
-	    for(int i = 0; i < size; i++) {
-		if(data[i] != null && comp.compare(data[i], smallestTuple) == 0) {
-		    loop = true;
-		    DataBag bag = (DataBag) output.get(i + 1);
-		    //update lineage if it exists
-		    //Tuple temp = ((IndexedTuple) data[i].get(1)).toTuple();
-		    Tuple temp = (Tuple) data[i].get(2);
-		    if(lineageTracer != null) {
-			if(((ExampleTuple)temp).synthetic) tOut.synthetic = true;
-			lineageTracer.union(temp, tOut);
-		    }
-		    //bag.add(((IndexedTuple) data[i].get(1)).toTuple());
-		    bag.add(temp);
-		    if(its[i].hasNext()) 
-			data[i] = its[i].next();
-		    else
-			data[i] = null;
-			
-		    
-		}
-	    }
-	}
-	if(lineageTracer != null)
-	    res.result = tOut;
-	else
-	    res.result = output;
-	
-	res.returnStatus = POStatus.STATUS_OK;
-//    System.out.println(output);
-	for(int i = 0; i < size; i++) {
-	    if(inner != null && inner[i] && ((DataBag)output.get(i+1)).size() == 0) {
-	        res.returnStatus = POStatus.STATUS_NULL;
-	        break;
-	    }
-	}
-	
-	
-	return res;
-    }
-    
-    private void accumulateData() throws ExecException {
-	int size = inputs.size();
-	its = new Iterator[size];
-	data = new Tuple[size];
-	for(int i = 0; i < size; i++) {
-	    DataBag bag = new SortedDataBag(new groupComparator());
-	    for(Result input = inputs.get(i).getNext(dummyTuple); input.returnStatus != POStatus.STATUS_EOP; input = inputs.get(i).getNext(dummyTuple)) {
-	        if(input.returnStatus == POStatus.STATUS_ERR) {
-	            throw new ExecException("Error accumulating output at local Cogroup operator");
-	        }
-	        if(input.returnStatus == POStatus.STATUS_NULL)
-	            continue;
-	        bag.add((Tuple) input.result);
-	    }
-	    
-	    its[i] = bag.iterator();
-	    data[i] = its[i].next();
-	}
-
-    }
-    
-//    private Tuple getSmallest(Tuple[] data) {
-//	Tuple t = (Tuple) data[0];
-//	Comparator<Tuple> comp = new groupComparator();
-//	for(int i = 1; i < data.length; i++) {
-//	    if(comp.compare(t, (Tuple) data[i]) < 0) 
-//		t = data[i];
-//	}
-//	return t;
-//    }
-    
-    private Tuple getSmallest(Tuple[] data) {
-	Tuple t = null;
-	Comparator<Tuple> comp = new groupComparator();
-	
-	for(int i = 0; i < data.length; i++) {
-	    if(data[i] == null) continue;
-	    if(t == null) {
-		t = data[i];
-		continue; //since the previous data was probably null so we dont really need a comparison
-	    }
-	    if(comp.compare(t, data[i]) > 0) 
-		t = data[i];
-	}
-	return t;
-    }
-
-    @Override
-    public boolean supportsMultipleInputs() {
-	// TODO Auto-generated method stub
-	return true;
-    }
-
-    @Override
-    public boolean supportsMultipleOutputs() {
-	// TODO Auto-generated method stub
-	return false;
-    }
-    
-    private static class groupComparator implements Comparator<Tuple> {
-
-	public int compare(Tuple o1, Tuple o2) {
-	    //We want to make it as efficient as possible by only comparing the keys
-	    Object t1 = null;
-	    Object t2 = null;
-	    try {
-        // get the keys
-        t1 = o1.get(1);
-        t2 = o2.get(1);
-        if(t1 == t2 && t1 == null) {
-            // null keys from different inputs
-            // are not treated as equals
-            int firstInputIndex = (Byte)(o1.get(0));
-            int secondInputIndex = (Byte)(o2.get(0));
-            return firstInputIndex - secondInputIndex;
-        }
-	    } catch (ExecException e) {
-		// TODO Auto-generated catch block
-		throw new RuntimeException("Error comparing tuples");
-	    }
-	    
-	    int result = DataType.compare(t1, t2);
-	    
-	    // Further check if any field is null
-        // See PIG-927
-	    if (result == 0 && t1 instanceof Tuple && t2 instanceof Tuple)
-	    {
-	        try {
-    	        int firstInputIndex = (Byte)(o1.get(0));
-                int secondInputIndex = (Byte)(o2.get(0));
-    	        for (int i=0;i<((Tuple)t1).size();i++)
-                    if (((Tuple)t1).get(i)==null)
-                        return firstInputIndex - secondInputIndex;
-            } catch (ExecException e) {
-                throw new RuntimeException("Error comparing tuple fields", e);
-            }
-	    }
-	    return result;
-	}
-	
-    }
-
-}
diff --git a/src/org/apache/pig/pen/physicalOperators/POCounter.java b/src/org/apache/pig/pen/physicalOperators/POCounter.java
deleted file mode 100644
index 340c84f79..000000000
--- a/src/org/apache/pig/pen/physicalOperators/POCounter.java
+++ /dev/null
@@ -1,105 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.pig.pen.physicalOperators;
-
-import java.util.List;
-import java.util.Map;
-
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.impl.plan.OperatorKey;
-import org.apache.pig.impl.plan.VisitorException;
-
-public class POCounter extends PhysicalOperator {
-
-	static final long serialVersionUID = 1L;
-
-    private long count = 0;
-    
-    public POCounter(OperatorKey k, int rp, List<PhysicalOperator> inp) {
-        super(k, rp, inp);
-        // TODO Auto-generated constructor stub
-    }
-
-    public POCounter(OperatorKey k, int rp) {
-        super(k, rp);
-        // TODO Auto-generated constructor stub
-    }
-
-    public POCounter(OperatorKey k, List<PhysicalOperator> inp) {
-        super(k, inp);
-        // TODO Auto-generated constructor stub
-    }
-
-    public POCounter(OperatorKey k) {
-        super(k);
-        // TODO Auto-generated constructor stub
-    }
-
-    @Override
-    public void visit(PhyPlanVisitor v) throws VisitorException {
-        // TODO Auto-generated method stub
-
-    }
-
-    
-    @Override
-    public Result getNext(Tuple t) throws ExecException {
-        // TODO Auto-generated method stub
-        return getNext(processInput());
-    }
-    
-    private Result getNext(Result res) {
-        //System.out.println("Status = " + res.returnStatus);
-        if(res.returnStatus == POStatus.STATUS_OK) {
-            //System.out.println("Incrementing counter");
-            count++;
-        }
-        return res;
-    }
-
-    @Override
-    public String name() {
-        // TODO Auto-generated method stub
-        return "POCounter - " + mKey.toString();
-    }
-
-    @Override
-    public boolean supportsMultipleInputs() {
-        // TODO Auto-generated method stub
-        return false;
-    }
-
-    @Override
-    public boolean supportsMultipleOutputs() {
-        // TODO Auto-generated method stub
-        return false;
-    }
-    
-    public long getCount() {
-        return count;
-    }
-
-}
diff --git a/src/org/apache/pig/pen/physicalOperators/POCross.java b/src/org/apache/pig/pen/physicalOperators/POCross.java
deleted file mode 100644
index 986de8fed..000000000
--- a/src/org/apache/pig/pen/physicalOperators/POCross.java
+++ /dev/null
@@ -1,194 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.pig.pen.physicalOperators;
-
-import java.util.Iterator;
-import java.util.List;
-
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
-import org.apache.pig.data.BagFactory;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.data.TupleFactory;
-import org.apache.pig.impl.plan.OperatorKey;
-import org.apache.pig.impl.plan.VisitorException;
-import org.apache.pig.pen.util.ExampleTuple;
-
-/**
- * This is a local implementation of the cross. Its a blocking operator.
- * It accumulates inputs into databags and then applies logic similar to 
- * foreach flatten(*) to get the output tuples
- * 
- *
- */
-public class POCross extends PhysicalOperator {
-    private static final long serialVersionUID = 1L;    
-    DataBag [] inputBags;
-    Tuple [] data;
-    transient Iterator [] its;
-
-    public POCross(OperatorKey k) {
-        super(k);
-        // TODO Auto-generated constructor stub
-    }
-
-    public POCross(OperatorKey k, int rp) {
-        super(k, rp);
-        // TODO Auto-generated constructor stub
-    }
-
-    public POCross(OperatorKey k, List<PhysicalOperator> inp) {
-        super(k, inp);
-        // TODO Auto-generated constructor stub
-    }
-
-    public POCross(OperatorKey k, int rp, List<PhysicalOperator> inp) {
-        super(k, rp, inp);
-        // TODO Auto-generated constructor stub
-    }
-
-    @Override
-    public void visit(PhyPlanVisitor v) throws VisitorException {
-        // TODO Auto-generated method stub
-        v.visitPenCross(this);
-
-    }
-    
-    @Override
-    public Result getNext(Tuple t) throws ExecException {
-        Result res = new Result();
-        int noItems = inputs.size();
-        if(inputBags == null) {
-            accumulateData();
-        }
-        
-        if(its != null) {
-            //we check if we are done with processing
-            //we do that by checking if all the iterators are used up
-            boolean finished = true;
-            for(int i = 0; i < its.length; i++) {
-                finished &= !its[i].hasNext();
-            }
-            if(finished) {
-                res.returnStatus = POStatus.STATUS_EOP;
-                return res;
-            }
-            
-        }
-        
-        if(data == null) {
-            //getNext being called for the first time or starting on new input data
-            //we instantiate the template array and start populating it with data
-            data = new Tuple[noItems];
-            for(int i = 0; i < noItems; ++i) {
-                data[i] = (Tuple) its[i].next();
-
-            }
-            res.result = CreateTuple(data);
-            res.returnStatus = POStatus.STATUS_OK;
-            return res;
-        } else {
-            for(int index = noItems - 1; index >= 0; --index) {
-                if(its[index].hasNext()) {
-                    data[index] =  (Tuple) its[index].next();
-                    res.result = CreateTuple(data);
-                    res.returnStatus = POStatus.STATUS_OK;
-                    return res;
-                }
-                else{
-                    // reset this index's iterator so cross product can be achieved
-                    // we would be resetting this way only for the indexes from the end
-                    // when the first index which needs to be flattened has reached the
-                    // last element in its iterator, we won't come here - instead, we reset
-                    // all iterators at the beginning of this method.
-                    its[index] = (inputBags[index]).iterator();
-                    data[index] = (Tuple) its[index].next();
-                }
-
-            }
-        }
-        
-        return null;
-    }
-    
-    private void accumulateData() throws ExecException {
-        int count = 0;
-        inputBags = new DataBag[inputs.size()];
-        
-        its = new Iterator[inputs.size()];
-        for(PhysicalOperator op : inputs) {
-            DataBag bag = BagFactory.getInstance().newDefaultBag();
-            inputBags[count] = bag;
-            for(Result res = op.getNext(dummyTuple); res.returnStatus != POStatus.STATUS_EOP; res = op.getNext(dummyTuple)) {
-                if(res.returnStatus == POStatus.STATUS_NULL)
-                    continue;
-                if(res.returnStatus == POStatus.STATUS_ERR)
-                    throw new ExecException("Error accumulating data in the local Cross operator");
-                if(res.returnStatus == POStatus.STATUS_OK)
-                    bag.add((Tuple) res.result);
-            }
-            its[count++] = bag.iterator();
-        }
-    }
-    
-    private Tuple CreateTuple(Tuple[] data) throws ExecException {
-        Tuple out =  TupleFactory.getInstance().newTuple();
-        
-        for(int i = 0; i < data.length; ++i) {
-            Tuple t = data[i];
-            int size = t.size();
-            for(int j = 0; j < size; ++j) {
-                out.append(t.get(j));
-            }
-
-        }
-        
-        if(lineageTracer != null) {
-            ExampleTuple tOut = new ExampleTuple();
-            tOut.reference(out);
-            lineageTracer.insert(tOut);
-            for(int i = 0; i < data.length; i++) {
-                lineageTracer.union(tOut, data[i]);
-            }
-            return tOut;
-        }
-        return out;
-    }
-
-    @Override
-    public String name() {
-        return getAliasString() + "POCrossLocal" + " - " + mKey.toString();
-    }
-
-    @Override
-    public boolean supportsMultipleInputs() {
-        // TODO Auto-generated method stub
-        return true;
-    }
-
-    @Override
-    public boolean supportsMultipleOutputs() {
-        // TODO Auto-generated method stub
-        return false;
-    }
-
-}
diff --git a/src/org/apache/pig/pen/physicalOperators/POSplit.java b/src/org/apache/pig/pen/physicalOperators/POSplit.java
deleted file mode 100644
index 801c1bef2..000000000
--- a/src/org/apache/pig/pen/physicalOperators/POSplit.java
+++ /dev/null
@@ -1,107 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.pig.pen.physicalOperators;
-
-import java.util.List;
-
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
-import org.apache.pig.data.BagFactory;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.impl.plan.OperatorKey;
-import org.apache.pig.impl.plan.VisitorException;
-
-public class POSplit extends PhysicalOperator {
-    
-    
-    /**
-     * POSplit is a blocking operator. It reads the data from its input into a databag and then returns the iterator
-     * of that bag to POSplitOutputs which do the necessary filtering
-     */
-    private static final long serialVersionUID = 1L;
-
-    DataBag data = null;
-    
-    boolean processingDone = false;
-
-    public POSplit(OperatorKey k, int rp, List<PhysicalOperator> inp) {
-	super(k, rp, inp);
-	// TODO Auto-generated constructor stub
-	data = BagFactory.getInstance().newDefaultBag();
-    }
-
-    public POSplit(OperatorKey k, int rp) {
-	this(k, rp, null);
-    }
-
-    public POSplit(OperatorKey k, List<PhysicalOperator> inp) {
-	
-	this(k, -1, inp);
-    }
-
-    public POSplit(OperatorKey k) {
-	 this(k, -1, null);
-    }
-
-    public Result getNext(Tuple t) throws ExecException{
-	if(!processingDone) {
-	    for(Result input = inputs.get(0).getNext(dummyTuple); input.returnStatus != POStatus.STATUS_EOP; input = inputs.get(0).getNext(dummyTuple)) {
-		if(input.returnStatus == POStatus.STATUS_ERR) {
-		    throw new ExecException("Error accumulating output at local Split operator");
-		}
-
-                if (input.returnStatus != POStatus.STATUS_NULL) {
-                    data.add((Tuple) input.result);
-                }
-	    }
-	    processingDone = true;
-	}
-
-	Result res = new Result();
-	res.returnStatus = POStatus.STATUS_OK;
-	res.result = data.iterator();
-	return res;
-    }
-
-    @Override
-    public void visit(PhyPlanVisitor v) throws VisitorException {
-	v.visitPenSplit(this);
-    }
-
-    @Override
-    public String name() {
-	return "Split - " + mKey.toString();
-    }
-
-    @Override
-    public boolean supportsMultipleInputs() {
-	// TODO Auto-generated method stub
-	return false;
-    }
-
-    @Override
-    public boolean supportsMultipleOutputs() {
-	// TODO Auto-generated method stub
-	return true;
-    }
-
-}
diff --git a/src/org/apache/pig/pen/physicalOperators/POSplitOutput.java b/src/org/apache/pig/pen/physicalOperators/POSplitOutput.java
deleted file mode 100644
index 0dfa04ca2..000000000
--- a/src/org/apache/pig/pen/physicalOperators/POSplitOutput.java
+++ /dev/null
@@ -1,131 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.pig.pen.physicalOperators;
-
-import java.util.Iterator;
-import java.util.List;
-
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.impl.plan.OperatorKey;
-import org.apache.pig.impl.plan.VisitorException;
-import org.apache.pig.pen.util.ExampleTuple;
-
-public class POSplitOutput extends PhysicalOperator {
-
-    /**
-     * POSplitOutput reads from POSplit using an iterator
-     */
-    private static final long serialVersionUID = 1L;
-    
-    PhysicalOperator compOp;
-    PhysicalPlan compPlan;
-    transient Iterator<Tuple> it;
-    
-    public POSplitOutput(OperatorKey k, int rp, List<PhysicalOperator> inp) {
-	super(k, rp, inp);
-	// TODO Auto-generated constructor stub
-    }
-
-    public POSplitOutput(OperatorKey k, int rp) {
-	super(k, rp);
-	// TODO Auto-generated constructor stub
-    }
-
-    public POSplitOutput(OperatorKey k, List<PhysicalOperator> inp) {
-	super(k, inp);
-	// TODO Auto-generated constructor stub
-    }
-
-    public POSplitOutput(OperatorKey k) {
-	super(k);
-	// TODO Auto-generated constructor stub
-    }
-
-    @Override
-    public void visit(PhyPlanVisitor v) throws VisitorException {
-	// TODO Auto-generated method stub
-
-    }
-    
-    @SuppressWarnings("unchecked")
-    public Result getNext(Tuple t) throws ExecException {
-	if(it == null) {
-	    PhysicalOperator op = getInputs().get(0);
-	    Result res = getInputs().get(0).getNext(t);
-	    if(res.returnStatus == POStatus.STATUS_OK)
-		it = (Iterator<Tuple>) res.result;
-	}
-	Result res = null;
-	Result inp = new Result();
-	while(true) {
-	    if(it.hasNext())
-		inp.result = it.next();
-	    else {
-		inp.returnStatus = POStatus.STATUS_EOP;
-		return inp;
-	    }
-	    inp.returnStatus = POStatus.STATUS_OK;
-
-	    compPlan.attachInput((Tuple) inp.result);
-
-	    res = compOp.getNext(dummyBool);
-	    if (res.returnStatus != POStatus.STATUS_OK 
-		    && res.returnStatus != POStatus.STATUS_NULL) 
-		return res;
-
-	    if (res.result != null && (Boolean) res.result == true) {
-		if(lineageTracer != null) {
-		    ExampleTuple tIn = (ExampleTuple) inp.result;
-		    lineageTracer.insert(tIn);
-		    lineageTracer.union(tIn, tIn);
-		}
-		return inp;
-	    }
-	}
-        
-    }
-
-    @Override
-    public String name() {
-        return getAliasString() + "POSplitOutput " + mKey.toString();
-    }
-
-    @Override
-    public boolean supportsMultipleInputs() {
-	// TODO Auto-generated method stub
-	return false;
-    }
-
-    @Override
-    public boolean supportsMultipleOutputs() {
-	// TODO Auto-generated method stub
-	return false;
-    }
-    
-    public void setPlan(PhysicalPlan compPlan) {
-	this.compPlan = compPlan;
-	this.compOp = compPlan.getLeaves().get(0);
-    }
-
-}
diff --git a/src/org/apache/pig/pen/physicalOperators/POStreamLocal.java b/src/org/apache/pig/pen/physicalOperators/POStreamLocal.java
deleted file mode 100644
index c2bb7d6aa..000000000
--- a/src/org/apache/pig/pen/physicalOperators/POStreamLocal.java
+++ /dev/null
@@ -1,168 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.pig.pen.physicalOperators;
-
-import java.util.Properties;
-
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStream;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.impl.plan.OperatorKey;
-import org.apache.pig.impl.streaming.ExecutableManager;
-import org.apache.pig.impl.streaming.StreamingCommand;
-
-public class POStreamLocal extends POStream {
-
-    /**
-     * 
-     */
-    private static final long serialVersionUID = 2L;
-
-    public POStreamLocal(OperatorKey k, ExecutableManager executableManager,
-            StreamingCommand command, Properties properties) {
-        super(k, executableManager, command, properties);
-        // TODO Auto-generated constructor stub
-    }
-    
-    
-    /**
-     * This is different from the Map-Reduce implementation of the POStream since there is no
-     * push model here. POStatus_EOP signals the end of input and can be used to decide when 
-     * to stop the stdin to the process
-     */
-    @Override
-    public Result getNext(Tuple t) throws ExecException {
-        // The POStream Operator works with ExecutableManager to
-        // send input to the streaming binary and to get output
-        // from it. To achieve a tuple oriented behavior, two queues
-        // are used - one for output from the binary and one for 
-        // input to the binary. In each getNext() call:
-        // 1) If there is no more output expected from the binary, an EOP is
-        // sent to successor
-        // 2) If there is any output from the binary in the queue, it is passed
-        // down to the successor
-        // 3) if neither of these two are true and if it is possible to
-        // send input to the binary, then the next tuple from the
-        // predecessor is got and passed to the binary
-        try {
-            // if we are being called AFTER all output from the streaming 
-            // binary has already been sent to us then just return EOP
-            // The "allOutputFromBinaryProcessed" flag is set when we see
-            // an EOS (End of Stream output) from streaming binary
-            if(allOutputFromBinaryProcessed) {
-                return new Result(POStatus.STATUS_EOP, null);
-            }
-            
-            // if we are here AFTER all map() calls have been completed
-            // AND AFTER we process all possible input to be sent to the
-            // streaming binary, then all we want to do is read output from
-            // the streaming binary
-            if(allInputFromPredecessorConsumed) {
-                Result r = binaryOutputQueue.take();
-                if(r.returnStatus == POStatus.STATUS_EOS) {
-                    // If we received EOS, it means all output
-                    // from the streaming binary has been sent to us
-                    // So we can send an EOP to the successor in
-                    // the pipeline. Also since we are being called
-                    // after all input from predecessor has been processed
-                    // it means we got here from a call from close() in
-                    // map or reduce. So once we send this EOP down, 
-                    // getNext() in POStream should never be called. So
-                    // we don't need to set any flag noting we saw all output
-                    // from binary
-                    r.returnStatus = POStatus.STATUS_EOP;
-                }
-                return(r);
-            }
-            
-            // if we are here, we haven't consumed all input to be sent
-            // to the streaming binary - check if we are being called
-            // from close() on the map or reduce
-            //if(this.parentPlan.endOfAllInput) {
-                Result r = getNextHelper(t);
-                if(r.returnStatus == POStatus.STATUS_EOP) {
-                    // we have now seen *ALL* possible input
-                    // check if we ever had any real input
-                    // in the course of the map/reduce - if we did
-                    // then "initialized" will be true. If not, just
-                    // send EOP down.
-                    if(initialized) {
-                        // signal End of ALL input to the Executable Manager's 
-                        // Input handler thread
-                        binaryInputQueue.put(r);
-                        // note this state for future calls
-                        allInputFromPredecessorConsumed  = true;
-                        // look for output from binary
-                        r = binaryOutputQueue.take();
-                        if(r.returnStatus == POStatus.STATUS_EOS) {
-                            // If we received EOS, it means all output
-                            // from the streaming binary has been sent to us
-                            // So we can send an EOP to the successor in
-                            // the pipeline. Also since we are being called
-                            // after all input from predecessor has been processed
-                            // it means we got here from a call from close() in
-                            // map or reduce. So once we send this EOP down, 
-                            // getNext() in POStream should never be called. So
-                            // we don't need to set any flag noting we saw all output
-                            // from binary
-                            r.returnStatus = POStatus.STATUS_EOP;
-                        }
-                    }
-                    
-                } else if(r.returnStatus == POStatus.STATUS_EOS) {
-                    // If we received EOS, it means all output
-                    // from the streaming binary has been sent to us
-                    // So we can send an EOP to the successor in
-                    // the pipeline. Also we are being called
-                    // from close() in map or reduce (this is so because
-                    // only then this.parentPlan.endOfAllInput is true).
-                    // So once we send this EOP down, getNext() in POStream
-                    // should never be called. So we don't need to set any 
-                    // flag noting we saw all output from binary
-                    r.returnStatus = POStatus.STATUS_EOP;
-                }
-                return r;
-//            } else {
-//                // we are not being called from close() - so
-//                // we must be called from either map() or reduce()
-//                // get the next Result from helper
-//                Result r = getNextHelper(t);
-//                if(r.returnStatus == POStatus.STATUS_EOS) {
-//                    // If we received EOS, it means all output
-//                    // from the streaming binary has been sent to us
-//                    // So we can send an EOP to the successor in
-//                    // the pipeline and also note this condition
-//                    // for future calls
-//                    r.returnStatus = POStatus.STATUS_EOP;
-//                    allOutputFromBinaryProcessed  = true;
-//                }
-//                return r;
-//            }
-            
-        } catch(Exception e) {
-            throw new ExecException("Error while trying to get next result in POStream", e);
-        }
-            
-        
-    }
-
-    
-
-}
diff --git a/src/org/apache/pig/pen/util/DisplayExamples.java b/src/org/apache/pig/pen/util/DisplayExamples.java
index 94f877767..6e3e9f78a 100644
--- a/src/org/apache/pig/pen/util/DisplayExamples.java
+++ b/src/org/apache/pig/pen/util/DisplayExamples.java
@@ -21,13 +21,18 @@ import java.util.Collection;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
+import java.util.Set;
+import java.util.HashSet;
 
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.data.DataBag;
 import org.apache.pig.data.DataType;
 import org.apache.pig.data.Tuple;
 import org.apache.pig.impl.logicalLayer.FrontendException;
+import org.apache.pig.impl.logicalLayer.LOForEach;
 import org.apache.pig.impl.logicalLayer.LogicalOperator;
+import org.apache.pig.impl.logicalLayer.LOStore;
+import org.apache.pig.impl.logicalLayer.LOLimit;
 import org.apache.pig.impl.logicalLayer.LogicalPlan;
 import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;
 import org.apache.pig.impl.util.IdentityHashSet;
@@ -62,26 +67,36 @@ public class DisplayExamples {
     }
 
     public static String printTabular(LogicalPlan lp,
-            Map<LogicalOperator, DataBag> exampleData) {
+            Map<LogicalOperator, DataBag> exampleData,
+            Map<LOForEach, Map<LogicalOperator, DataBag>> forEachInnerLogToDataMap) {
         StringBuffer output = new StringBuffer();
-
-        LogicalOperator currentOp = lp.getLeaves().get(0);
-        printTabular(currentOp, exampleData, output);
+        Set<LogicalOperator> seen = new HashSet<LogicalOperator>();
+        for (LogicalOperator currentOp : lp.getLeaves())
+            printTabular(currentOp, exampleData, forEachInnerLogToDataMap, seen, output);
         return output.toString();
     }
 
     static void printTabular(LogicalOperator op,
-            Map<LogicalOperator, DataBag> exampleData, StringBuffer output) {
-        DataBag bag = exampleData.get(op);
+            Map<LogicalOperator, DataBag> exampleData,
+            Map<LOForEach, Map<LogicalOperator, DataBag>> forEachInnerLogToDataMap,
+            Set<LogicalOperator> seen,
+            StringBuffer output) {
 
         List<LogicalOperator> inputs = op.getPlan().getPredecessors(op);
         if (inputs != null) { // to avoid an exception when op == LOLoad
             for (LogicalOperator Op : inputs) {
-                printTabular(Op, exampleData, output);
+                if (!seen.contains(Op))
+                  printTabular(Op, exampleData, forEachInnerLogToDataMap, seen, output);
             }
         }
+        seen.add(op);
+        // print inner block first
+        if ((op instanceof LOForEach)) {
+            printNestedTabular((LOForEach)op, forEachInnerLogToDataMap, exampleData.get(op), output);
+        }
+        
         if (op.getAlias() != null) {
-            // printTable(op, bag, output);
+            DataBag bag = exampleData.get(op);
             try {
                 DisplayTable(MakeArray(op, bag), op, bag, output);
             } catch (FrontendException e) {
@@ -95,6 +110,45 @@ public class DisplayExamples {
 
     }
 
+    // print out nested gen block in ForEach
+    static void printNestedTabular(LOForEach foreach,
+            Map<LOForEach, Map<LogicalOperator, DataBag>> forEachInnerLogToDataMap,
+            DataBag foreachData,
+            StringBuffer output) {
+        List<LogicalPlan> plans = foreach.getForEachPlans();
+        if (plans != null) {
+            for (LogicalPlan plan : plans) {
+                printNestedTabular(plan.getLeaves().get(0), foreach.getAlias(), foreachData, forEachInnerLogToDataMap.get(foreach), output);
+            }
+        }
+    }
+
+    static void printNestedTabular(LogicalOperator lo, String foreachAlias, DataBag foreachData, 
+            Map<LogicalOperator, DataBag> logToDataMap, StringBuffer output) {
+        
+        List<LogicalOperator> inputs = lo.getPlan().getPredecessors(lo);
+        if (inputs != null) {
+            for (LogicalOperator op : inputs)
+                printNestedTabular(op, foreachAlias, foreachData, logToDataMap, output);
+        }
+        
+        DataBag bag = logToDataMap.get(lo);
+        if (bag == null)
+          return;
+        
+        if (lo.getAlias() != null) {
+            try {
+              DisplayNestedTable(MakeArray(lo, bag), lo, foreachAlias, foreachData, bag, output);
+            } catch (FrontendException e) {
+              // TODO Auto-generated catch block
+              e.printStackTrace();
+            } catch (Exception e) {
+              // TODO Auto-generated catch block
+              e.printStackTrace();
+            }
+        }
+    }
+    
     public static void printSimple(LogicalOperator op,
             Map<LogicalOperator, DataBag> exampleData) {
         DataBag bag = exampleData.get(op);
@@ -126,6 +180,9 @@ public class DisplayExamples {
 
     static void DisplayTable(String[][] table, LogicalOperator op, DataBag bag,
             StringBuffer output) throws FrontendException {
+        if (op instanceof LOStore && ((LOStore) op).isTmpStore())
+            return;
+        
         int cols = op.getSchema().getFields().size();
         List<FieldSchema> fields = op.getSchema().getFields();
         int rows = (int) bag.size();
@@ -136,7 +193,7 @@ public class DisplayExamples {
                 maxColSizes[i] = 5;
         }
         int total = 0;
-        int aliasLength = op.getAlias().length() + 4;
+        int aliasLength = (op instanceof LOStore ? op.getAlias().length() + 12 : op.getAlias().length() + 4);
         for (int j = 0; j < cols; ++j) {
             for (int i = 0; i < rows; ++i) {
                 int length = table[i][j].length();
@@ -146,12 +203,20 @@ public class DisplayExamples {
             total += maxColSizes[j];
         }
 
+        // Note of limit reset
+        if (op instanceof LOLimit) {
+            output.append("\nThe limit now in use, " + ((LOLimit)op).getLimit() + ", may have been changed for ILLUSTRATE purpose.\n");
+        }
+        
         // Display the schema first
         output
                 .append(AddSpaces(total + 3 * (cols + 1) + aliasLength + 1,
                         false)
                         + "\n");
-        output.append("| " + op.getAlias() + AddSpaces(4, true) + " | ");
+        if (op instanceof LOStore)
+            output.append("| Store : " + op.getAlias() + AddSpaces(4, true) + " | ");
+        else
+            output.append("| " + op.getAlias() + AddSpaces(4, true) + " | ");
         for (int i = 0; i < cols; ++i) {
             String field = fields.get(i).toString();
             output.append(field
@@ -178,6 +243,61 @@ public class DisplayExamples {
                         + "\n");
     }
 
+    static void DisplayNestedTable(String[][] table, LogicalOperator op, String foreachAlias, DataBag bag,
+            DataBag foreachData, StringBuffer output) throws FrontendException {
+        int cols = op.getSchema().getFields().size();
+        List<FieldSchema> fields = op.getSchema().getFields();
+        int rows = (int) bag.size();
+        int[] maxColSizes = new int[cols];
+        for (int i = 0; i < cols; ++i) {
+            maxColSizes[i] = fields.get(i).toString().length();
+            if (maxColSizes[i] < 5)
+                maxColSizes[i] = 5;
+        }
+        int total = 0;
+        int aliasLength = op.getAlias().length() + +foreachAlias.length() + 5;
+        for (int j = 0; j < cols; ++j) {
+            for (int i = 0; i < rows; ++i) {
+                int length = table[i][j].length();
+                if (length > maxColSizes[j])
+                    maxColSizes[j] = length;
+            }
+            total += maxColSizes[j];
+        }
+
+        // Display the schema first
+        output
+                .append(AddSpaces(total + 3 * (cols + 1) + aliasLength + 1,
+                        false)
+                        + "\n");
+        output.append("| " + foreachAlias + "." + op.getAlias() + AddSpaces(4, true) + " | ");
+        for (int i = 0; i < cols; ++i) {
+            String field;
+            field = fields.get(i).toString();
+            output.append(field
+                    + AddSpaces(maxColSizes[i] - field.length(), true) + " | ");
+        }
+        output.append("\n"
+                + AddSpaces(total + 3 * (cols + 1) + aliasLength + 1, false)
+                + "\n");
+        // now start displaying the data
+        for (int i = 0; i < rows; ++i) {
+            output.append("| " + AddSpaces(aliasLength, true) + " | ");
+            for (int j = 0; j < cols; ++j) {
+                String str = table[i][j];
+                output.append(str
+                        + AddSpaces(maxColSizes[j] - str.length(), true)
+                        + " | ");
+            }
+            output.append("\n");
+        }
+        // now display the finish line
+        output
+                .append(AddSpaces(total + 3 * (cols + 1) + aliasLength + 1,
+                        false)
+                        + "\n");
+    }
+
     static String[][] MakeArray(LogicalOperator op, DataBag bag)
             throws Exception {
         int rows = (int) bag.size();
@@ -201,8 +321,12 @@ public class DisplayExamples {
         else {
             // System.out.println("Unrecognized data-type received!!!");
             // return null;
-            if (DataType.findTypeName(d) != null)
-                return d.toString();
+            if (DataType.findTypeName(d) != null) {
+                if (d == null)
+                    return "";
+                else
+                    return d.toString();
+            }
         }
         System.out.println("Unrecognized data-type received!!!");
         return null;
diff --git a/src/org/apache/pig/pen/util/ExampleTuple.java b/src/org/apache/pig/pen/util/ExampleTuple.java
index 0a3b0f242..f434972b8 100644
--- a/src/org/apache/pig/pen/util/ExampleTuple.java
+++ b/src/org/apache/pig/pen/util/ExampleTuple.java
@@ -24,9 +24,7 @@ import java.io.IOException;
 import java.util.List;
 
 import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DefaultTuple;
 import org.apache.pig.data.Tuple;
-import org.apache.pig.data.TupleFactory;
 
 //Example tuple adds 2 booleans to Tuple
 //synthetic say whether the tuple was generated synthetically
@@ -36,17 +34,21 @@ public class ExampleTuple implements Tuple {
 
     public boolean synthetic = false;
     public boolean omittable = true;
-    Tuple t;
+    Object expr = null;
+    Tuple t = null;
 
     public ExampleTuple() {
 
     }
+    
+    public ExampleTuple(Object expr) {
+      this.expr = expr;
+    }
 
     public ExampleTuple(Tuple t) {
         // Have to do it like this because Tuple is an interface, we don't
         // have access to its internal structures.
         this.t = t;
-
     }
 
     @Override
diff --git a/src/org/apache/pig/pen/util/LineageTracer.java b/src/org/apache/pig/pen/util/LineageTracer.java
index 74e949a3c..35dbcc472 100644
--- a/src/org/apache/pig/pen/util/LineageTracer.java
+++ b/src/org/apache/pig/pen/util/LineageTracer.java
@@ -20,7 +20,6 @@ package org.apache.pig.pen.util;
 import java.util.*;
 
 import org.apache.pig.data.Tuple;
-import org.apache.pig.impl.util.IdentityHashSet;
 
 public class LineageTracer {
 
diff --git a/src/org/apache/pig/pen/util/MetricEvaluation.java b/src/org/apache/pig/pen/util/MetricEvaluation.java
index 3e4a1cfa8..201d85768 100644
--- a/src/org/apache/pig/pen/util/MetricEvaluation.java
+++ b/src/org/apache/pig/pen/util/MetricEvaluation.java
@@ -19,13 +19,11 @@
 package org.apache.pig.pen.util;
 
 import java.util.Collection;
-import java.util.HashMap;
 import java.util.Iterator;
 import java.util.Map;
 
 import org.apache.pig.data.DataBag;
 import org.apache.pig.data.Tuple;
-import org.apache.pig.impl.logicalLayer.LOFilter;
 import org.apache.pig.impl.logicalLayer.LogicalOperator;
 import org.apache.pig.impl.util.IdentityHashSet;
 
@@ -118,24 +116,13 @@ public class MetricEvaluation {
         int noClasses = 0;
         int noCoveredClasses = 0;
         int noOperators = 0;
-        Map<Integer, Boolean> coveredClasses;
         float completeness = 0;
         if (!overallCompleteness) {
             Collection<IdentityHashSet<Tuple>> eqClasses = OperatorToEqClasses
                     .get(op);
-            DataBag bag;
 
-            if (op instanceof LOFilter)
-                bag = exampleData.get(((LOFilter) op).getInput());
-            else
-                bag = exampleData.get(op);
-            coveredClasses = getCompletenessLogic(bag, eqClasses);
+            noCoveredClasses = getCompletenessLogic(eqClasses);
             noClasses = eqClasses.size();
-            for (Map.Entry<Integer, Boolean> e : coveredClasses.entrySet()) {
-                if (e.getValue()) {
-                    noCoveredClasses++;
-                }
-            }
 
             return 100 * ((float) noCoveredClasses) / (float) noClasses;
         } else {
@@ -150,20 +137,8 @@ public class MetricEvaluation {
                     continue; // we want to consider join a single operator
                 noOperators++;
                 Collection<IdentityHashSet<Tuple>> eqClasses = e.getValue();
-                LogicalOperator lop = e.getKey();
-                DataBag bag;
-                if (lop instanceof LOFilter)
-                    bag = exampleData.get(((LOFilter) lop).getInput());
-                else
-                    bag = exampleData.get(lop);
-                coveredClasses = getCompletenessLogic(bag, eqClasses);
+                noCoveredClasses = getCompletenessLogic(eqClasses);
                 noClasses += eqClasses.size();
-                for (Map.Entry<Integer, Boolean> e_result : coveredClasses
-                        .entrySet()) {
-                    if (e_result.getValue()) {
-                        noCoveredClasses++;
-                    }
-                }
                 completeness += 100 * ((float) noCoveredClasses / (float) noClasses);
             }
             completeness /= (float) noOperators;
@@ -173,23 +148,13 @@ public class MetricEvaluation {
 
     }
 
-    private static Map<Integer, Boolean> getCompletenessLogic(DataBag bag,
+    private static int getCompletenessLogic(
             Collection<IdentityHashSet<Tuple>> eqClasses) {
-        Map<Integer, Boolean> coveredClasses = new HashMap<Integer, Boolean>();
-
-        for (Iterator<Tuple> it = bag.iterator(); it.hasNext();) {
-            Tuple t = it.next();
-            int classId = 0;
-            for (IdentityHashSet<Tuple> eqClass : eqClasses) {
-
-                if (eqClass.contains(t) || eqClass.size() == 0) {
-                    coveredClasses.put(classId, true);
-                }
-                classId++;
-            }
+        int nCoveredClasses = 0;
+        for (IdentityHashSet<Tuple> eqClass : eqClasses) {
+            if (!eqClass.isEmpty())
+                nCoveredClasses++;    
         }
-
-        return coveredClasses;
-
+        return nCoveredClasses;
     }
 }
diff --git a/src/org/apache/pig/pen/util/PreOrderDepthFirstWalker.java b/src/org/apache/pig/pen/util/PreOrderDepthFirstWalker.java
index 15b41d3ab..708e53e40 100644
--- a/src/org/apache/pig/pen/util/PreOrderDepthFirstWalker.java
+++ b/src/org/apache/pig/pen/util/PreOrderDepthFirstWalker.java
@@ -33,6 +33,9 @@ import org.apache.pig.impl.util.Utils;
 
 public class PreOrderDepthFirstWalker<O extends Operator, P extends OperatorPlan<O>>
         extends PlanWalker<O, P> {
+  
+    private boolean branchFlag = false;
+    
     /**
      * @param plan
      *            Plan for this walker to traverse.
@@ -41,6 +44,14 @@ public class PreOrderDepthFirstWalker<O extends Operator, P extends OperatorPlan
         super(plan);
     }
 
+    public void setBranchFlag() {
+        branchFlag = true;
+    }
+    
+    public boolean getBranchFlag() {
+        return branchFlag;
+    }
+    
     /**
      * Begin traversing the graph.
      * 
@@ -66,8 +77,10 @@ public class PreOrderDepthFirstWalker<O extends Operator, P extends OperatorPlan
         if (predecessors == null)
             return;
 
+        boolean thisBranchFlag = branchFlag;
         for (O pred : predecessors) {
             if (seen.add(pred)) {
+                branchFlag = thisBranchFlag;
                 pred.visit(visitor);
                 Collection<O> newPredecessors = Utils.mergeCollection(mPlan.getPredecessors(pred), mPlan.getSoftLinkPredecessors(pred));
                 depthFirst(pred, newPredecessors, seen, visitor);
diff --git a/src/org/apache/pig/pen/util/ReverseDepthFirstWalker.java b/src/org/apache/pig/pen/util/ReverseDepthFirstWalker.java
new file mode 100644
index 000000000..c4b471187
--- /dev/null
+++ b/src/org/apache/pig/pen/util/ReverseDepthFirstWalker.java
@@ -0,0 +1,107 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.pig.pen.util;
+
+import java.util.Collection;
+import java.util.HashSet;
+import java.util.Set;
+import java.util.Map;
+
+import org.apache.pig.impl.plan.Operator;
+import org.apache.pig.impl.plan.OperatorPlan;
+import org.apache.pig.impl.plan.PlanVisitor;
+import org.apache.pig.impl.plan.PlanWalker;
+import org.apache.pig.impl.plan.VisitorException;
+import org.apache.pig.impl.util.Utils;
+import org.apache.pig.impl.logicalLayer.LogicalOperator;
+import org.apache.pig.impl.logicalLayer.LogicalPlan;
+
+@SuppressWarnings("unchecked")
+public class ReverseDepthFirstWalker<O extends Operator, P extends OperatorPlan<O>>
+        extends PlanWalker<O, P> {
+    private HashSet<O> stoppers = null;
+    private LogicalOperator lo;
+    private LogicalPlan lp;
+    Map<LogicalOperator, O> l2pMap;
+    Set<O> seen;
+    
+    /**
+     * @param plan
+     *            Plan for this walker to traverse.
+     */
+    public ReverseDepthFirstWalker(P plan, LogicalOperator op, LogicalPlan lp, Map<LogicalOperator, O> l2pMap, Set<O> seen) {
+        super(plan);
+        if (lp != null && lp.getPredecessors(op) != null)
+        {
+            stoppers = new HashSet<O>();
+            for (LogicalOperator lo :  lp.getPredecessors(op))
+                stoppers.add(l2pMap.get(lo));
+        }
+        lo = op;
+        this.lp = lp;
+        this.l2pMap = l2pMap;
+        if (seen == null)
+            this.seen = new HashSet<O>();
+        else
+            this.seen = seen;
+    }
+
+    public ReverseDepthFirstWalker(P plan, LogicalOperator op, LogicalPlan lp, Map<LogicalOperator, O> l2pMap)
+    {
+        this(plan, op, lp, l2pMap, null);
+    }
+    
+    public PlanWalker<O, P> spawnChildWalker(P plan) {
+        return new ReverseDepthFirstWalker<O, P>(plan, lo, lp, l2pMap, seen);
+    }
+        
+    /**
+     * Begin traversing the graph.
+     * 
+     * @param visitor
+     *            Visitor this walker is being used by.
+     * @throws VisitorException
+     *             if an error is encountered while walking.
+     */
+    public void walk(PlanVisitor<O, P> visitor) throws VisitorException {
+        O po = l2pMap.get(lo);
+        // TODO: After use new LO, this should be removed. Found necessary in the testForeach test.
+        if (po == null)
+            return;
+        depthFirst(null, mPlan.getPredecessors(po), visitor);
+        if (seen.add(po))
+            po.visit(visitor);
+    }
+
+    private void depthFirst(O node, Collection<O> predecessors,
+            PlanVisitor<O, P> visitor) throws VisitorException {
+        if (predecessors == null)
+            return;
+
+        for (O pred : predecessors) {
+            if (seen.add(pred)) {
+                Collection<O> newPredecessors = Utils.mergeCollection(mPlan.getPredecessors(pred), mPlan.getSoftLinkPredecessors(pred));
+                depthFirst(pred, newPredecessors, visitor);
+                pred.visit(visitor);
+            }
+            if (stoppers.contains(pred))
+                continue;
+        }
+    }
+}
diff --git a/src/org/apache/pig/tools/grunt/GruntParser.java b/src/org/apache/pig/tools/grunt/GruntParser.java
index 3e5670702..c00a11ae5 100644
--- a/src/org/apache/pig/tools/grunt/GruntParser.java
+++ b/src/org/apache/pig/tools/grunt/GruntParser.java
@@ -96,6 +96,7 @@ public class GruntParser extends PigScriptParser {
         mDone = false;
         mLoadOnly = false;
         mExplain = null;
+        mScriptIllustrate = false;
     }
 
     private void setBatchOn() {
@@ -202,6 +203,10 @@ public class GruntParser extends PigScriptParser {
         mJobConf = execEngine.getJobConf();
     }
 
+    public void setScriptIllustrate() {
+        mScriptIllustrate = true;
+    }
+    
     @Override
     public void prompt()
     {
@@ -271,7 +276,7 @@ public class GruntParser extends PigScriptParser {
                 }
                 setBatchOn();
                 try {
-                    loadScript(script, true, true, params, files);
+                    loadScript(script, true, true, false, params, files);
                 } catch(IOException e) {
                     discardBatch();
                     throw e;
@@ -416,20 +421,20 @@ public class GruntParser extends PigScriptParser {
                 setBatchOn();
                 mPigServer.setJobName(script);
                 try {
-                    loadScript(script, true, mLoadOnly, params, files);
+                    loadScript(script, true, false, mLoadOnly, params, files);
                     executeBatch();
                 } finally {
                     discardBatch();
                 }
             } else {
-                loadScript(script, false, mLoadOnly, params, files);
+                loadScript(script, false, false, mLoadOnly, params, files);
             }
         } else {
             log.warn("'run/exec' statement is ignored while processing 'explain -script' or '-check'");
         }
     }
 
-    private void loadScript(String script, boolean batch, boolean loadOnly,
+    private void loadScript(String script, boolean batch, boolean loadOnly, boolean illustrate,
                             List<String> params, List<String> files) 
         throws IOException, ParseException {
         
@@ -467,6 +472,8 @@ public class GruntParser extends PigScriptParser {
         parser.setConsoleReader(reader);
         parser.setInteractive(interactive);
         parser.setLoadOnly(loadOnly);
+        if (illustrate)
+            parser.setScriptIllustrate();
         parser.mExplain = mExplain;
         
         parser.prompt();
@@ -621,12 +628,43 @@ public class GruntParser extends PigScriptParser {
     }
     
     @Override
-    protected void processIllustrate(String alias) throws IOException
+    protected void processIllustrate(String alias, String script, String target, List<String> params, List<String> files) throws IOException, ParseException
     {
-        if(mExplain == null) { // process only if not in "explain" mode
-            mPigServer.getExamples(alias);
-        } else {
+        if (mScriptIllustrate)
+            throw new ParseException("'illustrate' statement can not appear in a script that is illustrated opon.");
+
+        if ((alias != null) && (script != null))
+            throw new ParseException("'illustrate' statement on an alias does not work when a script is in effect");
+        else if (mExplain != null)
             log.warn("'illustrate' statement is ignored while processing 'explain -script' or '-check'");
+        else {
+            try {
+                if (script != null) {
+                    if (!"true".equalsIgnoreCase(mPigServer.
+                                                 getPigContext()
+                                                 .getProperties().
+                                                 getProperty("opt.multiquery","true"))) {
+                        throw new ParseException("Cannot explain script if multiquery is disabled.");
+                    }
+                    setBatchOn();
+                    try {
+                        loadScript(script, true, true, true, params, files);
+                    } catch(IOException e) {
+                        discardBatch();
+                        throw e;
+                    } catch (ParseException e) {
+                        discardBatch();
+                        throw e;
+                    }
+                } else if (alias == null) {
+                    throw new ParseException("'illustrate' statement must be on an alias or on a script.");
+                }
+                mPigServer.getExamples(alias);
+            } finally {
+                if (script != null) {
+                    discardBatch();
+                }
+            }
         }
     }
 
@@ -998,4 +1036,5 @@ public class GruntParser extends PigScriptParser {
     private int mNumFailedJobs;
     private int mNumSucceededJobs;
     private FsShell shell;
+    private boolean mScriptIllustrate;
 }
diff --git a/src/org/apache/pig/tools/pigscript/parser/PigScriptParser.jj b/src/org/apache/pig/tools/pigscript/parser/PigScriptParser.jj
index 787aed18c..a26bea9a7 100644
--- a/src/org/apache/pig/tools/pigscript/parser/PigScriptParser.jj
+++ b/src/org/apache/pig/tools/pigscript/parser/PigScriptParser.jj
@@ -108,7 +108,7 @@ public abstract class PigScriptParser
 
 	abstract protected void processRemove(String path, String opt) throws IOException;
 	
-	abstract protected void processIllustrate(String alias) throws IOException;
+	abstract protected void processIllustrate(String alias, String script, String target, List<String> params, List<String> files) throws IOException, ParseException;
 
 	abstract protected void processScript(String script, boolean batch, List<String> params, List<String> files) throws IOException, ParseException;
 
@@ -454,9 +454,7 @@ void parse() throws IOException:
 	t1 = <IDENTIFIER>
 	{processDump(t1.image);}
 	|
-	<ILLUSTRATE>
-	t1 = <IDENTIFIER>
-	{processIllustrate(t1.image);}
+    Illustrate()
 	|
 	<DESCRIBE>
 	(
@@ -552,6 +550,46 @@ void parse() throws IOException:
 	)
 }
 
+void Illustrate() throws IOException:
+{
+	Token t;
+	String alias = null;
+        String script = null;
+	String target=null;
+	ArrayList<String> params;
+	ArrayList<String> files;
+
+}
+{
+	<ILLUSTRATE>
+	{
+		params = new ArrayList<String>(); 
+		files = new ArrayList<String>();
+	}
+	(
+		<OUT>
+		t = GetPath()
+		{target = t.image;}
+		|
+		<SCRIPT>
+		t = GetPath()
+		{script = t.image;}
+		|
+		<PARAM>
+		t = GetPath()
+		{params.add(t.image);}
+		|
+		<PARAM_FILE>
+		t = GetPath()
+		{files.add(t.image);}
+	)*
+	(
+		t = <IDENTIFIER>
+		{alias = t.image;}
+	)?
+	{processIllustrate(alias, script, target, params, files);}
+}
+
 void Explain() throws IOException:
 {
 	Token t;
diff --git a/test/org/apache/pig/test/TestExampleGenerator.java b/test/org/apache/pig/test/TestExampleGenerator.java
index 2dee27b56..0902c2828 100644
--- a/test/org/apache/pig/test/TestExampleGenerator.java
+++ b/test/org/apache/pig/test/TestExampleGenerator.java
@@ -97,6 +97,20 @@ public class TestExampleGenerator extends TestCase {
         dat.close();
     }
 
+    @Test
+    public void testLoad() throws Exception {
+
+        PigServer pigserver = new PigServer(pigContext);
+
+        String query = "A = load " + A
+                + " using PigStorage() as (x : int, y : int);\n";
+        pigserver.registerQuery(query);
+        Map<LogicalOperator, DataBag> derivedData = pigserver.getExamples("A");
+
+        assertTrue(derivedData != null);
+
+    }
+
     @Test
     public void testFilter() throws Exception {
 
@@ -113,6 +127,43 @@ public class TestExampleGenerator extends TestCase {
 
     }
 
+    @Test
+    public void testFilter2() throws Exception {
+
+        PigServer pigserver = new PigServer(pigContext);
+
+        String query = "A = load " + A
+                + " using PigStorage() as (x : int, y : int);\n";
+        pigserver.registerQuery(query);
+        query = "B = filter A by x > 5 AND y < 6;";
+        pigserver.registerQuery(query);
+        Map<LogicalOperator, DataBag> derivedData = pigserver.getExamples("B");
+
+        assertTrue(derivedData != null);
+    }
+    
+    @Test
+    public void testFilter3() throws Exception {
+
+        PigServer pigserver = new PigServer(pigContext);
+
+        String query = "A = load " + A
+                + " using PigStorage() as (x : int, y : int);\n";
+        pigserver.registerQuery(query);
+        query = "B = filter A by x > 10;";
+        pigserver.registerQuery(query);
+        query = "C = FOREACH B GENERATE (x+1) as x1, (y+1) as y1;";
+        pigserver.registerQuery(query);
+        query = "D = FOREACH C GENERATE (x1+1) as x2, (y1+1) as y2;";
+        pigserver.registerQuery(query);
+        query = "E = DISTINCT D;";
+        pigserver.registerQuery(query);
+        Map<LogicalOperator, DataBag> derivedData = pigserver.getExamples("E");
+
+        assertTrue(derivedData != null);
+
+    }
+    
     @Test
     public void testForeach() throws ExecException, IOException {
         PigServer pigServer = new PigServer(pigContext);
@@ -139,6 +190,19 @@ public class TestExampleGenerator extends TestCase {
         assertTrue(derivedData != null);
     }
 
+    @Test
+    public void testJoin2() throws IOException, ExecException {
+        PigServer pigServer = new PigServer(pigContext);
+        pigServer.registerQuery("A1 = load " + A + " as (x, y);");
+        pigServer.registerQuery("B1 = load " + A + " as (x, y);");
+
+        pigServer.registerQuery("E = join A1 by x, B1 by x;");
+
+        Map<LogicalOperator, DataBag> derivedData = pigServer.getExamples("E");
+
+        assertTrue(derivedData != null);
+    }
+
     @Test
     public void testCogroupMultipleCols() throws Exception {
 
@@ -173,6 +237,68 @@ public class TestExampleGenerator extends TestCase {
 
     }
     
+    @Test
+    public void testGroup2() throws Exception {
+        PigServer pigServer = new PigServer(pigContext);
+        pigServer.registerQuery("A = load " + A.toString() + " as (x:int, y:int);");
+        pigServer.registerQuery("B = group A by x;");
+        pigServer.registerQuery("C = foreach B generate group, COUNT(A);};");
+        Map<LogicalOperator, DataBag> derivedData = pigServer.getExamples("C");
+
+        assertTrue(derivedData != null);
+
+    }
+
+    @Test
+    public void testGroup3() throws Exception {
+        PigServer pigServer = new PigServer(pigContext);
+        pigServer.registerQuery("A = load " + A.toString() + " as (x:int, y:int);");
+        pigServer.registerQuery("B = FILTER A by x  > 3;");
+        pigServer.registerQuery("C = group B by y;");
+        pigServer.registerQuery("D = foreach C generate group, COUNT(B);};");
+        Map<LogicalOperator, DataBag> derivedData = pigServer.getExamples("D");
+
+        assertTrue(derivedData != null);
+
+    }
+    
+    @Test
+    public void testFilterUnion() throws Exception {
+        PigServer pigServer = new PigServer(pigContext);
+        pigServer.registerQuery("A = load " + A.toString() + " as (x:int, y:int);");
+        pigServer.registerQuery("B = FILTER A by x  > 3;");
+        pigServer.registerQuery("C = FILTER A by x < 3;");
+        pigServer.registerQuery("D = UNION B, C;");
+        Map<LogicalOperator, DataBag> derivedData = pigServer.getExamples("D");
+
+        assertTrue(derivedData != null);
+
+    }
+    
+    @Test
+    public void testForEachNestedBlock() throws Exception {
+        PigServer pigServer = new PigServer(pigContext);
+        pigServer.registerQuery("A = load " + A.toString() + " as (x:int, y:int);");
+        pigServer.registerQuery("B = group A by x;");
+        pigServer.registerQuery("C = foreach B { FA = filter A by y == 6; generate group, COUNT(FA);};");
+        Map<LogicalOperator, DataBag> derivedData = pigServer.getExamples("C");
+
+        assertTrue(derivedData != null);
+
+    }
+
+    @Test
+    public void testForEachNestedBlock2() throws Exception {
+        PigServer pigServer = new PigServer(pigContext);
+        pigServer.registerQuery("A = load " + A.toString() + " as (x:int, y:int);");
+        pigServer.registerQuery("B = group A by x;");
+        pigServer.registerQuery("C = foreach B { FA = filter A by y == 6; DA = DISTINCT FA; generate group, COUNT(DA);};");
+        Map<LogicalOperator, DataBag> derivedData = pigServer.getExamples("C");
+
+        assertTrue(derivedData != null);
+
+    }
+    
     @Test
     public void testUnion() throws Exception {
         PigServer pigServer = new PigServer(pigContext);
@@ -184,4 +310,34 @@ public class TestExampleGenerator extends TestCase {
         assertTrue(derivedData != null);
     }
 
+    @Test
+    public void testDistinct() throws Exception {
+        PigServer pigServer = new PigServer(pigContext);
+        pigServer.registerQuery("A = load " + A.toString() + " as (x, y);");
+        pigServer.registerQuery("B = DISTINCT A;");
+        Map<LogicalOperator, DataBag> derivedData = pigServer.getExamples("B");
+
+        assertTrue(derivedData != null);
+    }
+    
+    @Test
+    public void testCross() throws Exception {
+        PigServer pigServer = new PigServer(pigContext);
+        pigServer.registerQuery("A = load " + A.toString() + " as (x, y);");
+        pigServer.registerQuery("B = load " + B.toString() + " as (x, y);");
+        pigServer.registerQuery("C = CROSS A, B;");
+        Map<LogicalOperator, DataBag> derivedData = pigServer.getExamples("C");
+
+        assertTrue(derivedData != null);
+    }
+    
+    @Test
+    public void testLimit() throws Exception {
+        PigServer pigServer = new PigServer(pigContext);
+        pigServer.registerQuery("A = load " + A.toString() + " as (x, y);");
+        pigServer.registerQuery("B = limit A 5;");
+        Map<LogicalOperator, DataBag> derivedData = pigServer.getExamples("B");
+
+        assertTrue(derivedData != null);
+    }
 }
diff --git a/test/org/apache/pig/test/TestGrunt.java b/test/org/apache/pig/test/TestGrunt.java
index d8fef7fa3..3ffbc9d4e 100644
--- a/test/org/apache/pig/test/TestGrunt.java
+++ b/test/org/apache/pig/test/TestGrunt.java
@@ -24,8 +24,6 @@ import org.junit.runner.RunWith;
 import org.junit.runners.JUnit4;
 
 import junit.framework.TestCase;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
 import org.apache.log4j.Appender;
 import org.apache.log4j.FileAppender;
 import org.apache.log4j.PatternLayout;
@@ -34,9 +32,7 @@ import org.apache.pig.PigException;
 import org.apache.pig.PigServer;
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.impl.PigContext;
-import org.apache.pig.test.Util.ProcessReturnInfo;
 import org.apache.pig.tools.grunt.Grunt;
-import org.apache.pig.tools.parameters.ParameterSubstitutionPreprocessor;
 import org.apache.pig.tools.pigscript.parser.ParseException;
 import org.apache.pig.impl.util.LogUtils;
 
@@ -45,17 +41,12 @@ import java.io.File;
 import java.io.FileReader;
 import java.io.InputStreamReader;
 import java.io.BufferedReader;
-import java.io.StringReader;
-import java.io.StringWriter;
-import java.util.ArrayList;
 
 @RunWith(JUnit4.class)
 public class TestGrunt extends TestCase {
     static MiniCluster cluster = MiniCluster.buildCluster();
     private String basedir = "test/org/apache/pig/test/data";
 
-    private final Log log = LogFactory.getLog(getClass());
-
     @BeforeClass
     public static void oneTimeSetup() throws Exception {
         cluster.setProperty("opt.multiquery","true");
@@ -468,6 +459,105 @@ public class TestGrunt extends TestCase {
         grunt.exec();
     }
 
+    @Test
+    public void testIllustrateScript() throws Throwable {
+        PigServer server = new PigServer(ExecType.LOCAL, cluster.getProperties());
+        PigContext context = server.getPigContext();
+        
+        String strCmd = "illustrate -script "
+                + basedir + "/illustrate.pig;";
+        
+        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
+        InputStreamReader reader = new InputStreamReader(cmd);
+        
+        Grunt grunt = new Grunt(new BufferedReader(reader), context);
+    
+        grunt.exec();
+    }
+
+    @Test
+    public void testIllustrateScript2() throws Throwable {
+        PigServer server = new PigServer(ExecType.LOCAL, cluster.getProperties());
+        PigContext context = server.getPigContext();
+        
+        String strCmd = "illustrate -script "
+                + basedir + "/illustrate2.pig;";
+        
+        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
+        InputStreamReader reader = new InputStreamReader(cmd);
+        
+        Grunt grunt = new Grunt(new BufferedReader(reader), context);
+    
+        grunt.exec();
+    }
+    
+    @Test
+    public void testIllustrateScript3() throws Throwable {
+        PigServer server = new PigServer(ExecType.LOCAL, cluster.getProperties());
+        PigContext context = server.getPigContext();
+        
+        String strCmd = "illustrate -script "
+                + basedir + "/illustrate3.pig;";
+        
+        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
+        InputStreamReader reader = new InputStreamReader(cmd);
+        
+        Grunt grunt = new Grunt(new BufferedReader(reader), context);
+    
+        grunt.exec();
+    }
+    
+    @Test
+    public void testIllustrateScript4() throws Throwable {
+        // empty line/field test
+        PigServer server = new PigServer(ExecType.LOCAL, cluster.getProperties());
+        PigContext context = server.getPigContext();
+        
+        String strCmd = "illustrate -script "
+                + basedir + "/illustrate4.pig;";
+        
+        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
+        InputStreamReader reader = new InputStreamReader(cmd);
+        
+        Grunt grunt = new Grunt(new BufferedReader(reader), context);
+    
+        grunt.exec();
+    }
+    
+    @Test
+    public void testIllustrateScript5() throws Throwable {
+        // empty line/field test
+        PigServer server = new PigServer(ExecType.LOCAL, cluster.getProperties());
+        PigContext context = server.getPigContext();
+        
+        String strCmd = "illustrate -script "
+                + basedir + "/illustrate5.pig;";
+        
+        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
+        InputStreamReader reader = new InputStreamReader(cmd);
+        
+        Grunt grunt = new Grunt(new BufferedReader(reader), context);
+    
+        grunt.exec();
+    }
+    
+    @Test
+    public void testIllustrateScript6() throws Throwable {
+        // empty line/field test
+        PigServer server = new PigServer(ExecType.LOCAL, cluster.getProperties());
+        PigContext context = server.getPigContext();
+        
+        String strCmd = "illustrate -script "
+                + basedir + "/illustrate6.pig;";
+        
+        ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
+        InputStreamReader reader = new InputStreamReader(cmd);
+        
+        Grunt grunt = new Grunt(new BufferedReader(reader), context);
+    
+        grunt.exec();
+    }
+    
     /**
      * verify that grunt commands are ignored in explain -script mode
      */
@@ -678,7 +768,7 @@ public class TestGrunt extends TestCase {
             +"f = foreach e generate group, COUNT($1);"
             +"store f into 'bla';"
             +"f1 = load 'bla' as (f:chararray);"
-            +"g = order f1 by $1;"
+            +"g = order f1 by $0;"
             +"illustrate g;";
 
         ByteArrayInputStream cmd = new ByteArrayInputStream(strCmd.getBytes());
diff --git a/test/org/apache/pig/test/TestLocalPOSplit.java b/test/org/apache/pig/test/TestLocalPOSplit.java
deleted file mode 100644
index 69c25568f..000000000
--- a/test/org/apache/pig/test/TestLocalPOSplit.java
+++ /dev/null
@@ -1,343 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.pig.test;
-
-import java.io.File;
-import java.io.FileOutputStream;
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-import java.util.Properties;
-import java.util.Random;
-
-import junit.framework.TestCase;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.pig.ExecType;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
-import org.apache.pig.pen.LocalLogToPhyTranslationVisitor;
-import org.apache.pig.data.BagFactory;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.impl.PigContext;
-import org.apache.pig.impl.logicalLayer.FrontendException;
-import org.apache.pig.impl.logicalLayer.LODefine;
-import org.apache.pig.impl.logicalLayer.LOLoad;
-import org.apache.pig.impl.logicalLayer.LogicalOperator;
-import org.apache.pig.impl.logicalLayer.LogicalPlan;
-import org.apache.pig.impl.logicalLayer.LogicalPlanBuilder;
-import org.apache.pig.impl.logicalLayer.PlanSetter;
-import org.apache.pig.impl.logicalLayer.validators.LogicalPlanValidationExecutor;
-import org.apache.pig.impl.plan.CompilationMessageCollector;
-import org.apache.pig.impl.plan.OperatorKey;
-import org.apache.pig.impl.plan.VisitorException;
-import org.apache.pig.pen.util.FunctionalLogicalOptimizer;
-import org.junit.Test;
-
-public class TestLocalPOSplit extends TestCase {
-
-    Random r = new Random();
-
-    Log log = LogFactory.getLog(getClass());
-
-    PigContext pigContext = new PigContext(ExecType.LOCAL, new Properties());
-
-    @Test
-    public void testSplit() throws IOException, VisitorException, ExecException {
-        init();
-
-        pigContext.connect();
-        File datFile = File.createTempFile("tempA", ".dat");
-
-        FileOutputStream dat = new FileOutputStream(datFile);
-
-        for (int i = 0; i < 100; i++) {
-            String str = r.nextInt(10) + "\n";
-            dat.write(str.getBytes());
-
-        }
-
-        dat.close();
-
-        String query = "split (load '" + Util.encodeEscape(datFile.getAbsolutePath())
-                + "') into a if $0 == 2, b if $0 == 9, c if $0 == 7 ;";
-
-        LogicalPlan plan = buildPlan(query);
-        PhysicalPlan pp = buildPhysicalPlan(plan);
-
-        DataBag[] bag = new DataBag[pp.getLeaves().size()];
-
-        for (int i = 0; i < bag.length; i++) {
-            bag[i] = BagFactory.getInstance().newDefaultBag();
-        }
-
-        for (int i = 0; i < pp.getLeaves().size(); i++) {
-            Tuple t = null;
-            for (Result res = pp.getLeaves().get(i).getNext(t); res.returnStatus != POStatus.STATUS_EOP; res = pp
-                    .getLeaves().get(i).getNext(t)) {
-                if (res.returnStatus == POStatus.STATUS_OK)
-                    bag[i].add((Tuple) res.result);
-            }
-        }
-        
-        // Depending on how the "maps" in the physical plan are
-        // built the leaves could be in different order between different runs.
-        // lets test the first tuple out of each leaf to 
-        // 1) ensure the value was not seen before
-        // 2) all the remaining tuples from that leaf are same
-        //    as the first value
-        Map<DataByteArray, Boolean> seen = new HashMap<DataByteArray, Boolean>();
-        seen.put(new DataByteArray("7".getBytes()), false);
-        seen.put(new DataByteArray("9".getBytes()), false);
-        seen.put(new DataByteArray("2".getBytes()), false);
-        
-        for (int i = 0; i < bag.length; i++) {
-            DataByteArray firstValue = null;
-            Iterator<Tuple> it = bag[i].iterator();
-            if (it.hasNext()) {
-                // check that we have not seen this value before
-                Tuple t = it.next();
-                System.out.println(t);
-                firstValue = (DataByteArray) t.get(0);
-                assertFalse((Boolean) seen.get(firstValue));
-                seen.put(firstValue, true);
-
-            }
-            // check that all remaining tuples from this 
-            // leaf have the same values as the first value
-            for (; it.hasNext();) {
-                Tuple t = it.next();
-                System.out.println(t);
-                assertEquals(t.get(0), firstValue);
-            }
-        }
-        datFile.delete();
-    }
-
-    @Test
-    public void testSplitNulls() throws IOException, VisitorException, ExecException {
-        init();
-
-        pigContext.connect();
-        File datFile1 = File.createTempFile("tempN1", ".dat");
-        String path1 = Util.encodeEscape(datFile1.getAbsolutePath());
-
-        FileOutputStream dat = new FileOutputStream(datFile1);
-        String s1 = "1\n2\n3\n42\n4\n5\n";
-        dat.write(s1.getBytes());
-        dat.close();
-
-        String s2 = "1\n2\n43\n3\n4\n5\n";
-
-        File datFile2 = File.createTempFile("tempN2", ".dat");
-        String path2 = Util.encodeEscape(datFile2.getAbsolutePath());
-                
-        dat = new FileOutputStream(datFile2);
-        dat.write(s2.getBytes());
-        dat.close();
-
-        String query = "a = load '"+path1+"'; b = load '"+path2+"'; "+
-            "c = cogroup a by $0, b by $0; d = foreach c generate $0, flatten($1), flatten($2); "+
-            "split d into e if 1==1, f if 1==1;";
-
-        LogicalPlan plan = buildPlan(query);
-        PhysicalPlan pp = buildPhysicalPlan(plan);
-
-        DataBag[] bag = new DataBag[pp.getLeaves().size()];
-
-        for (int i = 0; i < bag.length; i++) {
-            bag[i] = BagFactory.getInstance().newDefaultBag();
-        }
-
-        for (int i = 0; i < pp.getLeaves().size(); i++) {
-            System.out.println("Leaves: "+i);
-            Tuple t = null;
-            for (Result res = pp.getLeaves().get(i).getNext(t); res.returnStatus != POStatus.STATUS_EOP; res = pp
-                    .getLeaves().get(i).getNext(t)) {
-                if (res.returnStatus == POStatus.STATUS_OK)
-                    bag[i].add((Tuple) res.result);
-                System.out.println("Split: "+res.result);
-            }
-        }
-        
-        Map<DataByteArray, Integer> seen = new HashMap<DataByteArray, Integer>();
-        seen.put(new DataByteArray("1".getBytes()), new Integer(0));
-        seen.put(new DataByteArray("2".getBytes()), new Integer(0));
-        seen.put(new DataByteArray("3".getBytes()), new Integer(0));
-        seen.put(new DataByteArray("4".getBytes()), new Integer(0));
-        seen.put(new DataByteArray("5".getBytes()), new Integer(0));
-        
-        for (int i = 0; i < bag.length; i++) {
-            DataByteArray value = null;
-            Iterator<Tuple> it = bag[i].iterator();
-            while (it.hasNext()) {
-                Tuple t = it.next();
-                System.out.println("Value: "+t);
-                value = (DataByteArray) t.get(0);
-                Integer count = seen.get(value);
-                seen.put(value, ++count);
-            }
-        }
-
-        for (Integer j: seen.values()) {
-            assertEquals(j, new Integer(2));
-        }
-        datFile1.delete();
-        datFile2.delete();
-    }
-
-    public PhysicalPlan buildPhysicalPlan(LogicalPlan lp)
-            throws VisitorException {
-        LocalLogToPhyTranslationVisitor visitor = new LocalLogToPhyTranslationVisitor(
-                lp);
-        visitor.setPigContext(pigContext);
-        visitor.visit();
-        return visitor.getPhysicalPlan();
-    }
-
-    public LogicalPlan buildPlan(String query) {
-        return buildPlan(query, LogicalPlanBuilder.class.getClassLoader());
-    }
-
-    public LogicalPlan buildPlan(String query, ClassLoader cldr) {
-        LogicalPlanBuilder.classloader = cldr;
-
-        LogicalPlanBuilder builder = new LogicalPlanBuilder(pigContext); //
-
-        try {
-            String[] qs = query.split(";");
-            LogicalPlan lp = null;
-            for (String q: qs) {
-                q = q.trim();
-                if (q.equals(""))
-                    continue;
-                q += ";";
-                System.out.println(q);
-                lp = builder.parse("Test-Plan-Builder", q, aliases,
-                                   logicalOpTable, aliasOp, fileNameMap);
-            }
-
-            List<LogicalOperator> roots = lp.getRoots();
-
-            if (roots.size() > 0) {
-                for (LogicalOperator op : roots) {
-                    if (!(op instanceof LOLoad) && !(op instanceof LODefine)) {
-                        throw new Exception(
-                                "Cannot have a root that is not the load or define operator. Found "
-                                        + op.getClass().getName());
-                    }
-                }
-            }
-
-            System.err.println("Query: " + query);
-
-            // Just the top level roots and their children
-            // Need a recursive one to travel down the tree
-
-            for (LogicalOperator op : lp.getRoots()) {
-                System.err.println("Logical Plan Root: "
-                        + op.getClass().getName() + " object " + op);
-
-                List<LogicalOperator> listOp = lp.getSuccessors(op);
-
-                if (null != listOp) {
-                    Iterator<LogicalOperator> iter = listOp.iterator();
-                    while (iter.hasNext()) {
-                        LogicalOperator lop = iter.next();
-                        System.err.println("Successor: "
-                                + lop.getClass().getName() + " object " + lop);
-                    }
-                }
-            }
-            lp = refineLogicalPlan(lp);
-            assertTrue(lp != null);
-            return lp;
-        } catch (IOException e) {
-            // log.error(e);
-            // System.err.println("IOException Stack trace for query: " +
-            // query);
-            // e.printStackTrace();
-            fail("IOException: " + e.getMessage());
-        } catch (Exception e) {
-            log.error(e);
-            // System.err.println("Exception Stack trace for query: " + query);
-            // e.printStackTrace();
-            fail(e.getClass().getName() + ": " + e.getMessage() + " -- "
-                    + query);
-        }
-        return null;
-    }
-
-    private LogicalPlan refineLogicalPlan(LogicalPlan plan) {
-        PlanSetter ps = new PlanSetter(plan);
-        try {
-            ps.visit();
-
-        } catch (VisitorException e) {
-            // TODO Auto-generated catch block
-            e.printStackTrace();
-        }
-
-        // run through validator
-        CompilationMessageCollector collector = new CompilationMessageCollector();
-        FrontendException caught = null;
-        try {
-            boolean isBeforeOptimizer = true;
-            LogicalPlanValidationExecutor validator = new LogicalPlanValidationExecutor(
-                    plan, pigContext, isBeforeOptimizer);
-            validator.validate(plan, collector);
-
-            FunctionalLogicalOptimizer optimizer = new FunctionalLogicalOptimizer(
-                    plan);
-            optimizer.optimize();
-            
-            isBeforeOptimizer = false;
-            validator = new LogicalPlanValidationExecutor(
-                    plan, pigContext, isBeforeOptimizer);
-            validator.validate(plan, collector);
-        } catch (FrontendException fe) {
-            // Need to go through and see what the collector has in it. But
-            // remember what we've caught so we can wrap it into what we
-            // throw.
-            caught = fe;
-        }
-
-        return plan;
-
-    }
-
-    private void init() {
-        aliases = new HashMap<LogicalOperator, LogicalPlan>();
-        logicalOpTable = new HashMap<OperatorKey, LogicalOperator>();
-        aliasOp = new HashMap<String, LogicalOperator>();
-        fileNameMap = new HashMap<String, String>();
-    }
-
-    Map<LogicalOperator, LogicalPlan> aliases;
-    Map<OperatorKey, LogicalOperator> logicalOpTable;
-    Map<String, LogicalOperator> aliasOp;
-    Map<String, String> fileNameMap;
-}
diff --git a/test/org/apache/pig/test/TestMultiQueryLocal.java b/test/org/apache/pig/test/TestMultiQueryLocal.java
index e28b12d91..abc1a7316 100644
--- a/test/org/apache/pig/test/TestMultiQueryLocal.java
+++ b/test/org/apache/pig/test/TestMultiQueryLocal.java
@@ -427,6 +427,7 @@ public class TestMultiQueryLocal extends TestCase {
             
             GruntParser parser = new GruntParser(new StringReader(script));
             parser.setInteractive(false);
+            myPig.getPigContext().getProperties().setProperty("pig.usenewlogicalplan", "true");
             parser.setParams(myPig);
             parser.parseStopOnError();
 
@@ -435,6 +436,7 @@ public class TestMultiQueryLocal extends TestCase {
             Assert.fail();
         } finally {
             deleteOutputFiles();
+            myPig.getPigContext().getProperties().setProperty("pig.usenewlogicalplan", "false");
         }
     }
 
diff --git a/test/org/apache/pig/test/TestPOCogroup.java b/test/org/apache/pig/test/TestPOCogroup.java
deleted file mode 100644
index badc2b60c..000000000
--- a/test/org/apache/pig/test/TestPOCogroup.java
+++ /dev/null
@@ -1,211 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.pig.test;
-
-import java.util.LinkedList;
-import java.util.List;
-import java.util.Random;
-
-import junit.framework.TestCase;
-
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrangeForIllustrate;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PORead;
-import org.apache.pig.pen.physicalOperators.POCogroup;
-import org.apache.pig.data.BagFactory;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataType;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.data.TupleFactory;
-import org.apache.pig.impl.plan.OperatorKey;
-import org.apache.pig.impl.plan.PlanException;
-
-public class TestPOCogroup extends TestCase {
-    Random r = new Random();
-
-    public void testCogroup2Inputs() throws Exception {
-        DataBag bag1 = BagFactory.getInstance().newDefaultBag();
-        Tuple t = TupleFactory.getInstance().newTuple();
-        t.append(new Integer(2));
-        bag1.add(t);
-        t = TupleFactory.getInstance().newTuple();
-        t.append(new Integer(1));
-        bag1.add(t);
-        t = TupleFactory.getInstance().newTuple();
-        t.append(new Integer(1));
-        bag1.add(t);
-
-        DataBag bag2 = BagFactory.getInstance().newDefaultBag();
-        t = TupleFactory.getInstance().newTuple();
-        t.append(new Integer(2));
-        bag2.add(t);
-        t = TupleFactory.getInstance().newTuple();
-        t.append(new Integer(2));
-        bag2.add(t);
-        t = TupleFactory.getInstance().newTuple();
-        t.append(new Integer(1));
-        bag2.add(t);
-
-        PORead poread1 = new PORead(new OperatorKey("", r.nextLong()), bag1);
-        PORead poread2 = new PORead(new OperatorKey("", r.nextLong()), bag2);
-
-        List<PhysicalOperator> inputs1 = new LinkedList<PhysicalOperator>();
-        inputs1.add(poread1);
-
-        POProject prj1 = new POProject(new OperatorKey("", r.nextLong()), -1, 0);
-        prj1.setResultType(DataType.INTEGER);
-        PhysicalPlan p1 = new PhysicalPlan();
-        p1.add(prj1);
-        List<PhysicalPlan> in1 = new LinkedList<PhysicalPlan>();
-        in1.add(p1);
-        POLocalRearrangeForIllustrate lr1 = new POLocalRearrangeForIllustrate(new OperatorKey("", r
-                .nextLong()), -1, inputs1);
-        lr1.setPlans(in1);
-        lr1.setIndex(0);
-
-        List<PhysicalOperator> inputs2 = new LinkedList<PhysicalOperator>();
-        inputs2.add(poread2);
-
-        POProject prj2 = new POProject(new OperatorKey("", r.nextLong()), -1, 0);
-        prj2.setResultType(DataType.INTEGER);
-        PhysicalPlan p2 = new PhysicalPlan();
-        p2.add(prj2);
-        List<PhysicalPlan> in2 = new LinkedList<PhysicalPlan>();
-        in2.add(p2);
-        POLocalRearrangeForIllustrate lr2 = new POLocalRearrangeForIllustrate(new OperatorKey("", r
-                .nextLong()), -1, inputs2);
-        lr2.setPlans(in2);
-        lr2.setIndex(1);
-
-        List<PhysicalOperator> inputs = new LinkedList<PhysicalOperator>();
-        inputs.add(lr1);
-        inputs.add(lr2);
-
-        POCogroup poc = new POCogroup(new OperatorKey("", r.nextLong()), -1,
-                inputs);
-
-        List<Tuple> expected = new LinkedList<Tuple>();
-
-        Tuple t1 = TupleFactory.getInstance().newTuple();
-        t1.append(1);
-        DataBag b1 = BagFactory.getInstance().newDefaultBag();
-        Tuple temp = TupleFactory.getInstance().newTuple();
-        temp.append(1);
-        b1.add(temp);
-        b1.add(temp);
-        t1.append(b1);
-
-        DataBag b2 = BagFactory.getInstance().newDefaultBag();
-        b2.add(temp);
-        t1.append(b2);
-
-        expected.add(t1);
-
-        t1 = TupleFactory.getInstance().newTuple();
-        t1.append(2);
-        DataBag b3 = BagFactory.getInstance().newDefaultBag();
-        temp = TupleFactory.getInstance().newTuple();
-        temp.append(2);
-        b3.add(temp);
-        t1.append(b3);
-
-        DataBag b4 = BagFactory.getInstance().newDefaultBag();
-        b4.add(temp);
-        b4.add(temp);
-        t1.append(b4);
-
-        expected.add(t1);
-        // System.out.println(expected.get(0) + " " + expected.get(1));
-        List<Tuple> obtained = new LinkedList<Tuple>();
-        for (Result res = poc.getNext(t); res.returnStatus != POStatus.STATUS_EOP; res = poc
-                .getNext(t)) {
-            System.out.println(res.result);
-            obtained.add((Tuple) res.result);
-            assertTrue(expected.contains((Tuple) res.result));
-        }
-        assertEquals(expected.size(), obtained.size());
-    }
-
-    public void testCogroup1Input() throws ExecException, PlanException {
-        DataBag input = BagFactory.getInstance().newDefaultBag();
-        Tuple t = TupleFactory.getInstance().newTuple();
-        t.append(1);
-        t.append(2);
-        input.add(t);
-        input.add(t);
-        Tuple t2 = TupleFactory.getInstance().newTuple();
-        t2.append(2);
-        t2.append(2);
-        Tuple t3 = TupleFactory.getInstance().newTuple();
-        t3.append(3);
-        t3.append(4);
-        input.add(t2);
-        input.add(t3);
-
-        PORead poread1 = new PORead(new OperatorKey("", r.nextLong()), input);
-        List<PhysicalOperator> inputs1 = new LinkedList<PhysicalOperator>();
-        inputs1.add(poread1);
-
-        POProject prj1 = new POProject(new OperatorKey("", r.nextLong()), -1, 1);
-        prj1.setResultType(DataType.INTEGER);
-        PhysicalPlan p1 = new PhysicalPlan();
-        p1.add(prj1);
-        List<PhysicalPlan> in1 = new LinkedList<PhysicalPlan>();
-        in1.add(p1);
-        POLocalRearrangeForIllustrate lr1 = new POLocalRearrangeForIllustrate(new OperatorKey("", r
-                .nextLong()), -1, inputs1);
-        lr1.setPlans(in1);
-        lr1.setIndex(0);
-
-        List<PhysicalOperator> inputs = new LinkedList<PhysicalOperator>();
-        inputs.add(lr1);
-
-        List<Tuple> expected = new LinkedList<Tuple>();
-        Tuple tOut1 = TupleFactory.getInstance().newTuple();
-        tOut1.append(2);
-        DataBag t11 = BagFactory.getInstance().newDefaultBag();
-        t11.add(t);
-        t11.add(t);
-        t11.add(t2);
-        tOut1.append(t11);
-        expected.add(tOut1);
-        Tuple tOut2 = TupleFactory.getInstance().newTuple();
-        tOut2.append(4);
-        DataBag t22 = BagFactory.getInstance().newDefaultBag();
-        t22.add(t3);
-        tOut2.append(t22);
-        expected.add(tOut2);
-        POCogroup poc = new POCogroup(new OperatorKey("", r.nextLong()), -1,
-                inputs);
-        int count = 0;
-        for (Result res = poc.getNext(t); res.returnStatus != POStatus.STATUS_EOP; res = poc
-                .getNext(t)) {
-            System.out.println(res.result);
-            count++;
-            assertTrue(expected.contains(res.result));
-        }
-        assertEquals(expected.size(), count);
-    }
-
-}
diff --git a/test/org/apache/pig/test/TestPOCross.java b/test/org/apache/pig/test/TestPOCross.java
deleted file mode 100644
index 2cb53ad50..000000000
--- a/test/org/apache/pig/test/TestPOCross.java
+++ /dev/null
@@ -1,121 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.pig.test;
-
-import java.util.LinkedList;
-import java.util.List;
-import java.util.Random;
-
-import junit.framework.TestCase;
-
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PORead;
-import org.apache.pig.pen.physicalOperators.POCross;
-import org.apache.pig.data.BagFactory;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataType;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.data.TupleFactory;
-import org.apache.pig.impl.plan.OperatorKey;
-import org.junit.Before;
-import org.junit.Test;
-
-public class TestPOCross extends TestCase {
-
-    DataBag [] inputs = new DataBag[2];
-    Random r = new Random();
-    
-    @Before
-    public void setUp() {
-        Tuple t = TupleFactory.getInstance().newTuple();
-        t.append(1);
-        inputs[0] = BagFactory.getInstance().newDefaultBag();
-        inputs[0].add(t);
-        t = TupleFactory.getInstance().newTuple();
-        t.append(2);
-        inputs[0].add(t);
-        t = TupleFactory.getInstance().newTuple();
-        t.append(3);
-        inputs[0].add(t);
-        
-        t = TupleFactory.getInstance().newTuple();
-        t.append(5);
-        inputs[1] = BagFactory.getInstance().newDefaultBag();
-        inputs[1].add(t);
-        t = TupleFactory.getInstance().newTuple();
-        t.append(6);
-        inputs[1].add(t);
-        
-        
-    }
-    
-    @Test
-    public void testCross() throws ExecException {
-        PORead pr1 = new PORead(new OperatorKey("", r.nextLong()), inputs[0]);
-        PORead pr2 = new PORead(new OperatorKey("", r.nextLong()), inputs[1]);
-        List<PhysicalOperator> inputs = new LinkedList<PhysicalOperator>();
-        inputs.add(pr1);
-        inputs.add(pr2);
-        
-        //create the expected data bag
-        DataBag expected = BagFactory.getInstance().newDefaultBag();
-        //List<Tuple> expected = new LinkedList<Tuple>();
-        Tuple t = TupleFactory.getInstance().newTuple();
-        t.append(1);
-        t.append(5);
-        expected.add(t);
-        t = TupleFactory.getInstance().newTuple();
-        t.append(2);
-        t.append(5);
-        expected.add(t);
-        t = TupleFactory.getInstance().newTuple();
-        t.append(3);
-        t.append(5);
-        expected.add(t);
-        t = TupleFactory.getInstance().newTuple();
-        t.append(1);
-        t.append(6);
-        expected.add(t);
-        t = TupleFactory.getInstance().newTuple();
-        t.append(2);
-        t.append(6);
-        expected.add(t);
-        t = TupleFactory.getInstance().newTuple();
-        t.append(3);
-        t.append(6);
-        expected.add(t);
-        
-        POCross poc = new POCross(new OperatorKey("", r.nextLong()), inputs);
-        DataBag obtained = BagFactory.getInstance().newDefaultBag();
-        
-        for(Result res = poc.getNext(t); res.returnStatus != POStatus.STATUS_EOP; res = poc.getNext(t)) {
-            if(res.returnStatus == POStatus.STATUS_OK) 
-                obtained.add((Tuple) res.result);
-            System.out.println(res.result);
-        }
-        
-        assertEquals(expected.size(), obtained.size());
-        
-        assertEquals(0, DataType.compare(expected, obtained));
-        
-        
-    }
-}
diff --git a/test/org/apache/pig/test/TestStore.java b/test/org/apache/pig/test/TestStore.java
index bfe7df1e2..4ca6ff360 100644
--- a/test/org/apache/pig/test/TestStore.java
+++ b/test/org/apache/pig/test/TestStore.java
@@ -67,7 +67,6 @@ import org.apache.pig.impl.logicalLayer.validators.InputOutputFileVisitor;
 import org.apache.pig.impl.plan.CompilationMessageCollector;
 import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.impl.plan.PlanValidationException;
-import org.apache.pig.pen.physicalOperators.POCounter;
 import org.apache.pig.test.utils.GenRandomData;
 import org.apache.pig.test.utils.LogicalPlanTester;
 import org.apache.pig.test.utils.TestHelper;
@@ -87,7 +86,6 @@ public class TestStore extends junit.framework.TestCase {
     PigContext pc;
     POProject proj;
     PigServer pig;
-    POCounter pcount;
         
     String inputFileName;
     String outputFileName;
@@ -136,10 +134,8 @@ public class TestStore extends junit.framework.TestCase {
         PhysicalPlan pp = new PhysicalPlan();
         pp.add(proj);
         pp.add(st);
-        pp.add(pcount);
         //pp.connect(proj, st);
-        pp.connect(proj, pcount);
-        pp.connect(pcount, st);
+        pp.connect(proj, st);
         pc.setExecType(ExecType.LOCAL);
         return new MapReduceLauncher().launchPig(pp, "TestStore", pc);
     }
diff --git a/test/org/apache/pig/test/data/TestIllustrateInput.txt b/test/org/apache/pig/test/data/TestIllustrateInput.txt
new file mode 100644
index 000000000..aa16c4c92
--- /dev/null
+++ b/test/org/apache/pig/test/data/TestIllustrateInput.txt
@@ -0,0 +1,7 @@
+4	6
+3	1
+5	7
+1	4
+0	9
+7	8
+10	9
diff --git a/test/org/apache/pig/test/data/TestIllustrateInput2.txt b/test/org/apache/pig/test/data/TestIllustrateInput2.txt
new file mode 100644
index 000000000..a20b60192
--- /dev/null
+++ b/test/org/apache/pig/test/data/TestIllustrateInput2.txt
@@ -0,0 +1,7 @@
+14	16
+13	11
+15	17
+11	14
+10	19
+17	18
+20	19
diff --git a/test/org/apache/pig/test/data/TestIllustrateInput_invalid.txt b/test/org/apache/pig/test/data/TestIllustrateInput_invalid.txt
new file mode 100644
index 000000000..9196e26a9
--- /dev/null
+++ b/test/org/apache/pig/test/data/TestIllustrateInput_invalid.txt
@@ -0,0 +1,2 @@
+,
+1,2
diff --git a/test/org/apache/pig/test/data/illustrate.pig b/test/org/apache/pig/test/data/illustrate.pig
new file mode 100644
index 000000000..119534130
--- /dev/null
+++ b/test/org/apache/pig/test/data/illustrate.pig
@@ -0,0 +1,6 @@
+A = load 'test/org/apache/pig/test/data/TestIllustrateInput.txt'   as (x:int, y:int);
+B = distinct A;
+C = FILTER B by x  > 3;
+D = FILTER B by x < 3;
+store C into 'Bigger';
+store D into 'Smaller';
diff --git a/test/org/apache/pig/test/data/illustrate2.pig b/test/org/apache/pig/test/data/illustrate2.pig
new file mode 100644
index 000000000..fef501fa6
--- /dev/null
+++ b/test/org/apache/pig/test/data/illustrate2.pig
@@ -0,0 +1,5 @@
+A = load 'test/org/apache/pig/test/data/TestIllustrateInput.txt'   as (x:int, y:int);
+B = FILTER A by x  > 3;
+C = FILTER A by x < 3;
+store B into 'Bigger';
+store C into 'Smaller';
diff --git a/test/org/apache/pig/test/data/illustrate3.pig b/test/org/apache/pig/test/data/illustrate3.pig
new file mode 100644
index 000000000..a9a05ba39
--- /dev/null
+++ b/test/org/apache/pig/test/data/illustrate3.pig
@@ -0,0 +1,8 @@
+A = load 'test/org/apache/pig/test/data/TestIllustrateInput.txt'   as (x:int);
+A1 = group A by x;
+A2 = foreach A1 generate group, COUNT(A);
+store A2 into 'A';
+B = load 'test/org/apache/pig/test/data/TestIllustrateInput.txt'   as (x:double, y:int);
+B1 = group B by x;
+B2 = foreach B1 generate group, COUNT(B);
+store B2 into 'B';
diff --git a/test/org/apache/pig/test/data/illustrate4.pig b/test/org/apache/pig/test/data/illustrate4.pig
new file mode 100644
index 000000000..bf84e558f
--- /dev/null
+++ b/test/org/apache/pig/test/data/illustrate4.pig
@@ -0,0 +1,2 @@
+A = load 'test/org/apache/pig/test/data/TestIllustrateInput_invalid.txt' using PigStorage(',')  as (x:int, y:int);
+STORE A INTO 'A.txt';
diff --git a/test/org/apache/pig/test/data/illustrate5.pig b/test/org/apache/pig/test/data/illustrate5.pig
new file mode 100644
index 000000000..0b26bc4c5
--- /dev/null
+++ b/test/org/apache/pig/test/data/illustrate5.pig
@@ -0,0 +1,5 @@
+A = load 'test/org/apache/pig/test/data/TestIllustrateInput.txt'   as (x:int, y:int);
+B = group A by x;
+C = foreach B generate group, COUNT(A);
+store C into 'out1';
+store A into 'out2';
diff --git a/test/org/apache/pig/test/data/illustrate6.pig b/test/org/apache/pig/test/data/illustrate6.pig
new file mode 100644
index 000000000..eac3d959c
--- /dev/null
+++ b/test/org/apache/pig/test/data/illustrate6.pig
@@ -0,0 +1,3 @@
+a = load 'test/org/apache/pig/test/data/TestIllustrateInput.txt' as (x:int, y:int);
+b = group a all;
+c = foreach b generate COUNT(a) as count; d = foreach a generate x / c.count; store d into 'test.out';
diff --git a/test/org/apache/pig/test/utils/POCastDummy.java b/test/org/apache/pig/test/utils/POCastDummy.java
index 908e2c559..b83ed516d 100644
--- a/test/org/apache/pig/test/utils/POCastDummy.java
+++ b/test/org/apache/pig/test/utils/POCastDummy.java
@@ -21,6 +21,7 @@ import java.util.List;
 
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.data.DataByteArray;
+import org.apache.pig.data.Tuple;
 import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
@@ -103,4 +104,7 @@ public class POCastDummy extends ExpressionOperator {
         return null;
     }    
 
+    public Tuple illustratorMarkup(Object in, Object out, int eqClassIndex) {
+        return null;
+    }
 }
