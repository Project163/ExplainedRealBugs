diff --git a/CHANGES.txt b/CHANGES.txt
index 62f153278..a36ea1d12 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -156,6 +156,8 @@ PIG-3013: BinInterSedes improve chararray sort performance (rohini)
 
 BUG FIXES
 
+PIG-3193: Fix "ant docs" warnings (cheolsoo)
+
 PIG-3186: tar/deb/pkg ant targets should depend on piggybank (lbendig via gates)
 
 PIG-3270: Union onschema failing at runtime when merging incompatible types (knoguchi via daijy)
diff --git a/build.xml b/build.xml
index 4cfb7f18a..2adda84f8 100644
--- a/build.xml
+++ b/build.xml
@@ -576,7 +576,7 @@
         </javadoc>
     </target>
 
-    <target name="javadoc-all" depends="jar-withouthadoop, ivy-javadoc" description="Create documentation including all contrib projects">
+    <target name="javadoc-all" depends="jar-withouthadoop, piggybank, zebra, ivy-javadoc" description="Create documentation including all contrib projects">
         <mkdir dir="${build.javadoc}" />
         <javadoc overview="${src.dir}/overview.html" packagenames="org.apache.pig*,org.apache.hadoop.zebra*" destdir="${build.javadoc}" author="true" version="true" use="true" windowtitle="${Name} ${version} API" doctitle="${Name} ${version} API" bottom="Copyright &amp;copy; ${year} The Apache Software Foundation">
             <packageset dir="${src.dir}" />
@@ -586,6 +586,7 @@
             <classpath>
                 <path refid="javadoc-classpath" />
                 <pathelement path="${output.jarfile.withdependencies}" />
+                <pathelement path="${piggybank.jarfile}"/>
                 <fileset dir="build">
                     <include name="**/zebra-*-dev.jar"/>
                 </fileset>
@@ -1620,6 +1621,10 @@
          <ant antfile="contrib/piggybank/java/build.xml" inheritAll="false"/>
      </target>
 
+     <target name="zebra" depends="jar">
+         <ant antfile="contrib/zebra/build.xml" inheritAll="false"/>
+     </target>
+
      <import file="./build-site.xml" optional="true"/>
 
 </project>
diff --git a/src/docs/forrest.properties b/src/docs/forrest.properties
index 044e2fe53..d2e3261e4 100644
--- a/src/docs/forrest.properties
+++ b/src/docs/forrest.properties
@@ -92,11 +92,7 @@
 #forrest.validate=true
 #forrest.validate.xdocs=${forrest.validate}
 #forrest.validate.skinconf=${forrest.validate}
-
-# PIG-1508: Workaround for http://issues.apache.org/jira/browse/FOR-984
-# Remove when forrest-0.9 is available
-forrest.validate.sitemap=false
-
+#forrest.validate.sitemap=${forrest.validate}
 #forrest.validate.stylesheets=${forrest.validate}
 #forrest.validate.skins=${forrest.validate}
 #forrest.validate.skins.stylesheets=${forrest.validate.skins}
@@ -151,7 +147,7 @@ project.configfile=${project.home}/src/documentation/conf/cli.xconf
 # version.
 # Run "forrest available-plugins" for a list of plug-ins currently available.
 
-project.required.plugins=org.apache.forrest.plugin.output.pdf,org.apache.forrest.plugin.input.simplifiedDocbook
+project.required.plugins=org.apache.forrest.plugin.output.pdf
 
 
 # codename: Dispatcher
diff --git a/src/docs/src/documentation/content/xdocs/basic.xml b/src/docs/src/documentation/content/xdocs/basic.xml
index 51bdce534..2c8a7e221 100644
--- a/src/docs/src/documentation/content/xdocs/basic.xml
+++ b/src/docs/src/documentation/content/xdocs/basic.xml
@@ -102,7 +102,7 @@
                <p>In general, uppercase type indicates elements the system supplies.</p>
                <p>In general, lowercase type indicates elements that you supply.</p>
                <p>(These conventions are not strictly adherered to in all examples.)</p>
-               <p>See <a href="#Case-Sensitivity">Case Sensitivity</a></p>
+               <p>See <a href="#case-sensitivity">Case Sensitivity</a></p>
             </td>
             <td>
                <p>Pig Latin statement:</p>
@@ -249,7 +249,7 @@
    <section id="case-sensitivity">
    <title>Case Sensitivity</title>
    <p>The names (aliases) of relations and fields are case sensitive. The names of Pig Latin functions are case sensitive. 
-   The names of parameters (see <a href="cont.html#Parameter-Sub">Parameter Substitution</a>) and all other Pig Latin keywords (see <a href="#Reserved-Keywords">Reserved Keywords</a>) are case insensitive.</p>
+   The names of parameters (see <a href="cont.html#Parameter-Sub">Parameter Substitution</a>) and all other Pig Latin keywords (see <a href="#reserved-keywords">Reserved Keywords</a>) are case insensitive.</p>
    <p>In the example below, note the following:</p>
    <ul>
       <li>
@@ -1486,11 +1486,11 @@ C = FOREACH G GENERATE COUNT(*)
 <p>Project-range can be used in all cases where the <a href="#sexp">star expression</a> ( * ) is allowed.</p>
 
 <p>Project-range can be used in the following statements:
-<a href="#FOREACH">FOREACH</a>, 
-<a href="#JOIN+%28inner%29">JOIN</a>,  
-<a href="#GROUP">GROUP</a>, 
-<a href="#COGROUP">COGROUP</a>, and  
-<a href="#ORDER+BY">ORDER BY</a> (also when ORDER BY is used within a nested FOREACH block).</p>
+<a href="#foreach">FOREACH</a>,
+<a href="#join-inner">JOIN</a>,
+<a href="#group">GROUP</a>,
+<a href="#cogroup">COGROUP</a>, and
+<a href="#order-by">ORDER BY</a> (also when ORDER BY is used within a nested FOREACH block).</p>
 
 <p>A few examples are shown here:</p>
 <source>
@@ -1557,15 +1557,15 @@ SORT = order IN by $2 .. $3, $6 ..;
    
    
     <!-- ================================================== --> 
-   <section id="Schemas">
+   <section id="schemas">
    <title>Schemas</title>
 
    <p>Schemas enable you to assign names to fields and declare types for fields. Schemas are optional but we encourage you to use them whenever possible; type declarations result in better parse-time error checking and more efficient code execution.</p>  
    
    <p>Schemas for <a href="#schema-simple">simple types</a> and <a href="#schema-complex">complex types</a> can be used anywhere a schema definition is appropriate.</p>   
    
-   <p>Schemas are defined with the <a href="#LOAD">LOAD</a>, <a href="#STREAM">STREAM</a>, and <a href="#FOREACH">FOREACH</a> operators using the AS clause. If you define a schema using the LOAD operator, then it is the load function that enforces the schema 
-   (see <a href="#LOAD">LOAD</a> and <a href="udf.html">User Defined Functions</a> for more information).</p>
+   <p>Schemas are defined with the <a href="#load">LOAD</a>, <a href="#stream">STREAM</a>, and <a href="#foreach">FOREACH</a> operators using the AS clause. If you define a schema using the LOAD operator, then it is the load function that enforces the schema
+   (see <a href="#load">LOAD</a> and <a href="udf.html">User Defined Functions</a> for more information).</p>
 
    <p></p>
    <p><strong>Known Schema Handling</strong></p>
@@ -1601,7 +1601,7 @@ a: Schema for a unknown
    <p></p>
    <p><strong>How Pig Handles Schema</strong></p>
    
-   <p>As shown above, with a few exceptions Pig can infer the schema of a relationship up front. You can examine the schema of particular relation using <a href="test.html#DESCRIBE">DESCRIBE</a>. Pig enforces this computed schema during the actual execution by casting the input data to the expected data type. If the process is successful the results are returned to the user; otherwise, a warning is generated for each record that failed to convert.  Note that Pig does not know the actual types of the fields in the input data prior to the execution; rather, Pig determines the data types and performs the right conversions on the fly.</p>
+   <p>As shown above, with a few exceptions Pig can infer the schema of a relationship up front. You can examine the schema of particular relation using <a href="test.html#describe">DESCRIBE</a>. Pig enforces this computed schema during the actual execution by casting the input data to the expected data type. If the process is successful the results are returned to the user; otherwise, a warning is generated for each record that failed to convert.  Note that Pig does not know the actual types of the fields in the input data prior to the execution; rather, Pig determines the data types and performs the right conversions on the fly.</p>
   
 <p>Having a deterministic schema is very powerful; however, sometimes it comes at the cost of performance. Consider the following example:</p>  
   
@@ -1950,7 +1950,7 @@ DUMP A;
          <tr>
             <td><p>type</p></td>
             <td><p>(Optional) The datatype (all types allowed, bytearray is the default).</p>
-            <p>The type applies to the map value only; the map key is always type chararray (see <a href="#Map">Map</a>).</p>
+            <p>The type applies to the map value only; the map key is always type chararray (see <a href="#map">Map</a>).</p>
             <p>If a type is declared then ALL values in the map must be of this type.</p>
             </td>
          </tr>
@@ -5233,9 +5233,9 @@ B = FOREACH A GENERATE -x, y;
 <title>Relational Operators</title>
 
 <!-- =================================================================== -->
- <section id="COGROUP">
+ <section id="cogroup">
 <title>COGROUP</title>
-   <p>See the <a href="#GROUP">GROUP</a> operator.</p>
+   <p>See the <a href="#group">GROUP</a> operator.</p>
 </section>
 
 <!-- =================================================================== -->
@@ -5815,7 +5815,7 @@ DUMP X;
                <p>A schema using the AS keyword (see <a href="#schemas">Schemas</a>).</p>
                <ul>
                   <li>
-                     <p>If the <a href="#Flatten">FLATTEN</a> operator is used, enclose the schema in parentheses.</p>
+                     <p>If the <a href="#flatten">FLATTEN</a> operator is used, enclose the schema in parentheses.</p>
                   </li>
                   <li>
                      <p>If the FLATTEN operator is not used, don't enclose the schema in parentheses.</p>
@@ -5946,7 +5946,7 @@ DUMP X;
    
    <section id="flatten-example">
    <title>Example: Flatten</title>
-   <p>In this example the <a href="#Flatten">FLATTEN</a> operator is used to eliminate nesting. </p>
+   <p>In this example the <a href="#flatten">FLATTEN</a> operator is used to eliminate nesting. </p>
 <source>
 X = FOREACH C GENERATE group, FLATTEN(A);
 
@@ -6074,7 +6074,7 @@ DUMP X;
             
             
 <!-- =================================================================== -->
-   <section id="GROUP">
+   <section id="group">
    <title>GROUP</title>
    <p>Groups the data in one or more relations.</p>
    <p>Note: The GROUP and COGROUP operators are identical. Both operators work with one or more relations. 
@@ -7094,7 +7094,7 @@ ILLUSTRATE A;
                <p>STORE ... INTO ... USING</p>
             </td>
             <td>
-               <p>See <a href="basic.html#STORE">STORE</a></p>
+               <p>See <a href="basic.html#store">STORE</a></p>
                <p>Store alias2 into the inputLocation using storeFunc, which is then used by the MapReduce job to read its data.</p>
                 
             </td>
@@ -7105,7 +7105,7 @@ ILLUSTRATE A;
                <p>LOAD ... USING ... AS </p>
             </td>
             <td>
-               <p>See <a href="basic.html#LOAD">LOAD</a></p>
+               <p>See <a href="basic.html#load">LOAD</a></p>
                <p>After running mr.jar's MapReduce job, load back the data from outputLocation into alias1 using loadFunc as schema.</p>
             </td>
      </tr>
@@ -7209,7 +7209,7 @@ B = MAPREDUCE 'wordcount.jar' STORE A INTO 'inputDir' LOAD 'outputDir'
             </td>
             <td>
                <p>Increase the parallelism of a job by specifying the number of reduce tasks, n.</p>
-               <p>For more information, see <a href="perf.html#Parallel">Use the Parallel Features</a>.</p>
+               <p>For more information, see <a href="perf.html#parallel">Use the Parallel Features</a>.</p>
             </td>
          </tr> 
    </table></section>
@@ -7672,7 +7672,7 @@ output_var = FILTER input_var BY (field1 is not null);
                   <li>
                   
                   
-                     <p>You can use a built in function (see the <a href="func.html#Load-Store-Functions">Load/Store Functions</a>). PigStorage is the default store function and does not need to be specified (simply omit the USING clause).</p>
+                     <p>You can use a built in function (see the <a href="func.html#load-store-functions">Load/Store Functions</a>). PigStorage is the default store function and does not need to be specified (simply omit the USING clause).</p>
                   </li>
                   <li>
                      <p>You can write your own store function  
@@ -7687,7 +7687,7 @@ output_var = FILTER input_var BY (field1 is not null);
    <title>Usage</title>
    <p>Use the STORE operator to run (execute) Pig Latin statements and save (persist) results to the file system. Use STORE for production scripts and batch mode processing.</p>
    
-   <p>Note: To debug scripts during development, you can use <a href="test.html#DUMP">DUMP</a> to check intermediate results.</p>
+   <p>Note: To debug scripts during development, you can use <a href="test.html#dump">DUMP</a> to check intermediate results.</p>
 </section>
    
    <section>
@@ -8087,7 +8087,7 @@ DUMP U;
                <p>alias</p>
             </td>
             <td>
-               <p>The name for a UDF function or the name for a streaming command (the cmd_alias for the <a href="#STREAM">STREAM</a> operator). </p>
+               <p>The name for a UDF function or the name for a streaming command (the cmd_alias for the <a href="#stream">STREAM</a> operator). </p>
             </td>
          </tr>
          <tr>
@@ -8387,7 +8387,7 @@ X = STREAM A THROUGH Y;
  
      <section>
    <title>Example: DEFINE with STREAM</title>
-<p>In this example a command is defined for use with the <a href="#STREAM">STREAM</a> operator.</p>
+<p>In this example a command is defined for use with the <a href="#stream">STREAM</a> operator.</p>
 <source>
 A = LOAD 'data';
 
diff --git a/src/docs/src/documentation/content/xdocs/cont.xml b/src/docs/src/documentation/content/xdocs/cont.xml
index 14d576c9b..8c364af8c 100644
--- a/src/docs/src/documentation/content/xdocs/cont.xml
+++ b/src/docs/src/documentation/content/xdocs/cont.xml
@@ -124,7 +124,7 @@ public static void main(String[] args) {
 
 <p><strong>Invocation Process</strong></p>
 
-<p>You invoke Pig in the host scripting language through an embedded <a href="#pig-Object">Pig object</a>. </p>  
+<p>You invoke Pig in the host scripting language through an embedded <a href="#pig-object">Pig object</a>. </p>
 
 <p><strong>Compile:</strong> Compile is a static function on the Pig class and in its simplest form takes a fragment of Pig Latin that defines the pipeline as its input:</p>  
 
@@ -371,7 +371,7 @@ public static void main(String[] args) {
 <li id="PigProgressNotificationListener2">The PigProgressNotificationListener interface was modified to add script id to all its methods.  </li>
 </ul>
 <p></p>
-<p>For more details, see <a href="#Java-Objects">Java Objects</a>.</p>
+<p>For more details, see <a href="#java-objects">Java Objects</a>.</p>
 
 </section> 
 
diff --git a/src/docs/src/documentation/content/xdocs/func.xml b/src/docs/src/documentation/content/xdocs/func.xml
index fef221db6..9f8d740e0 100644
--- a/src/docs/src/documentation/content/xdocs/func.xml
+++ b/src/docs/src/documentation/content/xdocs/func.xml
@@ -277,7 +277,7 @@ DUMP X;
     The COUNT function follows syntax semantics and ignores nulls. 
     What this means is that a tuple in the bag will not be counted if the FIRST FIELD in this tuple is NULL. 
     If you want to include NULL values in the count computation, use 
-    <a href="#COUNT-STAR">COUNT_STAR</a>.
+    <a href="#count-star">COUNT_STAR</a>.
    </p>   
    
    <p>
@@ -404,7 +404,7 @@ DUMP X;
    <p>Use the COUNT_STAR function to compute the number of elements in a bag.
    COUNT_STAR requires a preceding GROUP ALL statement for global counts and a GROUP BY statement for group counts.</p>
    <p>COUNT_STAR includes NULL values in the count computation 
-   (unlike <a href="#COUNT">COUNT</a>, which ignores NULL values).
+   (unlike <a href="#count">COUNT</a>, which ignores NULL values).
    </p>
    </section>
    
@@ -1271,7 +1271,7 @@ store A into ‘myoutput.bz’;
    <p>Pig uses BinStorage to load and store the temporary data that is generated between multiple MapReduce jobs.</p>
    <ul>
    <li>BinStorage works with data that is represented on disk in machine-readable format. 
-   BinStorage does NOT support <a href="#Handling-Compression">compression</a>.</li>
+   BinStorage does NOT support <a href="#handling-compression">compression</a>.</li>
    <li>BinStorage supports multiple locations (files, directories, globs) as input.</li>
    </ul>
     <p></p>
@@ -1502,7 +1502,7 @@ STORE X INTO 'output' USING PigDump();
    
    <section>
    <title>Usage</title>
-   <p>PigStorage is the default function used by Pig to load/store the data. PigStorage supports structured text files (in human-readable UTF-8 format) in compressed or uncompressed form (see <a href="#Handling-Compression">Handling Compression</a>). All Pig <a href="basic.html#data-types">data types</a> (both simple and complex) can be read/written using this function. The input data to the load can be a file, a directory or a glob.</p>
+   <p>PigStorage is the default function used by Pig to load/store the data. PigStorage supports structured text files (in human-readable UTF-8 format) in compressed or uncompressed form (see <a href="#handling-compression">Handling Compression</a>). All Pig <a href="basic.html#data-types">data types</a> (both simple and complex) can be read/written using this function. The input data to the load can be a file, a directory or a glob.</p>
 
  <p><strong>Load/Store Statements</strong></p> 
   <p>Load statements – PigStorage expects data to be formatted using field delimiters, either the tab character  ('\t') or other specified character.</p>
@@ -1613,7 +1613,7 @@ a = load '1.txt' as (a0:{t:(m:map[int],d:double)});
    
    <section>
    <title>Usage</title>
-   <p>TextLoader works with unstructured data in UTF8 format. Each resulting tuple contains a single field with one line of input text. TextLoader also supports <a href="#Handling-Compression">compression</a>.</p>
+   <p>TextLoader works with unstructured data in UTF8 format. Each resulting tuple contains a single field with one line of input text. TextLoader also supports <a href="#handling-compression">compression</a>.</p>
    <p>Currently, TextLoader support for compression is limited.</p>  
    <p>TextLoader cannot be used to store data.</p>
    </section>
diff --git a/src/docs/src/documentation/content/xdocs/perf.xml b/src/docs/src/documentation/content/xdocs/perf.xml
index f18107454..108ae7e6a 100644
--- a/src/docs/src/documentation/content/xdocs/perf.xml
+++ b/src/docs/src/documentation/content/xdocs/perf.xml
@@ -36,7 +36,7 @@
 
 <section>
 <title>When the Combiner is Used</title> 
-<p>The combiner is generally used in the case of non-nested foreach where all projections are either expressions on the group column or expressions on algebraic UDFs (see  <a href="#Algebraic-interface">Make Your UDFs Algebraic</a>).</p>
+<p>The combiner is generally used in the case of non-nested foreach where all projections are either expressions on the group column or expressions on algebraic UDFs (see  <a href="#algebraic-interface">Make Your UDFs Algebraic</a>).</p>
 
 <p>Example:</p>
 
diff --git a/src/docs/src/documentation/content/xdocs/start.xml b/src/docs/src/documentation/content/xdocs/start.xml
index 3b14d6435..804d526d7 100644
--- a/src/docs/src/documentation/content/xdocs/start.xml
+++ b/src/docs/src/documentation/content/xdocs/start.xml
@@ -485,7 +485,7 @@ To this:       &lt;property name="pigjar" value="../pig-0.9.0-core.jar" /&gt;
 $ tar -xzf pigtutorial.tar.gz
 </source>
 </li>
-<li>A new directory named pigtmp is created. This directory contains the <a href="#Pig+Tutorial+Files">Pig Tutorial Files</a>. These files work with Hadoop 0.20.2 and include everything you need to run <a href="#pig-script-1">Pig Script 1</a> and <a href="#pig-script-2">Pig Script 2</a>.</li>
+<li>A new directory named pigtmp is created. This directory contains the <a href="#pig-tutorial-files">Pig Tutorial Files</a>. These files work with Hadoop 0.20.2 and include everything you need to run <a href="#pig-script-1">Pig Script 1</a> and <a href="#pig-script-2">Pig Script 2</a>.</li>
 </ol>
 
 
@@ -553,7 +553,7 @@ $ hadoop fs -cat 'script1-hadoop-results/*' | less
 </section>
 
  <!-- ++++++++++++++++++++++++++++++++++ -->   
-<section>
+<section id="pig-tutorial-files">
 <title> Pig Tutorial Files</title>
 
 <p>The contents of the Pig tutorial file (pigtutorial.tar.gz) are described here. </p>
diff --git a/src/docs/src/documentation/content/xdocs/udf.xml b/src/docs/src/documentation/content/xdocs/udf.xml
index b64fef450..68b497e8d 100644
--- a/src/docs/src/documentation/content/xdocs/udf.xml
+++ b/src/docs/src/documentation/content/xdocs/udf.xml
@@ -374,7 +374,7 @@ public class IsEmpty extends FilterFunc {
 
 <section id="udf_simulation">
 <title>Implement UDF by Simulation</title>
-<p>When implementing more advanced types of EvalFuncs, the simpler implementations can be automatically provided by Pig.  Thus if your UDF implements <a href="#Algebraic-Interface">Algebraic</a> then you will get the Accumulator interface and basic the basic EvalFunc exec method for free.  Similarly, if your UDF implements <a href="#Accumulator-Interface">Accumulator Interface</a> you will get the basic EvalFunc exec method for free.  You will not get the Algebraic implemenation.  Note that these free implementations are based on simulation, which might not be the most efficient.  If you wish to ensure the efficiency of your Accumulator of EvalFunc exec method, you may still implement them yourself and your implementations will be used.</p>
+<p>When implementing more advanced types of EvalFuncs, the simpler implementations can be automatically provided by Pig.  Thus if your UDF implements <a href="#algebraic-interface">Algebraic</a> then you will get the Accumulator interface and basic the basic EvalFunc exec method for free.  Similarly, if your UDF implements <a href="#accumulator-interface">Accumulator Interface</a> you will get the basic EvalFunc exec method for free.  You will not get the Algebraic implemenation.  Note that these free implementations are based on simulation, which might not be the most efficient.  If you wish to ensure the efficiency of your Accumulator of EvalFunc exec method, you may still implement them yourself and your implementations will be used.</p>
 </section>
 <!-- +++++++++++++++++++++++++++++++++++++++++++++++++ -->
 <section id="pig-types">
@@ -1328,7 +1328,7 @@ DUMP B;
 
 <section id="udf-interfaces">
 <title>UDF Interfaces</title>
-<p>Java UDFs can be invoked multiple ways. The simplest UDF can just extend EvalFunc, which requires only the exec function to be implemented (see <a href="#eval-functions-write"> How to Write a Simple Eval Function</a>). Every eval UDF must implement this. Additionally, if a function is algebraic, it can implement <code>Algebraic</code> interface to significantly improve query performance in the cases when combiner can be used (see <a href="#Algebraic-Interface">Algebraic Interface</a>). Finally, a function that can process tuples in an incremental fashion can also implement the Accumulator interface to improve query memory consumption (see <a href="#Accumulator-Interface">Accumulator Interface</a>).
+<p>Java UDFs can be invoked multiple ways. The simplest UDF can just extend EvalFunc, which requires only the exec function to be implemented (see <a href="#eval-functions-write"> How to Write a Simple Eval Function</a>). Every eval UDF must implement this. Additionally, if a function is algebraic, it can implement <code>Algebraic</code> interface to significantly improve query performance in the cases when combiner can be used (see <a href="#algebraic-interface">Algebraic Interface</a>). Finally, a function that can process tuples in an incremental fashion can also implement the Accumulator interface to improve query memory consumption (see <a href="#accumulator-interface">Accumulator Interface</a>).
 </p>
 
 <p>The optimizer selects the exact method by which a UDF is invoked based on the UDF type and the query. Note that only a single interface is used at any given time. The optimizer tries to find the most efficient way to execute the function. If a combiner is used and the function implements the Algebraic interface then this interface will be used to invoke the function. If the combiner is not invoked but the accumulator can be used and the function implements Accumulator interface then that interface is used. If neither of the conditions is satisfied then the exec function is used to invoke the UDF. 
