diff --git a/CHANGES.txt b/CHANGES.txt
index 77377b2fb..c483cff1d 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -58,6 +58,8 @@ PIG-4333: Split BigData tests into multiple groups (rohini)
  
 BUG FIXES
 
+PIG-4480: Pig script failure on Tez with split and order by due to missing sample collection (rohini)
+
 PIG-4484: Ant pull jetty-6.1.26.zip on some platform (daijy)
 
 PIG-4479: Pig script with union within nested splits followed by join failed on Tez (rohini)
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POReservoirSample.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POReservoirSample.java
index 38be76080..71d5a6f76 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POReservoirSample.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POReservoirSample.java
@@ -103,6 +103,14 @@ public class POReservoirSample extends PhysicalOperator {
                 rowProcessed++;
             } else if (res.returnStatus == POStatus.STATUS_NULL) {
                 continue;
+            } else if (res.returnStatus == POStatus.STATUS_EOP) {
+                if (this.parentPlan.endOfAllInput) {
+                    break;
+                } else {
+                    // In case of Split can get EOP in between.
+                    // Return here instead of setting lastSample to EOP in getSample
+                    return res;
+                }
             } else {
                 break;
             }
diff --git a/src/org/apache/pig/builtin/mock/Storage.java b/src/org/apache/pig/builtin/mock/Storage.java
index afc1d2923..84d012f80 100644
--- a/src/org/apache/pig/builtin/mock/Storage.java
+++ b/src/org/apache/pig/builtin/mock/Storage.java
@@ -33,6 +33,7 @@ import java.util.Map;
 import java.util.Map.Entry;
 import java.util.Properties;
 import java.util.Set;
+import java.util.TreeMap;
 
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.Writable;
@@ -102,7 +103,7 @@ import org.apache.pig.parser.ParserException;
  *  pigServer.registerQuery("STORE B INTO 'bar' USING mock.Storage();");
  *
  *  assertEquals(schema("a:chararray,b:chararray"), data.getSchema("bar"));
- *  
+ *
  *  List<Tuple> out = data.get("bar");
  *  assertEquals(tuple("a", "a"), out.get(0));
  *  assertEquals(tuple("b", "b"), out.get(1));
@@ -132,7 +133,7 @@ public class Storage extends LoadFunc implements StoreFuncInterface, LoadMetadat
   public static DataBag bag(Tuple... tuples) {
     return new NonSpillableDataBag(Arrays.asList(tuples));
   }
-  
+
   /**
    * @param schema
    * @return the schema represented by the string
@@ -193,7 +194,8 @@ public class Storage extends LoadFunc implements StoreFuncInterface, LoadMetadat
 
   private static class Parts {
     final String location;
-    final Map<String, Collection<Tuple>> parts = new HashMap<String, Collection<Tuple>>();
+    // TreeMap to read part files in order
+    final Map<String, Collection<Tuple>> parts = new TreeMap<String, Collection<Tuple>>();
 
     public Parts(String location) {
       super();
@@ -216,7 +218,7 @@ public class Storage extends LoadFunc implements StoreFuncInterface, LoadMetadat
     }
 
   }
-  
+
   /**
    * An isolated data store to avoid side effects
    *
@@ -249,7 +251,7 @@ public class Storage extends LoadFunc implements StoreFuncInterface, LoadMetadat
     public void set(String location, String schema, Tuple... data) throws ParserException {
       set(location, Utils.getSchemaFromString(schema), Arrays.asList(data));
     }
-    
+
     /**
      * to set the data in a location with a known schema
      *
@@ -316,7 +318,7 @@ public class Storage extends LoadFunc implements StoreFuncInterface, LoadMetadat
     public void set(String location, Tuple... data) {
         set(location, Arrays.asList(data));
     }
-    
+
     /**
      *
      * @param location
@@ -330,7 +332,7 @@ public class Storage extends LoadFunc implements StoreFuncInterface, LoadMetadat
     }
 
     /**
-     * 
+     *
      * @param location
      * @return the schema stored in this location
      */
@@ -352,7 +354,7 @@ public class Storage extends LoadFunc implements StoreFuncInterface, LoadMetadat
   private String location;
 
   private Data data;
-  
+
   private Schema schema;
 
   private Iterator<Tuple> dataBeingRead;
@@ -403,9 +405,9 @@ private MockRecordWriter mockRecordWriter;
   public void setUDFContextSignature(String signature) {
     super.setUDFContextSignature(signature);
   }
-  
+
   // LoadMetaData
-  
+
   @Override
   public ResourceSchema getSchema(String location, Job job) throws IOException {
 	init(location, job);
@@ -477,7 +479,7 @@ private MockRecordWriter mockRecordWriter;
   }
 
   // StoreMetaData
-  
+
   @Override
   public void storeStatistics(ResourceStatistics stats, String location, Job job)
   		throws IOException {
@@ -490,7 +492,7 @@ private MockRecordWriter mockRecordWriter;
 	init(location, job);
 	data.setSchema(location, Schema.getPigSchema(schema));
   }
-  
+
   // Mocks for LoadFunc
 
   private static class MockRecordReader extends RecordReader<Object, Object> {
diff --git a/src/pig-default.properties b/src/pig-default.properties
index 22e62ee95..0d5d9be9f 100644
--- a/src/pig-default.properties
+++ b/src/pig-default.properties
@@ -57,4 +57,4 @@ pig.sql.type=hcat
 pig.output.committer.recovery.support=false
 
 pig.stats.output.size.reader=org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader
-pig.stats.output.size.reader.unsupported=org.apache.hcatalog.pig.HCatStorer,org.apache.hive.hcatalog.pig.HCatStorer,org.apache.pig.piggybank.storage.DBStorage
+pig.stats.output.size.reader.unsupported=org.apache.pig.builtin.mock.Storage,org.apache.hcatalog.pig.HCatStorer,org.apache.hive.hcatalog.pig.HCatStorer,org.apache.pig.piggybank.storage.DBStorage
diff --git a/test/org/apache/pig/test/TestMultiQuery.java b/test/org/apache/pig/test/TestMultiQuery.java
index 0eb36e19d..0c12819b8 100644
--- a/test/org/apache/pig/test/TestMultiQuery.java
+++ b/test/org/apache/pig/test/TestMultiQuery.java
@@ -28,6 +28,7 @@ import java.util.Properties;
 import org.apache.pig.PigConfiguration;
 import org.apache.pig.PigServer;
 import org.apache.pig.backend.executionengine.ExecJob;
+import org.apache.pig.builtin.mock.Storage;
 import org.apache.pig.data.Tuple;
 import org.apache.pig.impl.PigContext;
 import org.junit.After;
@@ -37,7 +38,6 @@ import org.junit.BeforeClass;
 import org.junit.Test;
 import org.junit.runner.RunWith;
 import org.junit.runners.JUnit4;
-import org.apache.pig.builtin.mock.Storage;
 
 @RunWith(JUnit4.class)
 public class TestMultiQuery {
@@ -838,6 +838,37 @@ public class TestMultiQuery {
         iter.next().toString().equals("(world,{(2,world)})");
     }
 
+    @Test
+    public void testMultiQueryJiraPig4480() throws Exception {
+
+        Storage.Data data = Storage.resetData(myPig);
+        data.set("inputLocation",
+                Storage.tuple(1, Storage.bag(Storage.tuple("hello"), Storage.tuple("world"), Storage.tuple("program"))),
+                Storage.tuple(2, Storage.bag(Storage.tuple("my"), Storage.tuple("world"))));
+
+        myPig.setBatchOn();
+        myPig.registerQuery("A = load 'inputLocation' using mock.Storage() as (a:int, b:bag{(c:chararray)});");
+        myPig.registerQuery("A = foreach A generate a, flatten(b);");
+        myPig.registerQuery("A1 = foreach A generate a;");
+        myPig.registerQuery("A1 = distinct A1;");
+        myPig.registerQuery("A2 = filter A by c == 'world';");
+        myPig.registerQuery("A2 = ORDER A2 by a parallel 2;");
+        myPig.registerQuery("store A1 into 'output1' using mock.Storage();");
+        myPig.registerQuery("store A2 into 'output2' using mock.Storage();");
+
+        myPig.executeBatch();
+
+        List<Tuple> actualResults = data.get("output1");
+        List<Tuple> expectedResults = Util.getTuplesFromConstantTupleStrings(
+                new String[] {"(1)", "(2)"});
+        Util.checkQueryOutputs(actualResults.iterator(), expectedResults);
+
+        actualResults = data.get("output2");
+        expectedResults = Util.getTuplesFromConstantTupleStrings(
+                new String[] {"(1, 'world')", "(2, 'world')"});
+        Util.checkQueryOutputs(actualResults.iterator(), expectedResults);
+    }
+
     // --------------------------------------------------------------------------
     // Helper methods
 
