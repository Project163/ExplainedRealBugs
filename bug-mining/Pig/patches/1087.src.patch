diff --git a/CHANGES.txt b/CHANGES.txt
index 056eb3c5c..3a37f76dd 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -58,6 +58,8 @@ PIG-3013: BinInterSedes improve chararray sort performance (rohini)
 
 BUG FIXES
 
+PIG-2924: PigStats should not be assuming all Storage classes to be file-based storage (cheolsoo)
+
 PIG-3046: An empty file name in -Dpig.additional.jars throws an error (prkommireddi via cheolsoo)
 
 PIG-2989: Illustrate for Rank Operator (xalan via gates)
diff --git a/conf/pig.properties b/conf/pig.properties
index a453940c0..001a75ed0 100644
--- a/conf/pig.properties
+++ b/conf/pig.properties
@@ -128,7 +128,7 @@ hcat.bin=/usr/local/hcat/bin/hcat
 
 # By default, the logic to estimate the number of reducers to use for a given job lives in:
 #   org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator
-# This logic can be relaced by implementing the following interface:
+# This logic can be replaced by implementing the following interface:
 #   org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigReducerEstimator
 
 # This class will be invoked to estimate the number of reducers to use.
@@ -139,4 +139,22 @@ hcat.bin=/usr/local/hcat/bin/hcat
 
 #####################################################################
 
+###### Override the default Pig Stats Output Size Reader logic ######
+
+# By default, the size of reducers output is computed as the total size of
+# output files. But since not every storage is file-based, this logic is not
+# always applicable. If that is the case, the logic can be replaced by
+# implementing the following interface:
+#   org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigStatsOutputSizeReader
+
+# This class will be invoked to compute the size of reducers output.
+# pig.stats.output.size.reader = <fully qualified class name of a PigStatsOutputSizeReader implementation>
+
+# If you need to register more than one reader, you can register them as a comma
+# separated list. Every reader implements a boolean supports(POStore sto) method.
+# When there are more than one reader, they are consulted in order, and the
+# first one whose supports() method returns true will be used.
+#
+#####################################################################
+
 #pig.load.default.statements=
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/FileBasedOutputSizeReader.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/FileBasedOutputSizeReader.java
new file mode 100644
index 000000000..3ea517f7c
--- /dev/null
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/FileBasedOutputSizeReader.java
@@ -0,0 +1,82 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.pig.backend.hadoop.executionengine.mapReduceLayer;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
+import org.apache.pig.impl.util.UriUtil;
+
+import java.io.IOException;
+
+/**
+ * Class that computes the size of output for file-based systems.
+ */
+public class FileBasedOutputSizeReader implements PigStatsOutputSizeReader {
+
+    private static final Log log = LogFactory.getLog(FileBasedOutputSizeReader.class);
+
+    /** 
+     * Returns whether the given POStore is supported by this output size reader
+     * or not. We check whether the uri scheme of output file is one of hdfs,
+     * local, and s3.
+     * @param sto POStore
+     */
+    @Override
+    public boolean supports(POStore sto) {
+        return UriUtil.isHDFSFileOrLocalOrS3N(getLocationUri(sto));
+    }
+
+    /**
+     * Returns the total size of output files in bytes
+     * @param sto POStore
+     * @param conf configuration
+     */
+    @Override
+    public long getOutputSize(POStore sto, Configuration conf) throws IOException {
+        if (!supports(sto)) {
+            log.warn("'" + sto.getStoreFunc().getClass().getName()
+                    + "' is not supported by " + getClass().getName());
+            return -1;
+        }
+
+        long bytes = 0;
+        Path p = new Path(getLocationUri(sto));
+        FileSystem fs = p.getFileSystem(conf);
+        FileStatus[] lst = fs.listStatus(p);
+        if (lst != null) {
+            for (FileStatus status : lst) {
+                bytes += status.getLen();
+            }
+        }
+
+        return bytes;
+    }
+
+    /**
+     * Returns the uri of output file in string
+     * @param sto POStore
+     */
+    private static String getLocationUri(POStore sto) {
+        return sto.getSFile().getFileName();
+    }
+}
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigStatsOutputSizeReader.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigStatsOutputSizeReader.java
new file mode 100644
index 000000000..6371fec6f
--- /dev/null
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigStatsOutputSizeReader.java
@@ -0,0 +1,55 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.pig.backend.hadoop.executionengine.mapReduceLayer;
+
+import java.io.IOException;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
+import org.apache.pig.classification.InterfaceAudience;
+import org.apache.pig.classification.InterfaceStability;
+
+/**
+ * Interface to implement when you want to customize the way of computing the
+ * size of output in PigStats. Since not every storage is file-based (e.g.
+ * HBaseStorage), the output size cannot always be computed as the total size of
+ * output files.
+ *
+ * @see FileBasedOutputSizeReader 
+ */
+@InterfaceAudience.Public
+@InterfaceStability.Evolving
+public interface PigStatsOutputSizeReader {
+
+    static final String OUTPUT_SIZE_READER_KEY = "pig.stats.output.size.reader";
+
+    /** 
+     * Returns whether the given PSStore is supported by this output size reader
+     * or not.
+     * @param sto POStore
+     */
+    public boolean supports(POStore sto);
+
+    /**
+     * Returns the size of output in bytes. If the size of output cannot be
+     * computed for any reason, -1 should be returned.
+     * @param sto POStore
+     * @param conf configuration
+     */
+    public long getOutputSize(POStore sto, Configuration conf) throws IOException;
+}
diff --git a/src/org/apache/pig/tools/pigstats/JobStats.java b/src/org/apache/pig/tools/pigstats/JobStats.java
index fca29d726..bdc08a5ac 100644
--- a/src/org/apache/pig/tools/pigstats/JobStats.java
+++ b/src/org/apache/pig/tools/pigstats/JobStats.java
@@ -18,9 +18,8 @@
 
 package org.apache.pig.tools.pigstats;
 
+import java.io.FileNotFoundException;
 import java.io.IOException;
-import java.net.URI;
-import java.net.URISyntaxException;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
@@ -32,9 +31,6 @@ import java.util.Map;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.mapred.Counters;
 import org.apache.hadoop.mapred.JobClient;
 import org.apache.hadoop.mapred.JobID;
@@ -42,13 +38,16 @@ import org.apache.hadoop.mapred.RunningJob;
 import org.apache.hadoop.mapred.TaskReport;
 import org.apache.hadoop.mapred.Counters.Counter;
 import org.apache.pig.PigCounters;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOper;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigStatsOutputSizeReader;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
 import org.apache.pig.classification.InterfaceAudience;
 import org.apache.pig.classification.InterfaceStability;
 import org.apache.pig.newplan.Operator;
 import org.apache.pig.newplan.PlanVisitor;
+import org.apache.pig.impl.PigContext;
 import org.apache.pig.impl.io.FileSpec;
 import org.apache.pig.impl.logicalLayer.FrontendException;
 import org.apache.pig.impl.util.ObjectSerializer;
@@ -523,6 +522,39 @@ public final class JobStats extends Operator {
         }
     }
     
+    /**
+     * Looks up the output size reader from OUTPUT_SIZE_READER_KEY and invokes
+     * it to get the size of output. If OUTPUT_SIZE_READER_KEY is not set,
+     * defaults to FileBasedOutputSizeReader.
+     * @param sto POStore
+     * @param conf configuration
+     */
+    static long getOutputSize(POStore sto, Configuration conf) {
+        PigStatsOutputSizeReader reader = null;
+        String readerNames = conf.get(
+                PigStatsOutputSizeReader.OUTPUT_SIZE_READER_KEY,
+                FileBasedOutputSizeReader.class.getCanonicalName());
+
+        for (String className : readerNames.split(",")) {
+            reader = (PigStatsOutputSizeReader) PigContext.instantiateFuncFromSpec(className);
+            if (reader.supports(sto)) {
+                LOG.info("using output size reader: " + className);
+                try {
+                    return reader.getOutputSize(sto, conf);
+                } catch (FileNotFoundException e) {
+                    LOG.warn("unable to find the output file", e);
+                    return -1;
+                } catch (IOException e) {
+                    LOG.warn("unable to get byte written of the job", e);
+                    return -1;
+                }
+            }
+        }
+
+        LOG.warn("unable to find an output size reader");
+        return -1;
+    }
+
     private void addOneOutputStats(POStore sto) {
         long records = -1;
         if (sto.isMultiStore()) {
@@ -531,30 +563,9 @@ public final class JobStats extends Operator {
         } else {
             records = mapOutputRecords;
         }
-        String location = sto.getSFile().getFileName();        
-        URI uri = null;
-        try {
-            uri = new URI(location);
-        } catch (URISyntaxException e1) {
-            LOG.warn("invalid syntax for output location: " + location, e1);
-        }
-        long bytes = -1;
-        if (uri != null
-                && (uri.getScheme() == null || uri.getScheme()
-                        .equalsIgnoreCase("hdfs"))) {
-            try {
-                Path p = new Path(location);
-                FileSystem fs = p.getFileSystem(conf);
-                FileStatus[] lst = fs.listStatus(p);
-                if (lst != null) {
-                    for (FileStatus status : lst) {
-                        bytes += status.getLen();
-                    } 
-                }
-            } catch (IOException e) {
-                LOG.warn("unable to get byte written of the job", e);
-            }
-        }
+
+        long bytes = getOutputSize(sto, conf);
+        String location = sto.getSFile().getFileName();
         OutputStats ds = new OutputStats(location, bytes, records,
                 (state == JobState.SUCCESS));  
         ds.setPOStore(sto);
diff --git a/test/org/apache/pig/test/TestJobStats.java b/test/org/apache/pig/test/TestJobStats.java
index 42926bc45..cceff84fb 100644
--- a/test/org/apache/pig/test/TestJobStats.java
+++ b/test/org/apache/pig/test/TestJobStats.java
@@ -18,17 +18,32 @@
 
 package org.apache.pig.test;
 
+import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertTrue;
 
+import java.io.File;
+import java.io.IOException;
+import java.io.RandomAccessFile;
 import java.lang.reflect.Constructor;
 import java.lang.reflect.InvocationTargetException;
 import java.lang.reflect.Method;
+import java.util.Properties;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.mapred.JobClient;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.JobID;
 import org.apache.hadoop.mapred.TaskReport;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.pig.FuncSpec;
+import org.apache.pig.StoreFuncInterface;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigStatsOutputSizeReader;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
+import org.apache.pig.backend.hadoop.hbase.HBaseStorage;
+import org.apache.pig.impl.io.FileSpec;
+import org.apache.pig.impl.plan.OperatorKey;
+import org.apache.pig.impl.util.UDFContext;
 import org.apache.pig.tools.pigstats.JobStats;
 import org.apache.pig.tools.pigstats.PigStats;
 import org.apache.pig.tools.pigstats.PigStats.JobGraph;
@@ -162,4 +177,143 @@ public class TestJobStats {
 		assertTrue(msg.startsWith(sb.toString()));
 		
 	}
+
+    /**
+     * Dummy output size reader class for testing JobStats.getOutputSize()
+     */
+    public static class DummyOutputSizeReader implements PigStatsOutputSizeReader {
+        public static final long SIZE = 12345;
+
+        /** 
+         * Returns true always
+         * @param sto POStore
+         */
+        @Override
+        public boolean supports(POStore sto) {
+            return true;
+        }
+
+        /**
+         * Returns a dummy constant value
+         * @param sto POStore
+         * @param conf configuration
+         */
+        @Override
+        public long getOutputSize(POStore sto, Configuration conf) throws IOException {
+            return SIZE;
+        }
+    }
+
+    private static POStore createPOStoreForFileBasedSystem(long size, StoreFuncInterface storeFunc,
+            Configuration conf) throws Exception {
+
+        File file = File.createTempFile("tempFile", ".tmp");
+        file.deleteOnExit();
+        RandomAccessFile f = new RandomAccessFile(file, "rw");
+        f.setLength(size);
+        f.close();
+
+        storeFunc.setStoreLocation(file.getAbsolutePath(), new Job(conf));
+        FuncSpec funcSpec = new FuncSpec(storeFunc.getClass().getCanonicalName());
+        POStore poStore = new POStore(new OperatorKey());
+        poStore.setSFile(new FileSpec(file.getAbsolutePath(), funcSpec));
+        poStore.setStoreFunc(storeFunc);
+        poStore.setUp();
+
+        return poStore;
+    }
+
+    private static POStore createPOStoreForNonFileBasedSystem(StoreFuncInterface storeFunc,
+            Configuration conf) throws Exception {
+
+        String nonFileBasedUri = "hbase://tableName";
+        storeFunc.setStoreLocation(nonFileBasedUri, new Job(conf));
+        FuncSpec funcSpec = new FuncSpec(storeFunc.getClass().getCanonicalName());
+        POStore poStore = new POStore(new OperatorKey());
+        poStore.setSFile(new FileSpec(nonFileBasedUri, funcSpec));
+        poStore.setStoreFunc(storeFunc);
+        poStore.setUp();
+
+        return poStore;
+    }
+
+    @Test
+    public void testGetOuputSizeUsingFileBasedStorage() throws Exception {
+        // By default, FileBasedOutputSizeReader is used to compute the size of output.
+        Configuration conf = new Configuration();
+
+        long size = 2L * 1024 * 1024 * 1024;
+        Method getOutputSize = getJobStatsMethod("getOutputSize", POStore.class, Configuration.class);
+        long outputSize = (Long) getOutputSize.invoke(
+                null, createPOStoreForFileBasedSystem(size, new PigStorageWithStatistics(), conf), conf);
+
+        assertEquals("The returned output size is expected to be the same as the file size",
+                size, outputSize);
+    }
+
+    @Test
+    public void testGetOuputSizeUsingNonFileBasedStorage1() throws Exception {
+        // By default, FileBasedOutputSizeReader is used to compute the size of output.
+        Configuration conf = new Configuration();
+
+        // ClientSystemProps is needed to instantiate HBaseStorage
+        UDFContext.getUDFContext().setClientSystemProps(new Properties());
+        Method getOutputSize = getJobStatsMethod("getOutputSize", POStore.class, Configuration.class);
+        long outputSize = (Long) getOutputSize.invoke(
+                null, createPOStoreForNonFileBasedSystem(new HBaseStorage("colName"), conf), conf);
+
+        assertEquals("The default output size reader returns -1 for a non-file-based uri",
+                -1, outputSize);
+    }
+
+    @Test
+    public void testGetOuputSizeUsingNonFileBasedStorage2() throws Exception {
+        // Register a custom output size reader in configuration
+        Configuration conf = new Configuration();
+        conf.set(PigStatsOutputSizeReader.OUTPUT_SIZE_READER_KEY,
+                DummyOutputSizeReader.class.getName());
+
+        // ClientSystemProps is needed to instantiate HBaseStorage
+        UDFContext.getUDFContext().setClientSystemProps(new Properties());
+        Method getOutputSize = getJobStatsMethod("getOutputSize", POStore.class, Configuration.class);
+        long outputSize = (Long) getOutputSize.invoke(
+                null, createPOStoreForNonFileBasedSystem(new HBaseStorage("colName"), conf), conf);
+
+        assertEquals("The dummy output size reader always returns " + DummyOutputSizeReader.SIZE,
+                DummyOutputSizeReader.SIZE, outputSize);
+    }
+
+    @Test(expected = InvocationTargetException.class)
+    public void testGetOuputSizeUsingNonFileBasedStorage3() throws Exception {
+        // Register an invalid output size reader in configuration, and verify
+        // that an exception is thrown at run-time.
+        Configuration conf = new Configuration();
+        conf.set(PigStatsOutputSizeReader.OUTPUT_SIZE_READER_KEY, "bad_output_size_reader");
+
+        // ClientSystemProps is needed to instantiate HBaseStorage
+        UDFContext.getUDFContext().setClientSystemProps(new Properties());
+        Method getOutputSize = getJobStatsMethod("getOutputSize", POStore.class, Configuration.class);
+
+        getOutputSize.invoke(
+                null, createPOStoreForNonFileBasedSystem(new HBaseStorage("colName"), conf), conf);
+    }
+
+    @Test
+    public void testGetOuputSizeUsingNonFileBasedStorage4() throws Exception {
+        // Register a comma-separated list of readers in configuration, and
+        // verify that the one that supports a non-file-based uri is used.
+        Configuration conf = new Configuration();
+        conf.set(PigStatsOutputSizeReader.OUTPUT_SIZE_READER_KEY,
+                FileBasedOutputSizeReader.class.getName() + ","
+                        + DummyOutputSizeReader.class.getName());
+
+        // ClientSystemProps needs to be initialized to instantiate HBaseStorage
+        UDFContext.getUDFContext().setClientSystemProps(new Properties());
+        Method getOutputSize = getJobStatsMethod("getOutputSize", POStore.class, Configuration.class);
+        long outputSize = (Long) getOutputSize.invoke(
+                null, createPOStoreForNonFileBasedSystem(new HBaseStorage("colName"), conf), conf);
+
+        assertEquals("The dummy output size reader always returns " + DummyOutputSizeReader.SIZE,
+                DummyOutputSizeReader.SIZE, outputSize);
+    }
 }
