diff --git a/CHANGES.txt b/CHANGES.txt
index 514a7c03a..9ec7315f1 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -381,6 +381,8 @@ PIG-2347: Fix Pig Unit tests for hadoop 23 (daijy)
 
 BUG FIXES
 
+PIG-2462: getWrappedSplit is incorrectly returning the first split instead of the current split. (arov via daijy)
+
 PIG-2472: piggybank unit tests write directly to /tmp (thw via daijy)
 
 PIG-2413: e2e test should support testing against two cluster (daijy)
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigRecordReader.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigRecordReader.java
index c04cc4f5e..c52cf1a01 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigRecordReader.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigRecordReader.java
@@ -223,14 +223,14 @@ public class PigRecordReader extends RecordReader<Text, Tuple> {
         // get a record reader for the idx-th chunk
         try {
           
-
-            curReader =  inputformat.createRecordReader(pigSplit.getWrappedSplit(idx), context);
-            LOG.info("Current split being processed "+pigSplit.getWrappedSplit(idx));
+            pigSplit.setCurrentIdx(idx);
+            curReader =  inputformat.createRecordReader(pigSplit.getWrappedSplit(), context);
+            LOG.info("Current split being processed "+pigSplit.getWrappedSplit());
 
             if (idx > 0) {
                 // initialize() for the first RecordReader will be called by MapTask;
                 // we're responsible for initializing subsequent RecordReaders.
-                curReader.initialize(pigSplit.getWrappedSplit(idx), context);
+                curReader.initialize(pigSplit.getWrappedSplit(), context);
                 loadfunc.prepareToRead(curReader, pigSplit);
             }
         } catch (Exception e) {
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigSplit.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigSplit.java
index 7b032ef1b..ff4c37eed 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigSplit.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigSplit.java
@@ -81,6 +81,9 @@ public class PigSplit extends InputSplit implements Writable, Configurable {
     // index
     private int splitIndex;
     
+    // index of current splits being process
+    private int currentIdx;
+    
     // the flag indicates this is a multi-input join (i.e. join)
     // so that custom Hadoop counters will be created in the 
     // back-end to track the number of records for each input.
@@ -122,6 +125,7 @@ public class PigSplit extends InputSplit implements Writable, Configurable {
         this.inputIndex = inputIndex;
         this.targetOps = new ArrayList<OperatorKey>(targetOps);
         this.splitIndex = splitIndex;
+        this.currentIdx = 0;
     }
     
     public List<OperatorKey> getTargetOps() {
@@ -135,7 +139,7 @@ public class PigSplit extends InputSplit implements Writable, Configurable {
      * @return the wrappedSplit
      */
     public InputSplit getWrappedSplit() {
-        return wrappedSplits[0];
+        return wrappedSplits[currentIdx];
     }
     
     /**
@@ -390,4 +394,8 @@ public class PigSplit extends InputSplit implements Writable, Configurable {
     public boolean disableCounter() {
         return disableCounter;
     }
+    
+    public void setCurrentIdx(int idx) {
+        this.currentIdx = idx;
+    }
 }
diff --git a/test/org/apache/pig/test/TestSplitIndex.java b/test/org/apache/pig/test/TestSplitIndex.java
new file mode 100644
index 000000000..f0e5f812c
--- /dev/null
+++ b/test/org/apache/pig/test/TestSplitIndex.java
@@ -0,0 +1,114 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.pig.test;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.Iterator;
+import java.util.Properties;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.mapreduce.RecordReader;
+import org.apache.hadoop.mapreduce.lib.input.FileSplit;
+import org.apache.pig.ExecType;
+import org.apache.pig.PigServer;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigSplit;
+import org.apache.pig.builtin.PigStorage;
+import org.apache.pig.data.Tuple;
+import org.junit.Assert;
+import org.junit.Before;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.JUnit4;
+
+@RunWith(JUnit4.class)
+public class TestSplitIndex {
+    private PigServer pigServer;
+    File inputDir;
+    @Before
+    public void setUp() throws Exception{
+        pigServer = new PigServer(ExecType.LOCAL, new Properties());
+        inputDir = File.createTempFile("tmp", "");
+        inputDir.delete();
+        inputDir.mkdir();
+        Util.createLocalInputFile(inputDir.getAbsolutePath()+"/1", new String[] {"1\t2"});
+        Util.createLocalInputFile(inputDir.getAbsolutePath()+"/2", new String[] {"3\t4"});
+    }
+    
+    @Test
+    public void testSplitIndex() throws Exception {
+        pigServer.registerQuery("a = load '" + inputDir + "' using " + SplitSensitiveLoadFunc.class.getName() + "();");
+        Iterator<Tuple> iter = pigServer.openIterator("a");
+        
+        boolean file1exist=false, file2exist=false;
+        Tuple t = iter.next();
+        if (t.get(2).toString().endsWith("/1"))
+            file1exist = true;
+        if (t.get(2).toString().endsWith("/2"))
+            file2exist = true;
+        t = iter.next();
+        if (t.get(2).toString().endsWith("/1"))
+            file1exist = true;
+        if (t.get(2).toString().endsWith("/2"))
+            file2exist = true;
+        if (!file1exist || !file2exist)
+            Assert.fail();
+    }
+    
+    @Test
+    public void testSplitIndexNoCombine() throws Exception {
+        pigServer.getPigContext().getProperties().setProperty("pig.splitCombination", "false");
+        pigServer.registerQuery("a = load '" + inputDir + "' using " + SplitSensitiveLoadFunc.class.getName() + "();");
+        Iterator<Tuple> iter = pigServer.openIterator("a");
+        
+        boolean file1exist=false, file2exist=false;
+        Tuple t = iter.next();
+        if (t.get(2).toString().endsWith("/1"))
+            file1exist = true;
+        if (t.get(2).toString().endsWith("/2"))
+            file2exist = true;
+        t = iter.next();
+        if (t.get(2).toString().endsWith("/1"))
+            file1exist = true;
+        if (t.get(2).toString().endsWith("/2"))
+            file2exist = true;
+        if (!file1exist || !file2exist)
+            Assert.fail();
+    }
+    
+    public static class SplitSensitiveLoadFunc extends PigStorage {
+        Path path = null;
+        public SplitSensitiveLoadFunc() {
+            super();
+        }
+        @Override
+        public void prepareToRead(RecordReader reader, PigSplit split) {
+            in = reader;
+            path = ((FileSplit)split.getWrappedSplit()).getPath();
+        }
+        
+        @Override
+        public Tuple getNext() throws IOException {
+            Tuple myTuple = super.getNext();
+            if (myTuple != null)
+                myTuple.append(path.toString());
+            return myTuple;
+        }
+    }
+}
