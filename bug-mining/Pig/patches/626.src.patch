diff --git a/CHANGES.txt b/CHANGES.txt
index bd1070019..f5c2d8232 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -24,6 +24,8 @@ INCOMPATIBLE CHANGES
 
 IMPROVEMENTS
 
+PIG-1728: more doc updates (chandec via olgan)
+
 PIG-1793: Add macro expansion to Pig Latin (rding)
 
 PIG-847: Setting twoLevelAccessRequired field in a bag schema should not be required to access fields in the tuples of the bag (daijy)
diff --git a/src/docs/src/documentation/content/xdocs/basic.xml b/src/docs/src/documentation/content/xdocs/basic.xml
index f4043cce9..dbcbdc629 100644
--- a/src/docs/src/documentation/content/xdocs/basic.xml
+++ b/src/docs/src/documentation/content/xdocs/basic.xml
@@ -23,7 +23,6 @@
   </header>
   <body>
 
-
 <!-- CONVENTIONS -->
 <section>
 <title>Conventions</title>
@@ -173,7 +172,7 @@
          
          <tr>
             <td> <p>-- I </p> </td>
-            <td> <p>if, illustrate, inner, input, int, into, is</p> </td>
+            <td> <p>if, illustrate, import, inner, input, int, into, is</p> </td>
          </tr>  
          
          <tr>
@@ -1477,26 +1476,32 @@ C = FOREACH G GENERATE COUNT(*)
    
    
     <!-- ++++++++++++++++++++++++++++++++++ --> 
-   <section>
+   <section id="Schemas">
    <title>Schemas</title>
-   <p>Schemas enable you to assign names to and declare types for fields. Schemas are optional but we encourage you to use them whenever possible; type declarations result in better parse-time error checking and more efficient code execution. </p>
-   <p>Schemas are defined using the AS keyword with the LOAD, STREAM, and FOREACH operators. If you define a schema using the LOAD operator, then it is the load function that enforces the schema (see the <a href="#LOAD">LOAD</a> operator and <a href="udf.html">User Defined Functions</a> for more information).</p>
+   <p>Schemas enable you to assign names to fields and declare types for fields. Schemas are optional but we encourage you to use them whenever possible; type declarations result in better parse-time error checking and more efficient code execution. </p>
+   <p>Schemas are defined for the <a href="#LOAD">LOAD</a>, <a href="#STREAM">STREAM</a>, and <a href="#FOREACH">FOREACH</a> operators using the AS clause. If you define a schema using the LOAD operator, then it is the load function that enforces the schema 
+   (see <a href="#LOAD">LOAD</a> and <a href="udf.html">User Defined Functions</a> for more information).</p>
+   <p></p>
+   <p><strong>Known Schema Handling</strong></p>
    <p>Note the following:</p>
    <ul>
-      <li>
-         <p>You can define a schema that includes both the field name and field type.</p>
-      </li>
-      <li>
-         <p>You can define a schema that includes the field name only; in this case, the field type defaults to bytearray.</p>
-      </li>
-      <li>
-         <p>You can choose not to define a schema; in this case, the field is un-named and the field type defaults to bytearray.</p>
-      </li>
+      <li>You can define a schema that includes both the field name and field type.</li>
+      <li>You can define a schema that includes the field name only; in this case, the field type defaults to bytearray.</li>
+      <li>You can choose not to define a schema; in this case, the field is un-named and the field type defaults to bytearray.</li>
    </ul>
    <p>If you assign a name to a field, you can refer to that field using the name or by positional notation. If you don't assign a name to a field (the field is un-named) you can only refer to the field using positional notation.</p>
    <p>If you assign a type to a field, you can subsequently change the type using the cast operators. If you don't assign a type to a field, the field defaults to bytearray; you can change the default type using the cast operators.</p>
-   
-   <section>
+      <p></p>
+   <p><strong>Unknown Schema Handling</strong></p>
+      <p>Note the following:</p>
+   <ul>
+      <li>When you JOIN/COGROUP/CROSS multiple relations, if any relation has a null schema (no defined schema), the schema for the resulting relation is null. </li>
+      <li>If you FLATTEN a bag with empty inner schema, the schema for the resulting relation is null.</li>
+      <li>If you UNION two relations with incompatible schema, the schema for resulting relation is null.</li>
+      <li>If the schema is null, Pig treats all fields as bytearray (in the backend, Pig will determine the real type for the fields dynamically) </li>
+    </ul>      
+    
+   <section id="schema-load">
    <title>Schemas with LOAD and STREAM Statements</title>
    <p>With LOAD and STREAM statements, the schema following the AS keyword must be enclosed in parentheses.</p>
    <p>In this example the LOAD statement includes a schema definition for simple data types.</p>
@@ -4901,7 +4906,7 @@ DUMP X;
                <p>FOREACH…GENERATE used with a relation (outer bag). Use this syntax:</p>
                <p></p>
                <p>alias = FOREACH alias GENERATE expression [AS schema] [expression [AS schema]….];</p>
-               <p>See <a href="#schemaforeach">Schemas with FOREACH Statements</a></p>
+               <p>See <a href="#schemas">Schemas</a></p>
                
             </td>
          </tr>
@@ -4920,7 +4925,7 @@ DUMP X;
                <p>Where:</p>
                <p>The nested block is enclosed in opening and closing brackets { … }. </p>
                <p>The GENERATE keyword must be the last statement within the nested block.</p>
-               <p>See <a href="#schemaforeach">Schemas with FOREACH Statements</a></p>
+               <p>See <a href="#schemas">Schemas</a></p>
             </td>
          </tr>
          <tr>
@@ -4962,7 +4967,7 @@ DUMP X;
                <p>schema</p>
             </td>
             <td>
-               <p>A schema using the AS keyword (see Schemas).</p>
+               <p>A schema using the AS keyword (see <a href="#schemas">Schemas</a>).</p>
                <ul>
                   <li>
                      <p>If the <a href="#Flatten+Operator">FLATTEN</a> operator is used, enclose the schema in parentheses.</p>
@@ -5228,6 +5233,55 @@ DUMP X;
    
 </section></section>
    
+   
+   <section id="IMPORT">
+   <title>IMPORT</title>   
+   <p>Import macros defined in another Pig script.</p>
+    <section>
+   <title>Syntax</title>
+   <table>
+      <tr> 
+            <td>
+               <p>IMPORT 'pig_script';</p>
+            </td>
+      </tr> 
+   </table>
+   </section>
+     
+   <section>
+   <title>Terms</title>
+   <table>
+      <tr>
+            <td>
+               <p>pig_script </p>
+            </td>
+            <td>
+               <p>The file name of a Pig script containing the macro, enclosed in single quotes; for example, 'mypath/my_macro.pig'.</p>
+               <p></p>
+               <p>Files are imported based on either (1) the given file path or (2) the import path specified via the Pig property pig.import.search.path. If a file path is given, whether absolute or relative to the current directory (starting with . or ..), the import path will be ignored. </p>
+            </td>
+         </tr>
+      </table>   
+   </section>
+      
+   <section>
+   <title>Usage</title>
+   <p>IMPORT adds the macros defined in the Pig script to the Pig Latin namespace; these macros can then be invoked as if they were defined in the same file.</p>
+   
+   <p>See also: <a href="#define-macros">DEFINE (macros)</a></p>
+     </section> 
+     
+   <section>
+   <title>Example</title>
+   <p>In this example, because a path is not given, Pig will use the import path specified in <code>pig.import.search.path</code>.</p>
+<source>
+IMPORT 'my_macro.pig';
+</source>
+   
+    </section> 
+   </section> 
+            
+
    <section id="GROUP">
    <title>GROUP</title>
    <p>Groups the data in one or more relations.</p>
@@ -5720,7 +5774,6 @@ public class SimpleCustomPartitioner extends Partitioner &lt;PigNullableWritable
          </td>
      </tr> 
 
-         
          <tr>
             <td>
                <p>PARALLEL n</p>
@@ -5746,8 +5799,18 @@ public class SimpleCustomPartitioner extends Partitioner &lt;PigNullableWritable
          <p>The GROUP/COGROUP and JOIN operators handle null values differently (see <a href="#nulls_join">Nulls and JOIN Operator</a>).</p>
      </li>
    </ul>
+   <p></p>
+   <p id="self-joins"><strong>Self-Joins</strong></p>
+   <p>To perform self-joins in Pig load the same data multiple times, under different aliases, to avoid naming conflicts.</p>  
+   <p>In this example the same data is loaded twice using aliases A and B.</p>
+   <source>
+grunt> A = load 'mydata';
+grunt> B = load 'mydata';
+grunt> C = join A by $0, B by $0;
+grunt> explain C;
+</source>
     </section>
-    
+
    <section>
    <title>Example</title>
    <p>Suppose we have relations A and B.</p>
@@ -6156,7 +6219,7 @@ DUMP X;
                <p>schema</p>
             </td>
             <td>
-               <p>A schema using the AS keyword, enclosed in parentheses (see Schemas).</p>
+               <p>A schema using the AS keyword, enclosed in parentheses (see <a href="#schemas">Schemas</a>).</p>
                <p>The loader produces the data of the type specified by the schema. If the data does not conform to the schema, depending on the loader, either a null value or an error is generated.</p>
                <p>Note: For performance reasons the loader may not immediately convert the data to the specified format; however, you can still operate on the data assuming the specified type.</p>
             </td>
@@ -6379,6 +6442,8 @@ B = MAPREDUCE 'wordcount.jar' STORE A INTO 'inputDir' LOAD 'outputDir'
    
    <section>
    <title>Usage</title>
+   <p><strong>Note:</strong> ORDER BY is NOT stable; if multiple records have the same ORDER BY key, the order in which these records are returned is not defined and is not guarantted to be the same from one run to the next.</p>
+   
    <p>In Pig, relations are unordered (see <a href="#relations">Relations, Bags, Tuples, Fields</a>):</p>
    <ul>
       <li>
@@ -6801,7 +6866,7 @@ a:8,b:4,c:3
                <p>schema</p>
             </td>
             <td>
-               <p>A schema using the AS keyword, enclosed in parentheses (see Schemas).</p>
+               <p>A schema using the AS keyword, enclosed in parentheses (see <a href="#schemas">Schemas</a>).</p>
             </td>
          </tr> 
    </table></section>
@@ -6881,7 +6946,7 @@ E = STREAM C THROUGH `stream.pl`;
    <title>Example: Schemas</title>
    <p>In this example a schema is specified as part of the STREAM statement.</p>
 <source>
-X = STREAM A THROUGH `stream.pl` as (f1:int, f2;int, f3:int);
+X = STREAM A THROUGH `stream.pl` as (f1:int, f2:int, f3:int);
 </source>
    </section>
    </section>
@@ -6921,21 +6986,24 @@ X = STREAM A THROUGH `stream.pl` as (f1:int, f2;int, f3:int);
                <p>ONSCHEMA </p>  
             </td>
             <td>
-               <p>Use the keyword ONSCHEMA with UNION so that the union is based on column names of the input relations, and not column position. 
-If the following requirements are not met, the statement will throw an error:</p>
+               <p>Use the ONSCHEMA clause to base the union on named fields (rather than positional notation). 
+               If the following requirements are not met, the statement will throw an error:</p>
           <ul>
-             <li>
-             <p>All inputs to the union should have a non null schema. </p>
-             </li>
-             <li>
-             <p>The data type for columns with same name in different input schemas should be compatible. Numeric types are compatible, and if column having same name in different input schemas have different numeric types , an implicit conversion will happen. bytearray type is considered compatible with all other types, a cast will be added to convert to other type. Bags or tuples having different inner schema are considered incompatible. 
-</p>
+             <li>All inputs to the union should have a non null schema.</li>
+             <li>The data type for columns with same name in different input schemas should be compatible:  
+             <ul>
+             <li>Numeric types are compatible, and if column having same name in different input schemas have different numeric types, an implicit conversion will happen.</li>
+             <li>Bytearray type is considered compatible with all other types, a cast will be added to convert to other type. </li>
+             <li>Bags or tuples having different inner schema are considered incompatible. </li>
+             </ul>
              </li>
            </ul>
+           <p></p>
+           <p>See also: <a href="#schemas">Schemas</a></p>
             </td>
          </tr>
-          
-   </table></section>
+   </table>
+   </section>
    
    <section>
    <title>Usage</title>
@@ -6950,8 +7018,44 @@ If the following requirements are not met, the statement will throw an error:</p
       <li>
          <p>Does not eliminate duplicate tuples.</p>
       </li>
-   </ul></section>
+   </ul>
+   <p></p> 
+   <p><strong>Schema Behavior</strong></p>
+   <p>The behavior of schemas for UNION (positional notation / data types) and UNION ONSCHEMA (named fields / data types) is the same, except where noted.</p>
+
+<p>Union on relations with two different sizes result in a null schema (union only): </p>
+<source>
+A: (a1:long, a2:long) 
+B: (b1:long, b2:long, b3:long) 
+A union B: null 
+</source>
+  
+<p>Union columns with incompatible types result in a bytearray type: </p>
+<source>
+A: (a1:long, a2:long) 
+B: (b1:(b11:long, b12:long), b2:long) 
+A union B: (a1:bytearray, a2:long) 
+</source>
+
+<p>Union columns of compatible type will produce an "escalate" type. 
+The priority is chararray &gt; double &gt; float &gt; long &gt; int &gt; bytearray,  tuple|bag|map &gt; bytearray:</p>
+<source>
+A: (a1:int, a2:double, a3:int) 
+B: (b1:float, b2:chararray, b3:bytearray) 
+A union B: (a1:float, a2:chararray, a3:int) 
+</source>
+
+<p>Union of different inner types results in an empty complex type: </p>
+<source>
+A: (a1:(a11:long, a12:int), a2:{(a21:charray, a22:int)}) 
+B: (b1:(b11:int, b12:int), b2:{(b21:int, b22:int)}) 
+A union B: (a1:(), a2:{()}) 
+</source>  
+
+<p>The alias of the first relation is always taken as the alias of the unioned relation field. </p>  
    
+</section>
+
    <section>
    <title>Example</title>
    <p>In this example the union of relation A and B is computed.</p>
@@ -7017,20 +7121,195 @@ DUMP U;
     <!-- UDF STATEMENTS --> 
    <section>
    <title>UDF Statements</title>
+
+  <section id="define-macros">
+   <title>DEFINE (macros)</title>
+   <p>Defines a Pig macro.</p>
+   
+ <section>
+   <title>Syntax</title>
+   <p>Define Macro</p>
+   <table>
+      <tr> 
+            <td>
+               <p>DEFINE macro_name (param [, param ...]) RETURNS alias [, alias ...] { pig_latin_fragment }; </p>
+            </td>
+      </tr> 
+   </table>
+    <p>Expand Macro</p>
+      <table>
+      <tr> 
+            <td>
+               <p>alias [, alias ...] = macro_name (param [, param ...]) ; </p>
+            </td>
+      </tr> 
+   </table>
+ </section>   
    
+<section>
+   <title>Terms</title>
+   <table>
+      <tr>
+            <td>
+               <p>macro_name</p>
+            </td>
+            <td>
+               <p>The name of the macro. Macro names are global.</p>
+            </td>
+      </tr>
+            <tr>
+            <td>
+               <p>param</p>
+            </td>
+            <td>
+               <p>(optional) A comma-separated list of one or more parameters, including IN aliases (Pig relations), enclosed in parentheses, that are referenced in the Pig Latin fragment.</p>
+            </td>
+      </tr>
+            <tr>
+            <td>
+               <p>alias</p>
+            </td>
+            <td>
+               <p>A comma-separated list of one or more OUT aliases (Pig relations) that are referenced in the Pig Latin fragment.</p>
+            </td>
+      </tr>
+      <tr>
+            <td>
+               <p>pig_latin_fragment</p>
+            </td>
+            <td>
+               <p>One or more Pig Latin statements, enclosed in curly brackets.</p>
+            </td>
+      </tr>
+    </table>
+   </section>
+    
    <section>
-   <title>DEFINE</title>
-   <p>Assigns an alias to a UDF function or a streaming command.</p>
+   <title>Usage</title>
+<!-- +++++++++++++++++++++++++++++++++++++++++++++++ --> 
+   <p><strong>Macro Definition</strong></p>
+   <p>A macro definition can appear anywhere in a script as long as it appears prior to the first use. A macro definition can include references to other macros as long as the referenced macros are defined prior to the macro definition. Recursive references are not allowed. </p>
+
+<p>In this example the macro is named my_macro. Note that only aliases A and C are visible from the outside; alias B is not visible from the outside.</p>
+<source>
+ DEFINE my_macro(A, sortkey) RETURNS C {
+    B = FILTER $A BY my_filter(*);
+    $C = ORDER B BY $sortkey;
+}
+</source>
+<p></p>
+
+<!-- +++++++++++++++++++++++++++++++++++++++++++++++ -->
+<p><strong>Macro Expansion</strong></p>
+
+<p>A macro can be expanded inline using the macro expansion syntax. Note the following:</p>
+<ul>
+<li>Any alias in the macro which isn't visible from the outside will be prefixed with a macro name and suffixed with an instance id to avoid namespace collision. </li>
+<li>Macro expansion is not a complete replacement for function calls. Recursive expansions are not supported. </li>
+</ul>
+<p></p>
+<p>In this example my_macro (defined above) is expanded. Because alias B is not visible from the outside it is renamed macro_my_macro_B_0.</p>
+
+<source>
+/* These statements ... */
+
+X = LOAD 'users' AS (user, address, phone);
+Y = my_macro(X, user);
+STORE Y into 'bar';
+
+/* Are expanded into these statements ... */
+
+X = LOAD 'users' AS (user, address, phone);
+macro_my_macro_B_0 = FILTER X BY my_filter(*);
+Y = ORDER macro_my_macro_B_0  BY user;
+STORE Y INTO 'output';
+</source>
+
+<!-- +++++++++++++++++++++++++++++++++++++++++++++++ -->
+<p><strong>Macro Import</strong></p>
+<p>A macro can be imported from another Pig script (see <a href="#IMPORT">IMPORT</a>).</p>
+</section> 
+
+
+ <section>
+ <title>Examples</title>
+<p>In this example no parameters are passed to the macro.</p>
+<source>
+DEFINE my_macro() returns B {
+   D = LOAD 'data' AS (a0:int, a1:int, a2:int);   
+   $B = FILTER D BY ($1 == 8) OR (NOT ($0+$2 > $1));
+};
+
+X = my_macro();
+STORE X INTO 'output';
+</source>
+
+<p>In this example parameters are passed and returned.</p>
+<source>
+DEFINE group_and_count (A, group_key, reducers) RETURNS B {
+   D = GROUP $A BY $group_key PARALLEL $reducers;
+   $B = FOREACH D GENERATE group, COUNT($A);
+};
+
+X = LOAD 'users' AS (user, age, zip);
+Y = group_and_count (X, user, 20);
+Z = group_and_count (X, age, 30);
+STORE Y into 'byuser';
+STORE Z into 'byage';
+</source>
+
+<p>In this example a dummy alias is returned.</p>
+<source>
+DEFINE my_macro(input, output) RETURNS dummy {
+  D = LOAD '$input';   
+  STORE D INTO '$output';
+};
+dummy = my_macro('input.dat', '/tmp/output');
+</source>
+
+<p>In this example a macro (group_with_parallel) refers to another macro (foreach_count).</p>
+<source>
+DEFINE foreach_count(A, C) RETURNS B {
+   $B = FOREACH $A GENERATE group, COUNT($C);
+};
+
+DEFINE group_with_parallel (A, group_key, reducers) RETURNS B {
+   C = GROUP $A BY $group_key PARALLEL $reducers;
+   $B = foreach_count(C, $A);
+};
+       
+/* These statements ... */
+ 
+X = LOAD 'users' AS (user, age, zip);
+Y = group_with_parallel (X, user, 23);
+STORE Y INTO 'byuser';
+
+/* Are expanded into these statements ... */
+
+X = LOAD 'users' AS (user, age, zip);
+macro_group_with_parallel_C_0 = GROUP X by (user) PARALLEL 23;
+Y = FOREACH macro_group_with_parallel_C_0 GENERATE group, COUNT(X);
+STORE Y INTO 'byuser';
+</source>
+</section> 
+   
+  </section> 
+   
+
+   <section id="define">
+   <title>DEFINE (UDFs, streaming)</title>
+   <p>Assigns an alias to a UDF or streaming command.</p>
    
    <section>
-   <title>Syntax</title>
+   <title>Syntax: UDF and streaming</title>
    <table>
       <tr> 
             <td>
                <p>DEFINE alias {function | [`command` [input] [output] [ship] [cache]] };</p>
             </td>
          </tr> 
-   </table></section>
+   </table>
+   </section>
    
    <section>
    <title>Terms</title>
@@ -7238,8 +7517,6 @@ OP = stream IP through 'CMD';
 	</ul>   
    </section>
    
-
-
    
    <section>
    <title>About Cache</title>
@@ -7387,7 +7664,8 @@ B = FOREACH A GENERATE myFunc($0);
                (see <a href="start.html#Pig+Scripts">Pig Scripts</a>).</p>
             </td>
          </tr> 
-   </table></section>
+   </table>
+   </section>
    
    <section>
    <title>Usage</title>
diff --git a/src/docs/src/documentation/content/xdocs/cont.xml b/src/docs/src/documentation/content/xdocs/cont.xml
index 28d2ba4d1..23bbe5c4a 100644
--- a/src/docs/src/documentation/content/xdocs/cont.xml
+++ b/src/docs/src/documentation/content/xdocs/cont.xml
@@ -23,7 +23,14 @@
   
  <!-- ++++++++++++++++++++++++++++++++++ -->    
    <section>
-   <title>Control Flow</title>
+   <title>Pig Macros</title> 
+   <p>Pig Latin supports the definition, expansion, and import of macros.</p>
+   <p>See <a href="basic.html#define-macros">DEFINE (macros)</a> and <a href="/basic.html#IMPORT">IMPORT</a>.</p>
+   </section> 
+  
+ <!-- ++++++++++++++++++++++++++++++++++ -->    
+   <section>
+   <title>Embedded Pig and Control Flow </title>
    
    <p> To enable control flow, you can embed Pig Latin scripts in a host scripting language via a JDBC-like compile, bind, run model. The default scripting language is Jython which is shipped with Pig and available by default. </p>
    
@@ -31,6 +38,10 @@
    <title>Invocation Basics</title>
 <p>Embedded Pig Latin is supported in batch mode only, not interactive mode. When Pig is invoked with a script it will look for a #! line at the beginning of the file. If the basename portion of the path matches jython or python it will use the reference embedding implementation. </p>
 
+<source>
+#! /usr/bin/jython
+</source>
+
 <p>You can also explicitly request that embedded Pig Latin be used by adding the <code>--embedded</code> option to the Pig command line. If this option is passed as an argument, that argument will refer to the language the Pig Latin is embedded in. If no argument is specified, it is taken to refer to the reference implementation.</p>
 
 <source>
@@ -39,7 +50,9 @@ OR
 $ java -cp $lt;jython jars&gt;:$lt;pig jars$gt; [--embedded jython] /tmp/myembedded.py
 </source>
 
-<p>You invoke Pig Latin in the host scripting language through an embedded <a href="#Pig+Object">Pig object</a>. The first step in this process is to compile your script. Compile is a static function on the Pig object and in its simplest form takes a fragment of Pig Latin that defines the pipeline as its input:</p>  
+<p>You invoke Pig Latin in the host scripting language through an embedded <a href="#pig-Object">Pig object</a>. </p>  
+
+<p>The first step in this process is to compile your script. Compile is a static function on the Pig object and in its simplest form takes a fragment of Pig Latin that defines the pipeline as its input:</p>  
 
 <source>
 # COMPILE: complie method returns a Pig object that represents the pipeline
@@ -48,7 +61,8 @@ P = Pig.compile("""A = load '$in’; store A into '$out’;""")
 
 <p>Compile returns an instance of Pig object. This object can have certain values undefined. For example, you may want to define a pipeline without yet specifying the location of the input to the pipeline. The parameter will be indicated by a dollar sign followed by a sequence of alpha-numeric or underscore characters. Values for these parameters must be provided later at the time bind() is called on the Pig object. To call run() on a Pig object without all parameters being bound is an error. </p>
 <p></p>
-<p>Parameters need to be resolved during the <strong>bind </strong>call.</p>
+
+<p>The second step is to resolve the parameters during the <strong>bind </strong>call.</p>
 
 <source>
 input = "original”
@@ -60,7 +74,7 @@ Q = P.bind({'in':input, 'out':output})
 
 <p>Please note that all parameters must be resolved during bind. Having unbound parameters while running your script is an error. Also note that even if your script is fully defined during compile, bind without parameters still must be called.</p>
 
-<p>Bind call returns an instance of <a href="#BoundScript+Object">BoundScript object</a> that can be used to execute the pipeline. The simplest way to execute the pipeline is to call runSingle function. (However, as mentioned later, this works only if  a single set of variables is bound to the parameters. Otherwise, if multiple set of variables are bound, an exception will be thrown if runSingle is called.)</p>
+<p>Bind call returns an instance of <a href="#BoundScript-Object">BoundScript object</a> that can be used to execute the pipeline. The simplest way to execute the pipeline is to call runSingle function. (However, as mentioned later, this works only if  a single set of variables is bound to the parameters. Otherwise, if multiple set of variables are bound, an exception will be thrown if runSingle is called.)</p>
 
 <source>
 
@@ -68,7 +82,7 @@ result = Q.runSingle()
 </source>
 
 
-<p>The function returns a <a href="#PigStats+Object">PigStats object</a> that tells you whether the run succeeded or failed. In case of success, additional run statistics are provided.</p>
+<p>The function returns a <a href="#PigStats-Object">PigStats object</a> that tells you whether the run succeeded or failed. In case of success, additional run statistics are provided.</p>
 
 <p>Here is a complete example:</p>
 
@@ -362,9 +376,9 @@ for i in [0, 1, 2]:
    
  <section>
 <title>Java Objects</title>
- <section>
-<title>Pig Object</title>
-<p><strong>Pig.java</strong></p>
+
+ <section id="pig-Object">
+<title>pig.java Object</title>
 <source>
 public class Pig {    
     /**
@@ -507,9 +521,8 @@ public class Pig {
 </source>
 </section>
 
-<section>
-<title>BoundScript Object</title>
-<p><strong>BoundScript.java</strong></p>
+<section id="BoundScript-Object">
+<title>BoundScript.java Object</title>
 <source>
 public class BoundScript {
     
@@ -595,9 +608,8 @@ public class BoundScript {
 </source>
 </section>  
 
-<section>
-<title>PigStats Object</title>
-<p><strong>PigStats.java</strong></p>
+<section id="PigStats-Object">
+<title>PigStats.java Object</title>
 <source>
 public abstract class PigStats {
     public abstract boolean isEmbedded();
@@ -614,9 +626,8 @@ public abstract class PigStats {
 </source>
 </section>  
 
-<section>
-<title>PigProgressNotificationListener Object</title>
-<p><strong>PigProgressNotificationListener.java</strong></p>
+<section id="PigProgressNotificationListener-Object">
+<title>PigProgressNotificationListener.java Object</title>
 <source>
 
 /** 
diff --git a/src/docs/src/documentation/content/xdocs/func.xml b/src/docs/src/documentation/content/xdocs/func.xml
index 9dd96734b..2abc3dce8 100644
--- a/src/docs/src/documentation/content/xdocs/func.xml
+++ b/src/docs/src/documentation/content/xdocs/func.xml
@@ -1110,17 +1110,47 @@ store A into ‘myoutput.bz’;
    
    <section>
    <title>Usage</title>
-   <p>BinStorage works with data that is represented on disk in machine-readable format. 
-   BinStorage does NOT support <a href="#Handling+Compression">compression</a>.</p>
-   
-    <p>BinStorage is used internally by Pig to store the temporary data that is created between multiple map/reduce jobs.</p>
-      
-    <p>BinStorage supports multiple locations (files, directories, globs) as input.</p>
-    
-      </section>
+
+   <p>Pig uses BinStorage to load and store the temporary data that is generated between multiple MapReduce jobs.</p>
+   <ul>
+   <li>BinStorage works with data that is represented on disk in machine-readable format. 
+   BinStorage does NOT support <a href="#Handling+Compression">compression</a>.</li>
+   <li>BinStorage supports multiple locations (files, directories, globs) as input.</li>
+   </ul>
+    <p></p>
+   <p>Occasionally, users use BinStorage to store their own data. However, because BinStorage is a proprietary binary format, the original data is never in BinStorage - it is always a derivation of some other data.</p>
+<p>We have seen several examples of users doing something like this:</p>
+
+<source>
+a = load 'b.txt' as (id, f);
+b = group a by id;
+store b into 'g' using BinStorage();
+</source>
+
+<p>And then later:</p>
+
+<source>
+a = load 'g/part*' using BinStorage() as (id, d:bag{t:(v, s)});
+b = foreach a generate (double)id, flatten(d);
+dump b;
+</source>
+
+<p>There is a problem with this sequence of events. The first script does not define data types and, as the result, the data is stored as a bytearray and a bag with a tuple that contains two bytearrays. The second script attempts to cast the bytearray to double; however, since the data originated from a different loader, it has no way to know the format of the bytearray or how to cast it to a different type. To solve this problem, Pig:</p>
+
+<ul>
+<li>Sends an error message when the second script is executed: "ERROR 1118: Cannot cast bytes loaded from BinStorage. Please provide a custom converter."</li>
+<li>Allows you to use a custom converter to perform the casting. <br></br>
+<source>
+a = load 'g/part*' using BinStorage('Utf8StorageConverter') as (id, d:bag{t:(v, s)});
+b = foreach a generate (double)id, flatten(d);
+dump b;
+</source>
+</li>
+</ul>
+</section>
    
    <section>
-   <title>Example</title>
+   <title>Examples</title>
    <p>In this example BinStorage is used with the LOAD and STORE functions.</p>
 <source>
 A = LOAD 'data' USING BinStorage();
@@ -1163,8 +1193,12 @@ dump X;
 ()
 ()
 </source>
+</section>
 
-   </section></section>
+<section>
+<title>More Examples</title>
+</section>
+</section>
    
    <section>
    <title>PigStorage</title>
diff --git a/src/docs/src/documentation/content/xdocs/perf.xml b/src/docs/src/documentation/content/xdocs/perf.xml
index bf12b0868..216d74124 100644
--- a/src/docs/src/documentation/content/xdocs/perf.xml
+++ b/src/docs/src/documentation/content/xdocs/perf.xml
@@ -22,6 +22,126 @@
   </header>
   <body> 
   
+<!-- ================================================================== -->
+<!-- COMBINER -->
+<section>
+<title>Combiner</title> 
+
+<p>The Pig combiner is an optimizer that is invoked when the statements in your scripts are arranged in certain ways. The examples below demonstrate when the combiner is used and not used. Whenever possible, make sure the combiner is used as it frequently yields an order of magnitude improvement in performance. </p>
+
+<section>
+<title>When the Combiner is Used</title> 
+<p>The combiner is generally used in the case of non-nested foreach where all projections are either expressions on the group column or expressions on algebraic UDFs (see  <a href="#Make+Your+UDFs+Algebraic">Make Your UDFs Algebraic</a>).</p>
+
+<p>Example:</p>
+
+<source>
+A = load 'studenttab10k' as (name, age, gpa);
+B = group A by age;
+C = foreach B generate ABS(SUM(A.gpa)), COUNT(org.apache.pig.builtin.Distinct(A.name)), (MIN(A.gpa) + MAX(A.gpa))/2, group.age;
+explain C;
+</source>
+<p></p>
+<p>In the above example:</p>
+<ul>
+<li>The GROUP statement can be referred to as a whole or by accessing individual fields (as in the example). </li>
+<li>The GROUP statement and its elements can appear anywhere in the projection. </li>
+</ul>
+
+<p>In the above example, a variety of expressions can be applied to algebraic functions including:</p>
+<ul>
+<li>A column transformation function such as ABS can be applied to an algebraic function SUM.</li>
+<li>An algebraic function (COUNT) can be applied to another algebraic function (Distinct), but only the inner function is computed using the combiner. </li>
+<li>A mathematical expression can be applied to one or more algebraic functions. </li>
+</ul>
+<p></p>
+
+<p>You can check if the combiner is used for your query by running <a href="test.html#EXPLAIN">EXPLAIN</a> on the FOREACH alias as shown above. You should see the combine section in the MapReduce part of the plan:</p>
+
+
+
+<source>
+.....
+Combine Plan
+B: Local Rearrange[tuple]{bytearray}(false) - scope-42
+| |
+| Project[bytearray][0] - scope-43
+|
+|---C: New For Each(false,false,false)[bag] - scope-28
+| |
+| Project[bytearray][0] - scope-29
+| |
+| POUserFunc(org.apache.pig.builtin.SUM$Intermediate)[tuple] - scope-30
+| |
+| |---Project[bag][1] - scope-31
+| |
+| POUserFunc(org.apache.pig.builtin.Distinct$Intermediate)[tuple] - scope-32
+| |
+| |---Project[bag][2] - scope-33
+|
+|---POCombinerPackage[tuple]{bytearray} - scope-36--------
+.....
+</source>
+
+<p>The combiner is also used with a nested foreach as long as the only nested operation used is DISTINCT
+(see <a href="basic.html#FOREACH">FOREACH</a> and <a href="basic.html#nestedblock">Example: Nested Block</a>).
+</p>
+
+<source>
+A = load 'studenttab10k' as (name, age, gpa);
+B = group A by age;
+C = foreach B { D = distinct (A.name); generate group, COUNT(D);}
+</source>
+<p></p>
+<p>Finally, use of the combiner is influenced by the surrounding environment of the GROUP and FOREACH statements.</p>
+ </section>
+
+ <section>
+<title>When the Combiner is Not Used</title> 
+<p>The combiner is generally not used if there is any operator that comes between the GROUP and FOREACH statements in the execution plan. Even if the statements are next to each other in your script, the optimizer might rearrange them. In this example, the optimizer will push FILTER above FOREACH which will prevent the use of the combiner:</p>
+<source>
+A = load 'studenttab10k' as (name, age, gpa);
+B = group A by age;
+C = foreach B generate group, COUNT (A);
+D = filter C by group.age &lt;30;
+</source>
+<p></p>
+
+<p>Please note that the script above can be made more efficient by performing filtering before the GROUP statement:</p>
+
+<source>
+A = load 'studenttab10k' as (name, age, gpa);
+B = filter A by age &lt;30;
+C = group B by age;
+D = foreach C generate group, COUNT (B);
+</source>
+<p></p>
+
+<p><strong>Note:</strong> One exception to the above rule is LIMIT. Starting with Pig 0.9, even if LIMIT comes between GROUP and FOREACH, the combiner will still be used. In this example, the optimizer will push LIMIT above FOREACH but this will not prevent the use of the combiner.</p>
+
+<source>
+A = load 'studenttab10k' as (name, age, gpa);
+B = group A by age;
+C = foreach B generate group, COUNT (A);
+D = limit C 20;
+</source>
+<p></p>
+
+<p>The combiner is also not used in the case where multiple FOREACH statements are associated with the same GROUP:</p>
+
+<source>
+A = load 'studenttab10k' as (name, age, gpa);
+B = group A by age;
+C = foreach B generate group, COUNT (A);
+D = foreach B generate group, MIN (A.gpa). MAX(A.gpa);
+.....
+</source>
+
+<p>Depending on your use case, it might be more efficient (improve performance) to split your script into multiple scripts.</p>
+ </section>
+</section> 
+  
+  
 <!-- ================================================================== -->
 <!-- MEMORY MANAGEMENT -->
 <section>
diff --git a/src/docs/src/documentation/content/xdocs/start.xml b/src/docs/src/documentation/content/xdocs/start.xml
index 10ad9e99c..c1a43cf44 100644
--- a/src/docs/src/documentation/content/xdocs/start.xml
+++ b/src/docs/src/documentation/content/xdocs/start.xml
@@ -117,14 +117,14 @@ Test the Pig installation with this simple command: <code>$ pig -help</code>
 	<title>Run Modes</title> 
 <p>Pig has two run modes or exectypes: </p>
 <ul>
-<li><strong>Local Mode</strong> - To run Pig in local mode, you need access to a single machine; all files are installed and run using your local host and file system. Specify local mode using the -x flag.
+<li><strong>Local Mode</strong> - To run Pig in local mode, you need access to a single machine; all files are installed and run using your local host and file system. Specify local mode using the -x flag (pig -x local).
 </li>
-<li><strong>Mapreduce Mode</strong> - To run Pig in mapreduce mode, you need access to a Hadoop cluster and HDFS installation. Mapreduce mode is the default mode; you can, <em>but don't need to</em>, specify it using the -x flag.
+<li><strong>Mapreduce Mode</strong> - To run Pig in mapreduce mode, you need access to a Hadoop cluster and HDFS installation. Mapreduce mode is the default mode; you can, <em>but don't need to</em>, specify it using the -x flag (pig -x mapreduce).
 </li>
 </ul>
 <p></p>
 
-<p>You can run the Grunt shell and Pig scripts in either mode using the "pig" or "java" command. 
+<p>You can run the Grunt shell and your Pig scripts in either mode using the "pig" command (the bin/pig Perl script) and/or the "java" command (java -cp pig.jar ...).
 You can compile and run embedded programs in either mode using the conventions of the host langugage. </p>
 
 
