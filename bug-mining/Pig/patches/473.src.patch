diff --git a/CHANGES.txt b/CHANGES.txt
index 0671be6f3..05ab426f6 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -110,6 +110,8 @@ PIG-1309: Map-side Cogroup (ashutoshc)
 
 BUG FIXES
 
+PIG-1513: Pig doesn't handle empty input directory (rding)
+
 PIG-1500: guava.jar should be removed from the lib folder (niraj via rding)
 
 PIG-1034: Pig does not support ORDER ... BY group alias (zjffdu)
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/partitioners/SkewedPartitioner.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/partitioners/SkewedPartitioner.java
index 1675106a4..5f580e573 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/partitioners/SkewedPartitioner.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/partitioners/SkewedPartitioner.java
@@ -76,6 +76,9 @@ public class SkewedPartitioner extends Partitioner<PigNullableWritable, Writable
             keyTuple = (Tuple)key.getValueAsPigType();
         }
 
+        // if the partition file is empty, use numPartitions
+        totalReducers = (totalReducers > 0) ? totalReducers : numPartitions;
+        
         indexes = reducerMap.get(keyTuple);
         // if the reducerMap does not contain the key, do the default hash based partitioning
         if (indexes == null) {
@@ -109,7 +112,8 @@ public class SkewedPartitioner extends Partitioner<PigNullableWritable, Writable
             Integer [] redCnt = new Integer[1]; 
             reducerMap = MapRedUtil.loadPartitionFileFromLocalCache(
                     keyDistFile, redCnt, DataType.TUPLE);
-            totalReducers = redCnt[0];
+            // check if the partition file is empty
+            totalReducers = (redCnt[0] == null) ? -1 : redCnt[0];
         } catch (Exception e) {
             throw new RuntimeException(e);
         }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPartitionRearrange.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPartitionRearrange.java
index 32b1a7fa2..dc463cdda 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPartitionRearrange.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPartitionRearrange.java
@@ -92,7 +92,8 @@ public class POPartitionRearrange extends POLocalRearrange {
             reducerMap = MapRedUtil.loadPartitionFileFromLocalCache(
                     keyDistFile, redCnt, DataType.NULL);
 
-            totalReducers = redCnt[0];
+            // check if the partition file is empty
+            totalReducers = (redCnt[0] == null) ? -1 : redCnt[0];
             loaded = true;
         } catch (Exception e) {
             throw new RuntimeException(e);
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/util/MapRedUtil.java b/src/org/apache/pig/backend/hadoop/executionengine/util/MapRedUtil.java
index bcceead5c..eb19ebc30 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/util/MapRedUtil.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/util/MapRedUtil.java
@@ -90,8 +90,10 @@ public class MapRedUtil {
                 keyDistFile, 0);
         DataBag partitionList;
         Tuple t = loader.getNext();
-        if(t==null) {
-            throw new RuntimeException("Empty samples file");
+        if (t == null) {
+            // this could happen if the input directory for sampling is empty
+            log.warn("Empty dist file: " + keyDistFile);
+            return reducerMap;
         }
         // The keydist file is structured as (key, min, max)
         // min, max being the index of the reducers
diff --git a/src/org/apache/pig/impl/builtin/DefaultIndexableLoader.java b/src/org/apache/pig/impl/builtin/DefaultIndexableLoader.java
index 696123e4c..0ca276193 100644
--- a/src/org/apache/pig/impl/builtin/DefaultIndexableLoader.java
+++ b/src/org/apache/pig/impl/builtin/DefaultIndexableLoader.java
@@ -23,6 +23,8 @@ import java.util.LinkedList;
 import java.util.List;
 import java.util.Properties;
 
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.mapreduce.InputFormat;
 import org.apache.hadoop.mapreduce.Job;
@@ -40,7 +42,6 @@ import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigSplit;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad;
-import org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil;
 import org.apache.pig.data.Tuple;
 import org.apache.pig.data.TupleFactory;
 import org.apache.pig.impl.PigContext;
@@ -50,6 +51,8 @@ import org.apache.pig.impl.plan.NodeIdGenerator;
 import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.impl.util.ObjectSerializer;
 
+
+
 /**
  * Used by MergeJoin . Takes an index on sorted data
  * consisting of sorted tuples of the form
@@ -59,6 +62,7 @@ import org.apache.pig.impl.util.ObjectSerializer;
  */
 public class DefaultIndexableLoader extends LoadFunc implements IndexableLoadFunc{
 
+    private static final Log LOG = LogFactory.getLog(DefaultIndexableLoader.class);
     
     // FileSpec of index file which will be read from HDFS.
     private String indexFile;
@@ -160,25 +164,24 @@ public class DefaultIndexableLoader extends LoadFunc implements IndexableLoadFun
                 prevIdxEntry = curIdxEntry;
         }
 
-        if(matchedEntry == null){
+        if (matchedEntry == null) {
+            LOG.warn("Empty index file: input directory is empty");
+        } else {
+        
+            Object extractedKey = extractKeysFromIdxTuple(matchedEntry);
             
-            int errCode = 2165;
-            String errMsg = "Problem in index construction.";
-            throw new ExecException(errMsg,errCode,PigException.BUG);
+            if (extractedKey != null) {
+                Class idxKeyClass = extractedKey.getClass();
+                if( ! firstLeftKey.getClass().equals(idxKeyClass)){
+    
+                    // This check should indeed be done on compile time. But to be on safe side, we do it on runtime also.
+                    int errCode = 2166;
+                    String errMsg = "Key type mismatch. Found key of type "+firstLeftKey.getClass().getCanonicalName()+" on left side. But, found key of type "+ idxKeyClass.getCanonicalName()+" in index built for right side.";
+                    throw new ExecException(errMsg,errCode,PigException.BUG);
+                }
+            } 
         }
         
-        Object extractedKey = extractKeysFromIdxTuple(matchedEntry);
-        
-        if(extractedKey != null){
-            Class idxKeyClass = extractedKey.getClass();
-            if( ! firstLeftKey.getClass().equals(idxKeyClass)){
-
-                // This check should indeed be done on compile time. But to be on safe side, we do it on runtime also.
-                int errCode = 2166;
-                String errMsg = "Key type mismatch. Found key of type "+firstLeftKey.getClass().getCanonicalName()+" on left side. But, found key of type "+ idxKeyClass.getCanonicalName()+" in index built for right side.";
-                throw new ExecException(errMsg,errCode,PigException.BUG);
-            }
-        }
         //add remaining split indexes to splitsAhead array
         int [] splitsAhead = new int[index.size()];
         int splitsAheadIdx = 0;
