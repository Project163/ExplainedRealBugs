diff --git a/CHANGES.txt b/CHANGES.txt
index 3bb5d3fd3..cb3c86c44 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -27,6 +27,7 @@ IMPROVEMENTS
 OPTIMIZATIONS
  
 BUG FIXES
+PIG-5416: Spark unit tests failing randomly with "java.lang.RuntimeException: Unexpected job execution status RUNNING" (knoguchi)
 
 
 Release 0.18.0 - Unreleased
diff --git a/src/org/apache/pig/tools/pigstats/spark/SparkStatsUtil.java b/src/org/apache/pig/tools/pigstats/spark/SparkStatsUtil.java
index 12aae3e9c..eb786c3e4 100644
--- a/src/org/apache/pig/tools/pigstats/spark/SparkStatsUtil.java
+++ b/src/org/apache/pig/tools/pigstats/spark/SparkStatsUtil.java
@@ -56,6 +56,20 @@ public class SparkStatsUtil {
         // this driver thread calling SparkStatusTracker.
         // To workaround this, we will wait for this job to "finish".
         jobStatisticCollector.waitForJobToEnd(jobID);
+
+        // Horrible hack.  After spark 3.2, somehow even after calling waitForJobToEnd
+        // job is still being reported as RUNNING.  Here, adding a poll to wait till the
+        // job gets out of the RUNNING state.
+        // Not sure if this issue is limited to local unit testings or not
+        for( int i=0; i < 10; i++) {
+            if( getJobInfo(jobID, sparkContext).status() != JobExecutionStatus.RUNNING ) {
+                break;
+            }
+            try {
+                Thread.sleep(1000);
+            } catch (InterruptedException e) {}
+        }
+
         sparkPigStats.addJobStats(poStore, sparkOperator, jobID, jobStatisticCollector,
                 sparkContext);
         jobStatisticCollector.cleanup(jobID);
@@ -160,4 +174,4 @@ public class SparkStatsUtil {
     public static void addFailedNativeJobStats(PigStats ps, NativeSparkOperator nativeSparkOperator, Exception e) {
         ((SparkPigStats) ps).addNativeJobStats(nativeSparkOperator, nativeSparkOperator.getJobId(), false, e);
     }
-}
\ No newline at end of file
+}
