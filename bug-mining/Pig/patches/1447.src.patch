diff --git a/CHANGES.txt b/CHANGES.txt
index 6944b1510..fd166a9c0 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -153,6 +153,8 @@ PIG-3882: Multiquery off mode execution is not done in batch and very inefficien
  
 BUG FIXES
 
+PIG-3958: TestMRJobStats is broken in 0.13 and trunk (aniket486)
+
 PIG-3949: HiveColumnarStorage compile failure with Hive 0.14.0 (daijy)
 
 PIG-3960: Compile fail against Hadoop 2.4.0 after PIG-3913 (daijy)
diff --git a/src/org/apache/pig/tools/pigstats/mapreduce/MRJobStats.java b/src/org/apache/pig/tools/pigstats/mapreduce/MRJobStats.java
index b3ba80292..e4b91be8c 100644
--- a/src/org/apache/pig/tools/pigstats/mapreduce/MRJobStats.java
+++ b/src/org/apache/pig/tools/pigstats/mapreduce/MRJobStats.java
@@ -331,6 +331,22 @@ public final class MRJobStats extends JobStats {
         }
     }
 
+    private class TaskStat {
+        int size;
+        long max;
+        long min;
+        long avg;
+        long median;
+
+        public TaskStat(int size, long max, long min, long avg, long median) {
+            this.size = size;
+            this.max = max;
+            this.min = min;
+            this.avg = avg;
+            this.median = median;
+        }
+    }
+
     void addMapReduceStatistics(Job job) {
         TaskReport[] maps = null;
         try {
@@ -338,26 +354,42 @@ public final class MRJobStats extends JobStats {
         } catch (IOException e) {
             LOG.warn("Failed to get map task report", e);
         }
-        if (maps != null && maps.length > 0) {
-            int size = maps.length;
-            long max = 0;
-            long min = Long.MAX_VALUE;
-            long median = 0;
-            long total = 0;
-            long durations[] = new long[size];
-
-            for (int i = 0; i < maps.length; i++) {
-                TaskReport rpt = maps[i];
-                long duration = rpt.getFinishTime() - rpt.getStartTime();
-                durations[i] = duration;
-                max = (duration > max) ? duration : max;
-                min = (duration < min) ? duration : min;
-                total += duration;
-            }
-            long avg = total / size;
+        TaskReport[] reduces = null;
+        try {
+            reduces = HadoopShims.getTaskReports(job, TaskType.REDUCE);
+        } catch (IOException e) {
+            LOG.warn("Failed to get reduce task report", e);
+        }
+        addMapReduceStatistics(maps, reduces);
+    }
 
-            median = calculateMedianValue(durations);
-            setMapStat(size, max, min, avg, median);
+    private TaskStat getTaskStat(TaskReport[] tasks) {
+        int size = tasks.length;
+        long max = 0;
+        long min = Long.MAX_VALUE;
+        long median = 0;
+        long total = 0;
+        long durations[] = new long[size];
+
+        for (int i = 0; i < tasks.length; i++) {
+            TaskReport rpt = tasks[i];
+            long duration = rpt.getFinishTime() - rpt.getStartTime();
+            durations[i] = duration;
+            max = (duration > max) ? duration : max;
+            min = (duration < min) ? duration : min;
+            total += duration;
+        }
+        long avg = total / size;
+
+        median = calculateMedianValue(durations);
+
+        return new TaskStat(size, max, min, avg, median);
+    }
+
+    private void addMapReduceStatistics(TaskReport[] maps, TaskReport[] reduces) {
+        if (maps != null && maps.length > 0) {
+            TaskStat st = getTaskStat(maps);
+            setMapStat(st.size, st.max, st.min, st.avg, st.median);
         } else {
             int m = conf.getInt("mapred.map.tasks", 1);
             if (m > 0) {
@@ -365,31 +397,9 @@ public final class MRJobStats extends JobStats {
             }
         }
 
-        TaskReport[] reduces = null;
-        try {
-            reduces = HadoopShims.getTaskReports(job, TaskType.REDUCE);
-        } catch (IOException e) {
-            LOG.warn("Failed to get reduce task report", e);
-        }
         if (reduces != null && reduces.length > 0) {
-            int size = reduces.length;
-            long max = 0;
-            long min = Long.MAX_VALUE;
-            long median = 0;
-            long total = 0;
-            long durations[] = new long[size];
-
-            for (int i = 0; i < reduces.length; i++) {
-                TaskReport rpt = reduces[i];
-                long duration = rpt.getFinishTime() - rpt.getStartTime();
-                durations[i] = duration;
-                max = (duration > max) ? duration : max;
-                min = (duration < min) ? duration : min;
-                total += duration;
-            }
-            long avg = total / size;
-            median = calculateMedianValue(durations);
-            setReduceStat(size, max, min, avg, median);
+            TaskStat st = getTaskStat(reduces);
+            setReduceStat(st.size, st.max, st.min, st.avg, st.median);
         } else {
             int m = conf.getInt("mapred.reduce.tasks", 1);
             if (m > 0) {
diff --git a/test/org/apache/pig/test/TestMRJobStats.java b/test/org/apache/pig/test/TestMRJobStats.java
index fc7938228..5616fbeac 100644
--- a/test/org/apache/pig/test/TestMRJobStats.java
+++ b/test/org/apache/pig/test/TestMRJobStats.java
@@ -30,7 +30,6 @@ import java.util.Properties;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.JobID;
 import org.apache.hadoop.mapred.TaskReport;
 import org.apache.hadoop.mapreduce.Job;
@@ -111,8 +110,6 @@ public class TestMRJobStats {
 
     @Test
     public void testMedianMapReduceTime() throws Exception {
-
-        JobConf jobConf = new JobConf();
         JobClient jobClient = Mockito.mock(JobClient.class);
 
         // mock methods to return the predefined map and reduce task reports
@@ -124,10 +121,10 @@ public class TestMRJobStats {
         getJobStatsMethod("setId", JobID.class).invoke(jobStats, jobID);
         jobStats.setSuccessful(true);
 
-        getJobStatsMethod("addMapReduceStatistics", JobClient.class, Configuration.class)
-            .invoke(jobStats, jobClient, jobConf);
-        String msg = (String)getJobStatsMethod("getDisplayString", boolean.class)
-            .invoke(jobStats, false);
+        getJobStatsMethod("addMapReduceStatistics", TaskReport[].class, TaskReport[].class)
+            .invoke(jobStats, mapTaskReports, reduceTaskReports);
+        String msg = (String)getJobStatsMethod("getDisplayString")
+            .invoke(jobStats);
 
         System.out.println(JobStats.SUCCESS_HEADER);
         System.out.println(msg);
@@ -149,21 +146,15 @@ public class TestMRJobStats {
         Mockito.when(reduceTaskReports[0].getStartTime()).thenReturn(500L * ONE_THOUSAND);
         Mockito.when(reduceTaskReports[0].getFinishTime()).thenReturn(700L * ONE_THOUSAND);
 
-        JobConf jobConf = new JobConf();
-        JobClient jobClient = Mockito.mock(JobClient.class);
-
-        Mockito.when(jobClient.getMapTaskReports(jobID)).thenReturn(mapTaskReports);
-        Mockito.when(jobClient.getReduceTaskReports(jobID)).thenReturn(reduceTaskReports);
-
         PigStats.JobGraph jobGraph = new PigStats.JobGraph();
         MRJobStats jobStats = createJobStats("JobStatsTest", jobGraph);
         getJobStatsMethod("setId", JobID.class).invoke(jobStats, jobID);
         jobStats.setSuccessful(true);
 
-        getJobStatsMethod("addMapReduceStatistics", JobClient.class, Configuration.class)
-            .invoke(jobStats, jobClient, jobConf);
-        String msg = (String)getJobStatsMethod("getDisplayString", boolean.class)
-            .invoke(jobStats, false);
+        getJobStatsMethod("addMapReduceStatistics", TaskReport[].class, TaskReport[].class)
+            .invoke(jobStats, mapTaskReports, reduceTaskReports);
+        String msg = (String)getJobStatsMethod("getDisplayString")
+            .invoke(jobStats);
         System.out.println(JobStats.SUCCESS_HEADER);
         System.out.println(msg);
 
