diff --git a/CHANGES.txt b/CHANGES.txt
index ed92ea0dd..14ee89df9 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -429,6 +429,8 @@ PIG-1309: Map-side Cogroup (ashutoshc)
 
 BUG FIXES
 
+PIG-1993: PigStorageSchema throw NPE with ColumnPruning (daijy)
+
 PIG-1935: New logical plan: Should not push up filter in front of Bincond (daijy)
 
 PIG-1912: non-deterministic output when a file is loaded multiple times (daijy)
diff --git a/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/PigStorageSchema.java b/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/PigStorageSchema.java
index 8c9c3ee77..f920263f6 100644
--- a/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/PigStorageSchema.java
+++ b/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/PigStorageSchema.java
@@ -91,7 +91,7 @@ public class PigStorageSchema extends PigStorage implements LoadMetadata, StoreM
             // We walk the requiredColumns array to find required fields,
             // and cast those.
             for (int i = 0; i < fieldSchemas.length; i++) {
-                if (mRequiredColumns == null || mRequiredColumns[i]) {
+                if (mRequiredColumns == null || (mRequiredColumns.length>i && mRequiredColumns[i])) {
                     Object val = null;
                     if(tup.get(tupleIdx) != null){
                         byte[] bytes = ((DataByteArray) tup.get(tupleIdx)).get();
diff --git a/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/TestPigStorageSchema.java b/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/TestPigStorageSchema.java
index 2a6b1e9d3..068347843 100644
--- a/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/TestPigStorageSchema.java
+++ b/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/TestPigStorageSchema.java
@@ -219,4 +219,32 @@ public class TestPigStorageSchema {
 
 
     }
+    
+    // See PIG-1993
+    @Test
+    public void testColumnPrune() throws IOException {
+        Util.createInputFile(cluster, "originput2",
+                new String[] {"peter\t1", "samir\t2", "michael\t4",
+                "peter\t2", "peter\t4", "samir\t1", "john\t"
+        });
+        Util.createInputFile(cluster, ".pig_schema",
+                new String[] {
+                "{\"fields\":[{\"name\":\"name\",\"type\":55,\"schema\":null," +
+                "\"description\":\"autogenerated from Pig Field Schema\"}," +
+                "{\"name\":\"val\",\"type\":10,\"schema\":null,\"description\":"+
+                "\"autogenerated from Pig Field Schema\"}],\"version\":0," +
+                "\"sortKeys\":[],\"sortKeyOrders\":[]}"
+        });
+        pig.registerQuery("Events = LOAD 'originput2' USING org.apache.pig.piggybank.storage.PigStorageSchema();");
+        pig.registerQuery("EventsName = foreach Events generate name;");
+        Iterator<Tuple> sessions = pig.openIterator("EventsName");
+        sessions.next().toString().equals("(1)");
+        sessions.next().toString().equals("(2)");
+        sessions.next().toString().equals("(4)");
+        sessions.next().toString().equals("(2)");
+        sessions.next().toString().equals("(4)");
+        sessions.next().toString().equals("(1)");
+        sessions.next().toString().equals("()");
+        Assert.assertFalse(sessions.hasNext());
+    }
 }
