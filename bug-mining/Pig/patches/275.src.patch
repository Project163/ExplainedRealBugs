diff --git a/contrib/zebra/CHANGES.txt b/contrib/zebra/CHANGES.txt
index 20b3c7ad9..e020c651c 100644
--- a/contrib/zebra/CHANGES.txt
+++ b/contrib/zebra/CHANGES.txt
@@ -15,6 +15,9 @@ Trunk (unreleased changes)
 
   BUG FIXES
 
+	PIG-944  Change schema to be taken from StoreConfig instead of
+	TableStorer's constructor (yanz via gates).
+
     PIG-918. Fix infinite loop only columns in first column group are
     specified. (Yan Zhou via rangadi)
  
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/SchemaConverter.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/SchemaConverter.java
index 62be4c1eb..9888361f5 100644
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/SchemaConverter.java
+++ b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/SchemaConverter.java
@@ -21,72 +21,52 @@ package org.apache.hadoop.zebra.pig;
 import java.util.Iterator;
 
 import org.apache.hadoop.zebra.parser.ParseException;
+import org.apache.hadoop.zebra.schema.ColumnType;
 import org.apache.pig.data.DataType;
 import org.apache.pig.impl.logicalLayer.FrontendException;
 import org.apache.pig.impl.logicalLayer.schema.Schema;
 import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;
 
-/**
- * A simple schema converter that only understands three field types.
- */
 class SchemaConverter {
-  enum FieldSchemaMaker {
-    SimpleField("SF_") {
-      @Override
-      FieldSchema toFieldSchema(String name) {
-        return new FieldSchema(name, DataType.BYTEARRAY);
-      }
-    },
-
-    MapField("MF_") {
-      @Override
-      FieldSchema toFieldSchema(String name) {
-        // TODO: how to convey key and value types?
-        return new FieldSchema(name, DataType.MAP);
-      }
-    },
-
-    MapListField("MLF_") {
-      @Override
-      FieldSchema toFieldSchema(String name) throws FrontendException {
-        Schema tupleSchema = new Schema();
-        tupleSchema.add(MapField.toFieldSchema(null));
-        tupleSchema.setTwoLevelAccessRequired(true);
-        return new FieldSchema(name, tupleSchema, DataType.BAG);
-      }
-    };
-
-    private String prefix;
-
-    FieldSchemaMaker(String prefix) {
-      this.prefix = prefix;
-    }
-
-    abstract FieldSchema toFieldSchema(String name) throws FrontendException;
-
-    public static FieldSchema makeFieldSchema(String colname)
-        throws FrontendException {
-      for (FieldSchemaMaker e : FieldSchemaMaker.values()) {
-        if (colname.startsWith(e.prefix)) {
-          return e.toFieldSchema(colname.substring(e.prefix.length()));
-        }
-      }
-      throw new FrontendException("Cannot determine type from column name");
-    }
-    
-    public static String makeColumnName(FieldSchema fs)
-        throws FrontendException {
-      if (fs.alias == null) {
-        throw new FrontendException("No alias provided for field schema");
-      }
-      for (FieldSchemaMaker e : FieldSchemaMaker.values()) {
-        FieldSchema expected = e.toFieldSchema("dummy");
-        if (FieldSchema.equals(fs, expected, false, true)) {
-          return e.prefix + fs.alias;
-        }
-      }
-      throw new FrontendException("Unsupported field schema");
+  public static ColumnType toTableType(byte ptype)
+  {
+    ColumnType ret;
+    switch (ptype) {
+      case DataType.INTEGER:
+        ret = ColumnType.INT; 
+        break;
+      case DataType.LONG:
+        ret = ColumnType.LONG; 
+        break;
+      case DataType.FLOAT:
+        ret = ColumnType.FLOAT; 
+        break;
+      case DataType.DOUBLE:
+        ret = ColumnType.DOUBLE; 
+        break;
+      case DataType.BOOLEAN:
+        ret = ColumnType.BOOL; 
+        break;
+      case DataType.BAG:
+        ret = ColumnType.COLLECTION; 
+        break;
+      case DataType.MAP:
+        ret = ColumnType.MAP; 
+        break;
+      case DataType.TUPLE:
+        ret = ColumnType.RECORD; 
+        break;
+      case DataType.CHARARRAY:
+        ret = ColumnType.STRING; 
+        break;
+      case DataType.BYTEARRAY:
+        ret = ColumnType.BYTES; 
+        break;
+      default:
+        ret = null;
+        break;
     }
+    return ret;
   }
   
   public static Schema toPigSchema(
@@ -97,24 +77,39 @@ class SchemaConverter {
     	org.apache.hadoop.zebra.schema.Schema.ColumnSchema columnSchema = 
     		tschema.getColumn(col);
 			if (columnSchema != null) {
-        ret.add(new FieldSchema(col, columnSchema.getType().pigDataType()));
+        ColumnType ct = columnSchema.getType();
+        if (ct == org.apache.hadoop.zebra.schema.ColumnType.RECORD ||
+            ct == org.apache.hadoop.zebra.schema.ColumnType.COLLECTION)
+          ret.add(new FieldSchema(col, toPigSchema(columnSchema.getSchema()), ct.pigDataType()));
+        else
+          ret.add(new FieldSchema(col, ct.pigDataType()));
 			} else {
 				ret.add(new FieldSchema(null, null));
 			}
     }
-    
     return ret;
   }
 
   public static org.apache.hadoop.zebra.schema.Schema fromPigSchema(
       Schema pschema) throws FrontendException, ParseException {
-    String[] colnames = new String[pschema.size()];
-    int i = 0;
-    for (Iterator<FieldSchema> it = pschema.getFields().iterator(); it
-        .hasNext(); ++i) {
-      FieldSchema fs = it.next();
-      colnames[i] = FieldSchemaMaker.makeColumnName(fs);
+    org.apache.hadoop.zebra.schema.Schema tschema = new org.apache.hadoop.zebra.schema.Schema();
+    Schema.FieldSchema columnSchema;
+    for (int i = 0; i < pschema.size(); i++) {
+    	columnSchema = pschema.getField(i);
+    	if (columnSchema != null) {
+    		if (DataType.isSchemaType(columnSchema.type))
+    			tschema.add(new org.apache.hadoop.zebra.schema.Schema.ColumnSchema(columnSchema.alias, 
+    					fromPigSchema(columnSchema.schema), toTableType(columnSchema.type)));
+    		else if (columnSchema.type == DataType.MAP)
+    			tschema.add(new org.apache.hadoop.zebra.schema.Schema.ColumnSchema(columnSchema.alias, 
+    					new org.apache.hadoop.zebra.schema.Schema(new org.apache.hadoop.zebra.schema.Schema.ColumnSchema(null, 
+    							org.apache.hadoop.zebra.schema.ColumnType.BYTES)), toTableType(columnSchema.type)));
+    		else
+    			tschema.add(new org.apache.hadoop.zebra.schema.Schema.ColumnSchema(columnSchema.alias, toTableType(columnSchema.type)));
+		  } else {
+		  	tschema.add(new org.apache.hadoop.zebra.schema.Schema.ColumnSchema(null, ColumnType.ANY));
+		  }
     }
-    return new org.apache.hadoop.zebra.schema.Schema(colnames);
+    return tschema;
   }
 }
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/TableStorer.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/TableStorer.java
index 399de6b28..694d48c78 100644
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/TableStorer.java
+++ b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/TableStorer.java
@@ -35,18 +35,17 @@ import org.apache.hadoop.zebra.io.TableInserter;
 import org.apache.hadoop.zebra.parser.ParseException;
 import org.apache.pig.StoreConfig;
 import org.apache.pig.StoreFunc;
+import org.apache.pig.impl.logicalLayer.schema.Schema;
 import org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil;
 import org.apache.pig.data.Tuple;
 
 public class TableStorer implements StoreFunc {
-	private String schemaString;
 	private String storageHintString;
 
 	public TableStorer() {	  
 	}
 
-	public TableStorer(String schemaStr, String storageHintStr) throws ParseException, IOException {
-		schemaString = schemaStr;
+	public TableStorer(String storageHintStr) throws ParseException, IOException {
 		storageHintString = storageHintStr;
 	}
   
@@ -69,10 +68,6 @@ public class TableStorer implements StoreFunc {
 	public Class getStorePreparationClass() throws IOException {
 		return TableOutputFormat.class;
 	}
-
-	public String getSchemaString() {
-		return schemaString;  
-	}
   
 	public String getStorageHintString() {
 		return storageHintString;  
@@ -94,10 +89,16 @@ class TableOutputFormat implements OutputFormat<BytesWritable, Tuple> {
 	@Override
 	public void checkOutputSpecs(FileSystem ignored, JobConf job) throws IOException {
 		StoreConfig storeConfig = MapRedUtil.getStoreConfig(job);
-		String location = storeConfig.getLocation();
+		String location = storeConfig.getLocation(), schemaStr;
+    Schema schema = storeConfig.getSchema();
+    try {
+      schemaStr = SchemaConverter.fromPigSchema(schema).toString();
+    } catch (ParseException e) {
+      throw new IOException("Exception thrown from SchemaConverter: " + e.getMessage());
+    }
 		TableStorer storeFunc = (TableStorer)MapRedUtil.getStoreFunc(job);   
 		BasicTable.Writer writer = new BasicTable.Writer(new Path(location), 
-				storeFunc.getSchemaString(), storeFunc.getStorageHintString(), false, job);
+				schemaStr, storeFunc.getStorageHintString(), false, job);
 		writer.finish();
 	}
     
@@ -134,11 +135,11 @@ class TableRecordWriter implements RecordWriter<BytesWritable, Tuple> {
 		writer.finish();
 	}
 
-	@Override
-	public void write(BytesWritable key, Tuple value) throws IOException {
-		if (key == null) {
-			key = KEY0;
-		}
-		inserter.insert(key, value);
-	}
+  @Override
+  public void write(BytesWritable key, Tuple value) throws IOException {
+    if (key == null) {
+      key = KEY0;
+    }
+    inserter.insert(key, value);
+  }
 }
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestCollectionTableStorer.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestCollectionTableStorer.java
index f3f28c9c5..e0c7e6a4e 100644
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestCollectionTableStorer.java
+++ b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestCollectionTableStorer.java
@@ -143,6 +143,6 @@ public class TestCollectionTableStorer {
 
     pigServer.store("records", new Path(pathTable, "store").toString(),
         TableStorer.class.getCanonicalName()
-            + "('c:collection(a:double, b:float, c:bytes)', '[c]')");
+            + "('[c]')");
   }
 }
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMapTableStorer.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMapTableStorer.java
index 71efb631b..c2e7214ec 100644
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMapTableStorer.java
+++ b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMapTableStorer.java
@@ -132,6 +132,6 @@ public class TestMapTableStorer {
     pigServer
         .store("records", new Path(pathTable, "store").toString(),
             TableStorer.class.getCanonicalName()
-                + "('m:map(string)', '[m#{a|b}]')");
+                + "('[m#{a|b}]')");
   }
 }
\ No newline at end of file
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestRealCluster.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestRealCluster.java
index 1b5672b94..9624085c8 100644
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestRealCluster.java
+++ b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestRealCluster.java
@@ -75,7 +75,7 @@ public class TestRealCluster {
     /*
      * pigServer.store("records", new Path(pathTable, "store").toString(),
      * TableStorer.class.getCanonicalName() +
-     * "('SF_a,SF_b,SF_c,SF_d,SF_e,SF_f,SF_g', '[SF_a, SF_b, SF_c]; [SF_e]')" );
+     * "('[SF_a, SF_b, SF_c]; [SF_e]')" );
      */
   }
 }
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestSimpleType.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestSimpleType.java
index 95dee816a..f98216e25 100644
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestSimpleType.java
+++ b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestSimpleType.java
@@ -265,7 +265,7 @@ public class TestSimpleType {
             "records",
             new Path(newPath, "store").toString(),
             TableStorer.class.getCanonicalName()
-                + "('s1:bool, s2:int, s3:long, s4:float, s5:string, s6:bytes', '[s1, s2]; [s3, s4]')");
+                + "('[s1, s2]; [s3, s4]')");
 
   }
 
@@ -290,7 +290,7 @@ public class TestSimpleType {
             "newRecord",
             newPath.toString(),
             TableStorer.class.getCanonicalName()
-                + "('s1:bool, s2:int, s3:long, s4:float, s5:string, s6:bytes', '[s1, s2]; [s3, s4]')");
+                + "('[s1, s2]; [s3, s4]')");
 
     // check new table content
     String query3 = "newRecords = LOAD '"
@@ -340,7 +340,7 @@ public class TestSimpleType {
     Path newPath = new Path(getCurrentMethodName());
     pigServer.store("newRecord", newPath.toString(), TableStorer.class
         .getCanonicalName()
-        + "('s1:bool, s2:int, s3:long, s4:float, s5:string, s6:bytes', '')");
+        + "('')");
 
     // check new table content
     String query3 = "newRecords = LOAD '"
@@ -390,7 +390,7 @@ public class TestSimpleType {
     Path newPath = new Path(getCurrentMethodName());
     pigServer.store("newRecord", newPath.toString(), TableStorer.class
         .getCanonicalName()
-        + "('s1:bool, s2:int, s3:long, s4:float, s5:string, s6:bytes', '[]')");
+        + "('[]')");
 
     // check new table content
     String query3 = "newRecords = LOAD '"
@@ -443,7 +443,7 @@ public class TestSimpleType {
             "records",
             new Path(newPath, "store").toString(),
             TableStorer.class.getCanonicalName()
-                + "('s2:int, s3:long, s4:float, s5:string, s6:bytes', '[s1, s2]; [s3, s4]')");
+                + "('[s7, s2]; [s3, s4]')");
     Assert.assertNotNull(pigJob.getException());
     System.out.println(pigJob.getException());
   }
@@ -469,7 +469,7 @@ public class TestSimpleType {
               "records",
               new Path(newPath, "store").toString(),
               TableStorer.class.getCanonicalName()
-                  + "('s1:bool, s2:int, s3:long, s4:float, s5:string, s6:bytes', '[s1, s2]; [s1, s4]')");
+                  + "('[s1, s2]; [s1, s4]')");
       Assert.assertNotNull(pigJob.getException());
       System.out.println(pigJob.getException());
   }
@@ -495,7 +495,7 @@ public class TestSimpleType {
             "records",
             new Path(newPath, "store").toString(),
             TableStorer.class.getCanonicalName()
-                + "('s1:bool, s2:int, s3:long, s4:float, s5:string, s6:bytes', '[s1]; [s1]')");
+                + "('[s1]; [s1]')");
     Assert.assertNotNull(pigJob.getException());
     System.out.println(pigJob.getException());
   }
@@ -521,7 +521,7 @@ public class TestSimpleType {
             "records",
             new Path(newPath, "store").toString(),
             TableStorer.class.getCanonicalName()
-                + "('s1:int, s2:int, s3:long, s4:float, s5:string, s6:bytes', '[s1, s2]; [s3, s4]')");
+                + "('[s1, s2]; [s3, s4]')");
     Assert.assertNotNull(pigJob.getException());
     System.out.println(pigJob.getException());
   }
@@ -546,7 +546,7 @@ public class TestSimpleType {
             "records",
             path.toString(),
             TableStorer.class.getCanonicalName()
-                + "('s1:bool, s2:int, s3:long, s4:float, s5:string, s6:bytes', '[s1, s2]; [s3, s4]')");
+                + "('[s1, s2]; [s3, s4]')");
     Assert.assertNotNull(pigJob.getException());
     System.out.println(pigJob.getException());
   }
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestTableStorer.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestTableStorer.java
index 251208729..f77b6e6a1 100644
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestTableStorer.java
+++ b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestTableStorer.java
@@ -139,7 +139,7 @@ public class TestTableStorer {
             "records",
             new Path(pathTable, "store").toString(),
             TableStorer.class.getCanonicalName()
-                + "('SF_a,SF_b,SF_c,SF_d,SF_e,SF_f,SF_g', '[SF_a, SF_b, SF_c]; [SF_e]')");
+                + "('[SF_a, SF_b, SF_c]; [SF_e]')");
 
   }
 }
\ No newline at end of file
