diff --git a/CHANGES.txt b/CHANGES.txt
index b141f069b..77a116540 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -177,6 +177,8 @@ PIG-3882: Multiquery off mode execution is not done in batch and very inefficien
  
 BUG FIXES
 
+PIG-3996: Delete zebra from svn (cheolsoo)
+
 PIG-3991: TestErrorHandling.tesNegative7 is broken in trunk/branch-0.13 (cheolsoo)
 
 PIG-3990: ant docs is broken in trunk/branch-0.13 (cheolsoo)
diff --git a/build.xml b/build.xml
index 7dcbe7573..6eafd55ff 100644
--- a/build.xml
+++ b/build.xml
@@ -458,10 +458,6 @@
         <ant target="clean" dir="contrib/piggybank/java" inheritAll="false"/>
     </target>
 
-    <target name="clean-zebra" description="Cleanup Zebra">
-        <ant target="clean" dir="contrib/zebra" inheritAll="false"/>
-    </target>
-
     <target name="clean-tutorial" description="Cleanup Tutorial">
         <ant target="clean" dir="tutorial" inheritAll="false"/>
     </target>
@@ -629,24 +625,19 @@
         </javadoc>
     </target>
 
-    <target name="javadoc-all" depends="jar-withouthadoop, piggybank, zebra, ivy-javadoc" description="Create documentation including all contrib projects">
+    <target name="javadoc-all" depends="jar-withouthadoop, piggybank, ivy-javadoc" description="Create documentation including all contrib projects">
         <mkdir dir="${build.javadoc}" />
-        <javadoc overview="${src.dir}/overview.html" packagenames="org.apache.pig*,org.apache.hadoop.zebra*" destdir="${build.javadoc}" author="true" version="true" use="true" windowtitle="${Name} ${version} API" doctitle="${Name} ${version} API" bottom="Copyright &amp;copy; ${year} The Apache Software Foundation">
+        <javadoc overview="${src.dir}/overview.html" packagenames="org.apache.pig*" destdir="${build.javadoc}" author="true" version="true" use="true" windowtitle="${Name} ${version} API" doctitle="${Name} ${version} API" bottom="Copyright &amp;copy; ${year} The Apache Software Foundation">
             <packageset dir="${src.dir}" />
             <packageset dir="contrib/piggybank/java/src/main/java"/>
-            <packageset dir="contrib/zebra/src/java"/>
             <link href="${javadoc.link.java}" />
             <classpath>
                 <path refid="javadoc-classpath" />
                 <pathelement path="${output.jarfile.withdependencies}" />
                 <pathelement path="${piggybank.jarfile}"/>
-                <fileset dir="build">
-                    <include name="**/zebra-*-dev.jar"/>
-                </fileset>
             </classpath>
             <group title="pig" packages="org.apache.pig*" />
             <group title="contrib: Piggybank" packages="org.apache.pig.piggybank*" />
-            <group title="contrib: Zebra" packages="org.apache.hadoop.zebra*"/>
         </javadoc>
     </target>
 
@@ -920,14 +911,6 @@
         <antcall target="test-core" inheritRefs="true" inheritall="true"/>
     </target>
 
-    <target name="test-contrib-internal" unless="testcase">
-        <antcall target="test-contrib" inheritRefs="true" inheritall="true"/>
-    </target>
-
-    <target name="test-contrib" depends="compile-test" description="to call contrib tests">
-        <subant target="pigtest" buildpath="contrib/zebra" antfile="build.xml"/>
-    </target>
-
     <!-- ================================================================== -->
     <!-- End to end tests                                                   -->
     <!-- ================================================================== -->
@@ -1098,7 +1081,7 @@
     <!-- ================================================================== -->
     <!-- Make release tarball                                               -->
     <!-- ================================================================== -->
-    <target name="src-release" depends="clean, clean-piggybank, clean-zebra, clean-test-e2e, clean-tutorial" description="Source distribution">
+    <target name="src-release" depends="clean, clean-piggybank, clean-test-e2e, clean-tutorial" description="Source distribution">
         <mkdir dir="${build.dir}"/>
         <tar compression="gzip" longfile="gnu"
              destfile="${build.dir}/${final.name}-src.tar.gz">
@@ -1704,10 +1687,6 @@
          </ant>
      </target>
 
-     <target name="zebra" depends="jar">
-         <ant antfile="contrib/zebra/build.xml" inheritAll="false"/>
-     </target>
-
      <import file="./build-site.xml" optional="true"/>
 
 </project>
diff --git a/contrib/zebra/CHANGES.txt b/contrib/zebra/CHANGES.txt
deleted file mode 100644
index d271660b5..000000000
--- a/contrib/zebra/CHANGES.txt
+++ /dev/null
@@ -1,162 +0,0 @@
-Zebra Change Log
-
-Trunk (unreleased changes)
-
-  INCOMPATIBLE CHANGES
-
-    PIG-1455 Addition of test-unit as an ant target (yanz)
-
-    PIG-1451 Change the build.test property in build to test.build.dir to be consistent with PIG (yanz)
-
-    PIG-1444 Addition of test-smoke ant target (gauravj via yanz)
-
-    PIG-1357 Addition of Test cases of map-side GROUP-BY (yanz)
-
-    PIG-1282 make Zebra's pig test cases run on real cluster (chaow via yanz)
-
-    PIG-1164 Addition of smoke tests (gauravj via yanz)
-
-    PIG-1122 Changed version number of pig dev core jar used in Zebra build from 0.6.0 to 0.7.0
-        to match Pig version number (yanz)
-
-    PIG-1099 Changed version number to be 0.7.0 to match Pig version number
-	change (yanz via gates)
-
-  IMPROVEMENTS
-
-    PIG-1425 support of source table index on unsorted table in the mapred APIs (yanz)
-
-    PIG-1375 Support of multiple Zebra table writing through Pig (chaow via yanz)
-
-    PIG-1351 Addition of type check when writing to basic table (chaow via yanz)
-
-    PIG-1361 Zebra TableLoader.getSchema() should return the projectionSchema specified in the constructor of TableLoader instead of pruned proejction by pig (gauravj via daijy)
-
-    PIG-1291 Support of virtual column "source_table" on unsorted table (yanz)
-
-    PIG-1315 Implementing OrderedLoadFunc interface for Zebra TableLoader (xuefux via yanz)
-
-    PIG-1306 Support of locally sorted input splits (yanz)
-
-    PIG-1268 Need an ant target that runs all pig-related tests in Zebra (xuefuz via yanz)
-
-    PIG-1207 Data sanity check should be performed at the end of writing instead of later at query time (yanz)
-
-    PIG-1206 Storing descendingly sorted PIG table as unsorted table (yanz)
-
-    PIG-1240 zebra manifest file enhancement (gauravj via yanz)
-
-    PIG-1140 Support of Hadoop 2.0 API (xuefuz via yanz)
-
-    PIG-1170 new end-to-end and stress test cases (jing1234 via yanz)
-
-    PIG-1136 Support of map split on hash keys with leading underscore (xuefuz via yanz)
-
-    PIG-1125 Map/Reduce API Changes (Chao Wang via yanz)
-
-    PIG-1104 Streaming Support (Chao Wang via yanz)
-
-    PIG-1119 Support of "group" as a column name (Gaurav Jain via yanz)
-
-    PIG-653  Pig Projection Push Down (Gaurav Jain via yanz)
-
-    PIG-1111 Multiple Outputs Support (Gaurav Jain via yanz)
-
-    PIG-1098 Zebra Performance Optimizations (yanz via gates)
-
-	PIG-1074 Zebra store function should allow '::' in column names in output
-	schema (yanz via gates)
-
-    PIG-1077 Support record(row)-based file split in Zebra's
-	TableInputFormat (chaow via gates)
-
-    PIG-1089 Use Pig's version for Zebra's own versi (chaow via olgan)
-
-    PIG-1069  Order Preserving Sorted Table Union (yanz via gates)
-
-    PIG-997 Sorted Table Support by Zebra (yanz via gates)
-  	
-	PIG-996 Add findbugs, checkstyle, and clover to zebra build file (chaow via
-	        gates)
-
-	PIG-993 Ability to drop a column group in a table (yanz and rangadi via gates)
-
-    PIG-992 Separate schema related files into a schema package (yanz via
-	gates)
-
-  OPTIMIZATIONS
-
-    PIG-1198: performance improvements through use of unsorted input splits that span multiple files (yanz)
-
-  BUG FIXES
-
-    PIG-1453 Intermittent failure in pigtest (yanz)
-
-    PIG-1432 There are some debuging info output to STDOUT in PIG's TableStorer call path (yanz)
-
-    PIG-1421 Name node calls made by each mapper (xuefuz via yanz)
-
-    PIG-1342 Avoid making unnecessary name node calls for writes in Zebra (chaow via yanz) 
-
-    PIG-1356 TableLoader makes unnecessary calls to build a Job instance that create a new JobClient in the hadoop 0.20.9 (yanz)
-
-    PIG-1349 Hubson test failure in test case TestBasicUnion (xuefuz via yanz)
-
-    PIG-1340 The zebra version number should be changed from 0.7 to 0.8 (yanz)
-
-    PIG-1318 Invalid type for source_table field when using order-preserving Sorted Table Union (gauravj via yanz)
-
-    PIG-1258 Number of sorted input splits is unusually high (yanz)
-
-    PIG-1269 Restrict schema definition for collection (xuefuz via yanz)
-
-    PIG-1253: make map/reduce test cases run on real cluster (chaow via yanz)
-
-    PIG-1276: Pig resource schema interface changed, so Zebra needs to catch exception thrown from the new interfaces. (xuefuz via yanz)
-
-    PIG-1256: Bag field should always contain a tuple type as the field schema in ResourceSchema object converted from Zebra Schema (xuefuz via yanz)
-
-    PIG-1227: Throw exception if column group meta file is missing for an unsorted table (yanz)
-
-    PIG-1201: unnecessary name node calls by each mapper; too big input split serialization size by Pig's Slice implementation (yanz)
-
-    PIG-1115: cleanup of temp files left by failed tasks (gauravj via yanz)
-
-    PIG-1167: Hadoop file glob support (yanz)
-
-    PIG-1153: Record split exception fix (yanz)
-
-    PIG-1145: Merge Join on Large Table throws an EOF exception (yanz)
-
-    PIG-1095: Schema support of anonymous fields in COLECTION fails (yanz via
-	gates)
-
-    PIG-1078: merge join with empty table failed (yanz via gates)
-
-    PIG-1091: Exception when load with projection of map keys on a map column
-	that is not map split (yanz via gates).
-
-    PIG-1026: [zebra] map split returns null (yanz via pradeepkth)
-
-	PIG-1057 Zebra does not support concurrent deletions of column groups now
-	(chaow via gates).
-
-	PIG-944  Change schema to be taken from StoreConfig instead of
-	TableStorer's constructor (yanz via gates).
-
-    PIG-918. Fix infinite loop only columns in first column group are
-    specified. (Yan Zhou via rangadi)
- 
-    PIG-949. If an entire map is placed in non default column group, 
-    and a specific key placed in another CG, the second CG did not 
-    work as expected. (Yan Zhou, Jing Huang (tests) via rangadi)
-
-    PIG-987. Access control for column groups. Users can specifify
-    desired group, owner, and, permissions for a column group.
-    (Yan Zhou via rangadi)
-    
-    PIG-991. Various minor bugs. (Yan Zhou via rangadi)
-
-    PIG-986. Column groups can have explicit names specified in
-    storage hint. (Yan Zhou via rangadi)
-
diff --git a/contrib/zebra/build-contrib.xml b/contrib/zebra/build-contrib.xml
deleted file mode 100644
index a9216c3bb..000000000
--- a/contrib/zebra/build-contrib.xml
+++ /dev/null
@@ -1,263 +0,0 @@
-<?xml version="1.0"?>
-
-<!--
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
--->
-
-<!-- Imported by contrib/*/build.xml files to share generic targets. -->
-
-<project name="pigbuildcontrib">
-
-  <property name="name" value="${ant.project.name}"/>
-  <property name="root" value="${basedir}"/>
-  <property name="version" value="0.8.0-dev"/>
-
-  <!-- Load all the default properties, and any the user wants    -->
-  <!-- to contribute (without having to type -D or edit this file -->
-  <property file="${user.home}/${name}.build.properties" />
-  <property file="${root}/build.properties" />
-
-  <property name="pig.root" location="${root}/../../"/>
-
-  <property name="src.dir"  location="${root}/src/java"/>
-  <property name="src.docs.dir"  location="${root}/docs"/>
-  <property name="pig.src.dir"  location="${pig.root}/src"/>
-  <property name="src.test" location="${root}/src/test"/>
-  <property name="src.examples" location="${root}/src/examples"/>
-
-  <available file="${src.examples}" type="dir" property="examples.available"/>
-  <available file="${src.test}" type="dir" property="test.available"/>
-
-  <property name="conf.dir" location="${pig.root}/conf"/>
-  <property name="test.junit.output.format" value="plain"/>
-  <property name="test.output" value="no"/>
-  <property name="test.timeout" value="9000000"/>
-  <property name="build.dir" location="${pig.root}/build/contrib/${name}"/>
-  <property name="build.javadoc"
-	  location="${pig.root}/build/contrib/${name}/docs"/>
-  <property name="build.classes" location="${build.dir}/classes"/>
-  <property name="test.build.dir" location="${build.dir}/test"/>
-  <property name="build.examples" location="${build.dir}/examples"/>
-  <property name="pig.log.dir" location="${test.build.dir}/logs"/>
-  <property name="hadoop.jarfile" value="hadoop20.jar" />
-  <property name="pig.jarfile" value="pig-0.8.0-dev-core.jar" />
-  <property name="hbase.jarfile" value="hbase-0.18.1.jar" />
-  <property name="hbase.test.jarfile" value="hbase-0.18.1-test.jar" />
-
-  <!-- IVY properteis set here -->
-  <property name="build.ivy.dir" location="${pig.root}/build/ivy" />
-  <property name="build.ivy.lib.dir" location="${build.ivy.dir}/lib" />
-  <property name="ivy.lib.dir" location="${build.ivy.lib.dir}/Pig"/>
-
-  <property name="clover.db.dir" location="${test.build.dir}/clover/db"/>
-    <property name="clover.report.dir" location="${test.build.dir}/clover/reports"/>
-    <property name="clover.jar" location="${clover.home}/lib/clover.jar"/>
-    <available property="clover.present" file="${clover.jar}" />
-    <!-- check if clover reports should be generated -->
-    <condition property="clover.enabled">
-      <and>
-        <isset property="run.clover"/>
-        <isset property="clover.present"/>
-      </and>
-    </condition>
-
-  <!-- javacc properties -->
-  <property name="src.gen.dir" value="${basedir}/src-gen/" />
-  <property name="src.gen.zebra.parser.dir" value="${src.gen.dir}/org/apache/hadoop/zebra/parser" />
-
-  <!-- convert spaces to _ so that mac os doesn't break things -->
-  <exec executable="sed" inputstring="${os.name}"
-        outputproperty="nonspace.os">
-     <arg value="s/ /_/g"/>
-  </exec>
-  <property name="build.platform"
-            value="${nonspace.os}-${os.arch}-${sun.arch.data.model}"/>
-
-  <!-- all jars together -->
-  <property name="javac.deprecation" value="off"/>
-  <property name="javac.debug" value="on"/>
-
-  <property name="javadoc.link"
-	  value="http://java.sun.com/j2se/1.5.0/docs/api/"/>
-
-  <property name="build.encoding" value="ISO-8859-1"/>
-
-  <fileset id="lib.jars" dir="${root}" includes="lib/*.jar"/>
-
-  <!-- the normal classpath -->
-  <path id="classpath">
-    <fileset dir="${pig.root}/lib">
-      <include name="*.jar" />
-    </fileset>
-    <fileset dir="${pig.root}/build">
-      <include name="${pig.jarfile}" />
-    </fileset>
-    <fileset dir="${pig.root}/build/ivy/lib">
-       <include name="**/*.jar"/>
-    </fileset>
-    <fileset refid="lib.jars"/>
-    <pathelement location="${build.classes}"/>
-    <pathelement location="${pig.root}/build/classes"/>
-  </path>
-
-  <!-- the unit test classpath -->
-  <path id="test.classpath">
-    <pathelement location="${test.build.dir}" />
-    <pathelement location="${pig.root}/build/test/classes"/>
-    <pathelement location="${pig.root}/src/contrib/test"/>
-    <pathelement location="${conf.dir}"/>
-    <pathelement location="${pig.root}/build"/>
-    <pathelement location="${build.examples}"/>
-	<pathelement path="${clover.jar}"/>
-    <path refid="classpath"/>
-  </path>
-
-
-  <!-- to be overridden by sub-projects -->
-  <target name="check-contrib"/>
-  <target name="init-contrib"/>
-
-  <!-- ====================================================== -->
-  <!-- Stuff needed by all targets                            -->
-  <!-- ====================================================== -->
-  <target name="init" depends="check-contrib" unless="skip.contrib">
-    <echo message="contrib: ${name}"/>
-    <mkdir dir="${src.gen.zebra.parser.dir}"/>
-    <mkdir dir="${build.dir}"/>
-    <mkdir dir="${build.classes}"/>
-    <mkdir dir="${test.build.dir}"/>
-    <mkdir dir="${build.examples}"/>
-    <mkdir dir="${pig.log.dir}"/>
-    <antcall target="init-contrib"/>
-  </target>
-
-
-  <!-- ====================================================== -->
-  <!-- Compile a Pig contrib's files                          -->
-  <!-- ====================================================== -->
-  <target name="compile" depends="init, cc-compile" unless="skip.contrib">
-    <echo message="contrib: ${name}"/>
-    <javac
-     encoding="${build.encoding}"
-     srcdir="${src.dir};${src.gen.dir}"
-     includes="**/*.java"
-	 excludes="**/doc-files/examples/*.java"
-     destdir="${build.classes}"
-     debug="${javac.debug}"
-	 deprecation="${javac.deprecation}">
-	  <compilerarg value="-Xlint:deprecation" />
-      <classpath refid="classpath"/>
-    </javac>
-  </target>
-
-  <property name="javacc.home" location="${ivy.lib.dir}" />
-
-  <target name="cc-compile" depends="init, javacc-exists" description="Create and Compile Parser">
-    <javacc target="${root}/src/java/org/apache/hadoop/zebra/schema/SchemaParser.jjt" outputdirectory="${src.gen.zebra.parser.dir}" javacchome="${javacc.home}" />
-    <javacc target="${root}/src/java/org/apache/hadoop/zebra/types/TableStorageParser.jjt" outputdirectory="${src.gen.zebra.parser.dir}" javacchome="${javacc.home}" />
-  </target>
-
-  <target name="javacc-exists">
-    <fail>
-      <condition >
-        <not>
-          <available file="${javacc.home}/javacc.jar" />
-        </not>
-      </condition>
-          Not found: ${javacc.home}/javacc.jar
-          Please run the target "cc-compile" in the main build file
-    </fail>
-  </target>
-  <!-- ======================================================= -->
-  <!-- Compile a Pig contrib's example files (if available)    -->
-  <!-- ======================================================= -->
-  <target name="compile-examples" depends="compile" if="examples.available">
-    <echo message="contrib: ${name}"/>
-    <javac
-     encoding="${build.encoding}"
-     srcdir="${src.examples}"
-     includes="**/*.java"
-     destdir="${build.examples}"
-     debug="${javac.debug}">
-      <classpath refid="classpath"/>
-    </javac>
-  </target>
-
-
-  <!-- ================================================================== -->
-  <!-- Compile test code                                                  -->
-  <!-- ================================================================== -->
-  <target name="compile-test" depends="compile-examples" if="test.available">
-    <echo message="contrib: ${name}"/>
-    <javac
-     encoding="${build.encoding}"
-     srcdir="${src.test}"
-     includes="**/*.java"
-     destdir="${test.build.dir}"
-     debug="${javac.debug}">
-      <classpath refid="test.classpath"/>
-    </javac>
-  </target>
-  
-
-  <!-- ====================================================== -->
-  <!-- Make a Pig contrib's jar                               -->
-  <!-- ====================================================== -->
-  <target name="jar" depends="compile" unless="skip.contrib">
-    <echo message="contrib: ${name}"/>
-    <jar
-      jarfile="${build.dir}/pig-${version}-${name}.jar"
-      basedir="${build.classes}"      
-    />
-  </target>
-
-  
-  <!-- ====================================================== -->
-  <!-- Make a Pig contrib's examples jar                      -->
-  <!-- ====================================================== -->
-  <target name="jar-examples" depends="compile-examples"
-          if="examples.available" unless="skip.contrib">
-    <echo message="contrib: ${name}"/>
-    <jar jarfile="${build.dir}/pig-${version}-${name}-examples.jar">
-      <fileset dir="${build.classes}">
-      </fileset>
-      <fileset dir="${build.examples}">
-      </fileset>
-    </jar>
-  </target>
-  
-  <!-- ====================================================== -->
-  <!-- Package a Pig contrib                                  -->
-  <!-- ====================================================== -->
-  <target name="package" depends="jar, jar-examples" unless="skip.contrib"> 
-    <mkdir dir="${dist.dir}/contrib/${name}"/>
-    <copy todir="${dist.dir}/contrib/${name}" includeEmptyDirs="false" flatten="true">
-      <fileset dir="${build.dir}">
-        <include name="pig-${version}-${name}.jar" />
-      </fileset>
-    </copy>
-  </target>
-
-  <!-- ================================================================== -->
-  <!-- Clean.  Delete the build files, and their directories              -->
-  <!-- ================================================================== -->
-  <target name="clean">
-    <echo message="contrib: ${name}"/>
-    <delete dir="${build.dir}"/>
-    <delete dir="${src.gen.dir}" />
-  </target>
-
-</project>
diff --git a/contrib/zebra/build.xml b/contrib/zebra/build.xml
deleted file mode 100644
index 2ce502c3d..000000000
--- a/contrib/zebra/build.xml
+++ /dev/null
@@ -1,483 +0,0 @@
-<?xml version="1.0"?>
-
-<!--
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
--->
-
-<!-- 
-Before you can run these subtargets directly, you need 
-to call at top-level: ant deploy-contrib compile-core-test
--->
-<project name="zebra" default="jar">
-
-  <import file="build-contrib.xml"/>
-
-  <!-- Override jar target to specify main class -->
-  <target name="jar" depends="compile" description="Build zebra jar">
-    <tstamp>
-       <format property="timestamp" pattern="MMM dd yyyy, HH:mm:ss" />
-    </tstamp>
-    <svnversion outputproperty="svn.revision"/>
-
-    <jar
-      jarfile="${build.dir}/${name}-${version}.jar"
-      basedir="${build.classes}"      
-    >
-  	<manifest>
-          <section name="org/apache/hadoop/zebra">
-             <attribute name="Implementation-Vendor" value="Apache" />
-             <attribute name="Implementation-Title" value="Zebra" />
-             <attribute name="Implementation-Version" value="${version}" />
-             <attribute name="Build-TimeStamp" value="${timestamp}" />
-             <attribute name="Svn-Revision" value="${svn.revision}" />
-          </section>
-	</manifest>
-    </jar>
-  </target>
-
-  <macrodef name="svnversion">
-     <!-- the path needs to be small content otherwise it will take AGES ! -->
-     <attribute name="wcpath" default="${basedir}" />
-     <attribute name="outputproperty" />
-     <sequential>
-        <exec executable="svnversion" outputproperty="@{outputproperty}" failonerror="false" failifexecutionfails="false" >
-           <arg value="@{wcpath}" />
-           <redirector>
-             <outputfilterchain>
-                <tokenfilter>
-                   <!-- version can be xxxx, xxxx:yyyy, xxxxM, xxxxS or xxxx:yyyyMS , ... just get the working copy one -->
-                   <replaceregex pattern="((\d+).*)" replace="\2" />
-                   </tokenfilter>
-                </outputfilterchain>
-           </redirector>
-        </exec>
-    </sequential>
-  </macrodef>
-
-  <target name="schema-jar" depends="compile">
-    <jar
-      jarfile="${build.dir}/schema-${version}.jar"
-      basedir="${build.classes}"      
-      includes="**/schema/**"
-    >
-    </jar>
-  </target>
-
-  <!-- Temp check for hadoop jar file --> 
-  <target name="hadoop-jar-exists">
-     <fail message="${hadoop.jarfile} is missing. ${line.separator}
-     ####### Build can not proceed. Please read the following ######### ${line.separator}
-       ${line.separator}
-       ${hadoop.jarfile} is not found. This usually implies that you need to follow ${line.separator}
-       following extra steps to be able to build zebra. These steps are required ${line.separator}
-       until PIG-660 is committed : ${line.separator}
-          * Place hadoop20.jar atatched to PIG-833 under lib in top level directory ${line.separator}
-          * Apply the latest working patch from PIG-660 so that Pig works with ${line.separator}
-            Hadoop-20 ${line.separator}
-          * run 'ant clean jar' under top level Pig directory ${line.separator}
-       ${line.separator}
-     ###################################################################">
-       <condition>
-         <not>
-           <available file="${pig.root}/lib/${hadoop.jarfile}"/>
-         </not>
-       </condition>
-     </fail>
-  </target>
-
-
-  <target name="javadoc" depends="jar">
-	  <mkdir dir="${build.javadoc}" />
-	  <javadoc packagenames="org.apache.pig.*" overview="${src.docs.dir}/overview.html" destdir="${build.javadoc}" author="true" version="true" use="true" windowtitle="Hadoop Zebra API" doctitle="Hadoop Zebra API" bottom="Copyright &amp;copy; ${year} The Apache Software Foundation">
-		  <packageset dir="${src.dir}" excludes="**/examples" />
-		  <link href="${javadoc.link}" />
-          <classpath refid="classpath"/>
-		  <!--
-		  <classpath>
-			  <path refid="classpath" />
-			  <pathelement path="${java.class.path}" />
-			  <pathelement path="${output.jarfile}" />
-		  </classpath>
-		  -->
-		  <group title="zebra" packages="org.apache.hadoop.zebra.*" />
-	  </javadoc>
-  </target>
-
-  <!-- Override jar target to specify main class -->
-  <target name="jar-test" depends="jar, compile-test">
-    <jar
-      jarfile="${build.dir}/pig-${version}-${name}-test.jar"
-      basedir="${test.build.dir}"      
-    >
-    </jar>
-  </target>
-
-  <!-- ================================================================== -->
-  <!-- Run checkin tests                                                  -->
-  <!-- ================================================================== -->
-  <target name="test" depends="compile-test, compile" if="test.available" description="Run check-in test">
-    <echo message="contrib: ${name}"/>
-    <delete dir="${pig.log.dir}"/>
-    <mkdir dir="${pig.log.dir}"/>
-    <delete dir="${test.build.dir}/data"/>
-    <mkdir dir="${test.build.dir}/data"/>
-    <junit
-      printsummary="yes" showoutput="${test.output}" 
-      haltonfailure="no" fork="yes" maxmemory="1024m"
-      errorProperty="tests.failed" failureProperty="tests.failed"
-      timeout="${test.timeout}">
-      
-      <sysproperty key="test.build.data" value="${test.build.dir}/data"/>
-      <sysproperty key="test.build.dir" value="${test.build.dir}"/>
-      <sysproperty key="contrib.name" value="${name}"/>
-      <sysproperty key="hadoop.log.dir" value="${pig.log.dir}"/>
-       <sysproperty key="whichCluster" value="miniCluster"/>
-	      
-      <!-- requires fork=yes for: 
-        relative File paths to use the specified user.dir 
-        classpath to use build/contrib/*.jar
-      -->
-      <sysproperty key="user.dir" value="${test.build.dir}/data"/>
-      <syspropertyset>	
-    	<propertyset>
-    	  <propertyref name="fs.default.name"/>
-      	  <propertyref name="pig.test.localoutputfile"/>
-          <propertyref name="pig.log.dir"/>
-    	</propertyset>
-      </syspropertyset>	
-
-      <classpath refid="test.classpath"/>
-      <formatter type="${test.junit.output.format}" />
-
-      <batchtest todir="${test.build.dir}" unless="testcase">
-        <fileset dir="${src.test}"
-                 includes="**/TestCheckin*.java" excludes="**/${test.exclude}.java" />
-      </batchtest>
-      <batchtest todir="${test.build.dir}"  if="testcase">
-        <fileset dir="${src.test}"
-                 includes="**/${testcase}.java"/>
-      </batchtest>
-
-
-    </junit>
-    <fail if="tests.failed">Tests failed!</fail>
-  </target>
-
-  <target name="test-unit" depends="compile-test, compile" if="test.available" description="Run unit test">
-    <echo message="contrib: ${name}"/>
-    <delete dir="${pig.log.dir}"/>
-    <mkdir dir="${pig.log.dir}"/>
-    <delete dir="${test.build.dir}/data"/>
-    <mkdir dir="${test.build.dir}/data"/>
-    <junit
-      printsummary="yes" showoutput="${test.output}" 
-      haltonfailure="no" fork="yes" maxmemory="1024m"
-      errorProperty="tests.failed" failureProperty="tests.failed"
-      timeout="${test.timeout}">
-      
-      <sysproperty key="test.build.data" value="${test.build.dir}/data"/>
-      <sysproperty key="test.build.dir" value="${test.build.dir}"/>
-      <sysproperty key="contrib.name" value="${name}"/>
-      <sysproperty key="hadoop.log.dir" value="${pig.log.dir}"/>
-       <sysproperty key="whichCluster" value="miniCluster"/>
-	      
-      <!-- requires fork=yes for: 
-        relative File paths to use the specified user.dir 
-        classpath to use build/contrib/*.jar
-      -->
-      <sysproperty key="user.dir" value="${test.build.dir}/data"/>
-      
-      <sysproperty key="fs.default.name" value="${fs.default.name}"/>
-      <sysproperty key="pig.test.localoutputfile" value="${pig.test.localoutputfile}"/>
-      <sysproperty key="pig.log.dir" value="${pig.log.dir}"/> 
-      <classpath refid="test.classpath"/>
-      <formatter type="${test.junit.output.format}" />
-
-      <batchtest todir="${test.build.dir}" unless="testcase">
-        <fileset dir="${src.test}"
-          includes="**/io/*.java **/tfile/*.java **/types/*.java" excludes="**/TestCheckin*.java **/tfile/KVGenerator.java **/tfile/KeySampler.java **/tfile/NanoTimer.java **/tfile/RandomDistribution.java **/tfile/Timer.java"/>
-      </batchtest>
-      <batchtest todir="${test.build.dir}"  if="testcase">
-        <fileset dir="${src.test}"
-                 includes="**/${testcase}.java"/>
-      </batchtest>
-
-
-    </junit>
-    <fail if="tests.failed">Tests failed!</fail>
-  </target>
-  <!-- ================================================================== -->
-  <!-- Run smoke tests                                                  -->
-  <!-- ================================================================== -->
-  <target name="test-smoke" depends="compile-test, compile" if="test.available" description="Run smoke test">
-    <echo message="contrib: ${name}"/>
-    <delete dir="${pig.log.dir}"/>
-    <mkdir dir="${pig.log.dir}"/>
-    <delete dir="${test.build.dir}/data"/>
-    <mkdir dir="${test.build.dir}/data"/>
-    <junit
-      printsummary="yes" showoutput="${test.output}"
-      haltonfailure="no" fork="yes" maxmemory="1024m"
-      errorProperty="tests.failed" failureProperty="tests.failed"
-      timeout="${test.timeout}">
-
-      <sysproperty key="test.build.data" value="${test.build.dir}/data"/>
-      <sysproperty key="test.build.dir" value="${test.build.dir}"/>
-      <sysproperty key="contrib.name" value="${name}"/>
-      <sysproperty key="hadoop.log.dir" value="${pig.log.dir}"/>
-       <sysproperty key="whichCluster" value="miniCluster"/>
-
-      <!-- requires fork=yes for:
-        relative File paths to use the specified user.dir
-        classpath to use build/contrib/*.jar
-      -->
-      <sysproperty key="user.dir" value="${test.build.dir}/data"/>
-
-      <sysproperty key="fs.default.name" value="${fs.default.name}"/>
-      <sysproperty key="pig.test.localoutputfile" value="${pig.test.localoutputfile}"/>
-      <sysproperty key="pig.log.dir" value="${pig.log.dir}"/>
-      <classpath refid="test.classpath"/>
-      <formatter type="${test.junit.output.format}" />
-
-      <batchtest todir="${test.build.dir}" unless="testcase">
-        <fileset dir="${src.test}"
-                 includes="**/TestMultipleOutputs.java, **/TestTableLoaderP*.java" excludes="**/${test.exclude}.java" />
-      </batchtest>
-      <batchtest todir="${test.build.dir}"  if="testcase">
-        <fileset dir="${src.test}"
-                 includes="**/${testcase}.java"/>
-      </batchtest>
-
-
-    </junit>
-    <fail if="tests.failed">Tests failed!</fail>
-  </target>
-
-
-  <!-- ================================================================== -->
-  <!-- Run nightly tests                                                  -->
-  <!-- ================================================================== -->
-  <target name="nightly" depends="compile-test, compile" if="test.available">
-    <echo message="contrib: ${name}"/>
-    <delete dir="${pig.log.dir}"/>
-    <mkdir dir="${pig.log.dir}"/>
-    <junit
-      printsummary="yes" showoutput="${test.output}" 
-      haltonfailure="no" fork="yes" maxmemory="1024m"
-      errorProperty="tests.failed" failureProperty="tests.failed"
-      timeout="${test.timeout}">
-      
-      <sysproperty key="test.build.data" value="${test.build.dir}/data"/>
-      <sysproperty key="test.build.dir" value="${test.build.dir}"/>
-      <sysproperty key="contrib.name" value="${name}"/>
-      <sysproperty key="hadoop.log.dir" value="${pig.log.dir}"/>
-      <sysproperty key="whichCluster" value="miniCluster"/>
-      <!-- requires fork=yes for: 
-        relative File paths to use the specified user.dir 
-        classpath to use build/contrib/*.jar
-      -->
-      <sysproperty key="user.dir" value="${test.build.dir}/data"/>
-      
-      <sysproperty key="fs.default.name" value="${fs.default.name}"/>
-      <sysproperty key="pig.test.localoutputfile" value="${pig.test.localoutputfile}"/>
-      <sysproperty key="pig.log.dir" value="${pig.log.dir}"/> 
-      <classpath refid="test.classpath"/>
-      <formatter type="${test.junit.output.format}" />
-
-      <!-- For the time being, we disable two test cases that need a real cluster to run -->
-      <batchtest todir="${test.build.dir}">
-        <fileset dir="${src.test}" includes="**/Test*.java" excludes="**/TestCheckin*.java">
-          <not>
-            <filename name="**/TestSmoke*.java"/>
-          </not>
-
-        </fileset> 
-      </batchtest>
-    </junit>
-    <fail if="tests.failed">Tests failed!</fail>
-  </target>
-
-	  <!-- ================================================================== -->
-	  <!-- Run pig-related test cases only                                    -->
-	  <!-- ================================================================== -->
-	  <target name="pigtest" depends="compile-test, compile" if="test.available">
-	    <echo message="contrib: ${name}"/>
-	    <delete dir="${pig.log.dir}"/>
-	    <mkdir dir="${pig.log.dir}"/>
-	    <junit
-	      printsummary="yes" showoutput="${test.output}" 
-	      haltonfailure="no" fork="yes" maxmemory="1024m"
-	      errorProperty="tests.failed" failureProperty="tests.failed"
-	      timeout="${test.timeout}">
-	      
-	      <sysproperty key="test.build.data" value="${test.build.dir}/data"/>
-	      <sysproperty key="test.build.dir" value="${test.build.dir}"/>
-	      <sysproperty key="contrib.name" value="${name}"/>
-	      <sysproperty key="hadoop.log.dir" value="${pig.log.dir}"/>
-	      <sysproperty key="whichCluster" value="miniCluster"/>
-	      <!-- requires fork=yes for: 
-	        relative File paths to use the specified user.dir 
-	        classpath to use build/contrib/*.jar
-	      -->
-	      <sysproperty key="user.dir" value="${test.build.dir}/data"/>
-	      
-	      <sysproperty key="fs.default.name" value="${fs.default.name}"/>
-	      <sysproperty key="pig.test.localoutputfile" value="${pig.test.localoutputfile}"/>
-	      <sysproperty key="pig.log.dir" value="${pig.log.dir}"/> 
-	      <classpath refid="test.classpath"/>
-	      <formatter type="${test.junit.output.format}" />
-
-	      <!-- For the time being, we disable some test case to reduce the running time -->
-	      <batchtest todir="${test.build.dir}">
-	        <fileset dir="${src.test}" includes="**/pig/Test*.java" excludes="**/pig/TestCheckin*.java">
-	          <not>
-	            <filename name="**/pig/TestRealCluster.java"/> <!-- This requires a real cluster anyway-->
-	          </not>
-	        </fileset> 
-	      </batchtest>
-	    </junit>
-	    <fail if="tests.failed">Tests failed!</fail>
-	  </target>
-
-	<!-- ================================================================== -->
-  <!-- findbugs                                                           -->
-  <!-- ================================================================== -->
-  <target name="findbugs" depends="check-for-findbugs, jar" if="findbugs.present" description="Run findbugs if present">
-      <property name="findbugs.out.dir" value="${build.dir}/findbugs"/>
-      <property name="findbugs.report.htmlfile" value="${findbugs.out.dir}/zebra-findbugs-report.html"/>
-      <property name="findbugs.report.xmlfile" value="${findbugs.out.dir}/zebra-findbugs-report.xml"/>
-      <taskdef name="findbugs" classname="edu.umd.cs.findbugs.anttask.FindBugsTask"
-              classpath="${findbugs.home}/lib/findbugs-ant.jar" />
-      <mkdir dir="${findbugs.out.dir}"/>
-      <findbugs home="${findbugs.home}" output="xml:withMessages"
-              outputFile="${findbugs.report.xmlfile}" effort="max" jvmargs="-Xmx512M">
-            <sourcePath path="${src.dir}"/>
-            <class location="${build.dir}" />
-      </findbugs>
-      <xslt style="${findbugs.home}/src/xsl/default.xsl" in="${findbugs.report.xmlfile}"
-              out="${findbugs.report.htmlfile}"/>
-  </target>
-  
-  <target name="check-for-findbugs">
-      <available property="findbugs.present" file="${findbugs.home}/lib/findbugs.jar" />
-  </target>
-
-  <!-- ================================================================== -->
-  <!-- checkstyle                                                         -->
-  <!-- ================================================================== -->
-	<target name="checkstyle" depends="checkstyle.check, set-checkstyle-classpath" if="checkstyle.home"
-	        description="Run optional third-party tool targets">
-      <property name="checkstyle.dir" value="${build.dir}/checkstyle"/>
-	    <taskdef resource="checkstyletask.properties">
-	        <classpath refid="classpath"/>
-	        <classpath refid="checkstyle-classpath"/>
-	    </taskdef>
-	    <mkdir dir="${checkstyle.dir}"/>
-	    <checkstyle config="${checkstyle.dir}/checkstyle.xml" failOnViolation="false">
-	        <!--fileset dir="${src.dir}" includes="**/*.java" excludes="**/generated/**"/-->
-	        <fileset dir="${src.dir}" includes="**/*.java"/>
-	        <formatter type="xml" toFile="${checkstyle.dir}/checkstyle-errors.xml"/>
-	    </checkstyle>
-	    <xslt style="${checkstyle.dir}/checkstyle-noframes-sorted.xsl" in="${checkstyle.dir}/checkstyle-errors.xml"
-	        out="${checkstyle.dir}/checkstyle-errors.html"/>
-	</target>
-	
-	<target name="checkstyle.check" unless="checkstyle.home">
-	    <!--<fail message="'checkstyle.home' is not defined. Please pass -Dcheckstyle.home=&lt;base of checkstyle installation&gt; to Ant on the command-line." />-->
-	    <echo>
-        Checkstyle not found. Checkstyle disabled.
-      </echo>
-	</target>
-	
-	<target name="set-checkstyle-classpath">
-	    <path id="checkstyle-classpath">
-	        <fileset dir="${checkstyle.home}">
-	        <include name="**/*.jar"/>
-	         </fileset>
-	 </path>
-	</target>
-
-
-  <!-- ================================================================== -->
-  <!-- clover                                                             -->
-  <!-- ================================================================== -->
-  <target name="clover" depends="clover.setup, clover.info" description="Instrument the Unit tests using Clover.  
- 	   To use, specify -Dclover.home=&lt;base of clover installation&gt; -Drun.clover=true on the command line."/>
-
-  <target name="clover.setup" if="clover.enabled">
-  	<taskdef resource="cloverlib.xml" classpath="${clover.jar}"/>
-     <mkdir dir="${clover.db.dir}"/>
-     <clover-setup initString="${clover.db.dir}/zebra_coverage.db">
-     	<fileset dir="src" includes="**/*.java"/>
-	 </clover-setup>
-  </target>
-
-  <target name="clover.info" unless="clover.present">
-    	<echo>
-     	Clover not found. Code coverage reports disabled.
-    	</echo>
-  </target>
-  
-  <target name="clover.check">
-    <fail unless="clover.present">
-      ##################################################################
-  	  	Clover not found.
-  	  	Please specify -Dclover.home=&lt;base of clover installation&gt;
-  	  	on the command line.
-      ##################################################################
- 	  </fail>
-  </target>
-
-  <target name="generate-clover-reports" depends="clover.check, clover">
-    	<mkdir dir="${clover.report.dir}"/>
- 	<clover-report>
-    		<current outfile="${clover.report.dir}" title="${final.name}">
-    	    		<format type="html"/>
-   	  	</current>
- 	</clover-report>
-  <clover-report>
-         	<current outfile="${clover.report.dir}/clover.xml" title="${final.name}">
-           		<format type="xml"/>
-         	</current>
-       </clover-report>
-  </target> 
-  <target name="smoke-jar" depends="compile,test">
-      <jar destfile="${test.build.dir}/zebra_smoke.jar"
-	       basedir="${test.build.dir}"
-		   includes="**/TestSmokeMR*.class, **/TestTableLoaderP*.class"
-	  />
-  </target>
-  <target name="package-tests" depends="smoke-jar">
-    <tar longfile="gnu" destfile="${test.build.dir}/zebra_smoke.tar">
-    <tarfileset dir="${src.test}/smoke"
-             fullpath="bin/zebra_smoke_run.pl"
-             preserveLeadingSlashes="true">
-      <include name="zebra_smoke_run.pl"/>
-    </tarfileset>
-    <tarfileset dir="${test.build.dir}/../../../ivy/lib/Pig"
-             fullpath="lib/junit-4.5.jar"
-             preserveLeadingSlashes="true">
-      <include name="junit-4.5.jar"/>
-    </tarfileset>
-    <tarfileset dir="${test.build.dir}"
-             fullpath="lib/zebra_smoke.jar"
-             preserveLeadingSlashes="true">
-      <include name="zebra_smoke.jar"/>
-    </tarfileset>
-   </tar>
-  </target>
-</project>
-
diff --git a/contrib/zebra/docs/overview.html b/contrib/zebra/docs/overview.html
deleted file mode 100644
index 96791dc1c..000000000
--- a/contrib/zebra/docs/overview.html
+++ /dev/null
@@ -1,22 +0,0 @@
-<HTML>
-<!--
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
--->
-	<TITLE>Hadoop Table</TITLE>
-	<BODY>
-		Please see {@link org.apache.hadoop.zebra} package description.
-	</BODY>
-	</HTML>
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/io/BasicTable.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/io/BasicTable.java
deleted file mode 100644
index 6cad81a0b..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/io/BasicTable.java
+++ /dev/null
@@ -1,2178 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.Closeable;
-import java.io.DataInput;
-import java.io.DataInputStream;
-import java.io.DataOutput;
-import java.io.DataOutputStream;
-import java.io.FileNotFoundException;
-import java.io.IOException;
-import java.io.OutputStream;
-import java.io.PrintStream;
-import java.io.StringReader;
-import java.util.ArrayList;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Random;
-import java.util.Set;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.PathFilter;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.io.WritableUtils;
-import org.apache.hadoop.zebra.tfile.TFile;
-import org.apache.hadoop.zebra.tfile.Utils;
-import org.apache.hadoop.zebra.tfile.MetaBlockAlreadyExists;
-import org.apache.hadoop.zebra.tfile.MetaBlockDoesNotExist;
-import org.apache.hadoop.zebra.tfile.Utils.Version;
-import org.apache.hadoop.zebra.io.ColumnGroup.Reader.CGRangeSplit;
-import org.apache.hadoop.zebra.io.ColumnGroup.Reader.CGRowSplit;
-import org.apache.hadoop.zebra.io.ColumnGroup.Reader.CGScanner;
-import org.apache.hadoop.zebra.types.CGSchema;
-import org.apache.hadoop.zebra.mapreduce.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Partition;
-import org.apache.hadoop.zebra.types.Projection;
-import org.apache.hadoop.zebra.types.ZebraConf;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.parser.TableSchemaParser;
-import org.apache.hadoop.zebra.pig.TableStorer;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.types.SortInfo;
-import org.apache.pig.data.Tuple;
-
-/**
- * A materialized table that consists of one or more tightly coupled Column
- * Groups.
- * 
- * The following Configuration parameters can customize the behavior of
- * BasicTable.
- * <ul>
- * <li><b>table.output.tfile.minBlock.size</b> (int) Minimum compression block
- * size for underlying TFile (default to 1024*1024).
- * <li><b>table.output.tfile.compression</b> (String) Compression method (one of
- * "none", "lzo", "gz") (default is "gz"). @see
- * {@link TFile#getSupportedCompressionAlgorithms()}
- * <li><b>table.input.split.minSize</b> (int) Minimum split size (default to
- * 64*1024).
- * </ul>
- */
-public class BasicTable {
-  
-  static Log LOG = LogFactory.getLog(BasicTable.class);
-  
-  // name of the BasicTable schema file
-  private final static String BT_SCHEMA_FILE = ".btschema";
-  // schema version
-  private final static Version SCHEMA_VERSION =
-      new Version((short) 1, (short) 1);
-  // name of the BasicTable meta-data file
-  private final static String BT_META_FILE = ".btmeta";
-
-  private final static String DELETED_CG_PREFIX = ".deleted-";
-  
-  public final static String DELETED_CG_SEPARATOR_PER_TABLE = ",";
-
-  // no public ctor for instantiating a BasicTable object
-  private BasicTable() {
-    // no-op
-  }
-
-  /**
-   * Deletes the data for column group specified by cgName.
-   * When the readers try to read the fields that were stored in the
-   * column group get null since the underlying data is removed.
-   * <br> <br>
-   * 
-   * Effect on the readers that are currently reading from the table while
-   * a column group is droped is unspecified. Suggested practice is to 
-   * drop column groups when there are no readers or writes for the table.
-   * <br> <br>
-   * 
-   * Column group names are usually specified in the "storage hint" while
-   * creating a table. If no name is specified, system assigns a simple name.
-   * These names could be obtained through "dumpInfo()" and other methods.
-   * <br> <br> 
-   *
-   * Dropping a column group that has already been removed is a no-op no 
-   * exception is thrown.
-   * <br> <br> 
-   * 
-   * Note that this feature is experimental now and subject to changes in the
-   * future.
-   *
-   * @param path path to BasicTable
-   * @param conf Configuration determines file system and other parameters.
-   * @param cgName name of the column group to drop.
-   * @throws IOException IOException could occur for various reasons. E.g.
-   *         a user does not have permissions to write to table directory.
-   *         
-   */
-  public static void dropColumnGroup(Path path, Configuration conf,
-                                     String cgName) 
-                                     throws IOException {
-    
-    FileSystem fs = FileSystem.get(conf);
-    int triedCount = 0;
-    int numCGs =  SchemaFile.getNumCGs(path, conf);
-    SchemaFile schemaFile = null;
-    
-    /* Retry up to numCGs times accounting for other CG deleting threads or processes.*/
-    while (triedCount ++ < numCGs) {
-      try {
-        schemaFile = new SchemaFile(path, null, conf);
-        break;
-      } catch (FileNotFoundException e) {
-        LOG.info("Try " + triedCount + " times : " + e.getMessage());
-      } catch (Exception e) {
-        throw new IOException ("Cannot construct SchemaFile : " + e.getMessage());
-      }
-    }
-    
-    if (schemaFile == null) {
-      throw new IOException ("Cannot construct SchemaFile");
-    }
-    
-    int cgIdx = schemaFile.getCGByName(cgName);
-    if (cgIdx < 0) {
-      throw new IOException(path + 
-             " : Could not find a column group with the name '" + cgName + "'");
-    }
-    
-    Path cgPath = new Path(path, schemaFile.getName(cgIdx));
-        
-    //Clean up any previous unfinished attempts to drop column groups?    
-    if (schemaFile.isCGDeleted(cgIdx)) {
-      // Clean up unfinished delete if it exists. so that clean up can 
-      // complete if the previous deletion was interrupted for some reason.
-      if (fs.exists(cgPath)) {
-        LOG.info(path + " : " + 
-                 " clearing unfinished deletion of column group " +
-                 cgName + ".");
-        fs.delete(cgPath, true);
-      }
-      LOG.info(path + " : column group " + cgName + " is already deleted.");
-      return;
-    }
-    
-    // try to delete the column group:
-    
-    // first check if the user has enough permissions to list the directory
-    fs.listStatus(cgPath);   
-    
-    //verify if the user has enough permissions by trying to create
-    //a temporary file in cg.
-    OutputStream out = fs.create(
-              new Path(cgPath, ".tmp" + DELETED_CG_PREFIX + cgName), true);
-    out.close();
-    
-    //First try to create a file indicating a column group is deleted.
-    try {
-      Path deletedCGPath = new Path(path, DELETED_CG_PREFIX + cgName);
-      // create without overriding.
-      out = fs.create(deletedCGPath, false);
-      // should we write anything?
-      out.close();
-    } catch (IOException e) {
-      // one remote possibility is that another user 
-      // already deleted CG. 
-      SchemaFile tempSchema = new SchemaFile(path, null, conf);
-      if (tempSchema.isCGDeleted(cgIdx)) {
-        LOG.info(path + " : " + cgName + 
-                 " is deleted by someone else. That is ok.");
-        return;
-      }
-      // otherwise, it is some other error.
-      throw e;
-    }
-    
-    // At this stage, the CG is marked deleted. Now just try to
-    // delete the actual directory:
-    if (!fs.delete(cgPath, true)) {
-      String msg = path + " : Could not detete column group " +
-                   cgName + ". It is marked deleted.";
-      LOG.warn(msg);
-      throw new IOException(msg);
-    }
-    
-    LOG.info("Dropped " + cgName + " from " + path);
-  }
-  
-  /**
-   * BasicTable reader.
-   */
-  public static class Reader implements Closeable {
-    private Path path;
-    private boolean closed = true;
-    private SchemaFile schemaFile;
-    private Projection projection;
-    boolean inferredMapping;
-    private MetaFile.Reader metaReader;
-    private BasicTableStatus status;
-    private int firstValidCG = -1; /// First column group that exists.
-    private int rowSplitCGIndex = -1;
-    Partition partition;
-    ColumnGroup.Reader[] colGroups;
-    Tuple[] cgTuples;
-
-    private synchronized void checkInferredMapping() throws ParseException, IOException {
-      if (!inferredMapping) {
-        for (int i = 0; i < colGroups.length; ++i) {
-          if (colGroups[i] != null) {
-            colGroups[i].setProjection(partition.getProjection(i));
-          } 
-          if (partition.isCGNeeded(i)) {
-            if (isCGDeleted(i)) {
-              // this is a deleted column group. Warn about it.
-              LOG.warn("Trying to read from deleted column group " + 
-                       schemaFile.getName(i) + 
-                       ". NULL is returned for corresponding columns. " +
-                       "Table at " + path);
-            } else {
-              cgTuples[i] = TypesUtils.createTuple(colGroups[i].getSchema());
-            }
-          }
-          else
-            cgTuples[i] = null;
-        }
-        partition.setSource(cgTuples);
-        inferredMapping = true;
-      }
-      else {
-        // the projection is not changed, so we do not need to recalculate the
-        // mapping
-      }
-    }
-
-    /**
-     * Returns true if a column group is deleted.
-     */
-    private boolean isCGDeleted(int nx) {
-      return colGroups[nx] == null;
-    }
-    
-    /**
-     * Create a BasicTable reader.
-     * 
-     * @param path
-     *          The directory path to the BasicTable.
-     * @param conf
-     *          Optional configuration parameters.
-     * @throws IOException
-     */
-
-    public Reader(Path path, Configuration conf) throws IOException {
-      this(path, null, conf);
-    }
-    public Reader(Path path, String[] deletedCGs, Configuration conf) throws IOException {
-      try {
-        boolean mapper = (deletedCGs != null);
-        this.path = path;
-        schemaFile = new SchemaFile(path, deletedCGs, conf);
-        metaReader = MetaFile.createReader(new Path(path, BT_META_FILE), conf);
-        // create column group readers
-        int numCGs = schemaFile.getNumOfPhysicalSchemas();
-        Schema schema;
-        colGroups = new ColumnGroup.Reader[numCGs];
-        cgTuples = new Tuple[numCGs];
-        // set default projection that contains everything
-        schema = schemaFile.getLogical();
-        projection = new Projection(schema);
-        String storage = schemaFile.getStorageString();
-        String comparator = schemaFile.getComparator();
-        partition = new Partition(schema, projection, storage, comparator);
-        for (int nx = 0; nx < numCGs; nx++) {
-          if (!schemaFile.isCGDeleted(nx)) {
-            colGroups[nx] =
-              new ColumnGroup.Reader(new Path(path, partition.getCGSchema(nx).getName()),
-                                     conf, mapper);
-            if (firstValidCG < 0) {
-              firstValidCG = nx;
-            }
-          }
-          if (colGroups[nx] != null && partition.isCGNeeded(nx))
-            cgTuples[nx] = TypesUtils.createTuple(colGroups[nx].getSchema());
-          else
-            cgTuples[nx] = null;
-        }
-        closed = false;
-      }
-      catch (Exception e) {
-        throw new IOException("BasicTable.Reader constructor failed : "
-            + e.getMessage());
-      }
-      finally {
-        if (closed) {
-          /**
-           * Construction fails.
-           */
-          if (colGroups != null) {
-            for (int i = 0; i < colGroups.length; ++i) {
-              if (colGroups[i] != null) {
-                try {
-                  colGroups[i].close();
-                }
-                catch (Exception e) {
-                  // ignore error
-                }
-              }
-            }
-          }
-          if (metaReader != null) {
-            try {
-              metaReader.close();
-            }
-            catch (Exception e) {
-              // no-op
-            }
-          }
-        }
-      }
-    }
-
-    /**
-     * Is the Table sorted?
-     * 
-     * @return Whether the table is sorted.
-     */
-    public boolean isSorted() {
-      return schemaFile.isSorted();
-    }
-
-    /**
-     * @return the list of sorted columns
-     */
-    public SortInfo getSortInfo()
-    {
-      return schemaFile.getSortInfo();
-    }
-    
-    /**
-     * @return the name of i-th column group 
-     */
-    public String getName(int i) {
-      return schemaFile.getName(i);
-    }
-
-    /**
-     * Set the projection for the reader. This will affect calls to
-     * {@link #getScanner(RangeSplit, boolean)},
-     * {@link #getScanner(BytesWritable, BytesWritable, boolean)},
-     * {@link #getStatus()}, {@link #getSchema()}.
-     * 
-     * @param projection
-     *          The projection on the BasicTable for subsequent read operations.
-     *          For this version of implementation, the projection is a comma
-     *          separated list of column names, such as
-     *          "FirstName, LastName, Sex, Department". If we want select all
-     *          columns, pass projection==null.
-     * @throws IOException
-     */
-    public synchronized void setProjection(String projection)
-        throws ParseException, IOException {
-      if (projection == null) {
-        this.projection = new Projection(schemaFile.getLogical());
-        partition =
-            new Partition(schemaFile.getLogical(), this.projection, schemaFile
-                .getStorageString(), schemaFile.getComparator());
-      }
-      else {
-        /**
-         * the typed schema from projection which is untyped or actually typed
-         * as "bytes"
-         */
-        this.projection =
-            new Projection(schemaFile.getLogical(), projection);
-        partition =
-            new Partition(schemaFile.getLogical(), this.projection, schemaFile
-                .getStorageString(), schemaFile.getComparator());
-      }
-      inferredMapping = false;
-    }
-
-    /**
-     * Get the status of the BasicTable.
-     */
-    public BasicTableStatus getStatus() throws IOException {
-      if (status == null)
-        buildStatus();
-      return status;
-    }
-
-    /**
-     * Given a split range, calculate how the file data that fall into the range
-     * are distributed among hosts.
-     * 
-     * @param split
-     *          The range-based split. Can be null to indicate the whole TFile.
-     * @return An object that conveys how blocks fall in the split are
-     *         distributed across hosts.
-     * @see #rangeSplit(int)
-     */
-    public BlockDistribution getBlockDistribution(RangeSplit split)
-        throws IOException {
-      BlockDistribution bd = new BlockDistribution();
-      if (firstValidCG >= 0)
-      {
-        for (int nx = 0; nx < colGroups.length; nx++) {
-          if (partition.isCGNeeded(nx) && !isCGDeleted(nx)) {
-            bd.add(colGroups[nx].getBlockDistribution(split == null ? null : split.getCGRangeSplit()));
-          }
-        }
-      }
-      return bd;
-    }
-
-
-    /**
-     * Given a row-based split, calculate how the file data that fall into the split
-     * are distributed among hosts.
-     * 
-     * @param split The row-based split. <i>Cannot</i> be null.
-     * @return An object that conveys how blocks fall into the split are
-     *         distributed across hosts.
-     */
-    public BlockDistribution getBlockDistribution(RowSplit split)
-        throws IOException {
-      BlockDistribution bd = new BlockDistribution();
-      int cgIdx = split.getCGIndex();      
-      bd.add(colGroups[cgIdx].getBlockDistribution(split.getCGRowSplit()));
-      
-      return bd;
-    }
-
-    /**
-     * Collect some key samples and use them to partition the table. Only
-     * applicable to sorted BasicTable. The returned {@link KeyDistribution}
-     * object also contains information on how data are distributed for each
-     * key-partitioned bucket.
-     * 
-     * @param n
-     *          Targeted size of the sampling.
-     * @param nTables
-     *          Number of tables in union
-     * @return KeyDistribution object.
-     * @throws IOException
-     */
-    public KeyDistribution getKeyDistribution(int n, int nTables, BlockDistribution lastBd) throws IOException {
-      if (firstValidCG >= 0)
-      {
-        // pick the largest CG as in the row split case
-        return colGroups[getRowSplitCGIndex()].getKeyDistribution(n, nTables, lastBd);
-      }
-      return null;
-    }
-
-    /**
-     * Get a scanner that reads all rows whose row keys fall in a specific
-     * range. Only applicable to sorted BasicTable.
-     * 
-     * @param beginKey
-     *          The begin key of the scan range. If null, start from the first
-     *          row in the table.
-     * @param endKey
-     *          The end key of the scan range. If null, scan till the last row
-     *          in the table.
-     * @param closeReader
-     *          close the underlying Reader object when we close the scanner.
-     *          Should be set to true if we have only one scanner on top of the
-     *          reader, so that we should release resources after the scanner is
-     *          closed.
-     * @return A scanner object.
-     * @throws IOException
-     */
-    public synchronized TableScanner getScanner(BytesWritable beginKey,
-        BytesWritable endKey, boolean closeReader) throws IOException {
-      try {
-        checkInferredMapping();
-      }
-      catch (Exception e) {
-        throw new IOException("getScanner failed : " + e.getMessage());
-      }
-      return new BTScanner(beginKey, endKey, closeReader, partition);
-    }
-
-    /**
-     * Get a scanner that reads a consecutive number of rows as defined in the
-     * {@link RangeSplit} object, which should be obtained from previous calls
-     * of {@link #rangeSplit(int)}.
-     * 
-     * @param split
-     *          The split range. If null, get a scanner to read the complete
-     *          table.
-     * @param closeReader
-     *          close the underlying Reader object when we close the scanner.
-     *          Should be set to true if we have only one scanner on top of the
-     *          reader, so that we should release resources after the scanner is
-     *          closed.
-     * @return A scanner object.
-     * @throws IOException
-     */
-    public synchronized TableScanner getScanner(RangeSplit split,
-        boolean closeReader) throws IOException, ParseException {
-      checkInferredMapping();
-      return new BTScanner(split, partition, closeReader);
-    }
-
-    /**
-     * Get a scanner that reads a consecutive number of rows as defined in the
-     * {@link RowSplit} object.
-     * 
-     * @param closeReader
-     *          close the underlying Reader object when we close the scanner.
-     *          Should be set to true if we have only one scanner on top of the
-     *          reader, so that we should release resources after the scanner is
-     *          closed.
-     * @param rowSplit split based on row numbers.
-     * 
-     * @return A scanner object.
-     * @throws IOException
-     */
-    public synchronized TableScanner getScanner(boolean closeReader,
-                                                RowSplit rowSplit) 
-      throws IOException, ParseException, ParseException {
-      checkInferredMapping();
-      return new BTScanner(rowSplit, closeReader, partition);
-    }
-    /**
-     * Get the schema of the table. The schema may be different from
-     * {@link BasicTable.Reader#getSchema(Path, Configuration)} if a projection
-     * has been set on the table.
-     * 
-     * @return The schema of the BasicTable.
-     */
-    public Schema getSchema() {
-      return projection.getSchema();
-    }
-
-    /**
-     * Get the BasicTable schema without loading the full table index.
-     * 
-     * @param path
-     *          The path to the BasicTable.
-     * @deletedCGs
-     *          The deleted column groups from front end; null if unavailable from front end
-     * @param conf
-     * @return The logical Schema of the table (all columns).
-     * @throws IOException
-     */
-    public static Schema getSchema(Path path, Configuration conf)
-        throws IOException {
-      // fake an empty deleted cg list as getSchema does not care about deleted cgs
-      SchemaFile schF = new SchemaFile(path, new String[0], conf);
-      return schF.getLogical();
-    }
-
-    /**
-     * Get the path to the table.
-     * 
-     * @return The path string to the table.
-     */
-    public String getPath() {
-      return path.toString();
-    }
-    
-    /**
-     * Get the path filter used by the table.
-     */
-    public PathFilter getPathFilter(Configuration conf) {
-      ColumnGroup.CGPathFilter filter = new ColumnGroup.CGPathFilter();
-      ColumnGroup.CGPathFilter.setConf(conf);
-      return filter;
-    }
-
-    /**
-     * Split the table into at most n parts.
-     * 
-     * @param n Maximum number of parts in the output list.
-     * @return A list of RangeSplit objects, each of which can be used to
-     *         construct TableScanner later.
-     */
-    public List<RangeSplit> rangeSplit(int n) throws IOException {
-      // use the first non-deleted column group to do split, other column groups will be split exactly the same way.
-      List<RangeSplit> ret;
-      if (firstValidCG >= 0) {
-        List<CGRangeSplit> cgSplits = colGroups[firstValidCG].rangeSplit(n);
-        int numSlices = cgSplits.size();
-        ret = new ArrayList<RangeSplit>(numSlices);
-        for (int slice = 0; slice < numSlices; slice++) {
-          CGRangeSplit oneSliceSplit = cgSplits.get(slice);
-          ret.add(new BasicTable.Reader.RangeSplit(oneSliceSplit));
-        }
-
-        return ret;
-      } else { // all column groups are dropped.
-        ret = new ArrayList<RangeSplit>(1);
-        // add a dummy split
-        ret.add(new BasicTable.Reader.RangeSplit(new CGRangeSplit(0, 0)));
-        return ret;
-      }
-    }
-
-    /**
-     * We already use FileInputFormat to create byte offset-based input splits.
-     * Their information is encoded in starts, lengths and paths. This method is 
-     * to wrap this information to form RowSplit objects at basic table level.
-     * 
-     * @param starts array of starting byte of fileSplits.
-     * @param lengths array of length of fileSplits.
-     * @param paths array of path of fileSplits.
-     * @param splitCGIndex index of column group that is used to create fileSplits.
-     * @return A list of RowSplit objects, each of which can be used to
-     *         construct a TableScanner later. 
-     *         
-     */
-    public List<RowSplit> rowSplit(long[] starts, long[] lengths, Path[] paths,
-        int splitCGIndex, int[] batchSizes, int numBatches) throws IOException {
-      List<RowSplit> ret;      
-      List<CGRowSplit> cgSplits = colGroups[splitCGIndex].rowSplit(starts, lengths, paths, batchSizes, numBatches);
-      int numSlices = cgSplits.size();
-      ret = new ArrayList<RowSplit>(numSlices);
-      for (int slice = 0; slice < numSlices; slice++) {
-        CGRowSplit cgRowSplit = cgSplits.get(slice);
-        ret.add(new BasicTable.Reader.RowSplit(splitCGIndex, cgRowSplit));
-      }
-        
-      return ret;
-    }
-    
-    /**
-     * Rearrange the files according to the column group index ordering
-     * 
-     * @param filestatus array of FileStatus to be rearraged on 
-     */
-    public void rearrangeFileIndices(FileStatus[] fileStatus) throws IOException
-    {
-      colGroups[getRowSplitCGIndex()].rearrangeFileIndices(fileStatus);
-    }
-
-    /** 
-     * Get index of the column group that will be used for row-based split. 
-     * 
-     */
-    public int getRowSplitCGIndex() throws IOException {
-      // Try to find the largest non-deleted and used column group by projection;
-      // Try to find the largest non-deleted and used column group by projection;
-      if (rowSplitCGIndex == -1)
-      {
-        int largestCGIndex = -1;
-        long largestCGSize = -1;
-        for (int i=0; i<colGroups.length; i++) {
-          if (!partition.isCGNeeded(i) || isCGDeleted(i)) {
-            continue;
-          }
-          ColumnGroup.Reader reader = colGroups[i];
-          BasicTableStatus btStatus = reader.getStatus();
-          long size = btStatus.getSize();
-          if (size > largestCGSize) {
-            largestCGIndex = i;
-            largestCGSize = size;
-          }
-        }
-       
-        /* We do have a largest non-deleted and used column group,
-        and we use it to do split. */
-        if (largestCGIndex >= 0) { 
-          rowSplitCGIndex = largestCGIndex;
-        } else if (firstValidCG >= 0) { /* If all projection columns are either deleted or non-existing,
-                                        then we use the first non-deleted column group to do split if it exists. */
-          rowSplitCGIndex = firstValidCG;
-        } 
-      } 
-      return rowSplitCGIndex;
-    }
-
-
-    /**
-     * Close the BasicTable for reading. Resources are released.
-     */
-    @Override
-    public void close() throws IOException {
-      if (!closed) {
-        try {
-          closed = true;
-          metaReader.close();
-          for (int i = 0; i < colGroups.length; ++i) {
-            if (colGroups[i] != null) {
-              colGroups[i].close();
-            }
-          }
-        }
-        finally {
-          try {
-            metaReader.close();
-          }
-          catch (Exception e) {
-            // no-op
-          }
-          for (int i = 0; i < colGroups.length; ++i) {
-            try {
-              colGroups[i].close();
-            }
-            catch (Exception e) {
-              // no-op
-            }
-          }
-        }
-      }
-    }
-
-    String getBTSchemaString() {
-      return schemaFile.getBTSchemaString();
-    }
-
-    String getStorageString() {
-      return schemaFile.getStorageString();
-    }
-    
-    public String getDeletedCGs() {
-      return schemaFile.getDeletedCGs();
-    }
-
-    public static String getDeletedCGs(Path path, Configuration conf)
-    throws IOException {
-    	SchemaFile schF = new SchemaFile(path, new String[0], conf);
-    	return schF.getDeletedCGs();
-    }
-
-    private void buildStatus() throws IOException {
-      status = new BasicTableStatus();
-      if (firstValidCG >= 0) {
-        status.beginKey = colGroups[firstValidCG].getStatus().getBeginKey();
-        status.endKey = colGroups[firstValidCG].getStatus().getEndKey();
-        status.rows = colGroups[firstValidCG].getStatus().getRows();
-      } else {
-        status.beginKey = new BytesWritable(new byte[0]);
-        status.endKey = status.beginKey;
-        status.rows = 0;
-      }
-      status.size = 0;
-      for (int nx = 0; nx < colGroups.length; nx++) {
-        if (colGroups[nx] != null) {
-          status.size += colGroups[nx].getStatus().getSize();
-        }
-      }
-    }
-
-    /**
-     * Obtain an input stream for reading a meta block.
-     * 
-     * @param name
-     *          The name of the meta block.
-     * @return The input stream for reading the meta block.
-     * @throws IOException
-     * @throws MetaBlockDoesNotExist
-     */
-    public DataInputStream getMetaBlock(String name)
-        throws MetaBlockDoesNotExist, IOException {
-      return metaReader.getMetaBlock(name);
-    }
-
-    /**
-     * A range-based split on the metaReadertable.The content of the split is
-     * implementation-dependent.
-     */
-    public static class RangeSplit implements Writable {
-      //CGRangeSplit[] slice;
-      CGRangeSplit slice;
-
-      RangeSplit(CGRangeSplit split) {
-        slice = split;
-      }
-
-      /**
-       * Default constructor.
-       */
-      public RangeSplit() {
-        // no-op
-      }
-
-      /**
-       * @see Writable#readFields(DataInput)
-       */
-      @Override
-      public void readFields(DataInput in) throws IOException {
-        for (int nx = 0; nx < 1; nx++) {
-          CGRangeSplit cgrs = new CGRangeSplit();
-          cgrs.readFields(in);
-          slice = cgrs;
-        }
-      }
-
-      /**
-       * @see Writable#write(DataOutput)
-       */
-      @Override
-      public void write(DataOutput out) throws IOException {
-        //Utils.writeVInt(out, slice.length);
-        //for (CGRangeSplit split : slice) {
-        //  split.write(out);
-        //}
-        slice.write(out);
-      }
-
-      //CGRangeSplit get(int index) {
-       // return slice[index];
-      //}
-      
-      CGRangeSplit getCGRangeSplit() {
-        return slice;
-      }
-    }
-
-    /**
-     * A row-based split on the zebra table;
-     */
-    public static class RowSplit implements Writable {
-      int cgIndex;  // column group index where split lies on;
-      CGRowSplit slice; 
-
-      RowSplit(int cgidx, CGRowSplit split) {
-        this.cgIndex = cgidx;
-        this.slice = split;
-      }
-
-      /**
-       * Default constructor.
-       */
-      public RowSplit() {
-        // no-op
-      }
-      
-      @Override
-      public String toString() {
-        StringBuilder sb = new StringBuilder();
-        sb.append("{cgIndex = " + cgIndex + "}\n");
-        sb.append(slice.toString());
-        
-        return sb.toString();
-      }
-
-      /**
-       * @see Writable#readFields(DataInput)
-       */
-      @Override
-      public void readFields(DataInput in) throws IOException {
-        this.cgIndex = Utils.readVInt(in);
-        CGRowSplit cgrs = new CGRowSplit();
-        cgrs.readFields(in);
-        this.slice = cgrs;
-      }
-
-      /**
-       * @see Writable#write(DataOutput)
-       */
-      @Override
-      public void write(DataOutput out) throws IOException {
-        Utils.writeVInt(out, cgIndex);
-        slice.write(out);
-      }
-      
-      int getCGIndex() {
-        return cgIndex;
-      }
-
-      CGRowSplit getCGRowSplit() {
-        return slice;
-      }
-    }
-    
-    
-    /**
-     * BasicTable scanner class
-     */
-    private class BTScanner implements TableScanner {
-      private Projection schema;
-      private CGScanner[] cgScanners;
-      private int opCount = 0;
-      Random random = new Random(System.nanoTime());
-      // checking for consistency once every 1000 times.
-      private static final int VERIFY_FREQ = 1000;
-      private boolean sClosed = false;
-      private boolean closeReader;
-      private Partition partition;
-
-      private synchronized boolean checkIntegrity() {
-        return ((++opCount % VERIFY_FREQ) == 0) && (cgScanners.length > 1);
-      }
-
-      public BTScanner(BytesWritable beginKey, BytesWritable endKey,
-        boolean closeReader, Partition partition) throws IOException {
-        init(null, null, beginKey, endKey, closeReader, partition);
-      }
-      
-      public BTScanner(RangeSplit split, Partition partition,
-        boolean closeReader) throws IOException {
-        init(null, split, null, null, closeReader, partition);
-      }
-      
-      public BTScanner(RowSplit rowSplit,  boolean closeReader, 
-                       Partition partition) throws IOException {
-        init(rowSplit, null, null, null, closeReader, partition);
-      }      
-
-      /**
-       * Creates new CGRowSplit. If the startRow in rowSplit is not set 
-       * (i.e. < 0), it sets the startRow and numRows based on 'startByte' 
-       * and 'numBytes' from given rowSplit.
-       */
-      private CGRowSplit makeCGRowSplit(RowSplit rowSplit) throws IOException {
-        CGRowSplit inputCGSplit = rowSplit.getCGRowSplit(); 
-
-        int cgIdx = rowSplit.getCGIndex();
-        
-        CGRowSplit cgSplit = new CGRowSplit();
-
-        // Find the row range :
-        if (isCGDeleted(cgIdx)) {
-          throw new IOException("CG " + cgIdx + " is deleted.");
-        }
-        //fill the row numbers.
-        colGroups[cgIdx].fillRowSplit(cgSplit, inputCGSplit);
-        return cgSplit;
-      }
-    
-      // Helper function for initialization.
-      private CGScanner createCGScanner(int cgIndex, CGRowSplit cgRowSplit, 
-                                           RangeSplit rangeSplit,
-                                           BytesWritable beginKey, 
-                                           BytesWritable endKey) 
-                      throws IOException, ParseException, 
-                             ParseException {        
-        if (cgRowSplit != null) {
-          return colGroups[cgIndex].getScanner(false, cgRowSplit);
-        }      
-        if (beginKey != null || endKey != null) {
-          return colGroups[cgIndex].getScanner(beginKey, endKey, false);
-        }
-        return colGroups[cgIndex].getScanner
-                ((rangeSplit == null ? null : rangeSplit.getCGRangeSplit()), 
-                 false);
-      }
-      
-      /**
-       * If rowRange is not null, scanners will be created based on the 
-       * row range. <br>
-       * If RangeSplit is not null, scaller will be based on the range, <br>
-       * otherwise, these are based on keys.
-       */
-      private void init(RowSplit rowSplit, RangeSplit rangeSplit,
-                   BytesWritable beginKey, BytesWritable endKey, 
-                   boolean closeReader, Partition partition) throws IOException {
-        this.partition = partition;
-        boolean anyScanner = false;
-        
-        CGRowSplit cgRowSplit = null;
-        if (rowSplit != null) {
-          cgRowSplit = makeCGRowSplit(rowSplit);
-        }
-
-        try {
-          schema = partition.getProjection();
-          cgScanners = new CGScanner[colGroups.length];
-          for (int i = 0; i < colGroups.length; ++i) {
-            if (!isCGDeleted(i) && partition.isCGNeeded(i)) 
-            {
-              anyScanner = true;
-              cgScanners[i] = createCGScanner(i, cgRowSplit, rangeSplit,
-                                              beginKey, endKey);                                                             
-            } else
-              cgScanners[i] = null;
-          }
-          if (!anyScanner && firstValidCG >= 0) {
-            // if no CG is needed explicitly by projection but the "countRow" still needs to access some column group
-            cgScanners[firstValidCG] = createCGScanner(firstValidCG, cgRowSplit, 
-                                                       rangeSplit,
-                                                       beginKey, endKey);            
-          }
-          this.closeReader = closeReader;
-          sClosed = false;
-        }
-        catch (Exception e) {
-          throw new IOException("BTScanner constructor failed : "
-              + e.getMessage());
-        }
-        finally {
-          if (sClosed) {
-            if (cgScanners != null) {
-              for (int i = 0; i < cgScanners.length; ++i) {
-                if (cgScanners[i] != null) {
-                  try {
-                    cgScanners[i].close();
-                    cgScanners[i] = null;
-                  }
-                  catch (Exception e) {
-                    // no-op
-                  }
-                }
-              }
-            }
-          }
-        }
-      }
-
-      @Override
-      public boolean advance() throws IOException {
-        boolean first = false, cur, firstAdvance = true;
-        for (int nx = 0; nx < cgScanners.length; nx++) {
-          if (cgScanners[nx] != null)
-          {
-            cur = cgScanners[nx].advanceCG();
-            if (!firstAdvance) {
-              if (cur != first) {
-                throw new IOException(
-                    "advance() failed: Column Groups are not evenly positioned.");
-              }
-            }
-            else {
-              firstAdvance = false;
-              first = cur;
-            }
-          }
-        }
-        return first;
-      }
-
-      @Override
-      public boolean atEnd() throws IOException {
-        boolean ret = true;
-        int i;
-        for (i = 0; i < cgScanners.length; i++)
-        {
-          if (cgScanners[i] != null)
-          {
-            ret = cgScanners[i].atEnd();
-            break;
-          }
-        }
-
-        if (i == cgScanners.length)
-        {
-          return true;
-        }
-        
-        if (!checkIntegrity()) {
-          return ret;
-        }
-
-        while (true)
-        {
-          int index = random.nextInt(cgScanners.length);
-          if (cgScanners[index] != null) {
-            if (cgScanners[index].atEnd() != ret) {
-              throw new IOException(
-                  "atEnd() failed: Column Groups are not evenly positioned.");
-            }
-            break;
-          }
-        }
-        return ret;
-      }
-
-      @Override
-      public void getKey(BytesWritable key) throws IOException {
-        int i;
-        for (i = 0; i < cgScanners.length; i++)
-        {
-          if (cgScanners[i] != null)
-          {
-            cgScanners[i].getCGKey(key);
-            break;
-          }
-        }
-
-        if (i == cgScanners.length)
-          return;
-
-        if (!checkIntegrity()) {
-          return;
-        }
-
-        while (true)
-        {
-          int index = random.nextInt(cgScanners.length);
-          if (cgScanners[index] != null)
-          {
-            BytesWritable key2 = new BytesWritable();
-            cgScanners[index].getCGKey(key2);
-            if (key.equals(key2)) {
-              return;
-            }
-            break;
-          }
-        }
-        throw new IOException(
-            "getKey() failed: Column Groups are not evenly positioned.");
-      }
-
-      @Override
-      public void getValue(Tuple row) throws IOException {
-        if (row.size() < projection.getSchema().getNumColumns()) {
-          throw new IOException("Mismatched tuple object");
-        }
-
-        for (int i = 0; i < cgScanners.length; ++i)
-        {
-          if (cgScanners[i] != null)
-          {
-            if (partition.isCGNeeded(i))
-            {
-              if (cgTuples[i] == null)
-                throw new AssertionError("cgTuples["+i+"] is null");
-              cgScanners[i].getCGValue(cgTuples[i]);
-            }
-          }
-        }
-
-        try {
-          partition.read(row);
-        }
-        catch (Exception e) {
-          throw new IOException("getValue() failed: " + e.getMessage());
-        }
-      }
-
-      @Override
-      public boolean seekTo(BytesWritable key) throws IOException {
-        boolean first = false, cur, firstset = false;
-        for (int nx = 0; nx < cgScanners.length; nx++) {
-          if (cgScanners[nx] == null)
-            continue;
-          cur = cgScanners[nx].seekTo(key);
-          if (firstset) {
-            if (cur != first) {
-              throw new IOException(
-                  "seekTo() failed: Column Groups are not evenly positioned.");
-            }
-          }
-          else {
-            first = cur;
-            firstset = true;
-          }
-        }
-        return first;
-      }
-
-      @Override
-      public void seekToEnd() throws IOException {
-        for (int nx = 0; nx < cgScanners.length; nx++) {
-          if (cgScanners[nx] == null)
-            continue;
-          cgScanners[nx].seekToEnd();
-        }
-      }
-
-      @Override
-      public String getProjection() {
-        return schema.toString();
-      }
-      
-      @Override
-      public Schema getSchema() {
-        return schema.getSchema();
-      }
-
-      @Override
-      public void close() throws IOException {
-        if (sClosed) return;
-        sClosed = true;
-        try {
-          for (int nx = 0; nx < cgScanners.length; nx++) {
-            if (cgScanners[nx] == null)
-              continue;
-            cgScanners[nx].close();
-            cgScanners[nx] = null;
-          }
-          if (closeReader) {
-            BasicTable.Reader.this.close();
-          }
-        }
-        finally {
-          for (int nx = 0; nx < cgScanners.length; nx++) {
-            if (cgScanners[nx] == null)
-              continue;
-            try {
-              cgScanners[nx].close();
-              cgScanners[nx] = null;
-            }
-            catch (Exception e) {
-              // no-op
-            }
-          }
-          if (closeReader) {
-            try {
-              BasicTable.Reader.this.close();
-            }
-            catch (Exception e) {
-              // no-op
-            }
-          }
-        }
-      }
-    }
-  }
-
-  /**
-   * BasicTable writer.
-   */
-  public static class Writer implements Closeable {
-    private SchemaFile schemaFile;
-    private MetaFile.Writer metaWriter;
-    private boolean closed = true;
-    ColumnGroup.Writer[] colGroups;
-    Partition partition;
-    boolean sorted;
-    private boolean finished;
-    Tuple[] cgTuples;
-    private Path actualOutputPath;
-    private Configuration writerConf;
-
-
-
-    /**
-     * Create a BasicTable writer. The semantics are as follows:
-     * <ol>
-     * <li>If path does not exist:
-     * <ul>
-     * <li>create the path directory, and initialize the directory for future
-     * row insertion..
-     * </ul>
-     * <li>If path exists and the directory is empty: initialize the directory
-     * for future row insertion.
-     * <li>If path exists and contains what look like a complete BasicTable,
-     * IOException will be thrown.
-     * </ol>
-     * This constructor never removes a valid/complete BasicTable.
-     * 
-     * @param path
-     *          The path to the Basic Table, either not existent or must be a
-     *          directory.
-     * @param btSchemaString
-     *          The schema of the Basic Table. For this version of
-     *          implementation, the schema of a table is a comma or
-     *          semicolon-separated list of column names, such as
-     *          "FirstName, LastName; Sex, Department".
-     * @param sortColumns
-     *          String of comma-separated sorted columns: null for unsorted tables
-     * @param comparator
-     *          Name of the comparator used in sorted tables
-     * @param conf
-     *          Optional Configuration objects.
-     * 
-     * @throws IOException
-     * @see Schema
-     */
-    public Writer(Path path, String btSchemaString, String btStorageString, String sortColumns,
-        String comparator, Configuration conf) throws IOException {
-      try {
-      	actualOutputPath = path;
-    	  writerConf = conf;    	  
-        schemaFile =
-            new SchemaFile(path, btSchemaString, btStorageString, sortColumns,
-                comparator, conf);
-        partition = schemaFile.getPartition();
-        int numCGs = schemaFile.getNumOfPhysicalSchemas();
-        colGroups = new ColumnGroup.Writer[numCGs];
-        cgTuples = new Tuple[numCGs];
-        sorted = schemaFile.isSorted();
-        for (int nx = 0; nx < numCGs; nx++) {
-          colGroups[nx] =
-              new ColumnGroup.Writer( 
-                 new Path(path, schemaFile.getName(nx)),
-            		 schemaFile.getPhysicalSchema(nx), 
-            		 sorted, 
-                 comparator,
-            		 schemaFile.getName(nx),
-            		 schemaFile.getSerializer(nx), 
-            		 schemaFile.getCompressor(nx), 
-            		 schemaFile.getOwner(nx), 
-            		 schemaFile.getGroup(nx),
-            		 schemaFile.getPerm(nx), 
-            		 false, 
-            		 conf);
-          cgTuples[nx] = TypesUtils.createTuple(colGroups[nx].getSchema());
-        }
-        metaWriter = MetaFile.createWriter(new Path(path, BT_META_FILE), conf);
-        partition.setSource(cgTuples);
-        closed = false;
-      }
-      catch (Exception e) {
-        throw new IOException("ColumnGroup.Writer constructor failed : "
-            + e.getMessage());
-      }
-      finally {
-        ;
-        if (!closed) return;
-        if (metaWriter != null) {
-          try {
-            metaWriter.close();
-          }
-          catch (Exception e) {
-            // no-op
-          }
-        }
-        if (colGroups != null) {
-          for (int i = 0; i < colGroups.length; ++i) {
-            if (colGroups[i] != null) {
-              try {
-                colGroups[i].close();
-              }
-              catch (Exception e) {
-                // no-op
-              }
-            }
-          }
-        }
-      }
-    }
-
-    /**
-     * a wrapper to support backward compatible constructor
-     */
-    public Writer(Path path, String btSchemaString, String btStorageString,
-        Configuration conf) throws IOException {
-      this(path, btSchemaString, btStorageString, null, null, conf);
-    }
-
-    /**
-     * Reopen an already created BasicTable for writing. Exception will be
-     * thrown if the table is already closed, or is in the process of being
-     * closed.
-     */
-    public Writer(Path path, Configuration conf) throws IOException {
-      try {
-      	actualOutputPath = path;
-    	  writerConf = conf;
-    	  
-    	  if (ZebraConf.getOutputSchema(conf) != null) { 
-    	    schemaFile = new SchemaFile(conf);  // Read out schemaFile from conf, instead of from hdfs;
-    	  } else { // This is only for io test cases and it cannot happen for m/r and pig cases; 
-    	    schemaFile = new SchemaFile(path, new String[0], conf); // fake an empty deleted cg list as no cg should have been deleted now
-    	  }
-        int numCGs = schemaFile.getNumOfPhysicalSchemas();
-        partition = schemaFile.getPartition();
-        sorted = schemaFile.isSorted();
-        colGroups = new ColumnGroup.Writer[numCGs];
-        cgTuples = new Tuple[numCGs];
-        Path tmpWorkPath = new Path(path, "_temporary");	
-        for (int nx = 0; nx < numCGs; nx++) {
-          CGSchema cgschema = new CGSchema(schemaFile.getPhysicalSchema(nx), sorted, 
-              schemaFile.getComparator(), schemaFile.getName(nx), schemaFile.getSerializer(nx), schemaFile.getCompressor(nx),
-              schemaFile.getOwner(nx), schemaFile.getGroup(nx), schemaFile.getPerm(nx));
-          
-          colGroups[nx] =
-            new ColumnGroup.Writer(
-            		new Path(path, partition.getCGSchema(nx).getName()),
-            		new Path(tmpWorkPath, partition.getCGSchema(nx).getName()),
-                  cgschema,conf);
-          
-          cgTuples[nx] = TypesUtils.createTuple(colGroups[nx].getSchema());
-        }
-        partition.setSource(cgTuples);
-        metaWriter = MetaFile.createWriter(new Path(path, BT_META_FILE), conf);
-        closed = false;
-      }
-      catch (Exception e) {
-        throw new IOException("ColumnGroup.Writer failed : " + e.getMessage());
-      }
-      finally {
-        if (!closed) return;
-        if (metaWriter != null) {
-          try {
-            metaWriter.close();
-          }
-          catch (Exception e) {
-            // no-op
-          }
-        }
-        if (colGroups != null) {
-          for (int i = 0; i < colGroups.length; ++i) {
-            if (colGroups[i] != null) {
-              try {
-                colGroups[i].close();
-              }
-              catch (Exception e) {
-                // no-op
-              }
-            }
-          }
-        }
-      }
-    }
-
-    /**
-     * Release resources used by the object. Unlike close(), finish() does not
-     * make the table immutable.
-     */
-    public void finish() throws IOException {
-      if (finished) return;
-      finished = true;
-      try {
-        for (int nx = 0; nx < colGroups.length; nx++) {
-          if (colGroups[nx] != null) {
-            colGroups[nx].finish();
-          }
-        }
-        metaWriter.finish();
-      }
-      finally {
-        try {
-          metaWriter.finish();
-        }
-        catch (Exception e) {
-          // no-op
-        }
-        for (int i = 0; i < colGroups.length; ++i) {
-          try {
-            colGroups[i].finish();
-          }
-          catch (Exception e) {
-            // no-op
-          }
-        }
-      }
-    }
-
-    /**
-     * Close the BasicTable for writing. No more inserters can be obtained after
-     * close().
-     */
-    @Override
-    public void close() throws IOException {
-      cleanupTempDir();	
-      if (closed) return;
-      closed = true;
-      if (!finished)
-        finish();
-      try {
-        ColumnGroup.CGIndex firstCGIndex = null, cgIndex;
-        int first = -1;
-        for (int nx = 0; nx < colGroups.length; nx++) {
-          if (colGroups[nx] != null) {
-            colGroups[nx].close();
-            if (first == -1)
-            {
-              first = nx;
-              firstCGIndex = colGroups[nx].index;
-            } else {
-              cgIndex = colGroups[nx].index;
-              if (cgIndex.size() != firstCGIndex.size())
-                throw new IOException("Column Group "+colGroups[nx].path.getName()+
-                    " has different number of files than in column group " + colGroups[first].path.getName());
-              int size = firstCGIndex.size();
-              for (int i = 0; i < size; i++)
-              {
-                if (!cgIndex.get(i).name.equals(firstCGIndex.get(i).name))
-                  throw new IOException("File["+i+"] in Column Group "+colGroups[nx].path.getName()+
-                      " has a different name: "+cgIndex.get(i).name+" than " + 
-                      firstCGIndex.get(i).name + " in column group " + colGroups[first].path.getName());
-                if (cgIndex.get(i).rows != firstCGIndex.get(i).rows)
-                  throw new IOException("File "+cgIndex.get(i).name+"Column Group "+colGroups[nx].path.getName()+
-                      " has a different number of rows, " + cgIndex.get(i).rows + ", than " +
-                      firstCGIndex.get(i).rows + " in column group " + colGroups[first].path.getName());
-              }
-            }
-          }
-        }
-        metaWriter.close();
-      }
-      finally {
-        try {
-          metaWriter.close();
-        }
-        catch (Exception e) {
-          // no-op
-        }
-        for (int i = 0; i < colGroups.length; ++i) {
-          try {
-            colGroups[i].close();
-          }
-          catch (Exception e) {
-            // no-op
-          }
-        }
-      }
-    }
-
-    /**
-     * Removes the temporary directory underneath
-     * $path/_temporary used to create intermediate data
-     * during recrd writing
-     */
-    
-    private void cleanupTempDir() throws IOException {
-    	FileSystem fileSys = actualOutputPath.getFileSystem(writerConf);
-        Path pathToRemove = new Path(actualOutputPath, "_temporary");
-        if (fileSys.exists(pathToRemove)) {
-            if(!fileSys.delete(pathToRemove, true)) {
-              LOG.error("Failed to delete the temporary output" + 
-                      " directory: " + pathToRemove.toString());            
-            }
-        }
-    }    
-    
-    /**
-     * Get the schema of the table.
-     * 
-     * @return the Schema object.
-     */
-    public Schema getSchema() {
-      return schemaFile.getLogical();
-    }
-    
-    /**
-     * @return sortness
-     */
-    public boolean isSorted() {
-    	return sorted;
-    }
-
-    /**
-     * Get the list of sorted columns.
-     * @return the list of sorted columns
-     */
-    public SortInfo getSortInfo()
-    {
-      return schemaFile.getSortInfo();
-    }
-
-    /**
-     * Get a inserter with a given name.
-     * 
-     * @param name
-     *          the name of the inserter. If multiple calls to getInserter with
-     *          the same name has been called, we expect they are the result of
-     *          speculative execution and at most one of them will succeed.
-     * @param finishWriter
-     *          finish the underlying Writer object upon the close of the
-     *          Inserter. Should be set to true if there is only one inserter
-     *          operate on the table, so we should call finish() after the
-     *          Inserter is closed.
-     * 
-     * @return A inserter object.
-     * @throws IOException
-     */
-    public TableInserter getInserter(String name, boolean finishWriter)
-        throws IOException {
-      return this.getInserter(name, finishWriter, true);
-    }
-    
-    /**
-     * Get a inserter with a given name.
-     * 
-     * @param name
-     *          the name of the inserter. If multiple calls to getInserter with
-     *          the same name has been called, we expect they are the result of
-     *          speculative execution and at most one of them will succeed.
-     * @param finishWriter
-     *          finish the underlying Writer object upon the close of the
-     *          Inserter. Should be set to true if there is only one inserter
-     *          operate on the table, so we should call finish() after the
-     *          Inserter is closed.
-     * @param checktype 
-     *          whether or not do type check.
-     * 
-     * @return A inserter object.
-     * @throws IOException
-     */
-    public TableInserter getInserter(String name, boolean finishWriter, boolean checkType)
-        throws IOException {
-      if (closed) {
-        throw new IOException("BasicTable closed");
-      }
-      return new BTInserter(name, finishWriter, partition, checkType);
-    }
-
-
-    /**
-     * Obtain an output stream for creating a Meta Block with the specific name.
-     * This method can only be called after we insert all rows into the table.
-     * All Meta Blocks must be created by a single process prior to closing the
-     * table. No more inserter can be created after this call.
-     * 
-     * @param name
-     *          The name of the Meta Block
-     * @return The output stream. Close the stream to conclude the writing.
-     * @throws IOException
-     * @throws MetaBlockAlreadyExists
-     */
-    public DataOutputStream createMetaBlock(String name)
-        throws MetaBlockAlreadyExists, IOException {
-      return metaWriter.createMetaBlock(name);
-    }
-
-    private class BTInserter implements TableInserter {
-      private TableInserter cgInserters[];
-      private boolean sClosed = true;
-      private boolean finishWriter;
-      private Partition partition = null;
-
-      BTInserter(String name, boolean finishWriter, Partition partition)
-          throws IOException {
-        this(name, finishWriter, partition, true);
-      }
-      
-      BTInserter(String name, boolean finishWriter, Partition partition, boolean checkType)
-      throws IOException {
-        try {
-          cgInserters = new ColumnGroup.Writer.CGInserter[colGroups.length];
-          for (int nx = 0; nx < colGroups.length; nx++) {
-            cgInserters[nx] = colGroups[nx].getInserter(name, false, checkType);
-          }
-          this.finishWriter = finishWriter;
-          this.partition = partition;
-          sClosed = false;
-        }
-        catch (Exception e) {
-          throw new IOException("BTInsert constructor failed :"
-              + e.getMessage());
-        }
-        finally {
-          if (sClosed) {
-            if (cgInserters != null) {
-              for (int i = 0; i < cgInserters.length; ++i) {
-                if (cgInserters[i] != null) {
-                  try {
-                    cgInserters[i].close();
-                  }
-                  catch (Exception e) {
-                    // no-op
-                  }
-                }
-              }
-            }
-          }
-        }
-      }
-
-      @Override
-      public Schema getSchema() {
-        return Writer.this.getSchema();
-      }
-
-      @Override
-      public void insert(BytesWritable key, Tuple row) throws IOException {
-        if (sClosed) {
-          throw new IOException("Inserter already closed");
-        }
-
-        // break the input row into sub-tuples, then insert them into the
-        // corresponding CGs
-        int curTotal = 0;
-        try {
-          partition.insert(key, row);
-        }
-        catch (Exception e) {
-          throw new IOException("insert failed : " + e.getMessage());
-        }
-        for (int nx = 0; nx < colGroups.length; nx++) {
-          Tuple subTuple = cgTuples[nx];
-          int numCols = subTuple.size();
-          cgInserters[nx].insert(key, subTuple);
-          curTotal += numCols;
-        }
-      }
-
-      @Override
-      public void close() throws IOException {
-        if (sClosed) return;
-        sClosed = true;
-        try {
-          for (TableInserter ins : cgInserters) {
-            ins.close();
-          }
-          if (finishWriter) {
-            BasicTable.Writer.this.finish();
-          }
-        }
-        finally {
-          for (TableInserter ins : cgInserters) {
-            try {
-              ins.close();
-            }
-            catch (Exception e) {
-              // no-op
-            }
-          }
-          if (finishWriter) {
-            try {
-              BasicTable.Writer.this.finish();
-            }
-            catch (Exception e) {
-              // no-op
-            }
-          }
-        }
-      }
-    }
-  }
-
-  /**
-   * Drop a Basic Table, all files consisting of the BasicTable will be removed.
-   * 
-   * @param path
-   *          the path to the Basic Table.
-   * @param conf
-   *          The configuration object.
-   * @throws IOException
-   */
-  public static void drop(Path path, Configuration conf) throws IOException {
-    FileSystem fs = path.getFileSystem(conf);
-    fs.delete(path, true);
-  }
-
-  static class SchemaFile {
-    private Version version;
-    String comparator;
-    Schema logical;
-    Schema[] physical;
-    Partition partition;
-    boolean sorted;
-    SortInfo sortInfo = null;
-    String storage;
-    CGSchema[] cgschemas;
-    
-    // Array indicating if a physical schema is already dropped
-    // It is probably better to create "CGProperties" class and
-    // store multiple properties like name there.
-    boolean[] cgDeletedFlags;
-   
-    // ctor for reading
-    public SchemaFile(Path path, String[] deletedCGs, Configuration conf) throws IOException {
-      readSchemaFile(path, deletedCGs, conf);
-    }
-    
-    // ctor for reading from a job configuration object; we do not need a table path; 
-    // all information is held in the job configuration object.
-    public SchemaFile(Configuration conf) throws IOException {
-      String logicalStr = ZebraConf.getOutputSchema(conf);
-      storage = ZebraConf.getOutputStorageHint(conf);
-      String sortColumns = ZebraConf.getOutputSortColumns(conf) != null ? ZebraConf.getOutputSortColumns(conf) : "";
-      comparator = ZebraConf.getOutputComparator(conf) != null ? ZebraConf.getOutputComparator(conf) : "";      
-      
-      version = SCHEMA_VERSION;
-            
-      try {
-        logical = new Schema(logicalStr);
-      } catch (Exception e) {
-        throw new IOException("Schema build failed :" + e.getMessage());
-      }
-      
-      try {
-        partition = new Partition(logicalStr, storage, comparator, sortColumns);
-       
-      } catch (Exception e) {
-        throw new IOException("Partition constructor failed :" + e.getMessage());
-      }
- 
-      cgschemas = partition.getCGSchemas();
-      physical = new Schema[cgschemas.length];
-      //cgDeletedFlags = new boolean[physical.length];
-      
-      for (int nx = 0; nx < cgschemas.length; nx++) {
-        physical[nx] = cgschemas[nx].getSchema();
-      }
-      
-      this.sortInfo = partition.getSortInfo();
-      this.sorted = partition.isSorted();
-      this.comparator = (this.sortInfo == null ? null : this.sortInfo.getComparator());
-      if (this.comparator == null)
-        this.comparator = "";
-           
-      String[] sortColumnStr = sortColumns.split(",");
-      if (sortColumnStr.length > 0) {
-        sortInfo = SortInfo.parse(SortInfo.toSortString(sortColumnStr), logical, comparator);
-      }
-    }
-
-    public Schema[] getPhysicalSchema() {
-      return physical;
-    }
-
-    // ctor for writing
-    public SchemaFile(Path path, String btSchemaStr, String btStorageStr, String sortColumns,
-        String btComparator, Configuration conf)
-        throws IOException {
-      storage = btStorageStr;
-      try {
-        partition = new Partition(btSchemaStr, btStorageStr, btComparator, sortColumns);
-      }
-      catch (Exception e) {
-        throw new IOException("Partition constructor failed :" + e.getMessage());
-      }
-      this.sortInfo = partition.getSortInfo();
-      this.sorted = partition.isSorted();
-      this.comparator = (this.sortInfo == null ? null : this.sortInfo.getComparator());
-      if (this.comparator == null)
-        this.comparator = "";
-      logical = partition.getSchema();
-      cgschemas = partition.getCGSchemas();
-      physical = new Schema[cgschemas.length];
-      for (int nx = 0; nx < cgschemas.length; nx++) {
-        physical[nx] = cgschemas[nx].getSchema();
-      }
-      cgDeletedFlags = new boolean[physical.length];
-
-      version = SCHEMA_VERSION;
-
-      // write out the schema
-      createSchemaFile(path, conf);
-    }
-
-    public String getComparator() {
-      return comparator;
-    }
-
-    public Partition getPartition() {
-      return partition;
-    }
-
-    public boolean isSorted() {
-      return sorted;
-    }
-
-    public SortInfo getSortInfo() {
-      return sortInfo;
-    }
-
-    public Schema getLogical() {
-      return logical;
-    }
-
-    public int getNumOfPhysicalSchemas() {
-      return physical.length;
-    }
-
-    public Schema getPhysicalSchema(int nx) {
-      return physical[nx];
-    }
-    
-    public String getName(int nx) {
-      return cgschemas[nx].getName();
-    }
-    
-    public String getSerializer(int nx) {
-      return cgschemas[nx].getSerializer();
-    }
-
-    public String getCompressor(int nx) {
-      return cgschemas[nx].getCompressor();
-    }
-
-    /**
-     * Returns the index for CG with the given name. -1 indicates that there is
-     * no CG with the name.
-     */
-    int getCGByName(String cgName) {
-      for(int i=0; i<physical.length; i++) {
-        if (cgName.equals(getName(i))) {
-          return i;
-        }
-      }
-      return -1;
-    }
-    
-    /** Returns if the CG at the given index is delete */
-    boolean isCGDeleted(int idx) {
-      return cgDeletedFlags[idx];
-    }
-    
-    public String getOwner(int nx) {
-        return cgschemas[nx].getOwner();
-      }
-
-    public String getGroup(int nx) {
-        return cgschemas[nx].getGroup();
-    }
-
-    public short getPerm(int nx) {
-        return cgschemas[nx].getPerm();
-    }
-    
-    /**
-     * @return the string representation of the physical schema.
-     */
-    public String getBTSchemaString() {
-      return logical.toString();
-    }
-
-    /**
-     * @return the string representation of the storage hints
-     */
-    public String getStorageString() {
-      return storage;
-    }
-
-    private void createSchemaFile(Path path, Configuration conf)
-        throws IOException {
-      // TODO: overwrite existing schema file, or need a flag?
-      FSDataOutputStream outSchema =
-          path.getFileSystem(conf).create(makeSchemaFilePath(path), true);
-      version.write(outSchema);
-      WritableUtils.writeString(outSchema, comparator);
-      WritableUtils.writeString(outSchema, logical.toString());
-      WritableUtils.writeString(outSchema, storage);
-      WritableUtils.writeVInt(outSchema, physical.length);
-      for (int nx = 0; nx < physical.length; nx++) {
-        WritableUtils.writeString(outSchema, physical[nx].toString());
-      }
-      WritableUtils.writeVInt(outSchema, sorted ? 1 : 0);
-      WritableUtils.writeVInt(outSchema, sortInfo == null ? 0 : sortInfo.size());
-      if (sortInfo != null && sortInfo.size() > 0)
-      {
-        String[] sortedCols = sortInfo.getSortColumnNames();
-        for (int i = 0; i < sortInfo.size(); i++)
-        {
-          WritableUtils.writeString(outSchema, sortedCols[i]);
-        }
-      }
-      outSchema.close();
-    }
-
-    private void readSchemaFile(Path path, String[] deletedCGs, Configuration conf)
-        throws IOException {
-      Path pathSchema = makeSchemaFilePath(path);
-      if (!path.getFileSystem(conf).exists(pathSchema)) {
-        throw new IOException("BT Schema file doesn't exist: " + pathSchema);
-      }
-      // read schema file
-      FSDataInputStream in = path.getFileSystem(conf).open(pathSchema);
-      version = new Version(in);
-      // verify compatibility against SCHEMA_VERSION
-      if (!version.compatibleWith(SCHEMA_VERSION)) {
-        new IOException("Incompatible versions, expecting: " + SCHEMA_VERSION
-            + "; found in file: " + version);
-      }
-      comparator = WritableUtils.readString(in);
-      String logicalStr = WritableUtils.readString(in);
-      try {
-        logical = new Schema(logicalStr);
-      }
-      catch (Exception e) {
-        ;
-        throw new IOException("Schema build failed :" + e.getMessage());
-      }
-      storage = WritableUtils.readString(in);
-      try {
-        partition = new Partition(logicalStr, storage, comparator);
-      }
-      catch (Exception e) {
-        throw new IOException("Partition constructor failed :" + e.getMessage());
-      }
-      cgschemas = partition.getCGSchemas();
-      int numCGs = WritableUtils.readVInt(in);
-      physical = new Schema[numCGs];
-      cgDeletedFlags = new boolean[physical.length];
-      TableSchemaParser parser;
-      String cgschemastr;
-      
-      try {
-        for (int nx = 0; nx < numCGs; nx++) {
-          cgschemastr = WritableUtils.readString(in);
-          parser = new TableSchemaParser(new StringReader(cgschemastr));
-          physical[nx] = parser.RecordSchema(null);
-        }
-      }
-      catch (Exception e) {
-        throw new IOException("parser.RecordSchema failed :" + e.getMessage());
-      }
-      
-      sorted = WritableUtils.readVInt(in) == 1 ? true : false;
-      if (deletedCGs == null)
-        setCGDeletedFlags(path, conf);
-      else {
-        for (String deletedCG : deletedCGs)
-        {
-          for (int i = 0; i < cgschemas.length; i++)
-          {
-            if (cgschemas[i].getName().equals(deletedCG))
-              cgDeletedFlags[i] = true;
-          }
-        }
-      }
-      
-      if (version.compareTo(new Version((short)1, (short)0)) > 0)
-      {
-        int numSortColumns = WritableUtils.readVInt(in);
-        if (numSortColumns > 0)
-        {
-          String[] sortColumnStr = new String[numSortColumns];
-          for (int i = 0; i < numSortColumns; i++)
-          {
-            sortColumnStr[i] = WritableUtils.readString(in);
-          }
-          sortInfo = SortInfo.parse(SortInfo.toSortString(sortColumnStr), logical, comparator);
-        }
-      }
-      in.close();
-    }
-
-    private static int getNumCGs(Path path, Configuration conf) throws IOException {
-      Path pathSchema = makeSchemaFilePath(path);
-      if (!path.getFileSystem(conf).exists(pathSchema)) {
-        throw new IOException("BT Schema file doesn't exist: " + pathSchema);
-      }
-      // read schema file
-      FSDataInputStream in = path.getFileSystem(conf).open(pathSchema);
-      Version version = new Version(in);
-      // verify compatibility against SCHEMA_VERSION
-      if (!version.compatibleWith(SCHEMA_VERSION)) {
-        new IOException("Incompatible versions, expecting: " + SCHEMA_VERSION
-            + "; found in file: " + version);
-      }
-      
-      // read comparator
-      WritableUtils.readString(in);
-      // read logicalStr
-      WritableUtils.readString(in);
-      // read storage
-      WritableUtils.readString(in);
-      int numCGs = WritableUtils.readVInt(in);
-      in.close();
-
-      return numCGs;
-    }
-
-    private static Path makeSchemaFilePath(Path parent) {
-      return new Path(parent, BT_SCHEMA_FILE);
-    }
-    
-    /**
-     * Sets cgDeletedFlags array by checking presense of
-     * ".deleted-CGNAME" directory in the table top level
-     * directory. 
-     */
-    void setCGDeletedFlags(Path path, Configuration conf) throws IOException {
-      
-      Set<String> deletedCGs = new HashSet<String>(); 
-      
-      for (FileStatus file : path.getFileSystem(conf).listStatus(path)) {
-        if (!file.isDir()) {
-          String fname =  file.getPath().getName();
-          if (fname.startsWith(DELETED_CG_PREFIX)) {
-            deletedCGs.add(fname.substring(DELETED_CG_PREFIX.length()));
-          }
-        }
-      }
-      
-      for(int i=0; i<physical.length; i++) {
-        cgDeletedFlags[i] = deletedCGs.contains(getName(i));
-      }
-    }
-    
-    String getDeletedCGs() {
-      StringBuilder sb = new StringBuilder();
-      // comma separated
-      boolean first = true;
-      for (int i = 0; i < physical.length; i++) {
-        if (cgDeletedFlags[i])
-        {
-          if (first)
-            first = false;
-          else {
-            sb.append(DELETED_CG_SEPARATOR_PER_TABLE);
-          }
-          sb.append(getName(i));
-        }
-      }
-      return sb.toString();
-    }
-  }
-
-  static public void dumpInfo(String file, PrintStream out, Configuration conf)
-      throws IOException {
-    dumpInfo(file, out, conf, 0);
-  }
-
-  static public void dumpInfo(String file, PrintStream out, Configuration conf, int indent)
-      throws IOException {
-    IOutils.indent(out, indent);
-    out.println("Basic Table : " + file);
-    Path path = new Path(file);
-    try {
-      BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-      String schemaStr = reader.getBTSchemaString();
-      String storageStr = reader.getStorageString();
-      IOutils.indent(out, indent);
-      out.printf("Schema : %s\n", schemaStr);
-      IOutils.indent(out, indent);
-      out.printf("Storage Information : %s\n", storageStr);
-      SortInfo sortInfo = reader.getSortInfo();
-      if (sortInfo != null && sortInfo.size() > 0)
-      {
-        IOutils.indent(out, indent);
-        String[] sortedCols = sortInfo.getSortColumnNames();
-        out.println("Sorted Columns :");
-        for (int nx = 0; nx < sortedCols.length; nx++) {
-          if (nx > 0)
-            out.printf(" , ");
-          out.printf("%s", sortedCols[nx]);
-        }
-        out.printf("\n");
-      }
-      IOutils.indent(out, indent);
-      out.println("Column Groups within the Basic Table :");
-      for (int nx = 0; nx < reader.colGroups.length; nx++) {
-        IOutils.indent(out, indent);
-        out.printf("\nColumn Group [%d] :", nx);
-        if (reader.colGroups[nx] != null) {
-          ColumnGroup.dumpInfo(reader.colGroups[nx].path, out, conf, indent);
-        } else {
-          // print basic info for deleted column groups.
-          out.printf("\nColum Group : DELETED");
-          out.printf("\nName : %s", reader.schemaFile.getName(nx));
-          out.printf("\nSchema : %s\n", 
-                     reader.schemaFile.cgschemas[nx].getSchema().toString());
-        }
-      }
-    }
-    catch (Exception e) {
-      throw new IOException("BasicTable.Reader failed : " + e.getMessage());
-    }
-    finally {
-      // no-op
-    }
-  }
-
-  public static void main(String[] args) {
-    System.out.printf("BasicTable Dumper\n");
-    if (args.length == 0) {
-      System.out
-          .println("Usage: java ... org.apache.hadoop.zebra.io.BasicTable path [path ...]");
-      System.exit(0);
-    }
-    Configuration conf = new Configuration();
-    for (String file : args) {
-      try {
-        dumpInfo(file, System.out, conf);
-      }
-      catch (IOException e) {
-        e.printStackTrace(System.err);
-      }
-    }
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/io/BasicTableStatus.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/io/BasicTableStatus.java
deleted file mode 100644
index 4cbde03d7..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/io/BasicTableStatus.java
+++ /dev/null
@@ -1,110 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.zebra.tfile.Utils;
-
-/**
- * Status of a BasicTable. The status may be reported under some projection.
- * Projection is needed because the size of a BasicTable may be affected by the
- * projection.
- */
-public final class BasicTableStatus implements Writable {
-  long size;
-  long rows;
-  BytesWritable beginKey;
-  BytesWritable endKey;
-
-  /**
-   * Get the begin key. If the begin key is unknown, null will be returned.
-   * 
-   * @return begin key.
-   */
-  public BytesWritable getBeginKey() {
-    return beginKey;
-  }
-
-  /**
-   * Get the end key. If the end key is unknown, null will be returned.
-   * 
-   * @return end key.
-   */
-  public BytesWritable getEndKey() {
-    return endKey;
-  }
-
-  /**
-   * Get the size in bytes of the BasicTable.
-   * 
-   * @return Size (in bytes) of all files that make up the BasicTable.
-   */
-  public long getSize() {
-    return size;
-  }
-
-  /**
-   * Get the # of rows of the BasicTable. If the # of rows is unknown, -1 will
-   * be returned.
-   * 
-   * @return Number of rows of the BasicTable.
-   */
-  public long getRows() {
-    return rows;
-  }
-
-  /**
-   * @see Writable#readFields(DataInput)
-   */
-  @Override
-  public void readFields(DataInput in) throws IOException {
-    size = Utils.readVLong(in);
-    rows = Utils.readVLong(in);
-    if (rows == 0) {
-      beginKey = null;
-      endKey = null;
-    }
-    else {
-      if (beginKey == null) {
-        beginKey = new BytesWritable();
-      }
-      beginKey.readFields(in);
-      if (endKey == null) {
-        endKey = new BytesWritable();
-      }
-      endKey.readFields(in);
-    }
-  }
-
-  /**
-   * @see Writable#write(DataOutput)
-   */
-  @Override
-  public void write(DataOutput out) throws IOException {
-    Utils.writeVLong(out, size);
-    Utils.writeVLong(out, rows);
-    if (rows > 0) {
-      beginKey.write(out);
-      endKey.write(out);
-    }
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/io/BlockDistribution.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/io/BlockDistribution.java
deleted file mode 100644
index 9d7da4e00..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/io/BlockDistribution.java
+++ /dev/null
@@ -1,151 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.Comparator;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.Set;
-import java.util.Map.Entry;
-
-import org.apache.hadoop.fs.BlockLocation;
-
-/**
- * Class used to convey the information of how on-disk data that fall in a
- * specific split are distributed across hosts. This class is used by the
- * MapReduce layer to calculate intelligent splits.
- * 
- * @see BasicTable.Reader#getBlockDistribution(BasicTable.Reader.RangeSplit)
- * @see KeyDistribution#getBlockDistribution(BytesWritable)
- */
-public class BlockDistribution {
-  private long uniqueBytes;
-  private Map<String, Long> dataDistri; // map from host names to bytes.
-
-  public BlockDistribution() {
-    dataDistri = new HashMap<String, Long>();
-  }
-  
-  void add(long bytes, Map<String, Long> distri) {
-    this.uniqueBytes += bytes;
-    reduceDataDistri(dataDistri, distri);
-  }
-
-  static void reduceDataDistri(Map<String, Long> lv, Map<String, Long> rv) {
-    for (Iterator<Map.Entry<String, Long>> it = rv.entrySet().iterator(); it
-        .hasNext();) {
-      Map.Entry<String, Long> e = it.next();
-      String key = e.getKey();
-      Long sum = lv.get(key);
-      Long delta = e.getValue();
-      lv.put(key, (sum == null) ? delta : sum + delta);
-    }
-  }
-  
-  void add(BlockLocation blkLocation) throws IOException {
-    long blkLen = blkLocation.getLength();
-    Map<String, Long> tmp = new HashMap<String, Long>();
-    for (String host : blkLocation.getHosts()) {
-      tmp.put(host, blkLen);
-    }
-    add(blkLen, tmp);
-  }
-
-  /**
-   * Add a partial block.
-   * 
-   * @param blkLocation
-   * @param length
-   * @throws IOException
-   */
-  void add(BlockLocation blkLocation, long length) throws IOException {
-    Map<String, Long> tmp = new HashMap<String, Long>();
-    for (String host : blkLocation.getHosts()) {
-      tmp.put(host, length);
-    }
-    add(length, tmp);
-  }
-
-  /**
-   * Add another block distribution to this one.
-   * 
-   * @param other
-   *          The other block distribution.
-   */
-  public void add(BlockDistribution other) {
-    add(other.uniqueBytes, other.dataDistri);
-  }
-
-  /**
-   * Sum up two block distributions together.
-   * 
-   * @param a
-   *          first block distribution
-   * @param b
-   *          second block distribution
-   * @return aggregated block distribution. The input objects may no longer be
-   *         held.
-   */
-  public static BlockDistribution sum(BlockDistribution a, BlockDistribution b) {
-    if (a == null) return b;
-    if (b == null) return a;
-    a.add(b);
-    return a;
-  }
-  
-  /**
-   * Get the total number of bytes of all the blocks.
-   * 
-   * @return total number of bytes for the blocks.
-   */
-  public long getLength() {
-    return uniqueBytes;
-  }
-
-  /**
-   * Get up to n hosts that own the most bytes.
-   * 
-   * @param n
-   *          targeted number of hosts.
-   * @return A list of host names (up to n).
-   */
-  @SuppressWarnings("unchecked")
-  public String[] getHosts(int n) {
-    Set<Map.Entry<String, Long>> entrySet = dataDistri.entrySet();
-    Map.Entry<String, Long>[] hostSize =
-        entrySet.toArray(new Map.Entry[entrySet.size()]);
-    Arrays.sort(hostSize, new Comparator<Map.Entry<String, Long>>() {
-
-      @Override
-      public int compare(Entry<String, Long> o1, Entry<String, Long> o2) {
-        long diff = o1.getValue() - o2.getValue();
-        if (diff < 0) return 1;
-        if (diff > 0) return -1;
-        return 0;
-      }
-    });
-    int nHost = Math.min(hostSize.length, n);
-    String[] ret = new String[nHost];
-    for (int i = 0; i < nHost; ++i) {
-      ret[i] = hostSize[i].getKey();
-    }
-    return ret;
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/io/ColumnGroup.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/io/ColumnGroup.java
deleted file mode 100644
index 3272528ee..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/io/ColumnGroup.java
+++ /dev/null
@@ -1,2541 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-package org.apache.hadoop.zebra.io;
-
-import java.io.Closeable;
-import java.io.DataInput;
-import java.io.DataInputStream;
-import java.io.DataOutput;
-import java.io.DataOutputStream;
-import java.io.EOFException;
-import java.io.FileNotFoundException;
-import java.io.IOException;
-import java.io.PrintStream;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.Iterator;
-import java.util.LinkedHashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Random;
-import java.util.Set;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.permission.*;
-import org.apache.hadoop.fs.BlockLocation;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.PathFilter;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.zebra.tfile.TFile;
-import org.apache.hadoop.zebra.tfile.Utils;
-import org.apache.hadoop.zebra.tfile.ByteArray;
-import org.apache.hadoop.zebra.tfile.RawComparable;
-import org.apache.hadoop.zebra.types.CGSchema;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Partition;
-import org.apache.hadoop.zebra.types.Projection;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.types.TypesUtils.TupleReader;
-import org.apache.hadoop.zebra.types.TypesUtils.TupleWriter;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.Tuple;
-
-/**
- * ColumnGroup is the basic unit of a persistent table. The following
- * Configuration parameters can customize the behavior of ColumnGroup.
- * <ul>
- * <li><b>table.output.tfile.minBlock.size</b> (int) Minimum compression block
- * size for underlying TFile (default to 1024*1024).
- * <li><b>table.output.tfile.compression</b> (String) Compression method (one
- * of "none", "lzo", "gz") (default to "lzo").
- * 
- * @see {@link TFile#getSupportedCompressionAlgorithms()}
- *      <li><b>table.input.split.minSize</b> (int) Minimum split size (default
- *      to 64*1024).
- *      </ul>
- */
-class ColumnGroup {
-  static Log LOG = LogFactory.getLog(ColumnGroup.class);
-  
-  private final static String CONF_COMPRESS = "table.output.tfile.compression";
-  private final static String DEFAULT_COMPRESS = "gz";
-  private final static String CONF_MIN_BLOCK_SIZE = "table.tfile.minblock.size";
-  private final static int DEFAULT_MIN_BLOCK_SIZE = 1024 * 1024;
-
-  private final static String CONF_MIN_SPLIT_SIZE = "table.input.split.minSize";
-  private final static int DEFAULT_MIN_SPLIT_SIZE = 64 * 1024;
-
-  static final double SPLIT_SLOP = 1.1; // 10% slop
-
-  // excluding files start with the following prefix, may change to regex
-  private final static String CONF_NON_DATAFILE_PREFIX =
-      "table.cg.nondatafile.prefix";
-  private final static String SPECIAL_FILE_PREFIX = ".";
-
-  // tmp schema file name, used as a flag of unfinished CG
-  private final static String SCHEMA_FILE = ".schema";
-  // meta data TFile for entire CG, used as a flag of closed CG
-  final static String META_FILE = ".meta";
-
-  // sorted table key ranges for default sorted table split generations
-  private final static String KEY_RANGE_FOR_DEFAULT_SORTED_SPLIT = ".keyrange";
-
-  static final String BLOCK_NAME_INDEX = "ColumnGroup.index";
-
-  static Path makeMetaFilePath(Path parent) {
-    return new Path(parent, META_FILE);
-  }
-
-  static String getCompression(Configuration conf) {
-    return conf.get(CONF_COMPRESS, DEFAULT_COMPRESS);
-  }
-
-  static int getMinBlockSize(Configuration conf) {
-    return conf.getInt(CONF_MIN_BLOCK_SIZE, DEFAULT_MIN_BLOCK_SIZE);
-  }
-
-  static String getNonDataFilePrefix(Configuration conf) {
-    return conf.get(CONF_NON_DATAFILE_PREFIX, SPECIAL_FILE_PREFIX);
-  }
-
-  static int getMinSplitSize(Configuration conf) {
-    return conf.getInt(CONF_MIN_SPLIT_SIZE, DEFAULT_MIN_SPLIT_SIZE);
-  }
-
-  /**
-   * Drop a Column Group, maps to deleting all the files relating to this Column
-   * Group on the FileSystem.
-   * 
-   * @param path
-   *          the path to the ColumnGroup.
-   * @param conf
-   *          The configuration object.
-   */
-  public static void drop(Path path, Configuration conf) throws IOException {
-    FileSystem fs = path.getFileSystem(conf);
-    fs.delete(path, true);
-    // TODO:
-    // fs.close();
-  }
-
-  /**
-   * Scan the file system, looking for TFiles, and build an in-memory index of a
-   * column group.
-   * 
-   * @param fs
-   *          The file system
-   * @param path
-   *          The base path of the column group.
-   * @param dirty
-   *          Whether to build dirty index or not. Dirty index is built by only
-   *          looking at file-level status and not opening up individual TFiles.
-   *          The flag may only be set for unsorted ColumnGroups.
-   * @param conf
-   *          The configuration object.
-   * @return The in-memory index object.
-   * @throws IOException
-   */
-  static CGIndex buildIndex(FileSystem fs, Path path, boolean dirty,
-      Configuration conf) throws IOException {
-    CGIndex ret = new CGIndex();
-    CGPathFilter cgPathFilter = new CGPathFilter();
-    CGPathFilter.setConf(conf);
-    FileStatus[] files = fs.listStatus(path, cgPathFilter);
-    
-    Comparator<RawComparable> comparator = null;
-    for (FileStatus f : files) {
-      if (dirty) {
-        ret.add(f.getLen(), f.getPath().getName());
-      }
-      else {
-        FSDataInputStream dis = null;
-        TFile.Reader tr = null;
-        try {
-          dis = fs.open(f.getPath());
-          tr = new TFile.Reader(dis, f.getLen(), conf);
-          if (comparator == null) {
-            comparator = tr.getComparator();
-          }
-          if (tr.getEntryCount() > 0) {
-            CGIndexEntry range =
-                new CGIndexEntry(f.getPath().getName(), tr.getEntryCount(), tr
-                    .getFirstKey(), tr.getLastKey());
-            ret.add(f.getLen(), tr.getEntryCount(), range);
-          }
-        }
-        catch (IOException e) {
-          // TODO: log the error, ignore incorrect TFiles.
-          e.printStackTrace(System.err);
-        }
-        finally {
-          if (tr != null) {
-            tr.close();
-          }
-          if (dis != null) {
-            dis.close();
-          }
-        }
-      }
-    }
-
-    ret.sort(comparator);
-    
-    int idx = 0;
-    for (CGIndexEntry e : ret.getIndex()) {
-      e.setIndex(idx++);    
-    }
-    
-    return ret;
-  }   
-  
-  /**
-   * ColumnGroup reader.
-   */
-  public static class Reader implements Closeable {
-    Path path;
-    Configuration conf;
-    FileSystem fs;
-    CGSchema cgschema;
-    Comparator<RawComparable> comparator;
-    Projection projection;
-    CGIndex cgindex;
-    ArrayList<SplitColumn> exec;
-    SplitColumn top; // directly associated with logical schema
-    SplitColumn leaf; // corresponding to projection
-    boolean closed;
-    boolean dirty;
-
-    /**
-     * Get the Column Group physical schema without loading the full CG index.
-     * 
-     * @param path
-     *          The path to the ColumnGroup.
-     * @param conf
-     *          The configuration object.
-     * @return The ColumnGroup schema.
-     * @throws IOException
-     */
-
-    public static Schema getSchema(Path path, Configuration conf)
-        throws IOException, ParseException {
-      FileSystem fs = path.getFileSystem(conf);
-      CGSchema cgschema = CGSchema.load(fs, path);
-      return cgschema.getSchema();
-    }
-
-    /**
-     * Create a ColumnGroup reader.
-     * 
-     * @param path
-     *          The directory path to the column group.
-     * @param conf
-     *          Optional configuration parameters.
-     * @throws IOException
-     */
-    public Reader(Path path, Configuration conf) throws IOException,
-      ParseException {
-      this(path, conf, false);
-    }
-    
-    public Reader(Path path, Configuration conf, boolean mapper) throws IOException,
-      ParseException {
-      this(path, true, conf, mapper);
-    }
-    
-    Reader(Path path, boolean dirty, Configuration conf) throws IOException,
-      ParseException {
-      this(path, dirty, conf, false);
-    }
-
-    Reader(Path path, boolean dirty, Configuration conf, boolean mapper) throws IOException,
-      ParseException {
-      this.path = path;
-      this.conf = conf;
-      this.dirty = dirty;
-
-      fs = path.getFileSystem(conf);
-      // check existence of path
-      if (!fs.exists(path)) {
-        throw new IOException("Path doesn't exist: " + path);
-      }
-
-      if (!mapper && !fs.getFileStatus(path).isDir()) {
-        throw new IOException("Path exists but not a directory: " + path);
-      }
-
-      cgschema = CGSchema.load(fs, path);
-      if (cgschema.isSorted()) {
-        comparator = TFile.makeComparator(cgschema.getComparator());
-      }
-      projection = new Projection(cgschema.getSchema()); // default projection to CG schema.
-      Path metaFilePath = makeMetaFilePath(path);
-      /* If index file is not existing */
-      if (!fs.exists(metaFilePath)) {
-        throw new FileNotFoundException(
-              "Missing Meta File of " + metaFilePath);
-      }
-      else if (cgschema.isSorted()) {
-        MetaFile.Reader metaFile = MetaFile.createReader(metaFilePath, conf);
-        try {
-          cgindex = new CGIndex();
-          DataInputStream dis = metaFile.getMetaBlock(BLOCK_NAME_INDEX);
-          try {
-            cgindex.readFields(dis);
-          } catch (IOException e) {
-            throw new IOException("Index file read failure :"+ e.getMessage());
-          } finally {
-            dis.close();
-          }
-        }
-        finally {
-          metaFile.close();
-        }
-      }
-    }
-
-    /**
-     * Set the projection for the reader. This will affect calls to
-     * getScanner(), getStatus(), and getColumnNames().
-     * 
-     * @param projection
-     *          The projection on the column group for subsequent read
-     *          operations. If we want select all columns, pass
-     *          projection==null.
-     */
-    public synchronized void setProjection(String projection) throws ParseException {
-      if (projection == null) {
-        this.projection = new Projection(cgschema.getSchema());
-      }
-      else {
-        this.projection = new Projection(cgschema.getSchema(), projection);
-      }
-    }
-
-    /**
-     * Get the schema of columns of the table (possibly through projection).
-     * 
-     * @return Schema of the columns of the table (possibly through projection).
-     */
-    public Schema getSchema() throws ParseException {
-      return projection.getSchema();
-    }
-    
-    /**
-     * Get the projection
-     * @return Projection of this Reader
-     */
-    public Projection getProjection() {
-      return projection;
-    }
-
-    public String getName() {
-      return cgschema.getName();
-    }
-    
-    public String getSerializer() {
-      return cgschema.getSerializer();
-    }
-
-    public String getCompressor() {
-      return cgschema.getCompressor();
-    }
-
-    public CGSchema getCGSchema() {
-      return cgschema;
-    }
-
-    public String getGroup() {
-        return cgschema.getGroup();
-      }
-
-    public short getPerm() {
-        return cgschema.getPerm();
-    }
-
-    /**
-     * Get a scanner that reads all rows whose row keys fall in a specific
-     * range.
-     * 
-     * @param beginKey
-     *          The begin key of the scan range.
-     * @param endKey
-     *          The end key of the scan range.
-     * @param closeReader
-     *          close the underlying Reader object when we close the scanner.
-     *          Should be set to true if we have only one scanner on top of the
-     *          reader, so that we should release resources after the scanner is
-     *          closed.
-     * @return A scanner object.
-     * @throws IOException
-     */
-    public synchronized CGScanner getScanner(BytesWritable beginKey,
-        BytesWritable endKey, boolean closeReader) throws IOException,
-        ParseException {
-      if (closed) {
-        throw new EOFException("Reader already closed");
-      }
-      if (!isSorted()) {
-        throw new IOException(
-            "Cannot get key-bounded scanner for unsorted table");
-      }
-      RawComparable begin =
-          (beginKey != null) ? new ByteArray(beginKey.getBytes(), 0, beginKey
-              .getLength()) : null;
-      RawComparable end =
-          (endKey != null) ? new ByteArray(endKey.getBytes(), 0, endKey.getLength())
-              : null;
-      if (begin != null && end != null) {
-        if (comparator.compare(begin, end) >= 0) {
-          throw new IOException("Zero-key-range split");
-        }
-      }
-
-      return new CGScanner(begin, end, closeReader);
-    }
-
-    /**
-     * Get a scanner that reads a consecutive number of rows as defined in the
-     * CGRangeSplit object, which should be obtained from previous calls of
-     * rangeSplit().
-     * 
-     * @param split
-     *          The split range. If null, get a scanner to read the complete
-     *          column group.
-     * @param closeReader
-     *          close the underlying Reader object when we close the scanner.
-     *          Should be set to true if we have only one scanner on top of the
-     *          reader, so that we should release resources after the scanner is
-     *          closed.
-     * @return A scanner object.
-     * @throws IOException
-     */
-    public synchronized CGScanner getScanner(CGRangeSplit split,
-        boolean closeReader) throws IOException, ParseException {
-      if (closed) {
-        throw new EOFException("Reader already closed");
-      }
-
-      if (split == null) {
-        if (cgindex == null)
-          cgindex = buildIndex(fs, path, dirty, conf);
-        return getScanner(new CGRangeSplit(0, cgindex.size()), closeReader);
-      }
-      if (split.len < 0) {
-        throw new IllegalArgumentException("Illegal range split");
-      }
-
-      return new CGScanner(split, closeReader);
-    }
-
-    /**
-     * Get a scanner that reads the rows defined by rowRange. 
-     * 
-     * @param closeReader
-     *          close the underlying Reader object when we close the scanner.
-     *          Should be set to true if we have only one scanner on top of the
-     *          reader, so that we should release resources after the scanner is
-     *          closed.
-     * @param rowSplit specifies part index, start row, and end row.
-     * @return A scanner object.
-     */
-    public synchronized CGScanner getScanner(boolean closeReader, 
-                                                CGRowSplit rowSplit)
-                        throws IOException, ParseException {
-      if (closed) {
-        throw new EOFException("Reader already closed");
-      }
-      
-      return new CGScanner(rowSplit, closeReader);
-    }
-     
-    /**
-     * Given a split range, calculate how the file data that fall into the range
-     * are distributed among hosts.
-     * 
-     * @param split
-     *          The range-based split. If null, return all blocks.
-     * @return a map from host name to the amount of data (in bytes) the host
-     *         owns that fall roughly into the key range.
-     */
-    public BlockDistribution getBlockDistribution(CGRangeSplit split)
-        throws IOException {
-      if (split == null) {
-        return getBlockDistribution(new CGRangeSplit(0, cgindex.size()));
-      }
-
-      if (cgindex == null)
-        cgindex = buildIndex(fs, path, dirty, conf);
-      if ((split.start | split.len | (cgindex.size() - split.start - split.len)) < 0) {
-        throw new IndexOutOfBoundsException("Bad split");
-      }
-
-      BlockDistribution ret = new BlockDistribution();
-      for (int i = split.start; i < split.start + split.len; ++i) {
-        CGIndexEntry dfkr = cgindex.get(i);
-        Path tfilePath = new Path(path, dfkr.getName());
-        FileStatus tfileStatus = fs.getFileStatus(tfilePath);
-        BlockLocation[] locations =
-            fs.getFileBlockLocations(tfileStatus, 0, tfileStatus.getLen());
-        for (BlockLocation l : locations) {
-          ret.add(l);
-        }
-      }
-
-      return ret;
-    }
-    
-    /**
-     * Given a row range, calculate how the file data that fall into the range
-     * are distributed among hosts.
-     * 
-     * @param split
-     *          The row-based split. If null, return all blocks.
-     * @return a map from host name to the amount of data (in bytes) the host
-     *         owns that fall roughly into the key range.
-     */
-    public BlockDistribution getBlockDistribution(CGRowSplit split)
-        throws IOException {
-      if (split == null) {
-        throw new IOException("Row-based split cannot be null for getBlockDistribution()");
-      }
-
-      BlockDistribution ret = new BlockDistribution();
-      for (int i = 0; i < split.length; i++)
-      {
-        FileStatus tfileStatus = fs.getFileStatus(new Path(path, split.names[i]));
-        
-        BlockLocation[] locations = null;
-        long len = 0;
-        if (i == 0) {
-          if (split.startByteFirst != -1)
-          {
-            len = split.numBytesFirst;
-            locations = fs.getFileBlockLocations(tfileStatus, split.startByteFirst, len);
-          }
-        } else if (i == split.length - 1) {
-           if (split.numBytesLast != -1)
-           {
-             len = split.numBytesLast;
-             locations = fs.getFileBlockLocations(tfileStatus, 0, len);
-           }
-        }
-
-        if (locations == null)
-        {
-          len = tfileStatus.getLen();
-          locations = fs.getFileBlockLocations(tfileStatus, 0, len);
-        }
-
-        for (BlockLocation l : locations) {
-          ret.add(l);
-        }
-      }
-      return ret;
-    }
-
-  private int getStartBlockIndex(long[] startOffsets, long offset)
-  {
-    int index = Arrays.binarySearch(startOffsets, offset);
-    if (index < 0)
-      index = -index - 2;
-    return index;
-  }
-  
-  private int getEndBlockIndex(long[] startOffsets, long offset)
-  {
-    int index = Arrays.binarySearch(startOffsets, offset);
-    if (index < 0)
-      index = -index - 1;
-    return index;
-  }
-
-   /**
-    * Sets startRow and number of rows in rowSplit based on
-    * startOffset and length.
-    * 
-    * It is assumed that 'startByte' and 'numBytes' in rowSplit itself
-    * are not valid.
-    */
-    void fillRowSplit(CGRowSplit rowSplit, CGRowSplit src) throws IOException {
-
-      if (src.names == null || src.length == 0)
-        return;
-
-      boolean noSizeInIndex = false;
-      long[] sizes = rowSplit.sizes;
-      if (sizes == null)
-      {
-        /* the on disk table is sorted. Later this will be made unnecessary when
-         * CGIndexEntry serializes its bytes field and the meta file versioning is
-         * supported.
-         */ 
-        noSizeInIndex = true;
-      }
-      rowSplit.names = src.names;
-      rowSplit.length = src.length;
-      rowSplit.startByteFirst = src.startByteFirst;
-      rowSplit.numBytesFirst = src.numBytesFirst;
-      rowSplit.numBytesLast = src.numBytesLast;
-
-      Path firstPath = null, lastPath;
-      TFile.Reader reader = null;
-      
-      if (src.startByteFirst != -1)
-      {
-        firstPath = new Path(path, rowSplit.names[0]);
-        long size;
-        if (noSizeInIndex)
-        {
-          FileStatus tfile = fs.getFileStatus(firstPath);
-          size = tfile.getLen();
-        } else
-          size = sizes[0];
-        reader = new TFile.Reader(fs.open(firstPath), size, conf);
-        try {
-          long startRow = reader.getRecordNumNear(src.startByteFirst);
-          long endRow = reader.getRecordNumNear(src.startByteFirst + src.numBytesFirst);
-
-          if (endRow < startRow)
-            endRow = startRow;
-          rowSplit.startRowFirst = startRow;
-          rowSplit.numRowsFirst = endRow - startRow;
-        } catch (IOException e) {
-          reader.close();
-          throw e;
-        }
-      }
-      if (src.numBytesLast != -1 && rowSplit.length > 1)
-      {
-        lastPath = new Path(path, rowSplit.names[rowSplit.length - 1]);
-        if (reader == null || !firstPath.equals(lastPath))
-        {
-          if (reader != null)
-            reader.close();
-          long size;
-          if (noSizeInIndex)
-          {
-            FileStatus tfile = fs.getFileStatus(lastPath);
-            size = tfile.getLen();
-          } else
-            size = sizes[rowSplit.length - 1];
-          reader = new TFile.Reader(fs.open(lastPath), size, conf);
-        }
-        try {
-          long endRow = reader.getRecordNumNear(src.numBytesLast);
-          rowSplit.numRowsLast = endRow;
-        } catch (IOException e) {
-          reader.close();
-          throw e;
-        }
-      }
-      if (reader != null)
-        reader.close();
-    }
-    
-    /**
-     * Get a sampling of keys and calculate how data are distributed among
-     * key-partitioned buckets. The implementation attempts to calculate all
-     * information in one shot to avoid reading TFile index multiple times.
-     * Special care is also taken that memory requirement is not linear to the
-     * size of total data set for the column group.
-     * 
-     * @param n
-     *          Targeted size of the sampling.
-     * @param nTables
-     *          Number of tables in a union
-     * @return KeyDistribution object.
-     * @throws IOException
-     */
-    public KeyDistribution getKeyDistribution(int n, int nTables, BlockDistribution lastBd) throws IOException {
-      // TODO: any need for similar capability for unsorted for sorted CGs?
-      if (!isSorted()) {
-        throw new IOException("Cannot get key distribution for unsorted table");
-      }
-      KeyDistribution ret = new KeyDistribution(comparator);
-
-      if (n < 0)
-      {
-        /*
-        Path keyRangeFile = new Path(path, KEY_RANGE_FOR_DEFAULT_SORTED_SPLIT);
-        if (fs.exists(keyRangeFile))
-        {
-          try {
-            FSDataInputStream ins = fs.open(keyRangeFile);
-            long minStepSize = ins.readLong();
-            int size = ins.readInt();
-            for (int i = 0; i < size; i++)
-            {
-              BytesWritable keyIn = new BytesWritable();
-              keyIn.readFields(ins);
-              ByteArray key = new ByteArray(keyIn.getBytes());
-              ret.add(key);
-            }
-            ret.setMinStepSize(minStepSize);
-            return ret;
-          } catch (Exception e) {
-            // no-op
-          }
-        }
-        */
-        n = 1;
-      }
-
-      Path[] paths = new Path[cgindex.size()];
-      FileStatus[] tfileStatus = new FileStatus[paths.length];
-      long totalBytes = 0;
-      for (int i = 0; i < paths.length; ++i) {
-        paths[i] = cgindex.getPath(i, path);
-        tfileStatus[i] = fs.getFileStatus(paths[i]);
-        totalBytes += tfileStatus[i].getLen();
-      }
-
-      final long minSize = getMinSplitSize(conf);
-      final long EPSILON = (long) (minSize * (SPLIT_SLOP - 1));
-      long goalSize = totalBytes / n;
-      long batchSize = 0;
-      BlockDistribution bd = new BlockDistribution();;
-      RawComparable prevKey = null;
-
-      long minStepSize = -1;
-      FSDataInputStream nextFsdis = null;
-      TFile.Reader nextReader = null;
-      for (int i = 0; i < paths.length; ++i) {
-        FileStatus fstatus = tfileStatus[i];
-        long blkSize = fstatus.getBlockSize();
-        long fileLen = fstatus.getLen();
-        long stepSize = Math.max(minSize,
-            (goalSize < blkSize) ? goalSize : blkSize);
-        if (minStepSize== -1 || minStepSize > stepSize)
-          minStepSize = stepSize;
-        // adjust the block size by the scaling factor
-        blkSize /= nTables;
-        stepSize = Math.max(minSize,
-          (goalSize < blkSize) ? goalSize : blkSize);
-        FSDataInputStream fsdis = null;
-        TFile.Reader reader = null;
-        long remainLen = fileLen;
-        try {
-          if (nextReader == null)
-          {
-            fsdis = fs.open(paths[i]);
-            reader = new TFile.Reader(fsdis, fileLen, conf);
-          } else {
-            fsdis = nextFsdis;
-            reader = nextReader;
-          }
-          BlockLocation[] locations =
-              fs.getFileBlockLocations(fstatus, 0, fileLen);
-          if (locations.length == 0) {
-            throw new AssertionError(
-                "getFileBlockLocations returns 0 location");
-          }
-
-          Arrays.sort(locations, new Comparator<BlockLocation>() {
-            @Override
-            public int compare(BlockLocation o1, BlockLocation o2) {
-              long diff = o1.getOffset() - o2.getOffset();
-              if (diff < 0) return -1;
-              if (diff > 0) return 1;
-              return 0;
-            }
-          });
-          
-          long[] startOffsets = new long[locations.length];
-
-          for (int ii = 0; ii < locations.length; ii++)
-            startOffsets[ii] = locations[ii].getOffset();
-
-          boolean done = false;
-          while ((remainLen > 0) && !done) {
-            long splitBytes =
-                remainLen > stepSize ? stepSize : remainLen;
-            long offsetBegin = fileLen - remainLen;
-            long offsetEnd = offsetBegin + splitBytes;
-            int indexBegin = getStartBlockIndex(startOffsets, offsetBegin);
-            int indexEnd = getEndBlockIndex(startOffsets, offsetEnd);
-            BlockLocation firstBlock = locations[indexBegin];
-            BlockLocation lastBlock = locations[indexEnd-1];
-            long lastBlockOffsetBegin = lastBlock.getOffset();
-            long lastBlockOffsetEnd =
-                lastBlockOffsetBegin + lastBlock.getLength();
-            if ((firstBlock.getOffset() > offsetBegin)
-                || (lastBlockOffsetEnd < offsetEnd)) {
-              throw new AssertionError(
-                  "Block locations returned by getFileBlockLocations do not cover requested range");
-            }
-
-            // Adjust offsets
-            if ((offsetEnd > lastBlockOffsetBegin)
-                && (offsetEnd - lastBlockOffsetBegin < EPSILON)) {
-              // the split includes a bit of the next block, remove it.
-              if (offsetEnd != fileLen)
-              {
-            	// only if this is not the last chunk
-                offsetEnd = lastBlockOffsetBegin;
-                splitBytes = offsetEnd - offsetBegin;
-                indexEnd--;
-              }
-            }
-            else if ((lastBlockOffsetEnd > offsetEnd)
-                && (lastBlockOffsetEnd - offsetEnd < EPSILON)) {
-              // the split includes almost the whole block, fill it.
-              offsetEnd = lastBlockOffsetEnd;
-              splitBytes = offsetEnd - offsetBegin;
-            }
-
-            RawComparable key = reader.getKeyNear(offsetEnd);
-            if (key == null) {
-              offsetEnd = fileLen;
-              splitBytes = offsetEnd - offsetBegin;
-              if (i < paths.length-1)
-              {
-                nextFsdis = fs.open(paths[i+1]);
-                nextReader = new TFile.Reader(nextFsdis, tfileStatus[i+1].getLen(), conf);
-                key = nextReader.getFirstKey();
-              }
-              done = true; // TFile index too large? Is it necessary now?
-            }
-            remainLen -= splitBytes;
-            batchSize += splitBytes;
-
-            if (key != null && batchSize >= stepSize)
-            {
-              if (batchSize - splitBytes < EPSILON || splitBytes < EPSILON)
-              {
-                // the last chunk or this chunk is small enough to create a new range for this key
-                setBlockDistribution(bd, reader, locations, fstatus, startOffsets, prevKey, key);
-                ret.add(key, bd);
-                batchSize = 0;
-                bd = new BlockDistribution();
-              } else {
-                ret.add(prevKey, bd);
-                batchSize = splitBytes;
-                bd = new BlockDistribution();
-                if (batchSize >= stepSize)
-                {
-                  setBlockDistribution(bd, reader, locations, fstatus, startOffsets, prevKey, key);
-                  ret.add(key, bd);
-                  batchSize = 0;
-                  bd = new BlockDistribution();
-                } else {
-                  setBlockDistribution(bd, reader, locations, fstatus, startOffsets, prevKey, key);
-                }
-              }
-            } else {
-              setBlockDistribution(bd, reader, locations, fstatus, startOffsets, prevKey, key);
-            }
-            prevKey = key;
-          }
-        }
-        finally {
-          if (reader != null) {
-            try {
-              reader.close();
-            }
-            catch (Exception e) {
-              // no-op;
-            }
-          }
-          if (fsdis != null) {
-            try {
-              fsdis.close();
-            }
-            catch (Exception e) {
-              // no-op
-            }
-          }
-        }
-      }
-      if (lastBd != null)
-        lastBd.add(bd);
-      ret.setMinStepSize(minStepSize);
-      
-      return ret;
-    }
-
-    private void setBlockDistribution(BlockDistribution bd, TFile.Reader reader,
-        BlockLocation[] locations, FileStatus fileStatus, long[] startOffsets,
-        RawComparable begin, RawComparable end) throws IOException
-    {
-      long beginOffset, endOffset = -1;
-      if (begin == null)
-        beginOffset = 0;
-      else
-        beginOffset = reader.getOffsetForKey(begin);
-      if (end != null)
-      {
-        if (begin == null)
-          begin = reader.getFirstKey();
-        /* Only if the key range is empty. This is needed because TFile has a 16-byte
-         * Magic that causes getOffsetForKey to return 16 (not 0) even on the first key.
-         */
-        if (comparator.compare(begin, end) != 0)
-          endOffset = reader.getOffsetForKey(end);
-      }
-      int startBlockIndex = (beginOffset == 0 ? 0 : getStartBlockIndex(startOffsets, beginOffset));
-      BlockLocation l;
-      int endBlockIndex = (end == null ? locations.length : endOffset == -1 ?
-          startBlockIndex : getEndBlockIndex(startOffsets, endOffset));
-      for (int ii = startBlockIndex; ii < endBlockIndex; ii++) {
-        l = locations[ii];
-        long blkBeginOffset = l.getOffset();
-        long blkEndOffset = blkBeginOffset + l.getLength();
-        if (blkEndOffset > blkBeginOffset) {
-          bd.add(l, blkEndOffset - blkBeginOffset);
-        }
-      }
-      return;
-    }
-
-    /**
-     * Get the status of the ColumnGroup.
-     */
-    public BasicTableStatus getStatus() throws IOException {
-      if (cgindex == null)
-        cgindex = buildIndex(fs, path, dirty, conf);
-      return cgindex.status;
-    }
-
-    /**
-     * Split the ColumnGroup by file orders.
-     * 
-     * @param n
-     *          Targeted number of partitions.
-     * @return A list of range-based splits, whose size may be less than or
-     *         equal to n.
-     */
-    public List<CGRangeSplit> rangeSplit(int n) throws IOException {
-      // The output of this method must be only dependent on the cgindex and
-      // input parameter n - so that horizontally stitched column groups will
-      // get aligned splits.
-      if (cgindex == null)
-        cgindex = buildIndex(fs, path, dirty, conf);
-      int numFiles = cgindex.size();
-      if ((numFiles < n) || (n < 0)) {
-        return rangeSplit(numFiles);
-      }
-      List<CGRangeSplit> lst = new ArrayList<CGRangeSplit>();
-      int beginIndex = 0;
-      for (int i = 0; i < n; ++i) {
-        int endIndex = (int) ((long) (i + 1) * numFiles / n);
-        lst.add(new CGRangeSplit(beginIndex, endIndex - beginIndex));
-        beginIndex = endIndex;
-      }
-      return lst;
-    }
-
-    /**
-     * We already use FileInputFormat to create byte offset-based input splits.
-     * Their information is encoded in starts, lengths and paths. This method is 
-     * to wrap this information to form CGRowSplit objects at column group level.
-     * 
-     * @param starts array of starting byte of fileSplits.
-     * @param lengths array of length of fileSplits.
-     * @param paths array of path of fileSplits.
-     * @return A list of CGRowSplit objects. 
-     *         
-     */
-    public List<CGRowSplit> rowSplit(long[] starts, long[] lengths, Path[] paths,
-        int[] batches, int numBatches) throws IOException {
-      List<CGRowSplit> lst = new ArrayList<CGRowSplit>();
-      CGRowSplit cgRowSplit;
-      long startFirst, bytesFirst, bytesLast;
-      int length;
-       
-      if (numBatches == 0)
-      {
-        cgRowSplit = new CGRowSplit(null, null, 0, -1, 0, 0);
-        lst.add(cgRowSplit);
-        return lst;
-      }
-
-      if (cgindex == null)
-        cgindex = buildIndex(fs, this.path, dirty, conf);
-
-      if (cgindex.size() == 0)
-      {
-        cgRowSplit = new CGRowSplit(null, null, 0, -1, 0, 0);
-        lst.add(cgRowSplit);
-        return lst;
-      }
-
-      for (int i=0; i< numBatches; i++) {
-        int indexFirst = batches[i];
-        int indexLast = batches[i+1] - 1;
-        startFirst = starts[indexFirst];
-        bytesFirst = lengths[indexFirst];
-        bytesLast = lengths[indexLast];
-        length = batches[i+1] - batches[i];
-        String[] namesInSplit = new String[length];
-        long[] sizesInSplit = new long[length];
-        for (int j = 0; j < length; j++)
-        {
-          namesInSplit[j] = paths[indexFirst+j].getName();
-          sizesInSplit[j] = cgindex.get(cgindex.getFileIndex(paths[indexFirst+j])).bytes;
-        }
-        cgRowSplit = new CGRowSplit(namesInSplit, sizesInSplit, length, 
-                startFirst, bytesFirst, bytesLast);
-        lst.add(cgRowSplit);
-      }
-      
-      return lst;
-    }
-    
-    void rearrangeFileIndices(FileStatus[] fileStatus) throws IOException {
-      int size = fileStatus.length;
-      FileStatus[] result = new FileStatus[size];
-      if (cgindex == null)
-        cgindex = buildIndex(fs, path, dirty, conf);
-      if (size < cgindex.size())
-        throw new AssertionError("Incorrect file list size");
-      for (int j, i = 0; i < size; i++)
-      {
-        j = cgindex.getFileIndex(fileStatus[i].getPath());
-        if (j != -1)
-          result[j] = fileStatus[i];
-      }
-      for (int i = 0; i < size; i++)
-        fileStatus[i] = result[i];
-    }
-
-    /**
-     * Is the ColumnGroup sorted?
-     * 
-     * @return Whether the ColumnGroup is sorted.
-     */
-    public boolean isSorted() {
-      return cgschema.isSorted();
-    }
-
-    @Override
-    public void close() throws IOException {
-      if (!closed) {
-        closed = true;
-      }
-    }
-
-    /**
-     * A simple wrapper class over TFile.Reader.Scanner to simplify the creation
-     * and resource management.
-     */
-    static class TFileScanner implements Closeable {
-      boolean closed = true;
-      FSDataInputStream ins;
-      TFile.Reader reader;
-      TFile.Reader.Scanner scanner;
-      TupleReader tupleReader;
-
-      TFileScanner(FileSystem fs, Path path, CGRowSplit rowRange, 
-                    RawComparable begin, RawComparable end, boolean first, boolean last,
-                    CGSchema cgschema, Projection projection,
-          Configuration conf) throws IOException, ParseException {
-        try {
-          ins = fs.open(path);
-          /*
-           * compressor is inside cgschema
-           */
-          reader = new TFile.Reader(ins, fs.getFileStatus(path).getLen(), conf);
-          if (rowRange != null && rowRange.startByteFirst != -1) {
-            if (first && rowRange.startByteFirst != -1)
-              scanner = reader.createScannerByRecordNum(rowRange.startRowFirst, 
-                                              rowRange.startRowFirst + rowRange.numRowsFirst);
-            else if (last && rowRange.numBytesLast != -1)
-              scanner = reader.createScannerByRecordNum(0, rowRange.numRowsLast);
-            else
-              scanner = reader.createScanner();
-          } else {
-            /* TODO: more investigation is needed for the following.
-             *  using deprecated API just so that zebra can work with 
-             * hadoop jar that does not contain HADOOP-6218 (Record ids for
-             * TFile). This is expected to be temporary. Later we should 
-             * use the undeprecated API.
-             */
-            scanner = reader.createScanner(begin, end);
-          }
-          /*
-           * serializer is inside cgschema: different serializer will require
-           * different Reader: for pig, it's TupleReader
-           */
-          tupleReader = new TupleReader(cgschema.getSchema(), projection);
-          closed = false;
-        }
-        finally {
-          if (closed == true) { // failed to instantiate the object.
-            if (scanner != null) {
-              try {
-                scanner.close();
-              }
-              catch (Exception e) {
-                // no-op
-              }
-            }
-
-            if (reader != null) {
-              try {
-                reader.close();
-              }
-              catch (Exception e) {
-                // no op
-              }
-            }
-
-            if (ins != null) {
-              try {
-                ins.close();
-              }
-              catch (Exception e) {
-                // no op
-              }
-            }
-          }
-        }
-      }
-
-      void rewind() throws IOException {
-        scanner.rewind();
-      }
-
-      void getKey(BytesWritable key) throws IOException {
-        scanner.entry().getKey(key);
-      }
-
-      void getValue(Tuple val) throws IOException, ParseException {
-        DataInputStream dis = scanner.entry().getValueStream();
-        try {
-          tupleReader.get(dis, val);
-          
-        }
-        finally {
-          dis.close();
-        }
-      }
-
-      boolean seekTo(BytesWritable key) throws IOException {
-        return scanner.seekTo(key.getBytes(), 0, key.getLength());
-      }
-
-      boolean advance() throws IOException {
-        return scanner.advance();
-      }
-
-      boolean atEnd() {
-        return scanner.atEnd();
-      }
-
-      void seekToEnd() throws IOException {
-        scanner.seekToEnd();
-      }
-
-      @Override
-      public void close() throws IOException {
-        if (!closed) {
-          closed = true;
-          try {
-            scanner.close();
-          }
-          catch (Exception e) {
-            // no op
-          }
-
-          try {
-            reader.close();
-          }
-          catch (Exception e) {
-            // no op
-          }
-
-          try {
-            ins.close();
-          }
-          catch (Exception e) {
-            // no op
-          }
-        }
-      }
-    }
-
-    /**
-     * ColumnGroup scanner
-     */
-    class CGScanner implements TableScanner {
-      private Projection logicalSchema = null;
-      private TFileScannerInfo[] scanners;
-      private boolean closeReader;
-      private int beginIndex, endIndex;
-      private int current; // current scanner
-      private boolean scannerClosed = true;
-      private CGRowSplit rowRange;
-      private TFileScanner scanner;
-      
-      private class TFileScannerInfo {
-        boolean first, last;
-        Path path;
-        RawComparable begin, end;
-        TFileScannerInfo(boolean first, boolean last, Path path, RawComparable begin, RawComparable end) {
-          this.first = first;
-          this.last = last;
-          this.begin = begin;
-          this.end = end;
-          this.path = path;
-        }
-        
-        TFileScanner getTFileScanner() throws IOException {
-          try {
-            return new TFileScanner(fs, path, rowRange, 
-                  begin, end, first, last, cgschema, logicalSchema, conf);
-          } catch (ParseException e) {
-            throw new IOException(e.getMessage());
-          }
-        }
-      }
-
-
-      CGScanner(CGRangeSplit split, boolean closeReader) throws IOException,
-      ParseException {
-        if (cgindex== null)
-          cgindex = buildIndex(fs, path, dirty, conf);
-        if (split == null) {
-          beginIndex = 0;
-          endIndex = cgindex.size();
-        }
-        else {
-          beginIndex = split.start;
-          endIndex = split.start + split.len;
-        }
-        init(null, null, null, closeReader);
-      }
-      
-      /**
-       * Scanner for a range specified by the given row range.
-       * 
-       * @param rowRange see {@link CGRowSplit}
-       * @param closeReader
-       */
-      CGScanner(CGRowSplit rowRange, boolean closeReader) 
-                 throws IOException, ParseException {
-        beginIndex = 0;
-        endIndex = rowRange.length;
-        init(rowRange, null, null, closeReader);
-      }
-      
-      CGScanner(RawComparable beginKey, RawComparable endKey,
-          boolean closeReader) throws IOException, ParseException {
-        beginIndex = 0;
-        endIndex = cgindex.size();
-        if (beginKey != null) {
-          beginIndex = cgindex.lowerBound(beginKey, comparator);
-        }
-        if (endKey != null) {
-          endIndex = cgindex.lowerBound(endKey, comparator);
-          if (endIndex < cgindex.size()) {
-            ++endIndex;
-          }
-        }
-        init(null, beginKey, endKey, closeReader);
-      }
-
-      private void init(CGRowSplit rowRange, RawComparable beginKey, 
-                        RawComparable endKey, boolean doClose) 
-             throws IOException, ParseException {
-        this.rowRange = rowRange;
-        if (beginIndex > endIndex) {
-          throw new IllegalArgumentException("beginIndex > endIndex");
-        }
-        logicalSchema = ColumnGroup.Reader.this.getProjection();
-        List<TFileScannerInfo> tmpScanners =
-            new ArrayList<TFileScannerInfo>(endIndex - beginIndex);
-        try {
-          boolean first, last, realFirst = true;
-          Path myPath;
-          for (int i = beginIndex; i < endIndex; ++i) {
-            first = (i == beginIndex);
-            last = (i == endIndex -1);
-            RawComparable begin = first ? beginKey : null;
-            RawComparable end = last ? endKey : null;
-            TFileScannerInfo scanner;
-            if (rowRange == null)
-              myPath = cgindex.getPath(i, path);
-            else
-              myPath = new Path(path, rowRange.names[i]);
-            scanner =
-                new TFileScannerInfo(first, last, myPath, begin, end);
-            if (realFirst) {
-              this.scanner = scanner.getTFileScanner();
-              if (this.scanner.atEnd()) {
-                this.scanner.close();
-                this.scanner = null;
-              } else {
-                realFirst = false;
-                tmpScanners.add(scanner);
-              }
-            } else {
-              TFileScanner myScanner = scanner.getTFileScanner();
-              if (!myScanner.atEnd())
-                tmpScanners.add(scanner);
-              myScanner.close();
-            }
-          }
-          scanners = tmpScanners.toArray(new TFileScannerInfo[tmpScanners.size()]);
-          this.closeReader = doClose;
-          scannerClosed = false;
-        }
-        finally {
-          if (scannerClosed) { // failed to initialize the object.
-            if (scanner != null)
-              scanner.close();
-          }
-        }
-      }
-
-      @Override
-      public void getKey(BytesWritable key) throws IOException {
-          if (atEnd()) {
-            throw new EOFException("No more key-value to read");
-          }
-          scanner.getKey(key);
-        }
-
-        @Override
-        public void getValue(Tuple row) throws IOException {
-          if (atEnd()) {
-            throw new EOFException("No more key-value to read");
-          }
-          try {
-            scanner.getValue(row);
-          } catch (ParseException e) {
-            throw new IOException("Invalid Projection: "+e.getMessage());
-          }
-        }
-
-      public void getCGKey(BytesWritable key) throws IOException {
-        scanner.getKey(key);
-      }
-
-      public void getCGValue(Tuple row) throws IOException {
-        try {
-            scanner.getValue(row);
-          } catch (ParseException e) {
-            throw new IOException("Invalid Projection: "+e.getMessage());
-          }
-      }
-
-      @Override
-      public String getProjection() {
-        return logicalSchema.toString();
-      }
-      
-      public Schema getSchema() {
-        return logicalSchema.getSchema();
-      }
-
-      @Override
-      public boolean advance() throws IOException {
-        if (atEnd()) {
-          return false;
-        }
-        scanner.advance();
-        while (true)
-        {
-          if (scanner.atEnd()) {
-            scanner.close();
-            scanner = null;
-            ++current;
-            if (!atEnd()) {
-              scanner = scanners[current].getTFileScanner();
-            } else
-              return false;
-          } else
-            return true;
-        }
-      }
-
-      public boolean advanceCG() throws IOException {
-        scanner.advance();
-        while (true)
-        {
-          if (scanner.atEnd()) {
-            scanner.close();
-            scanner = null;
-            ++current;
-            if (!atEnd()) {
-              scanner = scanners[current].getTFileScanner();
-            } else
-              return false;
-          } else
-            return true;
-        }
-      }
-
-      @Override
-      public boolean atEnd() throws IOException {
-        return (current >= scanners.length);
-      }
-
-      @Override
-      public boolean seekTo(BytesWritable key) throws IOException {
-        if (!isSorted()) {
-          throw new IOException("Cannot seek in unsorted Column Gruop");
-        }
-        if (atEnd())
-        {
-          return false;
-        }
-        int index =
-            cgindex.lowerBound(new ByteArray(key.getBytes(), 0, key.getLength()),
-                comparator);
-        if (index >= endIndex) {
-          seekToEnd();
-          return false;
-        }
-
-        if ((index < beginIndex)) {
-          // move to the beginning
-          index = beginIndex;
-        }
-
-        int prevCurrent = current;
-        current = index - beginIndex;
-        if (current != prevCurrent)
-        {
-          if (scanner != null)
-          {
-            try {
-              scanner.close();
-            } catch (Exception e) {
-              // no-op
-            }
-          }
-          scanner = scanners[current].getTFileScanner();
-        }
-        return scanner.seekTo(key);
-      }
-
-      @Override
-      public void seekToEnd() throws IOException {
-        if (scanner != null)
-        {
-          try {
-            scanner.close();
-          } catch (Exception e) {
-            // no-op
-          }
-        }
-        scanner = null;
-        current = scanners.length;
-      }
-
-      @Override
-      public void close() throws IOException {
-        if (!scannerClosed) {
-          scannerClosed = true;
-          if (scanner != null)
-          {
-            try {
-              scanner.close();
-              scanner = null;
-            } catch (Exception e) {
-              // no-op
-            }
-          }
-          if (closeReader) {
-            Reader.this.close();
-          }
-        }
-      }
-    }
-
-    public static class CGRangeSplit implements Writable {
-      int start = 0; // starting index in the list
-      int len = 0;
-
-      CGRangeSplit(int start, int len) {
-        this.start = start;
-        this.len = len;
-      }
-
-      public CGRangeSplit() {
-        // no-op;
-      }
-
-      @Override
-      public void readFields(DataInput in) throws IOException {
-        start = Utils.readVInt(in);
-        len = Utils.readVInt(in);
-      }
-
-      @Override
-      public void write(DataOutput out) throws IOException {
-        Utils.writeVInt(out, start);
-        Utils.writeVInt(out, len);
-      }
-    }
-    
-    public static class CGRowSplit implements Writable {
-      int length; // number of files in the batch
-      long startByteFirst = -1;
-      long numBytesFirst;
-      long startRowFirst = -1;
-      long numRowsFirst = -1;
-      long numBytesLast = -1;
-      long numRowsLast = -1;
-      String[] names;
-      long[] sizes = null;
-
-      CGRowSplit(String[] names, long[] sizes, int length, long startFirst, long bytesFirst,
-          long bytesLast) throws IOException {
-        this.names = names;
-        this.sizes = sizes;
-        this.length = length;
-
-        if (startFirst != -1)
-        {
-          startByteFirst = startFirst;
-          numBytesFirst = bytesFirst;
-        }
-        if (bytesLast != -1 && this.length > 1)
-        {
-          numBytesLast = bytesLast;
-        }
-      }
-
-      public CGRowSplit() {
-        // no-op;
-      }
-      
-      @Override
-      public String toString() {
-        StringBuilder sb = new StringBuilder();
-        sb.append("{length = " + length + "}\n");
-        for (int i = 0; i < length; i++)
-        {
-          sb.append("{name = " + names[i] + "}\n");
-          sb.append("{size = " + sizes[i] + "}\n");
-        }
-        sb.append("{startByteFirst = " + startByteFirst + "}\n");
-        sb.append("{numBytesFirst = " + numBytesFirst + "}\n");
-        sb.append("{startRowFirst = " + startRowFirst + "}\n");
-        sb.append("{numRowsFirst = " + numRowsFirst + "}\n");
-        sb.append("{numBytesLast = " + numBytesLast + "}\n");
-        sb.append("{numRowsLast = " + numRowsLast + "}\n");
-        
-        return sb.toString();
-      }
-
-      @Override
-      public void readFields(DataInput in) throws IOException {
-        length = Utils.readVInt(in);
-        if (length > 0)
-        {
-          names = new String[length];
-          sizes = new long[length];
-        }
-        for (int i = 0; i < length; i++)
-        {
-          names[i] = Utils.readString(in);
-          sizes[i] = Utils.readVLong(in);
-        }
-        startByteFirst = Utils.readVLong(in);
-        numBytesFirst = Utils.readVLong(in);
-        startRowFirst = Utils.readVLong(in);
-        numRowsFirst = Utils.readVLong(in);
-        numBytesLast = Utils.readVLong(in);
-        numRowsLast = Utils.readVLong(in);
-      }
-
-      @Override
-      public void write(DataOutput out) throws IOException {
-        Utils.writeVInt(out, length);
-        for (int i = 0; i < length; i++)
-        {
-          Utils.writeString(out, names[i]);
-          Utils.writeVLong(out, sizes[i]);
-        }
-        Utils.writeVLong(out, startByteFirst);
-        Utils.writeVLong(out, numBytesFirst);
-        Utils.writeVLong(out, startRowFirst);
-        Utils.writeVLong(out, numRowsFirst);
-        Utils.writeVLong(out, numBytesLast);
-        Utils.writeVLong(out, numRowsLast);
-      }
-    }
-    
-    private static class SplitColumn {
-      SplitColumn(Partition.SplitType st) {
-        this.st = st;
-      }
-
-      SplitColumn(int fieldIndex, Partition.SplitType st) {
-        this.fieldIndex = fieldIndex;
-        this.st = st;
-      }
-
-      SplitColumn(int fieldIndex, String key, Partition.SplitType st) {
-        this.fieldIndex = fieldIndex;
-        this.key = key;
-        this.st = st;
-      }
-
-      SplitColumn(int fieldIndex, int projIndex, SplitColumn leaf, String key,
-          Partition.SplitType st) {
-        this(fieldIndex, key, st);
-        this.projIndex = projIndex;
-      }
-
-      int fieldIndex = -1; // field index to parent
-      int projIndex = -1; // index in projection: only used by leaves
-      SplitColumn leaf = null;
-      String key = null; // MAP key to parent
-      ArrayList<SplitColumn> children = null;
-      int index = -1; // index in the logical schema
-      Object field = null;
-      Partition.SplitType st = Partition.SplitType.NONE;
-
-      void dispatch(Object field) {
-        this.field = field;
-      }
-
-      @SuppressWarnings("unchecked")
-      void split() throws ExecException {
-        int size = children.size();
-        if (st == Partition.SplitType.RECORD) {
-          for (int i = 0; i < size; i++) {
-            if (children.get(i).projIndex != -1) // a leaf: set projection
-                                                  // directly
-            ((Tuple) (leaf.field)).set(projIndex, ((Tuple) field).get(children
-                .get(i).fieldIndex));
-            else children.get(i).field =
-                ((Tuple) field).get(children.get(i).fieldIndex);
-          }
-        }
-        else if (st == Partition.SplitType.MAP) {
-          for (int i = 0; i < size; i++) {
-            if (children.get(i).projIndex != -1) // a leaf: set projection
-                                                  // directly
-            ((Tuple) (leaf.field)).set(projIndex, ((Map<String, Object>) field)
-                .get(children.get(i).key));
-            else children.get(i).field =
-                ((Map<String, Object>) field).get(children.get(i).key);
-          }
-        }
-      }
-
-      void addChild(SplitColumn child) {
-        if (children == null) children = new ArrayList<SplitColumn>();
-        children.add(child);
-      }
-    }
-  }
-
-  /**
-   * Column Group writer.
-   */
-  public static class Writer implements Closeable {
-    Path path;
-    Path finalOutputPath;
-    Configuration conf;
-    FileSystem fs;
-    CGSchema cgschema;
-    private boolean finished, closed;
-    CGIndex index;
-
-    /**
-     * Create a ColumnGroup writer. The semantics are as follows:
-     * <ol>
-     * <li>If path does not exist:
-     * <ul>
-     * <li>create the path directory
-     * <li>write out the meta data file.
-     * </ul>
-     * <li>If path exists and the directory is empty: write out the meta data
-     * file.
-     * <li>If path exists and contains what look like a complete Column Group,
-     * ColumnGroupExists exception will be thrown.
-     * <li>If path exists and overwrite is true, remove all files under the
-     * directory and resume as in Step 2.
-     * <li>If path exists directory not empty and overwrite= false,
-     * ColumnGroupExists will be thrown.
-     * </ol>
-     * This constructor never removes a valid/complete ColumnGroup.
-     * 
-     * @param path
-     *          The path to the Column Group, either not existent or must be a
-     *          directory.
-     * @param schema
-     *          The schema of the ColumnGroup. For this version of
-     *          implementation, the schema of a table is a comma separated list
-     *          of column names, such as "FirstName, LastName, Sex, Department".
-     * @param sorted
-     *          Whether the column group to be created is sorted or not. If set
-     *          to true, we expect the rows inserted by every inserter created
-     *          from this Writer must be sorted. Additionally, there exists an
-     *          ordering of the inserters Ins-1, Ins-2, ... such that the rows
-     *          created by Ins-1, followed by rows created by Ins-2, ... form a
-     *          total order.
-     * @param overwrite
-     *          Should we overwrite the path if it already exists?
-     * @param conf
-     *          The optional configuration objects.
-     * @throws IOException
-     */
-    public Writer(Path path, String schema, boolean sorted, String name, String serializer,
-        String compressor, String owner, String group, short perm,boolean overwrite, Configuration conf)
-        throws IOException, ParseException {
-      this(path, new Schema(schema), sorted, null, name, serializer, compressor, owner, group, perm, overwrite,
-          conf);
-    }
-    
-    public Writer(Path path, Schema schema, boolean sorted, String name, String serializer,
-        String compressor, String owner, String group, short perm,boolean overwrite, Configuration conf)
-        throws IOException, ParseException {
-      this(path, schema, sorted, null, name, serializer, compressor, owner, group, perm, overwrite,
-          conf);
-    }
-
-    public Writer(Path path, String schema, boolean sorted, String comparator, String name, String serializer,
-        String compressor, String owner, String group, short perm,boolean overwrite, Configuration conf)
-        throws IOException, ParseException {
-      this(path, new Schema(schema), sorted, comparator, name, serializer, compressor, owner, group, perm, overwrite,
-          conf);
-    }
-
-    public Writer(Path path, Schema schema, boolean sorted, String comparator, String name, String serializer,
-        String compressor, String owner, String group, short perm, boolean overwrite, Configuration conf)
-        throws IOException, ParseException {
-      this.path = path;
-      this.conf = conf;
-      this.finalOutputPath = path;
-
-      fs = path.getFileSystem(conf);
-
-      // If meta file already exists, that means the ColumnGroup is complete and
-      // valid, we will not proceed.
-      checkMetaFile(path);
-
-      // if overwriting, remove everything
-      if (overwrite) {
-        fs.delete(path, true);
-      }
-
-      // create final output path and temporary output path
-      checkPath(path, true);
-      
-      Path parent = path.getParent();
-      Path tmpPath1 = new Path(parent, "_temporary");
-      Path tmpPath2 = new Path(tmpPath1, name);
-      checkPath(tmpPath2, true);
-      
-      cgschema = new CGSchema(schema, sorted, comparator, name, serializer, compressor, owner, group, perm);
-      CGSchema sfNew = CGSchema.load(fs, path);
-      if (sfNew != null) {
-        // sanity check - compare input with on-disk schema.
-        if (!sfNew.equals(cgschema)) {
-          throw new IOException("Schema passed in is different from the one on disk");
-        }
-      } else {
-        // create the schema file in FS
-        cgschema.create(fs, path);
-      }
-    }
-
-    /**
-     * Reopen an already created ColumnGroup for writing. It accepts
-     * a temporary path for column group where cginserter can write.
-     * RuntimeException will be thrown if the table is already closed, 
-     * or if createMetaBlock() is called by some other process.
-     */
-    public Writer(Path finalPath, Path workPath, Configuration conf) throws IOException,
-        ParseException {
-      this.path = workPath;
-      finalOutputPath = finalPath;
-      this.conf = conf;
-      fs = path.getFileSystem(conf);
-      checkPath(finalOutputPath, false);
-      checkPath(path, true);
-      checkMetaFile(finalOutputPath);
-      cgschema = CGSchema.load(fs, finalOutputPath);
-    }
-
-    /*
-     * Reopen an already created ColumnGroup for writing.
-     * It takes in a CGSchema to set its own cgschema instead of going
-     * to disk to fetch this information. 
-     */
-    public Writer(Path finalPath, Path workPath, CGSchema cgschema, Configuration conf) throws IOException, ParseException {
-      this.path = workPath;
-      finalOutputPath = finalPath;      
-      this.conf = conf;
-      fs = path.getFileSystem(conf);
-      this.cgschema = cgschema;
-    }
-
-    /**
-     * Reopen an already created ColumnGroup for writing. RuntimeException will
-     * be thrown if the table is already closed, or if createMetaBlock() is
-     * called by some other process.
-     */
-    public Writer(Path path, Configuration conf) throws IOException,
-        ParseException {
-      this.path = path;
-      finalOutputPath = path;
-      this.conf = conf;
-      fs = path.getFileSystem(conf);
-      checkPath(path, false);
-      checkMetaFile(path);
-      // read the schema file
-      cgschema = CGSchema.load(fs, path);
-    }
-
-    /**
-     * Release resources used by the object. Unlike close(), finish() does not
-     * make the table immutable. However, if a user already adds some meta data
-     * into the CG, then finish() would close the column group.
-     */
-    public void finish() {
-      if (!finished) {
-        finished = true;
-      }
-    }
-
-    @Override
-    public void close() throws IOException {
-      if (!finished) {
-        finish();
-      }
-      if (!closed) {
-        closed = true;
-        createIndex();
-      }
-    }
-
-    public Schema getSchema() {
-      return cgschema.getSchema();
-    }
-
-    /**
-     * Get a inserter with a given name.
-     * 
-     * @param name
-     *          the name of the inserter.
-     * @param finishWriter
-     *          finish the underlying Writer object upon the close of the
-     *          Inserter. Should be set to true if there is only one inserter
-     *          operate on the table, so we should call finish() after the
-     *          Inserter is closed.
-     * 
-     * @return A table inserter object.
-     * @throws IOException
-     */
-    public TableInserter getInserter(String name, boolean finishWriter)
-        throws IOException {
-      return getInserter(name, finishWriter, true);      
-    }
-    
-    /**
-     * Get a inserter with a given name.
-     * 
-     * @param name
-     *          the name of the inserter.
-     * @param finishWriter
-     *          finish the underlying Writer object upon the close of the
-     *          Inserter. Should be set to true if there is only one inserter
-     *          operate on the table, so we should call finish() after the
-     *          Inserter is closed.
-     * @param checktype
-     *          whether or not do type check.
-     * 
-     * @return A table inserter object.
-     * @throws IOException
-     */
-    public TableInserter getInserter(String name, boolean finishWriter, boolean checkType)
-        throws IOException {
-      if (finished) {
-        throw new IOException("ColumnGroup has been closed for insertion.");
-      }
-      return new CGInserter(name, finishWriter, checkType);
-    }
-
-    private void createIndex() throws IOException {
-      MetaFile.Writer metaFile =
-        MetaFile.createWriter(makeMetaFilePath(finalOutputPath), conf);
-      index = buildIndex(fs, finalOutputPath, false, conf);
-      DataOutputStream dos = metaFile.createMetaBlock(BLOCK_NAME_INDEX);
-      try {
-        index.write(dos);
-      }
-      finally {
-        dos.close();
-      }
-      metaFile.close();
-    }
-
-    private void checkPath(Path p, boolean createNew) throws IOException {
-      // check existence of path
-      if (!fs.exists(p)) {
-        if (createNew) {
-          fs.mkdirs(p);
-        }
-        else {
-          throw new IOException("Path doesn't exists for appending: " + p);
-        }
-      }
-      if (!fs.getFileStatus(p).isDir()) {
-        throw new IOException("Path exists but not a directory: " + p);
-      }
-    }
-
-    private void checkMetaFile(Path p) throws IOException {
-      Path pathMeta = new Path(p, META_FILE);
-      if (fs.exists(pathMeta)) {
-        throw new IOException("Index meta file already exists: " + pathMeta);
-      }
-    }
-
-    /**
-     * Inserter for ColumnGroup
-     */
-    class CGInserter implements TableInserter {
-      String name;
-      String tmpName;
-      boolean finishWriter;
-      FSDataOutputStream out;
-      TFile.Writer tfileWriter;
-      TupleWriter tupleWriter;
-      boolean closed = true;
-      boolean checkType = true;
-      
-      private void createTempFile() throws IOException {
-        int maxTrial = 10;
-        String prefix = ".tmp." + name + ".";
-        Random random = new Random();
-
-        while (true) {
-          /**
-           * Try to set a real random seed by throwing all the runtime
-           * ingredients into it.
-           */
-          random.setSeed(System.nanoTime() * Thread.currentThread().getId()
-              * Runtime.getRuntime().freeMemory());
-          try {
-            tmpName = prefix + String.format("%08X", random.nextInt());
-            Path tmpPath = new Path(path, tmpName);
-            fs.mkdirs(path);
-
-            if(cgschema.getOwner() != null  || cgschema.getGroup() != null) {
-              fs.setOwner(path, cgschema.getOwner(), cgschema.getGroup());
-            }  
-
-            FsPermission permission = null;
-            if(cgschema.getPerm() != -1) {
-                permission = new FsPermission((short) cgschema.getPerm());
-            	fs.setPermission(path, permission);
-            }  
-            
-            out = fs.create(tmpPath, false);
-
-            if(cgschema.getOwner() != null || cgschema.getGroup() != null) {
-                fs.setOwner(tmpPath, cgschema.getOwner(), cgschema.getGroup());
-       	    }  
-
-            if(cgschema.getPerm() != -1) {
-        	  fs.setPermission(tmpPath, permission);
-            }	
-            return;
-          }
-          catch (IOException e) {
-            --maxTrial;
-            if (maxTrial == 0) {
-              throw e;
-            }
-            Thread.yield();
-          }
-        }
-      }
-      
-      CGInserter(String name, boolean finishWriter, boolean checkType) throws IOException {
-        this.name = name;
-        this.finishWriter = finishWriter;
-        this.tupleWriter = new TupleWriter(getSchema());
-        this.checkType = checkType;
-        
-        try {
-          createTempFile();
-          tfileWriter =
-            new TFile.Writer(out, getMinBlockSize(conf), cgschema.getCompressor(), cgschema.getComparator(), conf);
-          closed = false;
-        }
-        finally {
-          if (closed) {
-            if (tfileWriter != null) {
-              try {
-                tfileWriter.close();
-              }
-              catch (Exception e) {
-                // no-op
-              }
-            }
-            if (out != null) {
-              try {
-                out.close();
-              }
-              catch (Exception e) {
-                // no-op
-              }
-            }
-            if (tmpName != null) {
-              try {
-                fs.delete(new Path(path, tmpName), false);
-              }
-              catch (Exception e) {
-                // no-op
-              }
-            }
-          }
-        }
-      }
-
-
-      @Override
-      public Schema getSchema() {
-        return ColumnGroup.Writer.this.getSchema();
-      }
-
-      @Override
-      public void insert(BytesWritable key, Tuple row) throws IOException {
-        /*
-         * If checkType is set to be true, we check for the first row - this is only a sanity check preventing
-         * users from messing up output schema;
-         * If checkType is set to be false, we do not do any type check. 
-         */
-        if (checkType == true) {
-          TypesUtils.checkCompatible(row, getSchema());
-          checkType = false;
-        }
-        
-        DataOutputStream outKey = tfileWriter.prepareAppendKey(key.getLength());
-        try {
-          outKey.write(key.getBytes(), 0, key.getLength());
-        }
-        finally {
-          outKey.close();
-        }
-
-        DataOutputStream outValue = tfileWriter.prepareAppendValue(-1);
-        try {
-          tupleWriter.put(outValue, row);
-        }
-        finally {
-          outValue.close();
-        }
-      }
-
-      @Override
-      public void close() throws IOException {
-        if (closed) {
-          return;
-        }
-        closed = true;
-
-        try {
-          // TODO: add schema to each TFile as a meta block?
-
-          tfileWriter.close();
-          tfileWriter = null;
-          out.close();
-          out = null;
-          // do renaming only if all the above is successful.
-          fs.rename(new Path(path, tmpName), new Path(finalOutputPath, name));
-
-/*
-          if(cgschema.getOwner() != null || cgschema.getGroup() != null) {
-            fs.setOwner(new Path(path, name), cgschema.getOwner(), cgschema.getGroup());
-          }  
-          FsPermission permission = null;
-          if(cgschema.getPerm() != -1) {
-            permission = new FsPermission((short) cgschema.getPerm());
-            fs.setPermission(path, permission);
-          }
-*/                     
-          tmpName = null;
-          if (finishWriter) {
-            finish();
-          }
-        }
-        finally {
-          if (tfileWriter != null) {
-            try {
-              tfileWriter.close();
-            }
-            catch (Exception e) {
-              // no-op
-            }
-          }
-          if (out != null) {
-            try {
-              out.close();
-            }
-            catch (Exception e) {
-              // no-op
-            }
-          }
-          if (tmpName != null) {
-            try {
-              fs.delete(new Path(path, tmpName), false);
-            }
-            catch (Exception e) {
-              // no-op
-            }
-          }
-          if (finishWriter) {
-            try {
-              finish();
-            }
-            catch (Exception e) {
-              // no-op
-            }
-          }
-        }
-      }
-    }
-
-  }
-
-  /**
-   * name, first and last key (inclusive) of a data file
-   */
-  static class CGIndexEntry implements RawComparable, Writable {
-    int index;
-    String name;
-    long rows, bytes;
-    RawComparable firstKey;
-    RawComparable lastKey;
-
-    // for reading
-    public CGIndexEntry() {
-      // no-op
-    }
-
-    // for writing
-    public CGIndexEntry(String name, long rows, RawComparable firstKey,
-        RawComparable lastKey) {
-      this.name = name;
-      this.rows = rows;
-      this.firstKey = firstKey;
-      this.lastKey = lastKey;
-    }
-
-    public int getIndex() {
-      return index;
-    }
-    
-    public String getName() {
-      return name;
-    }
-
-    public long getRows() {
-      return rows;
-    }
-
-    public RawComparable getFirstKey() {
-      return firstKey;
-    }
-
-    public RawComparable getLastKey() {
-      return lastKey;
-    }
-    
-    void setIndex (int idx) {
-      this.index = idx;
-    }
-
-    @Override
-    public byte[] buffer() {
-      return (lastKey != null) ? lastKey.buffer() : null;
-    }
-
-    @Override
-    public int offset() {
-      return (lastKey != null) ? lastKey.offset() : 0;
-    }
-
-    @Override
-    public int size() {
-      return (lastKey != null) ? lastKey.size() : 0;
-    }
-
-    @Override
-    public void readFields(DataInput in) throws IOException {
-      name = Utils.readString(in);
-      rows = Utils.readVLong(in);
-      if (rows == 0) {
-        firstKey = null;
-        lastKey = null;
-      }
-      else {
-        int firstKeyLen = Utils.readVInt(in);
-        byte[] firstKeyBuffer = new byte[firstKeyLen];
-        in.readFully(firstKeyBuffer);
-        int lastKeyLen = Utils.readVInt(in);
-        byte[] lastKeyBuffer = new byte[lastKeyLen];
-        in.readFully(lastKeyBuffer);
-        firstKey = new ByteArray(firstKeyBuffer);
-        lastKey = new ByteArray(lastKeyBuffer);
-      }
-    }
-
-    @Override
-    public void write(DataOutput out) throws IOException {
-      Utils.writeString(out, name);
-      Utils.writeVLong(out, rows);
-      if (rows > 0) {
-        if ((firstKey == null) && (lastKey == null)) {
-          throw new IOException("In-memory only entry");
-        }
-        Utils.writeVInt(out, firstKey.size());
-        out.write(firstKey.buffer(), firstKey.offset(), firstKey.size());
-        Utils.writeVInt(out, lastKey.size());
-        out.write(lastKey.buffer(), lastKey.offset(), lastKey.size());
-      }
-    }
-  }
-
-  static class CGIndex implements Writable {
-    boolean dirty = false;
-    boolean sorted = true;
-    BasicTableStatus status;
-    ArrayList<CGIndexEntry> index;
-
-    CGIndex() {
-      status = new BasicTableStatus();
-      index = new ArrayList<CGIndexEntry>();
-    }
-    
-    int getFileIndex(Path path) throws IOException {
-      String filename = path.getName();
-      if (index.isEmpty())
-        return -1;
-      for (CGIndexEntry cgie : index) {
-        if (cgie.getName().equals(filename)) {
-          return cgie.getIndex(); 
-        }
-      }
-      return -1;
-    }
-
-    int size() {
-      return index.size();
-    }
-
-    CGIndexEntry get(int i) {
-      return index.get(i);
-    }
-
-    List<CGIndexEntry> getIndex() {
-      return index;
-    }
-
-    Path getPath(int i, Path parent) {
-      return new Path(parent, index.get(i).getName());
-    }
-
-    void sort(final Comparator<RawComparable> comparator) throws IOException {
-      if (dirty && comparator != null) {
-        throw new IOException("Cannot sort dirty index");
-      }
-
-      if (comparator != null) {
-        // sort by keys. For empty TFiles, they are always sorted before
-        // non-empty TFiles, and they themselves are sorted by their names.
-        Collections.sort(index, new Comparator<CGIndexEntry>() {
-
-          @Override
-          public int compare(CGIndexEntry o1, CGIndexEntry o2) {
-            if ((o1.getRows() == 0) && (o2.getRows() == 0)) {
-              return o1.getName().compareTo(o2.getName());
-            }
-            if (o1.getRows() == 0) return -1;
-            if (o2.getRows() == 0) return 1;
-            int cmprv = comparator.compare(o1.lastKey, o2.lastKey);
-            if (cmprv == 0) {
-              cmprv = comparator.compare(o1.firstKey, o2.firstKey);
-              if (cmprv == 0) {
-                cmprv = o1.getName().compareTo(o2.getName());
-              }
-            }
-            return cmprv;
-          }
-        });
-
-        for (int i = 0; i < index.size() - 1; ++i) {
-          RawComparable prevLastKey = index.get(i).lastKey;
-          RawComparable nextFirstKey = index.get(i + 1).firstKey;
-          if (nextFirstKey == null) {
-            continue;
-          }
-          if (comparator.compare(prevLastKey, nextFirstKey) > 0) {
-            throw new IOException("Overlapping key ranges");
-          }
-        }
-      }
-      else {
-        // sort by name
-        Collections.sort(index, new Comparator<CGIndexEntry>() {
-
-          @Override
-          public int compare(CGIndexEntry o1, CGIndexEntry o2) {
-            return o1.name.compareTo(o2.name);
-          }
-        });
-      }
-
-      // update status
-      if ((!dirty) && (index.size() > 0)) {
-        RawComparable keyFirst = index.get(0).getFirstKey();
-        status.beginKey = new BytesWritable();
-        status.beginKey.set(keyFirst.buffer(), keyFirst.offset(), keyFirst
-            .size());
-        RawComparable keyLast = index.get(index.size() - 1).getLastKey();
-        status.endKey = new BytesWritable();
-        status.endKey.set(keyLast.buffer(), keyLast.offset(), keyLast.size());
-      }
-      sorted = true;
-    }
-
-    // building full index.
-    void add(long bytes, long rows, CGIndexEntry range) {
-      status.size += bytes;
-      status.rows += rows;
-      index.add(range);
-      sorted = false;
-      range.bytes = bytes;
-    }
-
-    // building dirty index
-    void add(long bytes, String name) {
-      dirty = true;
-      status.rows = -1; // reset rows to -1.
-      status.size += bytes;
-      CGIndexEntry next = new CGIndexEntry();
-      next.name = name;
-      index.add(next);
-      sorted = false;
-      next.bytes = bytes;
-    }
-
-    int lowerBound(RawComparable key, final Comparator<RawComparable> comparator)
-        throws IOException {
-      if ((key == null) || (comparator == null)) {
-        throw new IllegalArgumentException("CGIndex.lowerBound");
-      }
-
-      if (!sorted) {
-        sort(comparator);
-      }
-
-      // Treat null keys as the least key.
-      return Utils.lowerBound(index, key, new Comparator<RawComparable>() {
-        @Override
-        public int compare(RawComparable o1, RawComparable o2) {
-          if ((o1.buffer() == null) && (o2.buffer() == null)) {
-            return 0;
-          }
-          if (o1.buffer() == null) return -1;
-          if (o2.buffer() == null) return 1;
-          return comparator.compare(o1, o2);
-        }
-      });
-    }
-
-    @Override
-    public void readFields(DataInput in) throws IOException {
-      int n = Utils.readVInt(in);
-      index.clear();
-      index.ensureCapacity(n);
-      for (int i = 0; i < n; ++i) {
-        CGIndexEntry range = new CGIndexEntry();
-        range.readFields(in);
-        range.setIndex(i);
-        index.add(range);
-      }
-      status.readFields(in);
-      dirty = false;
-      sorted = true;
-    }
-
-    @Override
-    public void write(DataOutput out) throws IOException {
-      if (dirty) {
-        throw new IOException("Cannot write dirty index");
-      }
-      if (!sorted) {
-        throw new IOException("Please sort index before calling write");
-      }
-      Utils.writeVInt(out, index.size());
-      for (int i = 0; i < index.size(); ++i) {
-        index.get(i).write(out);
-      }
-      status.write(out);
-    }
-  }
-
-  public static class CGPathFilter implements PathFilter {
-    private static Configuration conf;
-   
-    public static void setConf(Configuration c) {
-      conf = c;
-    }
-
-    public boolean accept(Path p) {
-      return p.getName().equals(META_FILE) || p.getName().equals(SCHEMA_FILE)
-          || p.getName().startsWith(".tmp.")
-          || p.getName().startsWith("_")
-          || p.getName().startsWith("ttt")
-          || p.getName().startsWith(getNonDataFilePrefix(conf)) ? false : true;
-    }
-  }
-
-  /**
-   * Dump information about CG.
-   * 
-   * @param file
-   *          Path string of the CG
-   * @param out
-   *          PrintStream to output the information.
-   * @param conf
-   *          The configuration object.
-   * @throws IOException
-   */
-  static public void dumpInfo(String file, PrintStream out, Configuration conf)
-      throws IOException, Exception {
-    // final int maxKeySampleLen = 16;
-    dumpInfo(new Path(file), out, conf);
-  }
-
-  static public void dumpInfo(Path path, PrintStream out, Configuration conf)
-      throws IOException, Exception {
-      dumpInfo(path, out, conf, 0);
-  }
-
-  static public void dumpInfo(Path path, PrintStream out, Configuration conf, int indent)
-      throws IOException, Exception {
-    // final int maxKeySampleLen = 16;
-    IOutils.indent(out, indent);
-    out.println();
-    IOutils.indent(out, indent);
-    out.println("Column Group : " + path);
-    ColumnGroup.Reader reader = new ColumnGroup.Reader(path, false, conf);
-    try {
-      LinkedHashMap<String, String> properties =
-          new LinkedHashMap<String, String>();
-      IOutils.indent(out, indent);
-      out.println("Name: " + reader.getName());
-      IOutils.indent(out, indent);
-      out.println("Serializer: " + reader.getSerializer());
-      IOutils.indent(out, indent);
-      out.println("Compressor: " + reader.getCompressor());      
-      IOutils.indent(out, indent);
-      out.println("Group: " + reader.getGroup());      
-      IOutils.indent(out, indent);
-      out.println("Perm: " + reader.getPerm());      
-
-      properties.put("Schema", reader.getSchema().toString());
-      // Now output the properties table.
-      int maxKeyLength = 0;
-      Set<Map.Entry<String, String>> entrySet = properties.entrySet();
-      for (Iterator<Map.Entry<String, String>> it = entrySet.iterator(); it
-          .hasNext();) {
-        Map.Entry<String, String> e = it.next();
-        if (e.getKey().length() > maxKeyLength) {
-          maxKeyLength = e.getKey().length();
-        }
-      }
-      for (Iterator<Map.Entry<String, String>> it = entrySet.iterator(); it
-          .hasNext();) {
-        Map.Entry<String, String> e = it.next();
-        IOutils.indent(out, indent);
-        out.printf("%s : %s\n", e.getKey(), e.getValue());
-      }
-      out.println("TFiles within the Column Group :");
-      if (reader.cgindex == null)
-        reader.cgindex = buildIndex(reader.fs, reader.path, reader.dirty, conf);
-      for (CGIndexEntry entry : reader.cgindex.index) {
-        IOutils.indent(out, indent);
-        out.printf(" *Name : %s\n", entry.name);
-        IOutils.indent(out, indent);
-        out.printf("  Rows : %d\n", entry.rows);
-        if (entry.firstKey != null) {
-          IOutils.indent(out, indent);
-          out.printf("  First Key : %s\n", headToString(entry.firstKey));
-        }
-        if (entry.lastKey != null) {
-          IOutils.indent(out, indent);
-          out.printf("  Larst Key : %s\n", headToString(entry.lastKey));
-        }
-        // dump TFile info
-        // Path pathTFile = new Path(path, entry.name);
-        // TFile.dumpInfo(pathTFile.toString(), out, conf);
-      }
-    }
-    finally {
-      try {
-        reader.close();
-      }
-      catch (Exception e) {
-        // no-op
-      }
-    }
-  }
-
-  private static String headToString(RawComparable raw) {
-    return new String(raw.buffer(), raw.offset(), raw.size() > 70 ? 70 : raw
-        .size());
-  }
-
-  /**
-   * Dumping the CG information.
-   * 
-   * @param args
-   *          A list of CG paths.
-   */
-  public static void main(String[] args) throws Exception {
-    System.out.printf("ColumnGroup Dumper\n");
-    if (args.length == 0) {
-      System.out
-          .println("Usage: java ... org.apache.hadoop.zebra.io.ColumnGroup cg-path [cg-path ...]");
-      System.exit(0);
-    }
-    Configuration conf = new Configuration();
-    for (String file : args) {
-      try {
-        dumpInfo(file, System.out, conf);
-      }
-      catch (IOException e) {
-        e.printStackTrace(System.err);
-      }
-    }
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/io/IOutils.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/io/IOutils.java
deleted file mode 100644
index 30cff6729..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/io/IOutils.java
+++ /dev/null
@@ -1,39 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-package org.apache.hadoop.zebra.io;
-
-import java.io.PrintStream;
-
-/**
- * Helper for Zebra I/O
- */
-public class IOutils {
-  /**
-   * indent of some spaces
-   *
-   * @param os
-   *          print stream the indent space to be inserted
-   * @param amount
-   *          the number of spaces to be indented
-   */
-  public static void indent(PrintStream os, int amount)
-  {
-    for (int i = 0; i < amount; i++)
-        os.print(' ');
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/io/KeyDistribution.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/io/KeyDistribution.java
deleted file mode 100644
index 96bdf66c7..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/io/KeyDistribution.java
+++ /dev/null
@@ -1,234 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.util.Comparator;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.SortedMap;
-import java.util.TreeMap;
-import java.io.IOException;
-
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.tfile.RawComparable;
-import org.apache.hadoop.zebra.tfile.ByteArray;
-
-/**
- * Class used to convey the information of how on-disk data are distributed
- * among key-partitioned buckets. This class is used by the MapReduce layer to
- * calculate intelligent splits.
- */
-public class KeyDistribution {
-  private long uniqueBytes;
-  private long minStepSize = -1;
-  private SortedMap<RawComparable, BlockDistribution> data;
-
-  KeyDistribution(Comparator<? super RawComparable> comparator) {
-    data = new TreeMap<RawComparable, BlockDistribution>(comparator);
-  }
-
-  void add(RawComparable key) {
-    data.put(key, null);
-  }
-  
-  void add(RawComparable key, BlockDistribution bucket)
-  {
-    uniqueBytes += bucket.getLength();
-    data.put(key, BlockDistribution.sum(data.get(key), bucket));
-  }
-  
-  void setMinStepSize(long minStepSize)
-  {
-    this.minStepSize = minStepSize;
-  }
-
-  /**
-   * Get the total unique bytes contained in the key-partitioned buckets.
-   * 
-   * @return The total number of bytes contained in the key-partitioned buckets.
-   */
-  public long length() {
-    return uniqueBytes;
-  }
-
-  /**
-   * Get the size of the key sampling.
-   * 
-   * @return Number of key samples.
-   */
-  public int size() {
-    return data.size();
-  }
-
-  /**
-   * Get the minimum split step size from all tables in union
-   */
-  public long getMinStepSize() {
-    return minStepSize;
-  }
- 
-  /** Get the list of sampling keys
-   * 
-   * @return A list of sampling keys
-   */
-  public RawComparable[] getKeys() {
-    RawComparable[] ret = new RawComparable[data.size()];
-    return data.keySet().toArray(ret);
-  }
-  
-  public BlockDistribution getBlockDistribution(RawComparable key) {
-    return data.get(key);
-  }
-  
-  /**
-   * Merge the key samples
-   * 
-   * Algorithm: select the smallest key from all clean source ranges and ranges subsequent to
-   *            respective dirty ranges. A dirty range is a range that has been partially needed
-   *            by one or more of the previous final ranges.  
-   *
-   * @param sourceKeys
-   *          key samples to be merged
-   * @return the merged key samples
-   */
-  public static KeyDistribution merge(KeyDistribution[] sourceKeys) throws IOException {
-    if (sourceKeys == null || sourceKeys.length == 0)
-      return null;
-    int srcSize = sourceKeys.length;
-    if (srcSize == 1)
-      return sourceKeys[0];
-    
-    Comparator<? super RawComparable> comp = sourceKeys[0].data.comparator();
-    // TODO check the identical comparators used in the source keys
-    /*
-    for (int i = 1; i < srcSize; i++)
-      if (!comp.equals(sourceKeys[i].data.comparator()))
-        throw new IOException("Incompatible sort keys found:" + comp.toString() + " vs. "+ sourceKeys[i].data.comparator().toString());
-     */
-    
-    KeyDistribution result = new KeyDistribution(comp);
-    
-    result.minStepSize = sourceKeys[0].minStepSize;
-    for (int i = 1; i < srcSize; i++)
-      if (result.minStepSize > sourceKeys[i].minStepSize)
-        result.minStepSize = sourceKeys[i].minStepSize;
-    
-    RawComparable[][] its = new RawComparable[srcSize][];
-    for (int i = 0; i < srcSize; i++)
-      its[i] = sourceKeys[i].getKeys();
-    RawComparable min, current;
-    int minIndex = -1;
-    int[] index = new int[srcSize];
-    boolean[] dirty = new boolean[srcSize];
-    while (true)
-    {
-      min = null;
-      BlockDistribution bd = new BlockDistribution();
-      for (int i = 0; i < srcSize; i++)
-      {
-        if (index[i] >= its[i].length)
-          continue;
-        current = its[i][index[i]];
-        bd.add(sourceKeys[i].getBlockDistribution(current));
-        if (min == null || comp.compare(min, current) > 0)
-        {
-          min = current;
-          minIndex = i;
-        }
-      }
-      if (min == null)
-        break;
-
-      result.add(min, bd);
-      for (int i = 0; i < srcSize; i++)
-      {
-        if (index[i] >= its[i].length)
-          continue;
-        current = its[i][index[i]];
-        if (i != minIndex)
-        {
-          if (comp.compare(min, current) != 0)
-          {
-            if (!dirty[i])
-            {
-              dirty[i] = true;
-              index[i]++;
-            } else if (comp.compare(min, its[i][index[i] - 1]) > 0 )
-              index[i]++;
-          } else {
-            if (dirty[i])
-              dirty[i] = false;
-            index[i]++;
-          }
-        } else {
-          if (dirty[i])
-            dirty[i] = false;
-          index[i]++;
-        }
-      }
-    }
-    return result;
-  }
-  
-  public int resize(BlockDistribution lastBd)
-  {
-    Iterator<Map.Entry<RawComparable, BlockDistribution>> it =
-      data.entrySet().iterator();
-    KeyDistribution adjusted = new KeyDistribution(data.comparator());
-    long realSize = 0, mySize = 0;
-    RawComparable key = null;
-    BlockDistribution bd = null, bd0 = null;
-    while (it.hasNext())
-    {
-      Map.Entry<RawComparable, BlockDistribution> mapEntry = it.next();
-      bd0 = mapEntry.getValue();
-      mySize = bd0.getLength();
-      if (realSize >= minStepSize/2 ||
-          (realSize + mySize >= minStepSize*ColumnGroup.SPLIT_SLOP && 
-              realSize >= minStepSize * (ColumnGroup.SPLIT_SLOP-1)))
-      {
-        adjusted.add(key, bd);
-        bd = null;
-        realSize = 0;
-      }
-      key = mapEntry.getKey();
-      realSize += mySize;
-      bd = BlockDistribution.sum(bd, bd0);
-    }
-    if (bd != null)
-    {
-      realSize += lastBd.getLength();
-      if (realSize >= minStepSize/2 || adjusted.size() == 0)
-      {
-         // the last plus would contain more than liked, don't merge them.
-        adjusted.add(key, bd);
-      } else
-        BlockDistribution.sum(lastBd, bd);
-    }
-    swap(adjusted);
-    return data.size();
-  }
-  
-  private void swap(KeyDistribution other) {
-    long tmp = minStepSize;
-    minStepSize = other.minStepSize;
-    other.minStepSize = tmp;
-    SortedMap<RawComparable, BlockDistribution> tmp2 = data;
-    data = other.data;
-    other.data = tmp2;
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/io/MetaFile.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/io/MetaFile.java
deleted file mode 100644
index d1ab66ba7..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/io/MetaFile.java
+++ /dev/null
@@ -1,256 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.io;
-
-import java.io.Closeable;
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.zebra.tfile.TFile;
-import org.apache.hadoop.zebra.tfile.MetaBlockAlreadyExists;
-import org.apache.hadoop.zebra.tfile.MetaBlockDoesNotExist;
-
-
-/**
- * simple named meta block management.
- * 
- * This implementation uses TFile as the storage layer.
- * 
- * TODO: Use an alternative design to allow writing of meta blocks from
- * multiple processes with DFS append feature.
- */
-class MetaFile {
-  // MetaFile is just a namespace object.
-  private MetaFile() {
-    // no-op
-  }
-
-  /**
-   * Create a MetaFile writer.
-   * 
-   * @param path
-   *          path to the meta file.
-   * @param conf
-   *          Configuration object
-   * @return a MataFile.Writer object
-   * @throws IOException
-   *           upon error
-   * 
-   *           The implementation does not always hold a reference to the
-   *           underlying file. In fact, the file is not created until the first
-   *           time a meta block is added. For this implementation, we use TFile
-   *           as storage backend, and only one process will ever succeed in
-   *           creating the file and adding meta blocks. So it is possible that
-   *           we successfully open the Writer initially, but receive a
-   *           file-already-exist error later.
-   */
-  static Writer createWriter(Path path, Configuration conf)
-      throws IOException {
-    return new Writer(path, conf);
-  }
-
-  /**
-   * Create a MetaFile reader.
-   * 
-   * @param path
-   *          path to the meta file.
-   * @param conf
-   *          Configuration object
-   * @return a MataFile.Reader object
-   * @throws IOException
-   *           upon error
-   * 
-   *           The implementation does not always hold a reference to the
-   *           underlying file. In fact, the file is opened when the first time
-   *           a meta block is requested. So it is possible that we successfully
-   *           open the reader, but receive a file-not-exist error later.
-   */
-  static Reader createReader(Path path, Configuration conf)
-      throws IOException {
-    return new Reader(path, conf);
-  }
-
-  /**
-   * Reader
-   */
-  static class Reader implements Closeable {
-    private Path path;
-    private Configuration conf;
-    private FSDataInputStream fsdis = null;
-    private TFile.Reader metaFile = null;
-
-    /**
-     * Remember the settings. Creation of TFile reader is lazy.
-     * 
-     * @param path
-     * @param conf
-     */
-    Reader(Path path, Configuration conf) {
-      this.path = path;
-      this.conf = conf;
-    }
-
-    private synchronized void checkFile() throws IOException {
-      if (metaFile != null) return;
-      FileSystem fs = path.getFileSystem(conf);
-      fsdis = fs.open(path);
-      metaFile = new TFile.Reader(fsdis, fs.getFileStatus(path).getLen(), conf);
-    }
-    
-    DataInputStream getMetaBlock(String name)
-        throws MetaBlockDoesNotExist, IOException {
-      checkFile();
-      return metaFile.getMetaBlock(name);
-    }
-
-    public void close() throws IOException {
-      try {
-        if (metaFile != null) {
-          metaFile.close();
-          metaFile = null;
-        }
-        if (fsdis != null) {
-          fsdis.close();
-          fsdis = null;
-        }
-      } finally {
-        if (metaFile != null) {
-          try {
-            metaFile.close();
-          } catch (Exception e) {
-            // no-op
-          }
-          metaFile = null;
-        }
-        
-        if (fsdis != null) {
-          try {
-            fsdis.close();
-          } catch (Exception e) {
-            // no-op
-          }
-          fsdis = null;
-        }
-      }
-    }
-  }
-
-  /**
-   * Writer
-   */
-  static class Writer implements Closeable {
-    private Path path;
-    private Configuration conf;
-    private FSDataOutputStream fsdos = null;
-    private TFile.Writer metaFile = null;
-
-    // Creation of TFile writer is lazy.
-    Writer(Path path, Configuration conf) {
-      this.path = path;
-      this.conf = conf;
-    }
-    
-    /**
-     * Actually opening the file if not opened yet.
-     * 
-     * @throws IOException
-     */
-    private synchronized void checkFile() throws IOException {
-      if (metaFile != null) return;
-
-      FileSystem fs = path.getFileSystem(conf);
-      // Throw an exception if the meta file already exists.
-      fsdos = fs.create(path, false);   
-      // TODO Move getMinBlockSize to BasicTable
-      // TODO move getCompression to BasicTable
-      metaFile =
-          new TFile.Writer(fsdos, ColumnGroup.getMinBlockSize(conf),
-              ColumnGroup.getCompression(conf), null, conf);
-    }
-
-    /**
-     * Close the Writer if it is opened. Generally, finish() should allow future
-     * writer to append more data. But in this implementation, we are not able
-     * to achieve this.
-     * 
-     * @throws IOException
-     */
-    void finish() throws IOException {
-      close();
-    }
-
-    /**
-     * Close the Writer if it is opened.
-     * 
-     * @throws IOException
-     */
-    public void close() throws IOException {
-      try {
-        if (metaFile != null) {
-          metaFile.close();
-          metaFile = null;
-        }
-        if (fsdos != null) {
-          fsdos.close();
-          fsdos = null;
-        }
-      } finally {
-        if (metaFile != null) {
-          try {
-            metaFile.close();
-          } catch (Exception e) {
-            // no-op
-          }
-          metaFile = null;
-        }
-        
-        if (fsdos != null) {
-          try {
-            fsdos.close();
-          } catch (Exception e) {
-            // no-op
-          }
-          fsdos = null;
-        }
-      }
-    }
-
-    /**
-     * Obtain an output stream for creating a Meta Block with the specific
-     * name. The first time it is called, the meta file will be created.
-     * 
-     * @param name
-     *          The name of the Meta Block
-     * @return The output stream. Close the stream to conclude the writing.
-     * @throws IOException
-     * @throws MetaBlockAlreadyExists
-     */
-    public DataOutputStream createMetaBlock(String name)
-        throws MetaBlockAlreadyExists, IOException {
-      checkFile();
-      return metaFile.prepareMetaBlock(name);
-    }
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/io/TableInserter.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/io/TableInserter.java
deleted file mode 100644
index 5d4e25940..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/io/TableInserter.java
+++ /dev/null
@@ -1,49 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.io;
-
-import java.io.Closeable;
-import java.io.IOException;
-
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.pig.data.Tuple;
-
-/**
- * Inserter interface allows application to to insert a number of rows into
- * table.
- */
-public interface TableInserter extends Closeable {
-  /**
-   * Insert a new row into the table.
-   * 
-   * @param key
-   *          The row key.
-   * @param row
-   *          The row.
-   */
-  public void insert(BytesWritable key, Tuple row) throws IOException;
-
-  /**
-   * Get the schema of the underlying table we are writing to.
-   * 
-   * @return The schema of the underlying table.
-   */
-  public Schema getSchema();
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/io/TableScanner.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/io/TableScanner.java
deleted file mode 100644
index dcfc990cf..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/io/TableScanner.java
+++ /dev/null
@@ -1,97 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.io;
-
-import java.io.Closeable;
-import java.io.IOException;
-
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.pig.data.Tuple;
-
-/**
- * Scanner interface allows application to scan a range of rows of a table. It
- * is the caller's responsibility to close() the scanner after finishing using
- * it. Scanner allows one to fetch the key without fetching the value to allow
- * underlying implementation to instantiating tuple objects (Row) lazily.
- */
-public interface TableScanner extends Closeable {
-  /**
-   * Test whether the cursor is at the end of the scan range.
-   * 
-   * @return Whether the cursor is at the end of the scan range.
-   * @throws IOException
-   */
-  boolean atEnd() throws IOException;
-
-  /**
-   * Advance cursor to the next Row.
-   * 
-   * @return true if the cursor is moved. It will return true when the scanner
-   *         moves the cursor from the last row to the end position.
-   * @throws IOException
-   */
-  boolean advance() throws IOException;
-
-  /**
-   * Get the row key.
-   * 
-   * @param key
-   *          The output parameter to hold the result.
-   */
-  void getKey(BytesWritable key) throws IOException;
-
-  /**
-   * Get the row.
-   * 
-   * @param row
-   *          The output parameter to hold the result. It must conform to the
-   *          schema that the scanner is aware of.
-   * @see TableScanner#getSchema()
-   * @throws IOException
-   */
-  void getValue(Tuple row) throws IOException;
-
-  /**
-   * Seek to the key that is greater or equal to the provided key, or we reach
-   * the end. It is only applicable to sorted tables.
-   * 
-   * @param key
-   *          The input key.
-   * @return true if we find the exact match; false otherwise.
-   * @throws IOException
-   */
-  boolean seekTo(BytesWritable key) throws IOException;
-
-  /**
-   * Seek to the end of the scan range.
-   * 
-   * @throws IOException
-   */
-  void seekToEnd() throws IOException;
-
-  /**
-   */
-  public String getProjection();
-  
-  /**
-   * Get the projection's schema
-   */
-  public Schema getSchema();
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/io/package-info.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/io/package-info.java
deleted file mode 100644
index 57746a0d4..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/io/package-info.java
+++ /dev/null
@@ -1,21 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-/**
- * Physical I/O management of Hadoop Zebra Tables.
- */
-package org.apache.hadoop.zebra.io;
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/BasicTableExpr.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/BasicTableExpr.java
deleted file mode 100644
index 590078f3e..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/BasicTableExpr.java
+++ /dev/null
@@ -1,213 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.IOException;
-import java.io.StringReader;
-import java.io.PrintStream;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.Projection;
-import org.apache.pig.data.Tuple;
-
-/**
- * Table expression for reading a BasicTable.
- * 
- * @see <a href="doc-files/examples/ReadABasicTable.java">Usage example for
- *      BasicTableExpr</a>
- */
-class BasicTableExpr extends TableExpr {
-  private Path path;
-
-  /**
-   * default constructor.
-   */
-  public BasicTableExpr() {
-    // no-op
-  }
-
-  /**
-   * Constructor.
-   * 
-   * @param path
-   *          Path of the BasicTable.
-   */
-  public BasicTableExpr(Path path) {
-    this.path = path;
-  }
-
-  /**
-   * Set the path.
-   * 
-   * @param path
-   *          path to the BasicTable.
-   * @return self.
-   */
-  public BasicTableExpr setPath(Path path) {
-    this.path = path;
-    return this;
-  }
-
-  /**
-   * Get the path.
-   * 
-   * @return the path to the BasicTable.
-   */
-  public Path getPath() {
-    return path;
-  }
-
-  @Override
-  protected BasicTableExpr decodeParam(StringReader in) throws IOException {
-    String strPath = TableExprUtils.decodeString(in);
-    if (strPath == null) {
-      throw new RuntimeException("Incomplete expression");
-    }
-    path = new Path(strPath);
-    return this;
-  }
-
-  @Override
-  protected BasicTableExpr encodeParam(StringBuilder out) {
-    if (path == null) {
-      throw new RuntimeException("Incomplete expression");
-    }
-    TableExprUtils.encodeString(out, path.toString());
-    return this;
-  }
-
-  @Override
-  public List<LeafTableInfo> getLeafTables(String projection) {
-    ArrayList<LeafTableInfo> ret = new ArrayList<LeafTableInfo>(1);
-    ret.add(new LeafTableInfo(path, projection));
-    return ret;
-  }
-
-  @Override
-  public TableScanner getScanner(BytesWritable begin, BytesWritable end,
-      String projection, Configuration conf) throws IOException {
-    String[] deletedCGs = getDeletedCGs(conf);
-    BasicTable.Reader reader = new BasicTable.Reader(path, deletedCGs, conf);
-    try {
-      reader.setProjection(projection);
-    } catch (ParseException e) {
-    	throw new IOException("Projection parsing failed : "+e.getMessage());
-    } 
-    return reader.getScanner(begin, end, true);
-  } 
-
-  @Override
-  public TableScanner getScanner(RowTableSplit split, String projection,
-      Configuration conf) throws IOException, ParseException {
-    return new BasicTableScanner(split, projection, conf);
-  }
-
-  @Override
-  public Schema getSchema(Configuration conf) throws IOException {
-    return BasicTable.Reader.getSchema(path, conf);
-  }
-
-  @Override
-  public boolean sortedSplitCapable() {
-    return true;
-  }
-  
-  @Override
-  protected void dumpInfo(PrintStream ps, Configuration conf, int indent) throws IOException
-  {
-    BasicTable.dumpInfo(path.toString(), ps, conf, indent);
-  }
-
-  /**
-   * Basic Table Scanner
-   */
-  class BasicTableScanner implements TableScanner {
-    private int tableIndex = -1;
-    private Integer[] virtualColumnIndices = null;
-    private TableScanner scanner = null;
-    
-    BasicTableScanner(RowTableSplit split, String projection,
-        Configuration conf) throws IOException, ParseException, ParseException {
-      tableIndex = split.getTableIndex();
-      virtualColumnIndices = Projection.getVirtualColumnIndices(projection);
-      BasicTable.Reader reader =
-        new BasicTable.Reader(new Path(split.getPath()), getDeletedCGs(conf), conf);
-      reader.setProjection(projection);
-      scanner = reader.getScanner(true, split.getSplit());
-    }
-    
-    @Override
-    public boolean advance() throws IOException {
-      return scanner.advance();
-    }
-    
-    @Override
-    public boolean atEnd() throws IOException {
-      return scanner.atEnd();
-    }
-    
-    @Override
-    public Schema getSchema() {
-      return scanner.getSchema();
-    }
-    
-    @Override
-    public void getKey(BytesWritable key) throws IOException {
-      scanner.getKey(key);
-    }
-    
-    @Override
-    public void getValue(Tuple row) throws IOException {
-      scanner.getValue(row);
-      if (virtualColumnIndices != null)
-      {
-        for (int i = 0; i < virtualColumnIndices.length; i++)
-        {
-          row.set(virtualColumnIndices[i], tableIndex);
-        }
-      }
-    }
-    
-    @Override
-    public boolean seekTo(BytesWritable key) throws IOException {
-      return scanner.seekTo(key);
-    }
-    
-    @Override
-    public void seekToEnd() throws IOException {
-      scanner.seekToEnd();
-    }
-    
-    @Override 
-    public void close() throws IOException {
-      scanner.close();
-    }
-    
-    @Override
-    public String getProjection() {
-      return scanner.getProjection();
-    }
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/BasicTableOutputFormat.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/BasicTableOutputFormat.java
deleted file mode 100644
index 48b2f2233..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/BasicTableOutputFormat.java
+++ /dev/null
@@ -1,730 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.IOException;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.util.StringUtils;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.RawComparator;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.Mapper;
-import org.apache.hadoop.mapred.OutputFormat;
-import org.apache.hadoop.mapred.RecordWriter;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.util.Progressable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Partition;
-import org.apache.hadoop.zebra.types.SortInfo;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.tfile.TFile;
-import org.apache.pig.data.Tuple;
-import org.apache.hadoop.zebra.pig.comparator.*;
-import org.apache.hadoop.util.ReflectionUtils;
-
-
-/**
- * {@link org.apache.hadoop.mapred.OutputFormat} class for creating a
- * BasicTable.
- * 
- * Usage Example:
- * <p>
- * In the main program, add the following code.
- * 
- * <pre>
- * jobConf.setOutputFormat(BasicTableOutputFormat.class);
- * Path outPath = new Path(&quot;path/to/the/BasicTable&quot;);
- * BasicTableOutputFormat.setOutputPath(jobConf, outPath);
- * BasicTableOutputFormat.setSchema(jobConf, &quot;Name, Age, Salary, BonusPct&quot;);
- * </pre>
- * 
- * The above code does the following things:
- * <UL>
- * <LI>Set the output format class to BasicTableOutputFormat.
- * <LI>Set the single path to the BasicTable to be created.
- * <LI>Set the schema of the BasicTable to be created. In this case, the
- * to-be-created BasicTable contains three columns with names "Name", "Age",
- * "Salary", "BonusPct".
- * </UL>
- * 
- * To create multiple output paths. ZebraOutputPartitoner interface needs to be implemented
- * <pre>
- * String multiLocs = &quot;commaSeparatedPaths&quot;    
- * jobConf.setOutputFormat(BasicTableOutputFormat.class);
- * BasicTableOutputFormat.setMultipleOutputPaths(jobConf, multiLocs);
- * jobConf.setOutputFormat(BasicTableOutputFormat.class);
- * BasicTableOutputFormat.setSchema(jobConf, &quot;Name, Age, Salary, BonusPct&quot;);
- * BasicTableOutputFormat.setZebraOutputPartitionClass(
- * 		jobConf, MultipleOutputsTest.OutputPartitionerClass.class);
- * </pre>
- * 
- * 
- * The user ZebraOutputPartitionClass should like this
- * 
- * <pre>
- * 
- *   static class OutputPartitionerClass implements ZebraOutputPartition {
- *   &#064;Override
- *	  public int getOutputPartition(BytesWritable key, Tuple value) {		 
- *
- *        return someIndexInOutputParitionlist0;
- *	  }
- * 
- * </pre>
- * 
- * 
- * The user Reducer code (or similarly Mapper code if it is a Map-only job)
- * should look like the following:
- * 
- * <pre>
- * static class MyReduceClass implements Reducer&lt;K, V, BytesWritable, Tuple&gt; {
- *   // keep the tuple object for reuse.
- *   Tuple outRow;
- *   // indices of various fields in the output Tuple.
- *   int idxName, idxAge, idxSalary, idxBonusPct;
- * 
- *   &#064;Override
- *   public void configure(JobConf job) {
- *     Schema outSchema = BasicTableOutputFormat.getSchema(job);
- *     // create a tuple that conforms to the output schema.
- *     outRow = TypesUtils.createTuple(outSchema);
- *     // determine the field indices.
- *     idxName = outSchema.getColumnIndex(&quot;Name&quot;);
- *     idxAge = outSchema.getColumnIndex(&quot;Age&quot;);
- *     idxSalary = outSchema.getColumnIndex(&quot;Salary&quot;);
- *     idxBonusPct = outSchema.getColumnIndex(&quot;BonusPct&quot;);
- *   }
- * 
- *   &#064;Override
- *   public void reduce(K key, Iterator&lt;V&gt; values,
- *       OutputCollector&lt;BytesWritable, Tuple&gt; output, Reporter reporter)
- *       throws IOException {
- *     String name;
- *     int age;
- *     int salary;
- *     double bonusPct;
- *     // ... Determine the value of the individual fields of the row to be inserted.
- *     try {
- *       outTuple.set(idxName, name);
- *       outTuple.set(idxAge, new Integer(age));
- *       outTuple.set(idxSalary, new Integer(salary));
- *       outTuple.set(idxBonusPct, new Double(bonusPct));
- *       output.collect(new BytesWritable(name.getBytes()), outTuple);
- *     }
- *     catch (ExecException e) {
- *       // should never happen
- *     }
- *   }
- * 
- *   &#064;Override
- *   public void close() throws IOException {
- *     // no-op
- *   }
- * 
- * }
- * </pre>
- * 
- * @Deprecated Use (@link org.apache.hadoop.zebra.mapreduce.BasicTableOutputFormat) instead
- */
-@Deprecated
-public class BasicTableOutputFormat implements
-    OutputFormat<BytesWritable, Tuple> {
-  private static final String OUTPUT_PATH = "mapred.lib.table.output.dir";
-  private static final String MULTI_OUTPUT_PATH = "mapred.lib.table.multi.output.dirs";
-  private static final String OUTPUT_SCHEMA = "mapred.lib.table.output.schema";
-  private static final String OUTPUT_STORAGEHINT =
-      "mapred.lib.table.output.storagehint";
-  private static final String OUTPUT_SORTCOLUMNS =
-      "mapred.lib.table.output.sortcolumns";
-  private static final String OUTPUT_COMPARATOR =
-      "mapred.lib.table.output.comparator";
-  static final String IS_MULTI = "multi";
-  private static final String ZEBRA_OUTPUT_PARTITIONER_CLASS = "zebra.output.partitioner.class";
-  
-  
-  /**
-   * Set the multiple output paths of the BasicTable in JobConf
-   * 
-   * @param conf
-   *          The JobConf object.
-   * @param commaSeparatedLocations
-   *          The comma separated output paths to the tables. 
-   *          The path must either not existent, or must be an empty directory.
-   * @param theClass
-   * 	      Zebra output partitoner class
-   */
-  
-  public static void setMultipleOutputs(JobConf conf, String commaSeparatedLocations, Class<? extends ZebraOutputPartition> theClass)
-	throws IOException {
-	  
-	conf.set(MULTI_OUTPUT_PATH, commaSeparatedLocations);
-
-	if(conf.getBoolean(IS_MULTI, true) == false) {
-      throw new IllegalArgumentException("Job has been setup as single output path");
-	}
-	conf.setBoolean(IS_MULTI, true);
-	setZebraOutputPartitionClass(conf, theClass);
-	  
-  }
-
-  /**
-   * Set the multiple output paths of the BasicTable in JobConf
-   * 
-   * @param conf
-   *          The JobConf object.
-   * @param paths
-   *          The list of paths 
-   *          The path must either not existent, or must be an empty directory.
-   * @param theClass
-   * 	      Zebra output partitioner class
-   */
-  
-  public static void setMultipleOutputs(JobConf conf, Class<? extends ZebraOutputPartition> theClass, Path... paths)
-	throws IOException {
-    FileSystem fs = FileSystem.get(conf);
-    Path path = paths[0].makeQualified(fs);
-    StringBuffer str = new StringBuffer(StringUtils.escapeString(path.toString()));
-    for(int i = 1; i < paths.length;i++) {
-      str.append(StringUtils.COMMA_STR);
-      path = paths[i].makeQualified(fs);
-      str.append(StringUtils.escapeString(path.toString()));
-    }	  
-	conf.set(MULTI_OUTPUT_PATH, str.toString());
-
-	if(conf.getBoolean(IS_MULTI, true) == false) {
-      throw new IllegalArgumentException("Job has been setup as single output path");
-	}
-	conf.setBoolean(IS_MULTI, true);
-	setZebraOutputPartitionClass(conf, theClass);
-	  
-  }
-  
-  
-  /**
-   * Set the multiple output paths of the BasicTable in JobConf
-   * 
-   * @param conf
-   *          The JobConf object.
-   * @return path
-   *          The comma separated output paths to the tables. 
-   *          The path must either not existent, or must be an empty directory.
-   */
-  
-  public static Path[] getOutputPaths(JobConf conf)
-	throws IOException {
-
-	Path[] result;
-	String paths = conf.get(MULTI_OUTPUT_PATH);
-	String path = conf.get(OUTPUT_PATH);
-	
-	if(paths != null && path != null) {
-		throw new IllegalArgumentException("Illegal output paths specs. Both multi and single output locs are set");
-	}	
-	
-	if(conf.getBoolean(IS_MULTI, false) == true) {	  
-	    if (paths == null || paths.equals("")) {
-	    	throw new IllegalArgumentException("Illegal multi output paths");
-	    }	    
-		String [] list = StringUtils.split(paths);
-	    result = new Path[list.length];
-	    for (int i = 0; i < list.length; i++) {
-	      result[i] = new Path(StringUtils.unEscapeString(list[i]));
-	    }
-	} else {
-	    if (path == null || path.equals("")) {
-	      throw new IllegalArgumentException("Cannot find output path");
-	    }	    
-		result = new Path[1];
-		result[0] = new Path(path);
-	}
-
-    return result;
-	  
-  }
-  
-  
-  private static void setZebraOutputPartitionClass(
-		  JobConf conf, Class<? extends ZebraOutputPartition> theClass) throws IOException {
-	    if (!ZebraOutputPartition.class.isAssignableFrom(theClass))
-	        throw new IOException(theClass+" not "+ZebraOutputPartition.class.getName());
-	      conf.set(ZEBRA_OUTPUT_PARTITIONER_CLASS, theClass.getName());
-  }
-  
-
-  public static Class<? extends ZebraOutputPartition> getZebraOutputPartitionClass(JobConf conf) throws IOException {
-
-	Class<?> theClass;	  
-    String valueString = conf.get(ZEBRA_OUTPUT_PARTITIONER_CLASS);
-    if (valueString == null)
-    	throw new IOException("zebra output partitioner class not found");
-    try {
-    	theClass = conf.getClassByName(valueString);
-    } catch (ClassNotFoundException e) {
-      throw new IOException(e);
-    }	  
-
-    if (theClass != null && !ZebraOutputPartition.class.isAssignableFrom(theClass))
-        throw new IOException(theClass+" not "+ZebraOutputPartition.class.getName());
-    else if (theClass != null)
-      return theClass.asSubclass(ZebraOutputPartition.class);
-    else
-      return null;
-    
-  }
-  
-  
-  
-  /**
-   * Set the output path of the BasicTable in JobConf
-   * 
-   * @param conf
-   *          The JobConf object.
-   * @param path
-   *          The output path to the table. The path must either not existent,
-   *          or must be an empty directory.
-   */
-  public static void setOutputPath(JobConf conf, Path path) {
-    conf.set(OUTPUT_PATH, path.toString());
-	if(conf.getBoolean(IS_MULTI, false) == true) {
-	      throw new IllegalArgumentException("Job has been setup as multi output paths");
-	}
-	conf.setBoolean(IS_MULTI, false);
-
-  }
-
-  /**
-   * Get the output path of the BasicTable from JobConf
-   * 
-   * @param conf
-   *          job conf
-   * @return The output path.
-   */
-  public static Path getOutputPath(JobConf conf) {
-    String path = conf.get(OUTPUT_PATH);
-    return (path == null) ? null : new Path(path);
-  }
-
-  /**
-   * Set the table schema in JobConf
-   * 
-   * @param conf
-   *          The JobConf object.
-   * @param schema
-   *          The schema of the BasicTable to be created. For the initial
-   *          implementation, the schema string is simply a comma separated list
-   *          of column names, such as "Col1, Col2, Col3".
-   * 
-   * @deprecated Use {@link #setStorageInfo(JobConf, ZebraSchema, ZebraStorageHint, ZebraSortInfo)} instead.
-   */
-  public static void setSchema(JobConf conf, String schema) {
-    conf.set(OUTPUT_SCHEMA, Schema.normalize(schema));
-  }
-  
-  /**
-   * Get the table schema in JobConf.
-   * 
-   * @param conf
-   *          The JobConf object.
-   * @return The output schema of the BasicTable. If the schema is not defined
-   *         in the conf object at the time of the call, null will be returned.
-   */
-  public static Schema getSchema(JobConf conf) throws ParseException {
-    String schema = conf.get(OUTPUT_SCHEMA);
-    if (schema == null) {
-      return null;
-    }
-    //schema = schema.replaceAll(";", ",");
-    return new Schema(schema);
-  }
-
-  private static KeyGenerator makeKeyBuilder(byte[] elems) {
-	    ComparatorExpr[] exprs = new ComparatorExpr[elems.length];
-	    for (int i = 0; i < elems.length; ++i) {
-	      exprs[i] = ExprUtils.primitiveComparator(i, elems[i]);
-	    }
-	    return new KeyGenerator(ExprUtils.tupleComparator(exprs));
-  }
-
-  /**
-   * Generates a zebra specific sort key generator which is used to generate BytesWritable key 
-   * Sort Key(s) are used to generate this object
-   * 
-   * @param conf
-   *          The JobConf object.
-   * @return Object of type zebra.pig.comaprator.KeyGenerator. 
-   *         
-   */
-  public static Object getSortKeyGenerator(JobConf conf) throws IOException, ParseException {
-
-    SortInfo sortInfo = getSortInfo(conf);
-    Schema schema     = getSchema(conf);
-    String[] sortColNames = sortInfo.getSortColumnNames();
-
-    byte[] types = new byte[sortColNames.length];
-    for(int i =0 ; i < sortColNames.length; ++i){
-      types[i] = schema.getColumn(sortColNames[i]).getType().pigDataType();
-    }
-    KeyGenerator builder = makeKeyBuilder(types);
-    return builder;
-
-  }
-
-
-  /**
-   * Generates a BytesWritable key for the input key
-   * using keygenerate provided. Sort Key(s) are used to generate this object
-   *
-   * @param builder
-   *         Opaque key generator created by getSortKeyGenerator() method
-   * @param t
-   *         Tuple to create sort key from
-   * @return ByteWritable Key 
-   *
-   */
-  public static BytesWritable getSortKey(Object builder, Tuple t) throws Exception {
-	  KeyGenerator kg = (KeyGenerator) builder;
-	  return kg.generateKey(t);
-  }
-
-
-
-
-  /**
-   * Set the table storage hint in JobConf, should be called after setSchema is
-   * called.
-   * <br> <br>
-   * 
-   * Note that the "secure by" feature is experimental now and subject to
-   * changes in the future.
-   *
-   * @param conf
-   *          The JobConf object.
-   * @param storehint
-   *          The storage hint of the BasicTable to be created. The format would
-   *          be like "[f1, f2.subfld]; [f3, f4]".
-   * 
-   * @deprecated Use {@link #setStorageInfo(JobConf, ZebraSchema, ZebraStorageHint, ZebraSortInfo)} instead.
-   */
-  public static void setStorageHint(JobConf conf, String storehint) throws ParseException, IOException {
-    String schema = conf.get(OUTPUT_SCHEMA);
-
-    if (schema == null)
-      throw new ParseException("Schema has not been set");
-
-    // for sanity check purpose only
-    new Partition(schema, storehint, null);
-
-    conf.set(OUTPUT_STORAGEHINT, storehint);
-  }
-  
-  /**
-   * Get the table storage hint in JobConf.
-   * 
-   * @param conf
-   *          The JobConf object.
-   * @return The storage hint of the BasicTable. If the storage hint is not
-   *         defined in the conf object at the time of the call, an empty string
-   *         will be returned.
-   */
-  public static String getStorageHint(JobConf conf) {
-    String storehint = conf.get(OUTPUT_STORAGEHINT);
-    return storehint == null ? "" : storehint;
-  }
-
-  /**
-   * Set the sort info
-   *
-   * @param conf
-   *          The JobConf object.
-   *          
-   * @param sortColumns
-   *          Comma-separated sort column names
-   *          
-   * @param comparatorClass
-   *          comparator class name; null for default
-   *
-   * @deprecated Use {@link #setStorageInfo(JobConf, ZebraSchema, ZebraStorageHint, ZebraSortInfo)} instead.
-   */
-  public static void setSortInfo(JobConf conf, String sortColumns, Class<? extends RawComparator<Object>> comparatorClass) {
-    conf.set(OUTPUT_SORTCOLUMNS, sortColumns);
-    if (comparatorClass != null)
-      conf.set(OUTPUT_COMPARATOR, TFile.COMPARATOR_JCLASS+comparatorClass.getName());
-  }
-
-  /**
-   * Set the sort info
-   *
-   * @param conf
-   *          The JobConf object.
-   *          
-   * @param sortColumns
-   *          Comma-separated sort column names
-   *          
-   * @deprecated Use {@link #setStorageInfo(JobConf, ZebraSchema, ZebraStorageHint, ZebraSortInfo)} instead.          
-   */
-  public static void setSortInfo(JobConf conf, String sortColumns) {
-	  conf.set(OUTPUT_SORTCOLUMNS, sortColumns);
-  }  
-
-  /**
-   * Set the table storage info including ZebraSchema, 
-   *
-   * @param conf
-   *          The JobConf object.
-   *          
-   * @param zSchema The ZebraSchema object containing schema information.
-   *  
-   * @param zStorageHint The ZebraStorageHint object containing storage hint information.
-   * 
-   * @param zSortInfo The ZebraSortInfo object containing sorting information.
-   *          
-   */
-  public static void setStorageInfo(JobConf conf, ZebraSchema zSchema, ZebraStorageHint zStorageHint, ZebraSortInfo zSortInfo) 
-    throws ParseException, IOException {
-    String schemaStr = null;
-    String storageHintStr = null;
-
-    /* validity check on schema*/
-    if (zSchema == null) {
-      throw new IllegalArgumentException("ZebraSchema object cannot be null.");
-    } else {
-      schemaStr = zSchema.toString();
-    }
-    
-    Schema schema = null;
-    try {
-      schema = new Schema(schemaStr);
-    } catch (ParseException e) {
-      throw new ParseException("[" + zSchema + "] " + " is not a valid schema string: " + e.getMessage());
-    }
-
-    /* validity check on storage hint*/
-    if (zStorageHint == null) {
-      storageHintStr = "";
-    } else {
-      storageHintStr = zStorageHint.toString();
-    }
-    
-    try {
-      new Partition(schemaStr, storageHintStr, null);
-    } catch (ParseException e) {
-      throw new ParseException("[" + zStorageHint + "] " + " is not a valid storage hint string: " + e.getMessage()  ); 
-    } catch (IOException e) {
-      throw new ParseException("[" + zStorageHint + "] " + " is not a valid storage hint string: " + e.getMessage()  );
-    }
-    
-    conf.set(OUTPUT_SCHEMA, schemaStr);
-    conf.set(OUTPUT_STORAGEHINT, storageHintStr);
-    
-    /* validity check on sort info if user specifies it */
-    if (zSortInfo != null) {
-      String sortColumnsStr = zSortInfo.getSortColumns();
-      String comparatorStr = zSortInfo.getComparator();      
-
-      /* Check existence of comparable class if user specifies it */
-      if (comparatorStr != null && comparatorStr != "") {
-        try {
-          conf.getClassByName(comparatorStr.substring(TFile.COMPARATOR_JCLASS.length()).trim());
-        } catch (ClassNotFoundException e) {
-          throw new IOException("comparator Class cannot be found : " + e.getMessage());        
-        }
-      }
-      
-      try {
-        SortInfo.parse(sortColumnsStr, schema, comparatorStr);
-      } catch (IOException e) {
-        throw new IOException("[" + sortColumnsStr + " + " + comparatorStr + "] " 
-            + "is not a valid sort configuration: " + e.getMessage());
-      }
-      
-      if (sortColumnsStr != null)
-        conf.set(OUTPUT_SORTCOLUMNS, sortColumnsStr);
-      if (comparatorStr != null)
-        conf.set(OUTPUT_COMPARATOR, comparatorStr);
-    }
-  }
-  
-  /**
-   * Get the SortInfo object 
-   *
-   * @param conf
-   *          The JobConf object.
-   * @return SortInfo object; null if the Zebra table is unsorted 
-   *
-   */
-  public static SortInfo getSortInfo(JobConf conf)throws IOException
-  {
-    String sortColumns = conf.get(OUTPUT_SORTCOLUMNS);
-    if (sortColumns == null)
-    	return null;
-    Schema schema = null;
-    try {
-      schema = getSchema(conf);
-    } catch (ParseException e) {
-    	throw new IOException("Schema parsing failure : "+e.getMessage());
-    }
-    if (schema == null)
-    	throw new IOException("Schema not defined");
-    String comparator = getComparator(conf);
-    return SortInfo.parse(sortColumns, schema, comparator);
-  }
-
-  /**
-   * Get the  comparator for sort columns
-   *
-   * @param conf
-   *          The JobConf object.
-   * @return  comparator String
-   *
-   */
-  private static String getComparator(JobConf conf)
-  {
-    return conf.get(OUTPUT_COMPARATOR);
-  }
-
-  /**
-   * Get the output table as specified in JobConf. It is useful for applications
-   * to add more meta data after all rows have been added to the table.
-   * 
-   * @param conf
-   *          The JobConf object.
-   * @return The output BasicTable.Writer object.
-   * @throws IOException
-   */
-  private static BasicTable.Writer[] getOutput(JobConf conf) throws IOException {
-    Path[] paths = getOutputPaths(conf);
-    BasicTable.Writer[] writers = new BasicTable.Writer[paths.length]; 
-    for(int i = 0; i < paths.length; i++) {
-      writers[i] = new BasicTable.Writer(paths[i], conf);
-    }
-    
-    return writers;
-  }
-
-  /**
-   * Note: we perform the Initialization of the table here. So we expect this to
-   * be called before
-   * {@link BasicTableOutputFormat#getRecordWriter(FileSystem, JobConf, String, Progressable)}
-   * 
-   * @see OutputFormat#checkOutputSpecs(FileSystem, JobConf)
-   */
-  @Override
-  public void checkOutputSpecs(FileSystem ignored, JobConf conf)
-      throws IOException {
-    String schema = conf.get(OUTPUT_SCHEMA);
-    if (schema == null) {
-      throw new IllegalArgumentException("Cannot find output schema");
-    }
-
-    String storehint, sortColumns, comparator;
-    storehint = getStorageHint(conf);
-    sortColumns = (getSortInfo(conf) == null ? null : SortInfo.toSortString(getSortInfo(conf).getSortColumnNames()));
-    comparator = getComparator(conf);
-
-    Path [] paths = getOutputPaths(conf);
-    for (Path path : paths) {
-      BasicTable.Writer writer =
-        new BasicTable.Writer(path, schema, storehint, sortColumns, comparator, conf);
-      writer.finish();
-    }
-  }
-
-  /**
-   * @see OutputFormat#getRecordWriter(FileSystem, JobConf, String,
-   *      Progressable)
-   */
-  @Override
-  public RecordWriter<BytesWritable, Tuple> getRecordWriter(FileSystem ignored,
-      JobConf conf, String name, Progressable progress) throws IOException {
-    String path = conf.get(OUTPUT_PATH);
-    return new TableRecordWriter(path, name, conf, progress);
-  }
-
-  /**
-   * Close the output BasicTable, No more rows can be added into the table. A
-   * BasicTable is not visible for reading until it is "closed".
-   * 
-   * @param conf
-   *          The JobConf object.
-   * @throws IOException
-   */
-  public static void close(JobConf conf) throws IOException {
-    BasicTable.Writer tables[] = getOutput(conf);
-    for(int i =0; i < tables.length; ++i) {
-        tables[i].close();    	
-    }
-  }
-}
-
-/**
- * Adaptor class for BasicTable RecordWriter.
- */
-class TableRecordWriter implements RecordWriter<BytesWritable, Tuple> {
-  private final TableInserter inserter[];
-  private final Progressable progress;
-  private org.apache.hadoop.zebra.mapred.ZebraOutputPartition op = null;
-
-  
-  public TableRecordWriter(String path, String name, JobConf conf,
-      Progressable progress) throws IOException {
-	
-	if(conf.getBoolean(BasicTableOutputFormat.IS_MULTI, false) == true) {	  
-      op = (org.apache.hadoop.zebra.mapred.ZebraOutputPartition) 
-    	  ReflectionUtils.newInstance(BasicTableOutputFormat.getZebraOutputPartitionClass(conf), conf);
-
-	}  
-    Path [] paths = BasicTableOutputFormat.getOutputPaths(conf);
-    inserter = new TableInserter[paths.length];
-    for(int i = 0; i < paths.length; ++i) {
-    	BasicTable.Writer writer =
-    		new BasicTable.Writer(paths[i], conf);
-    	this.inserter[i] = writer.getInserter(name, true);
-    }	
-	  
-    this.progress = progress;
-  }
-
-  @Override
-  public void close(Reporter reporter) throws IOException {
-    for(int i = 0; i < this.inserter.length; ++i) {  
-      inserter[i].close();
-    }  
-    reporter.progress();
-  }
-
-  @Override
-  public void write(BytesWritable key, Tuple value) throws IOException {
-	  
-	if(op != null ) {	  
-		int idx = op.getOutputPartition(key, value);
-		if(idx < 0 || (idx >= inserter.length)) {
-			throw new IllegalArgumentException("index returned by getOutputPartition is out of range");
-		}
-		inserter[idx].insert(key, value);
-	} else {
-		inserter[0].insert(key, value);
-	}
-	progress.progress();
-
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/CachedTableScanner.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/CachedTableScanner.java
deleted file mode 100644
index 59873f8b4..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/CachedTableScanner.java
+++ /dev/null
@@ -1,171 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.Closeable;
-import java.io.IOException;
-
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.types.Projection;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.Tuple;
-
-/**
- * Utility class for those interested in writing their own TableExpr classes.
- * This class encapsulates a regular table scanner and changes the way how keys
- * and rows are accessed (with an internal cache).
- */
-final class CachedTableScanner implements Closeable {
-  private BytesWritable key;
-  private Tuple row;
-  private boolean keyReady;
-  private boolean rowReady;
-  private int index;
-  private TableScanner scanner;
-
-  /**
-   * Constructor
-   * 
-   * @param scanner
-   *          The scanner to be encapsulated
-   * @throws IOException 
-   */
-  public CachedTableScanner(TableScanner scanner, int index) throws IOException {
-    key = new BytesWritable();
-    row = TypesUtils.createTuple(Projection.getNumColumns(scanner.getProjection()));
-    keyReady = false;
-    rowReady = false;
-    this.index = index;
-    this.scanner = scanner;
-  }
-
-  /**
-   * Get the key at cursor.
-   * 
-   * @return The key at cursor
-   * @throws IOException
-   */
-  public BytesWritable getKey() throws IOException {
-    if (!keyReady) {
-      scanner.getKey(key);
-      keyReady = true;
-    }
-    return key;
-  }
-
-  /**
-   * Get the value (Tuple) at cursor.
-   * 
-   * @return the value at cursor.
-   * @throws IOException
-   */
-  public Tuple getValue() throws IOException {
-    if (!rowReady) {
-      scanner.getValue(row);
-      rowReady = true;
-    }
-    return row;
-  }
-
-  /**
-   * Get the table index in a union
-   * 
-   * @return the table index in union
-   */
-  public int getIndex() {
-    return index;
-    
-  }
-  /**
-   * Seek to a row whose key is greater than or equal to the input key.
-   * 
-   * @param inKey
-   *          the input key.
-   * @return true if an exact matching key is found. false otherwise.
-   * @throws IOException
-   */
-  public boolean seekTo(BytesWritable inKey) throws IOException {
-    boolean ret = scanner.seekTo(inKey);
-    reset();
-    return ret;
-  }
-
-  /**
-   * Advance the cursor.
-   * 
-   * @return whether the cursor actually moves.
-   * @throws IOException
-   */
-  public boolean advance() throws IOException {
-    boolean ret = scanner.advance();
-    reset();
-    return ret;
-  }
-
-  private void reset() throws IOException {
-    row = TypesUtils.createTuple(Projection.getNumColumns(scanner.getProjection()));
-    keyReady = false;
-    rowReady = false;
-  }
-
-  /**
-   * Is cursor at end?
-   * 
-   * @return Whether cursor is at the end.
-   * @throws IOException
-   */
-  public boolean atEnd() throws IOException {
-    return scanner.atEnd();
-  }
-
-  /**
-   * Get the schema of the tuples returned from this scanner.
-   * 
-   * @return The schema of the tuples returned from this scanner.
-   */
-  public String getProjection() {
-    return scanner.getProjection();
-  }
-  
-  /**
-   * Get the projected schema
-   * @return The projected schema
-   */
-  public Schema getSchema() {
-    return scanner.getSchema();
-  }
-
-  /**
-   * Seek to the end of the scanner.
-   * 
-   * @throws IOException
-   */
-  public void seekToEnd() throws IOException {
-    reset();
-    scanner.seekToEnd();
-  }
-
-  /**
-   * Close the scanner, release all resources.
-   */
-  @Override
-  public void close() throws IOException {
-    scanner.close();
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/CompositeTableExpr.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/CompositeTableExpr.java
deleted file mode 100644
index 5e75f9c2f..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/CompositeTableExpr.java
+++ /dev/null
@@ -1,305 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.IOException;
-import java.io.PrintStream;
-import java.io.StringReader;
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Iterator;
-import java.util.LinkedHashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-import java.util.TreeMap;
-
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-
-
-/**
- * An intermediate class to build concrete composite table expression classes.
- * Lengthy names are chosen to avoid naming conflicts.
- */
-abstract class CompositeTableExpr extends TableExpr {
-  public final static int INDENT_UNIT = 2;
-  protected ArrayList<TableExpr> composite;
-
-  protected CompositeTableExpr() {
-    composite = new ArrayList<TableExpr>();
-  }
-
-  protected CompositeTableExpr(int n) {
-    composite = new ArrayList<TableExpr>(n);
-    for (int i = 0; i < n; ++i) {
-      composite.add(null);
-    }
-  }
-
-  protected CompositeTableExpr addCompositeTable(TableExpr expr) {
-    composite.add(expr);
-    return this;
-  }
-
-  protected CompositeTableExpr addCompositeTables(TableExpr[] exprs) {
-    composite.ensureCapacity(composite.size() + exprs.length);
-    for (TableExpr expr : exprs) {
-      composite.add(expr);
-    }
-    return this;
-  }
-
-  protected CompositeTableExpr addCompositeTables(
-      Collection<? extends TableExpr> exprs) {
-    composite.addAll(exprs);
-    return this;
-  }
-
-  protected CompositeTableExpr setCompositeTable(int i, TableExpr expr) {
-    composite.set(i, expr);
-    return this;
-  }
-
-  protected List<TableExpr> getCompositeTables() {
-    return composite;
-  }
-
-  protected TableExpr getCompositeTable(int i) {
-    TableExpr expr = composite.get(i);
-    if (expr == null) {
-      throw new RuntimeException("Incomplete Nary expression");
-    }
-
-    return expr;
-  }
-
-  @Override
-  protected CompositeTableExpr decodeParam(StringReader in) throws IOException {
-    composite.clear();
-    int n = TableExprUtils.decodeInt(in);
-    composite.ensureCapacity(n);
-    for (int i = 0; i < n; ++i) {
-      TableExpr expr = TableExpr.parse(in);
-      composite.add(expr);
-    }
-
-    return this;
-  }
-
-  @Override
-  protected CompositeTableExpr encodeParam(StringBuilder out) {
-    int n = composite.size();
-    TableExprUtils.encodeInt(out, n);
-    for (int i = 0; i < composite.size(); ++i) {
-      getCompositeTable(i).encode(out);
-    }
-
-    return this;
-  }
-
-  @Override
-  public List<LeafTableInfo> getLeafTables(String projection) {
-    ArrayList<LeafTableInfo> ret = new ArrayList<LeafTableInfo>();
-
-    int n = composite.size();
-    for (int i = 0; i < n; ++i) {
-      ret.addAll(getCompositeTable(i).getLeafTables(projection));
-    }
-
-    return ret;
-  }
-
-  protected static class RowMappingEntry {
-    final int rowIndex;
-    final int fieldIndex;
-
-    public RowMappingEntry(int ri, int fi) {
-      rowIndex = ri;
-      fieldIndex = fi;
-    }
-
-    public int getRowIndex() {
-      return rowIndex;
-    }
-
-    public int getFieldIndex() {
-      return fieldIndex;
-    }
-  }
-
-  protected static class InferredProjection {
-    /**
-     * projections on each table in the composite. It should be of the same
-     * dimension as composite.
-     */
-    final Schema[] subProjections;
-
-    /**
-     * The actual (adjusted) input projection.
-     */
-    final Schema projection;
-
-    /**
-     * For each column in the input projection, what is the corresponding sub
-     * table (row index) and the corresponding projected column (field index).
-     */
-    final RowMappingEntry[] colMapping;
-
-    InferredProjection(List<String>[] subProj, String[] proj,
-        Map<String, RowMappingEntry> colMap) throws ParseException {
-      subProjections = new Schema[subProj.length];
-      for (int i = 0; i < subProj.length; ++i) {
-        List<String> subProjection = subProj[i];
-        subProjections[i] =
-            new Schema(subProjection.toArray(new String[subProjection.size()]));
-      }
-      
-      projection = new Schema(proj);
-      
-      this.colMapping = new RowMappingEntry[proj.length];
-      
-      for (int i = 0; i < proj.length; ++i) {
-        colMapping[i] = colMap.get(proj[i]);
-      }
-    }
-
-    public Schema[] getSubProjections() {
-      return subProjections;
-    }
-
-    public RowMappingEntry[] getColMapping() {
-      return colMapping;
-    }
-
-    public Schema getProjection() {
-      return projection;
-    }
-  }
-
-  /**
-   * Given the input projection, infer the projections that should be applied on
-   * individual table in the composite.
-   * 
-   * @param projection
-   *          The expected projection
-   * @return The inferred projection with other information.
-   * @throws IOException
-   */
-  @SuppressWarnings("unchecked")
-  protected InferredProjection inferProjection(String projection,
-      Configuration conf)
-      throws IOException, ParseException {
-    int n = composite.size();
-    ArrayList<String>[] subProjections = new ArrayList[n];
-    for (int i = 0; i < n; ++i) {
-      subProjections[i] = new ArrayList<String>();
-    }
-    
-    String[] columns;
-    LinkedHashMap<String, Integer> colNameMap =
-        new LinkedHashMap<String, Integer>();
-    TreeMap<String, RowMappingEntry> colMapping =
-        new TreeMap<String, RowMappingEntry>();
-
-    /**
-     * Create a collection of all column names and the corresponding index to
-     * the composite.
-     */
-    for (int i = 0; i < n; ++i) {
-      String[] cols = getCompositeTable(i).getSchema(conf).getColumns();
-      for (int j = 0; j < cols.length; ++j) {
-        if (colNameMap.get(cols[j]) != null) {
-          colNameMap.put(cols[j], -1); // special marking on duplicated names.
-        }
-        else {
-          colNameMap.put(cols[j], i);
-        }
-      }
-    }
-
-    if (projection == null) {
-      Set<String> keySet = colNameMap.keySet();
-      columns = keySet.toArray(new String[keySet.size()]);
-    }
-    else {
-      columns = projection.trim().split(Schema.COLUMN_DELIMITER);
-    }
-
-    for (int i = 0; i < columns.length; ++i) {
-      String col = columns[i];
-
-      if (colMapping.get(col) != null) {
-        throw new IllegalArgumentException(
-            "Duplicate column names in projection");
-      }
-
-      Integer rowIndex = colNameMap.get(col);
-      if (rowIndex == null) {
-        colMapping.put(col, null);
-      }
-      else {
-        if (rowIndex < 0) {
-          // The column name appears in more than one table in composite.
-          throw new RuntimeException("Ambiguous column in projection: "
-              + col);
-        }
-        subProjections[rowIndex].add(columns[i]);
-        colMapping.put(col, new RowMappingEntry(rowIndex,
-            subProjections[rowIndex].size() - 1));
-      }
-    }
-
-    return new InferredProjection(subProjections, columns, colMapping);
-  }
-
-  @Override
-  public Schema getSchema(Configuration conf) throws IOException {
-    Schema result = new Schema();
-    for (Iterator<TableExpr> it = composite.iterator(); it.hasNext();) {
-      TableExpr e = it.next();
-      try {
-        result.unionSchema(e.getSchema(conf));
-      } catch (ParseException exc) {
-        throw new IOException("Schema parsing failed :"+exc.getMessage());
-      }
-    }
-    return result;
-  }
-  
-  @Override
-  public boolean sortedSplitCapable() {
-    for (Iterator<TableExpr> it = composite.iterator(); it.hasNext();) {
-      TableExpr e = it.next();
-      if (!e.sortedSplitCapable()) return false;
-    }
-
-    return true;
-  }
-  
-  @Override
-  protected void dumpInfo(PrintStream ps, Configuration conf, int indent) throws IOException
-  {
-    for (Iterator<TableExpr> it = composite.iterator(); it.hasNext();) {
-      TableExpr e = it.next();
-      e.dumpInfo(ps, conf, indent+INDENT_UNIT);
-    }
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/NullScanner.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/NullScanner.java
deleted file mode 100644
index d03c520cb..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/NullScanner.java
+++ /dev/null
@@ -1,89 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.EOFException;
-import java.io.IOException;
-
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Projection;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.pig.data.Tuple;
-
-/**
- * A scanner that contains no rows.
- */
-class NullScanner implements TableScanner {
-  String projection;
-
-  public NullScanner(String projection) {
-    this.projection = projection;
-  }
-  
-  @Override
-  public boolean advance() throws IOException {
-    return false;
-  }
-
-  @Override
-  public boolean atEnd() throws IOException {
-    return true;
-  }
-
-  @Override
-  public String getProjection() {
-    return projection;
-  }
-  
-  @Override
-  public Schema getSchema() {
-    Schema result;
-    try {
-       result = Projection.toSchema(projection);
-    } catch (ParseException e) {
-      throw new AssertionError("Invalid Projection: "+e.getMessage());
-    }
-    return result;
-  }
-
-  @Override
-  public void getKey(BytesWritable key) throws IOException {
-    throw new EOFException("No more rows to read");
-  }
-
-  @Override
-  public void getValue(Tuple row) throws IOException {
-    throw new EOFException("No more rows to read");
-  }
-
-  @Override
-  public boolean seekTo(BytesWritable key) throws IOException {
-    return false;
-  }
-
-  @Override
-  public void seekToEnd() throws IOException {
-    // Do nothing
-  }
-
-  @Override
-  public void close() throws IOException {
-    // Do nothing
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/TableExpr.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/TableExpr.java
deleted file mode 100644
index 8088697cc..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/TableExpr.java
+++ /dev/null
@@ -1,245 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.IOException;
-import java.io.StringReader;
-import java.io.PrintStream;
-import java.util.List;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-
-/**
- * Table Expression - expression to describe an input table.
- */
-abstract class TableExpr {
-  private boolean sorted = false;
-  /**
-   * Factory method to create a TableExpr from a string.
-   * 
-   * @param in
-   *          The string stream that is pointed at the beginning of the table
-   *          expression string to be parsed.
-   * @return The instantiated TableExpr object. The string stream will move past
-   *         the table expression string.
-   * @throws IOException
-   */
-  @SuppressWarnings("unchecked")
-  public static TableExpr parse(StringReader in) throws IOException {
-    String clsName = TableExprUtils.decodeString(in);
-    try {
-      Class<? extends TableExpr> tblExprCls =
-          (Class<? extends TableExpr>) Class.forName(clsName);
-      return tblExprCls.newInstance().decodeParam(in);
-    }
-    catch (Exception e) {
-      throw new RuntimeException("Failed to load class: " + e.toString());
-    }
-  }
-
-  /**
-   * Encode the expression to a string stream.
-   * 
-   * @param out
-   *          The string stream that we should encode the expression into.
-   */
-  public final void encode(StringBuilder out) {
-    TableExprUtils.encodeString(out, getClass().getName());
-    encodeParam(out);
-  }
-
-  /**
-   * Decode the parameters of the expression from the input stream.
-   * 
-   * @param in
-   *          The string stream that is pointed at the beginning of the encoded
-   *          parameters.
-   * @return Reference to itself, and the TableExpr that has its parameters set
-   *         from the string stream. The string stream will move past the
-   *         encoded parameters for this expression.
-   */
-  protected abstract TableExpr decodeParam(StringReader in) throws IOException;
-
-  /**
-   * Encode the parameters of the expression into the string stream.
-   * 
-   * @param out
-   *          The string stream that we write encoded parameters into.
-   * @return Reference to itself,
-   */
-  protected abstract TableExpr encodeParam(StringBuilder out);
-
-  /**
-   * Get a TableScanner from the TableExpr object. This method only needs to be
-   * implemented by table expressions that support sorted split.
-   * 
-   * @param begin
-   *          the Begin key (inclusive). Can be null, meaning starting from the
-   *          first row of the table.
-   * @param end
-   *          the End key (exclusive). Can be null, meaning scan to the last row
-   *          of the table.
-   * @param projection
-   *          The projection schema. It should never be null.
-   * @see Schema
-   * @return A TableScanner object.
-   */
-  public TableScanner getScanner(BytesWritable begin,
-      BytesWritable end, String projection, Configuration conf)
-      throws IOException {
-    return null;
-  }
-
-  /**
-   * Get a scanner with an unsorted split.
-   * 
-   * @param split
-   *          The range split.
-   * @param projection
-   *          The projection schema. It should never be null.
-   * @param conf
-   *          The configuration
-   * @return A table scanner.
-   * @throws IOException
-   */
-  public TableScanner getScanner(RowTableSplit split, String projection,
-      Configuration conf) throws IOException, ParseException {
-    return null;
-  }
-  
-  /**
-   * A leaf table corresponds to a materialized table. It is represented by the
-   * path to the BasicTable and the projection.
-   */
-  public static final class LeafTableInfo {
-    private final Path path;
-    private final String projection;
-
-    public LeafTableInfo(Path path, String projection) {
-      this.path = path;
-      this.projection = projection;
-    }
-
-    public Path getPath() {
-      return path;
-    }
-
-    public String getProjection() {
-      return projection;
-    }
-  }
-
-  /**
-   * Get the information of all leaf tables that will be accessed by this table
-   * expression.
-   * 
-   * @param projection
-   *          The projection that is applied to the table expression.
-   */
-  public abstract List<LeafTableInfo> getLeafTables(
-      String projection);
-
-  /**
-   * Get the schema of the table.
-   * 
-   * @param conf
-   *          The configuration object.
-   */
-  public abstract Schema getSchema(Configuration conf) throws IOException;
-
-  /**
-   * Does this expression requires sorted split? If yes, we require all
-   * underlying BasicTables to be sorted and we split by key sampling. If this
-   * method returns true, we expect sortedSplitCapable() also return true.
-   * 
-   * @return Whether this expression may only be split by key.
-   */
-  public boolean sortedSplitRequired() {
-    return sorted;
-  }
-
-  /**
-   * Set the requirement for sorted table
-   */
-  public void setSortedSplit() {
-    sorted = true;
-  }
-
-  /**
-   * Is this expression capable of sorted split? If false, getScanner() should
-   * return null; otherwise, getScanner() should return a valid Scanner object.
-   * 
-   * This function should be overridden by sub classes that is capable of sorted
-   * split. Note that this method should not perform any actual I/O operation,
-   * such as checking whether the leaf tables (BasicTables) is in fact sorted or
-   * not. When this method returns true, while at least one of the leaf tables
-   * is not sorted, an {@link IOException} will be thrown in split generation
-   * time.
-   * 
-   * @return Whether the "table view" represented by the expression is sorted
-   *         and is thus splittable by key (sorted split).
-   */
-  public boolean sortedSplitCapable() {
-    return false;
-  }
-
-  /**
-   * dump table info
-   */
-  public final void dumpInfo(PrintStream ps, Configuration conf) throws IOException
-  {
-    dumpInfo(ps, conf, 0);
-  }
-  
-  /**
-   * dump table info with indent
-   */
-  protected abstract void dumpInfo(PrintStream ps, Configuration conf, int indent) throws IOException;
-  
-  /**
-   * get the deleted cg for tables in union
-   * @param conf The Configuration object
-   * @return
-   */
-  protected final String[] getDeletedCGsPerUnion(Configuration conf) {
-    return getDeletedCGs(conf, TableInputFormat.DELETED_CG_SEPARATOR_PER_UNION);
-  }
-  
-  protected final String[] getDeletedCGs(Configuration conf) {
-    return getDeletedCGs(conf, BasicTable.DELETED_CG_SEPARATOR_PER_TABLE);
-  }
-  
-  private final String[] getDeletedCGs(Configuration conf, String separator) {
-    String[] deletedCGs = null;
-    String fe;
-    if ((fe = conf.get(TableInputFormat.INPUT_FE)) != null && fe.equals("true"))
-    {
-      String original = conf.get(TableInputFormat.INPUT_DELETED_CGS, null);
-      if (original == null)
-        deletedCGs = new String[0]; // empty array needed to indicate it is fe checked
-      else
-        deletedCGs = original.split(separator, -1);
-    }
-    return deletedCGs;
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/TableExprUtils.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/TableExprUtils.java
deleted file mode 100644
index 6ca51efa2..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/TableExprUtils.java
+++ /dev/null
@@ -1,116 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.IOException;
-import java.io.StringReader;
-
-/**
- * Utility methods for people interested in writing their own TableExpr classes.
- */
-class TableExprUtils {
-  private TableExprUtils() {
-    // prevent instantiation
-  }
-
-  /**
-   * Encode a string into a StringBuilder.
-   * 
-   * @param out
-   *          output string builder.
-   * @param s
-   *          String to be encoded
-   */
-  public static void encodeString(StringBuilder out, String s) {
-    if (s == null) {
-      encodeInt(out, null);
-      return;
-    }
-    
-    encodeInt(out, s.length());
-    out.append(s);
-  }
-  
-  /**
-   * Decode a string previously encoded through encodeString.
-   * 
-   * @param in
-   *          The input StringReader.
-   * @return The decoded string object.
-   * @throws IOException
-   */
-  public static String decodeString(StringReader in) throws IOException {
-    Integer len = decodeInt(in);
-    if (len == null) {
-      return null;
-    }
-
-    char[] chars = new char[len];
-    in.read(chars);
-    return new String(chars);
-  }
-  
-  public static void encodeLong(StringBuilder out, Long l) {
-    if (l != null) {
-      out.append(l);
-    }
-    out.append(':');
-  }
-
-  public static Long decodeLong(StringReader in) throws IOException {
-    int c = in.read();
-    if (c == ':') {
-      return null;
-    }
-    if (Character.isDigit(c) == false) {
-      throw new IllegalArgumentException("Bad encoded string");
-    }
-    StringBuilder sb = new StringBuilder();
-    sb.append((char) c);
-
-    boolean colonSeen = false;
-    while ((c = in.read()) != -1) {
-      if (c == ':') {
-        colonSeen = true;
-        break;
-      }
-      if (!Character.isDigit(c)) {
-        break;
-      }
-      sb.append((char) c);
-    }
-    if (colonSeen == false) {
-      throw new IllegalArgumentException("Bad encoded string");
-    }
-    return new Long(sb.toString());
-  }
-  
-  public static void encodeInt(StringBuilder out, Integer i) {
-    if (i == null) {
-      encodeLong(out, null);
-    }
-    else {
-      encodeLong(out, (long) i.intValue());
-    }
-  }
-
-  public static Integer decodeInt(StringReader in) throws IOException {
-    Long l = decodeLong(in);
-    if (l == null) return null;
-    return (int) l.longValue();
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/TableInputFormat.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/TableInputFormat.java
deleted file mode 100644
index d1ed1e456..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/TableInputFormat.java
+++ /dev/null
@@ -1,1121 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-import java.io.StringReader;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.PathFilter;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.WritableUtils;
-import org.apache.hadoop.zebra.tfile.RawComparable;
-import org.apache.hadoop.mapred.FileInputFormat;
-import org.apache.hadoop.mapred.FileSplit;
-import org.apache.hadoop.mapred.InputFormat;
-import org.apache.hadoop.mapred.InputSplit;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.InvalidInputException;
-import org.apache.hadoop.mapred.RecordReader;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.BasicTableStatus;
-import org.apache.hadoop.zebra.io.BlockDistribution;
-import org.apache.hadoop.zebra.io.KeyDistribution;
-import org.apache.hadoop.zebra.io.BasicTable.Reader;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RowSplit;
-import org.apache.hadoop.zebra.mapred.TableExpr.LeafTableInfo;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Projection;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.SortInfo;
-import org.apache.hadoop.zebra.tfile.TFile;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-
-/**
- * {@link org.apache.hadoop.mapred.InputFormat} class for reading one or more
- * BasicTables.
- * 
- * Usage Example:
- * <p>
- * In the main program, add the following code.
- * 
- * <pre>
- * jobConf.setInputFormat(TableInputFormat.class);
- * TableInputFormat.setInputPaths(jobConf, new Path(&quot;path/to/table1&quot;, new Path(&quot;path/to/table2&quot;);
- * TableInputFormat.setProjection(jobConf, &quot;Name, Salary, BonusPct&quot;);
- * </pre>
- * 
- * The above code does the following things:
- * <UL>
- * <LI>Set the input format class to TableInputFormat.
- * <LI>Set the paths to the BasicTables to be consumed by user's Mapper code.
- * <LI>Set the projection on the input tables. In this case, the Mapper code is
- * only interested in three fields: "Name", "Salary", "BonusPct". "Salary"
- * (perhaps for the purpose of calculating the person's total payout). If no
- * project is specified, then all columns from the input tables will be
- * retrieved. If input tables have different schemas, then the input contains
- * the union of all columns from all the input tables. Absent fields will be
- * left as nul in the input tuple.
- * </UL>
- * The user Mapper code should look like the following:
- * 
- * <pre>
- * static class MyMapClass implements Mapper&lt;BytesWritable, Tuple, K, V&gt; {
- *   // keep the tuple object for reuse.
- *   // indices of various fields in the input Tuple.
- *   int idxName, idxSalary, idxBonusPct;
- * 
- *   &#064;Override
- *   public void configure(JobConf job) {
- *     Schema projection = TableInputFormat.getProjection(job);
- *     // determine the field indices.
- *     idxName = projection.getColumnIndex(&quot;Name&quot;);
- *     idxSalary = projection.getColumnIndex(&quot;Salary&quot;);
- *     idxBonusPct = projection.getColumnIndex(&quot;BonusPct&quot;);
- *   }
- * 
- *   &#064;Override
- *   public void map(BytesWritable key, Tuple value, OutputCollector&lt;K, V&gt; output,
- *       Reporter reporter) throws IOException {
- *     try {
- *       String name = (String) value.get(idxName);
- *       int salary = (Integer) value.get(idxSalary);
- *       double bonusPct = (Double) value.get(idxBonusPct);
- *       // do something with the input data
- *     } catch (ExecException e) {
- *       e.printStackTrace();
- *     }
- *   }
- * 
- *   &#064;Override
- *   public void close() throws IOException {
- *     // no-op
- *   }
- * }
- * </pre>
- * 
- * A little bit more explanation on the PIG {@link Tuple} objects. A Tuple is an
- * ordered list of PIG datum objects. The permitted PIG datum types can be
- * categorized as Scalar types and Composite types.
- * <p>
- * Supported Scalar types include seven native Java types: Boolean, Byte,
- * Integer, Long, Float, Double, String, as well as one PIG class called
- * {@link DataByteArray} that represents type-less byte array.
- * <p>
- * Supported Composite types include:
- * <UL>
- * <LI>{@link Map} : It is the same as Java Map class, with the additional
- * restriction that the key-type must be one of the scalar types PIG recognizes,
- * and the value-type any of the scaler or composite types PIG understands.
- * <LI>{@link DataBag} : A DataBag is a collection of Tuples.
- * <LI>{@link Tuple} : Yes, Tuple itself can be a datum in another Tuple.
- * </UL>
- * 
- * @Deprecated Use (@link org.apache.hadoop.zebra.mapreduce.TableInputFormat) instead
- */
-@Deprecated
-public class TableInputFormat implements InputFormat<BytesWritable, Tuple> {
-  static Log LOG = LogFactory.getLog(TableInputFormat.class);
-  
-  public static final String INPUT_EXPR = "mapred.lib.table.input.expr";
-  public static final String INPUT_PROJ = "mapred.lib.table.input.projection";
-  public static final String INPUT_SORT = "mapred.lib.table.input.sort";
-  public static final String INPUT_FE = "mapred.lib.table.input.fe";
-  public static final String INPUT_DELETED_CGS = "mapred.lib.table.input.deleted_cgs";
-  static final String DELETED_CG_SEPARATOR_PER_UNION = ";";
-
-  /**
-   * Set the paths to the input table.
-   * 
-   * @param conf
-   *          JobConf object.
-   * @param paths
-   *          one or more paths to BasicTables. The InputFormat class will
-   *          produce splits on the "union" of these BasicTables.
-   */
-  public static void setInputPaths(JobConf conf, Path... paths) {
-    if (paths.length < 1) {
-      throw new IllegalArgumentException("Requring at least one input path");
-    }
-    if (paths.length == 1) {
-      setInputExpr(conf, new BasicTableExpr(paths[0]));
-    }
-    else {
-      TableUnionExpr expr = new TableUnionExpr();
-      for (Path path : paths) {
-        expr.add(new BasicTableExpr(path));
-      }
-      setInputExpr(conf, expr);
-    }
-  }
-  
-  /**
-   * Set the input expression in the JobConf object.
-   * 
-   * @param conf
-   *          JobConf object.
-   * @param expr
-   *          The input table expression.
-   */
-  static void setInputExpr(JobConf conf, TableExpr expr) {
-    StringBuilder out = new StringBuilder();
-    expr.encode(out);
-    conf.set(INPUT_EXPR, out.toString());
-  }
-
-  static TableExpr getInputExpr(JobConf conf) throws IOException {
-    String expr = conf.get(INPUT_EXPR);
-    if (expr == null) {
-      // try setting from input path
-      Path[] paths = FileInputFormat.getInputPaths(conf);
-      if (paths != null) {
-        setInputPaths(conf, paths);
-      }
-      expr = conf.get(INPUT_EXPR);
-    }
-      
-    if (expr == null) {
-      throw new IllegalArgumentException("Input expression not defined.");
-    }
-    StringReader in = new StringReader(expr);
-    return TableExpr.parse(in);
-  }
-  
-  /**
-   * Get the schema of a table expr
-   * 
-   * @param conf
-   *          JobConf object.
-   *                    
-   */
-  public static Schema getSchema(JobConf conf) throws IOException
-  {
-	  TableExpr expr = getInputExpr(conf);
-	  return expr.getSchema(conf);
-  }  
-  
-  /**
-   * Set the input projection in the JobConf object.
-   * 
-   * @param conf
-   *          JobConf object.
-   * @param projection
-   *          A common separated list of column names. If we want select all
-   *          columns, pass projection==null. The syntax of the projection
-   *          conforms to the {@link Schema} string.
-   * @deprecated Use {@link #setProjection(JobConf, ZebraProjection)} instead.
-   */
-  public static void setProjection(JobConf conf, String projection) throws ParseException {
-    conf.set(INPUT_PROJ, Schema.normalize(projection));
-  }
-  
-  /**
-   * Set the input projection in the JobConf object.
-   * 
-   * @param conf
-   *          JobConf object.
-   * @param projection
-   *          A common separated list of column names. If we want select all
-   *          columns, pass projection==null. The syntax of the projection
-   *          conforms to the {@link Schema} string.
-   *
-   */
-  public static void setProjection(JobConf conf, ZebraProjection projection) throws ParseException {
-    /* validity check on projection */
-    Schema schema = null;
-    String normalizedProjectionString = Schema.normalize(projection.toString());
-    try {
-      schema = getSchema(conf);
-      new org.apache.hadoop.zebra.types.Projection(schema, normalizedProjectionString);
-    } catch (ParseException e) {
-      throw new ParseException("[" + projection + "] " + "is not a valid Zebra projection string " + e.getMessage());
-    } catch (IOException e) {
-      throw new ParseException("[" + projection + "] " + "is not a valid Zebra projection string " + e.getMessage());
-    }
-    
-    conf.set(INPUT_PROJ, normalizedProjectionString);
-  }  
-
-  /**
-   * Get the projection from the JobConf
-   * 
-   * @param conf
-   *          The JobConf object
-   * @return The projection schema. If projection has not been defined, or is
-   *         not known at this time, null will be returned. Note that by the time
-   *         when this method is called in Mapper code, the projection must
-   *         already be known.
-   * @throws IOException
-   *  
-   */
-  public static String getProjection(JobConf conf) throws IOException, ParseException {
-    String strProj = conf.get(INPUT_PROJ);
-    // TODO: need to be revisited
-    if (strProj != null) return strProj;
-    TableExpr expr = getInputExpr(conf);
-    if (expr != null) {
-      return expr.getSchema(conf).toProjectionString();
-    }
-    return null;
-  }
-      
-  /**
-   * Set requirement for sorted table
-   *
-   *@param conf
-   *          JobConf object.
-   */
-  private static void setSorted(JobConf conf) {
-    conf.setBoolean(INPUT_SORT, true);
-  }
-  
-  /**
-   * Get the SortInfo object regarding a Zebra table
-   *
-   * @param conf
-   *          JobConf object
-   * @return the zebra tables's SortInfo; null if the table is unsorted.
-   */
-  public static SortInfo getSortInfo(JobConf conf) throws IOException
-  {
-	  TableExpr expr = getInputExpr(conf);
-	  SortInfo result = null;
-	  int sortSize = 0;
-	  if (expr instanceof BasicTableExpr)
-    {
-      BasicTable.Reader reader = new BasicTable.Reader(((BasicTableExpr) expr).getPath(), conf);
-      SortInfo sortInfo = reader.getSortInfo();
-      reader.close();
-      result = sortInfo;
-	  } else {
-      List<LeafTableInfo> leaves = expr.getLeafTables(null);
-      for (Iterator<LeafTableInfo> it = leaves.iterator(); it.hasNext(); )
-      {
-        LeafTableInfo leaf = it.next();
-        BasicTable.Reader reader = new BasicTable.Reader(leaf.getPath(), conf);
-        SortInfo sortInfo = reader.getSortInfo();
-        reader.close();
-        if (sortSize == 0)
-        {
-          sortSize = sortInfo.size();
-          result = sortInfo;
-        } else if (sortSize != sortInfo.size()) {
-          throw new IOException("Tables of the table union do not possess the same sort property.");
-        }
-		  }
-	  }
-	  return result;
-  }
-
-  /**
-   * Requires sorted table or table union
-   * 
-   * @param conf
-   *          JobConf object.
-   * @param sortInfo
-   *          ZebraSortInfo object containing sorting information.
-   *        
-   */
-  public static void requireSortedTable(JobConf conf, ZebraSortInfo sortInfo) throws IOException {
-	 TableExpr expr = getInputExpr(conf);
-	 String comparatorName = null;
- 	 String[] sortcolumns = null;
-         if (sortInfo != null)
-         {
-           comparatorName = TFile.COMPARATOR_JCLASS+sortInfo.getComparator();
-           String sortColumnNames = sortInfo.getSortColumns();
-           if (sortColumnNames != null)
-             sortcolumns =  sortColumnNames.trim().split(SortInfo.SORTED_COLUMN_DELIMITER);
-           if (sortcolumns == null)
-             throw new IllegalArgumentException("No sort columns specified.");
-         }
-
-	 if (expr instanceof BasicTableExpr)
-	 {
-		 BasicTable.Reader reader = new BasicTable.Reader(((BasicTableExpr) expr).getPath(), conf);
-		 SortInfo mySortInfo = reader.getSortInfo();
-
-		 reader.close();
-		 if (mySortInfo == null)
-       throw new IOException("The table is not sorted");
-		 if (comparatorName == null)
-			 // cheat the equals method's comparator comparison
-			 comparatorName = mySortInfo.getComparator();
-		 if (sortcolumns != null && !mySortInfo.equals(sortcolumns, comparatorName))
-		 {
-			 throw new IOException("The table is not properly sorted");
-		 }
-    } else {
-		 List<LeafTableInfo> leaves = expr.getLeafTables(null);
-		 for (Iterator<LeafTableInfo> it = leaves.iterator(); it.hasNext(); )
-		 {
-			 LeafTableInfo leaf = it.next();
-			 BasicTable.Reader reader = new BasicTable.Reader(leaf.getPath(), conf);
-			 SortInfo mySortInfo = reader.getSortInfo();
-			 reader.close();
-			 if (mySortInfo == null)
-			   throw new IOException("The table is not sorted");
-			 if (comparatorName == null)
-				 comparatorName = mySortInfo.getComparator(); // use the first table's comparator as comparison base
-			 if (sortcolumns == null)
-       {
-         sortcolumns = mySortInfo.getSortColumnNames();
-         comparatorName = mySortInfo.getComparator();
-       } else {
-         if (!mySortInfo.equals(sortcolumns, comparatorName))
-         {
-           throw new IOException("The table is not properly sorted");
-         }
-       }
-		 }
-	 }
-    // need key range input splits for sorted table union
-    setSorted(conf);
-  }
-  
-  /**
-   * Get requirement for sorted table
-   *
-   *@param conf
-   *          JobConf object.
-   */
-  private static boolean getSorted(JobConf conf) {
-    return conf.getBoolean(INPUT_SORT, false);
-  }
-
-  /**
-   * @see InputFormat#getRecordReader(InputSplit, JobConf, Reporter)
-   */
-  @Override
-  public RecordReader<BytesWritable, Tuple> getRecordReader(InputSplit split,
-      JobConf conf, Reporter reporter) throws IOException {        
-    TableExpr expr = getInputExpr(conf);
-    if (expr == null) {
-      throw new IOException("Table expression not defined");
-    }
-
-    if (getSorted(conf))
-      expr.setSortedSplit();
-
-    String strProj = conf.get(INPUT_PROJ);
-    
-    String projection = null;
-    try {
-      if (strProj == null) {
-        projection = expr.getSchema(conf).toProjectionString();
-        TableInputFormat.setProjection(conf, projection);
-      } else {
-        projection = strProj;
-      }
-    } catch (ParseException e) {
-    	throw new IOException("Projection parsing failed : "+e.getMessage());
-    }
-
-    try {
-      return new TableRecordReader(expr, projection, split, conf);
-    } catch (ParseException e) {
-    	throw new IOException("Projection parsing faile : "+e.getMessage());
-    }
-  }
-  
-  /**
-   * Get a TableRecordReader on a single split
-   * 
-   * @param conf
-   *          JobConf object.
-   * @param projection
-   *          comma-separated column names in projection. null means all columns in projection
-   */
-  
-  public static TableRecordReader getTableRecordReader(JobConf conf, String projection) throws IOException, ParseException
-  {
-	// a single split is needed
-    if (projection != null)
-    	setProjection(conf, projection);
-    TableInputFormat inputFormat = new TableInputFormat();
-    InputSplit[] splits = inputFormat.getSplits(conf, 1);
-    return (TableRecordReader) inputFormat.getRecordReader(splits[0], conf, Reporter.NULL);
-  }
-
-  private static InputSplit[] getSortedSplits(JobConf conf, int numSplits,
-      TableExpr expr, List<BasicTable.Reader> readers,
-      List<BasicTableStatus> status) throws IOException {
-
-    if (expr.sortedSplitRequired() && !expr.sortedSplitCapable()) {
-      throw new IOException("Unable to created sorted splits");
-    }
-
-    long totalBytes = 0;
-    for (Iterator<BasicTableStatus> it = status.iterator(); it.hasNext();) {
-      BasicTableStatus s = it.next();
-      totalBytes += s.getSize();
-    }
-
-    long maxSplits = totalBytes / getMinSplitSize(conf);
-
-    if (maxSplits == 0)
-      numSplits = 1;
-    else if (numSplits > maxSplits) {
-      numSplits = -1;
-    }
-
-    ArrayList<InputSplit> splits = new ArrayList<InputSplit>();
-
-    for (Iterator<BasicTable.Reader> it = readers.iterator(); it.hasNext();) {
-      BasicTable.Reader reader = it.next();
-      if (!reader.isSorted()) {
-        throw new IOException("Attempting sorted split on unsorted table");
-      }
-    }
-
-    if (numSplits == 1) {
-      BlockDistribution bd = null;
-      for (Iterator<BasicTable.Reader> it = readers.iterator(); it.hasNext();) {
-        BasicTable.Reader reader = it.next();
-        bd = BlockDistribution.sum(bd, reader.getBlockDistribution((RangeSplit) null));
-      }
-      
-      SortedTableSplit split = new SortedTableSplit(null, null, bd, conf);
-      return new InputSplit[] { split };
-    }
-    
-    // TODO: Does it make sense to interleave keys for all leaf tables if
-    // numSplits <= 0 ?
-    int nLeaves = readers.size();
-    BlockDistribution lastBd = new BlockDistribution();
-    ArrayList<KeyDistribution> btKeyDistributions = new ArrayList<KeyDistribution>();
-    for (int i = 0; i < nLeaves; ++i) {
-      KeyDistribution btKeyDistri =
-          readers.get(i).getKeyDistribution(
-              (numSplits <= 0) ? -1 :
-              Math.max(numSplits * 5 / nLeaves, numSplits), nLeaves, lastBd);
-      btKeyDistributions.add(btKeyDistri);
-    }
-    int btSize = btKeyDistributions.size();
-    KeyDistribution[] btKds = new KeyDistribution[btSize];
-    Object[] btArray = btKeyDistributions.toArray();
-    for (int i = 0; i < btSize; i++)
-      btKds[i] = (KeyDistribution) btArray[i];
-    
-    KeyDistribution keyDistri = KeyDistribution.merge(btKds);
-
-    if (keyDistri == null) {
-      // should never happen.
-      SortedTableSplit split = new SortedTableSplit(null, null, null, conf);
-      return new InputSplit[] { split };
-    }
-    
-    keyDistri.resize(lastBd);
-    
-    RawComparable[] keys = keyDistri.getKeys();
-    for (int i = 0; i <= keys.length; ++i) {
-      RawComparable begin = (i == 0) ? null : keys[i - 1];
-      RawComparable end = (i == keys.length) ? null : keys[i];
-      BlockDistribution bd;
-      if (i < keys.length)
-        bd = keyDistri.getBlockDistribution(keys[i]);
-      else
-        bd = lastBd;
-      BytesWritable beginB = null, endB = null;
-      if (begin != null)
-        beginB = new BytesWritable(begin.buffer());
-      if (end != null)
-        endB = new BytesWritable(end.buffer());
-      SortedTableSplit split = new SortedTableSplit(beginB, endB, bd, conf);
-      splits.add(split);
-    }
-
-    return splits.toArray(new InputSplit[splits.size()]);
-  }
-  
-  static long getMinSplitSize(JobConf conf) {
-    return conf.getLong("table.input.split.minSize", 1 * 1024 * 1024L);
-  }
-
-  /**
-   * Set the minimum split size.
-   * 
-   * @param conf
-   *          The job conf object.
-   * @param minSize
-   *          Minimum size.
-   */
-  public static void setMinSplitSize(JobConf conf, long minSize) {
-    conf.setLong("table.input.split.minSize", minSize);
-  }
-  
-  private static class DummyFileInputFormat extends FileInputFormat<BytesWritable, Tuple> {
-    /**
-     * the next constant and class are copies from FileInputFormat
-     */
-    private static final PathFilter hiddenFileFilter = new PathFilter(){
-        public boolean accept(Path p){
-          String name = p.getName(); 
-          return !name.startsWith("_") && !name.startsWith("."); 
-        }
-      }; 
-
-    /**
-     * Proxy PathFilter that accepts a path only if all filters given in the
-     * constructor do. Used by the listPaths() to apply the built-in
-     * hiddenFileFilter together with a user provided one (if any).
-     */
-    private static class MultiPathFilter implements PathFilter {
-      private List<PathFilter> filters;
-
-      public MultiPathFilter(List<PathFilter> filters) {
-        this.filters = filters;
-      }
-
-      public boolean accept(Path path) {
-        for (PathFilter filter : filters) {
-          if (!filter.accept(path)) {
-            return false;
-          }
-        }
-        return true;
-      }
-    }
-    private Integer[] fileNumbers = null;
-
-    private List<BasicTable.Reader> readers;
-
-    public Integer[] getFileNumbers() {
-      return fileNumbers;
-    }
-
-    public DummyFileInputFormat(long minSplitSize, List<BasicTable.Reader> readers) {
-      super.setMinSplitSize(minSplitSize);
-      this.readers = readers;
-    }
-    
-    @Override
-    public RecordReader<BytesWritable, Tuple> getRecordReader(InputSplit split,
-        JobConf conf, Reporter reporter) throws IOException {
-      // no-op
-      return null;
-    }
-
-    @Override
-    public long computeSplitSize(long goalSize, long minSize, long blockSize) {
-      return super.computeSplitSize(goalSize, minSize, blockSize);
-    }
-
-    /**
-     * copy from FileInputFormat: add assignment to table file numbers
-     */
-    @Override
-    public FileStatus[] listStatus(JobConf job) throws IOException {
-      Path[] dirs = getInputPaths(job);
-      if (dirs.length == 0) {
-        throw new IOException("No input paths specified in job");
-      }
-
-      List<FileStatus> result = new ArrayList<FileStatus>();
-      List<IOException> errors = new ArrayList<IOException>();
-      
-      // creates a MultiPathFilter with the hiddenFileFilter and the
-      // user provided one (if any).
-      List<PathFilter> filters = new ArrayList<PathFilter>();
-      filters.add(hiddenFileFilter);
-      PathFilter jobFilter = getInputPathFilter(job);
-      if (jobFilter != null) {
-        filters.add(jobFilter);
-      }
-      PathFilter inputFilter = new MultiPathFilter(filters);
-
-      ArrayList<Integer> fileNumberList  = new ArrayList<Integer>();
-      int index = 0;
-      for (Path p: dirs) {
-        FileSystem fs = p.getFileSystem(job); 
-        FileStatus[] matches = fs.globStatus(p, inputFilter);
-        if (matches == null) {
-          errors.add(new IOException("Input path does not exist: " + p));
-        } else if (matches.length == 0) {
-          errors.add(new IOException("Input Pattern " + p + " matches 0 files"));
-        } else {
-          for (FileStatus globStat: matches) {
-            if (globStat.isDir()) {
-              FileStatus[] fileStatuses = fs.listStatus(globStat.getPath(), inputFilter);
-              // reorder according to CG index
-              BasicTable.Reader reader = readers.get(index);
-              reader.rearrangeFileIndices(fileStatuses);
-              for(FileStatus stat: fileStatuses) {
-                if (stat != null)
-                  result.add(stat);
-              }
-              fileNumberList.add(fileStatuses.length);
-            } else {
-              result.add(globStat);
-              fileNumberList.add(1);
-            }
-          }
-        }
-        index++;
-      }
-      fileNumbers = new Integer[fileNumberList.size()];
-      fileNumberList.toArray(fileNumbers);
-
-      if (!errors.isEmpty()) {
-        throw new InvalidInputException(errors);
-      }
-      LOG.info("Total input paths to process : " + result.size()); 
-      return result.toArray(new FileStatus[result.size()]);
-    }
-  }
-  
-  private static InputSplit[] getRowSplits(JobConf conf, int numSplits,
-      TableExpr expr, List<BasicTable.Reader> readers,
-      List<BasicTableStatus> status) throws IOException {
-    ArrayList<InputSplit> ret = new ArrayList<InputSplit>();
-
-    long minSplitSize = getMinSplitSize(conf);
-  
-    long minSize = Math.max(conf.getLong("mapred.min.split.size", 1), minSplitSize);
-    long totalBytes = 0;
-    for (Iterator<BasicTableStatus> it = status.iterator(); it.hasNext(); )
-    {
-      totalBytes += it.next().getSize();
-    }
-    long goalSize = totalBytes / (numSplits < 1 ? 1 : numSplits);
-    StringBuilder sb = new StringBuilder();
-    boolean first = true;
-    PathFilter filter = null;
-    List<BasicTable.Reader> realReaders = new ArrayList<BasicTable.Reader>();
-    int[] realReaderIndices = new int[readers.size()];
-
-    for (int i = 0; i < readers.size(); ++i) {
-      BasicTable.Reader reader = readers.get(i);
-      /* Get the index of the column group that will be used for row-split.*/
-      int splitCGIndex = reader.getRowSplitCGIndex();
-      
-      /* We can create input splits only if there does exist a valid column group for split.
-       * Otherwise, we do not create input splits. */
-      if (splitCGIndex >= 0) {        
-        realReaderIndices[realReaders.size()] = i;
-        realReaders.add(reader);
-        if (first)
-        {
-          // filter is identical across tables
-          filter = reader.getPathFilter(conf);
-          first = false;
-        } else
-          sb.append(",");
-        sb.append(reader.getPath().toString() + "/" + reader.getName(splitCGIndex));
-      }
-    }
-    
-    DummyFileInputFormat helper = new DummyFileInputFormat(minSplitSize, realReaders);
-
-    if (!realReaders.isEmpty())
-    {
-      DummyFileInputFormat.setInputPaths(conf, sb.toString());
-      DummyFileInputFormat.setInputPathFilter(conf, filter.getClass());
-      InputSplit[] inputSplits = helper.getSplits(conf, (numSplits < 1 ? 1 : numSplits));
-
-      int batchesPerSplit = inputSplits.length / (numSplits < 1 ? 1 : numSplits);
-      if (batchesPerSplit <= 0)
-        batchesPerSplit = 1;
-
-      /*
-       * Potential file batching optimizations include:
-       * 1) sort single file inputSplits in the descending order of their sizes so
-       *    that the ops of new file opens are spread to a maximum degree;
-       * 2) batching the files with maximum block distribution affinities into the same input split
-       */
-
-      int[] inputSplitBoundaries = new int[realReaders.size()];
-      long start, prevStart = Long.MIN_VALUE;
-      int tableIndex = 0, fileNumber = 0;
-      Integer[] fileNumbers = helper.getFileNumbers();
-      if (fileNumbers.length != realReaders.size())
-        throw new IOException("Number of tables in input paths of input splits is incorrect.");
-      for (int j=0; j<inputSplits.length; j++) {
-        FileSplit fileSplit = (FileSplit) inputSplits[j];
-        start = fileSplit.getStart();
-        if (start <= prevStart)
-        {
-          fileNumber++;
-          if (fileNumber >= fileNumbers[tableIndex])
-          {
-            inputSplitBoundaries[tableIndex++] = j;
-            fileNumber = 0;
-          }
-        }
-        prevStart = start;
-      }
-      inputSplitBoundaries[tableIndex++] =  inputSplits.length;
-      if (tableIndex != realReaders.size())
-        throw new IOException("Number of tables in input splits is incorrect.");
-      for (tableIndex = 0; tableIndex < realReaders.size(); tableIndex++)
-      {
-        int startSplitIndex = (tableIndex == 0 ? 0 : inputSplitBoundaries[tableIndex - 1]);
-        int splitLen = (tableIndex == 0 ? inputSplitBoundaries[0] :
-            inputSplitBoundaries[tableIndex] - inputSplitBoundaries[tableIndex-1]);
-        BasicTable.Reader reader = realReaders.get(tableIndex);
-        /* Get the index of the column group that will be used for row-split.*/
-        int splitCGIndex = reader.getRowSplitCGIndex();
-        
-        long starts[] = new long[splitLen];
-        long lengths[] = new long[splitLen];
-        int batches[] = new int[splitLen + 1];
-        batches[0] = 0;
-        int numBatches = 0;
-        int batchSize = 0;
-        Path paths[] = new Path [splitLen];
-        long totalLen = 0;
-        final double SPLIT_SLOP = 1.1;
-        int endSplitIndex = startSplitIndex + splitLen;
-        for (int j=startSplitIndex; j< endSplitIndex; j++) {
-          FileSplit fileSplit = (FileSplit) inputSplits[j];
-          Path p = fileSplit.getPath();
-          long blockSize = p.getFileSystem(conf).getBlockSize(p);
-          long splitSize = (long) (helper.computeSplitSize(goalSize, minSize, blockSize) * SPLIT_SLOP);
-          start = fileSplit.getStart();
-          long length = fileSplit.getLength();
-          int index = j - startSplitIndex;
-          starts[index] = start;
-          lengths[index] = length;
-          totalLen += length;
-          paths[index] = p;
-          if (totalLen >= splitSize)
-          {
-
-             for (int ii = batches[numBatches] + 1; ii < index - 1; ii++)
-               starts[ii] = -1; // all intermediate files are not split
-             batches[++numBatches] = index;
-             batchSize = 1;
-             totalLen = length;
-          } else if (batchSize + 1 > batchesPerSplit) {
-            for (int ii = batches[numBatches] + 1; ii < index - 1; ii++)
-              starts[ii] = -1; // all intermediate files are not split
-            batches[++numBatches] = index;
-            batchSize = 1;
-            totalLen = length;
-          } else {
-            batchSize++;
-          }
-        }
-        for (int ii = batches[numBatches] + 1; ii < splitLen - 1; ii++)
-          starts[ii] = -1; // all intermediate files are not split
-        if (splitLen > 0)
-          batches[++numBatches] = splitLen;
-        
-        List<RowSplit> subSplits = reader.rowSplit(starts, lengths, paths, splitCGIndex, batches, numBatches);
-        int realTableIndex = realReaderIndices[tableIndex];
-        for (Iterator<RowSplit> it = subSplits.iterator(); it.hasNext();) {
-          RowSplit subSplit = it.next();
-          RowTableSplit split = new RowTableSplit(reader, subSplit, realTableIndex, conf);
-          ret.add(split);
-        }
-      }
-    }
-
-    LOG.info("getSplits : returning " + ret.size() + " row splits.");
-    return ret.toArray(new InputSplit[ret.size()]);
-  }
-
-  /**
-   * @see InputFormat#getSplits(JobConf, int)
-   */
-  @Override
-  public InputSplit[] getSplits(JobConf conf, int numSplits) throws IOException {
-    TableExpr expr = getInputExpr(conf);
-    if (getSorted(conf))
-      expr.setSortedSplit();
-    if (expr.sortedSplitRequired() && !expr.sortedSplitCapable()) {
-      throw new IOException("Unable to created sorted splits");
-    }
-    
-    String projection;
-    try {
-      projection = getProjection(conf);
-    } catch (ParseException e) {
-      throw new IOException("getProjection failed : "+e.getMessage());
-    }
-    List<LeafTableInfo> leaves = expr.getLeafTables(projection);
-    int nLeaves = leaves.size();
-    ArrayList<BasicTable.Reader> readers =
-        new ArrayList<BasicTable.Reader>(nLeaves);
-    ArrayList<BasicTableStatus> status =
-        new ArrayList<BasicTableStatus>(nLeaves);
-
-    try {
-      StringBuilder sb = new StringBuilder();
-      boolean sorted = expr.sortedSplitRequired();
-      boolean first = true;
-      for (Iterator<LeafTableInfo> it = leaves.iterator(); it.hasNext();) {
-        LeafTableInfo leaf = it.next();
-        BasicTable.Reader reader =
-          new BasicTable.Reader(leaf.getPath(), conf);
-        reader.setProjection(leaf.getProjection());
-        BasicTableStatus s = reader.getStatus();
-        status.add(s);
-        readers.add(reader);
-        if (first)
-          first = false;
-        else {
-          sb.append(TableInputFormat.DELETED_CG_SEPARATOR_PER_UNION);
-        }
-        sb.append(reader.getDeletedCGs());
-      }
-      
-      conf.set(INPUT_FE, "true");
-      conf.set(INPUT_DELETED_CGS, sb.toString());
-      
-      if (readers.isEmpty()) {
-        return new InputSplit[0];
-      }
-      
-      if (sorted) {
-        return getSortedSplits(conf, numSplits, expr, readers, status);
-      }
-       
-      return getRowSplits(conf, numSplits, expr, readers, status);
-    } catch (ParseException e) {
-      throw new IOException("Projection parsing failed : "+e.getMessage());
-    }
-    finally {
-      for (Iterator<BasicTable.Reader> it = readers.iterator(); it.hasNext();) {
-        try {
-          it.next().close();
-        }
-        catch (Exception e) {
-          e.printStackTrace();
-          // TODO: log the error here.
-        }
-      }
-    }
-  }
-
-  @Deprecated
-  public synchronized void validateInput(JobConf conf) throws IOException {
-    // Validating imports by opening all Tables.
-    TableExpr expr = getInputExpr(conf);
-    try {
-      String projection = getProjection(conf);
-      List<LeafTableInfo> leaves = expr.getLeafTables(projection);
-      Iterator<LeafTableInfo> iterator = leaves.iterator();
-      while (iterator.hasNext()) {
-        LeafTableInfo leaf = iterator.next();
-        BasicTable.Reader reader =
-            new BasicTable.Reader(leaf.getPath(), conf);
-        reader.setProjection(projection);
-        reader.close();
-      }
-    } catch (ParseException e) {
-    	throw new IOException("Projection parsing failed : "+e.getMessage());
-    }
-  }
-}
-
-/**
- * Adaptor class for sorted InputSplit for table.
- */
-class SortedTableSplit implements InputSplit {
-  BytesWritable begin = null, end = null;
-  
-  String[] hosts;
-  long length = 1;
-
-  public SortedTableSplit()
-  {
-    // no-op for Writable construction
-  }
-  
-  public SortedTableSplit(BytesWritable begin, BytesWritable end,
-      BlockDistribution bd, JobConf conf) {
-    if (begin != null) {
-      this.begin = new BytesWritable();
-      this.begin.set(begin.get(), 0, begin.getSize());
-    }
-    if (end != null) {
-      this.end = new BytesWritable();
-      this.end.set(end.get(), 0, end.getSize());
-    }
-    
-    if (bd != null) {
-      length = bd.getLength();
-      hosts =
-        bd.getHosts(conf.getInt("mapred.lib.table.input.nlocation", 5));
-    }
-  }
-  
-  @Override
-  public long getLength() throws IOException {
-    return length;
-  }
-
-  @Override
-  public String[] getLocations() throws IOException {
-    if (hosts == null)
-    {
-      String[] tmp = new String[1];
-      tmp[0] = "";
-      return tmp;
-    }
-    return hosts;
-  }
-
-  @Override
-  public void readFields(DataInput in) throws IOException {
-    begin = end = null;
-    int bool = WritableUtils.readVInt(in);
-    if (bool == 1) {
-      begin = new BytesWritable();
-      begin.readFields(in);
-    }
-    bool = WritableUtils.readVInt(in);
-    if (bool == 1) {
-      end = new BytesWritable();
-      end.readFields(in);
-    }
-    length = WritableUtils.readVLong(in);
-    int size = WritableUtils.readVInt(in);
-    if (size > 0)
-      hosts = new String[size];
-    for (int i = 0; i < size; i++)
-    	hosts[i] = WritableUtils.readString(in);
-  }
-
-  @Override
-  public void write(DataOutput out) throws IOException {
-    if (begin == null) {
-      WritableUtils.writeVInt(out, 0);
-    }
-    else {
-      WritableUtils.writeVInt(out, 1);
-      begin.write(out);
-    }
-    if (end == null) {
-      WritableUtils.writeVInt(out, 0);
-    }
-    else {
-      WritableUtils.writeVInt(out, 1);
-      end.write(out);
-    }
-    WritableUtils.writeVLong(out, length);
-    WritableUtils.writeVInt(out, hosts == null ? 0 : hosts.length);
-    for (int i = 0; i < hosts.length; i++)
-    {
-    	WritableUtils.writeString(out, hosts[i]);
-    }
-  }
-  
-  public BytesWritable getBegin() {
-    return begin;
-  }
-
-  public BytesWritable getEnd() {
-    return end;
-  }
-}
-
-/**
- * Adaptor class for unsorted InputSplit for table.
- */
-class RowTableSplit implements InputSplit {
-  String path = null;
-  int tableIndex;
-  RowSplit split = null;
-  String[] hosts = null;
-  long length = 1;
-
-  public RowTableSplit(Reader reader, RowSplit split, int tableIndex, JobConf conf)
-      throws IOException {
-    this.path = reader.getPath();
-    this.split = split;
-    this.tableIndex = tableIndex;
-    BlockDistribution dataDist = reader.getBlockDistribution(split);
-    if (dataDist != null) {
-      length = dataDist.getLength();
-      hosts =
-          dataDist.getHosts(conf.getInt("mapred.lib.table.input.nlocation", 5));
-    }
-  }
-  
-  public RowTableSplit() {
-    // no-op for Writable construction
-  }
-  
-  @Override
-  public long getLength() throws IOException {
-    return length;
-  }
-
-  @Override
-  public String[] getLocations() throws IOException {
-    return hosts;
-  }
-
-  @Override
-  public void readFields(DataInput in) throws IOException {
-    tableIndex = WritableUtils.readVInt(in);
-    path = WritableUtils.readString(in);
-    int bool = WritableUtils.readVInt(in);
-    if (bool == 1) {
-      if (split == null) split = new RowSplit();
-      split.readFields(in);
-    }
-    else {
-      split = null;
-    }
-    hosts = WritableUtils.readStringArray(in);
-    length = WritableUtils.readVLong(in);
-  }
-
-  @Override
-  public void write(DataOutput out) throws IOException {
-    WritableUtils.writeVInt(out, tableIndex);
-    WritableUtils.writeString(out, path);
-    if (split == null) {
-      WritableUtils.writeVInt(out, 0);
-    }
-    else {
-      WritableUtils.writeVInt(out, 1);
-      split.write(out);
-    }
-    WritableUtils.writeStringArray(out, hosts);
-    WritableUtils.writeVLong(out, length);
-  }
-
-  public String getPath() {
-    return path;
-  }
-  
-  public RowSplit getSplit() {
-    return split;
-  }
-  
-  public int getTableIndex() {
-    return tableIndex;
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/TableRecordReader.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/TableRecordReader.java
deleted file mode 100644
index a39e9f70d..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/TableRecordReader.java
+++ /dev/null
@@ -1,126 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.IOException;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.mapred.InputSplit;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.RecordReader;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Projection;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.Tuple;
-
-/**
- * Adaptor class to implement RecordReader on top of Scanner.
- * 
- * @Deprecated Use (@link org.apache.hadoop.zebra.mapreduce.TableRecordReader) instead
- */
-@Deprecated
-public class TableRecordReader implements RecordReader<BytesWritable, Tuple> {
-  private final TableScanner scanner;
-  private long count = 0;
-
-  /**
-   * 
-   * @param expr
-   *          Table expression
-   * @param projection
-   *          projection schema. Should never be null.
-   * @param split
-   *          the split to work on
-   * @param conf
-   *          JobConf object
-   * @throws IOException
-   */
-  public TableRecordReader(TableExpr expr, String projection,
-		  InputSplit split, JobConf conf) throws IOException, ParseException {
-	  if( split instanceof RowTableSplit ) {
-		  RowTableSplit rowSplit = (RowTableSplit)split;
-		  scanner = expr.getScanner(rowSplit, projection, conf);
-	  } else {
-		  SortedTableSplit tblSplit = (SortedTableSplit)split;
-		  scanner = expr.getScanner( tblSplit.getBegin(), tblSplit.getEnd(), projection, conf );
-	  }
-  }
-  
-  @Override
-  public void close() throws IOException {
-    scanner.close();
-  }
-
-  @Override
-  public BytesWritable createKey() {
-    return new BytesWritable();
-  }
-
-  @Override
-  public Tuple createValue() {
-    try {
-      return TypesUtils.createTuple(Projection.getNumColumns(scanner.getProjection()));
-    }
-    catch (IOException e) {
-      // TODO Auto-generated catch block
-      e.printStackTrace();
-    }
-    return null;
-  }
-
-  @Override
-  public long getPos() throws IOException {
-    return count;
-  }
-
-  @Override
-  public float getProgress() throws IOException {
-    return  (float)((scanner.atEnd()) ? 1.0 : 0);
-  }
-
-  @Override
-  public boolean next(BytesWritable key, Tuple value) throws IOException {
-    if (scanner.atEnd()) {
-      return false;
-    }
-    scanner.getKey(key);
-    scanner.getValue(value);
-    scanner.advance();
-    ++count;
-    return true;
-  }
-  
-  /**
-   * Seek to the position at the first row which has the key
-   * or just after the key; only applicable for sorted Zebra table
-   *
-   * @param key
-   *          the key to seek on
-   */
-  public boolean seekTo(BytesWritable key) throws IOException {
-    return scanner.seekTo(key);
-  }
-  
-  /**
-   * Check if the end of the input has been reached
-   *
-   * @return true if the end of the input is reached
-   */
-  public boolean atEnd() throws IOException {
-	  return scanner.atEnd();
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/TableUnionExpr.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/TableUnionExpr.java
deleted file mode 100644
index 3f847c181..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/TableUnionExpr.java
+++ /dev/null
@@ -1,307 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.EOFException;
-import java.io.IOException;
-import java.io.StringReader;
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Comparator;
-import java.util.LinkedHashSet;
-import java.util.List;
-import java.util.concurrent.PriorityBlockingQueue;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.BasicTableStatus;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Projection;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.pig.data.Tuple;
-
-/**
- * Table expression supporting a union of BasicTables.
- * 
- * @see <a href="doc-files/examples/ReadTableUnion.java">Usage example for
- *      UnionTableExpr</a>
- */
-class TableUnionExpr extends CompositeTableExpr {
-  /**
-   * Add another BasicTable into the table-union.
-   * 
-   * @param expr
-   *          The expression for the BasicTable to be added.
-   * @return self.
-   */
-  public TableUnionExpr add(BasicTableExpr expr) {
-    super.addCompositeTable(expr);
-    return this;
-  }
-
-  /**
-   * Add an array of BasicTables into the table-union.
-   * 
-   * @param exprs
-   *          the expressions representing the BasicTables to be added.
-   * @return self.
-   */
-  public TableUnionExpr add(BasicTableExpr[] exprs) {
-    super.addCompositeTables(exprs);
-    return this;
-  }
-
-  /**
-   * Add a Collection of BasicTables into the table-union.
-   * 
-   * @param exprs
-   *          the expressions representing the BasicTables to be added.
-   * @return self.
-   */
-  public TableUnionExpr add(Collection<? extends BasicTableExpr> exprs) {
-    super.addCompositeTables(exprs);
-    return this;
-  }
-  
-  @Override
-  protected TableUnionExpr decodeParam(StringReader in) throws IOException {
-    super.decodeParam(in);
-    int n = composite.size();
-    for (int i = 0; i < n; ++i) {
-      if (!(composite.get(i) instanceof BasicTableExpr)) {
-        throw new RuntimeException("Not a BasicTableExpr");
-      }
-    }
-    return this;
-  }
-
-  @Override
-  protected TableUnionExpr encodeParam(StringBuilder out) {
-    super.encodeParam(out);
-    return this;
-  }
-
-  @Override
-  public TableScanner getScanner(BytesWritable begin, BytesWritable end,
-      String projection, Configuration conf) throws IOException {
-    int n = composite.size();
-    if (n==0) {
-      throw new IllegalArgumentException("Union of 0 table");
-    }
-    ArrayList<BasicTable.Reader> readers = new ArrayList<BasicTable.Reader>(n);
-    String[] deletedCGsInUnion = getDeletedCGsPerUnion(conf);
-    
-    if (deletedCGsInUnion != null && deletedCGsInUnion.length != n)
-      throw new IllegalArgumentException("Invalid string of deleted column group names: expected = "+
-          n + " actual =" + deletedCGsInUnion.length);
-    
-    for (int i = 0; i < n; ++i) {
-      String deletedCGs = (deletedCGsInUnion == null ? null : deletedCGsInUnion[i]);
-      String[] deletedCGList = (deletedCGs == null ? null : 
-        deletedCGs.split(BasicTable.DELETED_CG_SEPARATOR_PER_TABLE));
-      BasicTableExpr expr = (BasicTableExpr) composite.get(i);
-      BasicTable.Reader reader =
-          new BasicTable.Reader(expr.getPath(), deletedCGList, conf);
-      readers.add(reader);
-    }
-
-    String actualProjection = projection;
-    if (actualProjection == null) {
-      // Perform a union on all column names.
-      LinkedHashSet<String> colNameSet = new LinkedHashSet<String>();
-      for (int i = 0; i < n; ++i) {
-        String[] cols = readers.get(i).getSchema().getColumns();
-        for (String col : cols) {
-          colNameSet.add(col);
-        }
-      }
-
-      actualProjection = 
-          Projection.getProjectionStr(colNameSet.toArray(new String[colNameSet.size()]));
-    }
-    
-    ArrayList<TableScanner> scanners = new ArrayList<TableScanner>(n);
-    try {
-      for (int i=0; i<n; ++i) {
-        BasicTable.Reader reader = readers.get(i);
-        reader.setProjection(actualProjection);
-        TableScanner scanner = readers.get(i).getScanner(begin, end, true);
-        scanners.add(scanner);
-      }
-    } catch (ParseException e) {
-    	throw new IOException("Projection parsing failed : "+e.getMessage());
-    }
-    
-    if (scanners.isEmpty()) {
-      return new NullScanner(actualProjection);
-    }
-
-    Integer[] virtualColumnIndices = Projection.getVirtualColumnIndices(projection);
-    if (virtualColumnIndices != null && n == 1)
-      throw new IllegalArgumentException("virtual column requires union of multiple tables");
-    return new SortedTableUnionScanner(scanners, Projection.getVirtualColumnIndices(projection));
-  }
-
-  @Override
-  public TableScanner getScanner(RowTableSplit split, String projection,
-      Configuration conf) throws IOException, ParseException {
-    BasicTableExpr expr = (BasicTableExpr) composite.get(split.getTableIndex());
-    return expr.getScanner(split, projection, conf);
-  }
-}
-
-/**
- * Union scanner.
- */
-class SortedTableUnionScanner implements TableScanner {
-  CachedTableScanner[] scanners;
-  PriorityBlockingQueue<CachedTableScanner> queue;
-  boolean synced = false;
-  boolean hasVirtualColumns = false;
-  Integer[] virtualColumnIndices = null;
-  CachedTableScanner scanner = null; // the working scanner
-
-  SortedTableUnionScanner(List<TableScanner> scanners, Integer[] vcolindices) throws IOException {
-    if (scanners.isEmpty()) {
-      throw new IllegalArgumentException("Zero-sized table union");
-    }
-    
-    this.scanners = new CachedTableScanner[scanners.size()];
-    queue =
-        new PriorityBlockingQueue<CachedTableScanner>(scanners.size(),
-            new Comparator<CachedTableScanner>() {
-
-              @Override
-              public int compare(CachedTableScanner o1, CachedTableScanner o2) {
-                try {
-                  return o1.getKey().compareTo(o2.getKey());
-                }
-                catch (IOException e) {
-                  throw new RuntimeException("IOException: " + e.toString());
-                }
-              }
-
-            });
-    
-    for (int i = 0; i < this.scanners.length; ++i) {
-      TableScanner scanner = scanners.get(i);
-      this.scanners[i] = new CachedTableScanner(scanner, i);
-    }
-    // initial fill-ins
-    if (!atEnd())
-    	scanner = queue.poll();
-    virtualColumnIndices = vcolindices;
-    hasVirtualColumns = (vcolindices != null && vcolindices.length != 0);
-  }
-  
-
-  private void sync() throws IOException {
-    if (synced == false) {
-      queue.clear();
-      for (int i = 0; i < scanners.length; ++i) {
-        if (!scanners[i].atEnd()) {
-          queue.add(scanners[i]);
-        }
-      }
-      synced = true;
-    }
-  }
-  
-  @Override
-  public boolean advance() throws IOException {
-    sync();
-    scanner.advance();
-    if (!scanner.atEnd()) {
-      queue.add(scanner);
-    }
-    scanner = queue.poll();
-    return (scanner != null);
-  }
-
-  @Override
-  public boolean atEnd() throws IOException {
-    sync();
-    return (scanner == null && queue.isEmpty());
-  }
-
-  @Override
-  public String getProjection() {
-    return scanners[0].getProjection();
-  }
-  
-  @Override
-  public Schema getSchema() {
-    return scanners[0].getSchema();
-  }
-
-  @Override
-  public void getKey(BytesWritable key) throws IOException {
-    if (atEnd()) {
-      throw new EOFException("No more rows to read");
-    }
-    
-    key.set(scanner.getKey());
-  }
-
-  @Override
-  public void getValue(Tuple row) throws IOException {
-    if (atEnd()) {
-      throw new EOFException("No more rows to read");
-    }
-  
-    Tuple tmp = scanner.getValue();
-    if (hasVirtualColumns)
-    {
-       for (int i = 0; i < virtualColumnIndices.length; i++)
-       {
-         tmp.set(virtualColumnIndices[i], scanner.getIndex());
-       }
-    }
-    row.reference(tmp);
-  }
-
-  @Override
-  public boolean seekTo(BytesWritable key) throws IOException {
-    boolean rv = false;
-    for (CachedTableScanner scanner : scanners) {
-      rv = rv || scanner.seekTo(key);
-    }
-    synced = false;
-    if (!atEnd())
-      scanner = queue.poll();
-    return rv;
-  }
-
-  @Override
-  public void seekToEnd() throws IOException {
-    for (CachedTableScanner scanner : scanners) {
-      scanner.seekToEnd();
-    }
-    scanner = null;
-    synced = false;
-  }
-
-  @Override
-  public void close() throws IOException {
-    for (CachedTableScanner scanner : scanners) {
-      scanner.close();
-    }
-    queue.clear();
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/ZebraOutputPartition.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/ZebraOutputPartition.java
deleted file mode 100644
index 890ea06a6..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/ZebraOutputPartition.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.IOException;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.conf.*;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.pig.data.Tuple;
-
-
-public abstract class ZebraOutputPartition  implements Configurable{
-
-  protected  Configuration jobConf;	
-	
-  @Override
-  public void setConf( Configuration conf) {
-	  jobConf = conf;
-  }
-
-  @Override
-  public Configuration getConf( ) {
-	  return jobConf;
-  }
-  
-  public abstract int getOutputPartition(BytesWritable key, Tuple Value)
-  throws IOException;
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/ZebraProjection.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/ZebraProjection.java
deleted file mode 100644
index b0c66ac45..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/ZebraProjection.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.mapred;
-
-/**
- * A wrapper class for Projection.
- */
-public class ZebraProjection {
-  private String projectionStr = null;
-
-  private ZebraProjection(String projStr) {
-    projectionStr = projStr;
-  }
-  
-  public static ZebraProjection createZebraProjection(String projStr) {
-    return new ZebraProjection(projStr);
-  }
-  
-  public String toString() {
-    return projectionStr;
-  }
-}
\ No newline at end of file
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/ZebraSchema.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/ZebraSchema.java
deleted file mode 100644
index d9c360cdf..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/ZebraSchema.java
+++ /dev/null
@@ -1,39 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.mapred;
-
-import org.apache.hadoop.zebra.schema.Schema;
-
-/**
- * A wrapper class for Schema.
- */
-public class ZebraSchema {
-  private String schemaStr = null;
-
-  private ZebraSchema(String str) {
-    String normalizedStr = Schema.normalize(str);
-    schemaStr = normalizedStr; 
-  }
-  
-  public static ZebraSchema createZebraSchema(String str) {
-    return new ZebraSchema(str);
-  }
-  
-  public String toString() {
-    return schemaStr;
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/ZebraSortInfo.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/ZebraSortInfo.java
deleted file mode 100644
index 0046d70d3..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/ZebraSortInfo.java
+++ /dev/null
@@ -1,46 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.mapred;
-
-import org.apache.hadoop.io.RawComparator;
-import org.apache.hadoop.zebra.tfile.TFile;
-
-/**
- * A wrapper class for SortInfo.
- */
-public class ZebraSortInfo {
-  private String sortColumnsStr = null;
-  private String comparatorStr = null;
-  
-  private ZebraSortInfo(String sortColumns, Class<? extends RawComparator<Object>> comparatorClass) {
-    if (comparatorClass != null)
-      comparatorStr = TFile.COMPARATOR_JCLASS+comparatorClass.getName();
-    sortColumnsStr = sortColumns;
-  }
-  
-  public static ZebraSortInfo createZebraSortInfo(String sortColumns, Class<? extends RawComparator<Object>> comparatorClass) {
-    return new ZebraSortInfo(sortColumns, comparatorClass);
-  }
-  
-  public String getSortColumns() {
-    return sortColumnsStr;
-  }
-
-  public String getComparator() {
-    return comparatorStr;
-  }    
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/ZebraStorageHint.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/ZebraStorageHint.java
deleted file mode 100644
index bfc3912e7..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/ZebraStorageHint.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.mapred;
-
-/**
- * A wrapper class for StorageHint.
- */
-public class ZebraStorageHint {
-  private String storageHintStr = null;
-
-  private ZebraStorageHint(String shStr) {
-    storageHintStr = shStr;
-  }
-  
-  public static ZebraStorageHint createZebraStorageHint(String shStr) {
-    return new ZebraStorageHint(shStr);
-  }
-  
-  public String toString() {
-    return storageHintStr;
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/package-info.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/package-info.java
deleted file mode 100644
index e0f6eadda..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapred/package-info.java
+++ /dev/null
@@ -1,26 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-/**
- * Providing {@link org.apache.hadoop.mapred.InputFormat} and
- * {@link org.apache.hadoop.mapred.OutputFormat} adaptor classes for Hadoop
- * Zebra Table.
- * 
- * This package is deprecated. Please use org.apache.hadoop.zebra.mapreduce instead.
- */
-package org.apache.hadoop.zebra.mapred;
-
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/BasicTableExpr.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/BasicTableExpr.java
deleted file mode 100644
index ac7ee5490..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/BasicTableExpr.java
+++ /dev/null
@@ -1,213 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.IOException;
-import java.io.StringReader;
-import java.io.PrintStream;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.Projection;
-import org.apache.pig.data.Tuple;
-
-/**
- * Table expression for reading a BasicTable.
- * 
- * @see <a href="doc-files/examples/ReadABasicTable.java">Usage example for
- *      BasicTableExpr</a>
- */
-class BasicTableExpr extends TableExpr {
-  private Path path;
-
-  /**
-   * default constructor.
-   */
-  public BasicTableExpr() {
-    // no-op
-  }
-
-  /**
-   * Constructor.
-   * 
-   * @param path
-   *          Path of the BasicTable.
-   */
-  public BasicTableExpr(Path path) {
-    this.path = path;
-  }
-
-  /**
-   * Set the path.
-   * 
-   * @param path
-   *          path to the BasicTable.
-   * @return self.
-   */
-  public BasicTableExpr setPath(Path path) {
-    this.path = path;
-    return this;
-  }
-
-  /**
-   * Get the path.
-   * 
-   * @return the path to the BasicTable.
-   */
-  public Path getPath() {
-    return path;
-  }
-
-  @Override
-  protected BasicTableExpr decodeParam(StringReader in) throws IOException {
-    String strPath = TableExprUtils.decodeString(in);
-    if (strPath == null) {
-      throw new RuntimeException("Incomplete expression");
-    }
-    path = new Path(strPath);
-    return this;
-  }
-
-  @Override
-  protected BasicTableExpr encodeParam(StringBuilder out) {
-    if (path == null) {
-      throw new RuntimeException("Incomplete expression");
-    }
-    TableExprUtils.encodeString(out, path.toString());
-    return this;
-  }
-
-  @Override
-  public List<LeafTableInfo> getLeafTables(String projection) {
-    ArrayList<LeafTableInfo> ret = new ArrayList<LeafTableInfo>(1);
-    ret.add(new LeafTableInfo(path, projection));
-    return ret;
-  }
-
-  @Override
-  public TableScanner getScanner(BytesWritable begin, BytesWritable end,
-      String projection, Configuration conf) throws IOException {
-    String[] deletedCGs = getDeletedCGs(conf);
-    BasicTable.Reader reader = new BasicTable.Reader(path, deletedCGs, conf);
-    try {
-      reader.setProjection(projection);
-    } catch (ParseException e) {
-    	throw new IOException("Projection parsing failed : "+e.getMessage());
-    } 
-    return reader.getScanner(begin, end, true);
-  } 
-
-  @Override
-  public TableScanner getScanner(RowTableSplit split, String projection,
-      Configuration conf) throws IOException, ParseException {
-    return new BasicTableScanner(split, projection, conf);
-  }
-  
-  @Override
-  public Schema getSchema(Configuration conf) throws IOException {
-    return BasicTable.Reader.getSchema(path, conf);
-  }
-
-  @Override
-  public boolean sortedSplitCapable() {
-    return true;
-  }
-  
-  @Override
-  protected void dumpInfo(PrintStream ps, Configuration conf, int indent) throws IOException
-  {
-    BasicTable.dumpInfo(path.toString(), ps, conf, indent);
-  }
-  
-  /**
-   * Basic Table Scanner
-   */
-  class BasicTableScanner implements TableScanner {
-    private int tableIndex = -1;
-    private Integer[] virtualColumnIndices = null;
-    private TableScanner scanner = null;
-    
-    BasicTableScanner(RowTableSplit split, String projection,
-        Configuration conf) throws IOException, ParseException, ParseException {
-      tableIndex = split.getTableIndex();
-      virtualColumnIndices = Projection.getVirtualColumnIndices(projection);
-      BasicTable.Reader reader =
-        new BasicTable.Reader(new Path(split.getPath()), getDeletedCGs(conf), conf);
-      reader.setProjection(projection);
-      scanner = reader.getScanner(true, split.getSplit());
-    }
-    
-    @Override
-    public boolean advance() throws IOException {
-      return scanner.advance();
-    }
-    
-    @Override
-    public boolean atEnd() throws IOException {
-      return scanner.atEnd();
-    }
-    
-    @Override
-    public Schema getSchema() {
-      return scanner.getSchema();
-    }
-    
-    @Override
-    public void getKey(BytesWritable key) throws IOException {
-      scanner.getKey(key);
-    }
-    
-    @Override
-    public void getValue(Tuple row) throws IOException {
-      scanner.getValue(row);
-      if (virtualColumnIndices != null)
-      {
-        for (int i = 0; i < virtualColumnIndices.length; i++)
-        {
-          row.set(virtualColumnIndices[i], tableIndex);
-        }
-      }
-    }
-    
-    @Override
-    public boolean seekTo(BytesWritable key) throws IOException {
-      return scanner.seekTo(key);
-    }
-    
-    @Override
-    public void seekToEnd() throws IOException {
-      scanner.seekToEnd();
-    }
-    
-    @Override 
-    public void close() throws IOException {
-      scanner.close();
-    }
-    
-    @Override
-    public String getProjection() {
-      return scanner.getProjection();
-    }
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/BasicTableOutputFormat.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/BasicTableOutputFormat.java
deleted file mode 100644
index ffb688e49..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/BasicTableOutputFormat.java
+++ /dev/null
@@ -1,835 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.IOException;
-
-import org.apache.hadoop.util.ReflectionUtils;
-import org.apache.hadoop.util.StringUtils;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.RawComparator;
-import org.apache.hadoop.mapreduce.JobContext;
-import org.apache.hadoop.mapreduce.OutputCommitter;
-import org.apache.hadoop.mapreduce.OutputFormat;
-import org.apache.hadoop.mapreduce.TaskAttemptContext;
-import org.apache.hadoop.mapreduce.RecordWriter;
-import org.apache.hadoop.util.Progressable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Partition;
-import org.apache.hadoop.zebra.types.SortInfo;
-import org.apache.hadoop.zebra.types.ZebraConf;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.tfile.TFile;
-import org.apache.pig.data.Tuple;
-import org.apache.hadoop.zebra.pig.comparator.*;
-
-
-/**
- * {@link org.apache.hadoop.mapreduce.OutputFormat} class for creating a
- * BasicTable.
- * 
- * Usage Example:
- * <p>
- * In the main program, add the following code.
- * 
- * <pre>
- * job.setOutputFormatClass(BasicTableOutputFormat.class);
- * Path outPath = new Path(&quot;path/to/the/BasicTable&quot;);
- * BasicTableOutputFormat.setOutputPath(job, outPath);
- * BasicTableOutputFormat.setSchema(job, &quot;Name, Age, Salary, BonusPct&quot;);
- * </pre>
- * 
- * The above code does the following things:
- * <UL>
- * <LI>Set the output format class to BasicTableOutputFormat.
- * <LI>Set the single path to the BasicTable to be created.
- * <LI>Set the schema of the BasicTable to be created. In this case, the
- * to-be-created BasicTable contains three columns with names "Name", "Age",
- * "Salary", "BonusPct".
- * </UL>
- * 
- * To create multiple output paths. ZebraOutputPartitoner interface needs to be implemented
- * <pre>
- * String multiLocs = &quot;commaSeparatedPaths&quot;    
- * job.setOutputFormatClass(BasicTableOutputFormat.class);
- * BasicTableOutputFormat.setMultipleOutputPaths(job, multiLocs);
- * job.setOutputFormat(BasicTableOutputFormat.class);
- * BasicTableOutputFormat.setSchema(job, &quot;Name, Age, Salary, BonusPct&quot;);
- * BasicTableOutputFormat.setZebraOutputPartitionClass(
- * 		job, MultipleOutputsTest.OutputPartitionerClass.class);
- * </pre>
- * 
- * 
- * The user ZebraOutputPartitionClass should like this
- * 
- * <pre>
- * 
- *   static class OutputPartitionerClass implements ZebraOutputPartition {
- *   &#064;Override
- *	  public int getOutputPartition(BytesWritable key, Tuple value) {		 
- *
- *        return someIndexInOutputParitionlist0;
- *	  }
- * 
- * </pre>
- * 
- * 
- * The user Reducer code (or similarly Mapper code if it is a Map-only job)
- * should look like the following:
- * 
- * <pre>
- * static class MyReduceClass implements Reducer&lt;K, V, BytesWritable, Tuple&gt; {
- *   // keep the tuple object for reuse.
- *   Tuple outRow;
- *   // indices of various fields in the output Tuple.
- *   int idxName, idxAge, idxSalary, idxBonusPct;
- * 
- *   &#064;Override
- *   public void configure(Job job) {
- *     Schema outSchema = BasicTableOutputFormat.getSchema(job);
- *     // create a tuple that conforms to the output schema.
- *     outRow = TypesUtils.createTuple(outSchema);
- *     // determine the field indices.
- *     idxName = outSchema.getColumnIndex(&quot;Name&quot;);
- *     idxAge = outSchema.getColumnIndex(&quot;Age&quot;);
- *     idxSalary = outSchema.getColumnIndex(&quot;Salary&quot;);
- *     idxBonusPct = outSchema.getColumnIndex(&quot;BonusPct&quot;);
- *   }
- * 
- *   &#064;Override
- *   public void reduce(K key, Iterator&lt;V&gt; values,
- *       OutputCollector&lt;BytesWritable, Tuple&gt; output, Reporter reporter)
- *       throws IOException {
- *     String name;
- *     int age;
- *     int salary;
- *     double bonusPct;
- *     // ... Determine the value of the individual fields of the row to be inserted.
- *     try {
- *       outTuple.set(idxName, name);
- *       outTuple.set(idxAge, new Integer(age));
- *       outTuple.set(idxSalary, new Integer(salary));
- *       outTuple.set(idxBonusPct, new Double(bonusPct));
- *       output.collect(new BytesWritable(name.getBytes()), outTuple);
- *     }
- *     catch (ExecException e) {
- *       // should never happen
- *     }
- *   }
- * 
- *   &#064;Override
- *   public void close() throws IOException {
- *     // no-op
- *   }
- * 
- * }
- * </pre>
- */
-public class BasicTableOutputFormat extends OutputFormat<BytesWritable, Tuple> {
-	/**
-	 * Set the multiple output paths of the BasicTable in JobContext
-	 * 
-	 * @param jobContext
-	 *          The JobContext object.
-	 * @param commaSeparatedLocations
-	 *          The comma separated output paths to the tables. 
-	 *          The path must either not existent, or must be an empty directory.
-	 * @param theClass
-	 * 	      Zebra output partitioner class
-	 * 
-	 * @deprecated Use {@link #setMultipleOutputs(JobContext, class<? extends ZebraOutputPartition>, Path ...)} instead.
-	 * 
-	 */
-	public static void setMultipleOutputs(JobContext jobContext, String commaSeparatedLocations, Class<? extends ZebraOutputPartition> theClass)
-	throws IOException {
-		Configuration conf = jobContext.getConfiguration();
-		ZebraConf.setMultiOutputPath(conf, commaSeparatedLocations);
-
-		if (ZebraConf.getIsMulti(conf, true) == false) {
-			throw new IllegalArgumentException("Job has been setup as single output path");
-		}
-
-		ZebraConf.setIsMulti(conf, true);
-		setZebraOutputPartitionClass(jobContext, theClass);	  
-	}
-
-  /**
-	 * Set the multiple output paths of the BasicTable in JobContext
-	 * 
-	 * @param jobContext
-	 *          The JobContext object.
-   * @param theClass
-   *        Zebra output partitioner class          
-	 * @param paths
-	 *          The list of paths 
-	 *          The path must either not existent, or must be an empty directory.
-	 */
-	public static void setMultipleOutputs(JobContext jobContext, Class<? extends ZebraOutputPartition> theClass, Path... paths)
-	throws IOException {
-		Configuration conf = jobContext.getConfiguration();
-		FileSystem fs = FileSystem.get( conf );
-		Path path = paths[0].makeQualified(fs);
-		StringBuffer str = new StringBuffer(StringUtils.escapeString(path.toString()));
-		for(int i = 1; i < paths.length;i++) {
-			str.append(StringUtils.COMMA_STR);
-			path = paths[i].makeQualified(fs);
-			str.append(StringUtils.escapeString(path.toString()));
-		}	  
-		ZebraConf.setMultiOutputPath(conf, str.toString());
-
-		if (ZebraConf.getIsMulti(conf, true) == false) {
-			throw new IllegalArgumentException("Job has been setup as single output path");
-		}
-
-		ZebraConf.setIsMulti(conf, true);
-		setZebraOutputPartitionClass(jobContext, theClass);
-	}
-	
-	/**
-   * Set the multiple output paths of the BasicTable in JobContext
-   * 
-   * @param jobContext
-   *          The JobContext object.
-   * @param theClass
-   *          Zebra output partitioner class
-   * @param arguments
-   *          Arguments string to partitioner class
-   * @param paths
-   *          The list of paths 
-   *          The path must either not existent, or must be an empty directory.
-   */
-  public static void setMultipleOutputs(JobContext jobContext, Class<? extends ZebraOutputPartition> theClass, String arguments, Path... paths)
-  throws IOException {
-    setMultipleOutputs(jobContext, theClass, paths);
-    if (arguments != null) {
-      ZebraConf.setOutputPartitionClassArguments(jobContext.getConfiguration(), arguments);
-    }
-  }
-  
-  /**
-   * Get the output partition class arguments string from job configuration
-   * 
-   * @param conf
-   *          The job configuration object.
-   * @return the output partition class arguments string.
-   */
-  public static String getOutputPartitionClassArguments(Configuration conf) {
-    return ZebraConf.getOutputPartitionClassArguments(conf);
-  }  
-
-	/**
-	 * Get the multiple output paths of the BasicTable from JobContext
-	 * 
-	 * @param jobContext
-	 *          The JobContext object.
-	 * @return path
-	 *          The comma separated output paths to the tables. 
-	 *          The path must either not existent, or must be an empty directory.
-	 */
-	public static Path[] getOutputPaths(JobContext jobContext)
-	throws IOException {
-		Configuration conf = jobContext.getConfiguration();
-
-		Path[] result;
-		String paths = ZebraConf.getMultiOutputPath(conf);
-		String path = ZebraConf.getOutputPath(conf);
-
-		if(paths != null && path != null) {
-			throw new IllegalArgumentException("Illegal output paths specs. Both multi and single output locs are set");
-		}	
-
-		if (ZebraConf.getIsMulti(conf, false) == true) {
-			if (paths == null || paths.equals("")) {
-				throw new IllegalArgumentException("Illegal multi output paths");
-			}	    
-			String [] list = StringUtils.split(paths);
-			result = new Path[list.length];
-			for (int i = 0; i < list.length; i++) {
-				result[i] = new Path(StringUtils.unEscapeString(list[i]));
-			}
-		} else {
-			if (path == null || path.equals("")) {
-				throw new IllegalArgumentException("Cannot find output path");
-			}	    
-			result = new Path[1];
-			result[0] = new Path(path);
-		}
-
-		return result;
-
-	}
-
-	private static void setZebraOutputPartitionClass(
-			JobContext jobContext, Class<? extends ZebraOutputPartition> theClass) throws IOException {
-		if (!ZebraOutputPartition.class.isAssignableFrom(theClass))
-			throw new IOException(theClass+" not "+ZebraOutputPartition.class.getName());
-		ZebraConf.setZebraOutputPartitionerClass(jobContext.getConfiguration(), theClass.getName());
-	}
-
-	public static Class<? extends ZebraOutputPartition> getZebraOutputPartitionClass(JobContext jobContext) throws IOException {
-		Configuration conf = jobContext.getConfiguration();
-
-		Class<?> theClass;	  
-		String valueString = ZebraConf.getZebraOutputPartitionerClass(conf);
-		if (valueString == null)
-			throw new IOException("zebra output partitioner class not found");
-		try {
-			theClass = conf.getClassByName(valueString);
-		} catch (ClassNotFoundException e) {
-			throw new IOException(e);
-		}	  
-
-		if (theClass != null && !ZebraOutputPartition.class.isAssignableFrom(theClass))
-			throw new IOException(theClass+" not "+ZebraOutputPartition.class.getName());
-		else if (theClass != null)
-			return theClass.asSubclass(ZebraOutputPartition.class);
-		else
-			return null;
-
-	}
-
-	/**
-	 * Set the output path of the BasicTable in JobContext
-	 * 
-	 * @param jobContext
-	 *          The JobContext object.
-	 * @param path
-	 *          The output path to the table. The path must either not existent,
-	 *          or must be an empty directory.
-	 */
-	public static void setOutputPath(JobContext jobContext, Path path) {
-		Configuration conf = jobContext.getConfiguration();
-		ZebraConf.setOutputPath(conf, path.toString());
-		if (ZebraConf.getIsMulti(conf, false) == true) {
-			throw new IllegalArgumentException("Job has been setup as multi output paths");
-		}
-		ZebraConf.setIsMulti(conf, false);
-
-	}
-
-	/**
-	 * Get the output path of the BasicTable from JobContext
-	 * 
-	 * @param jobContext
-	 *          jobContext object
-	 * @return The output path.
-	 */
-	public static Path getOutputPath(JobContext jobContext) {
-		Configuration conf = jobContext.getConfiguration();
-		String path = ZebraConf.getOutputPath(conf);
-		return (path == null) ? null : new Path(path);
-	}
-
-	/**
-	 * Set the table schema in JobContext
-	 * 
-	 * @param jobContext
-	 *          The JobContext object.
-	 * @param schema
-	 *          The schema of the BasicTable to be created. For the initial
-	 *          implementation, the schema string is simply a comma separated list
-	 *          of column names, such as "Col1, Col2, Col3".
-	 * 
-	 * @deprecated Use {@link #setStorageInfo(JobContext, ZebraSchema, ZebraStorageHint, ZebraSortInfo)} instead.
-	 */
-	public static void setSchema(JobContext jobContext, String schema) {
-		Configuration conf = jobContext.getConfiguration();
- 		ZebraConf.setOutputSchema(conf, Schema.normalize(schema));
-		
-    // This is to turn off type check for potential corner cases - for internal use only;
-		if (System.getenv("zebra_output_checktype")!= null && System.getenv("zebra_output_checktype").equals("no")) {
-		  ZebraConf.setCheckType(conf, false);
-    }
-	}
-
-	/**
-	 * Get the table schema in JobContext.
-	 * 
-	 * @param jobContext
-	 *          The JobContext object.
-	 * @return The output schema of the BasicTable. If the schema is not defined
-	 *         in the jobContext object at the time of the call, null will be returned.
-	 */
-	public static Schema getSchema(JobContext jobContext) throws ParseException {
-		Configuration conf = jobContext.getConfiguration();
-		String schema = ZebraConf.getOutputSchema(conf);
-		if (schema == null) {
-			return null;
-		}
-		//schema = schema.replaceAll(";", ",");
-		return new Schema(schema);
-	}
-
-	private static KeyGenerator makeKeyBuilder(byte[] elems) {
-		ComparatorExpr[] exprs = new ComparatorExpr[elems.length];
-		for (int i = 0; i < elems.length; ++i) {
-			exprs[i] = ExprUtils.primitiveComparator(i, elems[i]);
-		}
-		return new KeyGenerator(ExprUtils.tupleComparator(exprs));
-	}
-
-	/**
-	 * Generates a zebra specific sort key generator which is used to generate BytesWritable key 
-	 * Sort Key(s) are used to generate this object
-	 * 
-	 * @param jobContext
-	 *          The JobContext object.
-	 * @return Object of type zebra.pig.comaprator.KeyGenerator. 
-	 *         
-	 */
-	public static Object getSortKeyGenerator(JobContext jobContext) throws IOException, ParseException {
-		SortInfo sortInfo = getSortInfo( jobContext );
-		Schema schema     = getSchema(jobContext);
-		String[] sortColNames = sortInfo.getSortColumnNames();
-
-		byte[] types = new byte[sortColNames.length];
-		for(int i =0 ; i < sortColNames.length; ++i){
-			types[i] = schema.getColumn(sortColNames[i]).getType().pigDataType();
-		}
-		KeyGenerator builder = makeKeyBuilder(types);
-		return builder;
-
-	}
-
-	/**
-	 * Generates a BytesWritable key for the input key
-	 * using keygenerate provided. Sort Key(s) are used to generate this object
-	 *
-	 * @param builder
-	 *         Opaque key generator created by getSortKeyGenerator() method
-	 * @param t
-	 *         Tuple to create sort key from
-	 * @return ByteWritable Key 
-	 *
-	 */
-	public static BytesWritable getSortKey(Object builder, Tuple t) throws Exception {
-		KeyGenerator kg = (KeyGenerator) builder;
-		return kg.generateKey(t);
-	}
-
-	/**
-	 * Set the table storage hint in JobContext, should be called after setSchema is
-	 * called.
-	 * <br> <br>
-	 * 
-	 * Note that the "secure by" feature is experimental now and subject to
-	 * changes in the future.
-	 *
-	 * @param jobContext
-	 *          The JobContext object.
-	 * @param storehint
-	 *          The storage hint of the BasicTable to be created. The format would
-	 *          be like "[f1, f2.subfld]; [f3, f4]".
-	 * 
-	 * @deprecated Use {@link #setStorageInfo(JobContext, ZebraSchema, ZebraStorageHint, ZebraSortInfo)} instead.
-	 */
-	public static void setStorageHint(JobContext jobContext, String storehint) throws ParseException, IOException {
-		Configuration conf = jobContext.getConfiguration();
-		String schema = ZebraConf.getOutputSchema(conf);
-
-		if (schema == null)
-			throw new ParseException("Schema has not been set");
-
-		// for sanity check purpose only
-		new Partition(schema, storehint, null);
-
-		ZebraConf.setOutputStorageHint(conf, storehint);
-	}
-
-	/**
-	 * Get the table storage hint in JobContext.
-	 * 
-	 * @param jobContext
-	 *          The JobContext object.
-	 * @return The storage hint of the BasicTable. If the storage hint is not
-	 *         defined in the jobContext object at the time of the call, an empty string
-	 *         will be returned.
-	 */
-	public static String getStorageHint(JobContext jobContext) {
-		Configuration conf = jobContext.getConfiguration();
-		String storehint = ZebraConf.getOutputStorageHint(conf);
-		return storehint == null ? "" : storehint;
-	}
-
-	/**
-	 * Set the sort info
-	 *
-	 * @param jobContext
-	 *          The JobContext object.
-	 *          
-	 * @param sortColumns
-	 *          Comma-separated sort column names
-	 *          
-	 * @param comparatorClass
-	 *          comparator class name; null for default
-	 *
-	 * @deprecated Use {@link #setStorageInfo(JobContext, ZebraSchema, ZebraStorageHint, ZebraSortInfo)} instead.
-	 */
-	public static void setSortInfo(JobContext jobContext, String sortColumns, Class<? extends RawComparator<Object>> comparatorClass) {
-		Configuration conf = jobContext.getConfiguration();
-		ZebraConf.setOutputSortColumns(conf, sortColumns);
-		if (comparatorClass != null)
-		  ZebraConf.setOutputComparator(conf, TFile.COMPARATOR_JCLASS+comparatorClass.getName());
-	}
-
-	/**
-	 * Set the sort info
-	 *
-	 * @param jobContext
-	 *          The JobContext object.
-	 *          
-	 * @param sortColumns
-	 *          Comma-separated sort column names
-	 *          
-	 * @deprecated Use {@link #setStorageInfo(JobContext, ZebraSchema, ZebraStorageHint, ZebraSortInfo)} instead.          
-	 */
-	public static void setSortInfo(JobContext jobContext, String sortColumns) {
-	  ZebraConf.setOutputSortColumns(jobContext.getConfiguration(), sortColumns);
-	}  
-
-	/**
-	 * Set the table storage info including ZebraSchema, 
-	 *
-	 * @param jobcontext
-	 *          The JobContext object.
-	 *          
-	 * @param zSchema The ZebraSchema object containing schema information.
-	 *  
-	 * @param zStorageHint The ZebraStorageHint object containing storage hint information.
-	 * 
-	 * @param zSortInfo The ZebraSortInfo object containing sorting information.
-	 *          
-	 */
-	public static void setStorageInfo(JobContext jobContext, ZebraSchema zSchema, ZebraStorageHint zStorageHint, ZebraSortInfo zSortInfo) 
-	throws ParseException, IOException {
-		String schemaStr = null;
-		String storageHintStr = null;
-
-		/* validity check on schema*/
-		if (zSchema == null) {
-			throw new IllegalArgumentException("ZebraSchema object cannot be null.");
-		} else {
-			schemaStr = zSchema.toString();
-		}
-
-		Schema schema = null;
-		try {
-			schema = new Schema(schemaStr);
-		} catch (ParseException e) {
-			throw new ParseException("[" + zSchema + "] " + " is not a valid schema string: " + e.getMessage());
-		}
-
-		/* validity check on storage hint*/
-		if (zStorageHint == null) {
-			storageHintStr = "";
-		} else {
-			storageHintStr = zStorageHint.toString();
-		}
-
-		try {
-			new Partition(schemaStr, storageHintStr, null);
-		} catch (ParseException e) {
-			throw new ParseException("[" + zStorageHint + "] " + " is not a valid storage hint string: " + e.getMessage()  ); 
-		} catch (IOException e) {
-			throw new ParseException("[" + zStorageHint + "] " + " is not a valid storage hint string: " + e.getMessage()  );
-		}
-
-		Configuration conf = jobContext.getConfiguration();
-		ZebraConf.setOutputSchema(conf, schemaStr);
-		ZebraConf.setOutputStorageHint(conf, storageHintStr);		
-
-		/* validity check on sort info if user specifies it */
-		if (zSortInfo != null) {
-			String sortColumnsStr = zSortInfo.getSortColumns();
-			String comparatorStr = zSortInfo.getComparator();      
-
-			/* Check existence of comparable class if user specifies it */
-			if (comparatorStr != null && comparatorStr != "") {
-				try {
-					conf.getClassByName(comparatorStr.substring(TFile.COMPARATOR_JCLASS.length()).trim());
-				} catch (ClassNotFoundException e) {
-					throw new IOException("comparator Class cannot be found : " + e.getMessage());        
-				}
-			}
-
-			try {
-				SortInfo.parse(sortColumnsStr, schema, comparatorStr);
-			} catch (IOException e) {
-				throw new IOException("[" + sortColumnsStr + " + " + comparatorStr + "] " 
-						+ "is not a valid sort configuration: " + e.getMessage());
-			}
-
-			if (sortColumnsStr != null)
-			  ZebraConf.setOutputSortColumns(conf, sortColumnsStr);
-			if (comparatorStr != null)
-			  ZebraConf.setOutputComparator(conf, comparatorStr);
-		}
-	}
-
-	/**
-	 * Get the SortInfo object 
-	 *
-	 * @param jobContext
-	 *          The JobContext object.
-	 * @return SortInfo object; null if the Zebra table is unsorted 
-	 *
-	 */
-	public static SortInfo getSortInfo(JobContext jobContext)throws IOException
-	{
-		Configuration conf = jobContext.getConfiguration();
-		String sortColumns = ZebraConf.getOutputSortColumns(conf);
-		if (sortColumns == null)
-			return null;
-		Schema schema = null;
-		try {
-			schema = getSchema(jobContext);
-		} catch (ParseException e) {
-			throw new IOException("Schema parsing failure : "+e.getMessage());
-		}
-		if (schema == null)
-			throw new IOException("Schema not defined");
-		String comparator = getComparator(jobContext);
-		return SortInfo.parse(sortColumns, schema, comparator);
-	}
-
-	/**
-	 * Get the  comparator for sort columns
-	 *
-	 * @param jobContext
-	 *          The JobContext object.
-	 * @return  comparator String
-	 *
-	 */
-	private static String getComparator(JobContext jobContext)
-	{
-	  return ZebraConf.getOutputComparator(jobContext.getConfiguration());
-	}
-
-	/**
-	 * Get the output table as specified in JobContext. It is useful for applications
-	 * to add more meta data after all rows have been added to the table.
-	 * 
-	 * @param conf
-	 *          The JobContext object.
-	 * @return The output BasicTable.Writer object.
-	 * @throws IOException
-	 */
-	private static BasicTable.Writer[] getOutput(JobContext jobContext) throws IOException {
-		Path[] paths = getOutputPaths(jobContext);
-		BasicTable.Writer[] writers = new BasicTable.Writer[paths.length]; 
-		for(int i = 0; i < paths.length; i++) {
-			writers[i] = new BasicTable.Writer(paths[i], jobContext.getConfiguration());
-		}
-
-		return writers;
-	}
-
-	/**
-	 * Note: we perform the Initialization of the table here. So we expect this to
-	 * be called before
-	 * {@link BasicTableOutputFormat#getRecordWriter(FileSystem, JobContext, String, Progressable)}
-	 * 
-	 * @see OutputFormat#checkOutputSpecs(JobContext)
-	 */
-	@Override
-	public void checkOutputSpecs(JobContext jobContext)
-	throws IOException {
-		Configuration conf = jobContext.getConfiguration();
-		String schema = ZebraConf.getOutputSchema(conf);
-		if (schema == null) {
-			throw new IllegalArgumentException("Cannot find output schema");
-		}
-		
-		String storehint, sortColumns, comparator;
-  	storehint = getStorageHint(jobContext);
-	  sortColumns = (getSortInfo(jobContext) == null ? null : SortInfo.toSortString(getSortInfo(jobContext).getSortColumnNames()));
-		comparator = getComparator( jobContext );
-		
-		Path[] paths = getOutputPaths(jobContext);
-
-	  for (Path path : paths) {
-      BasicTable.Writer writer =
-        new BasicTable.Writer(path, schema, storehint, sortColumns, comparator, conf);
-      writer.finish();
-	  }
-	}
-
-	/**
-	 * @see OutputFormat#getRecordWriter(TaskAttemptContext)
-	 */
-	@Override
-	public RecordWriter<BytesWritable, Tuple> getRecordWriter(TaskAttemptContext taContext)
-	throws IOException {
-	  String path = ZebraConf.getOutputPath(taContext.getConfiguration());
-		return new TableRecordWriter(path, taContext);
-	}
-
-	/**
-	 * Close the output BasicTable, No more rows can be added into the table. A
-	 * BasicTable is not visible for reading until it is "closed".
-	 * 
-	 * @param jobContext
-	 *          The JobContext object.
-	 * @throws IOException
-	 */
-	public static void close(JobContext jobContext) throws IOException {
-		BasicTable.Writer tables[] = getOutput(jobContext);
-		for(int i =0; i < tables.length; ++i) {
-			tables[i].close();    	
-		}
-	}
-
-	@Override
-	public OutputCommitter getOutputCommitter(TaskAttemptContext taContext)
-	throws IOException, InterruptedException {
-		return new TableOutputCommitter( taContext ) ;
-	}
-}
-
-class TableOutputCommitter extends OutputCommitter {
-	public TableOutputCommitter(TaskAttemptContext taContext) {
-
-	}
-
-	@Override
-	public void abortTask(TaskAttemptContext taContext) throws IOException {
-	}
-
-	@Override
-	public void cleanupJob(JobContext jobContext) throws IOException {
-	}
-
-	@Override
-	public void commitTask(TaskAttemptContext taContext) throws IOException {
-	}
-
-	@Override
-	public boolean needsTaskCommit(TaskAttemptContext taContext)
-	throws IOException {
-		return false;
-	}
-
-	@Override
-	public void setupJob(JobContext jobContext) throws IOException {
-		// TODO Auto-generated method stub
-
-	}
-
-	@Override
-	public void setupTask(TaskAttemptContext taContext) throws IOException {
-		// TODO Auto-generated method stub
-
-	}
-}
-
-/**
- * Adaptor class for BasicTable RecordWriter.
- */
-class TableRecordWriter extends RecordWriter<BytesWritable, Tuple> {
-	private final TableInserter inserter[];
-	private org.apache.hadoop.zebra.mapreduce.ZebraOutputPartition op = null;
- 	
- 	// for Pig's call path;
- 	final private BytesWritable KEY0 = new BytesWritable(new byte[0]);
-   private int[] sortColIndices = null;
-   private KeyGenerator builder = null;
-   private Tuple t = null;
-   
-	public TableRecordWriter(String path, TaskAttemptContext context) throws IOException {	
-		Configuration conf = context.getConfiguration();
-    if(ZebraConf.getIsMulti(conf, false) == true) {
-			op = (org.apache.hadoop.zebra.mapreduce.ZebraOutputPartition) 
-			ReflectionUtils.newInstance(BasicTableOutputFormat.getZebraOutputPartitionClass(context), conf);
-		}
-		
-    boolean checkType = ZebraConf.getCheckType(conf, true);
-		Path [] paths = BasicTableOutputFormat.getOutputPaths(context);
-		inserter = new TableInserter[paths.length];
-                String inserterName = "part-" + context.getTaskAttemptID().getTaskID().getId();
-		for(int i = 0; i < paths.length; ++i) {
-			BasicTable.Writer writer =
-				new BasicTable.Writer(paths[i], conf);
-			this.inserter[i] = writer.getInserter( inserterName, true, checkType);
-
-			// Set up SortInfo related stuff only once;
-			if (i == 0) {
-	      if (writer.getSortInfo() != null)
-	      {
-          sortColIndices = writer.getSortInfo().getSortIndices();
-          SortInfo sortInfo =  writer.getSortInfo();
-          String[] sortColNames = sortInfo.getSortColumnNames();
-          org.apache.hadoop.zebra.schema.Schema schema = writer.getSchema();
-
-          byte[] types = new byte[sortColNames.length];
-          
-          for(int j =0 ; j < sortColNames.length; ++j){
-              types[j] = schema.getColumn(sortColNames[j]).getType().pigDataType();
-          }
-          t = TypesUtils.createTuple(sortColNames.length);
-          builder = makeKeyBuilder(types);
-	      }
-			}
-		}
-	}
-	
-  private KeyGenerator makeKeyBuilder(byte[] elems) {
-    ComparatorExpr[] exprs = new ComparatorExpr[elems.length];
-    for (int i = 0; i < elems.length; ++i) {
-        exprs[i] = ExprUtils.primitiveComparator(i, elems[i]);
-    }
-    return new KeyGenerator(ExprUtils.tupleComparator(exprs));
-  }
-
-
-	@Override
-	public void close(TaskAttemptContext context) throws IOException {
-		for(int i = 0; i < this.inserter.length; ++i) {  
-			inserter[i].close();
-		}  
-	}
-
-	@Override
-	public void write(BytesWritable key, Tuple value) throws IOException {
-    if (key == null) {
-      if (sortColIndices != null) { // If this is a sorted table and key is null (Pig's call path);
-        for (int i =0; i < sortColIndices.length;++i) {
-          t.set(i, value.get(sortColIndices[i]));
-        }
-        key = builder.generateKey(t);        
-      } else { // for unsorted table;
-        key = KEY0;
-      }
-    }
-	  	  
-		if(op != null ) {	  
-			int idx = op.getOutputPartition(key, value);
-			if(idx < 0 || (idx >= inserter.length)) {
-				throw new IllegalArgumentException("index returned by getOutputPartition is out of range");
-			}
-			inserter[idx].insert(key, value);
-		} else {
-			inserter[0].insert(key, value);
-		}
-	}
-}
-
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/CachedTableScanner.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/CachedTableScanner.java
deleted file mode 100644
index 05591a446..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/CachedTableScanner.java
+++ /dev/null
@@ -1,171 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.Closeable;
-import java.io.IOException;
-
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.types.Projection;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.Tuple;
-
-/**
- * Utility class for those interested in writing their own TableExpr classes.
- * This class encapsulates a regular table scanner and changes the way how keys
- * and rows are accessed (with an internal cache).
- */
-final class CachedTableScanner implements Closeable {
-  private BytesWritable key;
-  private Tuple row;
-  private boolean keyReady;
-  private boolean rowReady;
-  private int index;
-  private TableScanner scanner;
-
-  /**
-   * Constructor
-   * 
-   * @param scanner
-   *          The scanner to be encapsulated
-   * @throws IOException 
-   */
-  public CachedTableScanner(TableScanner scanner, int index) throws IOException {
-    key = new BytesWritable();
-    row = TypesUtils.createTuple(Projection.getNumColumns(scanner.getProjection()));
-    keyReady = false;
-    rowReady = false;
-    this.index = index;
-    this.scanner = scanner;
-  }
-
-  /**
-   * Get the key at cursor.
-   * 
-   * @return The key at cursor
-   * @throws IOException
-   */
-  public BytesWritable getKey() throws IOException {
-    if (!keyReady) {
-      scanner.getKey(key);
-      keyReady = true;
-    }
-    return key;
-  }
-
-  /**
-   * Get the value (Tuple) at cursor.
-   * 
-   * @return the value at cursor.
-   * @throws IOException
-   */
-  public Tuple getValue() throws IOException {
-    if (!rowReady) {
-      scanner.getValue(row);
-      rowReady = true;
-    }
-    return row;
-  }
-
-  /**
-   * Get the table index in a union
-   * 
-   * @return the table index in union
-   */
-  public int getIndex() {
-    return index;
-    
-  }
-  /**
-   * Seek to a row whose key is greater than or equal to the input key.
-   * 
-   * @param inKey
-   *          the input key.
-   * @return true if an exact matching key is found. false otherwise.
-   * @throws IOException
-   */
-  public boolean seekTo(BytesWritable inKey) throws IOException {
-    boolean ret = scanner.seekTo(inKey);
-    reset();
-    return ret;
-  }
-
-  /**
-   * Advance the cursor.
-   * 
-   * @return whether the cursor actually moves.
-   * @throws IOException
-   */
-  public boolean advance() throws IOException {
-    boolean ret = scanner.advance();
-    reset();
-    return ret;
-  }
-
-  private void reset() throws IOException {
-    row = TypesUtils.createTuple(Projection.getNumColumns(scanner.getProjection()));
-    keyReady = false;
-    rowReady = false;
-  }
-
-  /**
-   * Is cursor at end?
-   * 
-   * @return Whether cursor is at the end.
-   * @throws IOException
-   */
-  public boolean atEnd() throws IOException {
-    return scanner.atEnd();
-  }
-
-  /**
-   * Get the schema of the tuples returned from this scanner.
-   * 
-   * @return The schema of the tuples returned from this scanner.
-   */
-  public String getProjection() {
-    return scanner.getProjection();
-  }
-  
-  /**
-   * Get the projected schema
-   * @return The projected schema
-   */
-  public Schema getSchema() {
-    return scanner.getSchema();
-  }
-
-  /**
-   * Seek to the end of the scanner.
-   * 
-   * @throws IOException
-   */
-  public void seekToEnd() throws IOException {
-    reset();
-    scanner.seekToEnd();
-  }
-
-  /**
-   * Close the scanner, release all resources.
-   */
-  @Override
-  public void close() throws IOException {
-    scanner.close();
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/CompositeTableExpr.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/CompositeTableExpr.java
deleted file mode 100644
index 200114cf0..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/CompositeTableExpr.java
+++ /dev/null
@@ -1,304 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.IOException;
-import java.io.PrintStream;
-import java.io.StringReader;
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Iterator;
-import java.util.LinkedHashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-import java.util.TreeMap;
-
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-
-
-/**
- * An intermediate class to build concrete composite table expression classes.
- * Lengthy names are chosen to avoid naming conflicts.
- */
-abstract class CompositeTableExpr extends TableExpr {
-  public final static int INDENT_UNIT = 2;
-  protected ArrayList<TableExpr> composite;
-
-  protected CompositeTableExpr() {
-    composite = new ArrayList<TableExpr>();
-  }
-
-  protected CompositeTableExpr(int n) {
-    composite = new ArrayList<TableExpr>(n);
-    for (int i = 0; i < n; ++i) {
-      composite.add(null);
-    }
-  }
-
-  protected CompositeTableExpr addCompositeTable(TableExpr expr) {
-    composite.add(expr);
-    return this;
-  }
-
-  protected CompositeTableExpr addCompositeTables(TableExpr[] exprs) {
-    composite.ensureCapacity(composite.size() + exprs.length);
-    for (TableExpr expr : exprs) {
-      composite.add(expr);
-    }
-    return this;
-  }
-
-  protected CompositeTableExpr addCompositeTables(
-      Collection<? extends TableExpr> exprs) {
-    composite.addAll(exprs);
-    return this;
-  }
-
-  protected CompositeTableExpr setCompositeTable(int i, TableExpr expr) {
-    composite.set(i, expr);
-    return this;
-  }
-
-  protected List<TableExpr> getCompositeTables() {
-    return composite;
-  }
-
-  protected TableExpr getCompositeTable(int i) {
-    TableExpr expr = composite.get(i);
-    if (expr == null) {
-      throw new RuntimeException("Incomplete Nary expression");
-    }
-
-    return expr;
-  }
-
-  @Override
-  protected CompositeTableExpr decodeParam(StringReader in) throws IOException {
-    composite.clear();
-    int n = TableExprUtils.decodeInt(in);
-    composite.ensureCapacity(n);
-    for (int i = 0; i < n; ++i) {
-      TableExpr expr = TableExpr.parse(in);
-      composite.add(expr);
-    }
-
-    return this;
-  }
-
-  @Override
-  protected CompositeTableExpr encodeParam(StringBuilder out) {
-    int n = composite.size();
-    TableExprUtils.encodeInt(out, n);
-    for (int i = 0; i < composite.size(); ++i) {
-      getCompositeTable(i).encode(out);
-    }
-
-    return this;
-  }
-
-  @Override
-  public List<LeafTableInfo> getLeafTables(String projection) {
-    ArrayList<LeafTableInfo> ret = new ArrayList<LeafTableInfo>();
-
-    int n = composite.size();
-    for (int i = 0; i < n; ++i) {
-      ret.addAll(getCompositeTable(i).getLeafTables(projection));
-    }
-
-    return ret;
-  }
-
-  protected static class RowMappingEntry {
-    final int rowIndex;
-    final int fieldIndex;
-
-    public RowMappingEntry(int ri, int fi) {
-      rowIndex = ri;
-      fieldIndex = fi;
-    }
-
-    public int getRowIndex() {
-      return rowIndex;
-    }
-
-    public int getFieldIndex() {
-      return fieldIndex;
-    }
-  }
-
-  protected static class InferredProjection {
-    /**
-     * projections on each table in the composite. It should be of the same
-     * dimension as composite.
-     */
-    final Schema[] subProjections;
-
-    /**
-     * The actual (adjusted) input projection.
-     */
-    final Schema projection;
-
-    /**
-     * For each column in the input projection, what is the corresponding sub
-     * table (row index) and the corresponding projected column (field index).
-     */
-    final RowMappingEntry[] colMapping;
-
-    InferredProjection(List<String>[] subProj, String[] proj,
-        Map<String, RowMappingEntry> colMap) throws ParseException {
-      subProjections = new Schema[subProj.length];
-      for (int i = 0; i < subProj.length; ++i) {
-        List<String> subProjection = subProj[i];
-        subProjections[i] =
-            new Schema(subProjection.toArray(new String[subProjection.size()]));
-      }
-      
-      projection = new Schema(proj);
-      
-      this.colMapping = new RowMappingEntry[proj.length];
-      
-      for (int i = 0; i < proj.length; ++i) {
-        colMapping[i] = colMap.get(proj[i]);
-      }
-    }
-
-    public Schema[] getSubProjections() {
-      return subProjections;
-    }
-
-    public RowMappingEntry[] getColMapping() {
-      return colMapping;
-    }
-
-    public Schema getProjection() {
-      return projection;
-    }
-  }
-
-  /**
-   * Given the input projection, infer the projections that should be applied on
-   * individual table in the composite.
-   * 
-   * @param projection
-   *          The expected projection
-   * @return The inferred projection with other information.
-   * @throws IOException
-   */
-  @SuppressWarnings("unchecked")
-  protected InferredProjection inferProjection(String projection,
-      Configuration conf)
-      throws IOException, ParseException {
-    int n = composite.size();
-    ArrayList<String>[] subProjections = new ArrayList[n];
-    for (int i = 0; i < n; ++i) {
-      subProjections[i] = new ArrayList<String>();
-    }
-    
-    String[] columns;
-    LinkedHashMap<String, Integer> colNameMap =
-        new LinkedHashMap<String, Integer>();
-    TreeMap<String, RowMappingEntry> colMapping =
-        new TreeMap<String, RowMappingEntry>();
-
-    /**
-     * Create a collection of all column names and the corresponding index to
-     * the composite.
-     */
-    for (int i = 0; i < n; ++i) {
-      String[] cols = getCompositeTable(i).getSchema(conf).getColumns();
-      for (int j = 0; j < cols.length; ++j) {
-        if (colNameMap.get(cols[j]) != null) {
-          colNameMap.put(cols[j], -1); // special marking on duplicated names.
-        }
-        else {
-          colNameMap.put(cols[j], i);
-        }
-      }
-    }
-
-    if (projection == null) {
-      Set<String> keySet = colNameMap.keySet();
-      columns = keySet.toArray(new String[keySet.size()]);
-    }
-    else {
-      columns = projection.trim().split(Schema.COLUMN_DELIMITER);
-    }
-
-    for (int i = 0; i < columns.length; ++i) {
-      String col = columns[i];
-
-      if (colMapping.get(col) != null) {
-        throw new IllegalArgumentException(
-            "Duplicate column names in projection");
-      }
-
-      Integer rowIndex = colNameMap.get(col);
-      if (rowIndex == null) {
-        colMapping.put(col, null);
-      }
-      else {
-        if (rowIndex < 0) {
-          // The column name appears in more than one table in composite.
-          throw new RuntimeException("Ambiguous column in projection: "
-              + col);
-        }
-        subProjections[rowIndex].add(columns[i]);
-        colMapping.put(col, new RowMappingEntry(rowIndex,
-            subProjections[rowIndex].size() - 1));
-      }
-    }
-
-    return new InferredProjection(subProjections, columns, colMapping);
-  }
-
-  @Override
-  public Schema getSchema(Configuration conf) throws IOException {
-    Schema result = new Schema();
-    for (Iterator<TableExpr> it = composite.iterator(); it.hasNext();) {
-      TableExpr e = it.next();
-      try {
-        result.unionSchema(e.getSchema(conf));
-      } catch (ParseException exc) {
-        throw new IOException("Schema parsing failed :"+exc.getMessage());
-      }
-    }
-    return result;
-  }
-  
-  @Override
-  public boolean sortedSplitCapable() {
-    for (Iterator<TableExpr> it = composite.iterator(); it.hasNext();) {
-      TableExpr e = it.next();
-      if (!e.sortedSplitCapable()) return false;
-    }
-
-    return true;
-  }
-  
-  @Override
-  protected void dumpInfo(PrintStream ps, Configuration conf, int indent) throws IOException
-  {
-    for (Iterator<TableExpr> it = composite.iterator(); it.hasNext();) {
-      TableExpr e = it.next();
-      e.dumpInfo(ps, conf, indent+INDENT_UNIT);
-    }
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/NullScanner.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/NullScanner.java
deleted file mode 100644
index 964e8a841..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/NullScanner.java
+++ /dev/null
@@ -1,89 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.EOFException;
-import java.io.IOException;
-
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Projection;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.pig.data.Tuple;
-
-/**
- * A scanner that contains no rows.
- */
-class NullScanner implements TableScanner {
-  String projection;
-
-  public NullScanner(String projection) {
-    this.projection = projection;
-  }
-  
-  @Override
-  public boolean advance() throws IOException {
-    return false;
-  }
-
-  @Override
-  public boolean atEnd() throws IOException {
-    return true;
-  }
-
-  @Override
-  public String getProjection() {
-    return projection;
-  }
-  
-  @Override
-  public Schema getSchema() {
-    Schema result;
-    try {
-       result = Projection.toSchema(projection);
-    } catch (ParseException e) {
-      throw new AssertionError("Invalid Projection: "+e.getMessage());
-    }
-    return result;
-  }
-
-  @Override
-  public void getKey(BytesWritable key) throws IOException {
-    throw new EOFException("No more rows to read");
-  }
-
-  @Override
-  public void getValue(Tuple row) throws IOException {
-    throw new EOFException("No more rows to read");
-  }
-
-  @Override
-  public boolean seekTo(BytesWritable key) throws IOException {
-    return false;
-  }
-
-  @Override
-  public void seekToEnd() throws IOException {
-    // Do nothing
-  }
-
-  @Override
-  public void close() throws IOException {
-    // Do nothing
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/SortedTableSplitComparable.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/SortedTableSplitComparable.java
deleted file mode 100644
index d71768df9..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/SortedTableSplitComparable.java
+++ /dev/null
@@ -1,85 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-
-import org.apache.hadoop.io.WritableComparable;
-import org.apache.pig.data.DataReaderWriter;
-
-public class SortedTableSplitComparable implements WritableComparable<SortedTableSplitComparable> {
-    private static final long serialVersionUID = 1L;
-    
-    protected Integer index;
-    
-    //need a default constructor to be able to de-serialize using just 
-    // the Writable interface
-    public SortedTableSplitComparable(){}
-    
-    public SortedTableSplitComparable(int index){
-        this.index = index;
-    }
-
-
-    @Override
-    public int compareTo(SortedTableSplitComparable other) {
-        return Integer.signum( index - other.index );
-    }
-
-
-    @Override
-    public void readFields(DataInput in) throws IOException {
-        index = (Integer)DataReaderWriter.readDatum(in);
-    }
-
-
-    @Override
-    public void write(DataOutput out) throws IOException {
-        DataReaderWriter.writeDatum(out, index);
-    }
-
-    @Override
-    public String toString(){
-        return "Index = " + index ; 
-    }
-
-    /* (non-Javadoc)
-     * @see java.lang.Object#hashCode()
-     */
-    @Override
-    public int hashCode() {
-        return index;
-    }
-
-    /* (non-Javadoc)
-     * @see java.lang.Object#equals(java.lang.Object)
-     */
-    @Override
-    public boolean equals(Object obj) {
-        if (this == obj)
-            return true;
-        if (obj == null)
-            return false;
-        if (getClass() != obj.getClass())
-            return false;
-        SortedTableSplitComparable other = (SortedTableSplitComparable) obj;
-        return this.index.intValue() == other.index.intValue();
-    }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/TableExpr.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/TableExpr.java
deleted file mode 100644
index eeb819874..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/TableExpr.java
+++ /dev/null
@@ -1,245 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.IOException;
-import java.io.StringReader;
-import java.io.PrintStream;
-import java.util.List;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-
-/**
- * Table Expression - expression to describe an input table.
- */
-abstract class TableExpr {
-  private boolean sorted = false;
-  /**
-   * Factory method to create a TableExpr from a string.
-   * 
-   * @param in
-   *          The string stream that is pointed at the beginning of the table
-   *          expression string to be parsed.
-   * @return The instantiated TableExpr object. The string stream will move past
-   *         the table expression string.
-   * @throws IOException
-   */
-  @SuppressWarnings("unchecked")
-  public static TableExpr parse(StringReader in) throws IOException {
-    String clsName = TableExprUtils.decodeString(in);
-    try {
-      Class<? extends TableExpr> tblExprCls =
-          (Class<? extends TableExpr>) Class.forName(clsName);
-      return tblExprCls.newInstance().decodeParam(in);
-    }
-    catch (Exception e) {
-      throw new RuntimeException("Failed to load class: " + e.toString());
-    }
-  }
-
-  /**
-   * Encode the expression to a string stream.
-   * 
-   * @param out
-   *          The string stream that we should encode the expression into.
-   */
-  public final void encode(StringBuilder out) {
-    TableExprUtils.encodeString(out, getClass().getName());
-    encodeParam(out);
-  }
-
-  /**
-   * Decode the parameters of the expression from the input stream.
-   * 
-   * @param in
-   *          The string stream that is pointed at the beginning of the encoded
-   *          parameters.
-   * @return Reference to itself, and the TableExpr that has its parameters set
-   *         from the string stream. The string stream will move past the
-   *         encoded parameters for this expression.
-   */
-  protected abstract TableExpr decodeParam(StringReader in) throws IOException;
-
-  /**
-   * Encode the parameters of the expression into the string stream.
-   * 
-   * @param out
-   *          The string stream that we write encoded parameters into.
-   * @return Reference to itself,
-   */
-  protected abstract TableExpr encodeParam(StringBuilder out);
-
-  /**
-   * Get a TableScanner from the TableExpr object. This method only needs to be
-   * implemented by table expressions that support sorted split.
-   * 
-   * @param begin
-   *          the Begin key (inclusive). Can be null, meaning starting from the
-   *          first row of the table.
-   * @param end
-   *          the End key (exclusive). Can be null, meaning scan to the last row
-   *          of the table.
-   * @param projection
-   *          The projection schema. It should never be null.
-   * @see Schema
-   * @return A TableScanner object.
-   */
-  public TableScanner getScanner(BytesWritable begin,
-      BytesWritable end, String projection, Configuration conf)
-      throws IOException {
-    return null;
-  }
-
-  /**
-   * Get a scanner with an row split.
-   * 
-   * @param split
-   *          The row split.
-   * @param projection
-   *          The projection schema. It should never be null.
-   * @param conf
-   *          The configuration
-   * @return A table scanner.
-   * @throws IOException
-   */
-  public TableScanner getScanner(RowTableSplit split, String projection,
-      Configuration conf) throws IOException, ParseException {
-    return null;
-  }
-  
-  /**
-   * A leaf table corresponds to a materialized table. It is represented by the
-   * path to the BasicTable and the projection.
-   */
-  public static final class LeafTableInfo {
-    private final Path path;
-    private final String projection;
-
-    public LeafTableInfo(Path path, String projection) {
-      this.path = path;
-      this.projection = projection;
-    }
-
-    public Path getPath() {
-      return path;
-    }
-
-    public String getProjection() {
-      return projection;
-    }
-  }
-
-  /**
-   * Get the information of all leaf tables that will be accessed by this table
-   * expression.
-   * 
-   * @param projection
-   *          The projection that is applied to the table expression.
-   */
-  public abstract List<LeafTableInfo> getLeafTables(
-      String projection);
-
-  /**
-   * Get the schema of the table.
-   * 
-   * @param conf
-   *          The configuration object.
-   */
-  public abstract Schema getSchema(Configuration conf) throws IOException;
-
-  /**
-   * Does this expression requires sorted split? If yes, we require all
-   * underlying BasicTables to be sorted and we split by key sampling. If this
-   * method returns true, we expect sortedSplitCapable() also return true.
-   * 
-   * @return Whether this expression may only be split by key.
-   */
-  public boolean sortedSplitRequired() {
-    return sorted;
-  }
-
-  /**
-   * Set the requirement for sorted table
-   */
-  public void setSortedSplit() {
-    sorted = true;
-  }
-
-  /**
-   * Is this expression capable of sorted split? If false, getScanner() should
-   * return null; otherwise, getScanner() should return a valid Scanner object.
-   * 
-   * This function should be overridden by sub classes that is capable of sorted
-   * split. Note that this method should not perform any actual I/O operation,
-   * such as checking whether the leaf tables (BasicTables) is in fact sorted or
-   * not. When this method returns true, while at least one of the leaf tables
-   * is not sorted, an {@link IOException} will be thrown in split generation
-   * time.
-   * 
-   * @return Whether the "table view" represented by the expression is sorted
-   *         and is thus splittable by key (sorted split).
-   */
-  public boolean sortedSplitCapable() {
-    return false;
-  }
-
-  /**
-   * dump table info
-   */
-  public final void dumpInfo(PrintStream ps, Configuration conf) throws IOException
-  {
-    dumpInfo(ps, conf, 0);
-  }
-  
-  /**
-   * dump table info with indent
-   */
-  protected abstract void dumpInfo(PrintStream ps, Configuration conf, int indent) throws IOException;
-
-  /**
-   * get the deleted cg for tables in union
-   * @param conf The Configuration object
-   * @return
-   */
-  protected final String[] getDeletedCGsPerUnion(Configuration conf) {
-    return getDeletedCGs(conf, TableInputFormat.DELETED_CG_SEPARATOR_PER_UNION);
-  }
-  
-  protected final String[] getDeletedCGs(Configuration conf) {
-    return getDeletedCGs(conf, BasicTable.DELETED_CG_SEPARATOR_PER_TABLE);
-  }
-  
-  private final String[] getDeletedCGs(Configuration conf, String separator) {
-    String[] deletedCGs = null;
-    String fe;
-    if ((fe = conf.get(TableInputFormat.INPUT_FE)) != null && fe.equals("true"))
-    {
-      String original = conf.get(TableInputFormat.INPUT_DELETED_CGS, null);
-      if (original == null)
-        deletedCGs = new String[0]; // empty array needed to indicate it is fe checked
-      else
-        deletedCGs = original.split(separator, -1);
-    }
-    return deletedCGs;
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/TableExprUtils.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/TableExprUtils.java
deleted file mode 100644
index eef5f54f9..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/TableExprUtils.java
+++ /dev/null
@@ -1,116 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.IOException;
-import java.io.StringReader;
-
-/**
- * Utility methods for people interested in writing their own TableExpr classes.
- */
-class TableExprUtils {
-  private TableExprUtils() {
-    // prevent instantiation
-  }
-
-  /**
-   * Encode a string into a StringBuilder.
-   * 
-   * @param out
-   *          output string builder.
-   * @param s
-   *          String to be encoded
-   */
-  public static void encodeString(StringBuilder out, String s) {
-    if (s == null) {
-      encodeInt(out, null);
-      return;
-    }
-    
-    encodeInt(out, s.length());
-    out.append(s);
-  }
-  
-  /**
-   * Decode a string previously encoded through encodeString.
-   * 
-   * @param in
-   *          The input StringReader.
-   * @return The decoded string object.
-   * @throws IOException
-   */
-  public static String decodeString(StringReader in) throws IOException {
-    Integer len = decodeInt(in);
-    if (len == null) {
-      return null;
-    }
-
-    char[] chars = new char[len];
-    in.read(chars);
-    return new String(chars);
-  }
-  
-  public static void encodeLong(StringBuilder out, Long l) {
-    if (l != null) {
-      out.append(l);
-    }
-    out.append(':');
-  }
-
-  public static Long decodeLong(StringReader in) throws IOException {
-    int c = in.read();
-    if (c == ':') {
-      return null;
-    }
-    if (Character.isDigit(c) == false) {
-      throw new IllegalArgumentException("Bad encoded string");
-    }
-    StringBuilder sb = new StringBuilder();
-    sb.append((char) c);
-
-    boolean colonSeen = false;
-    while ((c = in.read()) != -1) {
-      if (c == ':') {
-        colonSeen = true;
-        break;
-      }
-      if (!Character.isDigit(c)) {
-        break;
-      }
-      sb.append((char) c);
-    }
-    if (colonSeen == false) {
-      throw new IllegalArgumentException("Bad encoded string");
-    }
-    return new Long(sb.toString());
-  }
-  
-  public static void encodeInt(StringBuilder out, Integer i) {
-    if (i == null) {
-      encodeLong(out, null);
-    }
-    else {
-      encodeLong(out, (long) i.intValue());
-    }
-  }
-
-  public static Integer decodeInt(StringReader in) throws IOException {
-    Long l = decodeLong(in);
-    if (l == null) return null;
-    return (int) l.longValue();
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/TableInputFormat.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/TableInputFormat.java
deleted file mode 100644
index a6879fd27..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/TableInputFormat.java
+++ /dev/null
@@ -1,1268 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-import java.io.StringReader;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.PathFilter;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.WritableComparable;
-import org.apache.hadoop.io.WritableUtils;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.zebra.tfile.RawComparable;
-import org.apache.hadoop.mapreduce.InputFormat;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.InputSplit;
-import org.apache.hadoop.mapreduce.lib.input.FileSplit;
-import org.apache.hadoop.mapreduce.JobContext;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.lib.input.InvalidInputException;
-import org.apache.hadoop.mapreduce.RecordReader;
-import org.apache.hadoop.mapreduce.TaskAttemptContext;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.BasicTableStatus;
-import org.apache.hadoop.zebra.io.BlockDistribution;
-import org.apache.hadoop.zebra.io.KeyDistribution;
-import org.apache.hadoop.zebra.io.BasicTable.Reader;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RowSplit;
-import org.apache.hadoop.zebra.mapreduce.TableExpr.LeafTableInfo;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Projection;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.SortInfo;
-import org.apache.hadoop.zebra.tfile.TFile;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-
-/**
- * {@link org.apache.hadoop.mapreduce.InputFormat} class for reading one or more
- * BasicTables.
- * 
- * Usage Example:
- * <p>
- * In the main program, add the following code.
- * 
- * <pre>
- * job.setInputFormatClass(TableInputFormat.class);
- * TableInputFormat.setInputPaths(jobContext, new Path(&quot;path/to/table1&quot;, new Path(&quot;path/to/table2&quot;);
- * TableInputFormat.setProjection(jobContext, &quot;Name, Salary, BonusPct&quot;);
- * </pre>
- * 
- * The above code does the following things:
- * <UL>
- * <LI>Set the input format class to TableInputFormat.
- * <LI>Set the paths to the BasicTables to be consumed by user's Mapper code.
- * <LI>Set the projection on the input tables. In this case, the Mapper code is
- * only interested in three fields: "Name", "Salary", "BonusPct". "Salary"
- * (perhaps for the purpose of calculating the person's total payout). If no
- * project is specified, then all columns from the input tables will be
- * retrieved. If input tables have different schemas, then the input contains
- * the union of all columns from all the input tables. Absent fields will be
- * left as nul in the input tuple.
- * </UL>
- * The user Mapper code should look like the following:
- * 
- * <pre>
- * static class MyMapClass implements Mapper&lt;BytesWritable, Tuple, K, V&gt; {
- *   // keep the tuple object for reuse.
- *   // indices of various fields in the input Tuple.
- *   int idxName, idxSalary, idxBonusPct;
- * 
- *   &#064;Override
- *   public void configure(Job job) {
- *     Schema projection = TableInputFormat.getProjection(job);
- *     // determine the field indices.
- *     idxName = projection.getColumnIndex(&quot;Name&quot;);
- *     idxSalary = projection.getColumnIndex(&quot;Salary&quot;);
- *     idxBonusPct = projection.getColumnIndex(&quot;BonusPct&quot;);
- *   }
- * 
- *   &#064;Override
- *   public void map(BytesWritable key, Tuple value, OutputCollector&lt;K, V&gt; output,
- *       Reporter reporter) throws IOException {
- *     try {
- *       String name = (String) value.get(idxName);
- *       int salary = (Integer) value.get(idxSalary);
- *       double bonusPct = (Double) value.get(idxBonusPct);
- *       // do something with the input data
- *     } catch (ExecException e) {
- *       e.printStackTrace();
- *     }
- *   }
- * 
- *   &#064;Override
- *   public void close() throws IOException {
- *     // no-op
- *   }
- * }
- * </pre>
- * 
- * A little bit more explanation on the PIG {@link Tuple} objects. A Tuple is an
- * ordered list of PIG datum objects. The permitted PIG datum types can be
- * categorized as Scalar types and Composite types.
- * <p>
- * Supported Scalar types include seven native Java types: Boolean, Byte,
- * Integer, Long, Float, Double, String, as well as one PIG class called
- * {@link DataByteArray} that represents type-less byte array.
- * <p>
- * Supported Composite types include:
- * <UL>
- * <LI>{@link Map} : It is the same as Java Map class, with the additional
- * restriction that the key-type must be one of the scalar types PIG recognizes,
- * and the value-type any of the scaler or composite types PIG understands.
- * <LI>{@link DataBag} : A DataBag is a collection of Tuples.
- * <LI>{@link Tuple} : Yes, Tuple itself can be a datum in another Tuple.
- * </UL>
- */
-public class TableInputFormat extends InputFormat<BytesWritable, Tuple> {
-  public enum SplitMode {
-    UNSORTED, /* Data is not sorted. Default split mode*/
-    LOCALLY_SORTED, /* Output by the each mapper is sorted */
-    GLOBALLY_SORTED /* Output is locally sorted and the key ranges are not overlapped, not even on boundary */
-  };
-    
-  static Log LOG = LogFactory.getLog(TableInputFormat.class);
-  
-  private static final String INPUT_EXPR = "mapreduce.lib.table.input.expr";
-  private static final String INPUT_PROJ = "mapreduce.lib.table.input.projection";
-  private static final String INPUT_SORT = "mapreduce.lib.table.input.sort";
-  static final String INPUT_FE = "mapreduce.lib.table.input.fe";
-  static final String INPUT_DELETED_CGS = "mapreduce.lib.table.input.deleted_cgs";
-  private static final String INPUT_SPLIT_MODE = "mapreduce.lib.table.input.split_mode";
-  private static final String UNSORTED = "unsorted";
-  private static final String GLOBALLY_SORTED = "globally_sorted";
-  private static final String LOCALLY_SORTED = "locally_sorted";
-  static final String DELETED_CG_SEPARATOR_PER_UNION = ";";
-  
-  /**
-   * Set the paths to the input table.
-   * 
-   * @param conf
-   *          JobContext object.
-   * @param paths
-   *          one or more paths to BasicTables. The InputFormat class will
-   *          produce splits on the "union" of these BasicTables.
-   */
-  public static void setInputPaths(JobContext jobContext, Path... paths) {
-    if (paths.length < 1) {
-      throw new IllegalArgumentException("Requring at least one input path");
-    }
-    
-    Configuration conf = jobContext.getConfiguration();
-    
-    if (paths.length == 1) {
-      setInputExpr(conf, new BasicTableExpr(paths[0]));
-    }
-    else {
-      TableUnionExpr expr = new TableUnionExpr();
-      for (Path path : paths) {
-        expr.add(new BasicTableExpr(path));
-      }
-      setInputExpr(conf, expr);
-    }
-    
-    setDeletedCGsInConf( conf, paths );
-  }
-  
-  /**
-   * Temporary fix for name node call in each mapper. It sets two flags in the conf so that mapper can skip
-   * the work that's done here at frontend.
-   * 
-   * It needs to check if the flag is already set (because setInputPaths() is also called on the backend
-   * thru pig code path (thru TableLoader.setLocation()).
-   * 
-   * @param conf
-   * @param paths
-   */
-  private static void setDeletedCGsInConf(Configuration conf, Path[] paths) {
-      if( !conf.get( INPUT_FE, "false" ).equals( "true" )  ) {
-          try {
-              StringBuilder sb = new StringBuilder();
-              boolean first = true;
-              for( Path p : paths ) {
-                  if (first)
-                      first = false;
-                  else
-                      sb.append( DELETED_CG_SEPARATOR_PER_UNION );
-                  sb.append( BasicTable.Reader.getDeletedCGs( p, conf ) );
-              }
-
-              conf.set(INPUT_FE, "true");
-              conf.set(INPUT_DELETED_CGS, sb.toString());
-          } catch(Exception ex) {
-              throw new RuntimeException( "Failed to find deleted column groups" + ex.toString() );
-          }
-      }
-  }
-
-  
-  /**
-   * Set the input expression in the Configuration object.
-   * 
-   * @param conf
-   *          Configuration object.
-   * @param expr
-   *          The input table expression.
-   */
-  static void setInputExpr(Configuration conf, TableExpr expr) {
-    StringBuilder out = new StringBuilder();
-    expr.encode(out);
-    conf.set(INPUT_EXPR, out.toString());
-  }
-
-  static TableExpr getInputExpr(JobContext jobContext) throws IOException {
-    Configuration conf = jobContext.getConfiguration();
-    String expr = conf.get(INPUT_EXPR);
-    if (expr == null) {
-      // try setting from input path
-      Path[] paths = FileInputFormat.getInputPaths( jobContext );
-      if (paths != null) {
-        setInputPaths(jobContext, paths);
-      }
-      expr = conf.get(INPUT_EXPR);
-    }
-      
-    if (expr == null) {
-      throw new IllegalArgumentException("Input expression not defined.");
-    }
-    StringReader in = new StringReader(expr);
-    return TableExpr.parse(in);
-  }
-  
-  /**
-   * Get the schema of a table expr
-   * 
-   * @param jobContext
-   *          JobContext object.
-   *                    
-   */
-  public static Schema getSchema(JobContext jobContext) throws IOException
-  {
-	  TableExpr expr = getInputExpr( jobContext );
-	  return expr.getSchema( jobContext.getConfiguration() );
-  }  
-  
-  /**
-   * Set the input projection in the JobContext object.
-   * 
-   * @param jobContext
-   *          JobContext object.
-   * @param projection
-   *          A common separated list of column names. If we want select all
-   *          columns, pass projection==null. The syntax of the projection
-   *          conforms to the {@link Schema} string.
-   * @deprecated Use {@link #setProjection(JobContext, ZebraProjection)} instead.
-   */
-  public static void setProjection(JobContext jobContext, String projection) throws ParseException {
-	  setProjection( jobContext.getConfiguration(), projection );
-  }
-  
-  /**
-   * Set the input projection in the JobContext object.
-   * 
-   * @param conf
-   *          Configuration object.
-   * @param projection
-   *          A common separated list of column names. If we want select all
-   *          columns, pass projection==null. The syntax of the projection
-   *          conforms to the {@link Schema} string.
-   */
-  private static void setProjection(Configuration conf, String projection) throws ParseException {
-      conf.set(INPUT_PROJ, Schema.normalize(projection));
-  }
-
-  /**
-   * Set the input projection in the JobContext object.
-   * 
-   * @param jobContext
-   *          JobContext object.
-   * @param projection
-   *          A common separated list of column names. If we want select all
-   *          columns, pass projection==null. The syntax of the projection
-   *          conforms to the {@link Schema} string.
-   *
-   */
-  public static void setProjection(JobContext jobContext, ZebraProjection projection) throws ParseException {
-    /* validity check on projection */
-    Schema schema = null;
-    String normalizedProjectionString = Schema.normalize(projection.toString());
-    try {
-      schema = getSchema( jobContext );
-      new org.apache.hadoop.zebra.types.Projection(schema, normalizedProjectionString);
-    } catch (ParseException e) {
-      throw new ParseException("[" + projection + "] " + "is not a valid Zebra projection string " + e.getMessage());
-    } catch (IOException e) {
-      throw new ParseException("[" + projection + "] " + "is not a valid Zebra projection string " + e.getMessage());
-    }
-    
-    Configuration conf = jobContext.getConfiguration();
-    conf.set(INPUT_PROJ, normalizedProjectionString);
-
-    // virtual source_table columns require sorted table
-    if (Projection.getVirtualColumnIndices(projection.toString()) != null && !getSorted( conf ))
-      throw new ParseException("The source_table virtual column is only availabe for sorted table unions.");
-  }  
-
-  /**
-   * Get the projection from the JobContext
-   * 
-   * @param jobContext
-   *          The JobContext object
-   * @return The projection schema. If projection has not been defined, or is
-   *         not known at this time, null will be returned. Note that by the time
-   *         when this method is called in Mapper code, the projection must
-   *         already be known.
-   * @throws IOException
-   *  
-   */
-  public static String getProjection(JobContext jobContext) throws IOException, ParseException {
-    Configuration conf = jobContext.getConfiguration();
-    String strProj = conf.get(INPUT_PROJ);
-    // TODO: need to be revisited
-    if (strProj != null) return strProj;
-    TableExpr expr = getInputExpr( jobContext );
-    if (expr != null) {
-      return expr.getSchema(conf).toProjectionString();
-    }
-    return null;
-  }
-      
-  private static boolean globalOrderingRequired(JobContext jobContext)
-  {
-    Configuration conf = jobContext.getConfiguration();
-    String result = conf.get(INPUT_SPLIT_MODE, UNSORTED);
-    return result.equalsIgnoreCase(GLOBALLY_SORTED);
-  }
-
-  private static void setSorted(JobContext jobContext) {
-    Configuration conf = jobContext.getConfiguration();
-    conf.setBoolean(INPUT_SORT, true);
-  }
-  
-  private static void setSorted(JobContext jobContext, SplitMode sm)
-  {
-    setSorted(jobContext);
-    Configuration conf = jobContext.getConfiguration();
-	  if (sm == SplitMode.GLOBALLY_SORTED)
-      conf.set(INPUT_SPLIT_MODE, GLOBALLY_SORTED);
-    else if (sm == SplitMode.LOCALLY_SORTED)
-      conf.set(INPUT_SPLIT_MODE, LOCALLY_SORTED);
-  }
-  
-  /**
-   * Get the SortInfo object regarding a Zebra table
-   *
-   * @param jobContext
-   *          JobContext object
-   * @return the zebra tables's SortInfo; null if the table is unsorted.
-   */
-  public static SortInfo getSortInfo(JobContext jobContext) throws IOException
-  {
-      Configuration conf = jobContext.getConfiguration();
-	  TableExpr expr = getInputExpr(jobContext);
-	  SortInfo result = null;
-	  int sortSize = 0;
-	  if (expr instanceof BasicTableExpr)
-    {
-      BasicTable.Reader reader = new BasicTable.Reader(((BasicTableExpr) expr).getPath(), conf);
-      SortInfo sortInfo = reader.getSortInfo();
-      reader.close();
-      result = sortInfo;
-	  } else {
-      List<LeafTableInfo> leaves = expr.getLeafTables(null);
-      for (Iterator<LeafTableInfo> it = leaves.iterator(); it.hasNext(); )
-      {
-        LeafTableInfo leaf = it.next();
-        BasicTable.Reader reader = new BasicTable.Reader(leaf.getPath(), conf);
-        SortInfo sortInfo = reader.getSortInfo();
-        reader.close();
-        if (sortSize == 0)
-        {
-          sortSize = sortInfo.size();
-          result = sortInfo;
-        } else if (sortSize != sortInfo.size()) {
-          throw new IOException("Tables of the table union do not possess the same sort property.");
-        }
-		  }
-	  }
-	  return result;
-  }
-
-  /**
-   * Requires sorted table or table union
-   * 
-   * @deprecated
-   * 
-   * @param jobContext
-   *          JobContext object.
-   * @param sortInfo
-   *          ZebraSortInfo object containing sorting information.
-   */
-  
-   public static void requireSortedTable(JobContext jobContext, ZebraSortInfo sortInfo) throws IOException {
-     setSplitMode(jobContext, SplitMode.GLOBALLY_SORTED, sortInfo);
-   }
-
-  /**
-   * 
-   * @param conf
-   *          JonConf object
-   * @param sm
-   *          Split mode: unsorted, globally sorted, locally sorted. Default is unsorted
-   * @param sortInfo
-   *          ZebraSortInfo object containing sorting information. Will be ignored if
-   *          the split mode is null or unsorted
-   * @throws IOException
-   */
-  public static void setSplitMode(JobContext jobContext, SplitMode sm, ZebraSortInfo sortInfo) throws IOException {
-   if (sm == null || sm == SplitMode.UNSORTED)
-    return;
-	 TableExpr expr = getInputExpr( jobContext );
-     Configuration conf = jobContext.getConfiguration();
-	 String comparatorName = null;
- 	 String[] sortcolumns = null;
-         if (sortInfo != null)
-         {
-           comparatorName = TFile.COMPARATOR_JCLASS+sortInfo.getComparator();
-           String sortColumnNames = sortInfo.getSortColumns();
-           if (sortColumnNames != null)
-             sortcolumns =  sortColumnNames.trim().split(SortInfo.SORTED_COLUMN_DELIMITER);
-           if (sortcolumns == null)
-             throw new IllegalArgumentException("No sort columns specified.");
-         }
-
-	 if (expr instanceof BasicTableExpr)
-	 {
-		 BasicTable.Reader reader = new BasicTable.Reader(((BasicTableExpr) expr).getPath(), conf);
-		 SortInfo mySortInfo = reader.getSortInfo();
-
-		 reader.close();
-		 if (mySortInfo == null)
-       throw new IOException("The table is not sorted");
-		 if (comparatorName == null)
-			 // cheat the equals method's comparator comparison
-			 comparatorName = mySortInfo.getComparator();
-		 if (sortcolumns != null && !mySortInfo.equals(sortcolumns, comparatorName))
-		 {
-			 throw new IOException("The table is not properly sorted");
-		 }
-	 } else {
-		 List<LeafTableInfo> leaves = expr.getLeafTables(null);
-		 for (Iterator<LeafTableInfo> it = leaves.iterator(); it.hasNext(); )
-		 {
-			 LeafTableInfo leaf = it.next();
-			 BasicTable.Reader reader = new BasicTable.Reader(leaf.getPath(), conf);
-			 SortInfo mySortInfo = reader.getSortInfo();
-			 reader.close();
-			 if (mySortInfo == null)
-			   throw new IOException("The table is not sorted");
-			 if (comparatorName == null)
-				 comparatorName = mySortInfo.getComparator(); // use the first table's comparator as comparison base
-			 if (sortcolumns == null)
-       {
-         sortcolumns = mySortInfo.getSortColumnNames();
-         comparatorName = mySortInfo.getComparator();
-       } else {
-         if (!mySortInfo.equals(sortcolumns, comparatorName))
-         {
-           throw new IOException("The table is not properly sorted");
-         }
-       }
-		 }
-	 }
-	 setSorted(jobContext, sm);
-  }
-  
-  /**
-   * Get requirement for sorted table
-   *
-   *@param jobContext JobContext object.
-   */
-   private static boolean getSorted(Configuration conf) {
-    return conf.getBoolean(INPUT_SORT, false);
-  }
-
-    /**
-     * @see InputFormat#createRecordReader(InputSplit, TaskAttemptContext)
-     */
-    @Override
-    public RecordReader<BytesWritable, Tuple> createRecordReader(InputSplit split,
-    		TaskAttemptContext taContext) throws IOException,InterruptedException {
-        return createRecordReaderFromJobContext( split, taContext );
-    }
-
-    private RecordReader<BytesWritable, Tuple> createRecordReaderFromJobContext(InputSplit split,
-                JobContext jobContext) throws IOException,InterruptedException {
-        Configuration conf = jobContext.getConfiguration();
-
-    	TableExpr expr = getInputExpr( conf );
-    	if (expr == null) {
-    		throw new IOException("Table expression not defined");
-    	}
-
-    	if ( getSorted( conf ) )
-    		expr.setSortedSplit();
-
-    	String strProj = conf.get(INPUT_PROJ);
-    	String projection = null;
-    	try {
-    		if (strProj == null) {
-    			projection = expr.getSchema(conf).toProjectionString();
-    			ZebraProjection proj = ZebraProjection.createZebraProjection( projection );
-    			TableInputFormat.setProjection( jobContext, proj );
-    		} else {
-    			projection = strProj;
-    		}
-    	} catch (ParseException e) {
-    		throw new IOException("Projection parsing failed : "+e.getMessage());
-    	}
-
-    	try {
-    		return new TableRecordReader(expr, projection, split, jobContext);
-    	} catch (ParseException e) {
-    		throw new IOException("Projection parsing faile : "+e.getMessage());
-    	}
-    }
-  
-    private static TableExpr getInputExpr(Configuration conf) throws IOException {
-    	String expr = conf.get(INPUT_EXPR);
-    	if (expr == null) {
-    		throw new IllegalArgumentException("Input expression not defined.");
-    	}
-    	StringReader in = new StringReader(expr);
-    	return TableExpr.parse(in);
-
-    }
-
-    /**
-     * Get a TableRecordReader on a single split
-     * 
-     * @param jobContext
-     *          JobContext object.
-     * @param projection
-     *          comma-separated column names in projection. null means all columns in projection
-     */
-    public static TableRecordReader createTableRecordReader(JobContext jobContext, String projection)
-    throws IOException, ParseException, InterruptedException {
-    	if (projection != null)
-    		setProjection( jobContext, ZebraProjection.createZebraProjection( projection ) );
-    	TableInputFormat inputFormat = new TableInputFormat();
-
-    	// a single split is needed
-    	List<InputSplit> splits = inputFormat.getSplits( jobContext, true );
-    	if( splits.size() != 1 )
-    		throw new IOException("Unable to generated one single split for the sorted table (internal error)" );
-    	return (TableRecordReader)inputFormat.createRecordReaderFromJobContext( splits.get( 0 ), jobContext );
-    }
-
-  private static List<InputSplit> getSortedSplits(Configuration conf, int numSplits,
-      TableExpr expr, List<BasicTable.Reader> readers,
-      List<BasicTableStatus> status) throws IOException {
-    if (expr.sortedSplitRequired() && !expr.sortedSplitCapable()) {
-      throw new IOException("Unable to created sorted splits");
-    }
-
-    long totalBytes = 0;
-    for (Iterator<BasicTableStatus> it = status.iterator(); it.hasNext();) {
-      BasicTableStatus s = it.next();
-      totalBytes += s.getSize();
-    }
-
-    long maxSplits = totalBytes / getMinSplitSize( conf );
-    List<InputSplit> splits = new ArrayList<InputSplit>();
-    if (maxSplits == 0)                                                                               
-        numSplits = 1;                 
-    else if (numSplits > maxSplits) {
-        numSplits = -1;                
-    }    
-
-    for (Iterator<BasicTable.Reader> it = readers.iterator(); it.hasNext();) {
-      BasicTable.Reader reader = it.next();
-      if (!reader.isSorted()) {
-        throw new IOException("Attempting sorted split on unsorted table");
-      }
-    }
-
-    if (numSplits == 1) {
-      BlockDistribution bd = null;
-      for (Iterator<BasicTable.Reader> it = readers.iterator(); it.hasNext();) {
-        BasicTable.Reader reader = it.next();
-        bd = BlockDistribution.sum(bd, reader.getBlockDistribution((RangeSplit) null));
-      }
-      
-      SortedTableSplit split = new SortedTableSplit(0, null, null, bd, conf);
-      splits.add(split);
-      return splits;
-    }
-    
-    // TODO: Does it make sense to interleave keys for all leaf tables if
-    // numSplits <= 0 ?
-    int nLeaves = readers.size();
-    BlockDistribution lastBd = new BlockDistribution();
-    ArrayList<KeyDistribution> btKeyDistributions = new ArrayList<KeyDistribution>();
-    for (int i = 0; i < nLeaves; ++i) {
-      KeyDistribution btKeyDistri =
-          readers.get(i).getKeyDistribution(
-              (numSplits <= 0) ? -1 :
-              Math.max(numSplits * 5 / nLeaves, numSplits), nLeaves, lastBd);
-      btKeyDistributions.add(btKeyDistri);
-    }
-    
-    int btSize = btKeyDistributions.size();
-    KeyDistribution[] btKds = new KeyDistribution[btSize];
-    Object[] btArray = btKeyDistributions.toArray();
-    for (int i = 0; i < btSize; i++)
-      btKds[i] = (KeyDistribution) btArray[i];
-
-    KeyDistribution keyDistri = KeyDistribution.merge(btKds);
-
-    if (keyDistri == null) {
-      // should never happen.
-       return splits;
-    }
-
-    keyDistri.resize(lastBd);
-
-    RawComparable[] keys = keyDistri.getKeys();
-    for (int i = 0; i <= keys.length; ++i) {
-      RawComparable begin = (i == 0) ? null : keys[i - 1];
-      RawComparable end = (i == keys.length) ? null : keys[i];
-      BlockDistribution bd;
-      if (i < keys.length)
-        bd = keyDistri.getBlockDistribution(keys[i]);
-      else
-        bd = lastBd;
-      BytesWritable beginB = null, endB = null;
-      if (begin != null)
-        beginB = new BytesWritable(begin.buffer());
-      if (end != null)
-        endB = new BytesWritable(end.buffer());
-      SortedTableSplit split = new SortedTableSplit(i, beginB, endB, bd, conf);
-      splits.add(split);
-    }
-    LOG.info("getSplits : returning " + splits.size() + " sorted splits.");
-    return splits;
-  }
-  
-  static long getMinSplitSize(Configuration conf) {
-    return conf.getLong("table.input.split.minSize", 1 * 1024 * 1024L);
-  }
-
-  /**
-   * Set the minimum split size.
-   * 
-   * @param jobContext
-   *          The job conf object.
-   * @param minSize
-   *          Minimum size.
-   */
-  public static void setMinSplitSize(JobContext jobContext, long minSize) {
-    jobContext.getConfiguration().setLong("table.input.split.minSize", minSize);
-  }
-  
-  private static class DummyFileInputFormat extends FileInputFormat<BytesWritable, Tuple> {
-    /**
-      * the next constant and class are copies from FileInputFormat
-      */
-    private static final PathFilter hiddenFileFilter = new PathFilter(){
-        public boolean accept(Path p){
-          String name = p.getName(); 
-          return !name.startsWith("_") && !name.startsWith("."); 
-        }
-      }; 
- 
-    /**
-     * Proxy PathFilter that accepts a path only if all filters given in the
-     * constructor do. Used by the listPaths() to apply the built-in
-     * hiddenFileFilter together with a user provided one (if any).
-     */
-    private static class MultiPathFilter implements PathFilter {
-      private List<PathFilter> filters;
- 
-      public MultiPathFilter(List<PathFilter> filters) {
-        this.filters = filters;
-      }
- 
-      public boolean accept(Path path) {
-        for (PathFilter filter : filters) {
-          if (!filter.accept(path)) {
-            return false;
-          }
-        }
-        return true;
-      }
-    }
-    private Integer[] fileNumbers = null;
- 
-    private List<BasicTable.Reader> readers;
- 
-    public Integer[] getFileNumbers() {
-      return fileNumbers;
-    }
- 
-    public DummyFileInputFormat(Job job, long minSplitSize, List<BasicTable.Reader> readers) {
-     super.setMinInputSplitSize(job, minSplitSize);
-     this.readers = readers;
-    }
-    
-
-    @Override
-    public RecordReader<BytesWritable, Tuple> createRecordReader(
-    InputSplit arg0, TaskAttemptContext arg1) throws IOException,
-        InterruptedException {
-       // no-op
-      return null;
-    }
-
-    @Override
-    public long computeSplitSize(long blockSize, long minSize, long maxSize) {
-      return super.computeSplitSize(blockSize, minSize, maxSize);
-    }
-
-    /**
-     * copy from FileInputFormat: add assignment to table file numbers
-     */
-    @Override
-    public List<FileStatus> listStatus(JobContext jobContext) throws IOException {
-      Configuration job = jobContext.getConfiguration();
-      Path[] dirs = getInputPaths(jobContext);
-      if (dirs.length == 0) {
-        throw new IOException("No input paths specified in job");
-      }
-
-      List<FileStatus> result = new ArrayList<FileStatus>();
-      List<IOException> errors = new ArrayList<IOException>();
-      
-      // creates a MultiPathFilter with the hiddenFileFilter and the
-      // user provided one (if any).
-      List<PathFilter> filters = new ArrayList<PathFilter>();
-      filters.add(hiddenFileFilter);
-      PathFilter jobFilter = getInputPathFilter(jobContext);
-      if (jobFilter != null) {
-        filters.add(jobFilter);
-      }
-      PathFilter inputFilter = new MultiPathFilter(filters);
-
-      ArrayList<Integer> fileNumberList  = new ArrayList<Integer>();
-      int index = 0;
-      for (Path p: dirs) {
-        FileSystem fs = p.getFileSystem(job); 
-        FileStatus[] matches = fs.globStatus(p, inputFilter);
-        if (matches == null) {
-          errors.add(new IOException("Input path does not exist: " + p));
-        } else if (matches.length == 0) {
-          errors.add(new IOException("Input Pattern " + p + " matches 0 files"));
-        } else {
-          for (FileStatus globStat: matches) {
-            if (globStat.isDir()) {
-              FileStatus[] fileStatuses = fs.listStatus(globStat.getPath(), inputFilter);
-              // reorder according to CG index
-              BasicTable.Reader reader = readers.get(index);
-              if (fileStatuses.length > 1)
-                reader.rearrangeFileIndices(fileStatuses);
-              for(FileStatus stat: fileStatuses) {
-                if (stat != null)
-                  result.add(stat);
-              }
-              fileNumberList.add(fileStatuses.length);
-            } else {
-              result.add(globStat);
-              fileNumberList.add(1);
-            }
-          }
-        }
-        index++;
-      }
-      fileNumbers = new Integer[fileNumberList.size()];
-      fileNumberList.toArray(fileNumbers);
-
-      if (!errors.isEmpty()) {
-        throw new InvalidInputException(errors);
-      }
-      LOG.info("Total input paths to process : " + result.size()); 
-      return result;
-    }
-  }
-  
-  private static List<InputSplit> getRowSplits(Configuration conf,
-      TableExpr expr, List<BasicTable.Reader> readers,
-      List<BasicTableStatus> status) throws IOException {
-    ArrayList<InputSplit> ret = new ArrayList<InputSplit>();
-    Job job = new Job(conf);
-    long minSplitSize = getMinSplitSize(conf);
-  
-    long minSize = Math.max(conf.getLong("mapreduce.min.split.size", 1), minSplitSize);
-    long totalBytes = 0;
-    for (Iterator<BasicTableStatus> it = status.iterator(); it.hasNext(); )
-    {
-      totalBytes += it.next().getSize();
-    }
-    StringBuilder sb = new StringBuilder();
-    boolean first = true;
-    PathFilter filter = null;
-    List<BasicTable.Reader> realReaders = new ArrayList<BasicTable.Reader>();
-    int[] realReaderIndices = new int[readers.size()];
-
-    for (int i = 0; i < readers.size(); ++i) {
-      BasicTable.Reader reader = readers.get(i);
-      /* Get the index of the column group that will be used for row-split.*/
-      int splitCGIndex = reader.getRowSplitCGIndex();
-      
-      /* We can create input splits only if there does exist a valid column group for split.
-       * Otherwise, we do not create input splits. */
-      if (splitCGIndex >= 0) {
-        realReaderIndices[realReaders.size()] = i;
-        realReaders.add(reader);
-         if (first)
-         {
-           // filter is identical across tables
-           filter = reader.getPathFilter(conf);
-           first = false;
-         } else
-           sb.append(",");
-         sb.append(reader.getPath().toString() + "/" + reader.getName(splitCGIndex));
-       }
-     }
-     
-     DummyFileInputFormat helper = new DummyFileInputFormat(job,minSplitSize, realReaders);
- 
-     if (!realReaders.isEmpty())
-     {
-       DummyFileInputFormat.setInputPaths(job, sb.toString());
-       DummyFileInputFormat.setInputPathFilter(job, filter.getClass());
-       List<InputSplit> inputSplitList = helper.getSplits(job);
-       InputSplit[] inputSplits = inputSplitList.toArray(new InputSplit[0]);
- 
-       /*
-        * Potential file batching optimizations include:
-        * 1) sort single file inputSplits in the descending order of their sizes so
-        *    that the ops of new file opens are spread to a maximum degree;
-        * 2) batching the files with maximum block distribution affinities into the same input split
-        */
- 
-       int[] inputSplitBoundaries = new int[realReaders.size()];
-       long start, prevStart = Long.MIN_VALUE;
-       int tableIndex = 0, fileNumber = 0;
-       Integer[] fileNumbers = helper.getFileNumbers();
-       if (fileNumbers.length != realReaders.size())
-         throw new IOException("Number of tables in input paths of input splits is incorrect.");
-       for (int j=0; j<inputSplits.length; j++) {
-         FileSplit fileSplit = (FileSplit) inputSplits[j];
-         start = fileSplit.getStart();
-         if (start <= prevStart)
-         {
-           fileNumber++;
-           if (fileNumber >= fileNumbers[tableIndex])
-           {
-             inputSplitBoundaries[tableIndex++] = j;
-             fileNumber = 0;
-           }
-         }
-         prevStart = start;
-       }
-       inputSplitBoundaries[tableIndex++] =  inputSplits.length;
-       if (tableIndex != realReaders.size())
-         throw new IOException("Number of tables in input splits is incorrect.");
-       for (tableIndex = 0; tableIndex < realReaders.size(); tableIndex++)
-       {
-         int startSplitIndex = (tableIndex == 0 ? 0 : inputSplitBoundaries[tableIndex - 1]);
-         int splitLen = (tableIndex == 0 ? inputSplitBoundaries[0] :
-             inputSplitBoundaries[tableIndex] - inputSplitBoundaries[tableIndex-1]);
-         BasicTable.Reader reader = realReaders.get(tableIndex);
-         /* Get the index of the column group that will be used for row-split.*/
-         int splitCGIndex = reader.getRowSplitCGIndex();
-         
-         long starts[] = new long[splitLen];
-         long lengths[] = new long[splitLen];
-         int batches[] = new int[splitLen + 1];
-         batches[0] = 0;
-         int numBatches = 0;
-         Path paths[] = new Path [splitLen];
-         long totalLen = 0;
-         final double SPLIT_SLOP = 1.1;
-         int endSplitIndex = startSplitIndex + splitLen;
-         for (int j=startSplitIndex; j< endSplitIndex; j++) {
-          FileSplit fileSplit = (FileSplit) inputSplits[j];
-          Path p = fileSplit.getPath();
-          long blockSize = p.getFileSystem(conf).getBlockSize(p);
-          long splitSize = (long) (helper.computeSplitSize(blockSize, minSize, totalBytes) * SPLIT_SLOP);
-          start = fileSplit.getStart();
-          long length = fileSplit.getLength();
-          int index = j - startSplitIndex;
-          starts[index] = start;
-          lengths[index] = length;
-          totalLen += length;
-          paths[index] = p;
-          if (totalLen >= splitSize)
-          {
-
-             for (int ii = batches[numBatches] + 1; ii < index - 1; ii++)
-               starts[ii] = -1; // all intermediate files are not split
-             batches[++numBatches] = index;
-             totalLen = length;
-          }
-        }
-        for (int ii = batches[numBatches] + 1; ii < splitLen - 1; ii++)
-          starts[ii] = -1; // all intermediate files are not split
-        if (splitLen > 0)
-          batches[++numBatches] = splitLen;
-        
-        List<RowSplit> subSplits = reader.rowSplit(starts, lengths, paths, splitCGIndex,
-            batches, numBatches);
-        
-        int realTableIndex = realReaderIndices[tableIndex];
-        for (Iterator<RowSplit> it = subSplits.iterator(); it.hasNext();) {
-          RowSplit subSplit = it.next();
-          RowTableSplit split = new RowTableSplit(reader, subSplit, realTableIndex, conf);
-          ret.add(split);
-        }
-      }
-    }
-
-    LOG.info("getSplits : returning " + ret.size() + " row splits.");
-    return ret;
-  }
-
-    /**
-     * @see InputFormat#getSplits(JobContext)
-     */
-    @Override
-    public List<InputSplit> getSplits(JobContext jobContext) throws IOException {
-	    return getSplits( jobContext, false );
-    }
-
-    List<InputSplit> getSplits(JobContext jobContext, boolean singleSplit) throws IOException {
-    	Configuration conf = jobContext.getConfiguration();
-    	TableExpr expr = getInputExpr( conf );
-    	if( getSorted(conf) ) {
-    		expr.setSortedSplit();
-    	} else if( singleSplit ) {
-    		throw new IOException( "Table must be sorted in order to get a single sorted split" );
-    	}
-
-    	if( expr.sortedSplitRequired() && !expr.sortedSplitCapable() ) {
-    		throw new IOException( "Unable to created sorted splits" );
-    	}
-
-    	String projection;
-    	try {
-    		projection = getProjection(jobContext);
-    	} catch (ParseException e) {
-    		throw new IOException("getProjection failed : "+e.getMessage());
-    	}
-    	List<LeafTableInfo> leaves = expr.getLeafTables(projection);
-    	int nLeaves = leaves.size();
-    	ArrayList<BasicTable.Reader> readers =
-    		new ArrayList<BasicTable.Reader>(nLeaves);
-    	ArrayList<BasicTableStatus> status =
-    		new ArrayList<BasicTableStatus>(nLeaves);
-
-    	try {
-        boolean sorted = expr.sortedSplitRequired();
-    		for (Iterator<LeafTableInfo> it = leaves.iterator(); it.hasNext();) {
-    			LeafTableInfo leaf = it.next();
-    			BasicTable.Reader reader =
-    				new BasicTable.Reader(leaf.getPath(), conf );
-    			reader.setProjection(leaf.getProjection());
-    			BasicTableStatus s = reader.getStatus();
-    		    status.add(s);
-    			readers.add(reader);
-    		}
-
-    		if( readers.isEmpty() ) {
-    			// I think we should throw exception here.
-    			return new ArrayList<InputSplit>();
-    		}
-
-        List<InputSplit> result;
-        if (sorted)
-        {
-          if (singleSplit)
-            result = getSortedSplits( conf, 1, expr, readers, status);
-          else if (globalOrderingRequired(jobContext))
-            result = getSortedSplits(conf, -1, expr, readers, status);
-          else
-            result = getRowSplits( conf, expr, readers, status);
-        } else
-          result = getRowSplits( conf, expr, readers, status);
-
-        return result;
-
-    	} catch (ParseException e) {
-    		throw new IOException("Projection parsing failed : "+e.getMessage());
-    	} finally {
-    		for (Iterator<BasicTable.Reader> it = readers.iterator(); it.hasNext();) {
-    			try {
-    				it.next().close();
-    			}
-    			catch (Exception e) {
-    				e.printStackTrace();
-    				// TODO: log the error here.
-    			}
-    		}
-    	}
-    }
-
-  @Deprecated
-  public synchronized void validateInput(JobContext jobContext) throws IOException {
-    // Validating imports by opening all Tables.
-    TableExpr expr = getInputExpr(jobContext);
-    try {
-      String projection = getProjection(jobContext);
-      List<LeafTableInfo> leaves = expr.getLeafTables(projection);
-      Iterator<LeafTableInfo> iterator = leaves.iterator();
-      while (iterator.hasNext()) {
-        LeafTableInfo leaf = iterator.next();
-        BasicTable.Reader reader =
-            new BasicTable.Reader(leaf.getPath(), jobContext.getConfiguration());
-        reader.setProjection(projection);
-        reader.close();
-      }
-    } catch (ParseException e) {
-    	throw new IOException("Projection parsing failed : "+e.getMessage());
-    }
-  }
-
-  	/**
-  	 * Get a comparable object from the given InputSplit object.
-  	 * 
-  	 * @param inputSplit An InputSplit instance. It should be type of SortedTableSplit.
-  	 * @return a comparable object of type WritableComparable
-  	 */
-	public static WritableComparable<?> getSortedTableSplitComparable(InputSplit inputSplit) {
-        SortedTableSplit split = null;
-        if( inputSplit instanceof SortedTableSplit ) {
-            split = (SortedTableSplit)inputSplit;
-        } else {
-            throw new RuntimeException( "LoadFunc expected split of type [" + 
-            		SortedTableSplit.class.getCanonicalName() + "]" );
-        }
-		return new SortedTableSplitComparable( split.getIndex() );
-	}
-
-}
-
-/**
- * Adaptor class for sorted InputSplit for table.
- */
-class SortedTableSplit extends InputSplit implements Writable {
-	// the order of the split in all splits generated.
-	private int index;
-	
-	BytesWritable begin = null, end = null;
-  
-  String[] hosts;
-  long length = 1;
-
-  public SortedTableSplit()
-  {
-    // no-op for Writable construction
-  }
-  
-  public SortedTableSplit(int index, BytesWritable begin, BytesWritable end,
-      BlockDistribution bd, Configuration conf) {
-	  this.index = index;
-	  
-    if (begin != null) {
-      this.begin = new BytesWritable();
-      this.begin.set(begin.getBytes(), 0, begin.getLength());
-    }
-    if (end != null) {
-      this.end = new BytesWritable();
-      this.end.set(end.getBytes(), 0, end.getLength());
-    }
-    
-    if (bd != null) {
-      length = bd.getLength();
-      hosts =
-        bd.getHosts( conf.getInt("mapred.lib.table.input.nlocation", 5) );
-    }
-  }
-  
-	public int getIndex() {
-		return index;
-	}
-
-  @Override
-  public long getLength() throws IOException {
-    return length;
-  }
-
-  @Override
-  public String[] getLocations() throws IOException {
-    if (hosts == null)
-    {
-      String[] tmp = new String[1];
-      tmp[0] = "";
-      return tmp;
-    }
-    return hosts;
-  }
-
-  @Override
-  public void readFields(DataInput in) throws IOException {
-    begin = end = null;
-    int bool = WritableUtils.readVInt(in);
-    if (bool == 1) {
-      begin = new BytesWritable();
-      begin.readFields(in);
-    }
-    bool = WritableUtils.readVInt(in);
-    if (bool == 1) {
-      end = new BytesWritable();
-      end.readFields(in);
-    }
-    length = WritableUtils.readVLong(in);
-    int size = WritableUtils.readVInt(in);
-    if (size > 0)
-      hosts = new String[size];
-    for (int i = 0; i < size; i++)
-    	hosts[i] = WritableUtils.readString(in);
-  }
-
-  @Override
-  public void write(DataOutput out) throws IOException {
-    if (begin == null) {
-      WritableUtils.writeVInt(out, 0);
-    }
-    else {
-      WritableUtils.writeVInt(out, 1);
-      begin.write(out);
-    }
-    if (end == null) {
-      WritableUtils.writeVInt(out, 0);
-    }
-    else {
-      WritableUtils.writeVInt(out, 1);
-      end.write(out);
-    }
-    WritableUtils.writeVLong(out, length);
-    WritableUtils.writeVInt(out, hosts == null ? 0 : hosts.length);
-    for (int i = 0; i < hosts.length; i++)
-    {
-    	WritableUtils.writeString(out, hosts[i]);
-    }
-  }
-  
-  public BytesWritable getBegin() {
-    return begin;
-  }
-
-  public BytesWritable getEnd() {
-    return end;
-  }
-}
-
-/**
- * Adaptor class for unsorted InputSplit for table.
- */
-class RowTableSplit extends InputSplit implements Writable{
-  /**
-     * 
-     */
-  String path = null;
-  int tableIndex = 0;
-  RowSplit split = null;
-  String[] hosts = null;
-  long length = 1;
-
-  public RowTableSplit(Reader reader, RowSplit split, int tableIndex, Configuration conf)
-      throws IOException {
-    this.path = reader.getPath();
-    this.split = split;
-    this.tableIndex = tableIndex;
-    BlockDistribution dataDist = reader.getBlockDistribution(split);
-    if (dataDist != null) {
-      length = dataDist.getLength();
-      hosts =
-          dataDist.getHosts(conf.getInt("mapred.lib.table.input.nlocation", 5));
-    }
-  }
-  
-  public RowTableSplit() {
-    // no-op for Writable construction
-  }
-  
-  @Override
-  public long getLength() throws IOException {
-    return length;
-  }
-
-  @Override
-  public String[] getLocations() throws IOException {
-    return hosts;
-  }
-
-  @Override
-  public void readFields(DataInput in) throws IOException {
-    tableIndex = WritableUtils.readVInt(in); 
-    path = WritableUtils.readString(in);
-    int bool = WritableUtils.readVInt(in);
-    if (bool == 1) {
-      if (split == null) split = new RowSplit();
-      split.readFields(in);
-    }
-    else {
-      split = null;
-    }
-    hosts = WritableUtils.readStringArray(in);
-    length = WritableUtils.readVLong(in);
-  }
-
-  @Override
-  public void write(DataOutput out) throws IOException {
-    WritableUtils.writeVInt(out, tableIndex);
-    WritableUtils.writeString(out, path);
-    if (split == null) {
-      WritableUtils.writeVInt(out, 0);
-    }
-    else {
-      WritableUtils.writeVInt(out, 1);
-      split.write(out);
-    }
-    WritableUtils.writeStringArray(out, hosts);
-    WritableUtils.writeVLong(out, length);
-  }
-
-  public String getPath() {
-    return path;
-  }
-  
-  public RowSplit getSplit() {
-    return split;
-  }
-  
-  public int getTableIndex() {
-    return tableIndex;
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/TableRecordReader.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/TableRecordReader.java
deleted file mode 100644
index 81450177d..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/TableRecordReader.java
+++ /dev/null
@@ -1,135 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.mapreduce.InputSplit;
-import org.apache.hadoop.mapreduce.JobContext;
-import org.apache.hadoop.mapreduce.RecordReader;
-import org.apache.hadoop.mapreduce.TaskAttemptContext;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Projection;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.Tuple;
-
-/**
- * Adaptor class to implement RecordReader on top of Scanner.
- */
-public class TableRecordReader extends RecordReader<BytesWritable, Tuple> {
-  private final TableScanner scanner;
-  private long count = 0;
-  private BytesWritable key = null;
-  private Tuple value = null;
-
-  /**
-   * 
-   * @param expr
-   *          Table expression
-   * @param projection
-   *          projection schema. Should never be null.
-   * @param split
-   *          the split to work on
-   * @param jobContext
-   *          JobContext object
-   * @throws IOException
-   */
-  public TableRecordReader(TableExpr expr, String projection,
-		  InputSplit split, JobContext jobContext) throws IOException, ParseException {
-	  Configuration conf = jobContext.getConfiguration();
-	  if( split instanceof RowTableSplit ) {
-		  RowTableSplit rowSplit = (RowTableSplit)split;
-		  scanner = expr.getScanner(rowSplit, projection, conf);
-	  } else {
-		  SortedTableSplit tblSplit = (SortedTableSplit)split;
-		  scanner = expr.getScanner( tblSplit.getBegin(), tblSplit.getEnd(), projection, conf );
-	  }
-  }
-  
-  @Override
-  public void close() throws IOException {
-    scanner.close();
-  }
-
-  public long getPos() throws IOException {
-    return count;
-  }
-
-  @Override
-  public float getProgress() throws IOException {
-    return  (float)((scanner.atEnd()) ? 1.0 : 0);
-  }
-  /**
-   * Seek to the position at the first row which has the key
-   * or just after the key; only applicable for sorted Zebra table
-   *
-   * @param key
-   *          the key to seek on
-   */
-  public boolean seekTo(BytesWritable key) throws IOException {
-    return scanner.seekTo(key);
-  }
-  
-  /**
-   * Check if the end of the input has been reached
-   *
-   * @return true if the end of the input is reached
-   */
-  public boolean atEnd() throws IOException {
-	  return scanner.atEnd();
-  }
-
-	@Override
-	public BytesWritable getCurrentKey() throws IOException, InterruptedException {
-		return key;
-	}
-	
-	@Override
-	public Tuple getCurrentValue() throws IOException, InterruptedException {
-		return value;
-	}
-	
-	@Override
-	public void initialize(org.apache.hadoop.mapreduce.InputSplit arg0,
-			TaskAttemptContext arg1) throws IOException, InterruptedException {
-		// no-op
-	}
-	
-	@Override
-	public boolean nextKeyValue() throws IOException, InterruptedException {
-	    if( scanner.atEnd() ) {
-	    	key = null;
-	    	value = null;
-	        return false;
-	    }
-	    
-	    if( key == null ) {
-	        key = new BytesWritable();
-	    }
-	    if (value == null) {
-	        value = TypesUtils.createTuple(Projection.getNumColumns(scanner.getProjection()));
-	    }
-	    scanner.getKey(key);
-	    scanner.getValue(value);
-	    scanner.advance();
-	    count++;
-	    return true;
-	}
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/TableUnionExpr.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/TableUnionExpr.java
deleted file mode 100644
index 8a3ddfc0c..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/TableUnionExpr.java
+++ /dev/null
@@ -1,306 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.EOFException;
-import java.io.IOException;
-import java.io.StringReader;
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Comparator;
-import java.util.LinkedHashSet;
-import java.util.List;
-import java.util.concurrent.PriorityBlockingQueue;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Projection;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.pig.data.Tuple;
-
-/**
- * Table expression supporting a union of BasicTables.
- * 
- * @see <a href="doc-files/examples/ReadTableUnion.java">Usage example for
- *      UnionTableExpr</a>
- */
-class TableUnionExpr extends CompositeTableExpr {
-  /**
-   * Add another BasicTable into the table-union.
-   * 
-   * @param expr
-   *          The expression for the BasicTable to be added.
-   * @return self.
-   */
-  public TableUnionExpr add(BasicTableExpr expr) {
-    super.addCompositeTable(expr);
-    return this;
-  }
-
-  /**
-   * Add an array of BasicTables into the table-union.
-   * 
-   * @param exprs
-   *          the expressions representing the BasicTables to be added.
-   * @return self.
-   */
-  public TableUnionExpr add(BasicTableExpr[] exprs) {
-    super.addCompositeTables(exprs);
-    return this;
-  }
-
-  /**
-   * Add a Collection of BasicTables into the table-union.
-   * 
-   * @param exprs
-   *          the expressions representing the BasicTables to be added.
-   * @return self.
-   */
-  public TableUnionExpr add(Collection<? extends BasicTableExpr> exprs) {
-    super.addCompositeTables(exprs);
-    return this;
-  }
-  
-  @Override
-  protected TableUnionExpr decodeParam(StringReader in) throws IOException {
-    super.decodeParam(in);
-    int n = composite.size();
-    for (int i = 0; i < n; ++i) {
-      if (!(composite.get(i) instanceof BasicTableExpr)) {
-        throw new RuntimeException("Not a BasicTableExpr");
-      }
-    }
-    return this;
-  }
-
-  @Override
-  protected TableUnionExpr encodeParam(StringBuilder out) {
-    super.encodeParam(out);
-    return this;
-  }
-
-  @Override
-  public TableScanner getScanner(BytesWritable begin, BytesWritable end,
-      String projection, Configuration conf) throws IOException {
-    int n = composite.size();
-    if (n==0) {
-      throw new IllegalArgumentException("Union of 0 table");
-    }
-    ArrayList<BasicTable.Reader> readers = new ArrayList<BasicTable.Reader>(n);
-    String[] deletedCGsInUnion = getDeletedCGsPerUnion(conf);
-
-    if (deletedCGsInUnion != null && deletedCGsInUnion.length != n)
-      throw new IllegalArgumentException("Invalid string of deleted column group names: expected = "+
-          n + " actual =" + deletedCGsInUnion.length);
-
-    for (int i = 0; i < n; ++i) {
-      String deletedCGs = (deletedCGsInUnion == null ? null : deletedCGsInUnion[i]);
-      String[] deletedCGList = (deletedCGs == null ? null :
-        deletedCGs.split(BasicTable.DELETED_CG_SEPARATOR_PER_TABLE));
-      BasicTableExpr expr = (BasicTableExpr) composite.get(i);
-      BasicTable.Reader reader =
-          new BasicTable.Reader(expr.getPath(), deletedCGList, conf);
-      readers.add(reader);
-    }
-
-    String actualProjection = projection;
-    if (actualProjection == null) {
-      // Perform a union on all column names.
-      LinkedHashSet<String> colNameSet = new LinkedHashSet<String>();
-      for (int i = 0; i < n; ++i) {
-        String[] cols = readers.get(i).getSchema().getColumns();
-        for (String col : cols) {
-          colNameSet.add(col);
-        }
-      }
-
-      actualProjection = 
-          Projection.getProjectionStr(colNameSet.toArray(new String[colNameSet.size()]));
-    }
-    
-    ArrayList<TableScanner> scanners = new ArrayList<TableScanner>(n);
-    try {
-      for (int i=0; i<n; ++i) {
-        BasicTable.Reader reader = readers.get(i);
-        reader.setProjection(actualProjection);
-        TableScanner scanner = readers.get(i).getScanner(begin, end, true);
-        scanners.add(scanner);
-      }
-    } catch (ParseException e) {
-    	throw new IOException("Projection parsing failed : "+e.getMessage());
-    }
-    
-    if (scanners.isEmpty()) {
-      return new NullScanner(actualProjection);
-    }
-
-    Integer[] virtualColumnIndices = Projection.getVirtualColumnIndices(projection);
-    if (virtualColumnIndices != null && n == 1)
-      throw new IllegalArgumentException("virtual column requires union of multiple tables");
-    return new SortedTableUnionScanner(scanners, Projection.getVirtualColumnIndices(projection));
-  }
-  
-  @Override
-  public TableScanner getScanner(RowTableSplit split, String projection,
-      Configuration conf) throws IOException, ParseException {
-    BasicTableExpr expr = (BasicTableExpr) composite.get(split.getTableIndex());
-    return expr.getScanner(split, projection, conf);
-  }
-}
-
-/**
- * Union scanner.
- */
-class SortedTableUnionScanner implements TableScanner {
-  CachedTableScanner[] scanners;
-  PriorityBlockingQueue<CachedTableScanner> queue;
-  boolean synced = false;
-  boolean hasVirtualColumns = false;
-  Integer[] virtualColumnIndices = null;
-  CachedTableScanner scanner = null; // the working scanner
-
-  SortedTableUnionScanner(List<TableScanner> scanners, Integer[] vcolindices) throws IOException {
-    if (scanners.isEmpty()) {
-      throw new IllegalArgumentException("Zero-sized table union");
-    }
-    
-    this.scanners = new CachedTableScanner[scanners.size()];
-    queue =
-        new PriorityBlockingQueue<CachedTableScanner>(scanners.size(),
-            new Comparator<CachedTableScanner>() {
-
-              @Override
-              public int compare(CachedTableScanner o1, CachedTableScanner o2) {
-                try {
-                  return o1.getKey().compareTo(o2.getKey());
-                }
-                catch (IOException e) {
-                  throw new RuntimeException("IOException: " + e.toString());
-                }
-              }
-
-            });
-    
-    for (int i = 0; i < this.scanners.length; ++i) {
-      TableScanner scanner = scanners.get(i);
-      this.scanners[i] = new CachedTableScanner(scanner, i);
-    }
-    // initial fill-ins
-    if (!atEnd())
-    	scanner = queue.poll();
-    virtualColumnIndices = vcolindices;
-    hasVirtualColumns = (vcolindices != null && vcolindices.length != 0);
-  }
-  
-
-  private void sync() throws IOException {
-    if (synced == false) {
-      queue.clear();
-      for (int i = 0; i < scanners.length; ++i) {
-        if (!scanners[i].atEnd()) {
-          queue.add(scanners[i]);
-        }
-      }
-      synced = true;
-    }
-  }
-  
-  @Override
-  public boolean advance() throws IOException {
-    sync();
-    scanner.advance();
-    if (!scanner.atEnd()) {
-      queue.add(scanner);
-    }
-    scanner = queue.poll();
-    return (scanner != null);
-  }
-
-  @Override
-  public boolean atEnd() throws IOException {
-    sync();
-    return (scanner == null && queue.isEmpty());
-  }
-
-  @Override
-  public String getProjection() {
-    return scanners[0].getProjection();
-  }
-  
-  @Override
-  public Schema getSchema() {
-    return scanners[0].getSchema();
-  }
-
-  @Override
-  public void getKey(BytesWritable key) throws IOException {
-    if (atEnd()) {
-      throw new EOFException("No more rows to read");
-    }
-    
-    key.set(scanner.getKey());
-  }
-
-  @Override
-  public void getValue(Tuple row) throws IOException {
-    if (atEnd()) {
-      throw new EOFException("No more rows to read");
-    }
-  
-    Tuple tmp = scanner.getValue();
-    if (hasVirtualColumns)
-    {
-       for (int i = 0; i < virtualColumnIndices.length; i++)
-       {
-         tmp.set(virtualColumnIndices[i], scanner.getIndex());
-       }
-    }
-    row.reference(tmp);
-  }
-
-  @Override
-  public boolean seekTo(BytesWritable key) throws IOException {
-    boolean rv = false;
-    for (CachedTableScanner scanner : scanners) {
-      rv = rv || scanner.seekTo(key);
-    }
-    synced = false;
-    if (!atEnd())
-      scanner = queue.poll();
-    return rv;
-  }
-
-  @Override
-  public void seekToEnd() throws IOException {
-    for (CachedTableScanner scanner : scanners) {
-      scanner.seekToEnd();
-    }
-    scanner = null;
-    synced = false;
-  }
-
-  @Override
-  public void close() throws IOException {
-    for (CachedTableScanner scanner : scanners) {
-      scanner.close();
-    }
-    queue.clear();
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/ZebraOutputPartition.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/ZebraOutputPartition.java
deleted file mode 100644
index 04af2e289..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/ZebraOutputPartition.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.IOException;
-import org.apache.hadoop.conf.*;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.pig.data.Tuple;
-
-
-public abstract class ZebraOutputPartition  implements Configurable{
-
-  protected  Configuration conf;	
-	
-  @Override
-  public void setConf( Configuration conf) {
-	  this.conf = conf;
-  }
-
-  @Override
-  public Configuration getConf( ) {
-	  return conf;
-  }
-  
-  public abstract int getOutputPartition(BytesWritable key, Tuple Value)
-  throws IOException;
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/ZebraProjection.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/ZebraProjection.java
deleted file mode 100644
index 2a7fb21de..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/ZebraProjection.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.mapreduce;
-
-/**
- * A wrapper class for Projection.
- */
-public class ZebraProjection {
-  private String projectionStr = null;
-
-  private ZebraProjection(String projStr) {
-    projectionStr = projStr;
-  }
-  
-  public static ZebraProjection createZebraProjection(String projStr) {
-    return new ZebraProjection(projStr);
-  }
-  
-  public String toString() {
-    return projectionStr;
-  }
-}
\ No newline at end of file
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/ZebraSchema.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/ZebraSchema.java
deleted file mode 100644
index 51fe5dd58..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/ZebraSchema.java
+++ /dev/null
@@ -1,39 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.mapreduce;
-
-import org.apache.hadoop.zebra.schema.Schema;
-
-/**
- * A wrapper class for Schema.
- */
-public class ZebraSchema {
-  private String schemaStr = null;
-
-  private ZebraSchema(String str) {
-    String normalizedStr = Schema.normalize(str);
-    schemaStr = normalizedStr; 
-  }
-  
-  public static ZebraSchema createZebraSchema(String str) {
-    return new ZebraSchema(str);
-  }
-  
-  public String toString() {
-    return schemaStr;
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/ZebraSortInfo.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/ZebraSortInfo.java
deleted file mode 100644
index dc607e85f..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/ZebraSortInfo.java
+++ /dev/null
@@ -1,46 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.mapreduce;
-
-import org.apache.hadoop.io.RawComparator;
-import org.apache.hadoop.zebra.tfile.TFile;
-
-/**
- * A wrapper class for SortInfo.
- */
-public class ZebraSortInfo {
-  private String sortColumnsStr = null;
-  private String comparatorStr = null;
-  
-  private ZebraSortInfo(String sortColumns, Class<? extends RawComparator<Object>> comparatorClass) {
-    if (comparatorClass != null)
-      comparatorStr = TFile.COMPARATOR_JCLASS+comparatorClass.getName();
-    sortColumnsStr = sortColumns;
-  }
-  
-  public static ZebraSortInfo createZebraSortInfo(String sortColumns, Class<? extends RawComparator<Object>> comparatorClass) {
-    return new ZebraSortInfo(sortColumns, comparatorClass);
-  }
-  
-  public String getSortColumns() {
-    return sortColumnsStr;
-  }
-
-  public String getComparator() {
-    return comparatorStr;
-  }    
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/ZebraStorageHint.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/ZebraStorageHint.java
deleted file mode 100644
index 260e9e59e..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/ZebraStorageHint.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.mapreduce;
-
-/**
- * A wrapper class for StorageHint.
- */
-public class ZebraStorageHint {
-  private String storageHintStr = null;
-
-  private ZebraStorageHint(String shStr) {
-    storageHintStr = shStr;
-  }
-  
-  public static ZebraStorageHint createZebraStorageHint(String shStr) {
-    return new ZebraStorageHint(shStr);
-  }
-  
-  public String toString() {
-    return storageHintStr;
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/package-info.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/package-info.java
deleted file mode 100644
index e8dde2df0..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/mapreduce/package-info.java
+++ /dev/null
@@ -1,27 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-/**
- * Providing {@link org.apache.hadoop.mapreduce.InputFormat} and
- * {@link org.apache.hadoop.mapreduce.OutputFormat} adaptor classes for Hadoop
- * Zebra Table.
- * 
- * This package is based on org.apache.hadoop.mapred with adaptation of Hadoop 20 API. The
- * original package is now depreciated.
- */
-package org.apache.hadoop.zebra.mapreduce;
-
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/package-info.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/package-info.java
deleted file mode 100644
index b5a0f713c..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/package-info.java
+++ /dev/null
@@ -1,68 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-/**
- * Hadoop Table - tabular data storage for Hadoop MapReduce and PIG.
- * <p>
- * Hadoop Table provides tabular-type data storage for <a
- * href="http://hadoop.apache.org/core/docs/current/mapred_tutorial.html">Hadoop
- * MapReduce Framework</a>. It is also planned to allow Table to be closely
- * integrated with <a href="http://wiki.apache.org/pig/FrontPage">PIG</a>.
- * <p>
- * For this release, the basic construct of HadoopTable is called
- * {@link org.apache.hadoop.zebra.io.BasicTable}. A BasicTable is a create-once,
- * read-only kind of persisten data storage entity. A BasicTable contains zero
- * or more keyed rows.
- * <p>
- * The API uses Hadoop {@link org.apache.hadoop.io.BytesWritable} objects to
- * represent row keys, and PIG {@link org.apache.pig.data.Tuple} objects to
- * represent rows.
- * <p>
- * Each BasicTable maintains a {@link org.apache.hadoop.zebra.schema.Schema} ,
- * which, for this release, is nothing but a collection of column names. Given a
- * schema, we can deduce the integer index of a particular column, and use it to
- * extract (get) the desired datum from PIG Tuple object (which only allows
- * index-based access).
- * <p>
- * Typically, applications use
- * {@link org.apache.hadoop.zebra.mapreduce.BasicTableOutputFormat} (which implements
- * the Hadoop {@link org.apache.hadoop.mapred.OutputFormat} interface) to create
- * BasicTables through MapReduce. And they use
- * {@link org.apache.hadoop.zebra.mapreduce.TableInputFormat} (which implements the
- * Hadoop {@link org.apache.hadoop.mapred.InputFormat} to feed the data as their
- * MapReduce input.
- * <p>
- * The API is structured in three packages:
- * <UL>
- * <LI> {@link org.apache.hadoop.zebra.mapreduce} : The MapReduce layer. It contains
- * two classes: BasicTableOutputFormat for creating BasicTable; and
- * TableInputFormat for readding table.
- * 
- * <LI> {@link org.apache.hadoop.zebra.types} : Miscellaneous facilities that handle
- * column types and tuple serializations. Currently, it is a place holder that
- * redirects to PIG serialization. There is no type information being managed by
- * Table for individual columns.
- *
- * <LI> org.apache.hadoop.zebra.io : This is the internal IO layer. It deals 
- * with the physical storage (files) management of BasicTable. It also provides
- * facilities to help MapReduce layer create splits, such as partitioning
- * BasicTables for reading and reporting data block placement distributions
- * based on range-partitions or key-partitions.
- * </UL>
- */
-package org.apache.hadoop.zebra;
-
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/SchemaConverter.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/SchemaConverter.java
deleted file mode 100644
index 1bcdc418e..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/SchemaConverter.java
+++ /dev/null
@@ -1,198 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.IOException;
-
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.ColumnType;
-import org.apache.hadoop.zebra.schema.Schema.ColumnSchema;
-import org.apache.pig.ResourceSchema.ResourceFieldSchema;
-import org.apache.pig.data.DataType;
-import org.apache.pig.ResourceSchema;
-import org.apache.pig.impl.logicalLayer.FrontendException;
-import org.apache.pig.impl.logicalLayer.schema.Schema;
-import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;
-
-class SchemaConverter {
-    public static ColumnType toTableType(byte ptype) {
-        ColumnType ret;
-        switch (ptype) {
-        case DataType.INTEGER:
-            ret = ColumnType.INT; 
-            break;
-        case DataType.LONG:
-            ret = ColumnType.LONG; 
-            break;
-        case DataType.FLOAT:
-            ret = ColumnType.FLOAT; 
-            break;
-        case DataType.DOUBLE:
-            ret = ColumnType.DOUBLE; 
-            break;
-        case DataType.BOOLEAN:
-            ret = ColumnType.BOOL; 
-            break;
-        case DataType.DATETIME:
-            ret = ColumnType.DATETIME; 
-            break;
-        case DataType.BAG:
-            ret = ColumnType.COLLECTION; 
-            break;
-        case DataType.MAP:
-            ret = ColumnType.MAP; 
-            break;
-        case DataType.TUPLE:
-            ret = ColumnType.RECORD; 
-            break;
-        case DataType.CHARARRAY:
-            ret = ColumnType.STRING; 
-            break;
-        case DataType.BYTEARRAY:
-            ret = ColumnType.BYTES; 
-            break;
-        default:
-            ret = null;
-        break;
-        }
-        return ret;
-    }
-
-    public static Schema toPigSchema(
-            org.apache.hadoop.zebra.schema.Schema tschema)
-    throws FrontendException {
-        Schema ret = new Schema();
-        for (String col : tschema.getColumns()) {
-            org.apache.hadoop.zebra.schema.Schema.ColumnSchema columnSchema = 
-                tschema.getColumn(col);
-            if (columnSchema != null) {
-                ColumnType ct = columnSchema.getType();
-                if (ct == org.apache.hadoop.zebra.schema.ColumnType.RECORD ||
-                        ct == org.apache.hadoop.zebra.schema.ColumnType.COLLECTION)
-                    ret.add(new FieldSchema(col, toPigSchema(columnSchema.getSchema()), ct.pigDataType()));
-                else
-                    ret.add(new FieldSchema(col, ct.pigDataType()));
-            } else {
-                ret.add(new FieldSchema(null, null));
-            }
-        }
-        return ret;
-    }
-
-    public static org.apache.hadoop.zebra.schema.Schema fromPigSchema(
-            Schema pschema) throws FrontendException, ParseException {
-        org.apache.hadoop.zebra.schema.Schema tschema = new org.apache.hadoop.zebra.schema.Schema();
-        Schema.FieldSchema columnSchema;
-        for (int i = 0; i < pschema.size(); i++) {
-            columnSchema = pschema.getField(i);
-            if (columnSchema != null) {
-                if (DataType.isSchemaType(columnSchema.type))
-                    tschema.add(new org.apache.hadoop.zebra.schema.Schema.ColumnSchema(columnSchema.alias, 
-                            fromPigSchema(columnSchema.schema), toTableType(columnSchema.type)));
-                else if (columnSchema.type == DataType.MAP)
-                    tschema.add(new org.apache.hadoop.zebra.schema.Schema.ColumnSchema(columnSchema.alias, 
-                            new org.apache.hadoop.zebra.schema.Schema(new org.apache.hadoop.zebra.schema.Schema.ColumnSchema(null, 
-                                    org.apache.hadoop.zebra.schema.ColumnType.BYTES)), toTableType(columnSchema.type)));
-                else
-                    tschema.add(new org.apache.hadoop.zebra.schema.Schema.ColumnSchema(columnSchema.alias, toTableType(columnSchema.type)));
-            } else {
-                tschema.add(new org.apache.hadoop.zebra.schema.Schema.ColumnSchema(null, ColumnType.ANY));
-            }
-        }
-        return tschema;
-    }
-
-    public static org.apache.hadoop.zebra.schema.Schema convertFromResourceSchema(ResourceSchema rSchema)
-    throws ParseException {
-        if( rSchema == null )
-            return null;
-
-        org.apache.hadoop.zebra.schema.Schema schema = new org.apache.hadoop.zebra.schema.Schema();
-        ResourceSchema.ResourceFieldSchema[] fields = rSchema.getFields();
-        for( ResourceSchema.ResourceFieldSchema field : fields ) {
-            String name = field.getName();
-            ColumnType type = toTableType( field.getType() );
-            org.apache.hadoop.zebra.schema.Schema cSchema = convertFromResourceSchema( field.getSchema() );
-            if( type == ColumnType.MAP && cSchema == null ) {
-                cSchema = new org.apache.hadoop.zebra.schema.Schema();
-                cSchema.add( new org.apache.hadoop.zebra.schema.Schema.ColumnSchema( "", ColumnType.BYTES ) );
-            }
-            org.apache.hadoop.zebra.schema.Schema.ColumnSchema columnSchema = 
-                new org.apache.hadoop.zebra.schema.Schema.ColumnSchema( name, cSchema, type );
-            schema.add( columnSchema );
-        }
-
-        return schema;
-    }
-
-    public static ResourceSchema convertToResourceSchema(org.apache.hadoop.zebra.schema.Schema tSchema)
-    throws IOException {
-        if( tSchema == null )
-            return null;
-
-        ResourceSchema rSchema = new ResourceSchema();
-        int fieldCount = tSchema.getNumColumns();
-        ResourceFieldSchema[] rFields = new ResourceFieldSchema[fieldCount];
-        for( int i = 0; i < fieldCount; i++ ) {
-            org.apache.hadoop.zebra.schema.Schema.ColumnSchema cSchema = tSchema.getColumn( i );
-            if( cSchema != null ) 
-                rFields[i] = convertToResourceFieldSchema( cSchema );
-            else
-                rFields[i] = new ResourceFieldSchema();
-        }
-        rSchema.setFields( rFields );
-        return rSchema;
-    }
-
-    private static ResourceFieldSchema convertToResourceFieldSchema(
-            ColumnSchema cSchema) throws IOException {
-        ResourceFieldSchema field = new ResourceFieldSchema();
-
-        if( cSchema.getType() ==ColumnType.ANY && cSchema.getName().isEmpty() ) { // For anonymous column
-            field.setName( null );
-            field.setType(  DataType.BYTEARRAY );
-            field.setSchema( null );
-        } else {
-            field.setName( cSchema.getName() );
-            field.setType( cSchema.getType().pigDataType() );
-            if( cSchema.getType() == ColumnType.MAP ) {
-            	// Pig doesn't want any schema for a map field.
-                field.setSchema( null );
-            } else {
-            	org.apache.hadoop.zebra.schema.Schema fs = cSchema.getSchema();
-            	ResourceSchema rs = convertToResourceSchema( fs  );
-            	if( cSchema.getType() == ColumnType.COLLECTION ) {
-            		int count = fs.getNumColumns();
-            		if( count > 1 || ( count == 1 && fs.getColumn( 0 ).getType() != ColumnType.RECORD ) ) {
-            			// Pig requires a record (tuple) as the schema for a BAG field.
-            			ResourceFieldSchema fieldSchema = new ResourceFieldSchema();
-            			fieldSchema.setSchema( rs );
-            			fieldSchema.setType( ColumnType.RECORD.pigDataType() );
-            			rs = new ResourceSchema();
-            			rs.setFields( new ResourceFieldSchema[] { fieldSchema } );
-            		}
-            	}
-                field.setSchema( rs );
-            }
-        }
-
-        return field;
-    }
-  
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/TableLoader.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/TableLoader.java
deleted file mode 100644
index 0ab8e68b9..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/TableLoader.java
+++ /dev/null
@@ -1,551 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-import java.util.Properties;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.WritableComparable;
-import org.apache.hadoop.mapreduce.InputFormat;
-import org.apache.hadoop.mapreduce.InputSplit;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.mapreduce.TableInputFormat;
-import org.apache.hadoop.zebra.mapreduce.TableRecordReader;
-import org.apache.hadoop.zebra.mapreduce.TableInputFormat.SplitMode;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.ColumnType;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.schema.Schema.ColumnSchema;
-import org.apache.hadoop.zebra.types.Projection;
-import org.apache.hadoop.zebra.types.SortInfo;
-import org.apache.pig.Expression;
-import org.apache.pig.LoadFunc;
-import org.apache.pig.LoadMetadata;
-import org.apache.pig.LoadPushDown;
-import org.apache.pig.OrderedLoadFunc;
-import org.apache.pig.ResourceSchema;
-import org.apache.pig.ResourceStatistics;
-import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigSplit;
-import org.apache.pig.data.DefaultTupleFactory;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.impl.logicalLayer.FrontendException;
-import org.apache.pig.impl.util.UDFContext;
-import org.apache.hadoop.zebra.pig.comparator.*;
-import org.apache.pig.IndexableLoadFunc;
-import org.apache.pig.CollectableLoadFunc;
-
-/**
- * Pig IndexableLoadFunc and Slicer for Zebra Table
- */
-public class TableLoader extends LoadFunc implements LoadMetadata, LoadPushDown,
-        IndexableLoadFunc, CollectableLoadFunc, OrderedLoadFunc {
-    static final Log LOG = LogFactory.getLog(TableLoader.class);
-
-    private static final String UDFCONTEXT_PROJ_STRING = "zebra.UDFContext.projectionString";
-    private static final String UDFCONTEXT_GLOBAL_SORTING = "zebra.UDFContext.globalSorting";
-    private static final String UDFCONTEXT_PATHS_STRING = "zebra.UDFContext.pathsString";
-    private static final String UDFCONTEXT_INPUT_DELETED_CGS = "zebra.UDFContext.deletedcgs";
-    private static final String INPUT_SORT = "mapreduce.lib.table.input.sort";
-    private static final String INPUT_SPLIT_MODE = "mapreduce.lib.table.input.split_mode";
-
-    private static final String INPUT_FE = "mapreduce.lib.table.input.fe";
-    private static final String INPUT_DELETED_CGS = "mapreduce.lib.table.input.deleted_cgs";
-    private static final String GLOBALLY_SORTED = "globally_sorted";
-    private static final String LOCALLY_SORTED = "locally_sorted";
-
-    private String projectionString;
-
-    private TableRecordReader tableRecordReader = null;
-
-    private Schema schema;  
-    private SortInfo sortInfo;
-    private boolean sorted = false;
-    private Schema projectionSchema;
-    private String udfContextSignature = null;
-    
-    private Configuration conf = null;
-
-    private KeyGenerator keyGenerator = null;
-
-    /**
-     * default constructor
-     */
-    public TableLoader() {
-    }
-
-    /**
-     * @param projectionStr
-     *           projection string passed from pig query.
-     */
-    public TableLoader(String projectionStr) {
-        if( projectionStr != null && !projectionStr.isEmpty() )
-            projectionString = projectionStr;
-    }
-
-    /**
-     * @param projectionStr
-     *           projection string passed from pig query.
-     * @param sorted
-     *      need sorted table(s)?
-     */
-    public TableLoader(String projectionStr, String sorted) throws IOException {
-        this( projectionStr );
-        
-        if( sorted.equalsIgnoreCase( "sorted" ) )
-            this.sorted = true;
-        else
-            throw new IOException( "Invalid argument to the table loader constructor: " + sorted );
-    }
-
-    @Override
-    public void initialize(Configuration conf) throws IOException {
-        // Here we do ugly workaround for the problem in pig. the passed in parameter conf contains 
-        // value for TableInputFormat.INPUT_PROJ that was set by left table execution in a merge join
-        // case. Here, we try to get rid of the side effect and copy everything expect that entry.
-        this.conf = new Configuration( false );
-        Iterator<Map.Entry<String, String>> it = conf.iterator();
-        while( it.hasNext() ) {
-            Map.Entry<String, String> entry = it.next();
-            String key = entry.getKey();
-            if( key.equals( "mapreduce.lib.table.input.projection" ) ) // The string const is defined in TableInputFormat.
-                continue;
-            this.conf.set( entry.getKey(), entry.getValue() );
-        }
-
-        tableRecordReader = createIndexReader();
-
-        String[] sortColNames = sortInfo.getSortColumnNames();
-        byte[] types = new byte[sortColNames.length];
-        for(int i =0 ; i < sortColNames.length; ++i){
-            types[i] = schema.getColumn(sortColNames[i]).getType().pigDataType();
-        }
-        keyGenerator = makeKeyBuilder( types );
-    }
-
-    /**
-     * This method is called only once.
-     */
-    @Override
-    public void seekNear(Tuple tuple) throws IOException {
-        BytesWritable key = keyGenerator.generateKey( tuple );
-        tableRecordReader.seekTo( key );
-    }
-    
-    private TableRecordReader createIndexReader() throws IOException {
-        Job job = new Job( conf );
-
-        // Obtain the schema and sort info. for index reader, the table must be sorted.
-        schema = TableInputFormat.getSchema( job );
-        sorted = true;
-        
-        setSortOrder( job );
-        setProjection( job );
-
-        try {
-            return TableInputFormat.createTableRecordReader( job, TableInputFormat.getProjection( job ) );
-        } catch(ParseException ex) {
-            throw new IOException( "Exception from TableInputFormat.getTableRecordReader: "+ ex.getMessage() );
-        } catch(InterruptedException ex){
-            throw new IOException( "Exception from TableInputFormat.getTableRecordReader: " + ex.getMessage() );
-        }
-    }
-
-    
-    /**
-     * it processes sortedness of table .
-     * 
-     * @param job
-     * @throws IOException
-     */
-    private void setSortOrder(Job job) throws IOException {
-       Properties properties = UDFContext.getUDFContext().getUDFProperties( 
-              this.getClass(), new String[]{ udfContextSignature } );
-       boolean requireGlobalOrder = "true".equals(properties.getProperty( UDFCONTEXT_GLOBAL_SORTING));
-       if (requireGlobalOrder && !sorted)
-         throw new IOException("Global sorting can be only asked on table loaded as sorted");
-       if( sorted ) {
-           SplitMode splitMode = 
-             requireGlobalOrder ? SplitMode.GLOBALLY_SORTED : SplitMode.LOCALLY_SORTED;
-           TableInputFormat.setSplitMode(job, splitMode, null);
-           sortInfo = TableInputFormat.getSortInfo( job );
-       }    	
-    }
-
-    /**
-     * This is a light version of setSortOrder, which is called in getLocation(), which is also called at backend.
-     * We need to do this to avoid name node call in mappers. Original setSortOrder() is stilled called (in
-     * getSchema()), so it's okay to skip the checks that are performed when setSortOrder() is called.
-     * 
-     * This will go away once we have a better solution.
-     * 
-     * @param job
-     * @throws IOException
-     */
-    private void setSortOrderLight(Job job) throws IOException {
-        Properties properties = UDFContext.getUDFContext().getUDFProperties( 
-               this.getClass(), new String[]{ udfContextSignature } );
-        boolean requireGlobalOrder = "true".equals(properties.getProperty( UDFCONTEXT_GLOBAL_SORTING));
-        if (requireGlobalOrder && !sorted)
-          throw new IOException("Global sorting can be only asked on table loaded as sorted");
-        if( sorted ) {
-            SplitMode splitMode = 
-                requireGlobalOrder ? SplitMode.GLOBALLY_SORTED : SplitMode.LOCALLY_SORTED;
-
-            Configuration conf = job.getConfiguration();
-            conf.setBoolean(INPUT_SORT, true);
-            if (splitMode == SplitMode.GLOBALLY_SORTED)
-                conf.set(INPUT_SPLIT_MODE, GLOBALLY_SORTED);
-            else if (splitMode == SplitMode.LOCALLY_SORTED)
-                conf.set(INPUT_SPLIT_MODE, LOCALLY_SORTED);
-        }        
-     }
-
-    /**
-     * This method sets projection.
-     * 
-     * @param job
-     * @throws IOException
-     */
-    private void setProjection(Job job) throws IOException {
-      Properties properties = UDFContext.getUDFContext().getUDFProperties( 
-          this.getClass(), new String[]{ udfContextSignature } );   
-      try {
-          String prunedProjStr = properties.getProperty( UDFCONTEXT_PROJ_STRING );
-          
-          if( prunedProjStr != null ) {
-              TableInputFormat.setProjection( job, prunedProjStr );
-          } else if( projectionString != null ) {              
-              TableInputFormat.setProjection( job, projectionString );
-          }
-      } catch (ParseException ex) {
-          throw new IOException( "Schema parsing failed : " + ex.getMessage() );
-      }
-    }
-
-    private KeyGenerator makeKeyBuilder(byte[] elems) {
-        ComparatorExpr[] exprs = new ComparatorExpr[elems.length];
-        for (int i = 0; i < elems.length; ++i) {
-            exprs[i] = ExprUtils.primitiveComparator(i, elems[i]);
-        }
-        return new KeyGenerator(ExprUtils.tupleComparator(exprs));
-    }  
-
-    /*
-     * Hack: use FileInputFormat to decode comma-separated multiple path format.
-     */ 
-     private static Path[] getPathsFromLocation(String location, Job j) throws IOException {
-         FileInputFormat.setInputPaths( j, location );
-         Path[] paths = FileInputFormat.getInputPaths( j );
-
-         /**
-          * Performing glob pattern matching
-          */
-         List<Path> result = new ArrayList<Path>(paths.length);
-         Configuration conf = j.getConfiguration();
-         for (Path p : paths) {
-             FileSystem fs = p.getFileSystem(conf);
-             FileStatus[] matches = fs.globStatus(p);
-             if( matches == null ) {
-                 throw new IOException("Input path does not exist: " + p);
-             } else if (matches.length == 0) {
-                 LOG.warn("Input Pattern " + p + " matches 0 files");
-             } else {
-                 for (FileStatus globStat: matches) {
-                     if (globStat.isDir()) {
-                         result.add(globStat.getPath());
-                     } else {
-                         LOG.warn("Input path " + p + " is not a directory");
-                     }
-                 }
-             }
-         }
-         
-         if (result.isEmpty()) {
-             throw new IOException("No table specified for input");
-         }
-
-         LOG.info("Total input tables to process : " + result.size()); 
-
-         return result.toArray( new Path[result.size()] );
-     }
-
-     @Override
-     public Tuple getNext() throws IOException {
-         if (tableRecordReader.atEnd())
-             return null;
-         try {
-             tableRecordReader.nextKeyValue();
-             ArrayList<Object> fields = new ArrayList<Object>(tableRecordReader.getCurrentValue().getAll());
-             return DefaultTupleFactory.getInstance().newTuple(fields);
-         } catch (InterruptedException ex) {
-             throw new IOException( "InterruptedException:" + ex );
-         }
-     }
-
-     @Override
-     public void close() throws IOException {
-         if (tableRecordReader != null)
-             tableRecordReader.close();
-     }
-
-     @SuppressWarnings("unchecked")
-     @Override
-     public void prepareToRead(org.apache.hadoop.mapreduce.RecordReader reader, PigSplit split)
-     throws IOException {
-         tableRecordReader = (TableRecordReader)reader;
-         if( tableRecordReader == null )
-             throw new IOException( "Invalid object type passed to TableLoader" );
-     }
-
-     /**
-      * This method is called by pig on both frontend and backend.
-      */
-     @Override
-     public void setLocation(String location, Job job) throws IOException {
-         Properties properties = UDFContext.getUDFContext().getUDFProperties( 
-                 this.getClass(), new String[]{ udfContextSignature } );
-
-         // Retrieve paths from UDFContext to avoid name node call in mapper.
-         String pathString = properties.getProperty( UDFCONTEXT_PATHS_STRING );
-         Path[]paths = deserializePaths( pathString );
-         
-         // Retrieve deleted column group information to avoid name node call in mapper.
-         String deletedCGs = properties.getProperty( UDFCONTEXT_INPUT_DELETED_CGS );
-         job.getConfiguration().set(INPUT_FE, "true");
-         job.getConfiguration().set(INPUT_DELETED_CGS, deletedCGs );
-         
-         TableInputFormat.setInputPaths( job, paths );
-         
-         // The following obviously goes beyond of set location, but this is the only place that we
-         // can do and it's suggested by Pig team.
-         setSortOrderLight( job );
-         setProjection( job );
-     }
-     
-     private static String serializePaths(Path[] paths) {
-         StringBuilder sb = new StringBuilder();
-         for( int i = 0; i < paths.length; i++ ) {
-             sb.append( paths[i].toString() );
-             if( i < paths.length -1 ) {
-                 sb.append( ";" );
-             }
-         }
-         return sb.toString();
-     }
-     
-     private static Path[] deserializePaths(String val) {
-         String[] paths = val.split( ";" );
-         Path[] result = new Path[paths.length];
-         for( int i = 0; i < paths.length; i++ ) {
-             result[i] = new Path( paths[i] );
-         }
-         return result;
-     }
-
-     @SuppressWarnings("unchecked")
-     @Override
-     public InputFormat getInputFormat() throws IOException {
-         return new TableInputFormat();
-     }
-
-     @Override
-     public String[] getPartitionKeys(String location, Job job)
-     throws IOException {
-         return null;
-     }
-
-     @Override
-     public ResourceSchema getSchema(String location, Job job) throws IOException {
-         Properties properties = UDFContext.getUDFContext().getUDFProperties( 
-                 this.getClass(), new String[]{ udfContextSignature } );
-         
-         // Save the paths in UDFContext so that it can be retrieved in setLocation().
-         Path[] paths = getPathsFromLocation( location, job );
-         properties.setProperty( UDFCONTEXT_PATHS_STRING, serializePaths( paths ) );
-         
-         TableInputFormat.setInputPaths( job, paths );
-
-         // Save the deleted column group information in UDFContext so that it can be used in setLocation().
-         // Property INPUT_DELETED_CGS is set in TableInputFormat.setInputPaths(). We just retrieve it here.
-         properties.setProperty( UDFCONTEXT_INPUT_DELETED_CGS,  job.getConfiguration().get(INPUT_DELETED_CGS) );
-
-         Schema tableSchema = null;
-         if( paths.length == 1 ) {
-             tableSchema = BasicTable.Reader.getSchema( paths[0], job.getConfiguration() );
-         } else {
-             tableSchema = new Schema();
-             for (Path p : paths) {
-                 Schema schema = BasicTable.Reader.getSchema( p, job.getConfiguration() );
-                 try {
-                     tableSchema.unionSchema( schema );
-                 } catch (ParseException e) {
-                     throw new IOException(e.getMessage());
-                 }
-             }
-         }
-         
-         // This is needed as it does a check if a unsorted table is loaded as sorted
-         // It fails if unosrted table is loaded as sorted
-         setSortOrder( job );
-         
-         /*
-         As per pig team any changes to this job object will be thrown away.
-         getSchema is needed to return the projectionSchema for the projection
-         string specified in TableLoader constructor. So, projectionString is used
-         here. However, setLocation() calls setPojection() because that is called after
-         projectionPruning and needs to read projection string from UDFCONTEXT
-         That also sets/calls TableInputFormat.setProjection(job, $prunedProj). But the 
-         job object here in getSchema() is a different copy from setLocation() and hence 
-         the changes will not be overridden as per PIG TEAM.  
-        */
-
-         projectionSchema = tableSchema;
-         try {
-        	 if(projectionString != null)
-        		 TableInputFormat.setProjection(job, projectionString);        	         	 
-             Projection projection = new org.apache.hadoop.zebra.types.Projection( tableSchema, 
-                     TableInputFormat.getProjection( job ) );
-             projectionSchema = projection.getProjectionSchema();
-         } catch (ParseException e) {
-             throw new IOException( "Schema parsing failed : "+ e.getMessage() );
-         }
-
-         if( projectionSchema == null ) {
-             throw new IOException( "Cannot determine table projection schema" );
-         }
-
-         return SchemaConverter.convertToResourceSchema( projectionSchema );
-     }
-
-     @Override
-     public ResourceStatistics getStatistics(String location, Job job)
-     throws IOException {
-         // Statistics is not supported.
-         return null;
-     }
-
-     @Override
-     public void setPartitionFilter(Expression partitionFilter)
-     throws IOException {
-         // no-op. It should not be ever called since getPartitionKeys returns null.        
-     }
-
-     @Override
-     public List<OperatorSet> getFeatures() {
-         List<OperatorSet> features = new ArrayList<OperatorSet>(1);
-         features.add( LoadPushDown.OperatorSet.PROJECTION );
-         return features;
-     }
-
-     @Override
-     public RequiredFieldResponse pushProjection(RequiredFieldList requiredFieldList)
-     throws FrontendException {
-            List<RequiredField> rFields = requiredFieldList.getFields();
-            if( rFields == null) {
-                throw new FrontendException("requiredFieldList.getFields() can not return null in fieldsToRead");
-            }    
-
-            String projectionStr = "";
-            
-            Iterator<RequiredField> it= rFields.iterator();
-            while( it.hasNext() ) {
-                RequiredField rField = (RequiredField) it.next();
-                ColumnSchema cs = projectionSchema.getColumn(rField.getIndex());
-                
-                if(cs == null) {
-                    throw new FrontendException
-                    ("Null column schema in projection schema in fieldsToRead at index " + rField.getIndex()); 
-                }
-                
-                if(cs.getType() != ColumnType.MAP && (rField.getSubFields() != null)) {        
-                    throw new FrontendException
-                    ("Zebra cannot have subfields for a non-map column type in fieldsToRead " + 
-                     "ColumnType:" + cs.getType() + " index in zebra projection schema: " + rField.getIndex()
-                    );
-                }
-                String name = cs.getName();
-                projectionStr = projectionStr + name ;
-                if(cs.getType() == ColumnType.MAP) {        
-                    List<RequiredField> subFields = rField.getSubFields();
-                    
-                    if( subFields != null ) {
-                    
-                        Iterator<RequiredField> its= subFields.iterator();
-                        boolean flag = false;
-                        if(its.hasNext()) {
-                            flag = true;
-                            projectionStr += "#" + "{";
-                        }    
-                        String tmp = "";
-                        while(its.hasNext()) {
-                            RequiredField sField = (RequiredField) its.next();    
-                            tmp = tmp + sField.getAlias();
-                            if(its.hasNext()) {
-                                tmp = tmp + "|";
-                            }
-                        }  
-                        if( flag ) {
-                            projectionStr = projectionStr + tmp + "}";
-                        }
-                    }    
-                }
-                if(it.hasNext()) {
-                    projectionStr = projectionStr + " , ";
-                }
-            }
-            
-            Properties properties = UDFContext.getUDFContext().getUDFProperties( 
-                    this.getClass(), new String[]{ udfContextSignature } );
-            if( projectionStr != null && !projectionStr.isEmpty() )
-                properties.setProperty( UDFCONTEXT_PROJ_STRING, projectionStr );
-
-            return new RequiredFieldResponse( true );
-     }
-
-        @Override
-        public void setUDFContextSignature(String signature) {
-            udfContextSignature = signature;
-        }
-
-		@Override
-		public WritableComparable<?> getSplitComparable(InputSplit split)
-				throws IOException {
-	        return TableInputFormat.getSortedTableSplitComparable( split );
-		}
-        
-		@Override
-		public void ensureAllKeyInstancesInSameSplit() throws IOException {
-			Properties properties = UDFContext.getUDFContext().getUDFProperties(this.getClass(),
-					new String[] { udfContextSignature } );
-			properties.setProperty(UDFCONTEXT_GLOBAL_SORTING, "true");
-		}
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/TableStorer.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/TableStorer.java
deleted file mode 100644
index b4e3f73b1..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/TableStorer.java
+++ /dev/null
@@ -1,229 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.IOException;
-import java.util.Properties;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.RecordWriter;
-import org.apache.hadoop.zebra.mapreduce.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.mapreduce.ZebraOutputPartition;
-import org.apache.hadoop.zebra.mapreduce.ZebraSchema;
-import org.apache.hadoop.zebra.mapreduce.ZebraSortInfo;
-import org.apache.hadoop.zebra.mapreduce.ZebraStorageHint;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.ZebraConf;
-import org.apache.pig.LoadFunc;
-import org.apache.pig.ResourceSchema;
-import org.apache.pig.ResourceStatistics;
-import org.apache.pig.StoreFunc;
-import org.apache.pig.StoreMetadata;
-import org.apache.pig.ResourceSchema.ResourceFieldSchema;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.impl.util.UDFContext;
-
-/**
- * Pig LoadFunc implementation for Zebra Table
- */
-public class TableStorer extends StoreFunc implements StoreMetadata {
-    private static final String UDFCONTEXT_OUTPUT_SCHEMA = "zebra.UDFContext.outputSchema";
-    private static final String UDFCONTEXT_SORT_INFO = "zebra.UDFContext.sortInfo";
-    private static final String UDFCONTEXT_OUTPUT_CHECKTYPE = "zebra.UDFContext.checkType";
-
-    private String storageHintString = null;
-    private String udfContextSignature = null;
-    private RecordWriter<BytesWritable, Tuple> tableRecordWriter = null;
-    private String partitionClassString = null;
-    Class<? extends ZebraOutputPartition> partitionClass = null;
-    private String partitionClassArgumentsString = null;
-
-    public TableStorer() {
-    }
-
-    public TableStorer(String storageHintString) {
-      this.storageHintString = storageHintString;
-    }
-
-    public TableStorer(String storageHintString, String partitionClassString) {
-      this.storageHintString = storageHintString;
-      this.partitionClassString = partitionClassString;
-    }
-
-    public TableStorer(String storageHintString, String partitionClassString, String partitionClassArgumentsString) {
-      this.storageHintString = storageHintString;
-      this.partitionClassString = partitionClassString;
-      this.partitionClassArgumentsString = partitionClassArgumentsString;
-    }
-
-
-    @Override
-    public void putNext(Tuple tuple) throws IOException {
-      try {
-        tableRecordWriter.write( null, tuple );
-      } catch (InterruptedException e) {
-        throw new IOException(e.getMessage());
-      }
-    }
-
-    @Override
-    public void checkSchema(ResourceSchema schema) throws IOException {
-      // Get schemaStr and sortColumnNames from the given schema. In the process, we
-      // also validate the schema and sorting info.
-      ResourceSchema.Order[] orders = schema.getSortKeyOrders();
-      boolean descending = false;
-      for (ResourceSchema.Order order : orders)
-      {
-        if (order == ResourceSchema.Order.DESCENDING)
-        {
-          Log LOG = LogFactory.getLog(TableStorer.class);
-          LOG.warn("Sorting in descending order is not supported by Zebra and the table will be unsorted.");
-          descending = true;
-          break;
-        }
-      }
-      StringBuilder sortColumnNames = new StringBuilder();
-      if (!descending) {
-        ResourceSchema.ResourceFieldSchema[] fields = schema.getFields();
-        int[] index = schema.getSortKeys();
-      
-        for( int i = 0; i< index.length; i++ ) {
-          ResourceFieldSchema field = fields[index[i]];
-          String name = field.getName();
-          if( name == null )
-              throw new IOException("Zebra does not support column positional reference yet");
-          if( !org.apache.pig.data.DataType.isAtomic( field.getType() ) )
-              throw new IOException( "Field [" + name + "] is not of simple type as required for a sort column now." );
-          if( i > 0 )
-              sortColumnNames.append( "," );
-          sortColumnNames.append( name );
-        }
-      }
-
-      // Convert resource schema to zebra schema
-      org.apache.hadoop.zebra.schema.Schema zebraSchema;
-      try {
-          zebraSchema = SchemaConverter.convertFromResourceSchema( schema );
-      } catch (ParseException ex) {
-          throw new IOException("Exception thrown from SchemaConverter: " + ex.getMessage() );
-      }
-
-      Properties properties = UDFContext.getUDFContext().getUDFProperties( 
-              this.getClass(), new String[]{ udfContextSignature } );
-      properties.setProperty( UDFCONTEXT_OUTPUT_SCHEMA, zebraSchema.toString() );
-      properties.setProperty( UDFCONTEXT_SORT_INFO, sortColumnNames.toString() );
-        
-      // This is to turn off type check for potential corner cases - for internal use only;
-      if (System.getenv("zebra_output_checktype") != null && System.getenv("zebra_output_checktype").equals("no")) {
-        properties.setProperty( UDFCONTEXT_OUTPUT_CHECKTYPE, "no");
-      }
-    }
-
-    @SuppressWarnings("unchecked")
-    @Override
-    public org.apache.hadoop.mapreduce.OutputFormat getOutputFormat()
-    throws IOException {
-      return new BasicTableOutputFormat();
-    }
-
-    @SuppressWarnings("unchecked")
-    @Override
-    public void prepareToWrite(RecordWriter writer)
-    throws IOException {
-      tableRecordWriter = writer;
-      if( tableRecordWriter == null ) {
-          throw new IOException( "Invalid type of writer. Expected type: TableRecordWriter." );
-      }
-    }
-
-    @Override
-    public String relToAbsPathForStoreLocation(String location, Path curDir)
-    throws IOException {
-      return LoadFunc.getAbsolutePath( location, curDir );
-    }
-
-    @Override
-    public void setStoreLocation(String location, Job job) throws IOException {
-      Configuration conf = job.getConfiguration();
-      
-      String[] outputs = location.split(",");
-      
-      if (outputs.length == 1) {
-        BasicTableOutputFormat.setOutputPath(job, new Path(location));
-      } else if (outputs.length > 1) {
-        if (partitionClass == null) {
-          try {
-            partitionClass = (Class<? extends ZebraOutputPartition>) conf.getClassByName(partitionClassString);
-          } catch (ClassNotFoundException e) {
-            throw new IOException(e);
-          } 
-        }
-        
-        Path[] paths = new Path[outputs.length];
-        for (int i=0; i<paths.length; i++) {
-          paths[i] = new Path(outputs[i]);
-        }
-
-        BasicTableOutputFormat.setMultipleOutputs(job, partitionClass, partitionClassArgumentsString, paths);
-      } else {
-        throw new IOException( "Invalid location : " + location);
-      }
-
-      // Get schema string and sorting info from UDFContext and re-store them to
-      // job config.
-      Properties properties = UDFContext.getUDFContext().getUDFProperties( 
-              this.getClass(), new String[]{ udfContextSignature } );
-      ZebraSchema zSchema = ZebraSchema.createZebraSchema(properties.getProperty(UDFCONTEXT_OUTPUT_SCHEMA));
-      ZebraSortInfo zSortInfo = ZebraSortInfo.createZebraSortInfo(properties.getProperty(UDFCONTEXT_SORT_INFO), null);
-      ZebraStorageHint zStorageHint = ZebraStorageHint.createZebraStorageHint(storageHintString);
-      try {
-        BasicTableOutputFormat.setStorageInfo(job, zSchema, zStorageHint, zSortInfo);
-      } catch (ParseException e) {
-        throw new IOException("Invalid storage info: " + e.getMessage());
-      }
-        
-      // Get checktype information from UDFContext and re-store it to job config;
-      if (properties.getProperty(UDFCONTEXT_OUTPUT_CHECKTYPE) != null && properties.getProperty(UDFCONTEXT_OUTPUT_CHECKTYPE).equals("no")) {
-        ZebraConf.setCheckType(conf, false);
-      }
-    }
-
-    @Override
-    public void storeSchema(ResourceSchema schema, String location, Job job)
-    throws IOException {
-      //TODO: This is temporary - we will do close at cleanupJob() when OutputCommitter is ready.
-      BasicTableOutputFormat.close(job);
-    }
-
-    @Override
-    public void setStoreFuncUDFContextSignature(String signature) {
-      udfContextSignature = signature;
-    }
-
-    @Override
-    public void storeStatistics(ResourceStatistics stats, String location,
-            Job job) throws IOException {
-      // no-op
-    }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/BagExpr.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/BagExpr.java
deleted file mode 100644
index 55579699f..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/BagExpr.java
+++ /dev/null
@@ -1,95 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.pig.comparator;
-
-import java.io.PrintStream;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.Tuple;
-
-public class BagExpr extends LeafExpr {
-  protected List<LeafGenerator> list;
-  protected final ComparatorExpr expr;
-  protected int el, cel;
-  protected boolean c;
-
-  public BagExpr(int index, ComparatorExpr expr) {
-    super(index);
-    this.expr = expr;
-  }
-
-  @Override
-  protected
-  void appendObject(EncodingOutputStream out, Object object)
-      throws ExecException {
-
-    if (list == null) {
-      // This is the first time we get called. build the execution plan.
-      el = out.getEscapeLevel();
-      cel = out.getComescLevel();
-      c = out.getComplement();
-      list = new ArrayList<LeafGenerator>();
-      // requiring the individual items to be explicitly bounded.
-      expr.appendLeafGenerator(list, el, cel, c, true);
-    }
-
-    DataBag bag = (DataBag) object;
-    for (Iterator<Tuple> it = bag.iterator(); it.hasNext();) {
-      Tuple t = it.next();
-      for (Iterator<LeafGenerator> it2 = list.iterator(); it2.hasNext();) {
-        LeafGenerator g = it2.next();
-        g.append(out, t);
-      }
-    }
-  }
-
-  @Override
-  protected
-  boolean implicitBound() {
-    return true;
-  }
-
-  public void illustrate(PrintStream out, int escapeLevel, int comescLevel,
-      boolean complement) {
-    List<LeafGenerator> l = new ArrayList<LeafGenerator>();
-    expr.appendLeafGenerator(l, escapeLevel, comescLevel, complement, true);
-    out.printf("(%s, %d, [", getType(), index);
-    for (Iterator<LeafGenerator> it = l.iterator(); it.hasNext();) {
-      LeafGenerator leaf = it.next();
-      leaf.illustrate(out);
-    }
-    out.print("])");
-  }
-
-  @Override
-  protected
-  String getType() {
-    return "Bag";
-  }
-
-  @Override
-  protected
-  void toString(PrintStream out) {
-    out.printf("%s(%d, ", getType(), index);
-    expr.toString(out);
-    out.print(")");
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/BooleanExpr.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/BooleanExpr.java
deleted file mode 100644
index 42477bf49..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/BooleanExpr.java
+++ /dev/null
@@ -1,37 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.pig.comparator;
-
-public final class BooleanExpr extends FixedLengthPrimitive {
-  public BooleanExpr(int index) {
-    super(index, 1);
-  }
-
-  protected void convertValue(byte[] b, Object o) {
-    if (b.length != 1) {
-      throw new IllegalArgumentException("expecting array length == 1");
-    }
-    boolean l = (Boolean) o;
-    // Avoid using 0, 1, which may require escaping later.
-    b[0] = (byte) (l ? 3 : 2);
-  }
-
-  @Override
-  protected String getType() {
-    return "Boolean";
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/ByteExpr.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/ByteExpr.java
deleted file mode 100644
index c14a23dd6..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/ByteExpr.java
+++ /dev/null
@@ -1,37 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.pig.comparator;
-
-public final class ByteExpr extends FixedLengthPrimitive {
-  public ByteExpr(int index) {
-    super(index, 1);
-  }
-
-  protected void convertValue(byte[] b, Object o) {
-    if (b.length != 1) {
-      throw new IllegalArgumentException("Expecting length==1");
-    }
-    byte l = (Byte) o;
-    l ^= 1 << (Byte.SIZE - 1);
-    b[0] = l;
-  }
-
-  @Override
-  protected String getType() {
-    return "Byte";
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/BytesExpr.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/BytesExpr.java
deleted file mode 100644
index ee82a3fe6..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/BytesExpr.java
+++ /dev/null
@@ -1,41 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.pig.comparator;
-
-import org.apache.pig.data.DataByteArray;
-
-public final class BytesExpr extends LeafExpr {
-  public BytesExpr(int index) {
-    super(index);
-  }
-
-  @Override
-  protected void appendObject(EncodingOutputStream out, Object o) {
-    DataByteArray bytes = (DataByteArray) o;
-    out.write(bytes.get());
-  }
-
-  @Override
-  protected boolean implicitBound() {
-    return true;
-  }
-
-  @Override
-  protected String getType() {
-    return "ByteArray";
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/ComparatorExpr.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/ComparatorExpr.java
deleted file mode 100644
index 6b18cf620..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/ComparatorExpr.java
+++ /dev/null
@@ -1,30 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.pig.comparator;
-
-import java.io.PrintStream;
-import java.util.List;
-
-/**
- * Base class for comparator expressions
- */
-public abstract class ComparatorExpr {
-  protected abstract void appendLeafGenerator(List<LeafGenerator> list, int el,
-      int cel, boolean c, boolean explicitBound);
-
-  protected abstract void toString(PrintStream out);
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/DateTimeExpr.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/DateTimeExpr.java
deleted file mode 100644
index c1a7e6092..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/DateTimeExpr.java
+++ /dev/null
@@ -1,53 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.pig.comparator;
-
-import org.joda.time.DateTime;
-
-
-public final class DateTimeExpr extends FixedLengthPrimitive {
-  private static final int ONE_MINUTE = 60000;
-  public DateTimeExpr(int index) {
-    super(index, (Long.SIZE + Short.SIZE) / Byte.SIZE);
-  }
-
-  protected void convertValue(byte[] b, Object o) {
-    if (b.length != (Long.SIZE + Short.SIZE) / Byte.SIZE) {
-      throw new IllegalArgumentException("Incorrect buffer size");
-    }
-    long dt = ((DateTime) o).getMillis();
-    dt ^= 1L << (Long.SIZE - 1);
-    b[7] = (byte) dt;
-    b[6] = (byte) (dt >> 8);
-    b[5] = (byte) (dt >> 16);
-    b[4] = (byte) (dt >> 24);
-    b[3] = (byte) (dt >> 32);
-    b[2] = (byte) (dt >> 40);
-    b[1] = (byte) (dt >> 48);
-    b[0] = (byte) (dt >> 56);
-    
-    short dtz = (short) (((DateTime) o).getZone().getOffset((DateTime) o) / ONE_MINUTE);
-    dtz ^= 1 << (Short.SIZE - 1);
-    b[9] = (byte) dtz;
-    b[8] = (byte) (dtz >> 8);
-  }
-
-  @Override
-  protected String getType() {
-    return "DateTime";
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/DoubleExpr.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/DoubleExpr.java
deleted file mode 100644
index 6ba17ce5e..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/DoubleExpr.java
+++ /dev/null
@@ -1,52 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.pig.comparator;
-
-
-public class DoubleExpr extends FixedLengthPrimitive {
-  DoubleExpr(int index) {
-    super(index, Double.SIZE / Byte.SIZE);
-  }
-
-  @Override
-  protected void convertValue(byte[] b, Object o) {
-    if (b.length != Double.SIZE / Byte.SIZE) {
-      throw new IllegalArgumentException("Incorrect buffer size");
-    }
-    Double f = (Double) o;
-    long l = Double.doubleToLongBits(f);
-    if (l < 0) {
-      l = ~l;
-    } else {
-      l ^= 1L << (Long.SIZE - 1);
-    }
-    b[7] = (byte) l;
-    b[6] = (byte) (l >> 8);
-    b[5] = (byte) (l >> 16);
-    b[4] = (byte) (l >> 24);
-    b[3] = (byte) (l >> 32);
-    b[2] = (byte) (l >> 40);
-    b[1] = (byte) (l >> 48);
-    b[0] = (byte) (l >> 56);
-
-  }
-
-  @Override
-  protected String getType() {
-    return "Double";
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/ExprUtils.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/ExprUtils.java
deleted file mode 100644
index f5072c7b4..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/ExprUtils.java
+++ /dev/null
@@ -1,153 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.pig.comparator;
-
-import java.io.ByteArrayOutputStream;
-import java.io.PrintStream;
-import java.util.Collection;
-
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataType;
-
-public class ExprUtils {
-
-  /**
-   * Make a bag comparator expression.
-   * 
-   * @param index
-   *          the index of datum in the source tuple that is a {@link DataBag}
-   *          object.
-   * @param expr
-   *          a comparator expression that corresponds to the member tuples in
-   *          the bag.
-   * @return A comparator expression.
-   * 
-   *         <p>
-   *         Example: suppose we have tuple schema as follows: <code>
-   * Tuple {
-   *    int a;
-   *    String b;
-   *    Bag {
-   *        Tuple {
-   *            Bytes c;
-   *            int d;
-   *            String e;
-   *        }
-   *     } f
-   * }
-   * </code>
-   * 
-   *         We would like to sort by
-   *         <code>Tuple(b, Bag(Negate(Tuple(e, c))))</code>, we can construct
-   *         the ComparatorExpr as follows; <code>
-   * ComparatorExpr expr = tupleComparator(
-   *    leafComparator(1, DataType.CHARARRAY), 
-   *    bagComparator(2, 
-   *        negateComparator(
-   *            tupleComparator(
-   *                leafCmparator(2, DataType.CHARARRAY),
-   *                leafComparator(0, DataType.BYTEARRAY)))))
-   * </code>
-   */
-  public static ComparatorExpr bagComparator(int index, ComparatorExpr expr) {
-    return new BagExpr(index, expr);
-  }
-
-  /**
-   * Converting an expression to a string.
-   * 
-   * @param expr
-   * @return A string representation of the comparator expression.
-   */
-  public static String exprToString(ComparatorExpr expr) {
-    ByteArrayOutputStream bout = new ByteArrayOutputStream();
-    PrintStream out = new PrintStream(bout);
-    expr.toString(out);
-    out.close();
-    return new String(bout.toByteArray());
-  }
-
-  /**
-   * Comparator for primitive types.
-   * 
-   * @param index
-   *          Index in the source tuple.
-   * @param type
-   *          One of the constants defined in {@link DataType} for primitive
-   *          types.
-   * @return Comparator expression.
-   */
-  public static ComparatorExpr primitiveComparator(int index, int type) {
-    switch (type) {
-      case DataType.BOOLEAN:
-        return new BooleanExpr(index);
-      case DataType.BYTE:
-        return new ByteExpr(index);
-      case DataType.BYTEARRAY:
-        return new BytesExpr(index);
-      case DataType.CHARARRAY:
-        return new StringExpr(index);
-      case DataType.DOUBLE:
-        return new DoubleExpr(index);
-      case DataType.FLOAT:
-        return new FloatExpr(index);
-      case DataType.INTEGER:
-        return new IntExpr(index);
-      case DataType.LONG:
-        return new LongExpr(index);
-      case DataType.DATETIME:
-        return new DateTimeExpr(index);
-      default:
-        throw new RuntimeException("Not a prmitive PIG type");
-    }
-  }
-
-  /**
-   * Negate comparator
-   * 
-   * @param expr
-   *          expression to perform negation on.
-   * @return A comparator expression.
-   */
-  public static ComparatorExpr negationComparator(ComparatorExpr expr) {
-    return NegateExpr.makeNegateExpr(expr);
-  }
-
-  /**
-   * Make a Tuple comparator expression.
-   * 
-   * @param exprs
-   *          member comparator expressions.
-   * @return A comparator expression.
-   */
-  public static ComparatorExpr tupleComparator(
-      Collection<? extends ComparatorExpr> exprs) {
-    return TupleExpr.makeTupleComparator(exprs);
-  }
-
-  /**
-   * Make a Tuple comparator expression.
-   * 
-   * @param exprs
-   *          member comparator expressions.
-   * @return A comparator expression.
-   */
-  public static ComparatorExpr tupleComparator(ComparatorExpr... exprs) {
-    return TupleExpr.makeTupleExpr(exprs);
-  }
-
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/FixedLengthPrimitive.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/FixedLengthPrimitive.java
deleted file mode 100644
index dd36dee40..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/FixedLengthPrimitive.java
+++ /dev/null
@@ -1,46 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.pig.comparator;
-
-/**
- * Helper class for fixed length primitives
- */
-public abstract class FixedLengthPrimitive extends LeafExpr {
-  protected byte[] bytes;
-
-  /**
-   * Constructor
-   * 
-   * @param index
-   * @param length
-   */
-  protected FixedLengthPrimitive(int index, int length) {
-    super(index);
-    bytes = new byte[length];
-  }
- 
-  protected abstract void convertValue(byte[] b, Object o);
-
-  protected void appendObject(EncodingOutputStream out, Object o) {
-    convertValue(bytes, o);
-    out.write(bytes, 0, bytes.length);
-  }
-
-  protected boolean implicitBound() {
-    return false;
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/FloatExpr.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/FloatExpr.java
deleted file mode 100644
index a1efcd5a9..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/FloatExpr.java
+++ /dev/null
@@ -1,47 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.pig.comparator;
-
-
-public final class FloatExpr extends FixedLengthPrimitive {
-  public FloatExpr(int index) {
-    super(index, Float.SIZE / Byte.SIZE);
-  }
-
-  @Override
-  protected void convertValue(byte[] b, Object o) {
-    if (b.length != Float.SIZE / Byte.SIZE) {
-      throw new IllegalArgumentException("Incorrect buffer size");
-    }
-    Float f = (Float) o;
-    int l = Float.floatToIntBits(f);
-    if (l < 0) {
-      l = ~l;
-    } else {
-      l ^= 1 << (Integer.SIZE - 1);
-    }
-    b[3] = (byte) l;
-    b[2] = (byte) (l >> 8);
-    b[1] = (byte) (l >> 16);
-    b[0] = (byte) (l >> 24);
-  }
-
-  @Override
-  protected String getType() {
-    return "Float";
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/IntExpr.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/IntExpr.java
deleted file mode 100644
index b83da02e4..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/IntExpr.java
+++ /dev/null
@@ -1,41 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.pig.comparator;
-
-
-public final class IntExpr extends FixedLengthPrimitive {
-  public IntExpr(int index) {
-    super(index, Integer.SIZE / Byte.SIZE);
-  }
-
-  protected void convertValue(byte[] b, Object o) {
-    if (b.length != Integer.SIZE / Byte.SIZE) {
-      throw new IllegalArgumentException("Incorrect buffer size");
-    }
-    int l = (Integer) o;
-    l ^= 1 << (Integer.SIZE - 1);
-    b[3] = (byte) l;
-    b[2] = (byte) (l >> 8);
-    b[1] = (byte) (l >> 16);
-    b[0] = (byte) (l >> 24);
-  }
-
-  @Override
-  protected String getType() {
-    return "Integer";
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/KeyGenerator.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/KeyGenerator.java
deleted file mode 100644
index c55d5f46a..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/KeyGenerator.java
+++ /dev/null
@@ -1,370 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.pig.comparator;
-
-import java.io.ByteArrayOutputStream;
-import java.io.PrintStream;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Iterator;
-import java.util.List;
-
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.Tuple;
-
-/**
- * Extended from ByteArrayOutputStream with direct access to the underlying byte
- * array and explicit capacity expansion.
- * 
- * Also adding the capability of escaping:
- * 
- * <code>
- * el - escape level for 0x00, valid value 0-252
- * cel - escape level for 0xff, valid value 0-252
- *                  escaped
- * el       0x00            0x01        others
- * 0        AS-IS           AS-IS       AS-IS
- * >0       0x01 0x01+el    0x01FD      AS-IS
- *                  escaped             
- * cel      0xFF            0xFE        others
- * 0        AS-IS           AS-IS       AS-IS
- * >0       0xFE 0xFE-cel   0xFE02      AS-IS
- * </code>
- */
-class EncodingOutputStream extends ByteArrayOutputStream {
-  int escapeLevel = 0;
-  int comescLevel = 0;
-  boolean complement = false;
-
-  public EncodingOutputStream() {
-    super();
-  }
-
-  public EncodingOutputStream(int size) {
-    super(size);
-  }
-
-  public byte[] get() {
-    return buf;
-  }
-
-  public void ensureAvailable(int len) {
-    int newcount = count + len;
-    if (newcount > buf.length) {
-      buf = Arrays.copyOf(buf, Math.max(buf.length << 1, newcount));
-    }
-  }
-
-  public void setEscapeParams(int el, int cel, boolean c) {
-    escapeLevel = el;
-    comescLevel = cel;
-    complement = c;
-  }
-  
-  public int getEscapeLevel() {
-    return escapeLevel;
-  }
-
-  public int getComescLevel() {
-    return comescLevel;
-  }
-
-  public boolean getComplement() {
-    return complement;
-  }
-
-  void writeEscaped(int v, boolean c) {
-    ensureAvailable(2);
-    buf[count] = 0x01;
-    buf[count + 1] = (byte) v;
-    if (c) {
-      buf[count] = (byte) (~buf[count]);
-      buf[count + 1] = (byte) (~buf[count + 1]);
-    }
-    count += 2;
-  }
-
-  /**
-   * Write an escaped 0x00.
-   */
-  void escape00() {
-    writeEscaped(escapeLevel + 1, complement);
-  }
-
-  /**
-   * Write an escaped 0x01.
-   */
-  void escape01() {
-    writeEscaped(0xFD, complement);
-  }
-
-  /**
-   * Write an escaped 0xFE
-   */
-  void escapeFE() {
-    writeEscaped(0xFD, !complement);
-  }
-
-  /**
-   * write an escaped 0xFF
-   */
-  void escapeFF() {
-    writeEscaped(comescLevel + 1, !complement);
-  }
-
-  void complement(byte b[], int begin, int end) {
-    if (begin >= end) return;
-    ensureAvailable(end - begin);
-    if (!complement) {
-      System.arraycopy(b, begin, buf, count, end - begin);
-      count += (end - begin);
-    } else {
-      for (int i = begin; i < end; ++i) {
-        buf[count++] = (byte) ~b[i];
-      }
-    }
-  }
-
-  void escape(int b) {
-    switch (b) {
-      case 0:
-        escape00();
-        break;
-      case 1:
-        escape01();
-        break;
-      case 0xfe:
-        escapeFE();
-        break;
-      case 0xff:
-        escapeFF();
-        break;
-    }
-  }
-
-  public void write(int b) {
-    if (!shouldEscape(b, escapeLevel > 0, comescLevel > 0)) {
-      ensureAvailable(1);
-      if (complement) {
-        buf[count++] = (byte) ~b;
-      } else {
-        buf[count++] = (byte) b;
-      }
-    } else {
-      escape(b);
-    }
-  }
-
-  public void write(byte b[]) {
-    write(b, 0, b.length);
-  }
-
-  static boolean shouldEscape(int b, boolean checkLow, boolean checkHigh) {
-    if (checkLow && b < 0x2) return true;
-    if (checkHigh && b > 0xfd) return true;
-    return false;
-  }
-
-  public void write(byte b[], int off, int len) {
-    if ((escapeLevel > 0) || (comescLevel > 0)) {
-      ensureAvailable(len);
-      int begin = off;
-      int next = begin;
-      int end = off + len;
-      for (; begin < end; begin = next) {
-        while ((next < end)
-            && (!shouldEscape(b[next] & 0xff, escapeLevel > 0, comescLevel > 0))) {
-          ++next;
-        }
-        complement(b, begin, next);
-        if (next < end) {
-          escape(b[next] & 0xff);
-          ++next;
-        }
-      }
-    } else {
-      complement(b, off, off + len);
-    }
-  }
-}
-
-/**
- * Generating binary keys for algorithmic comparators. A user may construct an
- * algorithmic comparator by creating a ComparatorExpr object (through various
- * static methods in this class). She could then create a KeyGenerator object
- * and use it to create binary keys for tuple. The KeyGenerator object can be
- * reused for different tuples that conform to the same schema. Sorting the
- * tuples by the binary key yields the same ordering as sorting by the
- * algorithmic comparator.
- * 
- * Basic idea (without optimization):
- * <ul>
- * <li>define two operations: escape and complement, that takes in a byte array,
- * and outputs a byte array:
- * 
- * <pre>
- * escape(byte[] bytes) {
- *   for (byte b : bytes) {
- *     if (b == 0)
- *       emit(0x1, 0x0);
- *     else if (b == 1)
- *       emit(0x1, 0x2);
- *     else emit(b);
- *   }
- * }
- * 
- * complement(byte[] bytes) {
- *   for (byte b : bytes) {
- *     emit(&tilde;b);
- *   }
- * }
- * </pre>
- * 
- * <li>find ways to convert primitive types to bytes that compares in the same
- * order as those objects.
- * <li>operations:
- * <ul>
- * <li>negate(byte[] bytes) == complement(escape(bytes) + 0x0);
- * <li>tuple(byte[] bytes1, byte[] bytes2) == escape(bytes1) + 0x0 +
- * escape(bytes2)
- * <li>bag(byte[] bytes1, byte[] bytes2, ... ) = escape(bytes1) + 0x0 +
- * escape(bytes2) + ...
- * </ul>
- * <li>optimizations:
- * <ul>
- * <li>negate(negate(bytes)) == bytes;
- * <li>tuple(a) == a;
- * <li>tuple(a, tuple(b, c)) == tuple(a, b, c)
- * <li>the actual output would be a concatenation of f1(o1), f2(o2), ..., where
- * o1, o2, are leaf datums in the tuple or 0x0, and fi(oi) is a nested function
- * of escape() and complement() calls.
- * <li>The invariance we want to preserve is that escape(0x1) >
- * escape(escape(0x0)) > escape(0x0) > 0x0. In the basic algorithm, these are
- * escaped as 0x0102, 0x010100, 0x0100, 0x00, and are thus variable length. We
- * can actually collapse nested consecutive calls of
- * escape(escape(...escape(0))...) to escape(i, 0), where i is the level of
- * nesting, and fi may be represented as nested inter-leaved calling of
- * complement(bytes) and escape(i, bytes), where escape (i, 0x0) == 0x01 +
- * (0x01+i) and escape(i, 1) == 0x010xFD. We do limit the total nesting depth by
- * 252, which should be plenty.
- * <li>we can further optimize fi as either escape(i, j, bytes) or
- * complement(escape(i, j, bytes), where i is the level of nesting for escaping
- * 0x0 and 0x1, and j the level of nesting for escaping 0xff and 0xfe.
- * <li>If the binary keys being generated from a certain comparator either
- * compare equal or differ at some byte position, but never the case where one
- * is a prefix of another, then we do not need to add padding 0x0 for negate()
- * or tuple(). This is captured by the method implicitBound() in ComparatorExpr.
- * <li>We figure out how datums should be extracted from a tuple and being
- * escaped only once for any expression, and write to a modified
- * ByteArrayOutputStream in one pass.
- * </ul>
- * </ul>
- * 
- * TODO Remove the strong dependency with Pig by adding a DatumExtractor
- * interface that allow applications to extract leaf datum from user objects,
- * something like the following:
- * 
- * <pre>
- * interface DatumExtractor {
- *   Object extract(Object o);
- * }
- * </pre>
- * 
- * And user may do something like this:
- * 
- * <pre>
- * class MyObject {
- *  int a;
- *  String b;
- * }
- * 
- * ComparatorExpr expr = KeyBuilder.createLeafExpr(new DatumExtractor {
- *  Object extract(Object o) {
- *      MyObject obj = (MyObject)o;
- *      return obj.b;
- *  } }, DataType.CHARARRAY);
- * </pre>
- * 
- * TODO Change BagExpr to IteratorExpr, so that it may be used in more general
- * context (any Java collection).
- * 
- * TODO Add an ArrayExpr (for Java []).
- */
-public class KeyGenerator {
-  private EncodingOutputStream out;
-  private List<LeafGenerator> list;
-
-  /**
-   * Create a key builder that can generate binary keys for the input key
-   * expression.
-   * 
-   * @param expr
-   *          comparator expression
-   */
-  public KeyGenerator(ComparatorExpr expr) {
-    out = new EncodingOutputStream();
-    list = new ArrayList<LeafGenerator>();
-    expr.appendLeafGenerator(list, 0, 0, false, false);
-    // illustrate(System.out);
-  }
-
-  /**
-   * Reset the key builder for a new expression.
-   * 
-   * @param expr
-   *          comparator expression
-   */
-  public void reset(ComparatorExpr expr) {
-    list.clear();
-    expr.appendLeafGenerator(list, 0, 0, false, false);
-  }
-
-  /**
-   * Generate the binary key for the input tuple
-   * 
-   * @param t
-   *          input tuple
-   * @return A {@link BytesWritable} containing the binary sorting key for the
-   *         input tuple.
-   * @throws ExecException
-   */
-  public BytesWritable generateKey(Tuple t) throws ExecException {
-    out.reset();
-    for (Iterator<LeafGenerator> it = list.iterator(); it.hasNext();) {
-      LeafGenerator e = it.next();
-      e.append(out, t);
-    }
-    BytesWritable ret = new BytesWritable();
-    ret.set(out.get(), 0, out.size());
-    return ret;
-  }
-
-  /**
-   * Illustrate how the key would be generated from source.
-   * 
-   * @param ps
-   *          The output print stream.
-   */
-  public void illustrate(PrintStream ps) {
-    for (Iterator<LeafGenerator> it = list.iterator(); it.hasNext();) {
-      LeafGenerator e = it.next();
-      e.illustrate(ps);
-    }
-    ps.print("\n");
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/LeafExpr.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/LeafExpr.java
deleted file mode 100644
index fe0fffc21..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/LeafExpr.java
+++ /dev/null
@@ -1,93 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.pig.comparator;
-
-import java.io.PrintStream;
-import java.util.List;
-
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.Tuple;
-
-/**
- * Base class of comparator expressions that are the leaves of the expression
- * tree.
- */
-public abstract class LeafExpr extends ComparatorExpr {
-  int index;
-
-  /**
-   * Constructor
-   * 
-   * @param index
-   *          tuple position index
-   */
-  protected LeafExpr(int index) {
-    this.index = index;
-  }
-
-  /**
-   * Default illustrator for leaf expressions.
-   * 
-   * @param out
-   * @param escapeLevel
-   * @param comescLevel
-   * @param complement
-   */
-  protected void illustrate(PrintStream out, int escapeLevel, int comescLevel,
-      boolean complement) {
-    out.printf("(%s, %d, %d, %d, %b)", getType(), index, escapeLevel,
-        comescLevel, complement);
-  }
-
-  protected void append(EncodingOutputStream out, Tuple tuple)
-      throws ExecException {
-	  
-		if(index >= tuple.size())
-			return;
-	    Object o = tuple.get(index);
-	    if (o == null) {    
-	      out.write(0x0);
-	      return;
-	    }
-	    appendObject(out, o);
-  }
-
-  protected abstract void appendObject(EncodingOutputStream out, Object object)
-      throws ExecException;
-
-  protected abstract String getType();
-
-  protected abstract boolean implicitBound();
-
-  protected void appendLeafGenerator(List<LeafGenerator> list, int el, int cel,
-      boolean c, boolean explicitBound) {
-    if (explicitBound && implicitBound()) {
-      // requiring explicit bound, while the leaf does not already have it, so
-      // we do escaping and add a bound at end.
-      list.add(new LeafGenerator(this, el + 1, cel, c));
-      list.add(new LeafGenerator(null, el, cel, c));
-    } else {
-      list.add(new LeafGenerator(this, el, cel, c));
-    }
-  }
-
-  @Override
-  protected
-  void toString(PrintStream out) {
-    out.printf("%s(%d)", getType(), index);
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/LeafGenerator.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/LeafGenerator.java
deleted file mode 100644
index cbf9f3fe1..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/LeafGenerator.java
+++ /dev/null
@@ -1,67 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.pig.comparator;
-
-import java.io.PrintStream;
-
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.Tuple;
-
-/**
- * Generate partial binary key for a leaf expression.
- */
-public final class LeafGenerator {
-  final LeafExpr leaf; // may be null to represent adding '0'
-  final int escapeLevel;
-  final int comescLevel;
-  final boolean complement;
-
-  /**
-   * @param leaf
-   *          The leaf expression
-   * @param el
-   *          escape level
-   * @param cel
-   *          complement escape level
-   * @param complement
-   *          complement the value
-   */
-  public LeafGenerator(LeafExpr leaf, int el, int cel, boolean complement) {
-    this.leaf = leaf;
-    this.escapeLevel = el;
-    this.comescLevel = cel;
-    this.complement = complement;
-  }
-
-  void append(EncodingOutputStream out, Tuple tuple) throws ExecException {
-    out.setEscapeParams(escapeLevel, comescLevel, complement);
-    if (leaf == null) { // add a '\0'
-      out.write(0x0);
-    } else {
-      leaf.append(out, tuple);
-    }
-  }
-
-  void illustrate(PrintStream out) {
-    if (leaf == null) {
-      out.printf("(%s, NA, %d, %d, %b)", "NULL", escapeLevel, comescLevel,
-          complement);
-    } else {
-      leaf.illustrate(out, escapeLevel, comescLevel, complement);
-    }
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/LongExpr.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/LongExpr.java
deleted file mode 100644
index 1dcab6459..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/LongExpr.java
+++ /dev/null
@@ -1,45 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.pig.comparator;
-
-
-public final class LongExpr extends FixedLengthPrimitive {
-  public LongExpr(int index) {
-    super(index, Long.SIZE / Byte.SIZE);
-  }
-
-  protected void convertValue(byte[] b, Object o) {
-    if (b.length != Long.SIZE / Byte.SIZE) {
-      throw new IllegalArgumentException("Incorrect buffer size");
-    }
-    long l = (Long) o;
-    l ^= 1L << (Long.SIZE - 1);
-    b[7] = (byte) l;
-    b[6] = (byte) (l >> 8);
-    b[5] = (byte) (l >> 16);
-    b[4] = (byte) (l >> 24);
-    b[3] = (byte) (l >> 32);
-    b[2] = (byte) (l >> 40);
-    b[1] = (byte) (l >> 48);
-    b[0] = (byte) (l >> 56);
-  }
-
-  @Override
-  protected String getType() {
-    return "Long";
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/NegateExpr.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/NegateExpr.java
deleted file mode 100644
index c784c20cb..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/NegateExpr.java
+++ /dev/null
@@ -1,68 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.pig.comparator;
-
-import java.io.PrintStream;
-import java.util.List;
-
-/**
- * Negate expression
- */
-public class NegateExpr extends ComparatorExpr {
-  protected final ComparatorExpr expr;
-
-  /**
-   * constructor
-   * 
-   * @param expr
-   */
-  protected NegateExpr(ComparatorExpr expr) {
-    this.expr = expr;
-  }
-
-  @Override
-  protected void appendLeafGenerator(List<LeafGenerator> list, int el, int cel,
-      boolean c, boolean explicitBound) {
-    expr.appendLeafGenerator(list, cel, el, !c, true);
-  }
-
-  /**
-   * @return the child expression
-   */
-  public ComparatorExpr childExpr() {
-    return expr;
-  }
-
-  @Override
-  protected void toString(PrintStream out) {
-    out.print("Negate(");
-    expr.toString(out);
-    out.print(")");
-  }
-  
-  /**
-   * Make a negate expression
-   */
-  public static ComparatorExpr makeNegateExpr(ComparatorExpr expr) {
-    if (expr instanceof NegateExpr) {
-      NegateExpr negateExpr = (NegateExpr) expr;
-      return negateExpr.childExpr();
-    }
-
-    return new NegateExpr(expr);
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/ShortExpr.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/ShortExpr.java
deleted file mode 100644
index b64211132..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/ShortExpr.java
+++ /dev/null
@@ -1,39 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.pig.comparator;
-
-
-public final class ShortExpr extends FixedLengthPrimitive {
-  public ShortExpr(int index) {
-    super(index, Short.SIZE / Byte.SIZE);
-  }
-
-  protected void convertValue(byte[] b, Object o) {
-    if (b.length != Short.SIZE / Byte.SIZE) {
-      throw new IllegalArgumentException("Incorrect buffer size");
-    }
-    short l = (Short) o;
-    l ^= 1 << (Short.SIZE - 1);
-    b[1] = (byte) l;
-    b[0] = (byte) (l >> 8);
-  }
-
-  @Override
-  protected String getType() {
-    return "Short";
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/StringExpr.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/StringExpr.java
deleted file mode 100644
index d2bfbfbf3..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/StringExpr.java
+++ /dev/null
@@ -1,49 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.pig.comparator;
-
-import java.io.IOException;
-import java.nio.ByteBuffer;
-
-import org.apache.hadoop.io.Text;
-
-public final class StringExpr extends LeafExpr {
-  public StringExpr(int index) {
-    super(index);
-  }
-
-  @Override
-  protected void appendObject(EncodingOutputStream out, Object o) {
-    String s = (String) o;
-    try {
-      ByteBuffer bytes = Text.encode(s);
-      out.write(bytes.array(), 0, bytes.limit());
-    } catch (IOException e) {
-      throw new RuntimeException("Conversion error", e);
-    }
-  }
-
-  @Override
-  protected boolean implicitBound() {
-    return true;
-  }
-
-  @Override
-  protected String getType() {
-    return "String";
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/TupleExpr.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/TupleExpr.java
deleted file mode 100644
index e5511800c..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/TupleExpr.java
+++ /dev/null
@@ -1,111 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.pig.comparator;
-
-import java.io.PrintStream;
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Iterator;
-import java.util.List;
-
-public class TupleExpr extends ComparatorExpr {
-  protected final List<ComparatorExpr> exprs;
-
-  protected TupleExpr(List<ComparatorExpr> exprs) {
-    this.exprs = exprs;
-  }
-
-  @Override
-  protected void appendLeafGenerator(List<LeafGenerator> list, int el, int cel,
-      boolean c, boolean explicitBound) {
-    int length = exprs.size();
-    for (int i = 0; i < length - 1; ++i) {
-      ComparatorExpr e = exprs.get(i);
-      e.appendLeafGenerator(list, el, cel, c, true);
-    }
-    exprs.get(length - 1).appendLeafGenerator(list, el, cel, c, explicitBound);
-  }
-
-  /**
-   * Get the children expressions.
-   * 
-   * @return The children expressions.
-   */
-  public List<ComparatorExpr> childrenExpr() {
-    return exprs;
-  }
-
-  @Override
-  protected void toString(PrintStream out) {
-    out.print("Tuple");
-    String sep = "(";
-    for (Iterator<ComparatorExpr> it = exprs.iterator(); it.hasNext();) {
-      out.printf(sep);
-      it.next().toString(out);
-      sep = ", ";
-    }
-
-    out.print(")");
-  }
-  
-  /**
-   * Make a tuple expression
-   * 
-   * @param exprs
-   *          children expressions
-   * @return a comparator expression
-   */
-  public static ComparatorExpr makeTupleExpr(ComparatorExpr... exprs) {
-    if (exprs.length == 0)
-      throw new IllegalArgumentException("Zero-length expression list");
-    if (exprs.length == 1) return exprs[0];
-    List<ComparatorExpr> aggr = new ArrayList<ComparatorExpr>(exprs.length);
-    for (ComparatorExpr e : exprs) {
-      if (e instanceof TupleExpr) {
-        aggr.addAll(((TupleExpr) e).childrenExpr());
-      } else {
-        aggr.add(e);
-      }
-    }
-    return new TupleExpr(aggr);
-  }
-  
-  /**
-   * Make a tuple expression
-   * 
-   * @param exprs
-   *          children expressions
-   * @return a comparator expression
-   */
-  public static ComparatorExpr makeTupleComparator(
-      Collection<? extends ComparatorExpr> exprs) {
-    if (exprs.size() == 0)
-      throw new IllegalArgumentException("Zero-length expression list");
-    if (exprs.size() == 1) return exprs.iterator().next();
-    List<ComparatorExpr> aggr = new ArrayList<ComparatorExpr>(exprs.size());
-    for (Iterator<? extends ComparatorExpr> it = exprs.iterator(); it.hasNext();) {
-      ComparatorExpr e = it.next();
-      if (e instanceof TupleExpr) {
-        aggr.addAll(((TupleExpr) e).childrenExpr());
-      } else {
-        aggr.add(e);
-      }
-    }
-    return new TupleExpr(aggr);
-  }
-
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/package-info.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/package-info.java
deleted file mode 100644
index 14277b3fe..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/comparator/package-info.java
+++ /dev/null
@@ -1,22 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-/**
- * Utilities to allow PIG Storer to generate keys for sorted Zebra tables
- */
-package org.apache.hadoop.zebra.pig.comparator;
-
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/package-info.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/package-info.java
deleted file mode 100644
index 5b979a44e..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/pig/package-info.java
+++ /dev/null
@@ -1,22 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-/**
- * Implementation of PIG Storer/Loader Interfaces
- */
-package org.apache.hadoop.zebra.pig;
-
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/schema/ColumnType.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/schema/ColumnType.java
deleted file mode 100644
index baf332d1d..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/schema/ColumnType.java
+++ /dev/null
@@ -1,202 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.schema;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.zebra.tfile.Utils;
-import org.apache.pig.data.DataType;
-
-/**
- * Zebra Column Type
- */
-public enum ColumnType {
-  /**
-   * Any type
-   */
-  ANY("any") {
-	  public byte pigDataType() {
-		  return DataType.BYTEARRAY;
-	  }
-  },
-  /**
-   * Integer
-   */
-  INT("int") {
-    public byte pigDataType() {
-      return DataType.INTEGER;
-    }
-  },
-  /**
-   * Long
-   */
-  LONG("long") {
-    public byte pigDataType() {
-      return DataType.LONG;
-    }
-  },
-  /**
-   * Float
-   */
-  FLOAT("float") {
-    public byte pigDataType() {
-      return DataType.FLOAT;
-    }
-  },
-  /**
-   * Double
-   */
-  DOUBLE("double") {
-    public byte pigDataType() {
-      return DataType.DOUBLE;
-    }
-  },
-  /**
-   * Boolean
-   */
-  BOOL("bool") {
-    public byte pigDataType() {
-      return DataType.BOOLEAN;
-    }
-  },
-  /**
-   * DateTime
-   */
-  DATETIME("datetime") {
-    public byte pigDataType() {
-      return DataType.DATETIME;
-    }
-  },
-  /**
-   * Collection
-   */
-  COLLECTION("collection") {
-    public byte pigDataType() {
-      return DataType.BAG;
-    }
-  },
-  /**
-   * Map
-   */
-  MAP("map") {
-    public byte pigDataType() {
-      return DataType.MAP;
-    }
-  },
-  /**
-   * Record
-   */
-  RECORD("record") {
-    public byte pigDataType() {
-      return DataType.TUPLE;
-    }
-  },
-  /**
-   * String
-   */
-  STRING("string") {
-    public byte pigDataType() {
-      return DataType.CHARARRAY;
-    }
-  },
-  /**
-   * Bytes
-   */
-  BYTES("bytes") {
-    public byte pigDataType() {
-      return DataType.BYTEARRAY;
-    }
-  };
-
-  private String name;
-
-  private ColumnType(String name) {
-    this.name = name;
-  }
-
-  public abstract byte pigDataType();
-
-  /**
-   * To get the type based on the type name string.
-   * 
-   * @param name name of the type
-   * 
-   * @return ColumnType Enum for the type
-   */
-  public static ColumnType getTypeByName(String name) {
-    return ColumnType.valueOf(name.toUpperCase());
-  }
-
-  /**
-   * Get the Zebra type from a Pig type
-   *
-   * @param dt Pig type
-   *
-   * @return Zebra type
-   */
-  public static ColumnType getTypeByPigDataType(byte dt) {
-    for (ColumnType ct : ColumnType.values()) {
-      if (ct.pigDataType() == dt) {
-        return ct;
-      }
-    }
-    return null;
-  }
-
-  /**
-   * Get the Zebra type name for the passed in Zebra column type
-   *
-   * @param columntype Zebra column type
-   *
-   * @return string representation of the Zebra column type
-   */
-  public static String findTypeName(ColumnType columntype) {
-	  return columntype.getName();
-  }
-
-  /**
-   * Get the Zebra type name for this Zebra column type
-   *
-   * @return string representation of the Zebra column type
-   */
-  public String getName() {
-    return name;
-  }
-
-  /**
-   * Same as getType
-   */
-  public String toString() {
-    return name;
-  }
-
-  /**
-   * check if a column type contains internal schema
-   *
-   * @param columnType Zebra column type
-   *
-   * @return true if the type is RECORD, MAP or COLLECTION
-   */
-  public static boolean isSchemaType(ColumnType columnType) {
-	  return ((columnType == RECORD) || (columnType == MAP) || (columnType == COLLECTION));
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/schema/Schema.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/schema/Schema.java
deleted file mode 100644
index 509fce7ff..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/schema/Schema.java
+++ /dev/null
@@ -1,1041 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.schema;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.io.StringReader;
-import java.util.HashSet;
-
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.zebra.types.Projection;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.parser.TableSchemaParser;
-
-/**
- * Logical schema of tabular data.
- */
-public class Schema implements Comparable<Schema>, Writable {
-  public final static String COLUMN_DELIMITER = ",";
-
-  private static final long schemaVersion = 1L;
-
-  /**
-   * Column Schema in Schema
-   */
-  public static class ColumnSchema {
-    private String name;
-    private ColumnType type;
-    private Schema schema;
-    private int index; // field index in schema
-
-    /**
-     * construct a ColumnSchema for a native type
-     * 
-     * @param a
-     *          column name
-     * @param t
-     *          native column type
-     */
-    public ColumnSchema(String a, ColumnType t) {
-      name = a;
-      type = t;
-      schema = null;
-    }
-
-    /**
-     * construct a Column schema for a RECORD column type
-     * 
-     * @param a
-     *          column name
-     * @param s
-     *          column schema
-     */
-    public ColumnSchema(String a, Schema s) {
-      name = a;
-      type = ColumnType.RECORD;
-      schema = s;
-    }
-
-    /**
-     * access function to get the column name 
-     *
-     * @return name of the column
-     */
-    public String getName() {
-      return name;
-    }
-    
-    /**
-     * access function to get the column type 
-     *
-     * @return column type
-     */
-    public ColumnType getType() {
-      return type;
-    }
-    
-    /**
-     * access function to get the column name 
-     *
-     * @return column index in the parent schema
-     */
-    public int getIndex() {
-      return index;
-    }
-    
-    /**
-     * construct a column schema for a complex column type
-     * 
-     * @param a
-     *          column name
-     * @param s
-     *          column schema
-     * @param t
-     *          complex column type
-     * @throws ParseException
-     */
-    public ColumnSchema(String a, Schema s, ColumnType t) throws ParseException {
-      if ((null != s) && !(ColumnType.isSchemaType(t))) {
-        throw new ParseException(
-            "Only a COLLECTION or RECORD or MAP can have schemas.");
-      }
-      name = a;
-      schema = s;
-      type = t;
-    }
-
-    /**
-     * copy ctor
-     * 
-     * @param cs
-     *          source column schema
-     */
-    public ColumnSchema(ColumnSchema cs) {
-      name = cs.name;
-      type = cs.type;
-      schema = cs.schema;
-    }
-
-    /**
-     * Compare two field schema for equality
-     * 
-     * @param fschema one column schema to be compared
-     * @param fother the other column schema to be compared
-     * @return true if ColumnSchema are equal, false otherwise
-     */
-    public static boolean equals(ColumnSchema fschema, ColumnSchema fother) {
-      if (fschema == null) {
-        return false;
-      }
-
-      if (fother == null) {
-        return false;
-      }
-
-      if (fschema.type != fother.type) {
-        return false;
-      }
-
-      if ((fschema.name == null) && (fother.name == null)) {
-        // good
-      }
-      else if ((fschema.name != null) && (fother.name == null)) {
-        return false;
-      }
-      else if ((fschema.name == null) && (fother.name != null)) {
-        return false;
-      }
-      else if (!fschema.name.equals(fother.name)) {
-        return false;
-      }
-
-      if (ColumnType.isSchemaType(fschema.type)) {
-        // Don't do the comparison if both embedded schemas are
-        // null. That will cause Schema.equals to return false,
-        // even though we want to view that as true.
-        if (!(fschema.schema == null && fother.schema == null)) {
-          // compare recursively using schema
-          if (!fschema.schema.equals(fother.schema)) {
-            return false;
-          }
-        }
-      }
-      return true;
-    }
-
-    /**
-     * string representation of the schema
-     */
-    @Override
-    public String toString() {
-      StringBuilder sb = new StringBuilder();
-      if (name != null) {
-        sb.append(name);
-      }
-      sb.append(ColumnType.findTypeName(type));
-
-      if (schema != null) {
-        sb.append("(");
-        sb.append(schema.toString());
-        sb.append(")");
-      }
-
-      return sb.toString();
-    }
-
-    /*
-     * Returns the schema for the next level structure, in record, collection
-     * and map.
-     *
-     * @return Schema of the column
-     */
-    public Schema getSchema() {
-      return schema;
-    }
-  }
-
-  /**
-   * Helper class to parse a column name string one section at a time and find
-   * the required type for the parsed part.
-   */
-  public static class ParsedName {
-    private String mName;
-    private int mKeyOffset; // the offset where the keysstring starts
-    private ColumnType mDT = ColumnType.ANY; // parent's type
-
-    /**
-     * Default ctor
-     */
-    public ParsedName() {
-    }
-
-    /**
-     * Set the name
-     *
-     * @param name
-     *            column name string
-     */
-    public void setName(String name) {
-      mName = name;
-    }
-
-    /**
-     * Set the name and type
-     *
-     * @param name
-     *            column name string
-     * @param pdt
-     *            column type
-     */
-    public void setName(String name, ColumnType pdt) {
-      mName = name;
-      mDT = pdt;
-    }
-
-    void setName(String name, ColumnType pdt, int keyStrOffset) {
-      this.setName(name, pdt);
-      mKeyOffset = keyStrOffset;
-    }
-
-    /**
-     * Set the column type
-     *
-     * @param dt
-     *          column type to be set with
-     */
-    public void setDT(ColumnType dt) {
-      mDT = dt;
-    }
-
-    /**
-     * Get the column type
-     * 
-     * @return column type
-     */
-    public ColumnType getDT() {
-      return mDT;
-    }
-
-    /**
-     * Get the column name
-     *
-     * @return column name
-     */
-    public String getName() {
-      return mName;
-    }
-
-    /**
-     * Parse one sector of a fully qualified column name; also checks validity
-     * of use of the MAP and RECORD delimiters
-     *
-     * @param fs
-     *          column schema this column name is checked against with          
-     */
-    public String parseName(Schema.ColumnSchema fs) throws ParseException {
-      int fieldIndex, hashIndex;
-      fieldIndex = mName.indexOf('.');
-      hashIndex = mName.indexOf('#');
-      String prefix;
-      if (hashIndex != -1 && fieldIndex == -1) {
-        if (fs.type != ColumnType.MAP)
-          throw new ParseException(mName + " : is not of type MAP");
-        prefix = mName.substring(0, hashIndex);
-        setName(mName.substring(hashIndex + 1), ColumnType.MAP);
-      }
-      else if (hashIndex == -1 && fieldIndex != -1) {
-        if (fs.type != ColumnType.RECORD)
-          throw new ParseException(mName + " : is not of type RECORD");
-        prefix = mName.substring(0, fieldIndex);
-        setName(mName.substring(fieldIndex + 1), ColumnType.RECORD);
-      }
-      else if (hashIndex != -1 && fieldIndex != -1) {
-        if (hashIndex < fieldIndex) {
-          if (fs.type != ColumnType.MAP)
-            throw new ParseException(mName + " : is not of type MAP");
-          prefix = mName.substring(0, hashIndex);
-          setName(mName.substring(hashIndex + 1), ColumnType.MAP);
-        }
-        else {
-          if (fs.type != ColumnType.RECORD)
-            throw new ParseException(mName + " : is not of type RECORD");
-          prefix = mName.substring(0, fieldIndex);
-          setName(mName.substring(fieldIndex + 1), ColumnType.RECORD);
-        }
-      }
-      else {
-        prefix = mName; // no hash or subfield contained
-        mDT = ColumnType.ANY;
-      }
-      return prefix;
-    }
-  }
-
-  private ArrayList<ColumnSchema> mFields;
-  private HashMap<String, ColumnSchema> mNames;
-  private boolean projection;
-
-  /**
-   * Constructor - schema for empty schema (zero-column) .
-   */
-  public Schema() {
-    init();
-  }
-
-  /**
-   * Constructor - schema for empty projection/schema (zero-column) .
-   *
-   * @param projection
-   *           A projection schema or not
-   */
-  public Schema(boolean projection) {
-    this.projection = projection;
-    init();
-  }
-
-  /**
-   * Constructor - create a schema from a string representation.
-   * 
-   * @param schema
-   *          A string representation of the schema. For this version, the
-   *          schema string is simply a comma separated list of column names. Of
-   *          course, comma (,) and space characters are illegal in column
-   *          names. To maintain forward compatibility, please use only
-   *          alpha-numeric characters in column names.
-   */
-  public Schema(String schema) throws ParseException {
-    init(schema, false);
-  }
-
-  public Schema(String schema, boolean projection) throws ParseException {
-    this.projection = projection;
-    init(schema, projection);
-  }
-
-  public Schema(ColumnSchema fs) throws ParseException {
-    init();
-    add(fs);
-  }
-
-  /**
-   * Constructor - create a schema from an array of column names.
-   * 
-   * @param columns
-   *          An array of column names. To maintain forward compatibility,
-   *          please use only alpha-numeric characters in column names.
-   */
-  public Schema(String[] columns) throws ParseException {
-    init(columns, false);
-  }
-
-  /**
-   * add a column
-   * 
-   * @param f
-   *          Column to be added to the schema
-   */
-  public void add(ColumnSchema f) throws ParseException {
-    if (f == null) {
-      if (!projection)
-        throw new ParseException("Empty column schema is not allowed");
-      mFields.add(null);
-      return;
-    }
-    f.index = mFields.size();
-    mFields.add(f);
-    if (null != f && null != f.name) {
-      if (mNames.put(f.name, f) != null && !projection)
-        throw new ParseException("Duplicate field name: " + f.name);
-    }
-  }
-
-  /**
-   * get a column by name
-   */
-  public ColumnSchema getColumn(String name) {
-    return mNames.get(name);
-  }
-
-  /**
-   * Get the names of the individual columns.
-   * 
-   * @return An array of the column names.
-   */
-  public String[] getColumns() {
-    String[] result = new String[mFields.size()];
-    for (int i = 0; i < mFields.size(); i++) {
-      ColumnSchema cs = mFields.get(i);
-      if (cs != null) {
-        result[i] = mFields.get(i).name;
-      }
-      else {
-        result[i] = null;
-      }
-    }
-    return result;
-  }
-
-  /**
-   * Get a particular column's schema
-   */
-  public ColumnSchema getColumn(int index) {
-    return mFields.get(index);
-  }
-
-  public String getColumnName(int index) {
-    if (mFields.get(index) == null) return null;
-    return mFields.get(index).name;
-  }
-
-  /**
-   * Get the names and types of the individual columns.
-   * 
-   * @return An array of the column names.
-   */
-  public String[] getTypedColumns() {
-    String[] result = new String[mFields.size()];
-    for (int i = 0; i < mFields.size(); i++)
-      result[i] = mFields.get(i).name + ":" + mFields.get(i).toString();
-    return result;
-  }
-
-  /**
-   * Get the index of the column for the input column name.
-   * 
-   * @param name
-   *          input column name.
-   * @return The column index if the name is valid; -1 otherwise.
-   */
-  public int getColumnIndex(String name) {
-    ColumnSchema fs;
-    if ((fs = mNames.get(name)) != null) return fs.index;
-    return -1;
-  }
-
-  /**
-   * Get the number of columns as defined in the schema.
-   * 
-   * @return The number of columns as defined in the schema.
-   */
-  public int getNumColumns() {
-    return mFields.size();
-  }
-
-  /**
-   * Parse a schema string and create a schema object.
-   * 
-   * @param schema
-   *          comma separated schema string.
-   * @return Schema object
-   */
-  public static Schema parse(String schema) throws ParseException {
-    Schema s = new Schema();
-    s.init(schema, false);
-    return s;
-  }
-
-  /**
-   * Convert the schema to a String.
-   * 
-   * @return the string representation of the schema.
-   */
-  public String toString() {
-    StringBuilder sb = new StringBuilder();
-    try {
-      stringifySchema(sb, this, ColumnType.COLLECTION, true);
-    }
-    catch (Exception fee) {
-      throw new RuntimeException("PROBLEM PRINTING SCHEMA");
-    }
-    return sb.toString();
-  }
-
-  /**
-   * This is used for building up output string type can only be COLLECTION or
-   * RECORD or MAP
-   */
-  private void stringifySchema(StringBuilder sb, Schema schema,
-      ColumnType type, boolean top) {
-    if (!top) {
-      if (type == ColumnType.RECORD) {
-        sb.append("(");
-      }
-      else if (type == ColumnType.COLLECTION) {
-        sb.append("(");
-      }
-      else if (type == ColumnType.MAP) {
-        sb.append("(");
-      }
-    }
-    boolean isFirst = true;
-    for (int i = 0; i < schema.getNumColumns(); i++) {
-
-      if (!isFirst) {
-        sb.append(",");
-      }
-      else {
-        isFirst = false;
-      }
-
-      ColumnSchema fs = schema.getColumn(i);
-
-      if (fs == null) {
-        continue;
-      }
-
-      if (fs.name != null && !fs.name.equals("")) {
-        sb.append(fs.name);
-        sb.append(":");
-      }
-
-      sb.append(ColumnType.findTypeName(fs.type));
-      if ((fs.type == ColumnType.RECORD) || (fs.type == ColumnType.MAP)
-          || (fs.type == ColumnType.COLLECTION)) {
-        // safety net
-        if (this != fs.schema) {
-          stringifySchema(sb, fs.schema, fs.type, false);
-        }
-        else {
-          throw new IllegalArgumentException("Schema refers to itself "
-              + "as inner schema");
-        }
-      }
-    }
-    if (!top) {
-      if (type == ColumnType.RECORD) {
-        sb.append(")");
-      }
-      else if (type == ColumnType.COLLECTION) {
-        sb.append(")");
-      }
-      else if (type == ColumnType.MAP) {
-        sb.append(")");
-      }
-    }
-  }
-
-  /**
-   * Normalize the schema string.
-   * 
-   * @param value
-   *          the input string representation of the schema.
-   * @return the normalized string representation.
-   */
-  public static String normalize(String value) {
-    String result = new String();
-
-    if (value == null || value.trim().isEmpty()) return result;
-
-    StringBuilder sb = new StringBuilder();
-    String[] parts = value.trim().split(COLUMN_DELIMITER);
-    for (int nx = 0; nx < parts.length; nx++) {
-      if (nx > 0) sb.append(COLUMN_DELIMITER);
-      sb.append(parts[nx].trim());
-    }
-    return sb.toString();
-  }
-
-  /**
-   * @see Comparable#compareTo(Object)
-   */
-  @Override
-  public int compareTo(Schema other) {
-    int mFieldsSize = this.mFields.size(); 
-    if (mFieldsSize != other.mFields.size()) {
-      return mFieldsSize - other.mFields.size();
-    }
-    int ret = 0;
-    for (int nx = 0; nx < mFieldsSize; nx++) {
-      Schema mFieldSchema = mFields.get(nx).schema;
-      Schema otherFieldSchema = other.mFields.get(nx).schema;
-      if (mFieldSchema == null
-          && otherFieldSchema != null) return -1;
-      else if (mFieldSchema != null
-          && otherFieldSchema == null) return 1;
-      else if (mFieldSchema == null
-          && otherFieldSchema == null) return 0;
-      ret = mFieldSchema.compareTo(otherFieldSchema);
-      if (ret != 0) {
-        return ret;
-      }
-    }
-    return 0;
-  }
-
-  /**
-   * @see Object#equals(Object)
-   */
-  @Override
-  public boolean equals(Object obj) {
-    if (this == obj) return true;
-    if (obj == null) return false;
-    if (getClass() != obj.getClass()) return false;
-    Schema other = (Schema) obj;
-
-    if (mFields.size() != other.mFields.size()) return false;
-
-    Iterator<ColumnSchema> i = mFields.iterator();
-    Iterator<ColumnSchema> j = other.mFields.iterator();
-
-    while (i.hasNext()) {
-
-      ColumnSchema myFs = i.next();
-      ColumnSchema otherFs = j.next();
-
-      if ((myFs.name == null) && (otherFs.name == null)) {
-        // good
-      }
-      else if ((myFs.name != null) && (otherFs.name == null)) {
-        return false;
-      }
-      else if ((myFs.name == null) && (otherFs.name != null)) {
-        return false;
-      }
-      else if (!myFs.name.equals(otherFs.name)) {
-        return false;
-      }
-
-      if (myFs.type != otherFs.type) {
-        return false;
-      }
-
-      // Compare recursively using field schema
-      if (!ColumnSchema.equals(myFs, otherFs)) {
-        return false;
-      }
-    }
-    return true;
-  }
-
-  /**
-   * @see Writable#readFields(DataInput)
-   */
-  @Override
-  public void readFields(DataInput in) throws IOException {
-    long version = org.apache.hadoop.zebra.tfile.Utils.readVLong(in);
-
-    if (version > schemaVersion)
-      throw new IOException("Schema version is newer than that in software.");
-
-    // check-ups are needed for future versions for backward-compatibility
-    String strSchema = org.apache.hadoop.zebra.tfile.Utils.readString(in);
-    try {
-      init(strSchema, false);
-    }
-    catch (Exception e) {
-      throw new IOException(e.getMessage());
-    }
-  }
-
-  /**
-   * @see Writable#write(DataOutput)
-   */
-  @Override
-  public void write(DataOutput out) throws IOException {
-    org.apache.hadoop.zebra.tfile.Utils.writeVLong(out, schemaVersion);
-    org.apache.hadoop.zebra.tfile.Utils.writeString(out, toString());
-  }
-
-  private void init(String[] columnNames, boolean projection) throws ParseException {
-    // the arg must be of type or they will be treated as the default type
-    mFields = new ArrayList<ColumnSchema>();
-    mNames = new HashMap<String, ColumnSchema>();
-    StringBuilder sb = new StringBuilder();
-    for (int i = 0; i < columnNames.length; i++) {
-      if (columnNames[i].contains(COLUMN_DELIMITER))
-        throw new ParseException("Column name should not contain "
-            + COLUMN_DELIMITER);
-      if (i > 0) sb.append(",");
-      sb.append(columnNames[i]);
-    }
-    TableSchemaParser parser =
-        new TableSchemaParser(new StringReader(sb.toString()));
-    if (projection)
-      parser.ProjectionSchema(this);
-    else
-      parser.RecordSchema(this);
-  }
-
-  private void init() {
-    mFields = new ArrayList<ColumnSchema>();
-    mNames = new HashMap<String, ColumnSchema>();
-  }
-
-  private void init(String columnString, boolean projection) throws ParseException {
-    String trimmedColumnStr;
-    if (columnString == null || (trimmedColumnStr = columnString.trim()).isEmpty()) {
-      init();
-      return;
-    }
-
-    String[] parts = trimmedColumnStr.split(COLUMN_DELIMITER);
-    for (int nx = 0; nx < parts.length; nx++) {
-      parts[nx] = parts[nx].trim();
-    }
-    init(parts, projection);
-  }
-
-  /**
-   * Get a projection's schema
-   */
-  public Schema getProjectionSchema(String[] projcols,
-      HashMap<Schema.ColumnSchema, HashSet<String>> keysmap)
-      throws ParseException {
-    int ncols = projcols.length;
-    Schema result = new Schema(true);
-    ColumnSchema cs, mycs;
-    String keysStr;
-    String[] keys;
-    ParsedName pn = new ParsedName();
-    HashSet<String> keyentries;
-    for (int i = 0; i < ncols; i++) {
-	    if (Projection.isVirtualColumn(projcols[i]))
-	    {
-	      mycs = 
-	    	  new ColumnSchema(
-	    		  Projection.source_table_vcolumn_name, null, ColumnType.INT
-	    	  );	
-	      result.add(mycs);
-	      continue;
-	    }
-      pn.setName(projcols[i]);
-      if ((cs = getColumnSchemaOnParsedName(pn)) != null) {
-        mycs = new ColumnSchema(pn.mName, cs.schema, cs.type);
-        result.add(mycs);
-        if (pn.mDT == ColumnType.MAP) {
-          keysStr = projcols[i].substring(pn.mKeyOffset);
-          if (!keysStr.startsWith("{") || !keysStr.endsWith("}"))
-            throw new ParseException("Invalid map key specification in "
-                + projcols[i]);
-          keysStr = keysStr.substring(1, keysStr.length() - 1);
-          if ((keysStr.indexOf('{') != -1) || (keysStr.indexOf('}') != -1)
-              || (keysStr.indexOf('.') != -1) || (keysStr.indexOf('#') != -1))
-            throw new ParseException("Invalid map key specification in "
-                + projcols[i]);
-          keys = keysStr.split("\\|");
-          if ((keyentries = keysmap.get(mycs)) == null) {
-            keyentries = new HashSet<String>();
-            keysmap.put(mycs, keyentries);
-          }
-          for (int j = 0; j < keys.length; j++) {
-            keyentries.add(keys[j]);
-          }
-        }
-      } else { 
-    	  result.add( new ColumnSchema(pn.mName, null, ColumnType.ANY ) );
-      }
-    }
-    return result;
-  }
-
-  /**
-   * Get a column's schema
-   *
-   * @param name
-   *          column name
-   * @return Column schema for the named column
-   */
-  public ColumnSchema getColumnSchema(String name) throws ParseException {
-    int hashIndex, fieldIndex;
-    String currentName = name, prefix;
-    Schema currentSchema = this;
-    ColumnSchema fs = null;
-
-    while (true) {
-      /**
-       * this loop is necessary because the top columns in schema may contain .s
-       * and #s, particularly for column group schemas
-       */
-      fieldIndex = currentName.lastIndexOf('.');
-      hashIndex = currentName.lastIndexOf('#');
-      if (fieldIndex == -1 && hashIndex == -1) {
-        break;
-      }
-      else if (hashIndex != -1 && (fieldIndex == -1 || hashIndex > fieldIndex)) {
-        currentName = currentName.substring(0, hashIndex);
-        if (getColumn(currentName) != null) break;
-      }
-      else {
-        currentName = currentName.substring(0, fieldIndex);
-        if (getColumn(currentName) != null) break;
-      }
-    }
-
-    currentName = name;
-    while (true) {
-      if (fieldIndex == -1 && hashIndex == -1) {
-        fs = currentSchema.getColumn(currentName);
-        return fs;
-      }
-      else if (hashIndex != -1 && (fieldIndex == -1 || hashIndex < fieldIndex)) {
-        prefix = currentName.substring(0, hashIndex);
-        fs = currentSchema.getColumn(prefix);
-        if (fs == null)
-          throw new ParseException("Column " + name + " does not exist");
-        currentSchema = fs.schema;
-        if (fs.type != ColumnType.MAP)
-          throw new ParseException(name + " is not of type MAP");
-        fs = currentSchema.getColumn(0);
-        return fs;
-      }
-      else {
-        prefix = currentName.substring(0, fieldIndex);
-        if ((fs = currentSchema.getColumn(prefix)) == null)
-          throw new ParseException("Column " + name + " does not exist");
-        currentSchema = fs.schema;
-        if (fs.type != ColumnType.RECORD && fs.type != ColumnType.COLLECTION)
-          throw new ParseException(name
-              + " is not of type RECORD or COLLECTION");
-        currentName = currentName.substring(fieldIndex + 1);
-        if (currentName.length() == 0)
-          throw new ParseException("Column " + name
-              + " does not have field after the record field separator '.'");
-      }
-      fieldIndex = currentName.indexOf('.');
-      hashIndex = currentName.indexOf('#');
-    }
-  }
-
-  /**
-   * Get a subcolumn's schema and move the name just parsed into the next subtype
-   *
-   * @param pn
-   *           The name of subcolumn to be parsed. On return it contains the
-   *           subcolumn at the next level after parsing
-   *
-   * @return   the discovered Column Schema for the subcolumn
-   */
-  public ColumnSchema getColumnSchemaOnParsedName(ParsedName pn)
-      throws ParseException {
-    int hashIndex, fieldIndex, offset = 0;
-    String name = pn.mName;
-
-    /**
-     * strip of any possible type specs
-     */
-    String currentName = name, prefix;
-    Schema currentSchema = this;
-    ColumnSchema fs = null;
-
-    while (true) {
-      /**
-       * this loop is necessary because the top columns in schema may contain .s
-       * and #s, particularly for column group schemas
-       */
-      fieldIndex = currentName.lastIndexOf('.');
-      hashIndex = currentName.lastIndexOf('#');
-      if (fieldIndex == -1 && hashIndex == -1) {
-        break;
-      }
-      else if (hashIndex != -1 && (fieldIndex == -1 || hashIndex > fieldIndex)) {
-        currentName = currentName.substring(0, hashIndex);
-        if ((fs = getColumn(currentName)) != null) {
-          if (fs.type != ColumnType.MAP)
-            throw new ParseException(name + " is not of type MAP");
-          offset += hashIndex;
-          pn.setName(name.substring(0, hashIndex), ColumnType.MAP,
-              hashIndex + 1);
-          return fs;
-        }
-      }
-      else {
-        currentName = currentName.substring(0, fieldIndex);
-        if (getColumn(currentName) != null) break;
-      }
-    }
-
-    currentName = name;
-    ColumnType ct = ColumnType.ANY;
-
-    while (true) {
-      if (fieldIndex == -1 && hashIndex == -1) {
-        offset += currentName.length();
-        pn.setName(name.substring(0, offset), ct);
-        fs = currentSchema.getColumn(currentName);
-        return fs;
-      }
-      else if (hashIndex != -1 && (fieldIndex == -1 || hashIndex < fieldIndex)) {
-        prefix = currentName.substring(0, hashIndex);
-        fs = currentSchema.getColumn(prefix);
-        if (fs == null)
-          throw new ParseException("Column " + name + " does not exist");
-        currentSchema = fs.schema;
-        if (fs.type != ColumnType.MAP)
-          throw new ParseException(name + " is not of type MAP");
-        offset += hashIndex;
-        pn.setName(name.substring(0, offset), ColumnType.MAP, offset + 1);
-        return fs;
-      }
-      else {
-        prefix = currentName.substring(0, fieldIndex);
-        if ((fs = currentSchema.getColumn(prefix)) == null)
-          throw new ParseException("Column " + name + " does not exist");
-        currentSchema = fs.schema;
-        if (fs.type != ColumnType.RECORD && fs.type != ColumnType.COLLECTION)
-          throw new ParseException(name
-              + " is not of type RECORD or COLLECTION");
-        else if( fs.type == ColumnType.COLLECTION ) 
-		  throw new ParseException( name + "Projection within COLLECTION is not supported" );
-        currentName = currentName.substring(fieldIndex + 1);
-        if (currentName.length() == 0)
-          throw new ParseException("Column " + name
-              + " does not have field after the record field separator '.'");
-        offset += fieldIndex + 1;
-        ct = fs.type;
-      }
-      fieldIndex = currentName.indexOf('.');
-      hashIndex = currentName.indexOf('#');
-    }
-  }
-
-  /**
-   * find the most fitting subcolumn containing the name: the parsed name is set
-   * after the field name plus any possible separator of '.' or '#'.
-   *
-   * This is used to help discover the most fitting column schema in multiple
-   * CG schemas.
-   *
-   * For instance, if pn contains a name of r.r1.f11 and current schema has
-   * r.r1:record(f11:int, f12), it will return f11's column schema, and pn
-   * is set at "f12".
-   *
-   * @param pn
-   *           The name of subcolumn to be parsed. On return it contains the
-   *           subcolumn at the next level after parsing
-   *
-   * @return   the discovered Column Schema for the subcolumn
-   */
-  public ColumnSchema getColumnSchema(ParsedName pn) throws ParseException {
-    int maxlen = 0, size = getNumColumns(), len;
-    Schema.ColumnSchema fs = null;
-    String name = pn.getName(), fname;
-    boolean whole = false, record = false, map = false, tmprecord = false, tmpmap =
-        false;
-    for (int i = 0; i < size; i++) {
-      fname = getColumnName(i);
-      if ((whole = name.equals(fname))
-          || (tmprecord = name.startsWith(fname + "."))
-          || (tmpmap = name.startsWith(fname + "#"))) {
-        len = fname.length();
-        if (len > maxlen) {
-          maxlen = len;
-          record = tmprecord;
-          map = tmpmap;
-          fs = getColumn(i);
-          if (whole) break;
-        }
-      }
-    }
-    if (fs != null) {
-      name = name.substring(maxlen);
-      if (record) {
-        if (fs.type != ColumnType.RECORD && fs.type != ColumnType.COLLECTION)
-          throw new ParseException(name
-              + " is not of type RECORD or COLLECTION");
-        name = name.substring(1);
-        pn.setName(name, fs.type);
-      }
-      else if (map) {
-        if (fs.type != ColumnType.MAP)
-          throw new ParseException(name + " is not of type MAP");
-        name = name.substring(1);
-        pn.setName(name, ColumnType.MAP);
-      }
-      else pn.setName(name, ColumnType.ANY);
-      return fs;
-    }
-    else return null;
-  }
-
-  /**
-   * union compatible schemas. Exception will be thrown if a name appears in
-   * multiple schemas but the types are different.
-   */
-  public void unionSchema(Schema other) throws ParseException {
-    int size = other.getNumColumns();
-    ColumnSchema fs, otherfs;
-    for (int i = 0; i < size; i++) {
-      otherfs = other.getColumn(i);
-      if (otherfs == null) continue;
-      fs = getColumn(otherfs.name);
-      if (fs == null) add(otherfs);
-      else {
-        if (!ColumnSchema.equals(fs, otherfs))
-          throw new ParseException("Different types of column " + fs.name
-              + " in tables of a union");
-      }
-    }
-  }
-
-  /**
-   * return untyped schema string for projection
-   */
-  public String toProjectionString() {
-    String result = new String();
-    ColumnSchema fs;
-    for (int i = 0; i < mFields.size(); i++) {
-      if (i > 0) result += ",";
-      if ((fs = mFields.get(i)) != null) result += fs.name;
-    }
-    return result;
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/schema/SchemaParser.jjt b/contrib/zebra/src/java/org/apache/hadoop/zebra/schema/SchemaParser.jjt
deleted file mode 100644
index ca4e9b4c2..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/schema/SchemaParser.jjt
+++ /dev/null
@@ -1,360 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-options {
-      STATIC = false ;
-		IGNORE_CASE = true;
-}
-
-PARSER_BEGIN(TableSchemaParser)
-package org.apache.hadoop.zebra.parser;
-import java.io.*;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.schema.ColumnType;
-import org.apache.hadoop.zebra.types.Projection;
-
-public class TableSchemaParser {
-           public static void main( String[] args )
-           throws ParseException, TokenMgrError , java.io.FileNotFoundException, java.io.IOException {
-                TableSchemaParser parser = new TableSchemaParser( System.in ) ;
-					Schema schema = parser.RecordSchema(null);
-					System.out.println("OK");
-					FileOutputStream output = new FileOutputStream("/tmp/schema");
-					ObjectOutputStream objout = new ObjectOutputStream(output);
-					objout.writeObject(schema);
-					objout.close();
-					System.out.println(schema.toString());
-      }
-}
-PARSER_END(TableSchemaParser)
-
-// Skip all the new lines, tabs and spaces
-SKIP : { " " |	"\r" |	"\t" |	"\n" }
-
-// Skip comments(single line and multiline)
-SKIP : {
-   <"--"(~["\r","\n"])*>
-|  <"/*" (~["*"])* "*" ("*" | (~["*","/"] (~["*"])* "*"))* "/">
-}
-
-TOKEN : { <INT : "int"> }
-TOKEN : { <BOOL : "boolean"> }
-TOKEN : { <LONG : "long"> }
-TOKEN : { <FLOAT : "float"> }
-TOKEN : { <DOUBLE : "double"> }
-TOKEN : { <DATETIME : "datetime"> }
-TOKEN : { <STRING : "string"> }
-TOKEN : { <BYTES : "bytes"> }
-TOKEN : { <COLLECTION : "collection"> }
-TOKEN : { <RECORD : "record"> }
-TOKEN : { <MAP : "map"> }
-
-TOKEN:
-{
- 	<#LETTER : ["a"-"z", "A"-"Z"] >
-|	<#DIGIT : ["0"-"9"] >
-|   <#SPECIALCHAR : ["_", ".", "#"] >
-| <#SCOPEOP : "::">
-|	<IDENTIFIER: ( <LETTER> )+ ( <DIGIT> | <LETTER> | <SPECIALCHAR> )* ( <SCOPEOP>  ( <LETTER> )+ ( <DIGIT> | <LETTER> | <SPECIALCHAR> )*)* >
-}
-
-ColumnType Type() : 
-{ 
-	ColumnType type;
-}
-{
- (type = BasicType() | type = CompositeType()) 
- {
- 	return type;
- }	
-}
-
-ColumnType CompositeType() : 
-{ 
-	Token t = null;
-	ColumnType type;
-}
-{
- (t = <MAP>| t = <COLLECTION>| t = <RECORD>) 
- {
- 	type = ColumnType.getTypeByName(t.image);
- 	return type;
- }
-}
-
-ColumnType BasicType() : 
-{ 
-	Token t = null;
-	ColumnType type;
-}
-{
- (t = <INT>| t = <LONG>| t = <FLOAT>| t = <DOUBLE>| t = <STRING>| t = <BYTES>| t = <BOOL>) 
- {
-   String typeName = t.image;
- 	type = ColumnType.getTypeByName(typeName);
- 	return type;
- }
-}
-
-Schema.ColumnSchema ColumnSchema() throws ParseException: 
-{
-	Token t1; 
-	Schema item = null; 
-	Schema.ColumnSchema fs = null; 
-}
-{
-	(
-  LOOKAHEAD(3) fs = SchemaRecord()
-|	LOOKAHEAD(3) fs = SchemaCollection()
-|	LOOKAHEAD(3) fs = SchemaMap()
-|	fs = AtomSchema()
-	)
-	{
-		return fs;
-	}
-}
-
-Schema.ColumnSchema ProjectionColumnSchema() throws ParseException: 
-{
-	Token t1; 
-	Schema item = null; 
-	Schema.ColumnSchema fs = null; 
-}
-{
-  (
-	(
-  LOOKAHEAD(3) fs = SchemaRecord()
-|	LOOKAHEAD(3) fs = SchemaCollection()
-|	LOOKAHEAD(3) fs = SchemaMap()
-|	fs = AtomSchema()
-  )
-  { return fs; }
-|
-  { return null; }
-	)
-}
-
-Schema.ColumnSchema AtomSchema() throws ParseException : 
-{
-	Token t1 = null;
-	ColumnType type = ColumnType.BYTES;
-	Schema.ColumnSchema fs;
-}
-{
-	(	( t1 = <IDENTIFIER> [":" type = BasicType() ] )
-		{ 
-			fs = new Schema.ColumnSchema(t1.image, type); 
-			return fs;
-		}
-	)
-}
-
-Schema.ColumnSchema SchemaMap() throws ParseException :
-{
-	Token t1 = null; 
-	Schema s;
-	Schema.ColumnSchema fs;
-}
-{
-	t1 = <IDENTIFIER> ":" <MAP>  s = MapSchema()
-	{
-		fs = new Schema.ColumnSchema(t1.image, s, ColumnType.MAP);
-		return fs;
-	} 
-}
-
-Schema.ColumnSchema SchemaRecord() throws ParseException : 
-{
-	Token t1 = null; 
-	Schema s;
-	Schema.ColumnSchema fs;
-}
-{ 
-	t1 = <IDENTIFIER> ":" <RECORD> "(" s = RecordSchemaInternal() ")" 
-	{
-		fs = new Schema.ColumnSchema(t1.image, s, ColumnType.RECORD);
-		return fs;
-	} 
-}
-
-Schema.ColumnSchema SchemaCollection() throws ParseException : 
-{
-	Token t1 = null; 
-	Schema.ColumnSchema fs = null;
-}
-{ 
-	t1 = <IDENTIFIER> ":" <COLLECTION> "(" fs = SchemaCollectionEntry(t1.image) ")" 
-	{
-		return fs;
-	}
-}
-
-Schema.ColumnSchema SchemaCollectionEntry(String id) throws ParseException :
-{
-	Schema.ColumnSchema fs = null;
-}
-{
-  (
-  	fs = AnonymousSchemaRecord() | fs = SchemaRecord()
-  )
-  {
-    return new Schema.ColumnSchema(id, new Schema(fs), ColumnType.COLLECTION);
-  }
-}
-
-Schema.ColumnSchema AnonymousColumnSchema() throws ParseException : 
-{
-	Token t1; 
-	Schema item = null; 
-	Schema.ColumnSchema fs = null; 
-}
-{
-	(
-	fs = AnonymousSchemaRecord()
-|	fs = AnonymousSchemaCollection()
-|	fs = AnonymousSchemaMap()
-|	fs = AnonymousAtomSchema()
-	)
-	{
-		return fs;
-	}
-}
-
-Schema.ColumnSchema AnonymousAtomSchema() throws ParseException : 
-{
-	ColumnType type = ColumnType.BYTES;
-	Schema.ColumnSchema fs;
-}
-{
-	(  ( [ type = BasicType() ] )
-		{ 
-			fs = new Schema.ColumnSchema(null, type); 
-			return fs;
-		} 
-	)
-}
-
-Schema.ColumnSchema AnonymousSchemaMap() throws ParseException :
-{
-	Schema s;
-	Schema.ColumnSchema fs;
-}
-{
-	<MAP> s = MapSchema()
-	{
-		fs = new Schema.ColumnSchema(null, s, ColumnType.MAP);
-		return fs;
-	} 
-}
-
-Schema.ColumnSchema AnonymousSchemaRecord() throws ParseException : 
-{
-	Token t1 = null; 
-	Schema s;
-	Schema.ColumnSchema fs;
-}
-{ 
-	<RECORD> "(" s = RecordSchemaInternal() ")" 
-	{
-		fs = new Schema.ColumnSchema(null, s, ColumnType.RECORD);
-		return fs;
-	} 
-}
-
-Schema.ColumnSchema AnonymousSchemaCollection() throws ParseException : 
-{
-	Schema s;
-	Schema.ColumnSchema fs;
-}
-{ 
-	( <COLLECTION> "(" fs = SchemaCollectionEntry(null) ")"  )
-	{
-    s = new Schema(fs);
-		fs = new Schema.ColumnSchema(null, s, ColumnType.COLLECTION);
-		return fs;
-	} 
-}
-
-Schema RecordSchemaInternal() throws ParseException : 
-{
-  Schema list = new Schema(); 
-	Schema.ColumnSchema fs = null;
-}
-{
-  fs = ColumnSchema() {list.add(fs);} ( "," fs = ColumnSchema() {list.add(fs);})*
-	{ return list; }
-}
-
-Schema RecordSchema(Schema list) throws ParseException : 
-{
-	if (list == null)
-		list = new Schema(); 
-	Schema.ColumnSchema fs = null;
-}
-{
-	(
-      try {
-        fs = ColumnSchema() { if (fs != null && Projection.isVirtualColumn(fs.getName())) throw new ParseException("["+fs.getName()+"] is a reserved virtual column name"); list.add(fs);}
-		( "," fs = ColumnSchema() { if (fs != null && Projection.isVirtualColumn(fs.getName())) throw new ParseException("["+fs.getName()+"] is a reserved virtual column name"); list.add(fs);})* <EOF>
-      } catch (TokenMgrError e) {
-		throw new ParseException(e.getMessage());
-	  }
-	)	
-	{ return (list.getNumColumns() == 0 || (list.getNumColumns() == 1 && list.getColumn(0) == null) ? null : list); }
-}
-
-Schema ProjectionSchema(Schema list) throws ParseException : 
-{
-	if (list == null)
-		list = new Schema(); 
-	Schema.ColumnSchema fs = null;
-}
-{
-	(
-      try {
-		fs = ProjectionColumnSchema() { list.add(fs);}
-		( "," fs = ProjectionColumnSchema() { list.add(fs);})* <EOF>
-      } catch (TokenMgrError e) {
-		throw new ParseException(e.getMessage());
-	  }
-	)	
-	{ return (list.getNumColumns() == 0 || (list.getNumColumns() == 1 && list.getColumn(0) == null) ? null : list); }
-}
-
-Schema MapSchema() throws ParseException : 
-{
-	Schema list = new Schema(); 
-	Schema.ColumnSchema fs = null;
-}
-{
-	(
-	(
-		"(" fs = AnonymousColumnSchema() ")"
-	)
-|	{}
-	)
-	{
-	if (fs == null)
-	{
-		list.add(new Schema.ColumnSchema("", ColumnType.BYTES));
-	} else {
-		list.add(fs);
-	}
-	return list;
-	}
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/schema/package-info.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/schema/package-info.java
deleted file mode 100644
index a27a0c824..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/schema/package-info.java
+++ /dev/null
@@ -1,22 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-/**
- * Zebra Schema
- */
-package org.apache.hadoop.zebra.schema;
-
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/BCFile.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/BCFile.java
deleted file mode 100644
index 7a5ad809d..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/BCFile.java
+++ /dev/null
@@ -1,979 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-package org.apache.hadoop.zebra.tfile;
-
-import java.io.Closeable;
-import java.io.DataInput;
-import java.io.DataInputStream;
-import java.io.DataOutput;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Map;
-import java.util.TreeMap;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.compress.Compressor;
-import org.apache.hadoop.io.compress.Decompressor;
-import org.apache.hadoop.zebra.tfile.CompareUtils.Scalar;
-import org.apache.hadoop.zebra.tfile.CompareUtils.ScalarComparator;
-import org.apache.hadoop.zebra.tfile.CompareUtils.ScalarLong;
-import org.apache.hadoop.zebra.tfile.Compression.Algorithm;
-import org.apache.hadoop.zebra.tfile.Utils.Version;
-
-/**
- * Block Compressed file, the underlying physical storage layer for TFile.
- * BCFile provides the basic block level compression for the data block and meta
- * blocks. It is separated from TFile as it may be used for other
- * block-compressed file implementation.
- */
-final class BCFile {
-  // the current version of BCFile impl, increment them (major or minor) made
-  // enough changes
-  static final Version API_VERSION = new Version((short) 1, (short) 0);
-  static final Log LOG = LogFactory.getLog(BCFile.class);
-
-  /**
-   * Prevent the instantiation of BCFile objects.
-   */
-  private BCFile() {
-    // nothing
-  }
-
-  /**
-   * BCFile writer, the entry point for creating a new BCFile.
-   */
-  static public class Writer implements Closeable {
-    private final FSDataOutputStream out;
-    private final Configuration conf;
-    // the single meta block containing index of compressed data blocks
-    final DataIndex dataIndex;
-    // index for meta blocks
-    final MetaIndex metaIndex;
-    boolean blkInProgress = false;
-    private boolean metaBlkSeen = false;
-    private boolean closed = false;
-    long errorCount = 0;
-    // reusable buffers.
-    private BytesWritable fsOutputBuffer;
-
-    /**
-     * Call-back interface to register a block after a block is closed.
-     */
-    private static interface BlockRegister {
-      /**
-       * Register a block that is fully closed.
-       * 
-       * @param raw
-       *          The size of block in terms of uncompressed bytes.
-       * @param offsetStart
-       *          The start offset of the block.
-       * @param offsetEnd
-       *          One byte after the end of the block. Compressed block size is
-       *          offsetEnd - offsetStart.
-       */
-      public void register(long raw, long offsetStart, long offsetEnd);
-    }
-
-    /**
-     * Intermediate class that maintain the state of a Writable Compression
-     * Block.
-     */
-    private static final class WBlockState {
-      private final Algorithm compressAlgo;
-      private Compressor compressor; // !null only if using native
-      // Hadoop compression
-      private final FSDataOutputStream fsOut;
-      private final long posStart;
-      private final SimpleBufferedOutputStream fsBufferedOutput;
-      private OutputStream out;
-
-      /**
-       * @param compressionAlgo
-       *          The compression algorithm to be used to for compression.
-       * @throws IOException
-       */
-      public WBlockState(Algorithm compressionAlgo, FSDataOutputStream fsOut,
-          BytesWritable fsOutputBuffer, Configuration conf) throws IOException {
-        this.compressAlgo = compressionAlgo;
-        this.fsOut = fsOut;
-        this.posStart = fsOut.getPos();
-
-        fsOutputBuffer.setCapacity(TFile.getFSOutputBufferSize(conf));
-
-        this.fsBufferedOutput =
-            new SimpleBufferedOutputStream(this.fsOut, fsOutputBuffer.getBytes());
-        this.compressor = compressAlgo.getCompressor();
-
-        try {
-          this.out =
-              compressionAlgo.createCompressionStream(fsBufferedOutput,
-                  compressor, 0);
-        } catch (IOException e) {
-          compressAlgo.returnCompressor(compressor);
-          throw e;
-        }
-      }
-
-      /**
-       * Get the output stream for BlockAppender's consumption.
-       * 
-       * @return the output stream suitable for writing block data.
-       */
-      OutputStream getOutputStream() {
-        return out;
-      }
-
-      /**
-       * Get the current position in file.
-       * 
-       * @return The current byte offset in underlying file.
-       * @throws IOException
-       */
-      long getCurrentPos() throws IOException {
-        return fsOut.getPos() + fsBufferedOutput.size();
-      }
-
-      long getStartPos() {
-        return posStart;
-      }
-
-      /**
-       * Current size of compressed data.
-       * 
-       * @return
-       * @throws IOException
-       */
-      long getCompressedSize() throws IOException {
-        long ret = getCurrentPos() - posStart;
-        return ret;
-      }
-
-      /**
-       * Finishing up the current block.
-       */
-      public void finish() throws IOException {
-        try {
-          if (out != null) {
-            out.flush();
-            out = null;
-          }
-        } finally {
-          compressAlgo.returnCompressor(compressor);
-          compressor = null;
-        }
-      }
-    }
-
-    /**
-     * Access point to stuff data into a block.
-     * 
-     * TODO: Change DataOutputStream to something else that tracks the size as
-     * long instead of int. Currently, we will wrap around if the row block size
-     * is greater than 4GB.
-     */
-    public class BlockAppender extends DataOutputStream {
-      private final BlockRegister blockRegister;
-      private final WBlockState wBlkState;
-      @SuppressWarnings("hiding")
-      private boolean closed = false;
-
-      /**
-       * Constructor
-       * 
-       * @param register
-       *          the block register, which is called when the block is closed.
-       * @param wbs
-       *          The writable compression block state.
-       */
-      BlockAppender(BlockRegister register, WBlockState wbs) {
-        super(wbs.getOutputStream());
-        this.blockRegister = register;
-        this.wBlkState = wbs;
-      }
-
-      /**
-       * Get the raw size of the block.
-       * 
-       * @return the number of uncompressed bytes written through the
-       *         BlockAppender so far.
-       * @throws IOException
-       */
-      public long getRawSize() throws IOException {
-        /**
-         * Expecting the size() of a block not exceeding 4GB. Assuming the
-         * size() will wrap to negative integer if it exceeds 2GB.
-         */
-        return size() & 0x00000000ffffffffL;
-      }
-
-      /**
-       * Get the compressed size of the block in progress.
-       * 
-       * @return the number of compressed bytes written to the underlying FS
-       *         file. The size may be smaller than actual need to compress the
-       *         all data written due to internal buffering inside the
-       *         compressor.
-       * @throws IOException
-       */
-      public long getCompressedSize() throws IOException {
-        return wBlkState.getCompressedSize();
-      }
-
-      @Override
-      public void flush() {
-        // The down stream is a special kind of stream that finishes a
-        // compression block upon flush. So we disable flush() here.
-      }
-
-      /**
-       * Signaling the end of write to the block. The block register will be
-       * called for registering the finished block.
-       */
-      @Override
-      public void close() throws IOException {
-        if (closed == true) {
-          return;
-        }
-        try {
-          ++errorCount;
-          wBlkState.finish();
-          blockRegister.register(getRawSize(), wBlkState.getStartPos(),
-              wBlkState.getCurrentPos());
-          --errorCount;
-        } finally {
-          closed = true;
-          blkInProgress = false;
-        }
-      }
-    }
-
-    /**
-     * Constructor
-     * 
-     * @param fout
-     *          FS output stream.
-     * @param compressionName
-     *          Name of the compression algorithm, which will be used for all
-     *          data blocks.
-     * @throws IOException
-     * @see Compression#getSupportedAlgorithms
-     */
-    public Writer(FSDataOutputStream fout, String compressionName,
-        Configuration conf) throws IOException {
-      if (fout.getPos() != 0) {
-        throw new IOException("Output file not at zero offset.");
-      }
-
-      this.out = fout;
-      this.conf = conf;
-      dataIndex = new DataIndex(compressionName);
-      metaIndex = new MetaIndex();
-      fsOutputBuffer = new BytesWritable();
-      Magic.write(fout);
-    }
-
-    /**
-     * Close the BCFile Writer. Attempting to use the Writer after calling
-     * <code>close</code> is not allowed and may lead to undetermined results.
-     */
-    public void close() throws IOException {
-      if (closed == true) {
-        return;
-      }
-
-      try {
-        if (errorCount == 0) {
-          if (blkInProgress == true) {
-            throw new IllegalStateException(
-                "Close() called with active block appender.");
-          }
-
-          // add metaBCFileIndex to metaIndex as the last meta block
-          BlockAppender appender =
-              prepareMetaBlock(DataIndex.BLOCK_NAME,
-                  getDefaultCompressionAlgorithm());
-          try {
-            dataIndex.write(appender);
-          } finally {
-            appender.close();
-          }
-
-          long offsetIndexMeta = out.getPos();
-          metaIndex.write(out);
-
-          // Meta Index and the trailing section are written out directly.
-          out.writeLong(offsetIndexMeta);
-
-          API_VERSION.write(out);
-          Magic.write(out);
-          out.flush();
-        }
-      } finally {
-        closed = true;
-      }
-    }
-
-    private Algorithm getDefaultCompressionAlgorithm() {
-      return dataIndex.getDefaultCompressionAlgorithm();
-    }
-
-    private BlockAppender prepareMetaBlock(String name, Algorithm compressAlgo)
-        throws IOException, MetaBlockAlreadyExists {
-      if (blkInProgress == true) {
-        throw new IllegalStateException(
-            "Cannot create Meta Block until previous block is closed.");
-      }
-
-      if (metaIndex.getMetaByName(name) != null) {
-        throw new MetaBlockAlreadyExists("name=" + name);
-      }
-
-      MetaBlockRegister mbr = new MetaBlockRegister(name, compressAlgo);
-      WBlockState wbs =
-          new WBlockState(compressAlgo, out, fsOutputBuffer, conf);
-      BlockAppender ba = new BlockAppender(mbr, wbs);
-      blkInProgress = true;
-      metaBlkSeen = true;
-      return ba;
-    }
-
-    /**
-     * Create a Meta Block and obtain an output stream for adding data into the
-     * block. There can only be one BlockAppender stream active at any time.
-     * Regular Blocks may not be created after the first Meta Blocks. The caller
-     * must call BlockAppender.close() to conclude the block creation.
-     * 
-     * @param name
-     *          The name of the Meta Block. The name must not conflict with
-     *          existing Meta Blocks.
-     * @param compressionName
-     *          The name of the compression algorithm to be used.
-     * @return The BlockAppender stream
-     * @throws IOException
-     * @throws MetaBlockAlreadyExists
-     *           If the meta block with the name already exists.
-     */
-    public BlockAppender prepareMetaBlock(String name, String compressionName)
-        throws IOException, MetaBlockAlreadyExists {
-      return prepareMetaBlock(name, Compression
-          .getCompressionAlgorithmByName(compressionName));
-    }
-
-    /**
-     * Create a Meta Block and obtain an output stream for adding data into the
-     * block. The Meta Block will be compressed with the same compression
-     * algorithm as data blocks. There can only be one BlockAppender stream
-     * active at any time. Regular Blocks may not be created after the first
-     * Meta Blocks. The caller must call BlockAppender.close() to conclude the
-     * block creation.
-     * 
-     * @param name
-     *          The name of the Meta Block. The name must not conflict with
-     *          existing Meta Blocks.
-     * @return The BlockAppender stream
-     * @throws MetaBlockAlreadyExists
-     *           If the meta block with the name already exists.
-     * @throws IOException
-     */
-    public BlockAppender prepareMetaBlock(String name) throws IOException,
-        MetaBlockAlreadyExists {
-      return prepareMetaBlock(name, getDefaultCompressionAlgorithm());
-    }
-
-    /**
-     * Create a Data Block and obtain an output stream for adding data into the
-     * block. There can only be one BlockAppender stream active at any time.
-     * Data Blocks may not be created after the first Meta Blocks. The caller
-     * must call BlockAppender.close() to conclude the block creation.
-     * 
-     * @return The BlockAppender stream
-     * @throws IOException
-     */
-    public BlockAppender prepareDataBlock() throws IOException {
-      if (blkInProgress == true) {
-        throw new IllegalStateException(
-            "Cannot create Data Block until previous block is closed.");
-      }
-
-      if (metaBlkSeen == true) {
-        throw new IllegalStateException(
-            "Cannot create Data Block after Meta Blocks.");
-      }
-
-      DataBlockRegister dbr = new DataBlockRegister();
-
-      WBlockState wbs =
-          new WBlockState(getDefaultCompressionAlgorithm(), out,
-              fsOutputBuffer, conf);
-      BlockAppender ba = new BlockAppender(dbr, wbs);
-      blkInProgress = true;
-      return ba;
-    }
-
-    /**
-     * Callback to make sure a meta block is added to the internal list when its
-     * stream is closed.
-     */
-    private class MetaBlockRegister implements BlockRegister {
-      private final String name;
-      private final Algorithm compressAlgo;
-
-      MetaBlockRegister(String name, Algorithm compressAlgo) {
-        this.name = name;
-        this.compressAlgo = compressAlgo;
-      }
-
-      public void register(long raw, long begin, long end) {
-        metaIndex.addEntry(new MetaIndexEntry(name, compressAlgo,
-            new BlockRegion(begin, end - begin, raw)));
-      }
-    }
-
-    /**
-     * Callback to make sure a data block is added to the internal list when
-     * it's being closed.
-     * 
-     */
-    private class DataBlockRegister implements BlockRegister {
-      DataBlockRegister() {
-        // do nothing
-      }
-
-      public void register(long raw, long begin, long end) {
-        dataIndex.addBlockRegion(new BlockRegion(begin, end - begin, raw));
-      }
-    }
-  }
-
-  /**
-   * BCFile Reader, interface to read the file's data and meta blocks.
-   */
-  static public class Reader implements Closeable {
-    private final FSDataInputStream in;
-    private final Configuration conf;
-    final DataIndex dataIndex;
-    // Index for meta blocks
-    final MetaIndex metaIndex;
-    final Version version;
-
-    /**
-     * Intermediate class that maintain the state of a Readable Compression
-     * Block.
-     */
-    static private final class RBlockState {
-      private final Algorithm compressAlgo;
-      private Decompressor decompressor;
-      private final BlockRegion region;
-      private final InputStream in;
-
-      public RBlockState(Algorithm compressionAlgo, FSDataInputStream fsin,
-          BlockRegion region, Configuration conf) throws IOException {
-        this.compressAlgo = compressionAlgo;
-        this.region = region;
-        this.decompressor = compressionAlgo.getDecompressor();
-
-        try {
-          this.in =
-              compressAlgo
-                  .createDecompressionStream(new BoundedRangeFileInputStream(
-                      fsin, this.region.getOffset(), this.region
-                          .getCompressedSize()), decompressor, TFile
-                      .getFSInputBufferSize(conf));
-        } catch (IOException e) {
-          compressAlgo.returnDecompressor(decompressor);
-          throw e;
-        }
-      }
-
-      /**
-       * Get the output stream for BlockAppender's consumption.
-       * 
-       * @return the output stream suitable for writing block data.
-       */
-      public InputStream getInputStream() {
-        return in;
-      }
-
-      public String getCompressionName() {
-        return compressAlgo.getName();
-      }
-
-      public BlockRegion getBlockRegion() {
-        return region;
-      }
-
-      public void finish() throws IOException {
-        try {
-          in.close();
-        } finally {
-          compressAlgo.returnDecompressor(decompressor);
-          decompressor = null;
-        }
-      }
-    }
-
-    /**
-     * Access point to read a block.
-     */
-    public static class BlockReader extends DataInputStream {
-      private final RBlockState rBlkState;
-      private boolean closed = false;
-
-      BlockReader(RBlockState rbs) {
-        super(rbs.getInputStream());
-        rBlkState = rbs;
-      }
-
-      /**
-       * Finishing reading the block. Release all resources.
-       */
-      @Override
-      public void close() throws IOException {
-        if (closed == true) {
-          return;
-        }
-        try {
-          // Do not set rBlkState to null. People may access stats after calling
-          // close().
-          rBlkState.finish();
-        } finally {
-          closed = true;
-        }
-      }
-
-      /**
-       * Get the name of the compression algorithm used to compress the block.
-       * 
-       * @return name of the compression algorithm.
-       */
-      public String getCompressionName() {
-        return rBlkState.getCompressionName();
-      }
-
-      /**
-       * Get the uncompressed size of the block.
-       * 
-       * @return uncompressed size of the block.
-       */
-      public long getRawSize() {
-        return rBlkState.getBlockRegion().getRawSize();
-      }
-
-      /**
-       * Get the compressed size of the block.
-       * 
-       * @return compressed size of the block.
-       */
-      public long getCompressedSize() {
-        return rBlkState.getBlockRegion().getCompressedSize();
-      }
-
-      /**
-       * Get the starting position of the block in the file.
-       * 
-       * @return the starting position of the block in the file.
-       */
-      public long getStartPos() {
-        return rBlkState.getBlockRegion().getOffset();
-      }
-    }
-
-    /**
-     * Constructor
-     * 
-     * @param fin
-     *          FS input stream.
-     * @param fileLength
-     *          Length of the corresponding file
-     * @throws IOException
-     */
-    public Reader(FSDataInputStream fin, long fileLength, Configuration conf)
-        throws IOException {
-      this.in = fin;
-      this.conf = conf;
-
-      // move the cursor to the beginning of the tail, containing: offset to the
-      // meta block index, version and magic
-      fin.seek(fileLength - Magic.size() - Version.size() - Long.SIZE
-          / Byte.SIZE);
-      long offsetIndexMeta = fin.readLong();
-      version = new Version(fin);
-      Magic.readAndVerify(fin);
-
-      if (!version.compatibleWith(BCFile.API_VERSION)) {
-        throw new RuntimeException("Incompatible BCFile fileBCFileVersion.");
-      }
-
-      // read meta index
-      fin.seek(offsetIndexMeta);
-      metaIndex = new MetaIndex(fin);
-
-      // read data:BCFile.index, the data block index
-      BlockReader blockR = getMetaBlock(DataIndex.BLOCK_NAME);
-      try {
-        dataIndex = new DataIndex(blockR);
-      } finally {
-        blockR.close();
-      }
-    }
-
-    /**
-     * Get the name of the default compression algorithm.
-     * 
-     * @return the name of the default compression algorithm.
-     */
-    public String getDefaultCompressionName() {
-      return dataIndex.getDefaultCompressionAlgorithm().getName();
-    }
-
-    /**
-     * Get version of BCFile file being read.
-     * 
-     * @return version of BCFile file being read.
-     */
-    public Version getBCFileVersion() {
-      return version;
-    }
-
-    /**
-     * Get version of BCFile API.
-     * 
-     * @return version of BCFile API.
-     */
-    public Version getAPIVersion() {
-      return API_VERSION;
-    }
-
-    /**
-     * Finishing reading the BCFile. Release all resources.
-     */
-    public void close() {
-      // nothing to be done now
-    }
-
-    /**
-     * Get the number of data blocks.
-     * 
-     * @return the number of data blocks.
-     */
-    public int getBlockCount() {
-      return dataIndex.getBlockRegionList().size();
-    }
-
-    /**
-     * Stream access to a Meta Block.
-     * 
-     * @param name
-     *          meta block name
-     * @return BlockReader input stream for reading the meta block.
-     * @throws IOException
-     * @throws MetaBlockDoesNotExist
-     *           The Meta Block with the given name does not exist.
-     */
-    public BlockReader getMetaBlock(String name) throws IOException,
-        MetaBlockDoesNotExist {
-      MetaIndexEntry imeBCIndex = metaIndex.getMetaByName(name);
-      if (imeBCIndex == null) {
-        throw new MetaBlockDoesNotExist("name=" + name);
-      }
-
-      BlockRegion region = imeBCIndex.getRegion();
-      return createReader(imeBCIndex.getCompressionAlgorithm(), region);
-    }
-
-    /**
-     * Stream access to a Data Block.
-     * 
-     * @param blockIndex
-     *          0-based data block index.
-     * @return BlockReader input stream for reading the data block.
-     * @throws IOException
-     */
-    public BlockReader getDataBlock(int blockIndex) throws IOException {
-      if (blockIndex < 0 || blockIndex >= getBlockCount()) {
-        throw new IndexOutOfBoundsException(String.format(
-            "blockIndex=%d, numBlocks=%d", blockIndex, getBlockCount()));
-      }
-
-      BlockRegion region = dataIndex.getBlockRegionList().get(blockIndex);
-      return createReader(dataIndex.getDefaultCompressionAlgorithm(), region);
-    }
-
-    private BlockReader createReader(Algorithm compressAlgo, BlockRegion region)
-        throws IOException {
-      RBlockState rbs = new RBlockState(compressAlgo, in, region, conf);
-      return new BlockReader(rbs);
-    }
-
-    /**
-     * Find the smallest Block index whose starting offset is greater than or
-     * equal to the specified offset.
-     * 
-     * @param offset
-     *          User-specific offset.
-     * @return the index to the data Block if such block exists; or -1
-     *         otherwise.
-     */
-    public int getBlockIndexNear(long offset) {
-      ArrayList<BlockRegion> list = dataIndex.getBlockRegionList();
-      int idx =
-          Utils
-              .lowerBound(list, new ScalarLong(offset), new ScalarComparator());
-
-      if (idx == list.size()) {
-        return -1;
-      }
-
-      return idx;
-    }
-  }
-
-  /**
-   * Index for all Meta blocks.
-   */
-  static class MetaIndex {
-    // use a tree map, for getting a meta block entry by name
-    final Map<String, MetaIndexEntry> index;
-
-    // for write
-    public MetaIndex() {
-      index = new TreeMap<String, MetaIndexEntry>();
-    }
-
-    // for read, construct the map from the file
-    public MetaIndex(DataInput in) throws IOException {
-      int count = Utils.readVInt(in);
-      index = new TreeMap<String, MetaIndexEntry>();
-
-      for (int nx = 0; nx < count; nx++) {
-        MetaIndexEntry indexEntry = new MetaIndexEntry(in);
-        index.put(indexEntry.getMetaName(), indexEntry);
-      }
-    }
-
-    public void addEntry(MetaIndexEntry indexEntry) {
-      index.put(indexEntry.getMetaName(), indexEntry);
-    }
-
-    public MetaIndexEntry getMetaByName(String name) {
-      return index.get(name);
-    }
-
-    public void write(DataOutput out) throws IOException {
-      Utils.writeVInt(out, index.size());
-
-      for (MetaIndexEntry indexEntry : index.values()) {
-        indexEntry.write(out);
-      }
-    }
-  }
-
-  /**
-   * An entry describes a meta block in the MetaIndex.
-   */
-  static final class MetaIndexEntry {
-    private final String metaName;
-    private final Algorithm compressionAlgorithm;
-    private final static String defaultPrefix = "data:";
-
-    private final BlockRegion region;
-
-    public MetaIndexEntry(DataInput in) throws IOException {
-      String fullMetaName = Utils.readString(in);
-      if (fullMetaName.startsWith(defaultPrefix)) {
-        metaName =
-            fullMetaName.substring(defaultPrefix.length(), fullMetaName
-                .length());
-      } else {
-        throw new IOException("Corrupted Meta region Index");
-      }
-
-      compressionAlgorithm =
-          Compression.getCompressionAlgorithmByName(Utils.readString(in));
-      region = new BlockRegion(in);
-    }
-
-    public MetaIndexEntry(String metaName, Algorithm compressionAlgorithm,
-        BlockRegion region) {
-      this.metaName = metaName;
-      this.compressionAlgorithm = compressionAlgorithm;
-      this.region = region;
-    }
-
-    public String getMetaName() {
-      return metaName;
-    }
-
-    public Algorithm getCompressionAlgorithm() {
-      return compressionAlgorithm;
-    }
-
-    public BlockRegion getRegion() {
-      return region;
-    }
-
-    public void write(DataOutput out) throws IOException {
-      Utils.writeString(out, defaultPrefix + metaName);
-      Utils.writeString(out, compressionAlgorithm.getName());
-
-      region.write(out);
-    }
-  }
-
-  /**
-   * Index of all compressed data blocks.
-   */
-  static class DataIndex {
-    final static String BLOCK_NAME = "BCFile.index";
-
-    private final Algorithm defaultCompressionAlgorithm;
-
-    // for data blocks, each entry specifies a block's offset, compressed size
-    // and raw size
-    private final ArrayList<BlockRegion> listRegions;
-
-    // for read, deserialized from a file
-    public DataIndex(DataInput in) throws IOException {
-      defaultCompressionAlgorithm =
-          Compression.getCompressionAlgorithmByName(Utils.readString(in));
-
-      int n = Utils.readVInt(in);
-      listRegions = new ArrayList<BlockRegion>(n);
-
-      for (int i = 0; i < n; i++) {
-        BlockRegion region = new BlockRegion(in);
-        listRegions.add(region);
-      }
-    }
-
-    // for write
-    public DataIndex(String defaultCompressionAlgorithmName) {
-      this.defaultCompressionAlgorithm =
-          Compression
-              .getCompressionAlgorithmByName(defaultCompressionAlgorithmName);
-      listRegions = new ArrayList<BlockRegion>();
-    }
-
-    public Algorithm getDefaultCompressionAlgorithm() {
-      return defaultCompressionAlgorithm;
-    }
-
-    public ArrayList<BlockRegion> getBlockRegionList() {
-      return listRegions;
-    }
-
-    public void addBlockRegion(BlockRegion region) {
-      listRegions.add(region);
-    }
-
-    public void write(DataOutput out) throws IOException {
-      Utils.writeString(out, defaultCompressionAlgorithm.getName());
-
-      Utils.writeVInt(out, listRegions.size());
-
-      for (BlockRegion region : listRegions) {
-        region.write(out);
-      }
-    }
-  }
-
-  /**
-   * Magic number uniquely identifying a BCFile in the header/footer.
-   */
-  static final class Magic {
-    private final static byte[] AB_MAGIC_BCFILE =
-        {
-            // ... total of 16 bytes
-            (byte) 0xd1, (byte) 0x11, (byte) 0xd3, (byte) 0x68, (byte) 0x91,
-            (byte) 0xb5, (byte) 0xd7, (byte) 0xb6, (byte) 0x39, (byte) 0xdf,
-            (byte) 0x41, (byte) 0x40, (byte) 0x92, (byte) 0xba, (byte) 0xe1,
-            (byte) 0x50 };
-
-    public static void readAndVerify(DataInput in) throws IOException {
-      byte[] abMagic = new byte[size()];
-      in.readFully(abMagic);
-
-      // check against AB_MAGIC_BCFILE, if not matching, throw an
-      // Exception
-      if (!Arrays.equals(abMagic, AB_MAGIC_BCFILE)) {
-        throw new IOException("Not a valid BCFile.");
-      }
-    }
-
-    public static void write(DataOutput out) throws IOException {
-      out.write(AB_MAGIC_BCFILE);
-    }
-
-    public static int size() {
-      return AB_MAGIC_BCFILE.length;
-    }
-  }
-
-  /**
-   * Block region.
-   */
-  static final class BlockRegion implements Scalar {
-    private final long offset;
-    private final long compressedSize;
-    private final long rawSize;
-
-    public BlockRegion(DataInput in) throws IOException {
-      offset = Utils.readVLong(in);
-      compressedSize = Utils.readVLong(in);
-      rawSize = Utils.readVLong(in);
-    }
-
-    public BlockRegion(long offset, long compressedSize, long rawSize) {
-      this.offset = offset;
-      this.compressedSize = compressedSize;
-      this.rawSize = rawSize;
-    }
-
-    public void write(DataOutput out) throws IOException {
-      Utils.writeVLong(out, offset);
-      Utils.writeVLong(out, compressedSize);
-      Utils.writeVLong(out, rawSize);
-    }
-
-    public long getOffset() {
-      return offset;
-    }
-
-    public long getCompressedSize() {
-      return compressedSize;
-    }
-
-    public long getRawSize() {
-      return rawSize;
-    }
-
-    @Override
-    public long magnitude() {
-      return offset;
-    }
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/BoundedByteArrayOutputStream.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/BoundedByteArrayOutputStream.java
deleted file mode 100644
index f05fd6384..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/BoundedByteArrayOutputStream.java
+++ /dev/null
@@ -1,119 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-package org.apache.hadoop.zebra.tfile;
-
-import java.io.EOFException;
-import java.io.IOException;
-import java.io.OutputStream;
-
-/**
- * A byte array backed output stream with a limit. The limit should be smaller
- * than the buffer capacity. The object can be reused through <code>reset</code>
- * API and choose different limits in each round.
- */
-public class BoundedByteArrayOutputStream extends OutputStream {
-  private final byte[] buffer;
-  private int limit;
-  private int count;
-
-  /**
-   * Create a BoundedByteArrayOutputStream with the specified
-   * capacity
-   * @param capacity The capacity of the underlying byte array
-   */
-  public BoundedByteArrayOutputStream(int capacity) {
-    this(capacity, capacity);
-  }
-
-  /**
-   * Create a BoundedByteArrayOutputStream with the specified
-   * capacity and limit.
-   * @param capacity The capacity of the underlying byte array
-   * @param limit The maximum limit upto which data can be written
-   */
-  public BoundedByteArrayOutputStream(int capacity, int limit) {
-    if ((capacity < limit) || (capacity | limit) < 0) {
-      throw new IllegalArgumentException("Invalid capacity/limit");
-    }
-    this.buffer = new byte[capacity];
-    this.limit = limit;
-    this.count = 0;
-  }
-
-  @Override
-  public void write(int b) throws IOException {
-    if (count >= limit) {
-      throw new EOFException("Reaching the limit of the buffer.");
-    }
-    buffer[count++] = (byte) b;
-  }
-
-  @Override
-  public void write(byte b[], int off, int len) throws IOException {
-    if ((off < 0) || (off > b.length) || (len < 0) || ((off + len) > b.length)
-        || ((off + len) < 0)) {
-      throw new IndexOutOfBoundsException();
-    } else if (len == 0) {
-      return;
-    }
-
-    if (count + len > limit) {
-      throw new EOFException("Reach the limit of the buffer");
-    }
-
-    System.arraycopy(b, off, buffer, count, len);
-    count += len;
-  }
-
-  /**
-   * Reset the limit 
-   * @param newlim New Limit
-   */
-  public void reset(int newlim) {
-    if (newlim > buffer.length) {
-      throw new IndexOutOfBoundsException("Limit exceeds buffer size");
-    }
-    this.limit = newlim;
-    this.count = 0;
-  }
-
-  /** Reset the buffer */
-  public void reset() {
-    this.limit = buffer.length;
-    this.count = 0;
-  }
-
-  /** Return the current limit */
-  public int getLimit() {
-    return limit;
-  }
-
-  /** Returns the underlying buffer.
-   *  Data is only valid to {@link #size()}.
-   */
-  public byte[] getBuffer() {
-    return buffer;
-  }
-
-  /** Returns the length of the valid data 
-   * currently in the buffer.
-   */
-  public int size() {
-    return count;
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/BoundedRangeFileInputStream.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/BoundedRangeFileInputStream.java
deleted file mode 100644
index 6a799fd8d..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/BoundedRangeFileInputStream.java
+++ /dev/null
@@ -1,141 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-package org.apache.hadoop.zebra.tfile;
-
-import java.io.IOException;
-import java.io.InputStream;
-
-import org.apache.hadoop.fs.FSDataInputStream;
-
-/**
- * BoundedRangeFIleInputStream abstracts a contiguous region of a Hadoop
- * FSDataInputStream as a regular input stream. One can create multiple
- * BoundedRangeFileInputStream on top of the same FSDataInputStream and they
- * would not interfere with each other.
- */
-class BoundedRangeFileInputStream extends InputStream {
-
-  private FSDataInputStream in;
-  private long pos;
-  private long end;
-  private long mark;
-  private final byte[] oneByte = new byte[1];
-
-  /**
-   * Constructor
-   * 
-   * @param in
-   *          The FSDataInputStream we connect to.
-   * @param offset
-   *          Begining offset of the region.
-   * @param length
-   *          Length of the region.
-   * 
-   *          The actual length of the region may be smaller if (off_begin +
-   *          length) goes beyond the end of FS input stream.
-   */
-  public BoundedRangeFileInputStream(FSDataInputStream in, long offset,
-      long length) {
-    if (offset < 0 || length < 0) {
-      throw new IndexOutOfBoundsException("Invalid offset/length: " + offset
-          + "/" + length);
-    }
-
-    this.in = in;
-    this.pos = offset;
-    this.end = offset + length;
-    this.mark = -1;
-  }
-
-  @Override
-  public int available() throws IOException {
-    int avail = in.available();
-    if (pos + avail > end) {
-      avail = (int) (end - pos);
-    }
-
-    return avail;
-  }
-
-  @Override
-  public int read() throws IOException {
-    int ret = read(oneByte);
-    if (ret == 1) return oneByte[0] & 0xff;
-    return -1;
-  }
-
-  @Override
-  public int read(byte[] b) throws IOException {
-    return read(b, 0, b.length);
-  }
-
-  @Override
-  public int read(byte[] b, int off, int len) throws IOException {
-    if ((off | len | (off + len) | (b.length - (off + len))) < 0) {
-      throw new IndexOutOfBoundsException();
-    }
-
-    int n = (int) Math.min(Integer.MAX_VALUE, Math.min(len, (end - pos)));
-    if (n == 0) return -1;
-    int ret = 0;
-    synchronized (in) {
-      in.seek(pos);
-      ret = in.read(b, off, n);
-    }
-    if (ret < 0) {
-      end = pos;
-      return -1;
-    }
-    pos += ret;
-    return ret;
-  }
-
-  @Override
-  /*
-   * We may skip beyond the end of the file.
-   */
-  public long skip(long n) throws IOException {
-    long len = Math.min(n, end - pos);
-    pos += len;
-    return len;
-  }
-
-  @Override
-  public synchronized void mark(int readlimit) {
-    mark = pos;
-  }
-
-  @Override
-  public synchronized void reset() throws IOException {
-    if (mark < 0) throw new IOException("Resetting to invalid mark");
-    pos = mark;
-  }
-
-  @Override
-  public boolean markSupported() {
-    return true;
-  }
-
-  @Override
-  public void close() {
-    // Invalidate the state of the stream.
-    in = null;
-    pos = end;
-    mark = -1;
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/ByteArray.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/ByteArray.java
deleted file mode 100644
index 7d180254d..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/ByteArray.java
+++ /dev/null
@@ -1,92 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-package org.apache.hadoop.zebra.tfile;
-
-import org.apache.hadoop.io.BytesWritable;
-
-/**
- * Adaptor class to wrap byte-array backed objects (including java byte array)
- * as RawComparable objects.
- */
-public final class ByteArray implements RawComparable {
-  private final byte[] buffer;
-  private final int offset;
-  private final int len;
-
-  /**
-   * Constructing a ByteArray from a {@link BytesWritable}.
-   * 
-   * @param other
-   */
-  public ByteArray(BytesWritable other) {
-    this(other.getBytes(), 0, other.getLength());
-  }
-
-  /**
-   * Wrap a whole byte array as a RawComparable.
-   * 
-   * @param buffer
-   *          the byte array buffer.
-   */
-  public ByteArray(byte[] buffer) {
-    this(buffer, 0, buffer.length);
-  }
-
-  /**
-   * Wrap a partial byte array as a RawComparable.
-   * 
-   * @param buffer
-   *          the byte array buffer.
-   * @param offset
-   *          the starting offset
-   * @param len
-   *          the length of the consecutive bytes to be wrapped.
-   */
-  public ByteArray(byte[] buffer, int offset, int len) {
-    if ((offset | len | (buffer.length - offset - len)) < 0) {
-      throw new IndexOutOfBoundsException();
-    }
-    this.buffer = buffer;
-    this.offset = offset;
-    this.len = len;
-  }
-
-  /**
-   * @return the underlying buffer.
-   */
-  @Override
-  public byte[] buffer() {
-    return buffer;
-  }
-
-  /**
-   * @return the offset in the buffer.
-   */
-  @Override
-  public int offset() {
-    return offset;
-  }
-
-  /**
-   * @return the size of the byte array.
-   */
-  @Override
-  public int size() {
-    return len;
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/Chunk.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/Chunk.java
deleted file mode 100644
index cad81c231..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/Chunk.java
+++ /dev/null
@@ -1,429 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.tfile;
-
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
-
-/**
- * Several related classes to support chunk-encoded sub-streams on top of a
- * regular stream.
- */
-final class Chunk {
-
-  /**
-   * Prevent the instantiation of class.
-   */
-  private Chunk() {
-    // nothing
-  }
-
-  /**
-   * Decoding a chain of chunks encoded through ChunkEncoder or
-   * SingleChunkEncoder.
-   */
-  static public class ChunkDecoder extends InputStream {
-    private DataInputStream in = null;
-    private boolean lastChunk;
-    private int remain = 0;
-    private boolean closed;
-
-    public ChunkDecoder() {
-      lastChunk = true;
-      closed = true;
-    }
-
-    public void reset(DataInputStream downStream) {
-      // no need to wind forward the old input.
-      in = downStream;
-      lastChunk = false;
-      remain = 0;
-      closed = false;
-    }
-
-    /**
-     * Constructor
-     * 
-     * @param in
-     *          The source input stream which contains chunk-encoded data
-     *          stream.
-     */
-    public ChunkDecoder(DataInputStream in) {
-      this.in = in;
-      lastChunk = false;
-      closed = false;
-    }
-
-    /**
-     * Have we reached the last chunk.
-     * 
-     * @return true if we have reached the last chunk.
-     * @throws java.io.IOException
-     */
-    public boolean isLastChunk() throws IOException {
-      checkEOF();
-      return lastChunk;
-    }
-
-    /**
-     * How many bytes remain in the current chunk?
-     * 
-     * @return remaining bytes left in the current chunk.
-     * @throws java.io.IOException
-     */
-    public int getRemain() throws IOException {
-      checkEOF();
-      return remain;
-    }
-
-    /**
-     * Reading the length of next chunk.
-     * 
-     * @throws java.io.IOException
-     *           when no more data is available.
-     */
-    private void readLength() throws IOException {
-      remain = Utils.readVInt(in);
-      if (remain >= 0) {
-        lastChunk = true;
-      } else {
-        remain = -remain;
-      }
-    }
-
-    /**
-     * Check whether we reach the end of the stream.
-     * 
-     * @return false if the chunk encoded stream has more data to read (in which
-     *         case available() will be greater than 0); true otherwise.
-     * @throws java.io.IOException
-     *           on I/O errors.
-     */
-    private boolean checkEOF() throws IOException {
-      if (isClosed()) return true;
-      while (true) {
-        if (remain > 0) return false;
-        if (lastChunk) return true;
-        readLength();
-      }
-    }
-
-    @Override
-    /*
-     * This method never blocks the caller. Returning 0 does not mean we reach
-     * the end of the stream.
-     */
-    public int available() {
-      return remain;
-    }
-
-    @Override
-    public int read() throws IOException {
-      if (checkEOF()) return -1;
-      int ret = in.read();
-      if (ret < 0) throw new IOException("Corrupted chunk encoding stream");
-      --remain;
-      return ret;
-    }
-
-    @Override
-    public int read(byte[] b) throws IOException {
-      return read(b, 0, b.length);
-    }
-
-    @Override
-    public int read(byte[] b, int off, int len) throws IOException {
-      if ((off | len | (off + len) | (b.length - (off + len))) < 0) {
-        throw new IndexOutOfBoundsException();
-      }
-
-      if (!checkEOF()) {
-        int n = Math.min(remain, len);
-        int ret = in.read(b, off, n);
-        if (ret < 0) throw new IOException("Corrupted chunk encoding stream");
-        remain -= ret;
-        return ret;
-      }
-      return -1;
-    }
-
-    @Override
-    public long skip(long n) throws IOException {
-      if (!checkEOF()) {
-        long ret = in.skip(Math.min(remain, n));
-        remain -= ret;
-        return ret;
-      }
-      return 0;
-    }
-
-    @Override
-    public boolean markSupported() {
-      return false;
-    }
-
-    public boolean isClosed() {
-      return closed;
-    }
-
-    @Override
-    public void close() throws IOException {
-      if (closed == false) {
-        try {
-          while (!checkEOF()) {
-            skip(Integer.MAX_VALUE);
-          }
-        } finally {
-          closed = true;
-        }
-      }
-    }
-  }
-
-  /**
-   * Chunk Encoder. Encoding the output data into a chain of chunks in the
-   * following sequences: -len1, byte[len1], -len2, byte[len2], ... len_n,
-   * byte[len_n]. Where len1, len2, ..., len_n are the lengths of the data
-   * chunks. Non-terminal chunks have their lengths negated. Non-terminal chunks
-   * cannot have length 0. All lengths are in the range of 0 to
-   * Integer.MAX_VALUE and are encoded in Utils.VInt format.
-   */
-  static public class ChunkEncoder extends OutputStream {
-    /**
-     * The data output stream it connects to.
-     */
-    private DataOutputStream out;
-
-    /**
-     * The internal buffer that is only used when we do not know the advertised
-     * size.
-     */
-    private byte buf[];
-
-    /**
-     * The number of valid bytes in the buffer. This value is always in the
-     * range <tt>0</tt> through <tt>buf.length</tt>; elements <tt>buf[0]</tt>
-     * through <tt>buf[count-1]</tt> contain valid byte data.
-     */
-    private int count;
-
-    /**
-     * Constructor.
-     * 
-     * @param out
-     *          the underlying output stream.
-     * @param buf
-     *          user-supplied buffer. The buffer would be used exclusively by
-     *          the ChunkEncoder during its life cycle.
-     */
-    public ChunkEncoder(DataOutputStream out, byte[] buf) {
-      this.out = out;
-      this.buf = buf;
-      this.count = 0;
-    }
-
-    /**
-     * Write out a chunk.
-     * 
-     * @param chunk
-     *          The chunk buffer.
-     * @param offset
-     *          Offset to chunk buffer for the beginning of chunk.
-     * @param len
-     * @param last
-     *          Is this the last call to flushBuffer?
-     */
-    private void writeChunk(byte[] chunk, int offset, int len, boolean last)
-        throws IOException {
-      if (last) { // always write out the length for the last chunk.
-        Utils.writeVInt(out, len);
-        if (len > 0) {
-          out.write(chunk, offset, len);
-        }
-      } else {
-        if (len > 0) {
-          Utils.writeVInt(out, -len);
-          out.write(chunk, offset, len);
-        }
-      }
-    }
-
-    /**
-     * Write out a chunk that is a concatenation of the internal buffer plus
-     * user supplied data. This will never be the last block.
-     * 
-     * @param data
-     *          User supplied data buffer.
-     * @param offset
-     *          Offset to user data buffer.
-     * @param len
-     *          User data buffer size.
-     */
-    private void writeBufData(byte[] data, int offset, int len)
-        throws IOException {
-      if (count + len > 0) {
-        Utils.writeVInt(out, -(count + len));
-        out.write(buf, 0, count);
-        count = 0;
-        out.write(data, offset, len);
-      }
-    }
-
-    /**
-     * Flush the internal buffer.
-     * 
-     * Is this the last call to flushBuffer?
-     * 
-     * @throws java.io.IOException
-     */
-    private void flushBuffer() throws IOException {
-      if (count > 0) {
-        writeChunk(buf, 0, count, false);
-        count = 0;
-      }
-    }
-
-    @Override
-    public void write(int b) throws IOException {
-      if (count >= buf.length) {
-        flushBuffer();
-      }
-      buf[count++] = (byte) b;
-    }
-
-    @Override
-    public void write(byte b[]) throws IOException {
-      write(b, 0, b.length);
-    }
-
-    @Override
-    public void write(byte b[], int off, int len) throws IOException {
-      if ((len + count) >= buf.length) {
-        /*
-         * If the input data do not fit in buffer, flush the output buffer and
-         * then write the data directly. In this way buffered streams will
-         * cascade harmlessly.
-         */
-        writeBufData(b, off, len);
-        return;
-      }
-
-      System.arraycopy(b, off, buf, count, len);
-      count += len;
-    }
-
-    @Override
-    public void flush() throws IOException {
-      flushBuffer();
-      out.flush();
-    }
-
-    @Override
-    public void close() throws IOException {
-      if (buf != null) {
-        try {
-          writeChunk(buf, 0, count, true);
-        } finally {
-          buf = null;
-          out = null;
-        }
-      }
-    }
-  }
-
-  /**
-   * Encode the whole stream as a single chunk. Expecting to know the size of
-   * the chunk up-front.
-   */
-  static public class SingleChunkEncoder extends OutputStream {
-    /**
-     * The data output stream it connects to.
-     */
-    private final DataOutputStream out;
-
-    /**
-     * The remaining bytes to be written.
-     */
-    private int remain;
-    private boolean closed = false;
-
-    /**
-     * Constructor.
-     * 
-     * @param out
-     *          the underlying output stream.
-     * @param size
-     *          The total # of bytes to be written as a single chunk.
-     * @throws java.io.IOException
-     *           if an I/O error occurs.
-     */
-    public SingleChunkEncoder(DataOutputStream out, int size)
-        throws IOException {
-      this.out = out;
-      this.remain = size;
-      Utils.writeVInt(out, size);
-    }
-
-    @Override
-    public void write(int b) throws IOException {
-      if (remain > 0) {
-        out.write(b);
-        --remain;
-      } else {
-        throw new IOException("Writing more bytes than advertised size.");
-      }
-    }
-
-    @Override
-    public void write(byte b[]) throws IOException {
-      write(b, 0, b.length);
-    }
-
-    @Override
-    public void write(byte b[], int off, int len) throws IOException {
-      if (remain >= len) {
-        out.write(b, off, len);
-        remain -= len;
-      } else {
-        throw new IOException("Writing more bytes than advertised size.");
-      }
-    }
-
-    @Override
-    public void flush() throws IOException {
-      out.flush();
-    }
-
-    @Override
-    public void close() throws IOException {
-      if (closed == true) {
-        return;
-      }
-
-      try {
-        if (remain > 0) {
-          throw new IOException("Writing less bytes than advertised size.");
-        }
-      } finally {
-        closed = true;
-      }
-    }
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/CompareUtils.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/CompareUtils.java
deleted file mode 100644
index d6abd491a..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/CompareUtils.java
+++ /dev/null
@@ -1,98 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.tfile;
-
-import java.io.Serializable;
-import java.util.Comparator;
-
-import org.apache.hadoop.io.RawComparator;
-import org.apache.hadoop.io.WritableComparator;
-
-class CompareUtils {
-  /**
-   * Prevent the instantiation of class.
-   */
-  private CompareUtils() {
-    // nothing
-  }
-
-  /**
-   * A comparator to compare anything that implements {@link RawComparable}
-   * using a customized comparator.
-   */
-  public static final class BytesComparator implements
-      Comparator<RawComparable> {
-    private RawComparator<Object> cmp;
-
-    public BytesComparator(RawComparator<Object> cmp) {
-      this.cmp = cmp;
-    }
-
-    @Override
-    public int compare(RawComparable o1, RawComparable o2) {
-      return compare(o1.buffer(), o1.offset(), o1.size(), o2.buffer(), o2
-          .offset(), o2.size());
-    }
-
-    public int compare(byte[] a, int off1, int len1, byte[] b, int off2,
-        int len2) {
-      return cmp.compare(a, off1, len1, b, off2, len2);
-    }
-  }
-
-  /**
-   * Interface for all objects that has a single integer magnitude.
-   */
-  static interface Scalar {
-    long magnitude();
-  }
-
-  static final class ScalarLong implements Scalar {
-    private long magnitude;
-
-    public ScalarLong(long m) {
-      magnitude = m;
-    }
-
-    public long magnitude() {
-      return magnitude;
-    }
-  }
-
-  public static final class ScalarComparator implements Comparator<Scalar>, Serializable {
-    @Override
-    public int compare(Scalar o1, Scalar o2) {
-      long diff = o1.magnitude() - o2.magnitude();
-      if (diff < 0) return -1;
-      if (diff > 0) return 1;
-      return 0;
-    }
-  }
-
-  public static final class MemcmpRawComparator implements
-      RawComparator<Object>, Serializable {
-    @Override
-    public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
-      return WritableComparator.compareBytes(b1, s1, l1, b2, s2, l2);
-    }
-
-    @Override
-    public int compare(Object o1, Object o2) {
-      throw new RuntimeException("Object comparison not supported");
-    }
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/Compression.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/Compression.java
deleted file mode 100644
index d55b97b66..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/Compression.java
+++ /dev/null
@@ -1,359 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.tfile;
-
-import java.io.BufferedInputStream;
-import java.io.BufferedOutputStream;
-import java.io.FilterOutputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
-import java.util.ArrayList;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.compress.CodecPool;
-import org.apache.hadoop.io.compress.CompressionCodec;
-import org.apache.hadoop.io.compress.CompressionInputStream;
-import org.apache.hadoop.io.compress.CompressionOutputStream;
-import org.apache.hadoop.io.compress.Compressor;
-import org.apache.hadoop.io.compress.Decompressor;
-import org.apache.hadoop.io.compress.DefaultCodec;
-import org.apache.hadoop.util.ReflectionUtils;
-
-/**
- * Compression related stuff.
- */
-final class Compression {
-  static final Log LOG = LogFactory.getLog(Compression.class);
-
-  /**
-   * Prevent the instantiation of class.
-   */
-  private Compression() {
-    // nothing
-  }
-
-  static class FinishOnFlushCompressionStream extends FilterOutputStream {
-    public FinishOnFlushCompressionStream(CompressionOutputStream cout) {
-      super(cout);
-    }
-
-    @Override
-    public void write(byte b[], int off, int len) throws IOException {
-      out.write(b, off, len);
-    }
-
-    @Override
-    public void flush() throws IOException {
-      CompressionOutputStream cout = (CompressionOutputStream) out;
-      cout.finish();
-      cout.flush();
-      cout.resetState();
-    }
-  }
-
-  /**
-   * Compression algorithms.
-   */
-  static enum Algorithm {
-    LZO(TFile.COMPRESSION_LZO) {
-      private transient boolean checked = false;
-      private static final String defaultClazz =
-          "org.apache.hadoop.io.compress.LzoCodec";
-      private transient CompressionCodec codec = null;
-
-      @Override
-      public synchronized boolean isSupported() {
-        if (!checked) {
-          checked = true;
-          String extClazz =
-              (conf.get(CONF_LZO_CLASS) == null ? System
-                  .getProperty(CONF_LZO_CLASS) : null);
-          String clazz = (extClazz != null) ? extClazz : defaultClazz;
-          try {
-            LOG.info("Trying to load Lzo codec class: " + clazz);
-            codec =
-                (CompressionCodec) ReflectionUtils.newInstance(Class
-                    .forName(clazz), conf);
-          } catch (ClassNotFoundException e) {
-            // that is okay
-          }
-        }
-        return codec != null;
-      }
-
-      @Override
-      CompressionCodec getCodec() throws IOException {
-        if (!isSupported()) {
-          throw new IOException(
-              "LZO codec class not specified. Did you forget to set property "
-                  + CONF_LZO_CLASS + "?");
-        }
-
-        return codec;
-      }
-
-      @Override
-      public synchronized InputStream createDecompressionStream(
-          InputStream downStream, Decompressor decompressor,
-          int downStreamBufferSize) throws IOException {
-        if (!isSupported()) {
-          throw new IOException(
-              "LZO codec class not specified. Did you forget to set property "
-                  + CONF_LZO_CLASS + "?");
-        }
-        InputStream bis1 = null;
-        if (downStreamBufferSize > 0) {
-          bis1 = new BufferedInputStream(downStream, downStreamBufferSize);
-        } else {
-          bis1 = downStream;
-        }
-        CompressionInputStream cis =
-            codec.createInputStream(bis1, decompressor);
-        BufferedInputStream bis2 = new BufferedInputStream(cis, DATA_IBUF_SIZE);
-        return bis2;
-      }
-
-      @Override
-      public synchronized OutputStream createCompressionStream(
-          OutputStream downStream, Compressor compressor,
-          int downStreamBufferSize) throws IOException {
-        if (!isSupported()) {
-          throw new IOException(
-              "LZO codec class not specified. Did you forget to set property "
-                  + CONF_LZO_CLASS + "?");
-        }
-        OutputStream bos1 = null;
-        if (downStreamBufferSize > 0) {
-          bos1 = new BufferedOutputStream(downStream, downStreamBufferSize);
-        } else {
-          bos1 = downStream;
-        }
-        CompressionOutputStream cos =
-            codec.createOutputStream(bos1, compressor);
-        BufferedOutputStream bos2 =
-            new BufferedOutputStream(new FinishOnFlushCompressionStream(cos),
-                DATA_OBUF_SIZE);
-        return bos2;
-      }
-    },
-
-    GZ(TFile.COMPRESSION_GZ) {
-      private transient DefaultCodec codec;
-
-      @Override
-      CompressionCodec getCodec() {
-        if (codec == null) {
-          codec = new DefaultCodec();
-          codec.setConf(conf);
-        }
-
-        return codec;
-      }
-
-      @Override
-      public synchronized InputStream createDecompressionStream(
-          InputStream downStream, Decompressor decompressor,
-          int downStreamBufferSize) throws IOException {
-        // Set the internal buffer size to read from down stream.
-        if (downStreamBufferSize > 0) {
-          codec.getConf().setInt("io.file.buffer.size", downStreamBufferSize);
-        }
-        CompressionInputStream cis =
-            codec.createInputStream(downStream, decompressor);
-        BufferedInputStream bis2 = new BufferedInputStream(cis, DATA_IBUF_SIZE);
-        return bis2;
-      }
-
-      @Override
-      public synchronized OutputStream createCompressionStream(
-          OutputStream downStream, Compressor compressor,
-          int downStreamBufferSize) throws IOException {
-        OutputStream bos1 = null;
-        if (downStreamBufferSize > 0) {
-          bos1 = new BufferedOutputStream(downStream, downStreamBufferSize);
-        } else {
-          bos1 = downStream;
-        }
-        codec.getConf().setInt("io.file.buffer.size", 32 * 1024);
-        CompressionOutputStream cos =
-            codec.createOutputStream(bos1, compressor);
-        BufferedOutputStream bos2 =
-            new BufferedOutputStream(new FinishOnFlushCompressionStream(cos),
-                DATA_OBUF_SIZE);
-        return bos2;
-      }
-
-      @Override
-      public boolean isSupported() {
-        return true;
-      }
-    },
-
-    NONE(TFile.COMPRESSION_NONE) {
-      @Override
-      CompressionCodec getCodec() {
-        return null;
-      }
-
-      @Override
-      public synchronized InputStream createDecompressionStream(
-          InputStream downStream, Decompressor decompressor,
-          int downStreamBufferSize) throws IOException {
-        if (downStreamBufferSize > 0) {
-          return new BufferedInputStream(downStream, downStreamBufferSize);
-        }
-        return downStream;
-      }
-
-      @Override
-      public synchronized OutputStream createCompressionStream(
-          OutputStream downStream, Compressor compressor,
-          int downStreamBufferSize) throws IOException {
-        if (downStreamBufferSize > 0) {
-          return new BufferedOutputStream(downStream, downStreamBufferSize);
-        }
-
-        return downStream;
-      }
-
-      @Override
-      public boolean isSupported() {
-        return true;
-      }
-    };
-
-    // We require that all compression related settings are configured
-    // statically in the Configuration object.
-    protected static final Configuration conf = new Configuration();
-    private final String compressName;
-    // data input buffer size to absorb small reads from application.
-    private static final int DATA_IBUF_SIZE = 1 * 1024;
-    // data output buffer size to absorb small writes from application.
-    private static final int DATA_OBUF_SIZE = 4 * 1024;
-    public static final String CONF_LZO_CLASS =
-        "io.compression.codec.lzo.class";
-
-    Algorithm(String name) {
-      this.compressName = name;
-    }
-
-    abstract CompressionCodec getCodec() throws IOException;
-
-    public abstract InputStream createDecompressionStream(
-        InputStream downStream, Decompressor decompressor,
-        int downStreamBufferSize) throws IOException;
-
-    public abstract OutputStream createCompressionStream(
-        OutputStream downStream, Compressor compressor, int downStreamBufferSize)
-        throws IOException;
-
-    public abstract boolean isSupported();
-
-    public Compressor getCompressor() throws IOException {
-      CompressionCodec codec = getCodec();
-      if (codec != null) {
-        Compressor compressor = CodecPool.getCompressor(codec);
-        if (compressor != null) {
-          if (compressor.finished()) {
-            // Somebody returns the compressor to CodecPool but is still using
-            // it.
-            LOG.warn("Compressor obtained from CodecPool already finished()");
-          } else {
-            LOG.debug("Got a compressor: " + compressor.hashCode());
-          }
-          /**
-           * Following statement is necessary to get around bugs in 0.18 where a
-           * compressor is referenced after returned back to the codec pool.
-           */
-          compressor.reset();
-        }
-        return compressor;
-      }
-      return null;
-    }
-
-    public void returnCompressor(Compressor compressor) {
-      if (compressor != null) {
-        LOG.debug("Return a compressor: " + compressor.hashCode());
-        CodecPool.returnCompressor(compressor);
-      }
-    }
-
-    public Decompressor getDecompressor() throws IOException {
-      CompressionCodec codec = getCodec();
-      if (codec != null) {
-        Decompressor decompressor = CodecPool.getDecompressor(codec);
-        if (decompressor != null) {
-          if (decompressor.finished()) {
-            // Somebody returns the decompressor to CodecPool but is still using
-            // it.
-            LOG.warn("Deompressor obtained from CodecPool already finished()");
-          } else {
-            LOG.debug("Got a decompressor: " + decompressor.hashCode());
-          }
-          /**
-           * Following statement is necessary to get around bugs in 0.18 where a
-           * decompressor is referenced after returned back to the codec pool.
-           */
-          decompressor.reset();
-        }
-        return decompressor;
-      }
-
-      return null;
-    }
-
-    public void returnDecompressor(Decompressor decompressor) {
-      if (decompressor != null) {
-        LOG.debug("Returned a decompressor: " + decompressor.hashCode());
-        CodecPool.returnDecompressor(decompressor);
-      }
-    }
-
-    public String getName() {
-      return compressName;
-    }
-  }
-
-  static Algorithm getCompressionAlgorithmByName(String compressName) {
-    Algorithm[] algos = Algorithm.class.getEnumConstants();
-
-    for (Algorithm a : algos) {
-      if (a.getName().equals(compressName)) {
-        return a;
-      }
-    }
-
-    throw new IllegalArgumentException(
-        "Unsupported compression algorithm name: " + compressName);
-  }
-
-  static String[] getSupportedAlgorithms() {
-    Algorithm[] algos = Algorithm.class.getEnumConstants();
-
-    ArrayList<String> ret = new ArrayList<String>();
-    for (Algorithm a : algos) {
-      if (a.isSupported()) {
-        ret.add(a.getName());
-      }
-    }
-    return ret.toArray(new String[ret.size()]);
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/MetaBlockAlreadyExists.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/MetaBlockAlreadyExists.java
deleted file mode 100644
index c69ce5530..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/MetaBlockAlreadyExists.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-package org.apache.hadoop.zebra.tfile;
-
-import java.io.IOException;
-
-/**
- * Exception - Meta Block with the same name already exists.
- */
-@SuppressWarnings("serial")
-public class MetaBlockAlreadyExists extends IOException {
-  /**
-   * Constructor
-   * 
-   * @param s
-   *          message.
-   */
-  MetaBlockAlreadyExists(String s) {
-    super(s);
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/MetaBlockDoesNotExist.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/MetaBlockDoesNotExist.java
deleted file mode 100644
index 63bef104a..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/MetaBlockDoesNotExist.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-package org.apache.hadoop.zebra.tfile;
-
-import java.io.IOException;
-
-/**
- * Exception - No such Meta Block with the given name.
- */
-@SuppressWarnings("serial")
-public class MetaBlockDoesNotExist extends IOException {
-  /**
-   * Constructor
-   * 
-   * @param s
-   *          message.
-   */
-  MetaBlockDoesNotExist(String s) {
-    super(s);
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/RawComparable.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/RawComparable.java
deleted file mode 100644
index db264420e..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/RawComparable.java
+++ /dev/null
@@ -1,57 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-package org.apache.hadoop.zebra.tfile;
-
-import java.util.Collections;
-import java.util.Comparator;
-
-import org.apache.hadoop.io.RawComparator;
-
-/**
- * Interface for objects that can be compared through {@link RawComparator}.
- * This is useful in places where we need a single object reference to specify a
- * range of bytes in a byte array, such as {@link Comparable} or
- * {@link Collections#binarySearch(java.util.List, Object, Comparator)}
- * 
- * The actual comparison among RawComparable's requires an external
- * RawComparator and it is applications' responsibility to ensure two
- * RawComparable are supposed to be semantically comparable with the same
- * RawComparator.
- */
-public interface RawComparable {
-  /**
-   * Get the underlying byte array.
-   * 
-   * @return The underlying byte array.
-   */
-  abstract byte[] buffer();
-
-  /**
-   * Get the offset of the first byte in the byte array.
-   * 
-   * @return The offset of the first byte in the byte array.
-   */
-  abstract int offset();
-
-  /**
-   * Get the size of the byte range in the byte array.
-   * 
-   * @return The size of the byte range in the byte array.
-   */
-  abstract int size();
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/SimpleBufferedOutputStream.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/SimpleBufferedOutputStream.java
deleted file mode 100644
index ff6a0e3e1..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/SimpleBufferedOutputStream.java
+++ /dev/null
@@ -1,77 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-package org.apache.hadoop.zebra.tfile;
-
-import java.io.FilterOutputStream;
-import java.io.IOException;
-import java.io.OutputStream;
-
-/**
- * A simplified BufferedOutputStream with borrowed buffer, and allow users to
- * see how much data have been buffered.
- */
-class SimpleBufferedOutputStream extends FilterOutputStream {
-  protected byte buf[]; // the borrowed buffer
-  protected int count = 0; // bytes used in buffer.
-
-  // Constructor
-  public SimpleBufferedOutputStream(OutputStream out, byte[] buf) {
-    super(out);
-    this.buf = buf;
-  }
-
-  private void flushBuffer() throws IOException {
-    if (count > 0) {
-      out.write(buf, 0, count);
-      count = 0;
-    }
-  }
-
-  @Override
-  public void write(int b) throws IOException {
-    if (count >= buf.length) {
-      flushBuffer();
-    }
-    buf[count++] = (byte) b;
-  }
-
-  @Override
-  public void write(byte b[], int off, int len) throws IOException {
-    if (len >= buf.length) {
-      flushBuffer();
-      out.write(b, off, len);
-      return;
-    }
-    if (len > buf.length - count) {
-      flushBuffer();
-    }
-    System.arraycopy(b, off, buf, count, len);
-    count += len;
-  }
-
-  @Override
-  public synchronized void flush() throws IOException {
-    flushBuffer();
-    out.flush();
-  }
-
-  // Get the size of internal buffer being used.
-  public int size() {
-    return count;
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/TFile.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/TFile.java
deleted file mode 100644
index 2fca5a562..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/TFile.java
+++ /dev/null
@@ -1,2372 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-package org.apache.hadoop.zebra.tfile;
-
-import java.io.ByteArrayInputStream;
-import java.io.Closeable;
-import java.io.DataInput;
-import java.io.DataInputStream;
-import java.io.DataOutput;
-import java.io.DataOutputStream;
-import java.io.EOFException;
-import java.io.IOException;
-import java.io.OutputStream;
-import java.util.ArrayList;
-import java.util.Comparator;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.zebra.tfile.BoundedByteArrayOutputStream;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.DataInputBuffer;
-import org.apache.hadoop.io.DataOutputBuffer;
-import org.apache.hadoop.io.IOUtils;
-import org.apache.hadoop.io.RawComparator;
-import org.apache.hadoop.io.WritableComparator;
-import org.apache.hadoop.zebra.tfile.BCFile.Reader.BlockReader;
-import org.apache.hadoop.zebra.tfile.BCFile.Writer.BlockAppender;
-import org.apache.hadoop.zebra.tfile.Chunk.ChunkDecoder;
-import org.apache.hadoop.zebra.tfile.Chunk.ChunkEncoder;
-import org.apache.hadoop.zebra.tfile.CompareUtils.BytesComparator;
-import org.apache.hadoop.zebra.tfile.CompareUtils.MemcmpRawComparator;
-import org.apache.hadoop.zebra.tfile.Utils.Version;
-import org.apache.hadoop.io.serializer.JavaSerializationComparator;
-
-/**
- * A TFile is a container of key-value pairs. Both keys and values are type-less
- * bytes. Keys are restricted to 64KB, value length is not restricted
- * (practically limited to the available disk storage). TFile further provides
- * the following features:
- * <ul>
- * <li>Block Compression.
- * <li>Named meta data blocks.
- * <li>Sorted or unsorted keys.
- * <li>Seek by key or by file offset.
- * </ul>
- * The memory footprint of a TFile includes the following:
- * <ul>
- * <li>Some constant overhead of reading or writing a compressed block.
- * <ul>
- * <li>Each compressed block requires one compression/decompression codec for
- * I/O.
- * <li>Temporary space to buffer the key.
- * <li>Temporary space to buffer the value (for TFile.Writer only). Values are
- * chunk encoded, so that we buffer at most one chunk of user data. By default,
- * the chunk buffer is 1MB. Reading chunked value does not require additional
- * memory.
- * </ul>
- * <li>TFile index, which is proportional to the total number of Data Blocks.
- * The total amount of memory needed to hold the index can be estimated as
- * (56+AvgKeySize)*NumBlocks.
- * <li>MetaBlock index, which is proportional to the total number of Meta
- * Blocks.The total amount of memory needed to hold the index for Meta Blocks
- * can be estimated as (40+AvgMetaBlockName)*NumMetaBlock.
- * </ul>
- * <p>
- * The behavior of TFile can be customized by the following variables through
- * Configuration:
- * <ul>
- * <li><b>tfile.io.chunk.size</b>: Value chunk size. Integer (in bytes). Default
- * to 1MB. Values of the length less than the chunk size is guaranteed to have
- * known value length in read time (See
- * {@link TFile.Reader.Scanner.Entry#isValueLengthKnown()}).
- * <li><b>tfile.fs.output.buffer.size</b>: Buffer size used for
- * FSDataOutputStream. Integer (in bytes). Default to 256KB.
- * <li><b>tfile.fs.input.buffer.size</b>: Buffer size used for
- * FSDataInputStream. Integer (in bytes). Default to 256KB.
- * </ul>
- * <p>
- * Suggestions on performance optimization.
- * <ul>
- * <li>Minimum block size. We recommend a setting of minimum block size between
- * 256KB to 1MB for general usage. Larger block size is preferred if files are
- * primarily for sequential access. However, it would lead to inefficient random
- * access (because there are more data to decompress). Smaller blocks are good
- * for random access, but require more memory to hold the block index, and may
- * be slower to create (because we must flush the compressor stream at the
- * conclusion of each data block, which leads to an FS I/O flush). Further, due
- * to the internal caching in Compression codec, the smallest possible block
- * size would be around 20KB-30KB.
- * <li>The current implementation does not offer true multi-threading for
- * reading. The implementation uses FSDataInputStream seek()+read(), which is
- * shown to be much faster than positioned-read call in single thread mode.
- * However, it also means that if multiple threads attempt to access the same
- * TFile (using multiple scanners) simultaneously, the actual I/O is carried out
- * sequentially even if they access different DFS blocks.
- * <li>Compression codec. Use "none" if the data is not very compressable (by
- * compressable, I mean a compression ratio at least 2:1). Generally, use "lzo"
- * as the starting point for experimenting. "gz" overs slightly better
- * compression ratio over "lzo" but requires 4x CPU to compress and 2x CPU to
- * decompress, comparing to "lzo".
- * <li>File system buffering, if the underlying FSDataInputStream and
- * FSDataOutputStream is already adequately buffered; or if applications
- * reads/writes keys and values in large buffers, we can reduce the sizes of
- * input/output buffering in TFile layer by setting the configuration parameters
- * "tfile.fs.input.buffer.size" and "tfile.fs.output.buffer.size".
- * </ul>
- * 
- * Some design rationale behind TFile can be found at <a
- * href=https://issues.apache.org/jira/browse/HADOOP-3315>Hadoop-3315</a>.
- */
-public class TFile {
-  static final Log LOG = LogFactory.getLog(TFile.class);
-
-  private static final String CHUNK_BUF_SIZE_ATTR = "tfile.io.chunk.size";
-  private static final String FS_INPUT_BUF_SIZE_ATTR =
-      "tfile.fs.input.buffer.size";
-  private static final String FS_OUTPUT_BUF_SIZE_ATTR =
-      "tfile.fs.output.buffer.size";
-
-  static int getChunkBufferSize(Configuration conf) {
-    int ret = conf.getInt(CHUNK_BUF_SIZE_ATTR, 1024 * 1024);
-    return (ret > 0) ? ret : 1024 * 1024;
-  }
-
-  static int getFSInputBufferSize(Configuration conf) {
-    return conf.getInt(FS_INPUT_BUF_SIZE_ATTR, 256 * 1024);
-  }
-
-  static int getFSOutputBufferSize(Configuration conf) {
-    return conf.getInt(FS_OUTPUT_BUF_SIZE_ATTR, 256 * 1024);
-  }
-
-  private static final int MAX_KEY_SIZE = 64 * 1024; // 64KB
-  static final Version API_VERSION = new Version((short) 1, (short) 0);
-
-  /** compression: gzip */
-  public static final String COMPRESSION_GZ = "gz";
-  /** compression: lzo */
-  public static final String COMPRESSION_LZO = "lzo";
-  /** compression: none */
-  public static final String COMPRESSION_NONE = "none";
-  /** comparator: memcmp */
-  public static final String COMPARATOR_MEMCMP = "memcmp";
-  /** comparator prefix: java class */
-  public static final String COMPARATOR_JCLASS = "jclass:";
-
-  /**
-   * Make a raw comparator from a string name.
-   * 
-   * @param name
-   *          Comparator name
-   * @return A RawComparable comparator.
-   */
-  static public Comparator<RawComparable> makeComparator(String name) {
-    return TFileMeta.makeComparator(name);
-  }
-
-  // Prevent the instantiation of TFiles
-  private TFile() {
-    // nothing
-  }
-
-  /**
-   * Get names of supported compression algorithms. The names are acceptable by
-   * TFile.Writer.
-   * 
-   * @return Array of strings, each represents a supported compression
-   *         algorithm. Currently, the following compression algorithms are
-   *         supported.
-   *         <ul>
-   *         <li>"none" - No compression.
-   *         <li>"lzo" - LZO compression.
-   *         <li>"gz" - GZIP compression.
-   *         </ul>
-   */
-  public static String[] getSupportedCompressionAlgorithms() {
-    return Compression.getSupportedAlgorithms();
-  }
-
-  /**
-   * TFile Writer.
-   */
-  public static class Writer implements Closeable {
-    // minimum compressed size for a block.
-    private final int sizeMinBlock;
-
-    // Meta blocks.
-    final TFileIndex tfileIndex;
-    final TFileMeta tfileMeta;
-
-    // reference to the underlying BCFile.
-    private BCFile.Writer writerBCF;
-
-    // current data block appender.
-    BlockAppender blkAppender;
-    long blkRecordCount;
-
-    // buffers for caching the key.
-    BoundedByteArrayOutputStream currentKeyBufferOS;
-    BoundedByteArrayOutputStream lastKeyBufferOS;
-
-    // buffer used by chunk codec
-    private byte[] valueBuffer;
-
-    /**
-     * Writer states. The state always transits in circles: READY -> IN_KEY ->
-     * END_KEY -> IN_VALUE -> READY.
-     */
-    private enum State {
-      READY, // Ready to start a new key-value pair insertion.
-      IN_KEY, // In the middle of key insertion.
-      END_KEY, // Key insertion complete, ready to insert value.
-      IN_VALUE, // In value insertion.
-      // ERROR, // Error encountered, cannot continue.
-      CLOSED, // TFile already closed.
-    };
-
-    // current state of Writer.
-    State state = State.READY;
-    Configuration conf;
-    long errorCount = 0;
-
-    /**
-     * Constructor
-     * 
-     * @param fsdos
-     *          output stream for writing. Must be at position 0.
-     * @param minBlockSize
-     *          Minimum compressed block size in bytes. A compression block will
-     *          not be closed until it reaches this size except for the last
-     *          block.
-     * @param compressName
-     *          Name of the compression algorithm. Must be one of the strings
-     *          returned by {@link TFile#getSupportedCompressionAlgorithms()}.
-     * @param comparator
-     *          Leave comparator as null or empty string if TFile is not sorted.
-     *          Otherwise, provide the string name for the comparison algorithm
-     *          for keys. Two kinds of comparators are supported.
-     *          <ul>
-     *          <li>Algorithmic comparator: binary comparators that is language
-     *          independent. Currently, only "memcmp" is supported.
-     *          <li>Language-specific comparator: binary comparators that can
-     *          only be constructed in specific language. For Java, the syntax
-     *          is "jclass:", followed by the class name of the RawComparator.
-     *          Currently, we only support RawComparators that can be
-     *          constructed through the default constructor (with no
-     *          parameters). Parameterized RawComparators such as
-     *          {@link WritableComparator} or
-     *          {@link JavaSerializationComparator} may not be directly used.
-     *          One should write a wrapper class that inherits from such classes
-     *          and use its default constructor to perform proper
-     *          initialization.
-     *          </ul>
-     * @param conf
-     *          The configuration object.
-     * @throws IOException
-     */
-    public Writer(FSDataOutputStream fsdos, int minBlockSize,
-        String compressName, String comparator, Configuration conf)
-        throws IOException {
-      sizeMinBlock = minBlockSize;
-      tfileMeta = new TFileMeta(comparator);
-      tfileIndex = new TFileIndex(tfileMeta.getComparator());
-
-      writerBCF = new BCFile.Writer(fsdos, compressName, conf);
-      currentKeyBufferOS = new BoundedByteArrayOutputStream(MAX_KEY_SIZE);
-      lastKeyBufferOS = new BoundedByteArrayOutputStream(MAX_KEY_SIZE);
-      this.conf = conf;
-    }
-
-    /**
-     * Close the Writer. Resources will be released regardless of the exceptions
-     * being thrown. Future close calls will have no effect.
-     * 
-     * The underlying FSDataOutputStream is not closed.
-     */
-    public void close() throws IOException {
-      if ((state == State.CLOSED)) {
-        return;
-      }
-      try {
-        // First try the normal finish.
-        // Terminate upon the first Exception.
-        if (errorCount == 0) {
-          if (state != State.READY) {
-            throw new IllegalStateException(
-                "Cannot close TFile in the middle of key-value insertion.");
-          }
-
-          finishDataBlock(true);
-
-          // first, write out data:TFile.meta
-          BlockAppender outMeta =
-              writerBCF
-                  .prepareMetaBlock(TFileMeta.BLOCK_NAME, COMPRESSION_NONE);
-          try {
-            tfileMeta.write(outMeta);
-          } finally {
-            outMeta.close();
-          }
-
-          // second, write out data:TFile.index
-          BlockAppender outIndex =
-              writerBCF.prepareMetaBlock(TFileIndex.BLOCK_NAME);
-          try {
-            tfileIndex.write(outIndex);
-          } finally {
-            outIndex.close();
-          }
-
-          writerBCF.close();
-        }
-      } finally {
-        IOUtils.cleanup(LOG, blkAppender, writerBCF);
-        blkAppender = null;
-        writerBCF = null;
-        state = State.CLOSED;
-      }
-    }
-
-    /**
-     * Adding a new key-value pair to the TFile. This is synonymous to
-     * append(key, 0, key.length, value, 0, value.length)
-     * 
-     * @param key
-     *          Buffer for key.
-     * @param value
-     *          Buffer for value.
-     * @throws IOException
-     */
-    public void append(byte[] key, byte[] value) throws IOException {
-      append(key, 0, key.length, value, 0, value.length);
-    }
-
-    /**
-     * Adding a new key-value pair to TFile.
-     * 
-     * @param key
-     *          buffer for key.
-     * @param koff
-     *          offset in key buffer.
-     * @param klen
-     *          length of key.
-     * @param value
-     *          buffer for value.
-     * @param voff
-     *          offset in value buffer.
-     * @param vlen
-     *          length of value.
-     * @throws IOException
-     *           Upon IO errors.
-     *           <p>
-     *           If an exception is thrown, the TFile will be in an inconsistent
-     *           state. The only legitimate call after that would be close
-     */
-    public void append(byte[] key, int koff, int klen, byte[] value, int voff,
-        int vlen) throws IOException {
-      if ((koff | klen | (koff + klen) | (key.length - (koff + klen))) < 0) {
-        throw new IndexOutOfBoundsException(
-            "Bad key buffer offset-length combination.");
-      }
-
-      if ((voff | vlen | (voff + vlen) | (value.length - (voff + vlen))) < 0) {
-        throw new IndexOutOfBoundsException(
-            "Bad value buffer offset-length combination.");
-      }
-
-      try {
-        DataOutputStream dosKey = prepareAppendKey(klen);
-        try {
-          ++errorCount;
-          dosKey.write(key, koff, klen);
-          --errorCount;
-        } finally {
-          dosKey.close();
-        }
-
-        DataOutputStream dosValue = prepareAppendValue(vlen);
-        try {
-          ++errorCount;
-          dosValue.write(value, voff, vlen);
-          --errorCount;
-        } finally {
-          dosValue.close();
-        }
-      } finally {
-        state = State.READY;
-      }
-    }
-
-    /**
-     * Helper class to register key after close call on key append stream.
-     */
-    private class KeyRegister extends DataOutputStream {
-      private final int expectedLength;
-      private boolean closed = false;
-
-      public KeyRegister(int len) {
-        super(currentKeyBufferOS);
-        if (len >= 0) {
-          currentKeyBufferOS.reset(len);
-        } else {
-          currentKeyBufferOS.reset();
-        }
-        expectedLength = len;
-      }
-
-      @Override
-      public void close() throws IOException {
-        if (closed == true) {
-          return;
-        }
-
-        try {
-          ++errorCount;
-          byte[] key = currentKeyBufferOS.getBuffer();
-          int len = currentKeyBufferOS.size();
-          /**
-           * verify length.
-           */
-          if (expectedLength >= 0 && expectedLength != len) {
-            throw new IOException("Incorrect key length: expected="
-                + expectedLength + " actual=" + len);
-          }
-
-          Utils.writeVInt(blkAppender, len);
-          blkAppender.write(key, 0, len);
-          if (tfileIndex.getFirstKey() == null) {
-            tfileIndex.setFirstKey(key, 0, len);
-          }
-
-          if (tfileMeta.isSorted()) {
-            byte[] lastKey = lastKeyBufferOS.getBuffer();
-            int lastLen = lastKeyBufferOS.size();
-            if (tfileMeta.getComparator().compare(key, 0, len, lastKey, 0,
-                lastLen) < 0) {
-              throw new IOException("Keys are not added in sorted order");
-            }
-          }
-
-          BoundedByteArrayOutputStream tmp = currentKeyBufferOS;
-          currentKeyBufferOS = lastKeyBufferOS;
-          lastKeyBufferOS = tmp;
-          --errorCount;
-        } finally {
-          closed = true;
-          state = State.END_KEY;
-        }
-      }
-    }
-
-    /**
-     * Helper class to register value after close call on value append stream.
-     */
-    private class ValueRegister extends DataOutputStream {
-      private boolean closed = false;
-
-      public ValueRegister(OutputStream os) {
-        super(os);
-      }
-
-      // Avoiding flushing call to down stream.
-      @Override
-      public void flush() {
-        // do nothing
-      }
-
-      @Override
-      public void close() throws IOException {
-        if (closed == true) {
-          return;
-        }
-
-        try {
-          ++errorCount;
-          super.close();
-          blkRecordCount++;
-          // bump up the total record count in the whole file
-          tfileMeta.incRecordCount();
-          finishDataBlock(false);
-          --errorCount;
-        } finally {
-          closed = true;
-          state = State.READY;
-        }
-      }
-    }
-
-    /**
-     * Obtain an output stream for writing a key into TFile. This may only be
-     * called when there is no active Key appending stream or value appending
-     * stream.
-     * 
-     * @param length
-     *          The expected length of the key. If length of the key is not
-     *          known, set length = -1. Otherwise, the application must write
-     *          exactly as many bytes as specified here before calling close on
-     *          the returned output stream.
-     * @return The key appending output stream.
-     * @throws IOException
-     * 
-     */
-    public DataOutputStream prepareAppendKey(int length) throws IOException {
-      if (state != State.READY) {
-        throw new IllegalStateException("Incorrect state to start a new key: "
-            + state.name());
-      }
-
-      initDataBlock();
-      DataOutputStream ret = new KeyRegister(length);
-      state = State.IN_KEY;
-      return ret;
-    }
-
-    /**
-     * Obtain an output stream for writing a value into TFile. This may only be
-     * called right after a key appending operation (the key append stream must
-     * be closed).
-     * 
-     * @param length
-     *          The expected length of the value. If length of the value is not
-     *          known, set length = -1. Otherwise, the application must write
-     *          exactly as many bytes as specified here before calling close on
-     *          the returned output stream. Advertising the value size up-front
-     *          guarantees that the value is encoded in one chunk, and avoids
-     *          intermediate chunk buffering.
-     * @throws IOException
-     * 
-     */
-    public DataOutputStream prepareAppendValue(int length) throws IOException {
-      if (state != State.END_KEY) {
-        throw new IllegalStateException(
-            "Incorrect state to start a new value: " + state.name());
-      }
-
-      DataOutputStream ret;
-
-      // unknown length
-      if (length < 0) {
-        if (valueBuffer == null) {
-          valueBuffer = new byte[getChunkBufferSize(conf)];
-        }
-        ret = new ValueRegister(new ChunkEncoder(blkAppender, valueBuffer));
-      } else {
-        ret =
-            new ValueRegister(new Chunk.SingleChunkEncoder(blkAppender, length));
-      }
-
-      state = State.IN_VALUE;
-      return ret;
-    }
-
-    /**
-     * Obtain an output stream for creating a meta block. This function may not
-     * be called when there is a key append stream or value append stream
-     * active. No more key-value insertion is allowed after a meta data block
-     * has been added to TFile.
-     * 
-     * @param name
-     *          Name of the meta block.
-     * @param compressName
-     *          Name of the compression algorithm to be used. Must be one of the
-     *          strings returned by
-     *          {@link TFile#getSupportedCompressionAlgorithms()}.
-     * @return A DataOutputStream that can be used to write Meta Block data.
-     *         Closing the stream would signal the ending of the block.
-     * @throws IOException
-     * @throws MetaBlockAlreadyExists
-     *           the Meta Block with the same name already exists.
-     */
-    public DataOutputStream prepareMetaBlock(String name, String compressName)
-        throws IOException, MetaBlockAlreadyExists {
-      if (state != State.READY) {
-        throw new IllegalStateException(
-            "Incorrect state to start a Meta Block: " + state.name());
-      }
-
-      finishDataBlock(true);
-      DataOutputStream outputStream =
-          writerBCF.prepareMetaBlock(name, compressName);
-      return outputStream;
-    }
-
-    /**
-     * Obtain an output stream for creating a meta block. This function may not
-     * be called when there is a key append stream or value append stream
-     * active. No more key-value insertion is allowed after a meta data block
-     * has been added to TFile. Data will be compressed using the default
-     * compressor as defined in Writer's constructor.
-     * 
-     * @param name
-     *          Name of the meta block.
-     * @return A DataOutputStream that can be used to write Meta Block data.
-     *         Closing the stream would signal the ending of the block.
-     * @throws IOException
-     * @throws MetaBlockAlreadyExists
-     *           the Meta Block with the same name already exists.
-     */
-    public DataOutputStream prepareMetaBlock(String name) throws IOException,
-        MetaBlockAlreadyExists {
-      if (state != State.READY) {
-        throw new IllegalStateException(
-            "Incorrect state to start a Meta Block: " + state.name());
-      }
-
-      finishDataBlock(true);
-      return writerBCF.prepareMetaBlock(name);
-    }
-
-    /**
-     * Check if we need to start a new data block.
-     * 
-     * @throws IOException
-     */
-    private void initDataBlock() throws IOException {
-      // for each new block, get a new appender
-      if (blkAppender == null) {
-        blkAppender = writerBCF.prepareDataBlock();
-      }
-    }
-
-    /**
-     * Close the current data block if necessary.
-     * 
-     * @param bForceFinish
-     *          Force the closure regardless of the block size.
-     * @throws IOException
-     */
-    void finishDataBlock(boolean bForceFinish) throws IOException {
-      if (blkAppender == null) {
-        return;
-      }
-
-      // exceeded the size limit, do the compression and finish the block
-      if (bForceFinish || blkAppender.getCompressedSize() >= sizeMinBlock) {
-        // keep tracks of the last key of each data block, no padding
-        // for now
-        TFileIndexEntry keyLast =
-            new TFileIndexEntry(lastKeyBufferOS.getBuffer(), 0, lastKeyBufferOS
-                .size(), blkRecordCount);
-        tfileIndex.addEntry(keyLast);
-        // close the appender
-        blkAppender.close();
-        blkAppender = null;
-        blkRecordCount = 0;
-      }
-    }
-  }
-
-  /**
-   * TFile Reader. Users may only read TFiles by creating TFile.Reader.Scanner.
-   * objects. A scanner may scan the whole TFile ({@link Reader#createScanner()}
-   * ) , a portion of TFile based on byte offsets (
-   * {@link Reader#createScannerByByteRange(long, long)}), or a portion of TFile with keys
-   * fall in a certain key range (for sorted TFile only,
-   * {@link Reader#createScannerByKey(byte[], byte[])} or
-   * {@link Reader#createScannerByKey(RawComparable, RawComparable)}).
-   */
-  public static class Reader implements Closeable {
-    // The underlying BCFile reader.
-    final BCFile.Reader readerBCF;
-
-    // TFile index, it is loaded lazily.
-    TFileIndex tfileIndex = null;
-    final TFileMeta tfileMeta;
-    final BytesComparator comparator;
-
-    // global begin and end locations.
-    private final Location begin;
-    private final Location end;
-
-    /**
-     * Location representing a virtual position in the TFile.
-     */
-    static final class Location implements Comparable<Location>, Cloneable {
-      private int blockIndex;
-      // distance/offset from the beginning of the block
-      private long recordIndex;
-
-      Location(int blockIndex, long recordIndex) {
-        set(blockIndex, recordIndex);
-      }
-
-      void incRecordIndex() {
-        ++recordIndex;
-      }
-
-      Location(Location other) {
-        set(other);
-      }
-
-      int getBlockIndex() {
-        return blockIndex;
-      }
-
-      long getRecordIndex() {
-        return recordIndex;
-      }
-
-      void set(int blockIndex, long recordIndex) {
-        if ((blockIndex | recordIndex) < 0) {
-          throw new IllegalArgumentException(
-              "Illegal parameter for BlockLocation.");
-        }
-        this.blockIndex = blockIndex;
-        this.recordIndex = recordIndex;
-      }
-
-      void set(Location other) {
-        set(other.blockIndex, other.recordIndex);
-      }
-
-      /**
-       * @see java.lang.Comparable#compareTo(java.lang.Object)
-       */
-      @Override
-      public int compareTo(Location other) {
-        return compareTo(other.blockIndex, other.recordIndex);
-      }
-
-      int compareTo(int bid, long rid) {
-        if (this.blockIndex == bid) {
-          long ret = this.recordIndex - rid;
-          if (ret > 0) return 1;
-          if (ret < 0) return -1;
-          return 0;
-        }
-        return this.blockIndex - bid;
-      }
-
-      /**
-       * @see java.lang.Object#clone()
-       */
-      @Override
-      protected Location clone() {
-        return new Location(blockIndex, recordIndex);
-      }
-
-      /**
-       * @see java.lang.Object#hashCode()
-       */
-      @Override
-      public int hashCode() {
-        final int prime = 31;
-        int result = prime + blockIndex;
-        result = (int) (prime * result + recordIndex);
-        return result;
-      }
-
-      /**
-       * @see java.lang.Object#equals(java.lang.Object)
-       */
-      @Override
-      public boolean equals(Object obj) {
-        if (this == obj) return true;
-        if (obj == null) return false;
-        if (getClass() != obj.getClass()) return false;
-        Location other = (Location) obj;
-        if (blockIndex != other.blockIndex) return false;
-        if (recordIndex != other.recordIndex) return false;
-        return true;
-      }
-    }
-
-    /**
-     * Constructor
-     * 
-     * @param fsdis
-     *          FS input stream of the TFile.
-     * @param fileLength
-     *          The length of TFile. This is required because we have no easy
-     *          way of knowing the actual size of the input file through the
-     *          File input stream.
-     * @param conf
-     * @throws IOException
-     */
-    public Reader(FSDataInputStream fsdis, long fileLength, Configuration conf)
-        throws IOException {
-      readerBCF = new BCFile.Reader(fsdis, fileLength, conf);
-
-      // first, read TFile meta
-      BlockReader brMeta = readerBCF.getMetaBlock(TFileMeta.BLOCK_NAME);
-      try {
-        tfileMeta = new TFileMeta(brMeta);
-      } finally {
-        brMeta.close();
-      }
-
-      comparator = tfileMeta.getComparator();
-      // Set begin and end locations.
-      begin = new Location(0, 0);
-      end = new Location(readerBCF.getBlockCount(), 0);
-    }
-
-    /**
-     * Close the reader. The state of the Reader object is undefined after
-     * close. Calling close() for multiple times has no effect.
-     */
-    public void close() throws IOException {
-      readerBCF.close();
-    }
-
-    /**
-     * Get the begin location of the TFile.
-     * 
-     * @return If TFile is not empty, the location of the first key-value pair.
-     *         Otherwise, it returns end().
-     */
-    Location begin() {
-      return begin;
-    }
-
-    /**
-     * Get the end location of the TFile.
-     * 
-     * @return The location right after the last key-value pair in TFile.
-     */
-    Location end() {
-      return end;
-    }
-
-    /**
-     * Get the string representation of the comparator.
-     * 
-     * @return If the TFile is not sorted by keys, an empty string will be
-     *         returned. Otherwise, the actual comparator string that is
-     *         provided during the TFile creation time will be returned.
-     */
-    public String getComparatorName() {
-      return tfileMeta.getComparatorString();
-    }
-
-    /**
-     * Is the TFile sorted?
-     * 
-     * @return true if TFile is sorted.
-     */
-    public boolean isSorted() {
-      return tfileMeta.isSorted();
-    }
-
-    /**
-     * Get the number of key-value pair entries in TFile.
-     * 
-     * @return the number of key-value pairs in TFile
-     */
-    public long getEntryCount() {
-      return tfileMeta.getRecordCount();
-    }
-
-    /**
-     * Lazily loading the TFile index.
-     * 
-     * @throws IOException
-     */
-    synchronized void checkTFileDataIndex() throws IOException {
-      if (tfileIndex == null) {
-        BlockReader brIndex = readerBCF.getMetaBlock(TFileIndex.BLOCK_NAME);
-        try {
-          tfileIndex =
-              new TFileIndex(readerBCF.getBlockCount(), brIndex, tfileMeta
-                  .getComparator());
-        } finally {
-          brIndex.close();
-        }
-      }
-    }
-
-    /**
-     * Get the first key in the TFile.
-     * 
-     * @return The first key in the TFile.
-     * @throws IOException
-     */
-    public RawComparable getFirstKey() throws IOException {
-      checkTFileDataIndex();
-      return tfileIndex.getFirstKey();
-    }
-
-    /**
-     * Get the last key in the TFile.
-     * 
-     * @return The last key in the TFile.
-     * @throws IOException
-     */
-    public RawComparable getLastKey() throws IOException {
-      checkTFileDataIndex();
-      return tfileIndex.getLastKey();
-    }
-
-    /**
-     * Get a Comparator object to compare Entries. It is useful when you want
-     * stores the entries in a collection (such as PriorityQueue) and perform
-     * sorting or comparison among entries based on the keys without copying out
-     * the key.
-     * 
-     * @return An Entry Comparator..
-     */
-    public Comparator<Scanner.Entry> getEntryComparator() {
-      if (!isSorted()) {
-        throw new RuntimeException(
-            "Entries are not comparable for unsorted TFiles");
-      }
-
-      return new Comparator<Scanner.Entry>() {
-        /**
-         * Provide a customized comparator for Entries. This is useful if we
-         * have a collection of Entry objects. However, if the Entry objects
-         * come from different TFiles, users must ensure that those TFiles share
-         * the same RawComparator.
-         */
-        @Override
-        public int compare(Scanner.Entry o1, Scanner.Entry o2) {
-          return comparator.compare(o1.getKeyBuffer(), 0, o1.getKeyLength(), o2
-              .getKeyBuffer(), 0, o2.getKeyLength());
-        }
-      };
-    }
-
-    /**
-     * Get an instance of the RawComparator that is constructed based on the
-     * string comparator representation.
-     * 
-     * @return a Comparator that can compare RawComparable's.
-     */
-    public Comparator<RawComparable> getComparator() {
-      return comparator;
-    }
-
-    /**
-     * Stream access to a meta block.``
-     * 
-     * @param name
-     *          The name of the meta block.
-     * @return The input stream.
-     * @throws IOException
-     *           on I/O error.
-     * @throws MetaBlockDoesNotExist
-     *           If the meta block with the name does not exist.
-     */
-    public DataInputStream getMetaBlock(String name) throws IOException,
-        MetaBlockDoesNotExist {
-      return readerBCF.getMetaBlock(name);
-    }
-
-    /**
-     * if greater is true then returns the beginning location of the block
-     * containing the key strictly greater than input key. if greater is false
-     * then returns the beginning location of the block greater than equal to
-     * the input key
-     * 
-     * @param key
-     *          the input key
-     * @param greater
-     *          boolean flag
-     * @return
-     * @throws IOException
-     */
-    Location getBlockContainsKey(RawComparable key, boolean greater)
-        throws IOException {
-      if (!isSorted()) {
-        throw new RuntimeException("Seeking in unsorted TFile");
-      }
-      checkTFileDataIndex();
-      int blkIndex =
-          (greater) ? tfileIndex.upperBound(key) : tfileIndex.lowerBound(key);
-      if (blkIndex < 0) return end;
-      return new Location(blkIndex, 0);
-    }
-
-    Location getLocationByRecordNum(long recNum) throws IOException {
-      checkTFileDataIndex();
-      return tfileIndex.getLocationByRecordNum(recNum);
-    }
-
-    long getRecordNumByLocation(Location location) throws IOException {
-      checkTFileDataIndex();
-      return tfileIndex.getRecordNumByLocation(location);      
-    }
-    
-    int compareKeys(byte[] a, int o1, int l1, byte[] b, int o2, int l2) {
-      if (!isSorted()) {
-        throw new RuntimeException("Cannot compare keys for unsorted TFiles.");
-      }
-      return comparator.compare(a, o1, l1, b, o2, l2);
-    }
-
-    int compareKeys(RawComparable a, RawComparable b) {
-      if (!isSorted()) {
-        throw new RuntimeException("Cannot compare keys for unsorted TFiles.");
-      }
-      return comparator.compare(a, b);
-    }
-
-    /**
-     * Get the location pointing to the beginning of the first key-value pair in
-     * a compressed block whose byte offset in the TFile is greater than or
-     * equal to the specified offset.
-     * 
-     * @param offset
-     *          the user supplied offset.
-     * @return the location to the corresponding entry; or end() if no such
-     *         entry exists.
-     */
-    Location getLocationNear(long offset) {
-      int blockIndex = readerBCF.getBlockIndexNear(offset);
-      if (blockIndex == -1) return end;
-      return new Location(blockIndex, 0);
-    }
-
-    /**
-     * Get the RecordNum for the first key-value pair in a compressed block
-     * whose byte offset in the TFile is greater than or equal to the specified
-     * offset.
-     * 
-     * @param offset
-     *          the user supplied offset.
-     * @return the RecordNum to the corresponding entry. If no such entry
-     *         exists, it returns the total entry count.
-     * @throws IOException
-     */
-    public long getRecordNumNear(long offset) throws IOException {
-      return getRecordNumByLocation(getLocationNear(offset));
-    }
-    
-    /**
-     * Get a sample key that is within a block whose starting offset is greater
-     * than or equal to the specified offset.
-     * 
-     * @param offset
-     *          The file offset.
-     * @return the key that fits the requirement; or null if no such key exists
-     *         (which could happen if the offset is close to the end of the
-     *         TFile).
-     * @throws IOException
-     */
-    public RawComparable getKeyNear(long offset) throws IOException {
-      int blockIndex = readerBCF.getBlockIndexNear(offset);
-      if (blockIndex == -1) return null;
-      checkTFileDataIndex();
-      return new ByteArray(tfileIndex.getEntry(blockIndex).key);
-    }
-
-    public long getOffsetForKey(RawComparable key) throws IOException {
-      Location l = getBlockContainsKey(key, false);
-      int blockIndex = l.getBlockIndex();
-      if (blockIndex == end.blockIndex)
-      {
-        if (blockIndex > 0)
-        {
-          BCFile.Reader.BlockReader blkReader = readerBCF.getDataBlock(blockIndex - 1);
-          return blkReader.getStartPos() + blkReader.getCompressedSize();
-        } else
-          return 0;
-      } else
-        return readerBCF.getDataBlock(blockIndex).getStartPos();
-    }
-    
-    public long getLastDataOffset() throws IOException {
-      BCFile.Reader.BlockReader blkReader = readerBCF.getDataBlock(end.blockIndex - 1);
-      return blkReader.getStartPos() + blkReader.getCompressedSize();
-    }
-    /**
-     * Get a scanner than can scan the whole TFile.
-     * 
-     * @return The scanner object. A valid Scanner is always returned even if
-     *         the TFile is empty.
-     * @throws IOException
-     */
-    public Scanner createScanner() throws IOException {
-      return new Scanner(this, begin, end);
-    }
-
-    /**
-     * Get a scanner that covers a portion of TFile based on byte offsets.
-     * 
-     * @param offset
-     *          The beginning byte offset in the TFile.
-     * @param length
-     *          The length of the region.
-     * @return The actual coverage of the returned scanner tries to match the
-     *         specified byte-region but always round up to the compression
-     *         block boundaries. It is possible that the returned scanner
-     *         contains zero key-value pairs even if length is positive.
-     * @throws IOException
-     */
-    public Scanner createScannerByByteRange(long offset, long length) throws IOException {
-      return new Scanner(this, offset, offset + length);
-    }
-
-    /**
-     * Get a scanner that covers a portion of TFile based on keys.
-     * 
-     * @param beginKey
-     *          Begin key of the scan (inclusive). If null, scan from the first
-     *          key-value entry of the TFile.
-     * @param endKey
-     *          End key of the scan (exclusive). If null, scan up to the last
-     *          key-value entry of the TFile.
-     * @return The actual coverage of the returned scanner will cover all keys
-     *         greater than or equal to the beginKey and less than the endKey.
-     * @throws IOException
-     * 
-     * @deprecated Use {@link #createScannerByKey(byte[], byte[])} instead.
-     */
-    @Deprecated
-    public Scanner createScanner(byte[] beginKey, byte[] endKey)
-      throws IOException {
-      return createScannerByKey(beginKey, endKey);
-    }
-    
-    /**
-     * Get a scanner that covers a portion of TFile based on keys.
-     * 
-     * @param beginKey
-     *          Begin key of the scan (inclusive). If null, scan from the first
-     *          key-value entry of the TFile.
-     * @param endKey
-     *          End key of the scan (exclusive). If null, scan up to the last
-     *          key-value entry of the TFile.
-     * @return The actual coverage of the returned scanner will cover all keys
-     *         greater than or equal to the beginKey and less than the endKey.
-     * @throws IOException
-     */
-    public Scanner createScannerByKey(byte[] beginKey, byte[] endKey)
-        throws IOException {
-      return createScannerByKey((beginKey == null) ? null : new ByteArray(beginKey,
-          0, beginKey.length), (endKey == null) ? null : new ByteArray(endKey,
-          0, endKey.length));
-    }
-
-    /**
-     * Get a scanner that covers a specific key range.
-     * 
-     * @param beginKey
-     *          Begin key of the scan (inclusive). If null, scan from the first
-     *          key-value entry of the TFile.
-     * @param endKey
-     *          End key of the scan (exclusive). If null, scan up to the last
-     *          key-value entry of the TFile.
-     * @return The actual coverage of the returned scanner will cover all keys
-     *         greater than or equal to the beginKey and less than the endKey.
-     * @throws IOException
-     * 
-     * @deprecated Use {@link #createScannerByKey(RawComparable, RawComparable)}
-     *             instead.
-     */
-    @Deprecated
-    public Scanner createScanner(RawComparable beginKey, RawComparable endKey)
-        throws IOException {
-      return createScannerByKey(beginKey, endKey);
-    }
-
-    /**
-     * Get a scanner that covers a specific key range.
-     * 
-     * @param beginKey
-     *          Begin key of the scan (inclusive). If null, scan from the first
-     *          key-value entry of the TFile.
-     * @param endKey
-     *          End key of the scan (exclusive). If null, scan up to the last
-     *          key-value entry of the TFile.
-     * @return The actual coverage of the returned scanner will cover all keys
-     *         greater than or equal to the beginKey and less than the endKey.
-     * @throws IOException
-     */
-    public Scanner createScannerByKey(RawComparable beginKey, RawComparable endKey)
-        throws IOException {
-      if ((beginKey != null) && (endKey != null)
-          && (compareKeys(beginKey, endKey) >= 0)) {
-        return new Scanner(this, beginKey, beginKey);
-      }
-      return new Scanner(this, beginKey, endKey);
-    }
-
-    /**
-     * Create a scanner that covers a range of records.
-     * 
-     * @param beginRecNum
-     *          The RecordNum for the first record (inclusive).
-     * @param endRecNum
-     *          The RecordNum for the last record (exclusive). To scan the whole
-     *          file, either specify endRecNum==-1 or endRecNum==getEntryCount().
-     * @return The TFile scanner that covers the specified range of records.
-     * @throws IOException
-     */
-    public Scanner createScannerByRecordNum(long beginRecNum, long endRecNum)
-        throws IOException {
-      if (beginRecNum < 0) beginRecNum = 0;
-      if (endRecNum < 0 || endRecNum > getEntryCount()) {
-        endRecNum = getEntryCount();
-      }
-      return new Scanner(this, getLocationByRecordNum(beginRecNum),
-          getLocationByRecordNum(endRecNum));
-    }
-
-    /**
-     * The TFile Scanner. The Scanner has an implicit cursor, which, upon
-     * creation, points to the first key-value pair in the scan range. If the
-     * scan range is empty, the cursor will point to the end of the scan range.
-     * <p>
-     * Use {@link Scanner#atEnd()} to test whether the cursor is at the end
-     * location of the scanner.
-     * <p>
-     * Use {@link Scanner#advance()} to move the cursor to the next key-value
-     * pair (or end if none exists). Use seekTo methods (
-     * {@link Scanner#seekTo(byte[])} or
-     * {@link Scanner#seekTo(byte[], int, int)}) to seek to any arbitrary
-     * location in the covered range (including backward seeking). Use
-     * {@link Scanner#rewind()} to seek back to the beginning of the scanner.
-     * Use {@link Scanner#seekToEnd()} to seek to the end of the scanner.
-     * <p>
-     * Actual keys and values may be obtained through {@link Scanner.Entry}
-     * object, which is obtained through {@link Scanner#entry()}.
-     */
-    public static class Scanner implements Closeable {
-      // The underlying TFile reader.
-      final Reader reader;
-      // current block (null if reaching end)
-      private BlockReader blkReader;
-
-      Location beginLocation;
-      Location endLocation;
-      Location currentLocation;
-
-      // flag to ensure value is only examined once.
-      boolean valueChecked = false;
-      // reusable buffer for keys.
-      final byte[] keyBuffer;
-      // length of key, -1 means key is invalid.
-      int klen = -1;
-
-      static final int MAX_VAL_TRANSFER_BUF_SIZE = 128 * 1024;
-      BytesWritable valTransferBuffer;
-
-      DataInputBuffer keyDataInputStream;
-      ChunkDecoder valueBufferInputStream;
-      DataInputStream valueDataInputStream;
-      // vlen == -1 if unknown.
-      int vlen;
-
-      /**
-       * Constructor
-       * 
-       * @param reader
-       *          The TFile reader object.
-       * @param offBegin
-       *          Begin byte-offset of the scan.
-       * @param offEnd
-       *          End byte-offset of the scan.
-       * @throws IOException
-       * 
-       *           The offsets will be rounded to the beginning of a compressed
-       *           block whose offset is greater than or equal to the specified
-       *           offset.
-       */
-      protected Scanner(Reader reader, long offBegin, long offEnd)
-          throws IOException {
-        this(reader, reader.getLocationNear(offBegin), reader
-            .getLocationNear(offEnd));
-      }
-
-      /**
-       * Constructor
-       * 
-       * @param reader
-       *          The TFile reader object.
-       * @param begin
-       *          Begin location of the scan.
-       * @param end
-       *          End location of the scan.
-       * @throws IOException
-       */
-      Scanner(Reader reader, Location begin, Location end) throws IOException {
-        this.reader = reader;
-        // ensure the TFile index is loaded throughout the life of scanner.
-        reader.checkTFileDataIndex();
-        beginLocation = begin;
-        endLocation = end;
-
-        valTransferBuffer = new BytesWritable();
-        // TODO: remember the longest key in a TFile, and use it to replace
-        // MAX_KEY_SIZE.
-        keyBuffer = new byte[MAX_KEY_SIZE];
-        keyDataInputStream = new DataInputBuffer();
-        valueBufferInputStream = new ChunkDecoder();
-        valueDataInputStream = new DataInputStream(valueBufferInputStream);
-
-        if (beginLocation.compareTo(endLocation) >= 0) {
-          currentLocation = new Location(endLocation);
-        } else {
-          currentLocation = new Location(0, 0);
-          initBlock(beginLocation.getBlockIndex());
-          inBlockAdvance(beginLocation.getRecordIndex());
-        }
-      }
-
-      /**
-       * Constructor
-       * 
-       * @param reader
-       *          The TFile reader object.
-       * @param beginKey
-       *          Begin key of the scan. If null, scan from the first <K,V>
-       *          entry of the TFile.
-       * @param endKey
-       *          End key of the scan. If null, scan up to the last <K, V> entry
-       *          of the TFile.
-       * @throws IOException
-       */
-      protected Scanner(Reader reader, RawComparable beginKey,
-          RawComparable endKey) throws IOException {
-        this(reader, (beginKey == null) ? reader.begin() : reader
-            .getBlockContainsKey(beginKey, false), reader.end());
-        if (beginKey != null) {
-          inBlockAdvance(beginKey, false);
-          beginLocation.set(currentLocation);
-        }
-        if (endKey != null) {
-          seekTo(endKey, false);
-          endLocation.set(currentLocation);
-          seekTo(beginLocation);
-        }
-      }
-
-      /**
-       * Move the cursor to the first entry whose key is greater than or equal
-       * to the input key. Synonymous to seekTo(key, 0, key.length). The entry
-       * returned by the previous entry() call will be invalid.
-       * 
-       * @param key
-       *          The input key
-       * @return true if we find an equal key.
-       * @throws IOException
-       */
-      public boolean seekTo(byte[] key) throws IOException {
-        return seekTo(key, 0, key.length);
-      }
-
-      /**
-       * Move the cursor to the first entry whose key is greater than or equal
-       * to the input key. The entry returned by the previous entry() call will
-       * be invalid.
-       * 
-       * @param key
-       *          The input key
-       * @param keyOffset
-       *          offset in the key buffer.
-       * @param keyLen
-       *          key buffer length.
-       * @return true if we find an equal key; false otherwise.
-       * @throws IOException
-       */
-      public boolean seekTo(byte[] key, int keyOffset, int keyLen)
-          throws IOException {
-        return seekTo(new ByteArray(key, keyOffset, keyLen), false);
-      }
-
-      private boolean seekTo(RawComparable key, boolean beyond)
-          throws IOException {
-        Location l = reader.getBlockContainsKey(key, beyond);
-        if (l.compareTo(beginLocation) < 0) {
-          l = beginLocation;
-        } else if (l.compareTo(endLocation) >= 0) {
-          seekTo(endLocation);
-          return false;
-        }
-
-        // check if what we are seeking is in the later part of the current
-        // block.
-        if (atEnd() || (l.getBlockIndex() != currentLocation.getBlockIndex())
-            || (compareCursorKeyTo(key) >= 0)) {
-          // sorry, we must seek to a different location first.
-          seekTo(l);
-        }
-
-        return inBlockAdvance(key, beyond);
-      }
-
-      /**
-       * Move the cursor to the new location. The entry returned by the previous
-       * entry() call will be invalid.
-       * 
-       * @param l
-       *          new cursor location. It must fall between the begin and end
-       *          location of the scanner.
-       * @throws IOException
-       */
-      private void seekTo(Location l) throws IOException {
-        if (l.compareTo(beginLocation) < 0) {
-          throw new IllegalArgumentException(
-              "Attempt to seek before the begin location.");
-        }
-
-        if (l.compareTo(endLocation) > 0) {
-          throw new IllegalArgumentException(
-              "Attempt to seek after the end location.");
-        }
-
-        if (l.compareTo(endLocation) == 0) {
-          parkCursorAtEnd();
-          return;
-        }
-
-        if (l.getBlockIndex() != currentLocation.getBlockIndex()) {
-          // going to a totally different block
-          initBlock(l.getBlockIndex());
-        } else {
-          if (valueChecked) {
-            // may temporarily go beyond the last record in the block (in which
-            // case the next if loop will always be true).
-            inBlockAdvance(1);
-          }
-          if (l.getRecordIndex() < currentLocation.getRecordIndex()) {
-            initBlock(l.getBlockIndex());
-          }
-        }
-
-        inBlockAdvance(l.getRecordIndex() - currentLocation.getRecordIndex());
-
-        return;
-      }
-
-      /**
-       * Rewind to the first entry in the scanner. The entry returned by the
-       * previous entry() call will be invalid.
-       * 
-       * @throws IOException
-       */
-      public void rewind() throws IOException {
-        seekTo(beginLocation);
-      }
-
-      /**
-       * Seek to the end of the scanner. The entry returned by the previous
-       * entry() call will be invalid.
-       * 
-       * @throws IOException
-       */
-      public void seekToEnd() throws IOException {
-        parkCursorAtEnd();
-      }
-
-      /**
-       * Move the cursor to the first entry whose key is greater than or equal
-       * to the input key. Synonymous to lowerBound(key, 0, key.length). The
-       * entry returned by the previous entry() call will be invalid.
-       * 
-       * @param key
-       *          The input key
-       * @throws IOException
-       */
-      public void lowerBound(byte[] key) throws IOException {
-        lowerBound(key, 0, key.length);
-      }
-
-      /**
-       * Move the cursor to the first entry whose key is greater than or equal
-       * to the input key. The entry returned by the previous entry() call will
-       * be invalid.
-       * 
-       * @param key
-       *          The input key
-       * @param keyOffset
-       *          offset in the key buffer.
-       * @param keyLen
-       *          key buffer length.
-       * @throws IOException
-       */
-      public void lowerBound(byte[] key, int keyOffset, int keyLen)
-          throws IOException {
-        seekTo(new ByteArray(key, keyOffset, keyLen), false);
-      }
-
-      /**
-       * Move the cursor to the first entry whose key is strictly greater than
-       * the input key. Synonymous to upperBound(key, 0, key.length). The entry
-       * returned by the previous entry() call will be invalid.
-       * 
-       * @param key
-       *          The input key
-       * @throws IOException
-       */
-      public void upperBound(byte[] key) throws IOException {
-        upperBound(key, 0, key.length);
-      }
-
-      /**
-       * Move the cursor to the first entry whose key is strictly greater than
-       * the input key. The entry returned by the previous entry() call will be
-       * invalid.
-       * 
-       * @param key
-       *          The input key
-       * @param keyOffset
-       *          offset in the key buffer.
-       * @param keyLen
-       *          key buffer length.
-       * @throws IOException
-       */
-      public void upperBound(byte[] key, int keyOffset, int keyLen)
-          throws IOException {
-        seekTo(new ByteArray(key, keyOffset, keyLen), true);
-      }
-
-      /**
-       * Move the cursor to the next key-value pair. The entry returned by the
-       * previous entry() call will be invalid.
-       * 
-       * @return true if the cursor successfully moves. False when cursor is
-       *         already at the end location and cannot be advanced.
-       * @throws IOException
-       */
-      public boolean advance() throws IOException {
-        if (atEnd()) {
-          return false;
-        }
-
-        int curBid = currentLocation.getBlockIndex();
-        long curRid = currentLocation.getRecordIndex();
-        long entriesInBlock = reader.getBlockEntryCount(curBid);
-        if (curRid + 1 >= entriesInBlock) {
-          if (endLocation.compareTo(curBid + 1, 0) <= 0) {
-            // last entry in TFile.
-            parkCursorAtEnd();
-          } else {
-            // last entry in Block.
-            initBlock(curBid + 1);
-          }
-        } else {
-          inBlockAdvance(1);
-        }
-        return true;
-      }
-
-      /**
-       * Load a compressed block for reading. Expecting blockIndex is valid.
-       * 
-       * @throws IOException
-       */
-      private void initBlock(int blockIndex) throws IOException {
-        klen = -1;
-        if (blkReader != null) {
-          try {
-            blkReader.close();
-          } finally {
-            blkReader = null;
-          }
-        }
-        blkReader = reader.getBlockReader(blockIndex);
-        currentLocation.set(blockIndex, 0);
-      }
-
-      private void parkCursorAtEnd() throws IOException {
-        klen = -1;
-        currentLocation.set(endLocation);
-        if (blkReader != null) {
-          try {
-            blkReader.close();
-          } finally {
-            blkReader = null;
-          }
-        }
-      }
-
-      /**
-       * Close the scanner. Release all resources. The behavior of using the
-       * scanner after calling close is not defined. The entry returned by the
-       * previous entry() call will be invalid.
-       */
-      public void close() throws IOException {
-        parkCursorAtEnd();
-      }
-
-      /**
-       * Is cursor at the end location?
-       * 
-       * @return true if the cursor is at the end location.
-       */
-      public boolean atEnd() {
-        return (currentLocation.compareTo(endLocation) >= 0);
-      }
-
-      /**
-       * check whether we have already successfully obtained the key. It also
-       * initializes the valueInputStream.
-       */
-      void checkKey() throws IOException {
-        if (klen >= 0) return;
-        if (atEnd()) {
-          throw new EOFException("No key-value to read");
-        }
-        klen = -1;
-        vlen = -1;
-        valueChecked = false;
-
-        klen = Utils.readVInt(blkReader);
-        blkReader.readFully(keyBuffer, 0, klen);
-        valueBufferInputStream.reset(blkReader);
-        if (valueBufferInputStream.isLastChunk()) {
-          vlen = valueBufferInputStream.getRemain();
-        }
-      }
-
-      /**
-       * Get an entry to access the key and value.
-       * 
-       * @return The Entry object to access the key and value.
-       * @throws IOException
-       */
-      public Entry entry() throws IOException {
-        checkKey();
-        return new Entry();
-      }
-
-      /**
-       * Get the RecordNum corresponding to the entry pointed by the cursor.
-       * @return The RecordNum corresponding to the entry pointed by the cursor.
-       * @throws IOException
-       */
-      public long getRecordNum() throws IOException {
-        return reader.getRecordNumByLocation(currentLocation);
-      }
-      
-      /**
-       * Internal API. Comparing the key at cursor to user-specified key.
-       * 
-       * @param other
-       *          user-specified key.
-       * @return negative if key at cursor is smaller than user key; 0 if equal;
-       *         and positive if key at cursor greater than user key.
-       * @throws IOException
-       */
-      int compareCursorKeyTo(RawComparable other) throws IOException {
-        checkKey();
-        return reader.compareKeys(keyBuffer, 0, klen, other.buffer(), other
-            .offset(), other.size());
-      }
-
-      /**
-       * Entry to a &lt;Key, Value&gt; pair.
-       */
-      public class Entry implements Comparable<RawComparable> {
-        /**
-         * Get the length of the key.
-         * 
-         * @return the length of the key.
-         */
-        public int getKeyLength() {
-          return klen;
-        }
-
-        byte[] getKeyBuffer() {
-          return keyBuffer;
-        }
-
-        /**
-         * Copy the key and value in one shot into BytesWritables. This is
-         * equivalent to getKey(key); getValue(value);
-         * 
-         * @param key
-         *          BytesWritable to hold key.
-         * @param value
-         *          BytesWritable to hold value
-         * @throws IOException
-         */
-        public void get(BytesWritable key, BytesWritable value)
-            throws IOException {
-          getKey(key);
-          getValue(value);
-        }
-
-        /**
-         * Copy the key into BytesWritable. The input BytesWritable will be
-         * automatically resized to the actual key size.
-         * 
-         * @param key
-         *          BytesWritable to hold the key.
-         * @throws IOException
-         */
-        public int getKey(BytesWritable key) throws IOException {
-          key.setSize(getKeyLength());
-          getKey(key.getBytes());
-          return key.getLength();
-        }
-
-        /**
-         * Copy the value into BytesWritable. The input BytesWritable will be
-         * automatically resized to the actual value size. The implementation
-         * directly uses the buffer inside BytesWritable for storing the value.
-         * The call does not require the value length to be known.
-         * 
-         * @param value
-         * @throws IOException
-         */
-        public long getValue(BytesWritable value) throws IOException {
-          DataInputStream dis = getValueStream();
-          int size = 0;
-          try {
-            int remain;
-            while ((remain = valueBufferInputStream.getRemain()) > 0) {
-              value.setSize(size + remain);
-              dis.readFully(value.getBytes(), size, remain);
-              size += remain;
-            }
-            return value.getLength();
-          } finally {
-            dis.close();
-          }
-        }
-
-        /**
-         * Writing the key to the output stream. This method avoids copying key
-         * buffer from Scanner into user buffer, then writing to the output
-         * stream.
-         * 
-         * @param out
-         *          The output stream
-         * @return the length of the key.
-         * @throws IOException
-         */
-        public int writeKey(OutputStream out) throws IOException {
-          out.write(keyBuffer, 0, klen);
-          return klen;
-        }
-
-        /**
-         * Writing the value to the output stream. This method avoids copying
-         * value data from Scanner into user buffer, then writing to the output
-         * stream. It does not require the value length to be known.
-         * 
-         * @param out
-         *          The output stream
-         * @return the length of the value
-         * @throws IOException
-         */
-        public long writeValue(OutputStream out) throws IOException {
-          DataInputStream dis = getValueStream();
-          long size = 0;
-          try {
-            int chunkSize;
-            while ((chunkSize = valueBufferInputStream.getRemain()) > 0) {
-              chunkSize = Math.min(chunkSize, MAX_VAL_TRANSFER_BUF_SIZE);
-              valTransferBuffer.setSize(chunkSize);
-              dis.readFully(valTransferBuffer.getBytes(), 0, chunkSize);
-              out.write(valTransferBuffer.getBytes(), 0, chunkSize);
-              size += chunkSize;
-            }
-            return size;
-          } finally {
-            dis.close();
-          }
-        }
-
-        /**
-         * Copy the key into user supplied buffer.
-         * 
-         * @param buf
-         *          The buffer supplied by user. The length of the buffer must
-         *          not be shorter than the key length.
-         * @return The length of the key.
-         * 
-         * @throws IOException
-         */
-        public int getKey(byte[] buf) throws IOException {
-          return getKey(buf, 0);
-        }
-
-        /**
-         * Copy the key into user supplied buffer.
-         * 
-         * @param buf
-         *          The buffer supplied by user.
-         * @param offset
-         *          The starting offset of the user buffer where we should copy
-         *          the key into. Requiring the key-length + offset no greater
-         *          than the buffer length.
-         * @return The length of the key.
-         * @throws IOException
-         */
-        public int getKey(byte[] buf, int offset) throws IOException {
-          if ((offset | (buf.length - offset - klen)) < 0) {
-            throw new IndexOutOfBoundsException(
-                "Bufer not enough to store the key");
-          }
-          System.arraycopy(keyBuffer, 0, buf, offset, klen);
-          return klen;
-        }
-
-        /**
-         * Streaming access to the key. Useful for desrializing the key into
-         * user objects.
-         * 
-         * @return The input stream.
-         */
-        public DataInputStream getKeyStream() {
-          keyDataInputStream.reset(keyBuffer, klen);
-          return keyDataInputStream;
-        }
-
-        /**
-         * Get the length of the value. isValueLengthKnown() must be tested
-         * true.
-         * 
-         * @return the length of the value.
-         */
-        public int getValueLength() {
-          if (vlen >= 0) {
-            return vlen;
-          }
-
-          throw new RuntimeException("Value length unknown.");
-        }
-
-        /**
-         * Copy value into user-supplied buffer. User supplied buffer must be
-         * large enough to hold the whole value. The value part of the key-value
-         * pair pointed by the current cursor is not cached and can only be
-         * examined once. Calling any of the following functions more than once
-         * without moving the cursor will result in exception:
-         * {@link #getValue(byte[])}, {@link #getValue(byte[], int)},
-         * {@link #getValueStream}.
-         * 
-         * @return the length of the value. Does not require
-         *         isValueLengthKnown() to be true.
-         * @throws IOException
-         * 
-         */
-        public int getValue(byte[] buf) throws IOException {
-          return getValue(buf, 0);
-        }
-
-        /**
-         * Copy value into user-supplied buffer. User supplied buffer must be
-         * large enough to hold the whole value (starting from the offset). The
-         * value part of the key-value pair pointed by the current cursor is not
-         * cached and can only be examined once. Calling any of the following
-         * functions more than once without moving the cursor will result in
-         * exception: {@link #getValue(byte[])}, {@link #getValue(byte[], int)},
-         * {@link #getValueStream}.
-         * 
-         * @return the length of the value. Does not require
-         *         isValueLengthKnown() to be true.
-         * @throws IOException
-         */
-        public int getValue(byte[] buf, int offset) throws IOException {
-          DataInputStream dis = getValueStream();
-          try {
-            if (isValueLengthKnown()) {
-              if ((offset | (buf.length - offset - vlen)) < 0) {
-                throw new IndexOutOfBoundsException(
-                    "Buffer too small to hold value");
-              }
-              dis.readFully(buf, offset, vlen);
-              return vlen;
-            }
-
-            int nextOffset = offset;
-            while (nextOffset < buf.length) {
-              int n = dis.read(buf, nextOffset, buf.length - nextOffset);
-              if (n < 0) {
-                break;
-              }
-              nextOffset += n;
-            }
-            if (dis.read() >= 0) {
-              // attempt to read one more byte to determine whether we reached
-              // the
-              // end or not.
-              throw new IndexOutOfBoundsException(
-                  "Buffer too small to hold value");
-            }
-            return nextOffset - offset;
-          } finally {
-            dis.close();
-          }
-        }
-
-        /**
-         * Stream access to value. The value part of the key-value pair pointed
-         * by the current cursor is not cached and can only be examined once.
-         * Calling any of the following functions more than once without moving
-         * the cursor will result in exception: {@link #getValue(byte[])},
-         * {@link #getValue(byte[], int)}, {@link #getValueStream}.
-         * 
-         * @return The input stream for reading the value.
-         * @throws IOException
-         */
-        public DataInputStream getValueStream() throws IOException {
-          if (valueChecked == true) {
-            throw new IllegalStateException(
-                "Attempt to examine value multiple times.");
-          }
-          valueChecked = true;
-          return valueDataInputStream;
-        }
-
-        /**
-         * Check whether it is safe to call getValueLength().
-         * 
-         * @return true if value length is known before hand. Values less than
-         *         the chunk size will always have their lengths known before
-         *         hand. Values that are written out as a whole (with advertised
-         *         length up-front) will always have their lengths known in
-         *         read.
-         */
-        public boolean isValueLengthKnown() {
-          return (vlen >= 0);
-        }
-
-        /**
-         * Compare the entry key to another key. Synonymous to compareTo(key, 0,
-         * key.length).
-         * 
-         * @param buf
-         *          The key buffer.
-         * @return comparison result between the entry key with the input key.
-         */
-        public int compareTo(byte[] buf) {
-          return compareTo(buf, 0, buf.length);
-        }
-
-        /**
-         * Compare the entry key to another key. Synonymous to compareTo(new
-         * ByteArray(buf, offset, length)
-         * 
-         * @param buf
-         *          The key buffer
-         * @param offset
-         *          offset into the key buffer.
-         * @param length
-         *          the length of the key.
-         * @return comparison result between the entry key with the input key.
-         */
-        public int compareTo(byte[] buf, int offset, int length) {
-          return compareTo(new ByteArray(buf, offset, length));
-        }
-
-        /**
-         * Compare an entry with a RawComparable object. This is useful when
-         * Entries are stored in a collection, and we want to compare a user
-         * supplied key.
-         */
-        @Override
-        public int compareTo(RawComparable key) {
-          return reader.compareKeys(keyBuffer, 0, getKeyLength(), key.buffer(),
-              key.offset(), key.size());
-        }
-
-        /**
-         * Compare whether this and other points to the same key value.
-         */
-        @Override
-        public boolean equals(Object other) {
-          if (this == other) return true;
-          if (!(other instanceof Entry)) return false;
-          return ((Entry) other).compareTo(keyBuffer, 0, getKeyLength()) == 0;
-        }
-
-        @Override
-        public int hashCode() {
-          return WritableComparator.hashBytes(keyBuffer, getKeyLength());
-        }
-      }
-
-      /**
-       * Advance cursor by n positions within the block.
-       * 
-       * @param n
-       *          Number of key-value pairs to skip in block.
-       * @throws IOException
-       */
-      private void inBlockAdvance(long n) throws IOException {
-        for (long i = 0; i < n; ++i) {
-          checkKey();
-          if (!valueBufferInputStream.isClosed()) {
-            valueBufferInputStream.close();
-          }
-          klen = -1;
-          currentLocation.incRecordIndex();
-        }
-      }
-
-      /**
-       * Advance cursor in block until we find a key that is greater than or
-       * equal to the input key.
-       * 
-       * @param key
-       *          Key to compare.
-       * @param greater
-       *          advance until we find a key greater than the input key.
-       * @return true if we find a equal key.
-       * @throws IOException
-       */
-      private boolean inBlockAdvance(RawComparable key, boolean greater)
-          throws IOException {
-        int curBid = currentLocation.getBlockIndex();
-        long entryInBlock = reader.getBlockEntryCount(curBid);
-        if (curBid == endLocation.getBlockIndex()) {
-          entryInBlock = endLocation.getRecordIndex();
-        }
-
-        while (currentLocation.getRecordIndex() < entryInBlock) {
-          int cmp = compareCursorKeyTo(key);
-          if (cmp > 0) return false;
-          if (cmp == 0 && !greater) return true;
-          if (!valueBufferInputStream.isClosed()) {
-            valueBufferInputStream.close();
-          }
-          klen = -1;
-          currentLocation.incRecordIndex();
-        }
-
-        throw new RuntimeException("Cannot find matching key in block.");
-      }
-    }
-
-    long getBlockEntryCount(int curBid) {
-      return tfileIndex.getEntry(curBid).entries();
-    }
-
-    BlockReader getBlockReader(int blockIndex) throws IOException {
-      return readerBCF.getDataBlock(blockIndex);
-    }
-  }
-
-  /**
-   * Data structure representing "TFile.meta" meta block.
-   */
-  static final class TFileMeta {
-    final static String BLOCK_NAME = "TFile.meta";
-    final Version version;
-    private long recordCount;
-    private final String strComparator;
-    private final BytesComparator comparator;
-
-    // ctor for writes
-    public TFileMeta(String comparator) {
-      // set fileVersion to API version when we create it.
-      version = TFile.API_VERSION;
-      recordCount = 0;
-      strComparator = (comparator == null) ? "" : comparator;
-      this.comparator = makeComparator(strComparator);
-    }
-
-    // ctor for reads
-    public TFileMeta(DataInput in) throws IOException {
-      version = new Version(in);
-      if (!version.compatibleWith(TFile.API_VERSION)) {
-        throw new RuntimeException("Incompatible TFile fileVersion.");
-      }
-      recordCount = Utils.readVLong(in);
-      strComparator = Utils.readString(in);
-      comparator = makeComparator(strComparator);
-    }
-
-    @SuppressWarnings("unchecked")
-    static BytesComparator makeComparator(String comparator) {
-      if (comparator.length() == 0) {
-        // unsorted keys
-        return null;
-      }
-      if (comparator.equals(COMPARATOR_MEMCMP)) {
-        // default comparator
-        return new BytesComparator(new MemcmpRawComparator());
-      } else if (comparator.startsWith(COMPARATOR_JCLASS)) {
-        String compClassName =
-            comparator.substring(COMPARATOR_JCLASS.length()).trim();
-        try {
-          Class compClass = Class.forName(compClassName);
-          // use its default ctor to create an instance
-          return new BytesComparator((RawComparator<Object>) compClass
-              .newInstance());
-        } catch (Exception e) {
-          throw new IllegalArgumentException(
-              "Failed to instantiate comparator: " + comparator + "("
-                  + e.toString() + ")");
-        }
-      } else {
-        throw new IllegalArgumentException("Unsupported comparator: "
-            + comparator);
-      }
-    }
-
-    public void write(DataOutput out) throws IOException {
-      TFile.API_VERSION.write(out);
-      Utils.writeVLong(out, recordCount);
-      Utils.writeString(out, strComparator);
-    }
-
-    public long getRecordCount() {
-      return recordCount;
-    }
-
-    public void incRecordCount() {
-      ++recordCount;
-    }
-
-    public boolean isSorted() {
-      return !strComparator.equals("");
-    }
-
-    public String getComparatorString() {
-      return strComparator;
-    }
-
-    public BytesComparator getComparator() {
-      return comparator;
-    }
-
-    public Version getVersion() {
-      return version;
-    }
-  } // END: class MetaTFileMeta
-
-  /**
-   * Data structure representing "TFile.index" meta block.
-   */
-  static class TFileIndex {
-    final static String BLOCK_NAME = "TFile.index";
-    private ByteArray firstKey;
-    private final ArrayList<TFileIndexEntry> index;
-    private final ArrayList<Long> recordNumIndex;
-    private final BytesComparator comparator;
-    private long sum = 0;
-    
-    /**
-     * For reading from file.
-     * 
-     * @throws IOException
-     */
-    public TFileIndex(int entryCount, DataInput in, BytesComparator comparator)
-        throws IOException {
-      index = new ArrayList<TFileIndexEntry>(entryCount);
-      recordNumIndex = new ArrayList<Long>(entryCount);
-      int size = Utils.readVInt(in); // size for the first key entry.
-      if (size > 0) {
-        byte[] buffer = new byte[size];
-        in.readFully(buffer);
-        DataInputStream firstKeyInputStream =
-            new DataInputStream(new ByteArrayInputStream(buffer, 0, size));
-
-        int firstKeyLength = Utils.readVInt(firstKeyInputStream);
-        firstKey = new ByteArray(new byte[firstKeyLength]);
-        firstKeyInputStream.readFully(firstKey.buffer());
-
-        for (int i = 0; i < entryCount; i++) {
-          size = Utils.readVInt(in);
-          if (buffer.length < size) {
-            buffer = new byte[size];
-          }
-          in.readFully(buffer, 0, size);
-          TFileIndexEntry idx =
-              new TFileIndexEntry(new DataInputStream(new ByteArrayInputStream(
-                  buffer, 0, size)));
-          index.add(idx);
-          sum += idx.entries();
-          recordNumIndex.add(sum);
-        }
-      } else {
-        if (entryCount != 0) {
-          throw new RuntimeException("Internal error");
-        }
-      }
-      this.comparator = comparator;
-    }
-
-    /**
-     * @param key
-     *          input key.
-     * @return the ID of the first block that contains key >= input key. Or -1
-     *         if no such block exists.
-     */
-    public int lowerBound(RawComparable key) {
-      if (comparator == null) {
-        throw new RuntimeException("Cannot search in unsorted TFile");
-      }
-
-      if (firstKey == null) {
-        return -1; // not found
-      }
-
-      int ret = Utils.lowerBound(index, key, comparator);
-      if (ret == index.size()) {
-        return -1;
-      }
-      return ret;
-    }
-
-    /**
-     * @param key
-     *          input key.
-     * @return the ID of the first block that contains key > input key. Or -1
-     *         if no such block exists.
-     */
-    public int upperBound(RawComparable key) {
-      if (comparator == null) {
-        throw new RuntimeException("Cannot search in unsorted TFile");
-      }
-
-      if (firstKey == null) {
-        return -1; // not found
-      }
-
-      int ret = Utils.upperBound(index, key, comparator);
-      if (ret == index.size()) {
-        return -1;
-      }
-      return ret;
-    }
-
-    /**
-     * For writing to file.
-     */
-    public TFileIndex(BytesComparator comparator) {
-      index = new ArrayList<TFileIndexEntry>();
-      recordNumIndex = new ArrayList<Long>();
-      this.comparator = comparator;
-    }
-
-    public RawComparable getFirstKey() {
-      return firstKey;
-    }
-    
-    public Reader.Location getLocationByRecordNum(long recNum) {
-      int idx = Utils.upperBound(recordNumIndex, recNum);
-      long lastRecNum = (idx == 0)? 0: recordNumIndex.get(idx-1);
-      return new Reader.Location(idx, recNum-lastRecNum);
-    }
-
-    public long getRecordNumByLocation(Reader.Location location) {
-      int blkIndex = location.getBlockIndex();
-      long lastRecNum = (blkIndex == 0) ? 0: recordNumIndex.get(blkIndex-1);
-      return lastRecNum + location.getRecordIndex();
-    }
-    
-    public void setFirstKey(byte[] key, int offset, int length) {
-      firstKey = new ByteArray(new byte[length]);
-      System.arraycopy(key, offset, firstKey.buffer(), 0, length);
-    }
-
-    public RawComparable getLastKey() {
-      if (index.size() == 0) {
-        return null;
-      }
-      return new ByteArray(index.get(index.size() - 1).buffer());
-    }
-
-    public void addEntry(TFileIndexEntry keyEntry) {
-      index.add(keyEntry);
-      sum += keyEntry.entries();
-      recordNumIndex.add(sum);
-    }
-
-    public TFileIndexEntry getEntry(int bid) {
-      return index.get(bid);
-    }
-
-    public void write(DataOutput out) throws IOException {
-      if (firstKey == null) {
-        Utils.writeVInt(out, 0);
-        return;
-      }
-
-      DataOutputBuffer dob = new DataOutputBuffer();
-      Utils.writeVInt(dob, firstKey.size());
-      dob.write(firstKey.buffer());
-      Utils.writeVInt(out, dob.size());
-      out.write(dob.getData(), 0, dob.getLength());
-
-      for (TFileIndexEntry entry : index) {
-        dob.reset();
-        entry.write(dob);
-        Utils.writeVInt(out, dob.getLength());
-        out.write(dob.getData(), 0, dob.getLength());
-      }
-    }
-  }
-
-  /**
-   * TFile Data Index entry. We should try to make the memory footprint of each
-   * index entry as small as possible.
-   */
-  static final class TFileIndexEntry implements RawComparable {
-    final byte[] key;
-    // count of <key, value> entries in the block.
-    final long kvEntries;
-
-    public TFileIndexEntry(DataInput in) throws IOException {
-      int len = Utils.readVInt(in);
-      key = new byte[len];
-      in.readFully(key, 0, len);
-      kvEntries = Utils.readVLong(in);
-    }
-
-    // default entry, without any padding
-    public TFileIndexEntry(byte[] newkey, int offset, int len, long entries) {
-      key = new byte[len];
-      System.arraycopy(newkey, offset, key, 0, len);
-      this.kvEntries = entries;
-    }
-
-    @Override
-    public byte[] buffer() {
-      return key;
-    }
-
-    @Override
-    public int offset() {
-      return 0;
-    }
-
-    @Override
-    public int size() {
-      return key.length;
-    }
-
-    long entries() {
-      return kvEntries;
-    }
-
-    public void write(DataOutput out) throws IOException {
-      Utils.writeVInt(out, key.length);
-      out.write(key, 0, key.length);
-      Utils.writeVLong(out, kvEntries);
-    }
-  }
-
-  /**
-   * Dumping the TFile information.
-   * 
-   * @param args
-   *          A list of TFile paths.
-   */
-  public static void main(String[] args) {
-    System.out.printf("TFile Dumper (TFile %s, BCFile %s)\n", TFile.API_VERSION
-        .toString(), BCFile.API_VERSION.toString());
-    if (args.length == 0) {
-      System.out
-          .println("Usage: java ... org.apache.hadoop.io.file.tfile.TFile tfile-path [tfile-path ...]");
-      System.exit(0);
-    }
-    Configuration conf = new Configuration();
-
-    for (String file : args) {
-      System.out.println("===" + file + "===");
-      try {
-        TFileDumper.dumpInfo(file, System.out, conf);
-      } catch (IOException e) {
-        e.printStackTrace(System.err);
-      }
-    }
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/TFileDumper.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/TFileDumper.java
deleted file mode 100644
index fbfafc387..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/TFileDumper.java
+++ /dev/null
@@ -1,295 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.tfile;
-
-import java.io.IOException;
-import java.io.PrintStream;
-import java.util.Collection;
-import java.util.Iterator;
-import java.util.LinkedHashMap;
-import java.util.Map;
-import java.util.Set;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.IOUtils;
-import org.apache.hadoop.zebra.tfile.BCFile.BlockRegion;
-import org.apache.hadoop.zebra.tfile.BCFile.MetaIndexEntry;
-import org.apache.hadoop.zebra.tfile.TFile.TFileIndexEntry;
-import org.apache.hadoop.zebra.tfile.Utils.Version;
-
-/**
- * Dumping the information of a TFile.
- */
-class TFileDumper {
-  static final Log LOG = LogFactory.getLog(TFileDumper.class);
-
-  private TFileDumper() {
-    // namespace object not constructable.
-  }
-
-  private enum Align {
-    LEFT, CENTER, RIGHT, ZERO_PADDED;
-    static String format(String s, int width, Align align) {
-      if (s.length() >= width) return s;
-      int room = width - s.length();
-      Align alignAdjusted = align;
-      if (room == 1) {
-        alignAdjusted = LEFT;
-      }
-      if (alignAdjusted == LEFT) {
-        return s + String.format("%" + room + "s", "");
-      }
-      if (alignAdjusted == RIGHT) {
-        return String.format("%" + room + "s", "") + s;
-      }
-      if (alignAdjusted == CENTER) {
-        int half = room / 2;
-        return String.format("%" + half + "s", "") + s
-            + String.format("%" + (room - half) + "s", "");
-      }
-      throw new IllegalArgumentException("Unsupported alignment");
-    }
-
-    static String format(long l, int width, Align align) {
-      if (align == ZERO_PADDED) {
-        return String.format("%0" + width + "d", l);
-      }
-      return format(Long.toString(l), width, align);
-    }
-
-    static int calculateWidth(String caption, long max) {
-      return Math.max(caption.length(), Long.toString(max).length());
-    }
-  }
-
-  /**
-   * Dump information about TFile.
-   * 
-   * @param file
-   *          Path string of the TFile
-   * @param out
-   *          PrintStream to output the information.
-   * @param conf
-   *          The configuration object.
-   * @throws IOException
-   */
-  static public void dumpInfo(String file, PrintStream out, Configuration conf)
-      throws IOException {
-    final int maxKeySampleLen = 16;
-    Path path = new Path(file);
-    FileSystem fs = path.getFileSystem(conf);
-    long length = fs.getFileStatus(path).getLen();
-    FSDataInputStream fsdis = fs.open(path);
-    TFile.Reader reader = new TFile.Reader(fsdis, length, conf);
-    try {
-      LinkedHashMap<String, String> properties =
-          new LinkedHashMap<String, String>();
-      int blockCnt = reader.readerBCF.getBlockCount();
-      int metaBlkCnt = reader.readerBCF.metaIndex.index.size();
-      properties.put("BCFile Version", reader.readerBCF.version.toString());
-      properties.put("TFile Version", reader.tfileMeta.version.toString());
-      properties.put("File Length", Long.toString(length));
-      properties.put("Data Compression", reader.readerBCF
-          .getDefaultCompressionName());
-      properties.put("Record Count", Long.toString(reader.getEntryCount()));
-      properties.put("Sorted", Boolean.toString(reader.isSorted()));
-      if (reader.isSorted()) {
-        properties.put("Comparator", reader.getComparatorName());
-      }
-      properties.put("Data Block Count", Integer.toString(blockCnt));
-      long dataSize = 0, dataSizeUncompressed = 0;
-      if (blockCnt > 0) {
-        for (int i = 0; i < blockCnt; ++i) {
-          BlockRegion region =
-              reader.readerBCF.dataIndex.getBlockRegionList().get(i);
-          dataSize += region.getCompressedSize();
-          dataSizeUncompressed += region.getRawSize();
-        }
-        properties.put("Data Block Bytes", Long.toString(dataSize));
-        if (reader.readerBCF.getDefaultCompressionName() != "none") {
-          properties.put("Data Block Uncompressed Bytes", Long
-              .toString(dataSizeUncompressed));
-          properties.put("Data Block Compression Ratio", String.format(
-              "1:%.1f", (double) dataSizeUncompressed / dataSize));
-        }
-      }
-
-      properties.put("Meta Block Count", Integer.toString(metaBlkCnt));
-      long metaSize = 0, metaSizeUncompressed = 0;
-      if (metaBlkCnt > 0) {
-        Collection<MetaIndexEntry> metaBlks =
-            reader.readerBCF.metaIndex.index.values();
-        boolean calculateCompression = false;
-        for (Iterator<MetaIndexEntry> it = metaBlks.iterator(); it.hasNext();) {
-          MetaIndexEntry e = it.next();
-          metaSize += e.getRegion().getCompressedSize();
-          metaSizeUncompressed += e.getRegion().getRawSize();
-          if (e.getCompressionAlgorithm() != Compression.Algorithm.NONE) {
-            calculateCompression = true;
-          }
-        }
-        properties.put("Meta Block Bytes", Long.toString(metaSize));
-        if (calculateCompression) {
-          properties.put("Meta Block Uncompressed Bytes", Long
-              .toString(metaSizeUncompressed));
-          properties.put("Meta Block Compression Ratio", String.format(
-              "1:%.1f", (double) metaSizeUncompressed / metaSize));
-        }
-      }
-      properties.put("Meta-Data Size Ratio", String.format("1:%.1f",
-          (double) dataSize / metaSize));
-      long leftOverBytes = length - dataSize - metaSize;
-      long miscSize =
-          BCFile.Magic.size() * 2 + Long.SIZE / Byte.SIZE + Version.size();
-      long metaIndexSize = leftOverBytes - miscSize;
-      properties.put("Meta Block Index Bytes", Long.toString(metaIndexSize));
-      properties.put("Headers Etc Bytes", Long.toString(miscSize));
-      // Now output the properties table.
-      int maxKeyLength = 0;
-      Set<Map.Entry<String, String>> entrySet = properties.entrySet();
-      for (Iterator<Map.Entry<String, String>> it = entrySet.iterator(); it
-          .hasNext();) {
-        Map.Entry<String, String> e = it.next();
-        if (e.getKey().length() > maxKeyLength) {
-          maxKeyLength = e.getKey().length();
-        }
-      }
-      for (Iterator<Map.Entry<String, String>> it = entrySet.iterator(); it
-          .hasNext();) {
-        Map.Entry<String, String> e = it.next();
-        out.printf("%s : %s\n", Align.format(e.getKey(), maxKeyLength,
-            Align.LEFT), e.getValue());
-      }
-      out.println();
-      reader.checkTFileDataIndex();
-      if (blockCnt > 0) {
-        String blkID = "Data-Block";
-        int blkIDWidth = Align.calculateWidth(blkID, blockCnt);
-        int blkIDWidth2 = Align.calculateWidth("", blockCnt);
-        String offset = "Offset";
-        int offsetWidth = Align.calculateWidth(offset, length);
-        String blkLen = "Length";
-        int blkLenWidth =
-            Align.calculateWidth(blkLen, dataSize / blockCnt * 10);
-        String rawSize = "Raw-Size";
-        int rawSizeWidth =
-            Align.calculateWidth(rawSize, dataSizeUncompressed / blockCnt * 10);
-        String records = "Records";
-        int recordsWidth =
-            Align.calculateWidth(records, reader.getEntryCount() / blockCnt
-                * 10);
-        String endKey = "End-Key";
-        int endKeyWidth = Math.max(endKey.length(), maxKeySampleLen * 2 + 5);
-
-        out.printf("%s %s %s %s %s %s\n", Align.format(blkID, blkIDWidth,
-            Align.CENTER), Align.format(offset, offsetWidth, Align.CENTER),
-            Align.format(blkLen, blkLenWidth, Align.CENTER), Align.format(
-                rawSize, rawSizeWidth, Align.CENTER), Align.format(records,
-                recordsWidth, Align.CENTER), Align.format(endKey, endKeyWidth,
-                Align.LEFT));
-
-        for (int i = 0; i < blockCnt; ++i) {
-          BlockRegion region =
-              reader.readerBCF.dataIndex.getBlockRegionList().get(i);
-          TFileIndexEntry indexEntry = reader.tfileIndex.getEntry(i);
-          out.printf("%s %s %s %s %s ", Align.format(Align.format(i,
-              blkIDWidth2, Align.ZERO_PADDED), blkIDWidth, Align.LEFT), Align
-              .format(region.getOffset(), offsetWidth, Align.LEFT), Align
-              .format(region.getCompressedSize(), blkLenWidth, Align.LEFT),
-              Align.format(region.getRawSize(), rawSizeWidth, Align.LEFT),
-              Align.format(indexEntry.kvEntries, recordsWidth, Align.LEFT));
-          byte[] key = indexEntry.key;
-          boolean asAscii = true;
-          int sampleLen = Math.min(maxKeySampleLen, key.length);
-          for (int j = 0; j < sampleLen; ++j) {
-            byte b = key[j];
-            if ((b < 32 && b != 9) || (b == 127)) {
-              asAscii = false;
-            }
-          }
-          if (!asAscii) {
-            out.print("0X");
-            for (int j = 0; j < sampleLen; ++j) {
-              byte b = key[i];
-              out.printf("%X", b);
-            }
-          } else {
-            out.print(new String(key, 0, sampleLen));
-          }
-          if (sampleLen < key.length) {
-            out.print("...");
-          }
-          out.println();
-        }
-      }
-
-      out.println();
-      if (metaBlkCnt > 0) {
-        String name = "Meta-Block";
-        int maxNameLen = 0;
-        Set<Map.Entry<String, MetaIndexEntry>> metaBlkEntrySet =
-            reader.readerBCF.metaIndex.index.entrySet();
-        for (Iterator<Map.Entry<String, MetaIndexEntry>> it =
-            metaBlkEntrySet.iterator(); it.hasNext();) {
-          Map.Entry<String, MetaIndexEntry> e = it.next();
-          if (e.getKey().length() > maxNameLen) {
-            maxNameLen = e.getKey().length();
-          }
-        }
-        int nameWidth = Math.max(name.length(), maxNameLen);
-        String offset = "Offset";
-        int offsetWidth = Align.calculateWidth(offset, length);
-        String blkLen = "Length";
-        int blkLenWidth =
-            Align.calculateWidth(blkLen, metaSize / metaBlkCnt * 10);
-        String rawSize = "Raw-Size";
-        int rawSizeWidth =
-            Align.calculateWidth(rawSize, metaSizeUncompressed / metaBlkCnt
-                * 10);
-        String compression = "Compression";
-        int compressionWidth = compression.length();
-        out.printf("%s %s %s %s %s\n", Align.format(name, nameWidth,
-            Align.CENTER), Align.format(offset, offsetWidth, Align.CENTER),
-            Align.format(blkLen, blkLenWidth, Align.CENTER), Align.format(
-                rawSize, rawSizeWidth, Align.CENTER), Align.format(compression,
-                compressionWidth, Align.LEFT));
-
-        for (Iterator<Map.Entry<String, MetaIndexEntry>> it =
-            metaBlkEntrySet.iterator(); it.hasNext();) {
-          Map.Entry<String, MetaIndexEntry> e = it.next();
-          String blkName = e.getValue().getMetaName();
-          BlockRegion region = e.getValue().getRegion();
-          String blkCompression =
-              e.getValue().getCompressionAlgorithm().getName();
-          out.printf("%s %s %s %s %s\n", Align.format(blkName, nameWidth,
-              Align.LEFT), Align.format(region.getOffset(), offsetWidth,
-              Align.LEFT), Align.format(region.getCompressedSize(),
-              blkLenWidth, Align.LEFT), Align.format(region.getRawSize(),
-              rawSizeWidth, Align.LEFT), Align.format(blkCompression,
-              compressionWidth, Align.LEFT));
-        }
-      }
-    } finally {
-      IOUtils.cleanup(LOG, reader, fsdis);
-    }
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/Utils.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/Utils.java
deleted file mode 100644
index 7be2643bc..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/tfile/Utils.java
+++ /dev/null
@@ -1,517 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-package org.apache.hadoop.zebra.tfile;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-import java.util.Comparator;
-import java.util.List;
-
-import org.apache.hadoop.io.Text;
-
-/**
- * Supporting Utility classes used by TFile, and shared by users of TFile.
- */
-public final class Utils {
-
-  /**
-   * Prevent the instantiation of Utils.
-   */
-  private Utils() {
-    // nothing
-  }
-
-  /**
-   * Encoding an integer into a variable-length encoding format. Synonymous to
-   * <code>Utils#writeVLong(out, n)</code>.
-   * 
-   * @param out
-   *          output stream
-   * @param n
-   *          The integer to be encoded
-   * @throws IOException
-   * @see Utils#writeVLong(DataOutput, long)
-   */
-  public static void writeVInt(DataOutput out, int n) throws IOException {
-    writeVLong(out, n);
-  }
-
-  /**
-   * Encoding a Long integer into a variable-length encoding format.
-   * <ul>
-   * <li>if n in [-32, 127): encode in one byte with the actual value.
-   * Otherwise,
-   * <li>if n in [-20*2^8, 20*2^8): encode in two bytes: byte[0] = n/256 - 52;
-   * byte[1]=n&0xff. Otherwise,
-   * <li>if n IN [-16*2^16, 16*2^16): encode in three bytes: byte[0]=n/2^16 -
-   * 88; byte[1]=(n>>8)&0xff; byte[2]=n&0xff. Otherwise,
-   * <li>if n in [-8*2^24, 8*2^24): encode in four bytes: byte[0]=n/2^24 - 112;
-   * byte[1] = (n>>16)&0xff; byte[2] = (n>>8)&0xff; byte[3]=n&0xff. Otherwise:
-   * <li>if n in [-2^31, 2^31): encode in five bytes: byte[0]=-125; byte[1] =
-   * (n>>24)&0xff; byte[2]=(n>>16)&0xff; byte[3]=(n>>8)&0xff; byte[4]=n&0xff;
-   * <li>if n in [-2^39, 2^39): encode in six bytes: byte[0]=-124; byte[1] =
-   * (n>>32)&0xff; byte[2]=(n>>24)&0xff; byte[3]=(n>>16)&0xff;
-   * byte[4]=(n>>8)&0xff; byte[5]=n&0xff
-   * <li>if n in [-2^47, 2^47): encode in seven bytes: byte[0]=-123; byte[1] =
-   * (n>>40)&0xff; byte[2]=(n>>32)&0xff; byte[3]=(n>>24)&0xff;
-   * byte[4]=(n>>16)&0xff; byte[5]=(n>>8)&0xff; byte[6]=n&0xff;
-   * <li>if n in [-2^55, 2^55): encode in eight bytes: byte[0]=-122; byte[1] =
-   * (n>>48)&0xff; byte[2] = (n>>40)&0xff; byte[3]=(n>>32)&0xff;
-   * byte[4]=(n>>24)&0xff; byte[5]=(n>>16)&0xff; byte[6]=(n>>8)&0xff;
-   * byte[7]=n&0xff;
-   * <li>if n in [-2^63, 2^63): encode in nine bytes: byte[0]=-121; byte[1] =
-   * (n>>54)&0xff; byte[2] = (n>>48)&0xff; byte[3] = (n>>40)&0xff;
-   * byte[4]=(n>>32)&0xff; byte[5]=(n>>24)&0xff; byte[6]=(n>>16)&0xff;
-   * byte[7]=(n>>8)&0xff; byte[8]=n&0xff;
-   * </ul>
-   * 
-   * @param out
-   *          output stream
-   * @param n
-   *          the integer number
-   * @throws IOException
-   */
-  @SuppressWarnings("fallthrough")
-  public static void writeVLong(DataOutput out, long n) throws IOException {
-    if ((n < 128) && (n >= -32)) {
-      out.writeByte((int) n);
-      return;
-    }
-
-    long un = (n < 0) ? ~n : n;
-    // how many bytes do we need to represent the number with sign bit?
-    int len = (Long.SIZE - Long.numberOfLeadingZeros(un)) / 8 + 1;
-    int firstByte = (int) (n >> ((len - 1) * 8));
-    switch (len) {
-      case 1:
-        // fall it through to firstByte==-1, len=2.
-        firstByte >>= 8;
-      case 2:
-        if ((firstByte < 20) && (firstByte >= -20)) {
-          out.writeByte(firstByte - 52);
-          out.writeByte((int) n);
-          return;
-        }
-        // fall it through to firstByte==0/-1, len=3.
-        firstByte >>= 8;
-      case 3:
-        if ((firstByte < 16) && (firstByte >= -16)) {
-          out.writeByte(firstByte - 88);
-          out.writeShort((int) n);
-          return;
-        }
-        // fall it through to firstByte==0/-1, len=4.
-        firstByte >>= 8;
-      case 4:
-        if ((firstByte < 8) && (firstByte >= -8)) {
-          out.writeByte(firstByte - 112);
-          out.writeShort(((int) n) >>> 8);
-          out.writeByte((int) n);
-          return;
-        }
-        out.writeByte(len - 129);
-        out.writeInt((int) n);
-        return;
-      case 5:
-        out.writeByte(len - 129);
-        out.writeInt((int) (n >>> 8));
-        out.writeByte((int) n);
-        return;
-      case 6:
-        out.writeByte(len - 129);
-        out.writeInt((int) (n >>> 16));
-        out.writeShort((int) n);
-        return;
-      case 7:
-        out.writeByte(len - 129);
-        out.writeInt((int) (n >>> 24));
-        out.writeShort((int) (n >>> 8));
-        out.writeByte((int) n);
-        return;
-      case 8:
-        out.writeByte(len - 129);
-        out.writeLong(n);
-        return;
-      default:
-        throw new RuntimeException("Internel error");
-    }
-  }
-
-  /**
-   * Decoding the variable-length integer. Synonymous to
-   * <code>(int)Utils#readVLong(in)</code>.
-   * 
-   * @param in
-   *          input stream
-   * @return the decoded integer
-   * @throws IOException
-   * 
-   * @see Utils#readVLong(DataInput)
-   */
-  public static int readVInt(DataInput in) throws IOException {
-    long ret = readVLong(in);
-    if ((ret > Integer.MAX_VALUE) || (ret < Integer.MIN_VALUE)) {
-      throw new RuntimeException(
-          "Number too large to be represented as Integer");
-    }
-    return (int) ret;
-  }
-
-  /**
-   * Decoding the variable-length integer. Suppose the value of the first byte
-   * is FB, and the following bytes are NB[*].
-   * <ul>
-   * <li>if (FB >= -32), return (long)FB;
-   * <li>if (FB in [-72, -33]), return (FB+52)<<8 + NB[0]&0xff;
-   * <li>if (FB in [-104, -73]), return (FB+88)<<16 + (NB[0]&0xff)<<8 +
-   * NB[1]&0xff;
-   * <li>if (FB in [-120, -105]), return (FB+112)<<24 + (NB[0]&0xff)<<16 +
-   * (NB[1]&0xff)<<8 + NB[2]&0xff;
-   * <li>if (FB in [-128, -121]), return interpret NB[FB+129] as a signed
-   * big-endian integer.
-   * 
-   * @param in
-   *          input stream
-   * @return the decoded long integer.
-   * @throws IOException
-   */
-
-  public static long readVLong(DataInput in) throws IOException {
-    int firstByte = in.readByte();
-    if (firstByte >= -32) {
-      return firstByte;
-    }
-
-    switch ((firstByte + 128) / 8) {
-      case 11:
-      case 10:
-      case 9:
-      case 8:
-      case 7:
-        return ((firstByte + 52) << 8) | in.readUnsignedByte();
-      case 6:
-      case 5:
-      case 4:
-      case 3:
-        return ((firstByte + 88) << 16) | in.readUnsignedShort();
-      case 2:
-      case 1:
-        return ((firstByte + 112) << 24) | (in.readUnsignedShort() << 8)
-            | in.readUnsignedByte();
-      case 0:
-        int len = firstByte + 129;
-        switch (len) {
-          case 4:
-            return in.readInt();
-          case 5:
-            return ((long) in.readInt()) << 8 | in.readUnsignedByte();
-          case 6:
-            return ((long) in.readInt()) << 16 | in.readUnsignedShort();
-          case 7:
-            return ((long) in.readInt()) << 24 | (in.readUnsignedShort() << 8)
-                | in.readUnsignedByte();
-          case 8:
-            return in.readLong();
-          default:
-            throw new IOException("Corrupted VLong encoding");
-        }
-      default:
-        throw new RuntimeException("Internal error");
-    }
-  }
-
-  /**
-   * Write a String as a VInt n, followed by n Bytes as in Text format.
-   * 
-   * @param out
-   * @param s
-   * @throws IOException
-   */
-  public static void writeString(DataOutput out, String s) throws IOException {
-    if (s != null) {
-      Text text = new Text(s);
-      byte[] buffer = text.getBytes();
-      int len = text.getLength();
-      writeVInt(out, len);
-      out.write(buffer, 0, len);
-    } else {
-      writeVInt(out, -1);
-    }
-  }
-
-  /**
-   * Read a String as a VInt n, followed by n Bytes in Text format.
-   * 
-   * @param in
-   *          The input stream.
-   * @return The string
-   * @throws IOException
-   */
-  public static String readString(DataInput in) throws IOException {
-    int length = readVInt(in);
-    if (length == -1) return null;
-    byte[] buffer = new byte[length];
-    in.readFully(buffer);
-    return Text.decode(buffer);
-  }
-
-  /**
-   * A generic Version class. We suggest applications built on top of TFile use
-   * this class to maintain version information in their meta blocks.
-   * 
-   * A version number consists of a major version and a minor version. The
-   * suggested usage of major and minor version number is to increment major
-   * version number when the new storage format is not backward compatible, and
-   * increment the minor version otherwise.
-   */
-  public static final class Version implements Comparable<Version> {
-    private final short major;
-    private final short minor;
-
-    /**
-     * Construct the Version object by reading from the input stream.
-     * 
-     * @param in
-     *          input stream
-     * @throws IOException
-     */
-    public Version(DataInput in) throws IOException {
-      major = in.readShort();
-      minor = in.readShort();
-    }
-
-    /**
-     * Constructor.
-     * 
-     * @param major
-     *          major version.
-     * @param minor
-     *          minor version.
-     */
-    public Version(short major, short minor) {
-      this.major = major;
-      this.minor = minor;
-    }
-
-    /**
-     * Write the objec to a DataOutput. The serialized format of the Version is
-     * major version followed by minor version, both as big-endian short
-     * integers.
-     * 
-     * @param out
-     *          The DataOutput object.
-     * @throws IOException
-     */
-    public void write(DataOutput out) throws IOException {
-      out.writeShort(major);
-      out.writeShort(minor);
-    }
-
-    /**
-     * Get the major version.
-     * 
-     * @return Major version.
-     */
-    public int getMajor() {
-      return major;
-    }
-
-    /**
-     * Get the minor version.
-     * 
-     * @return The minor version.
-     */
-    public int getMinor() {
-      return minor;
-    }
-
-    /**
-     * Get the size of the serialized Version object.
-     * 
-     * @return serialized size of the version object.
-     */
-    public static int size() {
-      return (Short.SIZE + Short.SIZE) / Byte.SIZE;
-    }
-
-    /**
-     * Return a string representation of the version.
-     */
-    @Override
-    public String toString() {
-      return new StringBuilder("v").append(major).append(".").append(minor)
-          .toString();
-    }
-
-    /**
-     * Test compatibility.
-     * 
-     * @param other
-     *          The Version object to test compatibility with.
-     * @return true if both versions have the same major version number; false
-     *         otherwise.
-     */
-    public boolean compatibleWith(Version other) {
-      return major == other.major;
-    }
-
-    /**
-     * Compare this version with another version.
-     */
-    @Override
-    public int compareTo(Version that) {
-      if (major != that.major) {
-        return major - that.major;
-      }
-      return minor - that.minor;
-    }
-
-    @Override
-    public boolean equals(Object other) {
-      if (this == other) return true;
-      if (!(other instanceof Version)) return false;
-      return compareTo((Version) other) == 0;
-    }
-
-    @Override
-    public int hashCode() {
-      return (major << 16 + minor);
-    }
-  }
-
-  /**
-   * Lower bound binary search. Find the index to the first element in the list
-   * that compares greater than or equal to key.
-   * 
-   * @param <T>
-   *          Type of the input key.
-   * @param list
-   *          The list
-   * @param key
-   *          The input key.
-   * @param cmp
-   *          Comparator for the key.
-   * @return The index to the desired element if it exists; or list.size()
-   *         otherwise.
-   */
-  public static <T> int lowerBound(List<? extends T> list, T key,
-      Comparator<? super T> cmp) {
-    int low = 0;
-    int high = list.size();
-
-    while (low < high) {
-      int mid = (low + high) >>> 1;
-      T midVal = list.get(mid);
-      int ret = cmp.compare(midVal, key);
-      if (ret < 0)
-        low = mid + 1;
-      else high = mid;
-    }
-    return low;
-  }
-
-  /**
-   * Upper bound binary search. Find the index to the first element in the list
-   * that compares greater than the input key.
-   * 
-   * @param <T>
-   *          Type of the input key.
-   * @param list
-   *          The list
-   * @param key
-   *          The input key.
-   * @param cmp
-   *          Comparator for the key.
-   * @return The index to the desired element if it exists; or list.size()
-   *         otherwise.
-   */
-  public static <T> int upperBound(List<? extends T> list, T key,
-      Comparator<? super T> cmp) {
-    int low = 0;
-    int high = list.size();
-
-    while (low < high) {
-      int mid = (low + high) >>> 1;
-      T midVal = list.get(mid);
-      int ret = cmp.compare(midVal, key);
-      if (ret <= 0)
-        low = mid + 1;
-      else high = mid;
-    }
-    return low;
-  }
-
-  /**
-   * Lower bound binary search. Find the index to the first element in the list
-   * that compares greater than or equal to key.
-   * 
-   * @param <T>
-   *          Type of the input key.
-   * @param list
-   *          The list
-   * @param key
-   *          The input key.
-   * @return The index to the desired element if it exists; or list.size()
-   *         otherwise.
-   */
-  public static <T> int lowerBound(List<? extends Comparable<? super T>> list,
-      T key) {
-    int low = 0;
-    int high = list.size();
-
-    while (low < high) {
-      int mid = (low + high) >>> 1;
-      Comparable<? super T> midVal = list.get(mid);
-      int ret = midVal.compareTo(key);
-      if (ret < 0)
-        low = mid + 1;
-      else high = mid;
-    }
-    return low;
-  }
-
-  /**
-   * Upper bound binary search. Find the index to the first element in the list
-   * that compares greater than the input key.
-   * 
-   * @param <T>
-   *          Type of the input key.
-   * @param list
-   *          The list
-   * @param key
-   *          The input key.
-   * @return The index to the desired element if it exists; or list.size()
-   *         otherwise.
-   */
-  public static <T> int upperBound(List<? extends Comparable<? super T>> list,
-      T key) {
-    int low = 0;
-    int high = list.size();
-
-    while (low < high) {
-      int mid = (low + high) >>> 1;
-      Comparable<? super T> midVal = list.get(mid);
-      int ret = midVal.compareTo(key);
-      if (ret <= 0)
-        low = mid + 1;
-      else high = mid;
-    }
-    return low;
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/types/CGSchema.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/types/CGSchema.java
deleted file mode 100644
index b37c76cfd..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/types/CGSchema.java
+++ /dev/null
@@ -1,227 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-package org.apache.hadoop.zebra.types;
-
-import java.io.IOException;
-
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.permission.*;
-
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.WritableUtils;
-import org.apache.hadoop.zebra.tfile.Utils.Version;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.parser.ParseException;
-
-  /**
-   * ColumnGroup Schema. This object is first written to a schema file when the
-   * ColumnGroup is initially created, and is used to communicate meta
-   * information among writers.
-   */
-public class CGSchema {
-	private Version version;
-   private boolean sorted;
-   private String comparator;
-   private Schema schema;
-   private String name = null;
-   private String compressor = "lzo";
-   private String serializer = "pig";
-   private String group		 = null;
-   private String owner		 = null;
-   private short  perm		 = -1;
-   
-   // tmp schema file name, used as a flag of unfinished CG
-   private final static String SCHEMA_FILE = ".schema";
-	// schema version, should be same as BasicTable's most of the time
-   private final static Version SCHEMA_VERSION =
-     new Version((short) 1, (short) 1);
-
-   @Override
-   public String toString() {
-     StringBuilder sb = new StringBuilder();
-     sb.append("{Name = ");
-     sb.append(name);
-     sb.append("}\n");
-     sb.append("{Compressor = ");
-     sb.append(compressor);
-     sb.append("}\n");
-     sb.append("{Serializer = ");
-     sb.append(serializer);
-     sb.append("}\n");
-     sb.append(schema.toString());
-     
-     return sb.toString();
-   }
-   
-   public static Path makeFilePath(Path parent) {
-     return new Path(parent, SCHEMA_FILE);
-   }
-
-   public static CGSchema load(FileSystem fs, Path parent) throws IOException, ParseException {
-     if (!exists(fs, parent)) return null;
-     
-     CGSchema ret = new CGSchema();
-     ret.read(fs, parent);
-     return ret;
-   }
-      
-   public CGSchema() {
-     this.version = SCHEMA_VERSION;
-   }
-
-   public CGSchema(Schema schema, boolean sorted, String comparator) {
-     this.sorted = sorted;
-     this.comparator = (sorted) ? (comparator == null ? SortInfo.DEFAULT_COMPARATOR : comparator) : "";
-     this.schema = schema;
-     this.version = SCHEMA_VERSION;
-   }
-
-   public CGSchema(Schema schema, boolean sorted, String comparator, String name, String serializer, String compressor, String owner, String group, short perm) {
-  	this(schema, sorted, comparator);
-    this.name = name;
-  	this.serializer = serializer;
-  	this.compressor = compressor;
-  	this.group = group;
-  	this.perm  = perm;
-   }
-
-   @Override
-   public boolean equals(Object obj) {
-     CGSchema other = (CGSchema) obj;
-     return this.sorted == other.sorted
-         && this.comparator.equals(other.comparator)
-         && this.schema.equals(other.schema);
-   }
-
-   public static boolean exists(FileSystem fs, Path parent) {
-     try {
-       return fs.exists(makeFilePath(parent));
-     }
-     catch (IOException e) {
-       return false;
-     }
-   }
-
-   public static void drop(FileSystem fs, Path parent) throws IOException {
-     fs.delete(makeFilePath(parent), true);
-   }
-
-   public boolean isSorted() {
-     return sorted;
-   }
-
-   public String getComparator() {
-     return comparator;
-   }
-
-   public String getName() {
-     return name;
-   }
-   
-   public void setName(String newName) {
-     name = newName;
-   }
-
-   public String getSerializer() {
-     return serializer;
-   }
-
-   public String getCompressor() {
-     return compressor;
-   }
-
-   public String getGroup() {
-	     return this.group;
-   }
-
-   public short getPerm() {
-	     return this.perm;
-   }
-
-   public String getOwner() {
-	     return this.owner;
-   } 
-   
-   
-   
-   public void create(FileSystem fs, Path parent) throws IOException {
-	 fs.mkdirs(parent);
-	 if(this.owner != null || this.group != null) {
-	   fs.setOwner(parent, this.owner, this.group);
-	 }  
-
-	 if(this.perm != -1) {
-       fs.setPermission(parent, new FsPermission((short) this.perm));
-
-	 }
-	 
-	 FSDataOutputStream outSchema = fs.create(makeFilePath(parent), false);
-	 if(this.owner != null || this.group != null) {
-	   fs.setOwner(makeFilePath(parent), owner, group);
-	 }  
-	 if(this.perm != -1) {
-       fs.setPermission(makeFilePath(parent), new FsPermission((short) this.perm));
-
-	 }
-     version.write(outSchema);
-       
-	 WritableUtils.writeString(outSchema, schema.toString());
-     WritableUtils.writeVInt(outSchema, sorted ? 1 : 0);
-     WritableUtils.writeString(outSchema, comparator);
-	 WritableUtils.writeString(outSchema, this.getCompressor());
-	 WritableUtils.writeString(outSchema, this.getSerializer());
-	 WritableUtils.writeString(outSchema, this.getGroup());
-     WritableUtils.writeVInt(outSchema, this.getPerm());
-     WritableUtils.writeString(outSchema, name);
-
-     outSchema.close();
-   }
-
- public  void read(FileSystem fs, Path parent) throws IOException, ParseException {
-     FSDataInputStream in = fs.open(makeFilePath(parent));
-	   version = new Version(in);
-     // verify compatibility against SCHEMA_VERSION
-     if (!version.compatibleWith(SCHEMA_VERSION)) {
-       new IOException("Incompatible versions, expecting: " + SCHEMA_VERSION
-            + "; found in file: " + version);
-     }     
-     
-     name = parent.getName();     
-     String s = WritableUtils.readString(in);
-     schema = new Schema(s);
-     sorted = WritableUtils.readVInt(in) == 1 ? true : false;
-     comparator = WritableUtils.readString(in);
-     // V2 table;
-     if(version.compareTo(SCHEMA_VERSION) >= 0) { 
-       compressor = WritableUtils.readString(in);
-       serializer = WritableUtils.readString(in);  
-   	   owner		= null;
-       group 		= WritableUtils.readString(in);
-       perm 		= (short) WritableUtils.readVInt(in);
-       name = WritableUtils.readString(in);
-     }  
-     in.close();
-   }
-
-   public Schema getSchema() {
-     return schema;
-   }
- }
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/types/CsvZebraTupleOutput.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/types/CsvZebraTupleOutput.java
deleted file mode 100644
index 84c4f7183..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/types/CsvZebraTupleOutput.java
+++ /dev/null
@@ -1,289 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.types;
-
-import java.util.Iterator;
-import java.util.Map;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-
-/**
- */
-class CsvZebraTupleOutput {
-  private StringBuilder sb;
-  private boolean isFirst = true;
-  protected static final Log LOG = LogFactory.getLog(CsvZebraTupleOutput.class);
-  private static CsvZebraTupleOutput instance = null;
-
-  String toCSVString(String s) {
-    StringBuffer sb = new StringBuffer(s.length() + 1);
-    sb.append('\'');
-    int len = s.length();
-    for (int i = 0; i < len; i++) {
-      char c = s.charAt(i);
-      switch (c) {
-      case '\0':
-        sb.append("%00");
-        break;
-      case '\n':
-        sb.append("%0A");
-        break;
-      case '\r':
-        sb.append("%0D");
-        break;
-      case ',':
-        sb.append("%2C");
-        break;
-      case '}':
-        sb.append("%7D");
-        break;
-      case '%':
-        sb.append("%25");
-        break;
-      default:
-        sb.append(c);
-      }
-    }
-    return sb.toString();
-  }
-
-  String toCSVBuffer(DataByteArray buf) {
-    StringBuffer sb = new StringBuffer("#");
-    sb.append(buf.toString());
-    return sb.toString();
-  }
-
-  void printCommaUnlessFirst() {
-    if (!isFirst) {
-      sb.append(",");
-    }
-    isFirst = false;
-  }
-
-  /** Creates a new instance of CsvZebraTupleOutput */
-  private CsvZebraTupleOutput() {
-    sb = new StringBuilder();
-  }
-
-  void reset() {
-    sb.delete(0, sb.length());
-    isFirst = true;
-  }
-
-  static CsvZebraTupleOutput createCsvZebraTupleOutput() {
-    if (instance == null) {
-      instance = new CsvZebraTupleOutput();
-    } else {
-      instance.reset();
-    }
-
-    return instance;
-  }
-
-  @Override
-  public String toString() {
-    if (sb != null) {
-      return sb.toString();
-    }
-    return null;
-  }
-
-  void writeByte(byte b) {
-    writeLong((long) b);
-  }
-
-  void writeBool(boolean b) {
-    printCommaUnlessFirst();
-    String val = b ? "T" : "F";
-    sb.append(val);
-  }
-
-  void writeInt(int i) {
-    writeLong((long) i);
-  }
-
-  void writeLong(long l) {
-    printCommaUnlessFirst();
-    sb.append(l);
-  }
-
-  void writeFloat(float f) {
-    writeDouble((double) f);
-  }
-
-  void writeDouble(double d) {
-    printCommaUnlessFirst();
-    sb.append(d);
-  }
-
-  void writeString(String s) {
-    printCommaUnlessFirst();
-    sb.append(toCSVString(s));
-  }
-
-  void writeBuffer(DataByteArray buf) {
-    printCommaUnlessFirst();
-    sb.append(toCSVBuffer(buf));
-  }
-
-  void writeNull() {
-    printCommaUnlessFirst();
-  }
-
-  void startTuple(Tuple r) {
-  }
-
-  void endTuple(Tuple r) {
-    sb.append("\n");
-    isFirst = true;
-  }
-
-  /**
-   * Generate CSV-format string representations of Zebra tuples for Zebra
-   * streaming use.
-   * 
-   * @param tuple
-   * @return CSV format string representation of Tuple
-   */
-  @SuppressWarnings("unchecked")
-  void writeTuple(Tuple r) {
-    for (int i = 0; i < r.size(); i++) {
-      try {
-        Object d = r.get(i);
-
-        if (d != null) {
-          if (d instanceof Map) {
-            Map<String, Object> map = (Map<String, Object>) d;
-            startMap(map);
-            writeMap(map);
-            endMap(map);
-          } else if (d instanceof Tuple) {
-            Tuple t = (Tuple) d;
-            writeTuple(t);
-          } else if (d instanceof DataBag) {
-            DataBag bag = (DataBag) d;
-            writeBag(bag);
-          } else if (d instanceof Boolean) {
-            writeBool((Boolean) d);
-          } else if (d instanceof Byte) {
-            writeByte((Byte) d);
-          } else if (d instanceof Integer) {
-            writeInt((Integer) d);
-          } else if (d instanceof Long) {
-            writeLong((Long) d);
-          } else if (d instanceof Float) {
-            writeFloat((Float) d);
-          } else if (d instanceof Double) {
-            writeDouble((Double) d);
-          } else if (d instanceof String) {
-            writeString((String) d);
-          } else if (d instanceof DataByteArray) {
-            writeBuffer((DataByteArray) d);
-          } else {
-            throw new ExecException("Unknown data type");
-          }
-        } else { // if d is null, write nothing except ','
-          writeNull();
-        }
-      } catch (ExecException e) {
-        e.printStackTrace();
-        LOG.warn("Exception when CSV format Zebra tuple", e);
-      }
-    }
-  }
-
-  void startBag(DataBag bag) {
-    printCommaUnlessFirst();
-    sb.append("v{");
-    isFirst = true;
-  }
-
-  void writeBag(DataBag bag) {
-    Iterator<Tuple> iter = bag.iterator();
-    while (iter.hasNext()) {
-      Tuple t = (Tuple) iter.next();
-      startTuple(t);
-      writeTuple(t);
-      endTuple(t);
-    }
-  }
-
-  void endBag(DataBag bag) {
-    sb.append("}");
-    isFirst = false;
-  }
-
-  void startMap(Map<String, Object> m) {
-    printCommaUnlessFirst();
-    sb.append("m{");
-    isFirst = true;
-  }
-
-  void endMap(Map<String, Object> m) {
-    sb.append("}");
-    isFirst = false;
-  }
-
-  @SuppressWarnings("unchecked")
-  void writeMap(Map<String, Object> m) throws ExecException {
-    for (Map.Entry<String, Object> e : m.entrySet()) {
-      writeString(e.getKey());
-
-      Object d = e.getValue();
-      if (d != null) {
-        if (d instanceof Map) {
-          Map<String, Object> map = (Map<String, Object>) d;
-          startMap(map);
-          writeMap(map);
-          endMap(map);
-        } else if (d instanceof Tuple) {
-          Tuple t = (Tuple) d;
-          writeTuple(t);
-        } else if (d instanceof DataBag) {
-          DataBag bag = (DataBag) d;
-          writeBag(bag);
-        } else if (d instanceof Boolean) {
-          writeBool((Boolean) d);
-        } else if (d instanceof Byte) {
-          writeByte((Byte) d);
-        } else if (d instanceof Integer) {
-          writeInt((Integer) d);
-        } else if (d instanceof Long) {
-          writeLong((Long) d);
-        } else if (d instanceof Float) {
-          writeFloat((Float) d);
-        } else if (d instanceof Double) {
-          writeDouble((Double) d);
-        } else if (d instanceof String) {
-          writeString((String) d);
-        } else if (d instanceof DataByteArray) {
-          writeBuffer((DataByteArray) d);
-        } else {
-          throw new ExecException("Unknown data type");
-        }
-      } else { // if d is null, write nothing except ','
-        writeNull();
-      }
-    }
-  }
-}
\ No newline at end of file
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/types/Partition.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/types/Partition.java
deleted file mode 100644
index 2843d61ce..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/types/Partition.java
+++ /dev/null
@@ -1,1407 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.types;
-
-import org.apache.pig.data.Tuple;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Set;
-import java.util.Map;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.io.StringReader;
-import org.apache.hadoop.io.BytesWritable;
-import java.io.IOException;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.parser.TableSchemaParser;
-import org.apache.hadoop.zebra.parser.TableStorageParser;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.ColumnType;
-
-/**
- * Partition is a column group management class. Its responsibilities include
- * storing column groups, handle column groups spliting and stitching by
- * insertions and queries respectively.
- */
-public class Partition {
-  /**
-   * Storage split types
-   */
-  public enum SplitType {
-    NONE, RECORD, COLLECTION, MAP
-  }
-
-  /*
-   * This class holds the column group information as generated by
-   * TableStorageParser.
-   */
-  public class PartitionInfo {
-    private Map<Schema.ColumnSchema, PartitionFieldInfo> fieldMap =
-        new HashMap<Schema.ColumnSchema, PartitionFieldInfo>();
-    private Map<String, HashSet<ColumnMappingEntry>> mColMap =
-        new HashMap<String, HashSet<ColumnMappingEntry>>();
-    private Schema mSchema;
-
-    public PartitionInfo(Schema schema) {
-      mSchema = schema;
-    }
-    
-    public Map<String, HashSet<ColumnMappingEntry>> getColMap() {
-        return mColMap;
-    }
-
-    /*
-     * holds a mapping between a column in the table schema and its
-     * corresponding (sub)column group index and the field index inside the
-     * column group
-     */
-    class ColumnMappingEntry implements Comparable<ColumnMappingEntry>{
-      private int cgIndex = -1, fieldIndex = -1;
-      private Schema.ColumnSchema fs;
-      private HashSet<String> keySet;
-
-      public ColumnMappingEntry(int ri, int fi, Schema.ColumnSchema fs) {
-        cgIndex = ri;
-        fieldIndex = fi;
-        this.fs = fs;
-      }
-
-      public ColumnMappingEntry() {
-      }
-
-      /**
-       * add map keys
-       * return false if any key already exists but no rollback!
-       */
-      public boolean addKeys(HashSet<String> keys, HashSet<String> columnKeySet)
-      {
-        if (keySet == null)
-          keySet = new HashSet<String>();
-        String key;
-        for (Iterator<String> it = keys.iterator(); it.hasNext(); )
-        {
-          key = it.next();
-          
-          // if the key is used in another CG?
-          if (!columnKeySet.add(key))
-            return false;
-          
-          if (!keySet.add(key))
-            return false;
-        }
-        return true;
-      }
-
-      public int getCGIndex() {
-        return cgIndex;
-      }
-
-      public int getFieldIndex() {
-        return fieldIndex;
-      }
-
-      public Schema.ColumnSchema getColumnSchema() {
-        return fs;
-      }
-
-      public boolean invalid() {
-        return (cgIndex == -1 && fieldIndex == -1);
-      }
-      
-      public int compareTo(ColumnMappingEntry anotherEntry) {   	  
-    	  int r = anotherEntry.getCGIndex();
-    	  if (r != this.cgIndex)
-    		  return this.cgIndex - r;
-    	  else {
-    		  int f = anotherEntry.getFieldIndex();
-    		  return this.fieldIndex -f;
-    	  }	
-      }
-      
-      HashSet<String> getKeys() {
-        return keySet;
-      }
-      
-
-    }
-
-    /**
-     * This class holds the column group info for a (sub)column which is a unit
-     * in a column group
-     */
-    class PartitionFieldInfo {
-      private HashSet<PartitionInfo.ColumnMappingEntry> mSplitMaps =
-          new HashSet<ColumnMappingEntry>();
-      private ColumnMappingEntry mCGIndex = null;
-      private String mCGName = null; // fully qualified name
-      private HashSet<String> keySet = null;
-      private SplitType stype = SplitType.NONE;
-      private HashSet<String> splitChildren = new HashSet<String>();
-
-      /**
-       * set a MAP key split (sub)column
-       * returns false if sanity check fails
-       */
-      boolean setKeyCGIndex(int ri, int fi, String name, Schema.ColumnSchema fs, HashSet<String> keys) {
-        Partition.PartitionInfo.ColumnMappingEntry cme =
-              new Partition.PartitionInfo.ColumnMappingEntry( ri, fi, fs);
-        mSplitMaps.add(cme);
-        // multiple map splits on one MAP column is allowed!
-        if (keySet == null)
-          keySet = new HashSet<String>();
-        return cme.addKeys(keys, keySet);
-      }
-
-      /**
-       * set a record field split (sub)column
-       */
-      boolean setCGIndex(int ri, int fi, String name, Schema.ColumnSchema fs) {
-        if (mCGIndex != null) return false;
-        mCGIndex = new Partition.PartitionInfo.ColumnMappingEntry(ri, fi, fs);
-        mCGName = name;
-        return true;
-      }
-
-      ColumnMappingEntry getCGIndex() {
-        return mCGIndex;
-      }
-
-      String getCGName() {
-        return mCGName;
-      }
-
-      /**
-       * set the split type of a (sub)column
-       */
-      void setSplit(SplitType st, SplitType cst, String name, String childName, boolean splitChild) throws ParseException {
-        if (st == stype)
-        {
-          // multiple MAP splits of a field and its children on different keys are ok
-          if (st == SplitType.MAP || cst == SplitType.MAP)
-            return;
-        }
-        if (splitChild)
-        {
-          if (stype != SplitType.NONE && splitChildren.isEmpty())
-            throw new ParseException("Split on "+name+" is set at different levels.");
-          splitChildren.add(childName);
-        } else {
-          if (splitChildren.contains(childName))
-            throw new ParseException("Split on "+name+" is set at different levels.");
-        }
-        stype = st;
-      }
-
-      /*
-       * creates a default "catch-all" column schema if necessary
-       */
-      void generateDefaultCGSchema(Schema.ColumnSchema fs0, Schema schema,
-          int defaultCGIndex,
-          Map<String, HashSet<PartitionInfo.ColumnMappingEntry>> colmap, String prefix,
-          Map<Schema.ColumnSchema, PartitionFieldInfo> fmap)
-          throws ParseException {
-        if (schema == null) throw new AssertionError("Interal Logic Error.");
-        String name = fs0.getName();
-        if (prefix == null) prefix = name;
-        else if (name != null && !name.isEmpty())
-          prefix += "." + name;
-        Schema.ColumnSchema fs;
-        Schema schema0 = fs0.getSchema();
-        for (int i = 0; i < schema0.getNumColumns(); i++) {
-          fs = schema0.getColumn(i);
-          PartitionFieldInfo pi;
-          if ((pi = fmap.get(fs)) == null)
-            fmap.put(fs, pi = new PartitionFieldInfo());
-
-          /*
-           * won't go down for MAP split because only one level MAP split is
-           * supported now
-           */
-          if (pi.stype != SplitType.NONE && pi.stype != SplitType.MAP) {
-            /* go to the lower level */
-            pi.generateDefaultCGSchema(fs, schema, defaultCGIndex, colmap,
-                prefix, fmap);
-          }
-          else if (pi.mCGIndex != null) {
-            HashSet<ColumnMappingEntry> cms = mColMap.get(pi.mCGName);
-            if (cms == null)
-            {
-              cms = new HashSet<ColumnMappingEntry>();
-              colmap.put(pi.mCGName, cms);
-            }
-            cms.add(pi.mCGIndex);
-            if (!pi.mSplitMaps.isEmpty()) {
-              for (Iterator<ColumnMappingEntry> it = pi.mSplitMaps.iterator();
-                   it.hasNext(); )
-              {
-                cms.add(it.next());
-              }
-            }
-          } else {
-            String name0 = fs.getName();
-            HashSet<ColumnMappingEntry> cms = colmap.get(prefix+"."+name0);
-            if (cms == null)
-            {
-              cms = new HashSet<ColumnMappingEntry>();
-              colmap.put(prefix+"."+name0, cms);
-            }
-            pi.mCGIndex = new ColumnMappingEntry(defaultCGIndex,
-                  schema.getNumColumns(), fs);
-            pi.mCGName = prefix+"."+name0;
-            cms.add(pi.mCGIndex);
-
-            schema.add(new Schema.ColumnSchema(prefix + "." + name0,
-                fs.getSchema(), fs.getType()));
-            if (!pi.mSplitMaps.isEmpty()) {
-              for (Iterator<ColumnMappingEntry> it = pi.mSplitMaps.iterator();
-                   it.hasNext(); )
-              {
-                cms.add(it.next());
-              }
-            }
-          }
-        }
-      }
-    }
-
-    /**
-     * set a MAP key split (sub)column
-     */
-    public boolean setKeyCGIndex(Schema.ColumnSchema fs, int ri, int fi, String name, HashSet<String> keys) {
-      PartitionFieldInfo pi;
-      if ((pi = fieldMap.get(fs)) == null) {
-        pi = new PartitionFieldInfo();
-        fieldMap.put(fs, pi);
-      }
-
-      return pi.setKeyCGIndex(ri, fi, name, fs.getSchema().getColumn(0), keys);
-    }
-
-    /**
-     * set a record field split (sub)column
-     */
-    public boolean setCGIndex(Schema.ColumnSchema fs, int ri, int fi, String name) {
-      PartitionFieldInfo pi;
-      if ((pi = fieldMap.get(fs)) == null) {
-        pi = new PartitionFieldInfo();
-        fieldMap.put(fs, pi);
-      }
-      return pi.setCGIndex(ri, fi, name, fs);
-    }
-
-    /**
-     * set the split type of a (sub)column
-     */
-    void setSplit(Schema.ColumnSchema fs, SplitType st, SplitType cst, String name, String childName, boolean splitChild) throws ParseException {
-      PartitionFieldInfo pi;
-      if ((pi = fieldMap.get(fs)) == null) {
-        pi = new PartitionFieldInfo();
-        fieldMap.put(fs, pi);
-      }
-      pi.setSplit(st, cst, name, childName, splitChild);
-    }
-
-    /*
-     * creates a default "catch-all" CG schema if necessary
-     */
-    public CGSchema generateDefaultCGSchema(String name, String compressor,
-        String serializer, String owner, String group, 
-        short perm, final int defaultCGIndex, String comparator) throws ParseException {
-      Schema schema = new Schema();
-      Schema.ColumnSchema fs;
-      for (int i = 0; i < mSchema.getNumColumns(); i++) {
-        fs = mSchema.getColumn(i);
-        PartitionFieldInfo pi;
-        if ((pi = fieldMap.get(fs)) == null)
-          fieldMap.put(fs, pi = new PartitionFieldInfo());
-
-        /*
-         * won't go down for MAP split because only one level MAP split is
-         * supported now
-         */
-        if (pi.stype != SplitType.NONE && pi.stype != SplitType.MAP)
-        {
-          /* go to the lower level */
-          pi.generateDefaultCGSchema(fs, schema, defaultCGIndex, mColMap, null,
-             fieldMap);
-        } else if (pi.mCGIndex != null) {
-          HashSet<ColumnMappingEntry> cms = mColMap.get(pi.mCGName);
-          if (cms == null)
-          {
-            cms = new HashSet<ColumnMappingEntry>();
-            mColMap.put(pi.mCGName, cms);
-          }
-          cms.add(pi.mCGIndex);
-          for (Iterator<ColumnMappingEntry> it = pi.mSplitMaps.iterator();
-               it.hasNext(); )
-          {
-            cms.add(it.next());
-          }
-        }
-        else {
-          HashSet<ColumnMappingEntry> cms = mColMap.get(fs.getName());
-          if (cms == null)
-          {
-            cms = new HashSet<ColumnMappingEntry>();
-            mColMap.put(fs.getName(), cms);
-          }
-          ColumnMappingEntry cme = new ColumnMappingEntry(defaultCGIndex,
-                schema.getNumColumns(), fs);
-          cms.add(cme);
-          setCGIndex(fs, defaultCGIndex, schema.getNumColumns(), fs.getName());
-          schema.add(fs);
-          for (Iterator<ColumnMappingEntry> it = pi.mSplitMaps.iterator();
-               it.hasNext(); )
-          {
-            cms.add(it.next());
-          }
-        }
-      }
-      CGSchema defaultSchema =
-          (schema.getNumColumns() == 0 ? null : new CGSchema(schema, false, comparator, name, serializer, compressor, owner, group, perm));
-      return defaultSchema;
-    }
-
-    /**
-     * returns "hash key-to-(sub)column" map on a (sub)column which is MAP-split
-     * across different hash keys
-     */
-    public HashSet<PartitionInfo.ColumnMappingEntry> getSplitMap(
-        Schema.ColumnSchema fs) {
-      PartitionFieldInfo pi;
-      if ((pi = mPartitionInfo.fieldMap.get(fs)) == null) {
-        pi = new PartitionFieldInfo();
-        mPartitionInfo.fieldMap.put(fs, pi);
-      }
-      return pi.mSplitMaps;
-    }
-  }
-
-  /*
-   * This class records all column groups required and their corresponding users
-   * of tuples and their projections in turn.
-   */
-  private class CGEntry {
-    /* reference to needing partitioned columns */
-    private ArrayList<PartitionedColumn> mSources = null; // reference to
-    // needing partitioned
-    // columns
-    private ArrayList<String> mProjections = null; // projections
-    private int mSize = 0; // number of users
-    private int mCGIndex = -1;
-    private Tuple mTuple = null;
-    // map of a projection index to a set of interested hash keys
-    private HashMap<Integer, HashSet<String>> mKeyMap;
-
-    CGEntry(int cgindex) {
-      mCGIndex = cgindex;
-      mSources = new ArrayList<PartitionedColumn>();
-      mProjections = new ArrayList<String>();
-    }
-
-    void addUser(Partition.PartitionedColumn src, String projection) {
-      mSources.add(src);
-      mProjections.add(projection);
-      src.setProjIndex(mSize++);
-    }
-
-    void addUser(Partition.PartitionedColumn src, String projection, HashSet<String> keys) {
-      mSources.add(src);
-      mProjections.add(projection);
-      if (mKeyMap == null) {
-        mKeyMap = new HashMap<Integer, HashSet<String>>();
-      }
-      mKeyMap.put(mSize, keys);
-      src.setProjIndex(mSize++);
-    }
-
-    void cleanup()
-    {
-      for (int i = 0;  i < mSources.size(); i++)
-        mSources.get(i).cleanup();
-    }
-
-    void insert(final BytesWritable key) throws ExecException {
-      for (int i = 0; i < mSize; i++) {
-        PartitionedColumn mSource = mSources.get(i);
-        ((Tuple) mTuple).set(mSource.getProjIndex(), mSource.getRecord());
-      }
-    }
-
-    void read() throws ExecException {
-      for (int i = 0; i < mSize; i++) {
-        PartitionedColumn mSource = mSources.get(i);    	  
-        mSource.setRecord(mTuple.get(mSource.getProjIndex()));
-      }
-    }
-
-    void setSource(Tuple tuple) {
-      mTuple = tuple;
-    }
-
-    int getCGIndex() {
-      return mCGIndex;
-    }
-
-    /**
-     * return untyped projection schema
-     */
-    String getProjection() throws ParseException {
-      String result = new String();
-      HashSet<String> keySet;
-      for (int i = 0; i < mProjections.size(); i++) {
-        if (i > 0) result += ",";
-        result += mProjections.get(i);
-        if (mKeyMap != null && (keySet = mKeyMap.get(i)) != null)
-        {
-          if (keySet.isEmpty())
-            throw new AssertionError(
-              "Internal Logical Error: Empty key map.");
-          result += "#{";
-          int j = 0;
-          for (Iterator<String> it = keySet.iterator(); it.hasNext(); j++)
-          {
-            if (j > 0)
-              result += "|";
-            result += it.next();
-          }
-          result += "}";
-        }
-      }
-      return result;
-    }
-  }
-
-  /**
-   * stitch and split execution class
-   */
-  private class PartitionedColumn {
-    private ArrayList<PartitionedColumn> mChildren = null; // partitioned
-    // children
-    private int mChildrenLen = 0;
-    private int mFieldIndex = -1; // field index in parent
-    private int mProjIndex = -1; // field index in CG projection: only used by
-    // a leaf
-    private Partition.SplitType mSplitType = Partition.SplitType.NONE;
-    private Object mTuple = null;
-    private boolean mNeedTmpTuple;
-    private HashSet<String> mKeys; // interested hash keys
-    private PartitionedColumn parent = null;
-
-    PartitionedColumn(int fi, boolean needTmpTuple)
-        throws IOException {
-      mFieldIndex = fi;
-      mNeedTmpTuple = needTmpTuple;
-    }
-
-    PartitionedColumn(int fi, Partition.SplitType st,
-        boolean needTmpTuple) throws IOException {
-      this(fi, needTmpTuple);
-      mSplitType = st;
-    }
-
-    void setKeys(HashSet<String> keys) {
-      mKeys = keys;
-    }
-
-    private void setParent(PartitionedColumn parent) {
-      this.parent = parent;
-    }
-
-    /**
-     * stitch op
-     */
-    @SuppressWarnings("unchecked")
-    void stitch() throws IOException {
-      PartitionedColumn child;
-      if (mSplitType == Partition.SplitType.NONE) {
-        for (int i = 0; i < mChildrenLen; i++) {
-          child = mChildren.get(i);
-          ((Tuple) mTuple).set(child.mFieldIndex, child.getRecord());
-        }
-      }
-      else {
-        // stitch MAP-key partitioned hashes
-        for (int i = 0; i < mChildrenLen; i++) {
-          child = mChildren.get(i);
-          // add the new (key,value) to the existing map
-          ((Map<String, Object>) mTuple)
-              .putAll((Map<String, Object>)child.getRecord());
-        }
-      }
-    }
-
-    /**
-     * split op
-     */
-    @SuppressWarnings("unchecked")
-    void split() throws IOException {
-      PartitionedColumn child;
-      if (mSplitType == SplitType.NONE) {
-        // record split
-        for (int i = 0; i < mChildrenLen; i++) {
-          child = mChildren.get(i);
-          child.setRecord(((Tuple) mTuple).get(child.mFieldIndex));
-        }
-      }
-      else {
-        // split MAP columns excluding the hashes already in split key CGs
-        String key;
-
-        // make a clone so the input MAP is intact after keys are yanked
-        Object newmap = ((HashMap<String, Object>) mTuple).clone();
-        mTuple = newmap;
-        Map<String, Object> map_column =
-            (Map<String, Object>) (mTuple);
-        Map<String, Object> childMap;
-        Object value;
-        for (int i = 0; i < mChildrenLen; i++) {
-          child = mChildren.get(i);
-          childMap = (Map<String, Object>) child.getRecord();
-          for (Iterator<String> it = child.mKeys.iterator(); it.hasNext();)
-          {
-            key = it.next();
-            if ((value = map_column.get(key)) != null)
-            {
-              childMap.put(key, value);
-              map_column.remove(key);
-            }
-          }
-        }
-      }
-    }
-
-    Object getRecord() {
-      return mTuple;
-    }
-
-    void setRecord(Object t) {
-      mTuple = t;
-    }
-
-    void addChild(PartitionedColumn child) {
-      if (mChildren == null) mChildren = new ArrayList<PartitionedColumn>();
-      mChildren.add(child);
-      mChildrenLen++;
-      child.setParent(this);
-    }
-
-    void cleanup() {
-      if (parent != null) {
-        parent.removeChild(this);
-      }
-      if (mNeedTmpTuple && mTuple != null)
-        mTuple = null;
-    }
-
-    void removeChild(PartitionedColumn child)
-    {
-      for (int i = 0; i < mChildrenLen; i++)
-      {
-        if (mChildren.get(i) == child)
-        {
-          mChildren.remove(i);
-          mChildrenLen--;
-          i--;
-        }
-      }
-    }
-
-    void setProjIndex(int projindex) {
-      mProjIndex = projindex;
-    }
-
-    int getProjIndex() {
-      return mProjIndex;
-    }
-
-    void createTmpTuple() throws IOException {
-      if (mNeedTmpTuple)
-      {
-        int size = (mChildrenLen > 0 ? mChildrenLen : 1);
-        mTuple = TypesUtils.createTuple(size);
-      }
-    }
-
-    /**
-     * create maps if necessary
-     */
-    void createMap()
-    {
-      mTuple = new HashMap<String, Object>();
-    }
-
-    /**
-     * clear map
-     */
-    @SuppressWarnings("unchecked")
-    void clearMap()
-    {
-      ((Map)mTuple).clear();
-    }
-  }
-
-  private HashMap<Integer, CGEntry> mCGs = null; // involved CGs
-  private CGEntry[] mCGList = new CGEntry[0];
-  private ArrayList<PartitionedColumn> mExecs = null; // stitches to be
-  // performed in sequence:
-  // called by LOAD
-  private int mStitchSize = 0; // number of the stitches
-  private int mSplitSize = 0; // number of the splits
-  private Schema mSchema = null;
-  private CGSchema[] mCGSchemas;
-  private PartitionInfo mPartitionInfo;
-  private Projection mProjection = null;
-  private ArrayList<PartitionedColumn> mPCNeedTmpTuple = new ArrayList<PartitionedColumn>();
-  private ArrayList<PartitionedColumn> mPCNeedMap = new ArrayList<PartitionedColumn>();
-  private String comparator;
-  private boolean mSorted;
-  private SortInfo mSortInfo;
-
-  /*
-   * ctor used for LOAD
-   */
-  public Partition(Schema schema, Projection projection, String storage, String comparator)
-      throws ParseException, IOException {
-    mSchema = schema;
-    TableStorageParser sparser =
-        new TableStorageParser(new StringReader(storage), this, mSchema, comparator);
-    mPartitionInfo = new PartitionInfo(schema);
-    ArrayList<CGSchema> cgschemas = new ArrayList<CGSchema>();
-    sparser.StorageSchema(cgschemas);
-    mCGSchemas = cgschemas.toArray(new CGSchema[cgschemas.size()]);
-    mProjection = projection;
-    Schema projSchema = projection.getProjectionSchema();
-    int size = projSchema.getNumColumns();
-    HashMap<PartitionInfo.ColumnMappingEntry, HashSet<String>> cgindices;
-    PartitionInfo.ColumnMappingEntry cgindex;
-    mCGs = new HashMap<Integer, CGEntry>();
-    Schema.ColumnSchema fs;
-    CGEntry cgentry;
-    Schema.ParsedName pname = new Schema.ParsedName();
-    mExecs = new ArrayList<PartitionedColumn>();
-    PartitionedColumn parCol, curCol = new PartitionedColumn(-1, false); // top
-    // level:
-    // target
-    // of
-    // stitch
-    String name;
-    HashSet<String> projectedKeys;
-    for (int i = 0; i < size; i++) // depth-first
-    {
-      if (projSchema.getColumn(i) == null)
-        continue;
-      name = projSchema.getColumn(i).getName();
-      pname.setName(name);
-      projectedKeys = (projection.getKeys() == null ? null :
-                       projection.getKeys().get(projSchema.getColumn(i)));
-      cgindices = getColMapping(schema, name, pname, projectedKeys);
-      if (cgindices != null) {
-        // either needs split of a CG column or the projection is a CG proper
-        fs = schema.getColumnSchema(name);
-        if (getSplitMap(fs).isEmpty()) {
-          if (cgindices.size() != 1)
-            throw new AssertionError( "Internal Logical Error: one RECORD split is expected.");
-          Set<Map.Entry<PartitionInfo.ColumnMappingEntry, HashSet<String>>> entrySet = cgindices.entrySet();
-          Map.Entry<PartitionInfo.ColumnMappingEntry, HashSet<String>> mapentry;
-          mapentry = entrySet.iterator().next();
-          cgindex = mapentry.getKey();
-          if (cgindex == null)
-            throw new AssertionError( "Internal Logical Error: RECORD does not have a CG index.");
-          cgentry = getCGEntry(cgindex.getCGIndex());
-          parCol = new PartitionedColumn(i, false);
-          cgentry.addUser(parCol, name);
-          curCol.addChild(parCol);
-        } else {
-          // leaves are not added to exec!
-          if (!getSplitMap(fs).isEmpty()) {
-            // this subtype is MAP-split
-            handleMapStitch(pname, curCol, fs, i,
-                getCGIndex(fs).getFieldIndex(), cgindices);
-          }
-        }
-      }
-      else {
-        // a composite column of CGs
-        fs = schema.getColumnSchema(name);
-        if (fs == null) continue;
-        parCol = new PartitionedColumn(i, true);
-        mPCNeedTmpTuple.add(parCol);
-        buildStitch(fs, pname, parCol); // depth-first
-        mExecs.add(parCol);
-        mStitchSize++;
-        curCol.addChild(parCol);
-      }
-    }
-    for (int i = 0; i < mPCNeedTmpTuple.size(); i++)
-      mPCNeedTmpTuple.get(i).createTmpTuple();
-    mExecs.add(curCol);
-    mStitchSize++;
-  }
-
-  /*
-   * ctor used by STORE
-   */
-  public Partition(final String schema, final String storage, String comparator, String sortColumns)
-        throws ParseException, IOException
-  {
-    TableSchemaParser parser = new TableSchemaParser(new StringReader(schema));
-    mSchema = parser.RecordSchema(null);
-    mSortInfo = SortInfo.parse(sortColumns, mSchema, comparator);
-    mSorted = (mSortInfo != null && mSortInfo.size() > 0);
-    this.comparator = (mSorted ? mSortInfo.getComparator() : "");
-    storeConst(storage);
-  }
-
-  public Partition(String schema, final String storage, String comparator)
-      throws ParseException, IOException
-  {
-    TableSchemaParser parser = new TableSchemaParser(new StringReader(schema));
-    mSchema = parser.RecordSchema(null);
-    this.comparator = comparator;
-    storeConst(storage);
-  }
-
-  private void storeConst(final String storage)
-      throws ParseException, IOException
-  {
-    mPartitionInfo = new PartitionInfo(mSchema);
-    TableStorageParser sparser =
-      new TableStorageParser(new StringReader(storage), this, mSchema, this.comparator);
-    ArrayList<CGSchema> cgschemas = new ArrayList<CGSchema>();
-    sparser.StorageSchema(cgschemas);
-    mCGSchemas = cgschemas.toArray(new CGSchema[cgschemas.size()]);
-    int size = mSchema.getNumColumns();
-    PartitionInfo.ColumnMappingEntry cgindex;
-    mCGs = new HashMap<Integer, CGEntry>();
-    CGEntry cgentry;
-    mExecs = new ArrayList<PartitionedColumn>();
-    PartitionedColumn parCol, curCol = new PartitionedColumn(-1, false); // top
-    // level:
-    // target
-    // of
-    // stitch
-    mExecs.add(curCol); // breadth-first
-    mSplitSize++;
-    Schema.ColumnSchema fs;
-    for (int i = 0; i < size; i++) {
-      fs = mSchema.getColumn(i);
-      cgindex = getCGIndex(fs);
-      if (cgindex != null) {
-        // a CG field
-        cgentry = getCGEntry(cgindex.getCGIndex());
-        // leaves are not added to exec!
-        if (getSplitMap(fs).isEmpty()) {
-          parCol = new PartitionedColumn(i, false);
-          cgentry.addUser(parCol, null); // null mean all schema
-          parCol.setProjIndex(cgindex.getFieldIndex());
-          curCol.addChild(parCol);
-        } else {
-          // this subtype is MAP-split
-          // => need to add splits for all split keys
-          handleMapSplit(curCol, fs, i, cgentry, cgindex.getFieldIndex());
-        }
-      }
-      else {
-        // a composite column of CGs
-        parCol = new PartitionedColumn(i, false);
-        mExecs.add(parCol); // breadth-first
-        mSplitSize++;
-        buildSplit(fs, parCol);
-        curCol.addChild(parCol);
-      }
-    }
-    
-    for (int i = 0; i < mPCNeedTmpTuple.size(); i++)
-      mPCNeedTmpTuple.get(i).createTmpTuple();
-
-    for (int i = 0; i < mPCNeedMap.size(); i++)
-      mPCNeedMap.get(i).createMap();
-  }
-
-  public SortInfo getSortInfo() {
-    return mSortInfo;
-  }
-
-  public boolean isSorted() {
-    return mSorted;
-  }
-
-  public String getComparator() {
-    return comparator;
-  }
-
-  /**
-   * returns table schema
-   */
-  public Schema getSchema() {
-    return mSchema;
-  }
-
-  /*
-   * returns the partition info created by the parser
-   */
-  public PartitionInfo getPartitionInfo() {
-    return mPartitionInfo;
-  }
-
-  /*
-   * returns all column group schemas
-   */
-  public CGSchema[] getCGSchemas() {
-    return mCGSchemas;
-  }
-
-  /*
-   * returns a particular column group schemas
-   */
-  public CGSchema getCGSchema(int index) {
-    if (mCGSchemas == null) return null;
-    return mCGSchemas[index];
-  }
-
-  /*
-   * search from the most specific name until the least specific: if none found
-   * return null; In addition, the fq name portion after the matched CG's fq
-   * name is returned in pn. For MAP split columns, it returns the catch-all
-   * CG not the CGs with specific keys if this is not a key projection; otherwise
-   * a map of CG fields to projected keys is returned
-   */
-  private HashMap<PartitionInfo.ColumnMappingEntry, HashSet<String>> getColMapping(Schema schema,
-      String name, Schema.ParsedName pn, HashSet<String> projectedKeys) throws ParseException {
-    Map<String, HashSet<PartitionInfo.ColumnMappingEntry>> colmap =
-        mPartitionInfo.mColMap;
-    int fromIndex = name.length() - 1, lastHashIndex, lastFieldIndex;
-
-    HashSet<PartitionInfo.ColumnMappingEntry> results = colmap.get(name);
-    HashMap<PartitionInfo.ColumnMappingEntry, HashSet<String>> result = null;
-    HashSet<String> keys;
-    String ancestorName;
-    boolean map = false;
-    if (results != null)
-    {
-      if (projectedKeys != null)
-      {
-        pn.setDT(ColumnType.MAP);
-        map = true;
-      } else {
-        pn.setDT(ColumnType.ANY);
-        PartitionInfo.ColumnMappingEntry cme;
-        for (Iterator<PartitionInfo.ColumnMappingEntry> it = results.iterator(); it.hasNext(); )
-        {
-          cme = it.next();
-          if (cme.getKeys() == null)
-          {
-            // no specific keys are interested. Either a MAP-split without key or a RECORD-split is OK
-            result = new HashMap<PartitionInfo.ColumnMappingEntry, HashSet<String>>();
-            result.put(cme, null);
-            return result;
-          }
-        }
-        return null;
-      }
-    }
-
-    while (results == null) {
-      lastHashIndex = name.lastIndexOf('#', fromIndex);
-      lastFieldIndex = name.lastIndexOf('.', fromIndex);
-      if (lastHashIndex == -1 && lastFieldIndex == -1) break;
-      else if (lastHashIndex == -1) {
-        fromIndex = lastFieldIndex;
-      }
-      else if (lastFieldIndex == -1) {
-        fromIndex = lastHashIndex;
-        map = true;
-      }
-      else {
-        if (lastHashIndex == lastFieldIndex - 1
-            || lastFieldIndex == lastHashIndex - 1) break;
-        fromIndex =
-            (lastFieldIndex > lastHashIndex ? lastFieldIndex : lastHashIndex);
-      }
-      if (fromIndex <= 0) break;
-      ancestorName = name.substring(0, fromIndex);
-      results = colmap.get(ancestorName);
-      fromIndex--;
-    }
-    if (results != null) {
-      if (map)
-      {
-        // build a HashMap from ColumnGroupMappingEntry to a set of projected keys for MAP-split
-        if (results.isEmpty())
-          throw new AssertionError( "Internal Logical Error: split is expected.");
-        PartitionInfo.ColumnMappingEntry thisCG, defaultCG = null;
-        boolean found;
-        String projectedKey;
-        for (Iterator<String> projectedKeyIt = projectedKeys.iterator(); projectedKeyIt.hasNext();)
-        {
-          projectedKey = projectedKeyIt.next();
-          found = false;
-          thisCG = null;
-          for (Iterator<PartitionInfo.ColumnMappingEntry> it = results.iterator();
-              it.hasNext(); )
-          {
-            thisCG = it.next();
-            keys = thisCG.getKeys();
-            if (keys == null)
-            {
-              defaultCG = thisCG;
-            } else {
-              if (keys.contains(projectedKey))
-              {
-                found = true;
-                break;
-              }
-            }
-          }
-          
-          if (!found)
-          {
-            if (defaultCG == null)
-              throw new AssertionError( "Internal Logical Error: default MAP split CG is missing.");
-
-            thisCG =defaultCG;
-          }
-          if (result == null)
-            result = new HashMap<PartitionInfo.ColumnMappingEntry, HashSet<String>>();
-          if ((keys = result.get(thisCG)) == null)
-          {
-            keys = new HashSet<String>();
-            result.put(thisCG, keys);
-          }
-          keys.add(projectedKey);
-        }
-        if (result == null)
-          if (results.isEmpty())
-           throw new AssertionError( "Internal Logical Error: Default MAP split column is missing.");
-      } else {
-        // a RECORD-split
-        if (results.size() != 1)
-         throw new AssertionError(
-           "Internal Logical Error: A single split is expected.");
-        result = new HashMap<PartitionInfo.ColumnMappingEntry, HashSet<String>>();
-        result.put(results.iterator().next(), null);
-        return result;
-      }
-      // discard the CG fq name
-      fromIndex += 2;
-      if (!map)
-      {
-        // no need to crawl down after encountering a MAP-split subcolumn
-        pn.setName(name.substring(fromIndex));
-        pn.parseName(results.iterator().next().getColumnSchema());
-      }
-    }
-    return result;
-  }
-
-  /**
-   * recursively build stitch executions
-   * @throws IOException 
-   */
-  private void buildStitch(Schema.ColumnSchema fs, Schema.ParsedName pn,
-      PartitionedColumn parent) throws ParseException, IOException {
-    // depth-first traversal
-    CGEntry cgentry = null;
-    PartitionedColumn parCol;
-
-    Schema schema = fs.getSchema();
-    if (schema != null && schema.getNumColumns() > 0) {
-      Schema.ColumnSchema child;
-      pn.parseName(fs);
-      Schema.ParsedName oripn = new Schema.ParsedName();
-      for (int i = 0; i < schema.getNumColumns(); i++) {
-        oripn.setName(new String(pn.getName()), pn.getDT());
-        child = schema.getColumn(i);
-        if (getCGIndex(child) == null) {
-          // not a CG: go one level lower
-          parCol = new PartitionedColumn(i, true);
-          mPCNeedTmpTuple.add(parCol);
-          buildStitch(child, oripn, parCol); // depth-first
-          mExecs.add(parCol);
-          mStitchSize++;
-          parent.addChild(parCol);
-        }
-        else {
-          if (getSplitMap(child).isEmpty()) {
-            // this subtype is not MAP-split
-            cgentry = getCGEntry(getCGIndex(child).getCGIndex());
-            parCol = new PartitionedColumn(i, false);
-            cgentry.addUser(parCol, getCGName(child));
-            parent.addChild(parCol);
-          }
-          else {
-            // this subtype is MAP-split
-            Map<String, HashSet<PartitionInfo.ColumnMappingEntry>> colmap =
-              mPartitionInfo.mColMap;
-            HashSet<PartitionInfo.ColumnMappingEntry> msplits = colmap.get(getCGName(child));
-            HashMap<PartitionInfo.ColumnMappingEntry, HashSet<String>> cgindices;
-            cgindices = new HashMap<PartitionInfo.ColumnMappingEntry, HashSet<String>>();
-            PartitionInfo.ColumnMappingEntry cme;
-            for (Iterator<PartitionInfo.ColumnMappingEntry> it = msplits.iterator(); it.hasNext(); )
-            {
-              cme = it.next();
-
-              // all keys must be included
-              if (cme.getKeys() == null)
-                cgindices.put(cme, cme.getKeys());
-            }
-            handleMapStitch(oripn, parent, child, i, getCGIndex(
-                child).getFieldIndex(), cgindices);
-          }
-        }
-      }
-    }
-    else throw new AssertionError(
-        "Internal Logical Error: Leaf type must have a CG ancestor or is CG itself.");
-  }
-
-  /**
-   * build stitches on a MAP split (sub)column
-   * @throws IOException 
-   */
-  private void handleMapStitch(Schema.ParsedName pn,
-      PartitionedColumn parent, Schema.ColumnSchema child, int i,
-      int fi, HashMap<PartitionInfo.ColumnMappingEntry, HashSet<String>> cgindices) throws IOException {
-    CGEntry cgentry;
-    if (pn.getDT() == ColumnType.ANY) {
-      // this subtype is MAP split and the projection is on the whole MAP:
-      // => need to add stitches for all split keys
-
-      // first the map partitioned column that contain all non-key-partitioned
-      // hashes
-      if (cgindices == null || cgindices.size() != 1) {
-        throw new AssertionError(
-            "Internal Logical Error: Invalid map key size.");
-      }
-      
-      Set<Map.Entry<PartitionInfo.ColumnMappingEntry, HashSet<String>>> entrySet = cgindices.entrySet();
-      Map.Entry<PartitionInfo.ColumnMappingEntry, HashSet<String>> mapentry;
-      mapentry = entrySet.iterator().next();
-      if (mapentry.getValue() != null)
-        throw new AssertionError( "Internal Logical Error: RECORD should not have a split key map.");
-      
-      cgentry = getCGEntry(getCGIndex(child).getCGIndex());
-
-      PartitionedColumn mapParCol =
-          new PartitionedColumn(i, Partition.SplitType.MAP, false);
-      cgentry.addUser(mapParCol, getCGName(child));
-      mExecs.add(mapParCol); // not a leaf : MAP stitch needed
-      mStitchSize++;
-      parent.addChild(mapParCol);
-
-      // the map-key partitioned columns:
-      HashSet<PartitionInfo.ColumnMappingEntry> splitMap =
-          getSplitMap(child);
-      PartitionInfo.ColumnMappingEntry cgindex;
-      int index;
-      HashSet<Integer> projectedCGs = new HashSet<Integer>(); 
-      for (Iterator<PartitionInfo.ColumnMappingEntry> it = splitMap.iterator(); it.hasNext();) {
-        cgindex = it.next();
-        index = cgindex.getCGIndex();
-        cgentry = getCGEntry(index);
-        // if the CG is already included in this sub-column stitch, then no need
-        // to add it as a separate stitch since all keys therein is already included
-        if (!projectedCGs.contains(index))
-        {
-          PartitionedColumn parCol =
-             new PartitionedColumn(0, false);
-          // mPCNeedTmpTuple.add(parCol);
-          cgentry.addUser(parCol, getCGName(child), cgindex.getKeys());
-          mapParCol.addChild(parCol); // contribute to the non-key-partitioned
-         // hashes
-         projectedCGs.add(index);
-        }
-      }
-    }
-    else {
-      // this sub-type is MAP split and the projection is on another key which
-      // is not a split key:
-      // => need to add a specific key stitch
-      if (cgindices == null) {
-        throw new AssertionError(
-            "Internal Logical Error: MAP key set is empty.");
-      }
-
-      Set<Map.Entry<PartitionInfo.ColumnMappingEntry, HashSet<String>>> entrySet = cgindices.entrySet();
-      HashSet<String> projectedKeys;
-      Map.Entry<PartitionInfo.ColumnMappingEntry, HashSet<String>> mapentry;
-      boolean needParent = (entrySet.size() > 1);
-      boolean newParent = false;
-      PartitionedColumn parCol;
-      for (Iterator<Map.Entry<PartitionInfo.ColumnMappingEntry, HashSet<String>>> it = entrySet.iterator(); it.hasNext(); )
-      {
-        mapentry = it.next();
-        projectedKeys = mapentry.getValue();
-        cgentry = getCGEntry(mapentry.getKey().getCGIndex());
-        if (needParent)
-        {
-          parCol = new PartitionedColumn(i, Partition.SplitType.MAP, false);
-          mExecs.add(parCol); // not a leaf : MAP stitch needed
-          mStitchSize++;
-          parent.addChild(parCol);
-          parent = parCol;
-          needParent = false;
-          newParent = true;
-        } else {
-          parCol = new PartitionedColumn(newParent ? 0 : i, false);
-          parent.addChild(parCol);
-        }
-        cgentry.addUser(parCol, getCGName(child), projectedKeys);
-      }
-    }
-  }
-
-  private CGEntry getCGEntry(int cgindex)
-  {
-    CGEntry result = mCGs.get(cgindex);
-    if (result == null)
-    {
-      result = new CGEntry(cgindex);
-      mCGs.put(cgindex, result);
-      
-      // Constructing a collection of mCGs so that 
-      // we don't have to do it again [performance]
-      mCGList = (CGEntry[])mCGs.values().toArray(new CGEntry[mCGs.size()]);
-    }
-    return result;
-  }
-
-  /**
-   * recursively build split executions
-   * @throws IOException 
-   */
-  private void buildSplit(Schema.ColumnSchema fs, PartitionedColumn parent)
-      throws ParseException, IOException {
-    // depth-first traversal
-    CGEntry cgentry;
-    PartitionedColumn parCol;
-
-    Schema schema = fs.getSchema();
-    if (schema != null && schema.getNumColumns() > 0) {
-      Schema.ColumnSchema child;
-      for (int i = 0; i < schema.getNumColumns(); i++) {
-        child = schema.getColumn(i);
-        PartitionInfo.ColumnMappingEntry cgindex = getCGIndex(child);
-        if (cgindex == null) {
-          // not a CG: go one level lower
-          parCol = new PartitionedColumn(i, false);
-          mExecs.add(parCol); // breadth-first
-          mSplitSize++;
-          parent.addChild(parCol);
-          buildSplit(child, parCol);
-        }
-        else {
-          cgentry = getCGEntry(cgindex.getCGIndex());
-          if (getSplitMap(child).isEmpty()) {
-            // this subfield is not MAP-split
-            parCol = new PartitionedColumn(i, false);
-            cgentry.addUser(parCol, getCGName(child));
-            parCol.setProjIndex(cgindex.getFieldIndex());
-            parent.addChild(parCol);
-
-          }
-          else {
-            // this subfield is MAP-split
-            // => need to add splits for all split keys
-            handleMapSplit(parent, child, i, cgentry, cgindex.getFieldIndex());
-          }
-        }
-      }
-    }
-    else throw new AssertionError(
-        "Internal Logical Error: Leaf type must have a CG ancestor or is CG itself.");
-  }
-
-  /**
-   * build splits for MAP split (sub)columns
-   * 
-   * @throws IOException
-   */
-  private void handleMapSplit(PartitionedColumn parent,
-      Schema.ColumnSchema child, int i, CGEntry cgentry, int childProjIndex) throws ParseException, IOException {
-    // first the map partitioned column that contain all non-key-partitioned
-    // hashes
-    PartitionedColumn mapParCol =
-        new PartitionedColumn(i, Partition.SplitType.MAP, false);
-    cgentry.addUser(mapParCol, getCGName(child));
-    mapParCol.setProjIndex(childProjIndex);
-    mExecs.add(mapParCol); // not a leaf : MAP split needed
-    mSplitSize++;
-    parent.addChild(mapParCol);
-
-    // the map-key partitioned columns:
-    HashSet<PartitionInfo.ColumnMappingEntry> splitMaps = getSplitMap(child);
-    PartitionInfo.ColumnMappingEntry cgindex;
-    int index;
-    HashSet<String> keySet;
-    for (Iterator<PartitionInfo.ColumnMappingEntry> it = splitMaps.iterator(); it.hasNext();) {
-      cgindex = it.next();
-      index = cgindex.getCGIndex();
-      cgentry = getCGEntry(index);
-      keySet = cgindex.getKeys();
-      PartitionedColumn parCol;
-      parCol = new PartitionedColumn(0, false);
-      parCol.setKeys(keySet);
-      cgentry.addUser(parCol, getCGName(child), keySet);
-      parCol.setProjIndex(cgindex.getFieldIndex());
-      // contribute to the non-key-partitioned hashes
-      mapParCol.addChild(parCol);
-      mPCNeedMap.add(parCol); // children needs the tmp map
-    }
-  }
-
-  /**
-   * read in a tuple based on stitches
-   */
-  public void read(Tuple t) throws AssertionError, IOException, Exception {
-    if (mStitchSize == 0 || mCGs == null || mCGList.length == 0)
-      return;
-
-    // dispatch
-    mExecs.get(mStitchSize - 1).setRecord(t);
-
-    //Set<Map.Entry<Integer, CGEntry>> entrySet = mCGs.entrySet();
-    //Iterator<Map.Entry<Integer, CGEntry>> it = entrySet.iterator();
-    //while (it.hasNext())
-    //  it.next().getValue().read();
-
-    // read in CG data
-    for (int i = 0; i < mCGList.length; i++)
-      mCGList[i].read();
-
-    // dispatch
-    // mExecs.get(mStitchSize - 1).setRecord(t);
-
-    // start the stitch
-    for (int i = 0; i < mStitchSize; i++)
-      mExecs.get(i).stitch();
-
-    return;
-  }
-
-  /**
-   * insert a tuple after splits
-   */
-  public void insert(final BytesWritable key, final Tuple t)
-      throws AssertionError, IOException, Exception {
-    if (mSplitSize == 0 || mCGs == null || mCGList.length == 0)
-      throw new AssertionError("Empty Column Group List!");
-
-    // dispatch
-    mExecs.get(0).setRecord(t);
-    for (int i = 0; i < mPCNeedMap.size(); i++)
-      mPCNeedMap.get(i).clearMap();
-
-    for (int i = 0; i < mSplitSize; i++)
-      mExecs.get(i).split();
-
-    // insert CG data
-    //Set<Map.Entry<Integer, CGEntry>> entrySet = mCGs.entrySet();
-    //Iterator<Map.Entry<Integer, CGEntry>> it = entrySet.iterator();
-    //while (it.hasNext())
-    //  it.next().getValue().insert(key);
-    
-    for (int i = 0; i < mCGList.length; i++)
-      mCGList[i].insert(key);
-    
-    return;
-  }
-
-  /**
-   * sets the source tuple for the column group ops
-   */
-  public void setSource(Tuple[] tuples) throws ParseException {
-    if (tuples.length < mCGs.size())
-      throw new ParseException(
-          "Internal Logical Error: Invalid number of column groups");
-    for (int i = 0; i < tuples.length; i++) {
-      CGEntry mCG = mCGs.get(i);
-      if (mCG != null) {
-        if (tuples[i] == null) {
-          mCG.cleanup();
-          mCGs.remove(i);
-          // Constructing a collection of mCGs so that 
-          // we don't have to do it again [performance]
-          mCGList = (CGEntry[])mCGs.values().toArray(new CGEntry[mCGs.size()]);
-        } else {
-          mCG.setSource(tuples[i]);
-        }
-      }
-    }
-  }
-
-  /**
-   * returns projection schema for a particular column group
-   */
-  public String getProjection(int cgindex) throws ParseException {
-    CGEntry cgentry = mCGs.get(cgindex);
-    if (cgentry != null) return cgentry.getProjection();
-    return null;
-  }
-
-  /**
-   * returns table projection
-   */
-  public Projection getProjection() {
-    return mProjection;
-  }
-
-  public HashSet<PartitionInfo.ColumnMappingEntry> getSplitMap(
-      Schema.ColumnSchema fs) {
-    return mPartitionInfo.getSplitMap(fs);
-  }
-
-  public CGSchema generateDefaultCGSchema(String name, String compressor, String serializer,
-      String owner, String group, short perm, 
-      final int defaultCGIndex, String comparator) throws ParseException {
-    return mPartitionInfo.generateDefaultCGSchema(name, compressor, serializer,owner, group, perm,
-        defaultCGIndex, comparator);
-  }
-
-  public void setSplit(Schema.ColumnSchema fs, SplitType st, SplitType cst, String name, String childName, boolean splitChild) throws ParseException {
-    mPartitionInfo.setSplit(fs, st, cst, name, childName, splitChild);
-  }
-
-  public boolean setCGIndex(Schema.ColumnSchema fs, int ri, int fi, String name) {
-    return mPartitionInfo.setCGIndex(fs, ri, fi, name);
-  }
-
-  PartitionInfo.ColumnMappingEntry getCGIndex(Schema.ColumnSchema fs) {
-    PartitionInfo.PartitionFieldInfo pi;
-    if ((pi = mPartitionInfo.fieldMap.get(fs)) != null) return pi.getCGIndex();
-    return null;
-  }
-
-  /**
-   * @param fs
-   * @return fully qualified name for a CG column
-   */
-  String getCGName(Schema.ColumnSchema fs) {
-    PartitionInfo.PartitionFieldInfo pi;
-    if ((pi = mPartitionInfo.fieldMap.get(fs)) != null) return pi.getCGName();
-    return null;
-  }
-  
-  public boolean isCGNeeded(int i)
-  {
-    return mCGs.containsKey(i);
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/types/Projection.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/types/Projection.java
deleted file mode 100644
index 0a9e452ed..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/types/Projection.java
+++ /dev/null
@@ -1,228 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-package org.apache.hadoop.zebra.types;
-
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.ArrayList;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.parser.ParseException;
-
-/**
- * Projection for Table and Column Group
- */
-
-public class Projection {
-  public static final String source_table_vcolumn_name = "source_table";
-  private Schema mProjection; // schema as needed by the projection
-  private int mNumColumns;
-  private String mProjStr;
-  //TODO: change name and comment - to yan
-  private Schema mSchema; // the logical schema literally from projection string
-
-  /* the maps from the column in projection to a set of map keys
-   * the projection is interested. If null, all keys in schema are interested
-   */
-  private HashMap<Schema.ColumnSchema, HashSet<String>> mKeys;
-
-  /**
-   * ctor for full projection
-   */
-  public Projection(Schema s) {
-    mProjection = s;
-    if (s != null)
-    {
-      mNumColumns = s.getNumColumns();
-      mProjStr = s.toString();
-      mSchema = s;
-    }
-  }
-
-  /**
-   * if a column name is on a virtual column
-   */
-  public static boolean isVirtualColumn(String name)
-  {
-    if (name == null || name.isEmpty())
-      return false;
-    return name.trim().equalsIgnoreCase(source_table_vcolumn_name);
-  }
-
-  /**
-   * Get the indices of all virtual columns
-   */
-  public static Integer[] getVirtualColumnIndices(String projection)
-  {
-    if (projection == null)
-      return null;
-    String[] colnames = projection.trim().split(Schema.COLUMN_DELIMITER);
-    int size = colnames.length, realsize = 0;
-    ArrayList<Integer> vcol = new ArrayList();
-    
-    for (int i = 0; i < size; i++)
-    {
-      if (Projection.isVirtualColumn(colnames[i]))
-      {
-        vcol.add(i);
-      }
-    }
-    Integer[] result = null;
-    if (!vcol.isEmpty())
-    {
-      result = new Integer[vcol.size()];
-      vcol.toArray(result);
-    }
-    return result;
-  }
-  
-  /**
-   * ctor for partial projection
-   */
-  public Projection(Schema s, String projection) throws ParseException
-  {
-    String[] colnames = projection.trim().split(Schema.COLUMN_DELIMITER);
-    for (int nx = 0; nx < colnames.length; nx++)
-    {
-      colnames[nx] = colnames[nx].trim();
-    }
-    mKeys = new HashMap<Schema.ColumnSchema, HashSet<String>>();
-    mProjection = s.getProjectionSchema(colnames, mKeys);
-    mNumColumns = colnames.length;
-    mProjStr = projection;
-    mSchema = toSchema(projection);
-  }
-
-  /**
-   * accessor to the map keys
-   */
-  HashMap<Schema.ColumnSchema, HashSet<String>> getKeys() {
-    return mKeys;
-  }
-
-  /**
-   * accessor to the projected schema including invalid columns
-   */
-  public Schema getSchema() {
-     return mSchema;
-  }
-  
-  /**
-   *
-   */
-  public Schema getProjectionSchema()
-  {
-    return mProjection;
-  }
-
-  /**
-   * Get a particular projected column's schema
-   */
-  public Schema.ColumnSchema getColumnSchema(int i)
-  {
-    return mProjection.getColumn(i);
-  }
-  
-  /**
-   * Get the string representation
-   */
-  public String toString() {
-    return mProjStr;
-  }
-  
-  /**
-   * Get number of columns in the projection
-   */
-  public int getNumColumns() {
-    return mNumColumns;
-  }
-  
-  /**
-   * Get number of columns from a projection string
-   */
-  public static int getNumColumns(String projection) {
-    return projection.trim().split(Schema.COLUMN_DELIMITER).length;
-  }
-  
-  /**
-   * Get a projection string from a series of column names
-   */
-  public static String getProjectionStr(String[] names) {
-    if (names == null)
-      return null;
-    String result = new String();
-    for (String name :names)
-    {
-      if (!result.isEmpty())
-        result += ",";
-      result += name;
-    }
-    return result;
-  }
-  
-  /**
-   * Get schema from a projection string: all map keys are lost
-   */
-  public static Schema toSchema(String projection) throws ParseException
-  {
-    String[] colnames = projection.trim().split(Schema.COLUMN_DELIMITER);
-    String schemaStr = new String();
-    int startidx, i = 0;
-    for (String name : colnames)
-    {
-      startidx = name.indexOf("#{");
-      if (startidx != -1)
-      {
-        if (!name.endsWith("}"))
-          throw new ParseException("Invalid projection");
-        startidx += 2;
-        if (name.substring(startidx, name.length()-1).indexOf("#") != -1 ||
-          name.substring(startidx, name.length()-1).indexOf("{") != -1 ||
-          name.substring(startidx, name.length()-1).indexOf("}") != -1)
-          throw new ParseException("Invalid projection");
-        name = name.substring(0, startidx-2);
-      } else {
-        if (name.indexOf("#") != -1 || name.indexOf("{") != -1 ||
-            name.indexOf("}") != -1)
-          throw new ParseException("Invalid projection");
-      }
-      if (i++ > 0)
-        schemaStr += ",";
-      schemaStr += name;
-    }
-    return new Schema(schemaStr, true);
-  }
-  
-  /**
-   * Get a column's index in a projection. 
-   * @param projection
-   * @param colname
-   * @return -1 if the column is not found in projection
-   */
-  public static int getColumnIndex(String projection, String colname)
-  {
-    if (projection == null || colname == null)
-      return -1;
-    String[] colnames = projection.trim().split(Schema.COLUMN_DELIMITER);;
-    for (int i = 0; i < colnames.length; i++)
-    {
-      if (colnames[i].equals(colname))
-        return i;
-    }
-    return -1;
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/types/SortInfo.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/types/SortInfo.java
deleted file mode 100644
index 29521ca68..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/types/SortInfo.java
+++ /dev/null
@@ -1,198 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.types;
-
-import java.io.IOException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.schema.ColumnType;
-import org.apache.hadoop.zebra.tfile.TFile;
-
-/**
- * Sortness related Information
- */
-public class SortInfo {
-  public static final String DEFAULT_COMPARATOR =  TFile.COMPARATOR_MEMCMP;
-  private boolean global = true;
-  private String[] columns;
-  private int[] indices = null;
-  private ColumnType[] types = null;
-  private String comparator = null;
-  private Schema schema = null;
-  public static final String SORTED_COLUMN_DELIMITER = ",";
-
-  private SortInfo(String[] columns, int[] sortIndices, ColumnType[] sortColTypes, String comparator, Schema schema)
-  {
-    this.columns = columns;
-    this.indices = sortIndices;
-    this.comparator = comparator;
-    this.schema = schema;
-    this.types = sortColTypes;
-  }
-
-  /**
-   * Get an array of the sorted column names with the first column
-   * being the primary sort key, the second column being the
-   * secondary sort key, ..., etc.
-   *
-   * @return an array of strings of sorted column names
-   */
-  public String[] getSortColumnNames() {
-    return columns;
-  }
-  
-  /**
-   * Get an array of zebra types of the sorted columns with the first column
-   * being the primary sort key, the second column being the
-   * secondary sort key, ..., etc.
-   *
-   * @return an array of strings of sorted column names
-   */
-  public ColumnType[] getSortColumnTypes() {
-	  return types;
-  }
-
-  /**
-   * Get an array of column indices in schema of the sorted columns with the first column
-   * being the primary sort key, the second column being the
-   * secondary sort key, ..., etc.
-   *
-   * @return an array of strings of sorted column names
-   */
-  public int[] getSortIndices() {
-    return indices;
-  }
-
-  /**
-   * Get the number of sorted columns
-   *
-   * @return number of sorted columns
-   */
-  public int size() {
-    return (columns == null ? 0 : columns.length);
-  }
-
-  /**
-   * Get the comparator name 
-   *
-   * @return  comparator name
-   */
-  public String getComparator() {
-    return comparator;
-  }
-
-  /**
-   * Check if the two SortInfo objects are equal
-   *
-   * @return true if one's sort columns is equal to a leading portion of the other's
-   */
-  public boolean equals(String[] sortcolumns, String comparator) throws IOException {
-    if (sortcolumns == null || sortcolumns.length == 0)
-    {
-      return false;
-    }
-    for (String column : sortcolumns)
-    {
-    	if (schema.getColumn(column) == null)
-            throw new IOException(column + " does not exist in schema");
-    }
-    if (this.columns.length <= sortcolumns.length)
-    {
-      for (int i = 0; i < this.columns.length; i++)
-     {
-       if (!this.columns[i].equals(sortcolumns[i]))
-         return false;
-     }
-    } else {
-      for (int i = 0; i < sortcolumns.length; i++)
-     {
-       if (!sortcolumns[i].equals(this.columns[i]))
-         return false;
-     }
-    }
-    if (this.comparator == null && comparator == null)
-    {
-      return true;
-    } else if (this.comparator != null && comparator != null)
-    {
-      return (this.comparator.equals(comparator));
-    } else {
-      return false;
-    }
-  }
-
-  /**
-   * Build a SortInfo object from sort column names, schema, and comparator
-   *
-   * @param sortStr
-   *                     comma-separated sort column names
-   * @param schema
-   *                     schema of the Zebra table for the sort columns
-   * @param comparator
-   *                     comparator name
-   * @return             newly built SortInfo object
-   */
-  public static SortInfo parse(String sortStr, Schema schema, String comparator) throws IOException
-  {
-    if (sortStr == null || sortStr.trim().isEmpty())
-    {
-      return null;
-    }
-    String[] sortedColumns = sortStr.trim().split(SORTED_COLUMN_DELIMITER);
-    int[] sortColIndices = new int[sortedColumns.length];
-    ColumnType[] sortColTypes = new ColumnType[sortedColumns.length];
-    Schema.ColumnSchema cs;
-    for (int i = 0; i < sortedColumns.length; i++)
-    {
-      sortedColumns[i] = sortedColumns[i].trim();
-      /*
-       * sanity check the sort column's existence
-       */
-      if ((cs = schema.getColumn(sortedColumns[i])) == null)
-        throw new IOException(sortedColumns[i] + " does not exist in schema");
-      sortColIndices[i] = schema.getColumnIndex(sortedColumns[i]);
-      sortColTypes[i] = schema.getColumn(sortedColumns[i]).getType();
-    }
-    String comparatorInUse = (sortedColumns.length > 0 ?
-        (comparator == null || comparator.isEmpty() ?
-          DEFAULT_COMPARATOR : comparator) : null);
-    return new SortInfo(sortedColumns, sortColIndices, sortColTypes, comparatorInUse, schema);
-  }
-
-  /**
-   * Build a string of comma-separated sort column names from an array of sort column names
-   *
-   * @param names an array of sort column names
-   *
-   * @return a string of comma-separated sort column names
-   */
-  public static String toSortString(String[] names)
-  {
-    if (names == null || names.length == 0)
-      return null;
-
-    StringBuilder sb = new StringBuilder();
-    for (int i = 0; i < names.length; i++)
-    {
-      if (i > 0)
-        sb.append(SORTED_COLUMN_DELIMITER);
-      sb.append(names[i]);
-    }
-    return sb.toString();
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/types/SubColumnExtraction.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/types/SubColumnExtraction.java
deleted file mode 100644
index 90a2b7413..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/types/SubColumnExtraction.java
+++ /dev/null
@@ -1,427 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.types;
-
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.HashSet;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.data.DataBag;
-import java.util.Map;
-import java.util.HashMap;
-import java.io.IOException;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.schema.ColumnType;
-import org.apache.hadoop.zebra.parser.ParseException;
-
-
-/**
- * This class extracts a subfield from a column or subcolumn stored
- * in entirety on disk
- * It should be used only by readers whose serializers do not
- * support projection
- */
-public class SubColumnExtraction {
-	static class SubColumn {
-		Schema physical;
-		Projection projection;
-		ArrayList<SplitColumn> exec = null;
-		SplitColumn top = null; // directly associated with physical schema 
-		SplitColumn leaf = null; // target tuple corresponding to projection
-
-    // list of SplitColumns to be created maps on their children
-    ArrayList<SplitColumn> sclist = new ArrayList<SplitColumn>();
-
-		SubColumn(Schema physical, Projection projection) throws ParseException, ExecException
-		{
-			this.physical = physical;
-			this.projection = projection;
-			top = new SplitColumn(Partition.SplitType.RECORD);
-			exec = new ArrayList<SplitColumn>();
-			exec.add(top); // breadth-first 
-		
-			SplitColumn sc;
-			leaf = new SplitColumn(Partition.SplitType.RECORD);
-			Schema.ColumnSchema fs;
-			Schema.ParsedName pn = new Schema.ParsedName();
-			String name;
-			int j;
-			HashSet<String> keySet;
-			for (int i = 0; i < projection.getSchema().getNumColumns(); i++)
-			{
-				fs = projection.getColumnSchema(i);
-				if (fs == null)
-				  continue;
-				name = fs.getName();
-				if (name == null)
-					continue;
-				if (projection.getKeys() != null)
-				  keySet = projection.getKeys().get(fs);
-				else
-				  keySet = null;
-				pn.setName(name);
-				fs = physical.getColumnSchema(pn);
-				if (keySet != null)
-				  pn.setDT(ColumnType.MAP);
-				if (fs == null)
-		     		continue; // skip non-existing field
-		
-				j = fs.getIndex();
-				ColumnType ct = pn.getDT();
-				if (ct == ColumnType.MAP || ct == ColumnType.RECORD || ct == ColumnType.COLLECTION)
-				{
-					// record/map subfield is expected
-					sc = new SplitColumn(j, ct);
-          if (ct == ColumnType.MAP)
-            sclist.add(sc);
-					exec.add(sc); // breadth-first
-					// (i, j) represents the mapping between projection and physical schema
-					buildSplit(sc, fs, pn, i, (projection.getKeys() == null ? null :
-                keySet));
-				} else {
-					// (i, j) represents the mapping between projection and physical schema
-					sc = new SplitColumn(j, i, leaf, null, Partition.SplitType.NONE);
-					// no split on a leaf
-				}
-				top.addChild(sc);
-		 	}
-		}
-
-    /**
-     * build the split executions
-     */
-		private void buildSplit(SplitColumn parent, Schema.ColumnSchema fs,
-        final Schema.ParsedName pn, final int projIndex, HashSet<String> keys) throws ParseException, ExecException
-		{
-			// recursive call to get the next level schema
-		  ColumnType ct = pn.getDT();
-			if (ct != fs.getType())
-	      	throw new ParseException(fs.getName()+" is not of proper type.");
-	
-			String prefix;
-			int fieldIndex;
-			SplitColumn sc;
-			Partition.SplitType callerDT = (ct == ColumnType.MAP ? Partition.SplitType.MAP :
-				                               (ct == ColumnType.RECORD ? Partition.SplitType.RECORD :
-				                                 (ct == ColumnType.COLLECTION ? Partition.SplitType.COLLECTION :
-				                        	         Partition.SplitType.NONE)));
-			prefix = pn.parseName(fs);
-			Schema schema = fs.getSchema();
-			if (callerDT == Partition.SplitType.RECORD || callerDT == Partition.SplitType.COLLECTION)
-			{
-        if (keys != null)
-          throw new AssertionError("Internal Logical Error: empty key map expected.");
-				 if ((fieldIndex = schema.getColumnIndex(prefix)) == -1)
-	        		return; // skip non-existing fields
-				 fs = schema.getColumn(fieldIndex);
-			} else {       
-        parent.setKeys(keys); // map key is set at parent which is of type MAP
-        fs = schema.getColumn(0); // MAP value is a singleton type!
-				fieldIndex = 0;
-			}
-	
-		  ct = pn.getDT();
-			if (ct != ColumnType.ANY)
-			{
-				// record subfield is expected
-			 	sc = new SplitColumn(fieldIndex, ct);
-        if (ct == ColumnType.MAP)
-          sclist.add(sc);
-			 	exec.add(sc); // breadth-first
-			 	buildSplit(sc, fs, pn, projIndex, null);
-			} else {
-				sc = new SplitColumn(fieldIndex, projIndex, leaf, null, Partition.SplitType.NONE);
-				// no split on a leaf
-			}
-			parent.addChild(sc);
-		}
-
-    /**
-     * dispatch the source tuple from disk
-     */
-		void dispatchSource(Tuple src)
-		{
-			top.dispatch(src);
-	 	}
-
-    /**
-     * dispatch the target tuple
-     */
-		private void dispatch(Tuple tgt) throws ExecException
-		{
-			leaf.dispatch(tgt);
-			createMaps();
-			leaf.setBagFields();
-		}
-
-    /**
-     * the execution
-     */
-		void splitColumns(Tuple dest) throws ExecException, IOException
-		{
-			int i;
-			dispatch(dest);
-            clearMaps();
-            int execSize = exec.size();
-			for (i = 0; i < execSize; i++)
-			{
-			    SplitColumn execElement = exec.get(i);
-				if (execElement != null)
-				{
-					// split is necessary
-					execElement.split();
-				}
-			}
-		}
-
-    /**
-     * create MAP fields if necessary
-     */
-    private void createMaps() throws ExecException
-    {
-      for (int i = 0; i < sclist.size(); i++)
-        sclist.get(i).createMap();
-    }
-
-    /**
-     * clear map fields if necessary
-     */
-    private void clearMaps() throws ExecException
-    {
-      for (int i = 0; i < sclist.size(); i++)
-        sclist.get(i).clearMap();
-    }
-	}
-
-  /**
-   * helper class to represent one execution
-   */
-  private static class SplitColumn {
-	 int fieldIndex = -1; // field index to parent
-	 int projIndex = -1; // index in projection: only used by leaves
-	 ArrayList<SplitColumn> children = null;
-	 int index = -1; // index in the logical schema 
-	 Object field = null;
-	 SplitColumn leaf = null; // leaf holds the target tuple
-	 Partition.SplitType st = Partition.SplitType.NONE;
-	 HashSet<String> keys;
-	 Schema scratchSchema; // a temporary 1-column schema to be used to create a tuple
-	                       // for a COLLETION column
-	 ArrayList<Integer> bagFieldIndices;
-
-	 void dispatch(Object field) { this.field = field; }
-
-	 void setKeys(HashSet<String> keys) { this.keys = keys; }
-
-	 SplitColumn(Partition.SplitType st)
-	 {
-		 this.st = st;
-	 }
-
-	 SplitColumn(ColumnType ct)
-	 {
-		 if (ct == ColumnType.MAP)
-			 st = Partition.SplitType.MAP;
-		 else if (ct == ColumnType.RECORD)
-			 st = Partition.SplitType.RECORD;
-		 else if (ct == ColumnType.COLLECTION)
-		 {
-		   st = Partition.SplitType.COLLECTION;
-		   try {
-		      scratchSchema = new Schema("foo");
-		   } catch (ParseException e) {
-		     // no-op: should not throw at all.
-		   }
-		 } else
-			 st = Partition.SplitType.NONE;
-	 }
-
-	 SplitColumn(int fieldIndex, ColumnType ct)
-	 {
-		 this(ct);
-		 this.fieldIndex = fieldIndex;
-	 }
-
-	 SplitColumn(int fieldIndex, Partition.SplitType st)
-	 {
-		 this.fieldIndex = fieldIndex;
-		 this.st = st;
-	 }
-
-	 SplitColumn(int fieldIndex, HashSet<String> keys, Partition.SplitType st)
-	 {
-		 this.fieldIndex = fieldIndex;
-		 this.keys = keys;
-		 this.st = st;
-	 }
-
-	 SplitColumn(int fieldIndex,int projIndex, SplitColumn leaf, HashSet<String> keys, Partition.SplitType st)
-	 {
-		 this(fieldIndex, keys, st);
-		 this.projIndex = projIndex;
-		 this.leaf = leaf;
-	 }
-	 
-   /**
-    * the split op
-    */
-	 @SuppressWarnings("unchecked")
-	 void split() throws IOException, ExecException
-	 {
-		 if (children == null)
-			 return;
-		 
-		 int size = children.size();
-		 if (st == Partition.SplitType.RECORD)
-		 {
-			 for (int i = 0; i < size; i++)
-			 {
-			     SplitColumn child = children.get(i);
-				 if (child.projIndex != -1) // a leaf: set projection directly
-			 		((Tuple)child.leaf.field).set(child.projIndex, ((Tuple) field).get(child.fieldIndex));
-				 else
-					 child.field = ((Tuple) field).get(child.fieldIndex);
-			 }
-		 } else if (st == Partition.SplitType.COLLECTION) {
-		    DataBag srcBag, tgtBag;
-		    srcBag = (DataBag) field;
-		    Tuple tuple;
-		    for (int i = 0; i < size; i++)
-		    {
-		      SplitColumn child = children.get(i);
-		      if (child.projIndex != -1) // a leaf: set projection directly
-		      {
-		        tgtBag = (DataBag)((Tuple)child.leaf.field).get(child.projIndex);
-		      } else {
-		        tgtBag = (DataBag) child.field;
-		        tgtBag.clear();
-		      }
-		      for (Iterator<Tuple> it = srcBag.iterator(); it.hasNext(); )
-		      {
-		        tuple = TypesUtils.createTuple(scratchSchema);
-		        tuple.set(0, it.next().get(child.fieldIndex));
-		        tgtBag.add(tuple);
-		      }
-		    }
-		 } else if (st == Partition.SplitType.MAP && keys != null) {
-       String key;
-       Iterator<String> it;
-       Object value;
-			 for (int i = 0; i < size; i++)
-			 {
-			     SplitColumn child = children.get(i);
-				 if (child.projIndex != -1) // a leaf: set projection directly
-         {
-           for (it = keys.iterator(); it.hasNext(); )
-           {
-             key = it.next();
-             value = ((Map<String, Object>) field).get(key);
-             if (value == null)
-               continue;
-			 		   ((Map<String, Object>) (((Tuple)child.leaf.field).get(child.projIndex))).put(key, value);
-           }
-         } else {
-           for (it = keys.iterator(); it.hasNext(); )
-           {
-             key = it.next();
-					   child.field = ((Map<String, Object>) field).get(key);
-           }
-         }
-			 }
-		 }
-	 }
-
-   /**
-    * add a child that needs a subfield of this (sub)column
-    */
-	 void addChild(SplitColumn child) throws ExecException {
-		 if (children == null)
-			 children = new ArrayList<SplitColumn>();
-		 children.add(child);
-		 if (st == Partition.SplitType.COLLECTION) {
-		   if (child.projIndex != -1)
-		   {
-		     child.leaf.addBagFieldIndex(child.projIndex);
-		   } else {
-		     ((Tuple) child.field).set(child.fieldIndex, TypesUtils.createBag());
-		   }
-		 }
-	 }
-	 
-	 /**
-	  * add a bag field index
-	  */
-	 void addBagFieldIndex(int i)
-	 {
-	   if (bagFieldIndices == null)
-	     bagFieldIndices = new ArrayList<Integer>();
-	   bagFieldIndices.add(i);
-	 }
-	 
-	 /**
-	  * set bag fields if necessary
-	  */
-	 void setBagFields() throws ExecException
-	 {
-	   if (bagFieldIndices == null)
-	     return;
-	   for (int i = 0; i < bagFieldIndices.size(); i++)
-	   {
-	     ((Tuple) field).set(bagFieldIndices.get(i), TypesUtils.createBag());
-	   }
-	 }
-
-   /**
-    * create MAP fields for children
-    */
-   void createMap() throws ExecException
-   {
-     if (st == Partition.SplitType.MAP)
-     {
-       int size = children.size();
-       for (int i = 0; i < size; i++)
-       {
-				 if (children.get(i).projIndex != -1)
-			 		 ((Tuple)children.get(i).leaf.field).set(children.get(i).projIndex, new HashMap<String, Object>());
-				 else
-           children.get(i).field = new HashMap<String, Object>();
-       }
-     }
-   }
-   
-    /**
-     * clear map for children
-     */
-	  @SuppressWarnings("unchecked")
-    void clearMap() throws ExecException
-    {
-      if (st == Partition.SplitType.MAP)
-      {
-        int size = children.size();
-        for (int i = 0; i < size; i++)
-        {
-	 			  if (children.get(i).projIndex != -1)
-	 		 		  ((Map)((Tuple)children.get(i).leaf.field).get(children.get(i).projIndex)).clear();
-          else
-            ((Map)children.get(i).field).clear();
-        }
-      }
-    }
-   
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/types/TableStorageParser.jjt b/contrib/zebra/src/java/org/apache/hadoop/zebra/types/TableStorageParser.jjt
deleted file mode 100644
index 14298cb75..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/types/TableStorageParser.jjt
+++ /dev/null
@@ -1,535 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-options {
-      STATIC = false ;
-		IGNORE_CASE = true;
-}
-
-PARSER_BEGIN(TableStorageParser)
-package org.apache.hadoop.zebra.parser;
-import java.io.*;
-import java.util.*;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.schema.ColumnType;
-import org.apache.hadoop.zebra.types.*;
-
-public class TableStorageParser {
-		public TableStorageParser(java.io.Reader in, Partition partition, Schema schema, String comparator) { this(in); mSchema = schema; this.partition = partition; this.comparator = comparator; }
-		private Schema mSchema;
-		private int mDefaultCGIndex = -1;
-		private String mName = null;
-		private String mCompressor = "gz", mSerializer = "pig";
-		private String mOwner = null, mGroup = null;
-		private short mPerm = -1;
-		private int mCGCount = 0;
-		private Partition partition;
-    private String comparator = null;
-}
-PARSER_END(TableStorageParser)
-
-// Skip all the new lines, tabs and spaces
-SKIP : { " " |	"\r" |	"\t" |	"\n" }
-
-// Skip comments(single line and multiline)
-SKIP : {
-   <"--"(~["\r","\n"])*>
-|  <"/*" (~["*"])* "*" ("*" | (~["*","/"] (~["*"])* "*"))* "/">
-}
-
-TOKEN : { <COMPRESSOR : "lzo" | "gz"> }
-TOKEN : { <SERIALIZER : ("pig" | "avro")> }
-
-TOKEN : { <COMPRESS	 : "compress by"> }
-TOKEN : { <SERIALIZE : "serialize by"> }
-TOKEN : { <ASC : "ASC"> }
-TOKEN : { <DESC: "DESC"> }
-TOKEN : { <SECURE 	 : "secure by"> }
-TOKEN : { <USER		 : "uid"> }
-TOKEN : { <GROUP	 : "gid"> }
-TOKEN : { <PERM		 : "perm"> }
-TOKEN : { <AS		 : "as"> }
-
-
-TOKEN:
-{
- 	<#LETTER : ["a"-"z", "A"-"Z"] >
-|	<#DIGIT : ["0"-"9"] >
-|   <#OCTAL : ["0" - "7"] >
-|   <#SPECIALCHAR : ["_"] >
-|   <#FSSPECIALCHAR: ["-", ":", "/"]>
-| <#SCOPEOP : "::">
-| <IDENTIFIER: ( <LETTER> )+ ( <DIGIT> | <LETTER> | <SPECIALCHAR> )* ( <SCOPEOP>  ( <LETTER> )+ ( <DIGIT> | <LETTER> | <SPECIALCHAR> )*)* >
-| <MAPKEYIDENTIFIER: ( <LETTER> | <SPECIALCHAR> )+ ( <DIGIT> | <LETTER> | <SPECIALCHAR> )* ( <SCOPEOP>  ( <LETTER> )+ ( <DIGIT> | <LETTER> | <SPECIALCHAR> )*)* >
-|   <SHORT	:	(<OCTAL>){3}	>
-}
-
-void StorageSchema(ArrayList<CGSchema> s) throws ParseException :
-{
-	CGSchema fs;
-	CGSchema defaultSchema;
-}
-{
-	try {
-		fs = FieldSchema() {mCGCount++; if (fs != null) s.add(fs);}
-		(";" fs = FieldSchema() {mCGCount++; if (fs != null) s.add(fs);})* <EOF>
-		{
-			defaultSchema = partition.generateDefaultCGSchema(mName, mCompressor, mSerializer, mOwner, mGroup, mPerm, mDefaultCGIndex == -1 ? mDefaultCGIndex = mCGCount++ : mDefaultCGIndex, comparator);
-			if (defaultSchema != null)
-				s.add(defaultSchema);
-
-      // check column group names, add system created names when necessary;
-      HashSet<String> cgNames = new HashSet<String>();
-      ArrayList<CGSchema> unnamed = new ArrayList<CGSchema>(); 	
-      for (int i = 0; i < s.size(); i++) { 
-        CGSchema cgSchema = s.get(i);
-        String str = cgSchema.getName();
-        if (str != null) {
-          if (!cgNames.add(str)) {
-            throw new ParseException("Duplicate column group names.");
-          }
-        } else {
-          unnamed.add(cgSchema);
-        }
-      }
-      
-      int digits = 1;
-      int total = unnamed.size();
-      while (total >= 10) {
-        ++digits;
-        total /= 10;
-      }
-      String formatString = "%0" + digits + "d";
-      
-      int idx = 0;
-      for (int i = 0; i < unnamed.size(); i++) { 
-        CGSchema cgSchema = unnamed.get(i);
-        String str = null;
-        while (true) {
-          str = "CG" + String.format(formatString, idx++);
-          if (!cgNames.contains(str)) {
-            break;
-          }
-        }
-        cgSchema.setName(str);
-      }
-			return;
-		}
-	} catch (TokenMgrError e) {
-		throw new ParseException(e.getMessage());
-	}
-}
-
-boolean ascdsc() throws ParseException :
-{
-  Token t1 = null, t2 = null;
-}
-{
-  (
-		t1 = <ASC>
-  | t2 = <DESC>
-  )
-  {
-    if (t2 != null)
-      return false;
-    return true;
-  }
-}
-
-CGSchema FieldSchema() throws ParseException:
-{
-	Token t1 = null, t2 = null; 
-	Schema fs = null;
-	CGSchema cs = null;
-	String name = null;
-	String compressor = null;
-	String serializer = null;
-	String secureBy   = null;
-	String owner       = null;
-	String group	  = null;
-	short perm		  = -1;
-	String secure	  = null;
-}
-{
-
-	(
-  		( "[" fs = RecordSchema(null) "]" )?
-  		(
-  		  <AS>
-  		  t1 = <IDENTIFIER>
-  		  {
-  		    if (name != null)
-  		    {
-  		      String msg = "Column group name defined more than once";
-  		    } else {
-  		      name = t1.image;
-  		    }
-  		  }
-  		)?
-  		(
-    	  <COMPRESS>
-      	  t1 = <COMPRESSOR>
-      	  {
-      	    if(compressor != null )
-      	    {
-				String msg = "Compression information defined more than once";
-				throw new ParseException(msg);      	      
-      	    } else {
-      	    	compressor = t1.image;
-      	    }
-      	      
-      	  }
-      	  |
-      	  <SERIALIZE>
-      	  t1 = <SERIALIZER>
-      	  {
-      	    if(serializer != null )
-      	    {
-				String msg = "Serializer Information defined more than once";
-				throw new ParseException(msg);      	      
-      	    } else 
-      	    	serializer = t1.image;
-      	      
-      	  }
-      	  |
-      	  t1 = <SECURE>
-      	  {
-      	    if(secure != null)
-      	    {
-				String msg = "SECURE BY defined more than once";
-				throw new ParseException(msg);      	      
-      	      
-      	    } else
-      	    	secure = t1.image;
-      	      
-      	  }
-      	  (
-      	    <USER>
-      	    ":"
-      	    t1 = <IDENTIFIER>
-      	    {
-      	      if(owner != null) 
-      	      {
-				String msg = "Uid defined more than once";
-				throw new ParseException(msg);      	      
-      	      } else
-      	      		owner = t1.image;	
-      	    }
-      	    |
-      	    <GROUP>
-      	    ":"
-      	    t1 = <IDENTIFIER>
-      	    {
-      	      if(group != null) 
-      	      {
-				String msg = "Gid defined more than once";
-				throw new ParseException(msg);      	      
-      	      } else
-      	      		group = t1.image;
-      	    
-      	    }
-      	    |
-      	    <PERM>
-      	    ":"
-      	    t1 = <SHORT>
-      	    {
-      	      if(perm != -1) 
-      	      {
-				String msg = "Perms defined more than once";
-				throw new ParseException(msg);      	      
-      	      } else
-      	      		perm = Short.parseShort(t1.image, 8); 
-      	    
-      	    }
-      	  
-      	  )+
-      
-        )*
-       	
-     
-	)
-	{
-//		String compressor, serializer;
-		if (compressor == null)
-			compressor = "gz";
-		if (serializer == null)
-			serializer = "pig";
-		if (fs == null)
-		{
-			if (mDefaultCGIndex != -1)
-			{
-				String msg = "Default Storage Information defined more than once";
-				throw new ParseException(msg);
-			}
-			mDefaultCGIndex = mCGCount;
-			mName       = name;
-			mCompressor = compressor;
-			mSerializer = serializer;
-			mOwner		= owner;
-			mGroup		= group;
-			mPerm 		= perm; 
-		} else {
-			cs = new CGSchema(fs, false, comparator, name, serializer, compressor, owner, group, perm);
-    }
-		return cs;
-	}
-}
-
-
-
-Schema.ColumnSchema ColumnSchema(int colIndex) throws ParseException: 
-{
-	Token t1;
-	String name = "";
-	Schema.ColumnSchema fs = null; 
-}
-{
-	(
-	LOOKAHEAD(2) fs = SchemaRecord(mSchema, name, colIndex)
-|	LOOKAHEAD(2) fs = SchemaMap(mSchema, name, colIndex)
-|	LOOKAHEAD(2) fs = AtomSchema(mSchema, name, colIndex)
-	)
-	{
-		return fs;
-	}
-}
-
-Schema.ColumnSchema AtomSchema(Schema schema, String name, int colIndex) throws ParseException : 
-{
-	Token t1 = null;
-	Schema.ColumnSchema fs;
-}
-{
-	(	( t1 = <IDENTIFIER>  )
-		{ 
-			Schema.ColumnSchema fs0 = schema.getColumn(t1.image);
-			name += t1.image;
-			if (fs0 == null)
-			{
-				String msg = "Column "+name+" not defined in schema";
-				throw new ParseException(msg);
-			}
-			/* create a new field schema using concatenated name */
-			if (!partition.getPartitionInfo().setCGIndex(fs0, mCGCount, colIndex, name))
-			{
-				throw new ParseException("Column "+name+" specified more than once!");
-			}
-			fs = new Schema.ColumnSchema(name, fs0.getSchema(), fs0.getType());
-			return fs;
-		}
-	)
-}
-
-Schema.ColumnSchema SchemaMap(Schema schema, String name, int colIndex) throws ParseException :
-{
-	Token t1 = null; 
-	Schema.ColumnSchema fs;
-}
-{
-	t1 = <IDENTIFIER> "#" fs = AnonymousMapSchema(schema.getColumn(t1.image), t1.image, 0, colIndex)
-	{
-		return fs;
-	} 
-}
-
-Schema.ColumnSchema SchemaRecord(Schema schema, String name, int colIndex) throws ParseException : 
-{
-	Token t1 = null;
-	Schema.ColumnSchema fs;
-}
-{ 
-	t1 = <IDENTIFIER> "."  fs = AnonymousRecordSchema(schema.getColumn(t1.image), t1.image, 0, colIndex)
-	{
-		return fs;
-	} 
-}
-
-Schema.ColumnSchema AnonymousColumnSchema(Schema.ColumnSchema schema, String name, int cl, int colIndex) throws ParseException :
-{
-	Token t1; 
-	Schema.ColumnSchema fs = null; 
-}
-{
-	(
-	LOOKAHEAD(AnonymousSchemaRecord()) fs = AnonymousSchemaRecord(schema, name, cl, colIndex)
-|	LOOKAHEAD(AnonymousSchemaMap()) fs = AnonymousSchemaMap(schema, name, cl, colIndex)
-	)
-	{
-		return fs;
-	}
-}
-
-Schema.ColumnSchema AnonymousSchemaRecord(Schema.ColumnSchema schema, String name, int cl, int colIndex) throws ParseException :
-{
-	Schema.ColumnSchema fs;
-}
-{ 
-	"."  fs = AnonymousRecordSchema(schema, name, cl, colIndex)
-	{
-		return fs;
-	} 
-}
-
-Schema.ColumnSchema AnonymousSchemaMap(Schema.ColumnSchema schema, String name, int cl, int colIndex) throws ParseException:
-{
-	Schema.ColumnSchema fs;
-}
-{ 
-	"#"  fs = AnonymousMapSchema(schema, name, cl, colIndex)
-	{
-		return fs;
-	} 
-}
-
-
-Schema RecordSchema(Schema list) throws ParseException :
-{
-	if (list == null)
-		list = new Schema(); 
-	Schema.ColumnSchema fs = null;
-	int colIndex = 0;
-}
-{
-	(
-	(	
-		fs = ColumnSchema(colIndex) {list.add(fs);} 
-		( "," fs = ColumnSchema(++colIndex) {list.add(fs);})* 
-	)
-|		{} {list = null;}
-	)
-	{ return list; }
-}
-
-Schema.ColumnSchema AnonymousRecordSchema(Schema.ColumnSchema schema, String name, int cl, int colIndex) throws ParseException :
-{
-	Schema.ColumnSchema fs = null, fs0 = null;
-	Token t;
-  ColumnType ct;
-	if (schema == null)
-	{
-		String msg = "no matching column: " + name;
-		throw new ParseException(msg);
-	} else if ((ct = schema.getType()) != ColumnType.RECORD && ct != ColumnType.COLLECTION) {
-		String msg = "Column " + name + " is not a record or a collection of records";
-		throw new ParseException(msg);
-	} else if (ct == ColumnType.COLLECTION) {
-		String msg = "Split of COLLECTION Column, " + name + ", is not supported in this release";
-		throw new ParseException(msg);
-  }
-  Schema cschema = schema.getSchema();
-	if (ct == ColumnType.COLLECTION)
-	{
-		cl += 1;
-		if (cschema.getNumColumns() == 1 && cschema.getColumn(0).getType() == ColumnType.RECORD && (cschema.getColumn(0).getName() == null || cschema.getColumn(0).getName().isEmpty()))
-		// an anonymous record inside a collection: go one level lower
-		{
-			schema = cschema.getColumn(0);
-			partition.setSplit(schema, Partition.SplitType.RECORD, Partition.SplitType.RECORD, name, null, false);
-		}
-	}
-}
-{
-	(
-		LOOKAHEAD(2) t = <IDENTIFIER> fs = AnonymousSchemaRecord(cschema.getColumn(t.image), name + "." + t.image, cl, colIndex)
-		{
-	    partition.setSplit(schema, Partition.SplitType.RECORD, Partition.SplitType.RECORD, name, t.image, true);
-			return fs;
-		} 
-|
-		LOOKAHEAD(2) t = <IDENTIFIER> fs = AnonymousSchemaMap(cschema.getColumn(t.image), name + "." + t.image, cl, colIndex)
-		{
-	    partition.setSplit(schema, Partition.SplitType.RECORD, Partition.SplitType.MAP, name, t.image, true);
-			return fs;
-		} 
-|
-		t= <IDENTIFIER>
-		{
-	    partition.setSplit(schema, Partition.SplitType.RECORD, Partition.SplitType.NONE, name, t.image, false);
-			name = name + "." + t.image;
-			fs0 = cschema.getColumn(t.image);
-			if (fs0 == null)
-			{
-				String msg = "no matching column: " + name;
-				throw new ParseException(msg);
-			}
-			if (!partition.setCGIndex(fs0, mCGCount, colIndex, name))
-				throw new ParseException("Column "+name+" specified more than once!");
-			fs = new Schema.ColumnSchema(name, fs0.getSchema(), fs0.getType());
-		}
-	)
-	{ return fs; }
-}
-
-Schema.ColumnSchema AnonymousMapSchema(Schema.ColumnSchema schema, String name, int cl, int colIndex) throws ParseException :
-{
-	Schema.ColumnSchema fs = null;
-  HashSet<String> keys;
-	if (schema == null)
-	{
-		String msg = "no matching column: " + name;
-		throw new ParseException(msg);
-	} else if (schema.getType() != ColumnType.MAP) {
-		String msg = "Column " + name + " is not a map";
-		throw new ParseException(msg);
-	}
-	partition.setSplit(schema, Partition.SplitType.MAP, Partition.SplitType.RECORD, name, null, false);
-}
-{
-	(
-	/* will not support nested splits inside a MAP key. Might be enabled later along with other
-	   necessary changes
-	*/
-	/*
-		LOOKAHEAD(2)
-		t = <IDENTIFIER> fs = AnonymousColumnSchema(schema.schema.getColumn(0), name = name + "#" + t.image, cl, colIndex)
-		{
-			return fs;
-		} 
-|
-*/
-		"{" keys = hashKeys() "}"
-		{
-			if(!partition.getPartitionInfo().setKeyCGIndex(schema, mCGCount, colIndex, name, keys))
-				throw new ParseException("Column "+name+" has split keys splecified more than once.");
-			fs = new Schema.ColumnSchema(name, schema.getSchema(), schema.getType());
-		}
-	)
-	{ return fs; }
-}
-
-HashSet<String> hashKeys() :
-{
-  Token t;
-  HashSet<String> result = new HashSet<String>();
-}
-{
-  t = hashKey() { result.add(t.image); }
-  ("|" t = hashKey() { result.add(t.image); })*
-  {
-    return result;
-  }
-}
-
-Token hashKey() :
-{
-  Token t;
-}
-{
-  ( t = <MAPKEYIDENTIFIER> |  t = <IDENTIFIER> )
-  { return t; }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/types/TypesUtils.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/types/TypesUtils.java
deleted file mode 100644
index b28531a91..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/types/TypesUtils.java
+++ /dev/null
@@ -1,363 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.types;
-
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.util.Iterator;
-import java.util.Map;
-
-import junit.framework.Assert;
-
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.DefaultDataBag;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.data.TupleFactory;
-import org.apache.hadoop.zebra.schema.ColumnType;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.schema.Schema.ColumnSchema;
-import org.apache.hadoop.zebra.parser.ParseException;
-
-/**
- * Utility methods manipulating Table types (specifically, Tuple objects).
- */
-public class TypesUtils {
-  //static TupleFactory tf = ZebraTupleFactory.getInstance();
-  static TupleFactory tf = ZebraTupleFactory.getZebraTupleFactoryInstance();
-
-  /**
-   * Create a tuple based on a schema
-   * 
-   * @param schema
-   *          The schema that the tuple will conform to.
-   * @return A suitable Tuple object that can be used to read or write a Table
-   *         with the same input or output schema.
-   */
-  public static Tuple createTuple(Schema schema) throws IOException {
-    Tuple tuple = tf.newTuple(schema.getNumColumns());
-    for (int i = 0; i < schema.getNumColumns(); ++i) {
-      tuple.set(i, null);
-    }
-    return tuple;
-  }
-  
-  /**
-   * create a tuple based on number of columns
-   */
-  public static Tuple createTuple(int size) throws IOException {
-    Tuple tuple = tf.newTuple(size);
-    for (int i = 0; i < size; ++i) {
-      tuple.set(i, null);
-    }
-    return tuple;
-  }
-
-  /**
-   * Create a PIG Bag object.
-   * 
-   * @return A Pig DataBag object.
-   */
-  public static DataBag createBag() {
-    return new DefaultDataBag();
-  }
-
-  public static DataBag createBag(Schema schema) {
-    return new DefaultDataBag();
-  }
-
-  /**
-   * Reset the Tuple so that all fields are NULL field. This is different from
-   * clearing the tuple, in which case the size of the tuple will become zero.
-   * 
-   * @param tuple
-   *          Input tuple.
-   */
-  public static void resetTuple(Tuple tuple) {
-    try {
-      int tupleSize = tuple.size();
-      for (int i = 0; i < tupleSize; ++i) {
-        tuple.set(i, null);
-      }
-    }
-    catch (Exception e) {
-      throw new RuntimeException("Internal error: " + e.toString());
-    }
-  }
-  
-  private static void checkTypeError(ColumnSchema cs, ColumnType type) throws IOException {
-    throw new IOException("Incompatible Tuple object - datum is " + type + ", but schema says " + cs.getType());
-  }
-  
-  private static void checkColumnType(ColumnSchema cs, ColumnType type) throws IOException {
-    switch (type) {
-      case BOOL:
-      case DOUBLE:
-      case STRING:
-      case BYTES:
-      case MAP:
-      case COLLECTION:
-      case RECORD:
-        if (cs.getType() != type) {
-          checkTypeError(cs, type);
-        }
-        break;
-      case FLOAT:
-        if (cs.getType() != ColumnType.FLOAT && cs.getType() != ColumnType.DOUBLE) {
-          checkTypeError(cs, type);
-        }
-        break;
-      case LONG:
-        if (cs.getType() != ColumnType.LONG && cs.getType() != ColumnType.FLOAT && cs.getType() != ColumnType.DOUBLE) {
-          checkTypeError(cs, type);
-        }
-        break;
-      case INT:
-        if (cs.getType() != ColumnType.INT && cs.getType() != ColumnType.LONG && cs.getType() != ColumnType.FLOAT && cs.getType() != ColumnType.DOUBLE) {
-          checkTypeError(cs, type);
-        }
-        break;
-    }
-  }
-  
-  @SuppressWarnings("unchecked")
-  private static void checkColumn(Object d, ColumnSchema cs) throws IOException {
-    if (d instanceof Boolean) {
-      checkColumnType(cs, ColumnType.BOOL);   
-    } else if (d instanceof Integer) {
-      checkColumnType(cs, ColumnType.INT);
-    } else if (d instanceof Long) {
-      checkColumnType(cs, ColumnType.LONG);
-    } else if (d instanceof Float) {
-      checkColumnType(cs, ColumnType.FLOAT); 
-    } else if (d instanceof Double) {
-      checkColumnType(cs, ColumnType.DOUBLE);
-    } else if (d instanceof String) {
-      checkColumnType(cs, ColumnType.STRING);
-    } else if (d instanceof DataByteArray) {
-      checkColumnType(cs, ColumnType.BYTES);
-    } else if (d instanceof Map) {
-      checkMapColumn((Map<String, Object>)d, cs);
-    } else if (d instanceof DataBag) {
-      checkCollectionColumn((DataBag)d, cs);
-    } else if (d instanceof Tuple) {
-      checkRecordColumn((Tuple)d, cs);
-    } else {
-      throw new IOException("Unknown data type");
-    }
-  }
-    
-  private static void checkMapColumn(Map<String, Object> m, ColumnSchema cs) throws IOException {
-    checkColumnType(cs, ColumnType.MAP);
-    Schema schema = cs.getSchema();
-    Assert.assertTrue(schema.getNumColumns() == 1);
-    
-    ColumnSchema tempColumnSchema = schema.getColumn(0);
-    if (tempColumnSchema.getType() == ColumnType.BYTES) { // We do not check inside of map if its value type is BYTES;
-                                            // This is for Pig, since it only supports BYTES as map value type.
-      return;
-    }
-        
-    Map<String, Object> m1 = (Map<String, Object>)m;
-    for (Map.Entry<String, Object> e : m1.entrySet()) {
-      Object d = e.getValue();
-      if (d != null) {
-        checkColumn(d, tempColumnSchema);
-        return;   // We only check the first non-null map value in the map;
-      }
-    }
-  }
-  
-  private static void checkCollectionColumn(DataBag bag, ColumnSchema cs) throws IOException {
-    checkColumnType(cs, ColumnType.COLLECTION);
-    Schema schema = cs.getSchema();
-    Assert.assertTrue(schema.getNumColumns() == 1);
-    
-    Iterator<Tuple> iter = bag.iterator();
-    while (iter.hasNext()) {
-      Tuple tempTuple = iter.next();
-      // collection has to be on record;
-      if (tempTuple != null) {
-        checkRecordColumn(tempTuple, schema.getColumn(0));
-        return;     // We only check the first non-null record in the collection;
-      }
-    }     
-  }
-  
-  private static void checkRecordColumn(Tuple d, ColumnSchema cs) throws IOException {
-    checkColumnType(cs, ColumnType.RECORD);
-    checkNumberColumnCompatible(d, cs.getSchema());
-    
-    for (int i=0; i<d.size(); i++) {
-      if (d.get(i) != null) { // "null" can match any type;
-        checkColumn(d.get(i), cs.getSchema().getColumn(i));
-      }  
-    }
-  }
-  
-  /**
-   * Check whether the input row object is compatible with the expected schema
-   * 
-   * @param tuple
-   *          Input Tuple object
-   * @param schema
-   *          Table schema
-   * @throws IOException
-   */  
-  public static void checkCompatible(Tuple tuple, Schema schema) throws IOException {
-    // Create a dummy record ColumnSchema since we do not have it;
-    ColumnSchema dummy = new ColumnSchema("dummy", schema);
-
-    checkRecordColumn(tuple, dummy);
-  } 
-  
-  /**
-   * Check whether the input row object is compatible with the expected schema
-   * on number of Columns;
-   * @param tuple
-   *          Input Tuple object
-   * @param schema
-   *          Table schema
-   * @throws IOException
-   */
-  public static void checkNumberColumnCompatible(Tuple tuple, Schema schema)
-      throws IOException {
-    if (tuple.size() != schema.getNumColumns()) {
-      throw new IOException("Incompatible Tuple object - tuple has " + tuple.size() + " columns, but schema says " + schema.getNumColumns() + " columns");
-    }
-  }
-
-  /**
-   * Reading a tuple from disk with projection.
-   */
-  public static class TupleReader {
-    private Tuple tuple;
-    //@SuppressWarnings("unused")
-    private Schema physical;
-    private Projection projection;
-    SubColumnExtraction.SubColumn subcolextractor = null;
-
-    /**
-     * Constructor - create a TupleReader than can parse the serialized Tuple
-     * with the specified physical schema, and produce the Tuples based on the
-     * projection.
-     * 
-     * @param physical
-     *          The physical schema of on-disk data.
-     * @param projection
-     *          The logical schema of tuples user expect.
-     */
-    public TupleReader(Schema physical, Projection projection)
-        throws IOException, ParseException {
-      tuple = createTuple(physical);
-      this.physical = physical;
-      this.projection = projection;
-      subcolextractor = new SubColumnExtraction.SubColumn(physical, projection);
-      subcolextractor.dispatchSource(tuple);
-    }
-
-    public Schema getSchema() {
-      return physical;
-    }
-
-    public Projection getprojction() {
-      return projection;
-    }
-
-    /**
-     * Read a tuple from the stream, and perform projection.
-     * 
-     * @param in
-     *          The input stream
-     * @param row
-     *          The input tuple that should conform to the projection schema.
-     * @throws IOException
-     */
-    public void get(DataInputStream in, Tuple row) throws IOException, ParseException {
-      checkNumberColumnCompatible(row, projection.getSchema());
-      tuple.readFields(in);
-      TypesUtils.resetTuple(row);
-      try {
-        subcolextractor.splitColumns(row);
-      }
-      catch (ExecException e) {
-        // not going to happen.
-      }
-    }
-  }
-
-  /**
-   * Writing a tuple to disk.
-   */
-  public static class TupleWriter {
-    private Schema physical;
-
-    /**
-     * The constructor
-     * 
-     * @param physical
-     *          The physical schema of the tuple.
-     */
-    public TupleWriter(Schema physical) {
-      this.physical = physical;
-    }
-
-    /**
-     * Write a tuple to the output stream.
-     * 
-     * @param out
-     *          The output stream
-     * @param row
-     *          The user tuple that should conform to the physical schema.
-     * @throws IOException
-     */
-    public void put(DataOutputStream out, Tuple row) throws IOException {
-      row.write(out);
-    }
-  }
-
-  /**
-   * Checking and formatting an input tuple to conform to the input schema.<br>
-   * 
-   *           The current implementation always create a new tuple because PIG
-   *           expects Slice.next(tuple) always returning a brand new tuple.
-   * 
-   * @param tuple
-   * @throws IOException
-   * 
-   */
-  public static void formatTuple(Tuple tuple, int ncols) throws IOException {
-    Tuple one = createTuple(ncols);
-    tuple.reference(one);
-    return;
-    /*
-     * Dead code below.
-     */
-    // int n = schema.getNumColumns();
-    // if (tuple.size() == n) return;
-    // if (tuple.size() == 0) {
-    // for (int i = 0; i < schema.getNumColumns(); ++i) {
-    // tuple.append(null);
-    // }
-    // return;
-    // }
-    // throw new IOException("Tuple already formatted with " + tuple.size()
-    // + "  fields");
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/types/ZebraConf.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/types/ZebraConf.java
deleted file mode 100644
index 876f31782..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/types/ZebraConf.java
+++ /dev/null
@@ -1,134 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.types;
-
-import org.apache.hadoop.conf.Configuration;
-
-/**
- * Zebra's implementation of Pig's Tuple.
- * It's derived from Pig's DefaultTuple implementation.
- * 
- */
-public class ZebraConf {
-  // input configurations
-  
-  // output configurations
-  private static final String MAPREDUCE_OUTPUT_PATH = "mapreduce.lib.table.output.dir";
-  private static final String MAPREDUCE_MULTI_OUTPUT_PATH = "mapreduce.lib.table.multi.output.dirs";
-  private static final String MAPREDUCE_OUTPUT_SCHEMA = "mapreduce.lib.table.output.schema";
-  private static final String MAPREDUCE_OUTPUT_STORAGEHINT = "mapreduce.lib.table.output.storagehint";
-  private static final String MAPREDUCE_OUTPUT_SORTCOLUMNS = "mapreduce.lib.table.output.sortcolumns";
-  private static final String MAPREDUCE_OUTPUT_COMPARATOR =  "mapreduce.lib.table.output.comparator";
-  private static final String IS_MULTI = "multi";
-  private static final String ZEBRA_OUTPUT_PARTITIONER_CLASS = "zebra.output.partitioner.class";
-  private static final String ZEBRA_OUTPUT_PARTITIONER_CLASS_ARGUMENTS = "zebra.output.partitioner.class.arguments";
-  private static final String MAPREDUCE_OUTPUT_CHECKTYPE = "mapreduce.lib.table.output.checktype";
-  
-  private static final String MAPRED_OUTPUT_PATH = "mapred.lib.table.output.dir";
-  private static final String MAPRED_MULTI_OUTPUT_PATH = "mapred.lib.table.multi.output.dirs";
-  private static final String MAPRED_OUTPUT_SCHEMA = "mapred.lib.table.output.schema";
-  private static final String MAPRED_OUTPUT_STORAGEHINT = "mapred.lib.table.output.storagehint";
-  private static final String MAPRED_OUTPUT_SORTCOLUMNS = "mapred.lib.table.output.sortcolumns";
-  private static final String MAPRED_OUTPUT_COMPARATOR =  "mapred.lib.table.output.comparator";
-  
-
-    
-  static public String getOutputPath(Configuration conf) {
-    return conf.get(MAPREDUCE_OUTPUT_PATH) != null? conf.get(MAPREDUCE_OUTPUT_PATH): conf.get(MAPRED_OUTPUT_PATH);
-  }
-  
-  static public void setOutputPath(Configuration conf, String value) {
-    conf.set(MAPREDUCE_OUTPUT_PATH, value);
-  }
-  
-  static public String getMultiOutputPath(Configuration conf) {
-    return conf.get(MAPREDUCE_MULTI_OUTPUT_PATH) != null? conf.get(MAPREDUCE_MULTI_OUTPUT_PATH) : conf.get(MAPRED_MULTI_OUTPUT_PATH);
-  }
-  
-  static public void setMultiOutputPath(Configuration conf, String value) {
-    conf.set(MAPREDUCE_MULTI_OUTPUT_PATH, value);
-  }
-  
-  static public String getOutputSchema(Configuration conf) {
-    return conf.get(MAPREDUCE_OUTPUT_SCHEMA) != null? conf.get(MAPREDUCE_OUTPUT_SCHEMA): conf.get(MAPRED_OUTPUT_SCHEMA); 
-  }
-  
-  static public void setOutputSchema(Configuration conf, String value) {
-    conf.set(MAPREDUCE_OUTPUT_SCHEMA, value);
-  }
-  
-  static public String getOutputStorageHint(Configuration conf) {
-    return conf.get(MAPREDUCE_OUTPUT_STORAGEHINT) != null? conf.get(MAPREDUCE_OUTPUT_STORAGEHINT) : 
-      conf.get(MAPRED_OUTPUT_STORAGEHINT) != null? conf.get(MAPRED_OUTPUT_STORAGEHINT) : "";   
-  }
-  
-  static public void setOutputStorageHint(Configuration conf, String value) {
-    conf.set(MAPREDUCE_OUTPUT_STORAGEHINT, value);
-  }
-  
-  static public String getOutputSortColumns(Configuration conf) {
-    return conf.get(MAPREDUCE_OUTPUT_SORTCOLUMNS) != null? conf.get(MAPREDUCE_OUTPUT_SORTCOLUMNS) : conf.get(MAPRED_OUTPUT_SORTCOLUMNS);
-  }
-  
-  static public void setOutputSortColumns(Configuration conf, String value) {
-    if (value != null) {
-      conf.set(MAPREDUCE_OUTPUT_SORTCOLUMNS, value);
-    }
-  }
-  
-  static public String getOutputComparator(Configuration conf) {
-    return conf.get(MAPREDUCE_OUTPUT_COMPARATOR) != null? conf.get(MAPREDUCE_OUTPUT_COMPARATOR): conf.get(MAPRED_OUTPUT_COMPARATOR);
-  }
-  
-  static public void setOutputComparator(Configuration conf, String value) {
-    conf.set(MAPREDUCE_OUTPUT_COMPARATOR, value);
-  }
-  
-  static public Boolean getIsMulti(Configuration conf, boolean defaultValue) {
-    return conf.getBoolean(IS_MULTI, defaultValue);
-  }
-  
-  static public void setIsMulti(Configuration conf, boolean value) {
-    conf.setBoolean(IS_MULTI, value);
-  }
-  
-  static public Boolean getCheckType(Configuration conf, boolean defaultValue) {
-    return conf.getBoolean(MAPREDUCE_OUTPUT_CHECKTYPE, defaultValue);
-  }
-  
-  static public void setCheckType(Configuration conf, boolean value) {
-    conf.setBoolean(MAPREDUCE_OUTPUT_CHECKTYPE, value);
-  }
-  
-  static public String getZebraOutputPartitionerClass(Configuration conf) {
-    return conf.get(ZEBRA_OUTPUT_PARTITIONER_CLASS);
-  }
-  
-  static public void setZebraOutputPartitionerClass(Configuration conf, String value) {
-    conf.set(ZEBRA_OUTPUT_PARTITIONER_CLASS, value);
-  }
-  
-  static public String getOutputPartitionClassArguments(Configuration conf) {
-    return conf.get(ZEBRA_OUTPUT_PARTITIONER_CLASS_ARGUMENTS);
-  }
-  
-  static public void setOutputPartitionClassArguments(Configuration conf, String value) {
-    conf.set(ZEBRA_OUTPUT_PARTITIONER_CLASS_ARGUMENTS, value);
-  }  
-}
\ No newline at end of file
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/types/ZebraTuple.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/types/ZebraTuple.java
deleted file mode 100644
index a43289218..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/types/ZebraTuple.java
+++ /dev/null
@@ -1,91 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.types;
-
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-
-import org.apache.pig.data.DefaultTuple;
-import org.apache.pig.impl.util.TupleFormat;
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-
-/**
- * Zebra's implementation of Pig's Tuple.
- * It's derived from Pig's DefaultTuple implementation.
- * 
- */
-public class ZebraTuple extends DefaultTuple {
-  protected static final Log LOG = LogFactory.getLog(TupleFormat.class);
-  
-  private static final long serialVersionUID = 1L;
-  
-  /**
-   * Default constructor.  This constructor is public so that hadoop can call
-   * it directly.  However, inside pig you should never be calling this
-   * function.  Use TupleFactory instead.
-   */
-  public ZebraTuple() {
-    super();
-  }
-  
-  /** 
-   * Construct a tuple with a known number of fields.  Package level so
-   * that callers cannot directly invoke it.
-   * @param size Number of fields to allocate in the tuple.
-   */
-  ZebraTuple(int size) {
-    mFields = new ArrayList<Object>(size);
-    for (int i = 0; i < size; i++) mFields.add(null);
-  }   
-
-  /** 
-   * Construct a tuple from an existing list of objects.  Package
-   * level so that callers cannot directly invoke it.
-   * @param c List of objects to turn into a tuple.
-   */
-  ZebraTuple(List<Object> c) {
-    mFields = new ArrayList<Object>(c.size());
-
-    Iterator<Object> i = c.iterator();
-    int field;
-    for (field = 0; i.hasNext(); field++) mFields.add(field, i.next());
-  }
-
-  /**
-   * Construct a tuple from an existing list of objects.  Package
-   * level so that callers cannot directly invoke it.
-   * @param c List of objects to turn into a tuple.  This list will be kept
-   * as part of the tuple.
-   * @param junk Just used to differentiate from the constructor above that
-   * copies the list.
-   */
-  ZebraTuple(List<Object> c, int junk) {
-    mFields = c;
-  }
-
-  @Override
-  public String toString() {
-    CsvZebraTupleOutput csvZebraTupleOutput = CsvZebraTupleOutput.createCsvZebraTupleOutput();
-    csvZebraTupleOutput.writeTuple(this);
-    
-    return csvZebraTupleOutput.toString();
-  }
-}
\ No newline at end of file
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/types/ZebraTupleFactory.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/types/ZebraTupleFactory.java
deleted file mode 100644
index 3dca42a79..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/types/ZebraTupleFactory.java
+++ /dev/null
@@ -1,80 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.zebra.types;
-
-import java.util.List;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.data.TupleFactory;
-import org.apache.pig.backend.executionengine.ExecException;
-
-/**
- * Factory that produces ZebraTuples;
- */
-class ZebraTupleFactory extends TupleFactory {
-  private static ZebraTupleFactory self = null;
-  
-  public Tuple newTuple() {
-    return new ZebraTuple();
-  }   
-
-  public Tuple newTuple(int size) {
-    return new ZebraTuple(size);
-  }   
-  
-  @SuppressWarnings("unchecked")
-  public Tuple newTuple(List c) {
-    return new ZebraTuple(c);
-  }   
-
-  @SuppressWarnings("unchecked")
-  public Tuple newTupleNoCopy(List list) {
-    return new ZebraTuple(list, 1); 
-  }   
-
-  public Tuple newTuple(Object datum) {
-    Tuple t = new ZebraTuple(1);
-    try {
-        t.set(0, datum);
-    } catch (ExecException e) {
-        // The world has come to an end, we just allocated a tuple with one slot
-        // but we can't write to that slot.
-        throw new RuntimeException("Unable to write to field 0 in newly " + 
-            "allocated tuple of size 1!", e); 
-    }
-    return t;
-  }   
-
-  public Class tupleClass() {
-    return ZebraTuple.class;
-  }   
-
-  ZebraTupleFactory() {
-  }
-  
-  public static ZebraTupleFactory getZebraTupleFactoryInstance() {
-    if (self == null) {
-      self = new ZebraTupleFactory();
-    }
-    return self;
-  }
-
-  @Override
-  public boolean isFixedSize() {
-    return false;
-  }
-}
diff --git a/contrib/zebra/src/java/org/apache/hadoop/zebra/types/package-info.java b/contrib/zebra/src/java/org/apache/hadoop/zebra/types/package-info.java
deleted file mode 100644
index 159104904..000000000
--- a/contrib/zebra/src/java/org/apache/hadoop/zebra/types/package-info.java
+++ /dev/null
@@ -1,21 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-/**
- * Data types being shared between the io and mapred packages.
- */
-package org.apache.hadoop.zebra.types;
diff --git a/contrib/zebra/src/test/e2e/mergeJoin/1.txt b/contrib/zebra/src/test/e2e/mergeJoin/1.txt
deleted file mode 100644
index 8b96da90f..000000000
--- a/contrib/zebra/src/test/e2e/mergeJoin/1.txt
+++ /dev/null
@@ -1,4 +0,0 @@
-100	100.1	100	50e+2	something	someting	(0,0)	[a#0,b#0,c#0]
-1	1.1	11	1.1	some	some	(1,1)	[a#1,b#1,c#1]
--100	-100.1	-100	-50e+2	so	so	(2,2)	[a#2,b#2,c#2]
-3	1.1	11	1.1	some	some	(3,3)	[a#3,b#3,c#3]
diff --git a/contrib/zebra/src/test/e2e/mergeJoin/2.txt b/contrib/zebra/src/test/e2e/mergeJoin/2.txt
deleted file mode 100644
index d93526dc8..000000000
--- a/contrib/zebra/src/test/e2e/mergeJoin/2.txt
+++ /dev/null
@@ -1,4 +0,0 @@
-100	100.1	100	50e+2	something	someting	(0,0)	[b#0,c#0,a#0]
-2	1.2	12	1.2	somee	somee	(1,1)	[a#1,b#1,c#1]
--99	-99.1	-99	-40e+2	soo	soo	(2,2)	[a#2,b#2,c#2]
-3	1.1	11	1.1	some	some	(3,3)	[b#3,c#3,a#3]
diff --git a/contrib/zebra/src/test/e2e/mergeJoin/bool.pig b/contrib/zebra/src/test/e2e/mergeJoin/bool.pig
deleted file mode 100644
index 57f3a97a0..000000000
--- a/contrib/zebra/src/test/e2e/mergeJoin/bool.pig
+++ /dev/null
@@ -1,23 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load 'bool.txt' as (b1:Boolean, b2:Boolean);
-dump a1;
-
-             
diff --git a/contrib/zebra/src/test/e2e/mergeJoin/bool.txt b/contrib/zebra/src/test/e2e/mergeJoin/bool.txt
deleted file mode 100644
index 44f323a3e..000000000
--- a/contrib/zebra/src/test/e2e/mergeJoin/bool.txt
+++ /dev/null
@@ -1,2 +0,0 @@
-true	true
-false	false
diff --git a/contrib/zebra/src/test/e2e/mergeJoin/leftBigMerge.pig b/contrib/zebra/src/test/e2e/mergeJoin/leftBigMerge.pig
deleted file mode 100644
index 650637d71..000000000
--- a/contrib/zebra/src/test/e2e/mergeJoin/leftBigMerge.pig
+++ /dev/null
@@ -1,36 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-a2 = load 'reverse.txt' as (m1:map[],r1(f1:chararray,f2:chararray),f:bytearray,e:chararray,d:double,c:long,b:float,a:int);     
---dump a1;                
-                      
-a1order = order a1 by e;  
-a2order = order a2 by e;   
-       
-
-store a1order into 'new11' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');    
-store a2order into 'new22' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');
-
-rec1 = load 'new11' using org.apache.hadoop.zebra.pig.TableLoader(); 
-rec2 = load 'new22' using org.apache.hadoop.zebra.pig.TableLoader(); 
-joina = join rec1 by (e), rec2 by (e) using "merge" ;     
-dump joina;
-             
diff --git a/contrib/zebra/src/test/e2e/mergeJoin/mergeBytes.pig b/contrib/zebra/src/test/e2e/mergeJoin/mergeBytes.pig
deleted file mode 100644
index e4745ab62..000000000
--- a/contrib/zebra/src/test/e2e/mergeJoin/mergeBytes.pig
+++ /dev/null
@@ -1,37 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-a = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
---dump a1;                
-                      
-a1order = order a1 by f;  
-aorder = order a by f;   
-dump a1order;       
-
-store a1order into 'f1' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');    
-store aorder into 'f2' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');
-
-rec1 = load 'f1' using org.apache.hadoop.zebra.pig.TableLoader(); 
-rec2 = load 'f2' using org.apache.hadoop.zebra.pig.TableLoader(); 
---records1 = LOAD 'f1,f2' USING org.apache.hadoop.zebra.pig.TableLoader ('source_table,f,m1,a,b,c,d,e', 'sorted');
-joina = join rec1 by f, rec2 by f using "merge" ;     
-dump joina;
-             
diff --git a/contrib/zebra/src/test/e2e/mergeJoin/mergeDouble.pig b/contrib/zebra/src/test/e2e/mergeJoin/mergeDouble.pig
deleted file mode 100644
index d43bda575..000000000
--- a/contrib/zebra/src/test/e2e/mergeJoin/mergeDouble.pig
+++ /dev/null
@@ -1,38 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-a = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
---dump a;                
-                      
-a1order = order a1 by d;  
-aorder = order a by d;   
-       
-
-store a1order into 'd1' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');    
-store aorder into 'd2' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');
-
-rec1 = load 'd1' using org.apache.hadoop.zebra.pig.TableLoader(); 
-rec2 = load 'd2' using org.apache.hadoop.zebra.pig.TableLoader(); 
---records1 = LOAD 'd1,d' USING org.apache.hadoop.zebra.pig.TableLoader ('d,e,f,m1,a,b,c', 'sorted');
-joina = join rec1 by d, rec2 by d using "merge" ;     
---dump records1;
-dump joina;
-             
diff --git a/contrib/zebra/src/test/e2e/mergeJoin/mergeEmpty.pig b/contrib/zebra/src/test/e2e/mergeJoin/mergeEmpty.pig
deleted file mode 100644
index 62293f273..000000000
--- a/contrib/zebra/src/test/e2e/mergeJoin/mergeEmpty.pig
+++ /dev/null
@@ -1,36 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-a2 = load 'empty.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
---dump a1;                
-                      
-a1order = order a1 by a;  
-a2order = order a2 by a;   
-       
-
-store a1order into 'a1' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');    
-store a2order into 'empty' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');
-
-rec1 = load 'a1' using org.apache.hadoop.zebra.pig.TableLoader(); 
-rec2 = load 'empty' using org.apache.hadoop.zebra.pig.TableLoader(); 
-joina = join rec1 by a, rec2 by a using "merge" ;     
-dump joina;
-             
diff --git a/contrib/zebra/src/test/e2e/mergeJoin/mergeFloat.pig b/contrib/zebra/src/test/e2e/mergeJoin/mergeFloat.pig
deleted file mode 100644
index 0e04cbb8d..000000000
--- a/contrib/zebra/src/test/e2e/mergeJoin/mergeFloat.pig
+++ /dev/null
@@ -1,38 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-a = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
---dump a;                
-                      
-a1order = order a1 by b;  
-aorder = order a by b;   
-       
-
-store a1order into 'b1' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');    
-store aorder into 'b2' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');
-
-rec1 = load 'b1' using org.apache.hadoop.zebra.pig.TableLoader(); 
-rec2 = load 'b2' using org.apache.hadoop.zebra.pig.TableLoader(); 
---records1 = LOAD 'b1,b2' USING org.apache.hadoop.zebra.pig.TableLoader ('b,c,d,e,f,r1,m1,a', 'sorted');
-joina = join rec1 by b, rec2 by b using "merge" ;     
---dump records1;
-dump joina;
-             
diff --git a/contrib/zebra/src/test/e2e/mergeJoin/mergeInt.pig b/contrib/zebra/src/test/e2e/mergeJoin/mergeInt.pig
deleted file mode 100644
index 52ccf5fbe..000000000
--- a/contrib/zebra/src/test/e2e/mergeJoin/mergeInt.pig
+++ /dev/null
@@ -1,38 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-a = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
---dump a;                
-                      
-a1order = order a1 by a;  
-aorder = order a by a;   
-       
-
-store a1order into 'a1' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');    
-store aorder into 'a2' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');
-
-rec1 = load 'a1' using org.apache.hadoop.zebra.pig.TableLoader(); 
-rec2 = load 'a2' using org.apache.hadoop.zebra.pig.TableLoader(); 
---records1 = LOAD 'a1,a2' USING org.apache.hadoop.zebra.pig.TableLoader ('a,b,c,d,e,f,r1,m1', 'sorted');
-joina = join rec1 by a, rec2 by a using "merge" ;     
---dump records1;
-dump joina;
-             
diff --git a/contrib/zebra/src/test/e2e/mergeJoin/mergeLong.pig b/contrib/zebra/src/test/e2e/mergeJoin/mergeLong.pig
deleted file mode 100644
index 65a6059a4..000000000
--- a/contrib/zebra/src/test/e2e/mergeJoin/mergeLong.pig
+++ /dev/null
@@ -1,38 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-a = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
---dump a;                
-                      
-a1order = order a1 by c;  
-aorder = order a by c;   
-       
-
-store a1order into 'c1' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');    
-store aorder into 'c2' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');
-
-rec1 = load 'c1' using org.apache.hadoop.zebra.pig.TableLoader(); 
-rec2 = load 'c2' using org.apache.hadoop.zebra.pig.TableLoader(); 
---records1 = LOAD 'c1,c2' USING org.apache.hadoop.zebra.pig.TableLoader ('c,d,e,f,r1,m1,a,b', 'sorted');
-joina = join rec1 by c, rec2 by c using "merge" ;     
---dump records1;
-dump joina;
-             
diff --git a/contrib/zebra/src/test/e2e/mergeJoin/mergeNotCommonKey.pig b/contrib/zebra/src/test/e2e/mergeJoin/mergeNotCommonKey.pig
deleted file mode 100644
index 132fa0b56..000000000
--- a/contrib/zebra/src/test/e2e/mergeJoin/mergeNotCommonKey.pig
+++ /dev/null
@@ -1,38 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '3.txt' as (e:chararray,f:chararray,r1:chararray);
-
-a2 = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
---dump a1;                
-                      
-a1order = order a1 by e;  
-a2order = order a2 by a;   
-       
-
---store a1order into 'notCommonSortKey1' using org.apache.hadoop.zebra.pig.TableStorer('[e,f,r1]');    
-store a2order into 'notCommonSortKey2' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');
-
-rec1 = load 'notCommonSortKey1' using org.apache.hadoop.zebra.pig.TableLoader(); 
-rec2 = load 'notCommonSortKey2' using org.apache.hadoop.zebra.pig.TableLoader(); 
-joina = join rec1 by e, rec2 by a using "merge" ;     
-dump joina;
-
---ERROR 1107: Cannot merge join keys, incompatible types
-
diff --git a/contrib/zebra/src/test/e2e/mergeJoin/mergeString.pig b/contrib/zebra/src/test/e2e/mergeJoin/mergeString.pig
deleted file mode 100644
index 1423628bc..000000000
--- a/contrib/zebra/src/test/e2e/mergeJoin/mergeString.pig
+++ /dev/null
@@ -1,38 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-a = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
---dump a;                
-                      
-a1order = order a1 by e;  
-aorder = order a by e;   
-       
-
-store a1order into 'e1' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');    
-store aorder into 'e2' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');
-
-rec1 = load 'e1' using org.apache.hadoop.zebra.pig.TableLoader(); 
-rec2 = load 'e2' using org.apache.hadoop.zebra.pig.TableLoader(); 
---records1 = LOAD 'e1,e2' USING org.apache.hadoop.zebra.pig.TableLoader ('e,f,r1,m1,a,b,c,d', 'sorted');
-joina = join rec1 by e, rec2 by e using "merge" ;     
---dump records1;
-dump joina;
-             
diff --git a/contrib/zebra/src/test/e2e/mergeJoin/mergeab.pig b/contrib/zebra/src/test/e2e/mergeJoin/mergeab.pig
deleted file mode 100644
index d7bf26103..000000000
--- a/contrib/zebra/src/test/e2e/mergeJoin/mergeab.pig
+++ /dev/null
@@ -1,36 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-a2 = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
---dump a1;                
-                      
-a1order = order a1 by a,b;  
-a2order = order a2 by a,b;   
-       
-
-store a1order into 'ab1' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');    
-store a2order into 'ab2' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');
-
-rec1 = load 'ab1' using org.apache.hadoop.zebra.pig.TableLoader(); 
-rec2 = load 'ab2' using org.apache.hadoop.zebra.pig.TableLoader(); 
-joina = join rec1 by (a,b), rec2 by (a,b) using "merge" ;     
-dump joina;
-             
diff --git a/contrib/zebra/src/test/e2e/mergeJoin/mergeabc.pig b/contrib/zebra/src/test/e2e/mergeJoin/mergeabc.pig
deleted file mode 100644
index 3aca66715..000000000
--- a/contrib/zebra/src/test/e2e/mergeJoin/mergeabc.pig
+++ /dev/null
@@ -1,38 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-a2 = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
---dump a1;                
-                      
-a1order = order a1 by a,b,c;  
-a2order = order a2 by a,b,c;   
-       
-
-store a1order into 'abc1' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');    
-store a2order into 'abc2' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');
-
-rec1 = load 'abc1' using org.apache.hadoop.zebra.pig.TableLoader(); 
-rec2 = load 'abc2' using org.apache.hadoop.zebra.pig.TableLoader(); 
-joina = join rec1 by (a,b,c), rec2 by (a,b,c) using "merge" ;     
-dump joina;
-            
- 
---store rec1 into 'merge-table' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');    
diff --git a/contrib/zebra/src/test/e2e/mergeJoin/mergeac.pig b/contrib/zebra/src/test/e2e/mergeJoin/mergeac.pig
deleted file mode 100644
index 1c74b81f5..000000000
--- a/contrib/zebra/src/test/e2e/mergeJoin/mergeac.pig
+++ /dev/null
@@ -1,36 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-a2 = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
---dump a1;                
-                      
-a1order = order a1 by a,c;  
-a2order = order a2 by a,c;   
-       
-
-store a1order into 'ac1' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');    
-store a2order into 'ac2' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');
-
-rec1 = load 'ac1' using org.apache.hadoop.zebra.pig.TableLoader(); 
-rec2 = load 'ac2' using org.apache.hadoop.zebra.pig.TableLoader(); 
-joina = join rec1 by (a,c), rec2 by (a,c) using "merge" ;     
-dump joina;
-             
diff --git a/contrib/zebra/src/test/e2e/mergeJoin/readme b/contrib/zebra/src/test/e2e/mergeJoin/readme
deleted file mode 100644
index ddcdc1cc1..000000000
--- a/contrib/zebra/src/test/e2e/mergeJoin/readme
+++ /dev/null
@@ -1,6 +0,0 @@
-1. put zebra.jar to /grid/0/dev/hadoopqa/jars
-2. put 1.txt 2.txt empty.txt reverse.txt bool.txt to  hdfs /users/hadoopqa/.
-3. run each pig script.
-java  -cp /grid/0/dev/hadoopqa/jing1234/conf:/grid/0/dev/hadoopqa/jars/pig.jar:/grid/0/dev/hadoopqa/jars/zebra.jar  org.apache.pig.Main -M <my.pig>               
-  
-
diff --git a/contrib/zebra/src/test/e2e/mergeJoin/reverse.txt b/contrib/zebra/src/test/e2e/mergeJoin/reverse.txt
deleted file mode 100644
index c479a7f3a..000000000
--- a/contrib/zebra/src/test/e2e/mergeJoin/reverse.txt
+++ /dev/null
@@ -1,4 +0,0 @@
-[a#-1,b#-1,c#-1]	(-1,-1)	somet	someth	5e+2	10	10.1	10                  
-[a#0,b#0,c#0]	(0,0)	som	som	1.0	11	1.0	0                                    
-[a#-2,b#-22,c#-2]  	(-1,-1)	s	s 	-510e+2	-1010 	-1010.1	-1100                     
-[a2,b#2,c#2]	(0,0)	som 	som	1.0	10	1.0	0	
diff --git a/contrib/zebra/src/test/e2e/multi-query/filter.pig b/contrib/zebra/src/test/e2e/multi-query/filter.pig
deleted file mode 100644
index 90c7e341a..000000000
--- a/contrib/zebra/src/test/e2e/multi-query/filter.pig
+++ /dev/null
@@ -1,29 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/hadoop/lib/zebra.jar;
-A = load 'filter.txt' as (name:chararray, age:int);
-
-B = filter A by age < 20;
---dump B;
-store B into 'filter1' using org.apache.hadoop.zebra.pig.TableStorer('[name];[age]');
-
-C = filter A by age >= 20;
---dump C;
-Store C into 'filter2' using org.apache.hadoop.zebra.pig.TableStorer('[name];[age]');
-
diff --git a/contrib/zebra/src/test/e2e/multi-query/filterandgroup.pig b/contrib/zebra/src/test/e2e/multi-query/filterandgroup.pig
deleted file mode 100644
index b0eb84776..000000000
--- a/contrib/zebra/src/test/e2e/multi-query/filterandgroup.pig
+++ /dev/null
@@ -1,33 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/hadoop/lib/zebra.jar;
-A = load 'filter.txt' as (name:chararray, age:int);
-
-B = filter A by age < 20;
---dump B;
-store B into 'filter1' using org.apache.hadoop.zebra.pig.TableStorer('[name];[age]');
-
-C = filter A by age >= 20;
---dump C;
-Store C into 'filter2' using org.apache.hadoop.zebra.pig.TableStorer('[name];[age]');
-
-D = group A by age;
-E = foreach D generate group as group1,  COUNT(A.age) as myage;
---E = foreach D generate group;
-Store E into 'filterandgroup' using org.apache.hadoop.zebra.pig.TableStorer('[group1];[myage]');
diff --git a/contrib/zebra/src/test/e2e/multi-query/filterandgroup_wrong.pig b/contrib/zebra/src/test/e2e/multi-query/filterandgroup_wrong.pig
deleted file mode 100644
index b0eb84776..000000000
--- a/contrib/zebra/src/test/e2e/multi-query/filterandgroup_wrong.pig
+++ /dev/null
@@ -1,33 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/hadoop/lib/zebra.jar;
-A = load 'filter.txt' as (name:chararray, age:int);
-
-B = filter A by age < 20;
---dump B;
-store B into 'filter1' using org.apache.hadoop.zebra.pig.TableStorer('[name];[age]');
-
-C = filter A by age >= 20;
---dump C;
-Store C into 'filter2' using org.apache.hadoop.zebra.pig.TableStorer('[name];[age]');
-
-D = group A by age;
-E = foreach D generate group as group1,  COUNT(A.age) as myage;
---E = foreach D generate group;
-Store E into 'filterandgroup' using org.apache.hadoop.zebra.pig.TableStorer('[group1];[myage]');
diff --git a/contrib/zebra/src/test/e2e/multi-query/group.pig b/contrib/zebra/src/test/e2e/multi-query/group.pig
deleted file mode 100644
index eeb8df53e..000000000
--- a/contrib/zebra/src/test/e2e/multi-query/group.pig
+++ /dev/null
@@ -1,28 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/hadoop/lib/zebra.jar;
-A = load 'filter.txt' as (name:chararray, age:int);
-
-B = group A by name;
-C = foreach B generate group as group1, COUNT(A.name) as myname;
-Store C into 'group1' using org.apache.hadoop.zebra.pig.TableStorer('[group1];[myname]');
-D = group A by age;
-E = foreach D generate group as group2, COUNT(A.age) as myage;
-Store E into 'group2' using org.apache.hadoop.zebra.pig.TableStorer('[group2];[myage]');
- 
diff --git a/contrib/zebra/src/test/e2e/multi-query/group_wrong.pig b/contrib/zebra/src/test/e2e/multi-query/group_wrong.pig
deleted file mode 100644
index 6fc8f51dc..000000000
--- a/contrib/zebra/src/test/e2e/multi-query/group_wrong.pig
+++ /dev/null
@@ -1,23 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra-111.jar;
-A = load 'filter.txt' as (name:chararray, age:int);
-B = group A by name;
-C = foreach B generate group, COUNT(A.name) as cnt;
-Store C into 'group1' using org.apache.hadoop.zebra.pig.TableStorer('[group];[cnt]');
diff --git a/contrib/zebra/src/test/e2e/orderPreserveUnion/1.txt b/contrib/zebra/src/test/e2e/orderPreserveUnion/1.txt
deleted file mode 100644
index 8b96da90f..000000000
--- a/contrib/zebra/src/test/e2e/orderPreserveUnion/1.txt
+++ /dev/null
@@ -1,4 +0,0 @@
-100	100.1	100	50e+2	something	someting	(0,0)	[a#0,b#0,c#0]
-1	1.1	11	1.1	some	some	(1,1)	[a#1,b#1,c#1]
--100	-100.1	-100	-50e+2	so	so	(2,2)	[a#2,b#2,c#2]
-3	1.1	11	1.1	some	some	(3,3)	[a#3,b#3,c#3]
diff --git a/contrib/zebra/src/test/e2e/orderPreserveUnion/2.txt b/contrib/zebra/src/test/e2e/orderPreserveUnion/2.txt
deleted file mode 100644
index d93526dc8..000000000
--- a/contrib/zebra/src/test/e2e/orderPreserveUnion/2.txt
+++ /dev/null
@@ -1,4 +0,0 @@
-100	100.1	100	50e+2	something	someting	(0,0)	[b#0,c#0,a#0]
-2	1.2	12	1.2	somee	somee	(1,1)	[a#1,b#1,c#1]
--99	-99.1	-99	-40e+2	soo	soo	(2,2)	[a#2,b#2,c#2]
-3	1.1	11	1.1	some	some	(3,3)	[b#3,c#3,a#3]
diff --git a/contrib/zebra/src/test/e2e/orderPreserveUnion/readme b/contrib/zebra/src/test/e2e/orderPreserveUnion/readme
deleted file mode 100644
index 1794c10fb..000000000
--- a/contrib/zebra/src/test/e2e/orderPreserveUnion/readme
+++ /dev/null
@@ -1,6 +0,0 @@
-1. put zebra.jar to /grid/0/dev/hadoopqa/jars
-2. put 1.txt 2.txt  hdfs /users/hadoopqa/.
-3. run each pig script.
-java  -cp /grid/0/dev/hadoopqa/jing1234/conf:/grid/0/dev/hadoopqa/jars/pig.jar:/grid/0/dev/hadoopqa/jars/zebra.jar  org.apache.pig.Main -M <my.pig>               
-  
-
diff --git a/contrib/zebra/src/test/e2e/orderPreserveUnion/unionBytes.pig b/contrib/zebra/src/test/e2e/orderPreserveUnion/unionBytes.pig
deleted file mode 100644
index 4b5baface..000000000
--- a/contrib/zebra/src/test/e2e/orderPreserveUnion/unionBytes.pig
+++ /dev/null
@@ -1,37 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-a = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
---dump a1;                
-                      
-a1order = order a1 by f;  
-aorder = order a by f;   
---dump a1order;       
-
-store a1order into 'f1' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');    
-store aorder into 'f2' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');
-
-rec1 = load 'f1' using org.apache.hadoop.zebra.pig.TableLoader(); 
-rec2 = load 'f2' using org.apache.hadoop.zebra.pig.TableLoader(); 
-records1 = LOAD 'f1,f2' USING org.apache.hadoop.zebra.pig.TableLoader ('source_table,f,m1,a,b,c,d,e', 'sorted');
-joina = join rec1 by a, rec2 by a using "merge" ;     
-dump records1;
-             
diff --git a/contrib/zebra/src/test/e2e/orderPreserveUnion/unionDouble.pig b/contrib/zebra/src/test/e2e/orderPreserveUnion/unionDouble.pig
deleted file mode 100644
index a24be15d3..000000000
--- a/contrib/zebra/src/test/e2e/orderPreserveUnion/unionDouble.pig
+++ /dev/null
@@ -1,37 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-a = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
---dump a;                
-                      
-a1order = order a1 by d;  
-aorder = order a by d;   
-       
-
-store a1order into 'd1' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');    
-store aorder into 'd' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');
-
-rec1 = load 'd1' using org.apache.hadoop.zebra.pig.TableLoader(); 
-rec2 = load 'd' using org.apache.hadoop.zebra.pig.TableLoader(); 
-records1 = LOAD 'd1,d' USING org.apache.hadoop.zebra.pig.TableLoader ('d,e,f,m1,a,b,c', 'sorted');
---joina = join rec1 by a, rec2 by a using "merge" ;     
-dump records1;
-             
diff --git a/contrib/zebra/src/test/e2e/orderPreserveUnion/unionFloat.pig b/contrib/zebra/src/test/e2e/orderPreserveUnion/unionFloat.pig
deleted file mode 100644
index 4f3a26052..000000000
--- a/contrib/zebra/src/test/e2e/orderPreserveUnion/unionFloat.pig
+++ /dev/null
@@ -1,37 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-a = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
---dump a;                
-                      
-a1order = order a1 by b;  
-aorder = order a by b;   
-       
-
-store a1order into 'b1' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');    
-store aorder into 'b2' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');
-
-rec1 = load 'b1' using org.apache.hadoop.zebra.pig.TableLoader(); 
-rec2 = load 'b2' using org.apache.hadoop.zebra.pig.TableLoader(); 
-records1 = LOAD 'b1,b2' USING org.apache.hadoop.zebra.pig.TableLoader ('b,c,d,e,f,r1,m1,a', 'sorted');
---joina = join rec1 by a, rec2 by a2 using "merge" ;     
-dump records1;
-             
diff --git a/contrib/zebra/src/test/e2e/orderPreserveUnion/unionInt.pig b/contrib/zebra/src/test/e2e/orderPreserveUnion/unionInt.pig
deleted file mode 100644
index e717390dd..000000000
--- a/contrib/zebra/src/test/e2e/orderPreserveUnion/unionInt.pig
+++ /dev/null
@@ -1,37 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-a = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-dump a;                
-                      
-a1order = order a1 by a;  
-aorder = order a by a;   
-       
-
-store a1order into 'a1' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');    
-store aorder into 'a2' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');
-
-rec1 = load 'a1' using org.apache.hadoop.zebra.pig.TableLoader(); 
-rec2 = load 'a2' using org.apache.hadoop.zebra.pig.TableLoader(); 
-records1 = LOAD 'a1,a2' USING org.apache.hadoop.zebra.pig.TableLoader ('a,b,c,d,e,f,r1,m1', 'sorted');
---joina = join rec1 by a, rec2 by a2 using "merge" ;     
-dump records1;
-             
diff --git a/contrib/zebra/src/test/e2e/orderPreserveUnion/unionLong.pig b/contrib/zebra/src/test/e2e/orderPreserveUnion/unionLong.pig
deleted file mode 100644
index 97bcc3d5d..000000000
--- a/contrib/zebra/src/test/e2e/orderPreserveUnion/unionLong.pig
+++ /dev/null
@@ -1,37 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-a = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-dump a;                
-                      
-a1order = order a1 by c;  
-aorder = order a by c;   
-       
-
-store a1order into 'c1' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');    
-store aorder into 'c2' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');
-
-rec1 = load 'c1' using org.apache.hadoop.zebra.pig.TableLoader(); 
-rec2 = load 'c2' using org.apache.hadoop.zebra.pig.TableLoader(); 
-records1 = LOAD 'c1,c2' USING org.apache.hadoop.zebra.pig.TableLoader ('c,d,e,f,r1,m1,a,b', 'sorted');
---joina = join rec1 by a, rec2 by a2 using "merge" ;     
-dump records1;
-             
diff --git a/contrib/zebra/src/test/e2e/orderPreserveUnion/unionString.pig b/contrib/zebra/src/test/e2e/orderPreserveUnion/unionString.pig
deleted file mode 100644
index 3d0fb65c5..000000000
--- a/contrib/zebra/src/test/e2e/orderPreserveUnion/unionString.pig
+++ /dev/null
@@ -1,37 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-a = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
---dump a;                
-                      
-a1order = order a1 by e;  
-aorder = order a by e;   
-       
-
-store a1order into 'e1' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');    
-store aorder into 'e2' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');
-
-rec1 = load 'e1' using org.apache.hadoop.zebra.pig.TableLoader(); 
-rec2 = load 'e2' using org.apache.hadoop.zebra.pig.TableLoader(); 
-records1 = LOAD 'e1,e2' USING org.apache.hadoop.zebra.pig.TableLoader ('e,f,r1,m1,a,b,c,d', 'sorted');
---joina = join rec1 by a, rec2 by a2 using "merge" ;     
-dump records1;
-             
diff --git a/contrib/zebra/src/test/e2e/pruning/1.txt b/contrib/zebra/src/test/e2e/pruning/1.txt
deleted file mode 100644
index 8b96da90f..000000000
--- a/contrib/zebra/src/test/e2e/pruning/1.txt
+++ /dev/null
@@ -1,4 +0,0 @@
-100	100.1	100	50e+2	something	someting	(0,0)	[a#0,b#0,c#0]
-1	1.1	11	1.1	some	some	(1,1)	[a#1,b#1,c#1]
--100	-100.1	-100	-50e+2	so	so	(2,2)	[a#2,b#2,c#2]
-3	1.1	11	1.1	some	some	(3,3)	[a#3,b#3,c#3]
diff --git a/contrib/zebra/src/test/e2e/pruning/2.txt b/contrib/zebra/src/test/e2e/pruning/2.txt
deleted file mode 100644
index d93526dc8..000000000
--- a/contrib/zebra/src/test/e2e/pruning/2.txt
+++ /dev/null
@@ -1,4 +0,0 @@
-100	100.1	100	50e+2	something	someting	(0,0)	[b#0,c#0,a#0]
-2	1.2	12	1.2	somee	somee	(1,1)	[a#1,b#1,c#1]
--99	-99.1	-99	-40e+2	soo	soo	(2,2)	[a#2,b#2,c#2]
-3	1.1	11	1.1	some	some	(3,3)	[b#3,c#3,a#3]
diff --git a/contrib/zebra/src/test/e2e/pruning/foreach.pig b/contrib/zebra/src/test/e2e/pruning/foreach.pig
deleted file mode 100644
index d754c2281..000000000
--- a/contrib/zebra/src/test/e2e/pruning/foreach.pig
+++ /dev/null
@@ -1,36 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-a2 = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-n2 = foreach a1 generate a;
-
---this store will fail since there is no b and c
---store n2 into 'foreach1' using org.apache.hadoop.zebra.pig.TableStorer('[a];[b];[c]');
-
---this store will pass
-store n2 into 'foreach1' using org.apache.hadoop.zebra.pig.TableStorer('[a]');
-
---this load will pass, the second column 'b' will return empty string
-a3 = load 'foreach1' using org.apache.hadoop.zebra.pig.TableLoader('a,b');
-dump a3;
---store a2 into 'foreach2' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');
-
diff --git a/contrib/zebra/src/test/e2e/pruning/foreach2.pig b/contrib/zebra/src/test/e2e/pruning/foreach2.pig
deleted file mode 100644
index 00ae7613a..000000000
--- a/contrib/zebra/src/test/e2e/pruning/foreach2.pig
+++ /dev/null
@@ -1,36 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-a2 = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-n2 = foreach a1 generate a,b,c,d,e,f;
-
---this store will fail since there is no b and c
---store n2 into 'foreach1' using org.apache.hadoop.zebra.pig.TableStorer('[a];[b];[c]');
-
---this store will pass
-store n2 into 'foreach2' using org.apache.hadoop.zebra.pig.TableStorer('');
-
---this load will pass, the second column 'b' will return empty string
-a3 = load 'foreach2' using org.apache.hadoop.zebra.pig.TableLoader();
-dump a3;
---store a2 into 'foreach2' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');
-
diff --git a/contrib/zebra/src/test/e2e/pruning/j.pig b/contrib/zebra/src/test/e2e/pruning/j.pig
deleted file mode 100644
index 6409f543a..000000000
--- a/contrib/zebra/src/test/e2e/pruning/j.pig
+++ /dev/null
@@ -1,24 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-dump a1;
-store a1 into 'j1' using org.apache.hadoop.zebra.pig.TableStorer('');
-
-
diff --git a/contrib/zebra/src/test/e2e/pruning/join.pig b/contrib/zebra/src/test/e2e/pruning/join.pig
deleted file mode 100644
index 5a2d33fd3..000000000
--- a/contrib/zebra/src/test/e2e/pruning/join.pig
+++ /dev/null
@@ -1,29 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-b = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-
-c = join a by a, b by a;
-d = foreach c generate a::a as a, a::b as b, b::c as c;
-describe d;
---store d into 'join1' using org.apache.hadoop.zebra.pig.TableStorer('[a];[b];[c]');
-store d into 'join2' using org.apache.hadoop.zebra.pig.TableStorer('');
diff --git a/contrib/zebra/src/test/e2e/pruning/join2.pig b/contrib/zebra/src/test/e2e/pruning/join2.pig
deleted file mode 100644
index 2fa27b619..000000000
--- a/contrib/zebra/src/test/e2e/pruning/join2.pig
+++ /dev/null
@@ -1,29 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-b = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-
-c = join a by a, b by a;
-d = foreach c generate a::a, a::b, b::c;
-describe d;
-dump d;
-store d into 'join3' using org.apache.hadoop.zebra.pig.TableStorer('');
diff --git a/contrib/zebra/src/test/e2e/pruning/m.pig b/contrib/zebra/src/test/e2e/pruning/m.pig
deleted file mode 100644
index 4dbf866ee..000000000
--- a/contrib/zebra/src/test/e2e/pruning/m.pig
+++ /dev/null
@@ -1,25 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-b = store a into 'm1' using org.apache.hadoop.zebra.pig.TableStorer('');
-c = load 'm1' using org.apache.hadoop.zebra.pig.TableLoader('m1#{a}');
-store c  into 'm2' using org.apache.hadoop.zebra.pig.TableStorer('');
-
-
diff --git a/contrib/zebra/src/test/e2e/pruning/map1.pig b/contrib/zebra/src/test/e2e/pruning/map1.pig
deleted file mode 100644
index cdb5c449c..000000000
--- a/contrib/zebra/src/test/e2e/pruning/map1.pig
+++ /dev/null
@@ -1,31 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
---user has to have as xxx, otherwise the later store will fail. since there is no name and only type passed to zebra.
-b =  foreach a generate m1#'a' as ms1;
-describe b;
-
---this store will pass, in table map1 will only have m1#'a'
-store b into 'map1' using org.apache.hadoop.zebra.pig.TableStorer('');
---this also pass
---store b into 'map2' using org.apache.hadoop.zebra.pig.TableStorer('[ms1]');
-
-
diff --git a/contrib/zebra/src/test/e2e/pruning/map2.pig b/contrib/zebra/src/test/e2e/pruning/map2.pig
deleted file mode 100644
index de4aa2434..000000000
--- a/contrib/zebra/src/test/e2e/pruning/map2.pig
+++ /dev/null
@@ -1,32 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-a2 = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-n1 = foreach a1 generate a,b,m1#'a' as ms1;;
-n2 = foreach a2 generate a,b,m1#'b' as ms2;
-
-j = join n1 by (a,b), n2 by (a,b);
-dump j; 
---check table j should only have 6 columns, a,b, and m1#{a} of n1 and a,b, m1#{a} of n2
-store j into 'maps1' using org.apache.hadoop.zebra.pig.TableStorer('');
-
-
diff --git a/contrib/zebra/src/test/e2e/pruning/map3.pig b/contrib/zebra/src/test/e2e/pruning/map3.pig
deleted file mode 100644
index 53de0dffe..000000000
--- a/contrib/zebra/src/test/e2e/pruning/map3.pig
+++ /dev/null
@@ -1,29 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
---user has to have as xxx, otherwise the later store will fail. since there is no name and only type passed to zebra.
-b =  foreach a generate m1#'a' as ms1, m1#'b' as ms2;
-describe b;
-
---this store will pass, in table map1 will only have m1#'a', and m1#{'b'}
-store b into 'map3' using org.apache.hadoop.zebra.pig.TableStorer('');
-
-
diff --git a/contrib/zebra/src/test/e2e/pruning/map4.pig b/contrib/zebra/src/test/e2e/pruning/map4.pig
deleted file mode 100644
index 46b1ab24c..000000000
--- a/contrib/zebra/src/test/e2e/pruning/map4.pig
+++ /dev/null
@@ -1,29 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
---user has to have as xxx, otherwise the later store will fail. since there is no name and only type passed to zebra.
-b =  foreach a generate a,b,m1#'a' as ms1, m1#'b' as ms2;
-describe b;
-
---this store will pass, in table map1 will only have m1#'a', and m1#{'b'}
-store b into 'map4' using org.apache.hadoop.zebra.pig.TableStorer('');
-
-
diff --git a/contrib/zebra/src/test/e2e/pruning/merge.pig b/contrib/zebra/src/test/e2e/pruning/merge.pig
deleted file mode 100644
index fbc2b38d6..000000000
--- a/contrib/zebra/src/test/e2e/pruning/merge.pig
+++ /dev/null
@@ -1,41 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-a2 = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-                      
-a1order = order a1 by a,b;  
-a2order = order a2 by a,b;   
-       
-
-d = foreach a1order  generate a,b,c;
-store d into 'prune_ab1' using org.apache.hadoop.zebra.pig.TableStorer('[a];[b];[c]');    
-store a2order into 'prune_ab2' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');
-
-rec1 = load 'prune_ab1' using org.apache.hadoop.zebra.pig.TableLoader(); 
-rec2 = load 'prune_ab2' using org.apache.hadoop.zebra.pig.TableLoader(); 
-joina = join rec1 by (a,b), rec2 by (a,b) using "merge" ;     
-
-dump joina;
---table merge should only have 3 columns from left hand, 8 columns from right hand. 
-store joina into 'merge' using org.apache.hadoop.zebra.pig.TableStorer('');
-
-
-             
diff --git a/contrib/zebra/src/test/e2e/pruning/merge1.pig b/contrib/zebra/src/test/e2e/pruning/merge1.pig
deleted file mode 100644
index 934686a43..000000000
--- a/contrib/zebra/src/test/e2e/pruning/merge1.pig
+++ /dev/null
@@ -1,41 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-a2 = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-                      
-a1order = order a1 by a,b;  
-a2order = order a2 by a,b;   
-       
-
-d = foreach a2order  generate a,b;
-store a1order into 'prune_ab11' using org.apache.hadoop.zebra.pig.TableStorer('[a,b];[d,e,f,r1,m1]');    
-store d into 'prune_ab21' using org.apache.hadoop.zebra.pig.TableStorer('[a,b]');
-
-rec1 = load 'prune_ab11' using org.apache.hadoop.zebra.pig.TableLoader(); 
-rec2 = load 'prune_ab21' using org.apache.hadoop.zebra.pig.TableLoader(); 
-joina = join rec1 by (a,b), rec2 by (a,b) using "merge" ;     
-
-dump joina;
---table merge1 should only have 8 columns from left hand, 2 columns from right hand. 
-store joina into 'merge1' using org.apache.hadoop.zebra.pig.TableStorer('');
-
-
-             
diff --git a/contrib/zebra/src/test/e2e/pruning/merge2.pig b/contrib/zebra/src/test/e2e/pruning/merge2.pig
deleted file mode 100644
index 58dfbe6a4..000000000
--- a/contrib/zebra/src/test/e2e/pruning/merge2.pig
+++ /dev/null
@@ -1,42 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-a2 = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-                      
-a1order = order a1 by a,b;  
-a2order = order a2 by a,b;   
-       
-
-c = foreach a1order  generate a,b;
-d = foreach a2order  generate a,b;
-store c into 'prune_ab12' using org.apache.hadoop.zebra.pig.TableStorer('[a,b]');    
-store d into 'prune_ab22' using org.apache.hadoop.zebra.pig.TableStorer('[a,b]');
-
-rec1 = load 'prune_ab12' using org.apache.hadoop.zebra.pig.TableLoader(); 
-rec2 = load 'prune_ab22' using org.apache.hadoop.zebra.pig.TableLoader(); 
-joina = join rec1 by (a,b), rec2 by (a,b) using "merge" ;     
-
-dump joina;
---table merge1 should only have 8 columns from left hand, 2 columns from right hand. 
-store joina into 'merge2' using org.apache.hadoop.zebra.pig.TableStorer('');
-
-
-             
diff --git a/contrib/zebra/src/test/e2e/pruning/merge3.pig b/contrib/zebra/src/test/e2e/pruning/merge3.pig
deleted file mode 100644
index 38e22cf4d..000000000
--- a/contrib/zebra/src/test/e2e/pruning/merge3.pig
+++ /dev/null
@@ -1,41 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-a2 = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-                      
-a1order = order a1 by a,b,m1#'a';  
-a2order = order a2 by a,b,m1#'b';   
-       
-c = foreach a1order  generate a,b,m1#'a' as ms1;
-d = foreach a2order  generate a,b,m1#'b' as ms2;
-store c into 'prune_ab13' using org.apache.hadoop.zebra.pig.TableStorer('[a,b];[ms1]');    
-store d into 'prune_ab23' using org.apache.hadoop.zebra.pig.TableStorer('[a,b];[ms2]');
-
-rec1 = load 'prune_ab13' using org.apache.hadoop.zebra.pig.TableLoader(); 
-rec2 = load 'prune_ab23' using org.apache.hadoop.zebra.pig.TableLoader(); 
-joina = join rec1 by (a,b,ms1), rec2 by (a,b,ms2) using "merge" ;     
-
-dump joina;
---table merge1 should only have 3 columns from left hand, 3 columns from right hand. 
-store joina into 'merge3' using org.apache.hadoop.zebra.pig.TableStorer('');
-
-
-             
diff --git a/contrib/zebra/src/test/e2e/pruning/merge_question.pig b/contrib/zebra/src/test/e2e/pruning/merge_question.pig
deleted file mode 100644
index 18dc3e1ed..000000000
--- a/contrib/zebra/src/test/e2e/pruning/merge_question.pig
+++ /dev/null
@@ -1,42 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-a2 = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-                      
-a1order = order a1 by a,b;  
-a2order = order a2 by a,b;   
-       
-
---store a1order into 'ab1' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');    
---store a2order into 'ab2' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f,r1,m1]');
-
-rec1 = load 'ab1' using org.apache.hadoop.zebra.pig.TableLoader(); 
-rec2 = load 'ab2' using org.apache.hadoop.zebra.pig.TableLoader(); 
---dump rec1;
-d = foreach rec1  generate a,b;
-joina = join d by (a,b), rec2 by (a,b) using "merge" ;     
-
-dump joina;
---table merge1 should only have 3 columns from left hand, 8 columns from right hand. 
-store joina into 'merge_question' using org.apache.hadoop.zebra.pig.TableStorer('');
-
-
-             
diff --git a/contrib/zebra/src/test/e2e/pruning/merge_rh.pig b/contrib/zebra/src/test/e2e/pruning/merge_rh.pig
deleted file mode 100644
index 10984a979..000000000
--- a/contrib/zebra/src/test/e2e/pruning/merge_rh.pig
+++ /dev/null
@@ -1,41 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-a2 = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-a1order = order a1 by a,b;
-a2order = order a2 by a,b;
-
-
-d = foreach a2order  generate a,b;
-store a1order into 'p11' using org.apache.hadoop.zebra.pig.TableStorer('[a,b];[d,e,f,r1,m1]');
-store d into 'p21' using org.apache.hadoop.zebra.pig.TableStorer('[a,b]');
-
-rec1 = load 'p11' using org.apache.hadoop.zebra.pig.TableLoader('','sorted');
-rec2 = load 'p21' using org.apache.hadoop.zebra.pig.TableLoader('','sorted');
-dump rec1;
-dump rec2;
---joina = join rec1 by (a,b), rec2 by (a,b) using "merge" ;
-
---dump joina;
---table merge1 should only have 7 columns from left hand, 2 columns from right hand.
---store joina into 'merge1' using org.apache.hadoop.zebra.pig.TableStorer('');
-
diff --git a/contrib/zebra/src/test/e2e/pruning/mforeach.pig b/contrib/zebra/src/test/e2e/pruning/mforeach.pig
deleted file mode 100644
index c2a25dd19..000000000
--- a/contrib/zebra/src/test/e2e/pruning/mforeach.pig
+++ /dev/null
@@ -1,36 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-a2 = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-n1 = foreach a1 generate a;
-n2 = foreach a2 generate b;
-
-
---this store will pass
-store n1 into 'mforeach11' using org.apache.hadoop.zebra.pig.TableStorer('[a]');
-store n2 into 'mforeach12' using org.apache.hadoop.zebra.pig.TableStorer('');
-
-a3 = load 'mforeach11' using org.apache.hadoop.zebra.pig.TableLoader();
-dump a3;
-
-a4 = load 'mforeach12' using org.apache.hadoop.zebra.pig.TableLoader();
-dump a4;
diff --git a/contrib/zebra/src/test/e2e/pruning/mforeach2.pig b/contrib/zebra/src/test/e2e/pruning/mforeach2.pig
deleted file mode 100644
index 24bba5d59..000000000
--- a/contrib/zebra/src/test/e2e/pruning/mforeach2.pig
+++ /dev/null
@@ -1,36 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-a2 = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-n1 = foreach a1 generate a,c,e;
-n2 = foreach a2 generate b,d,f;
-
-
---this store will pass
-store n1 into 'mforeach21' using org.apache.hadoop.zebra.pig.TableStorer('');
-store n2 into 'mforeach22' using org.apache.hadoop.zebra.pig.TableStorer('');
-
-a3 = load 'mforeach21' using org.apache.hadoop.zebra.pig.TableLoader();
-dump a3;
-
-a4 = load 'mforeach22' using org.apache.hadoop.zebra.pig.TableLoader();
-dump a4;
diff --git a/contrib/zebra/src/test/e2e/pruning/mforeach3.pig b/contrib/zebra/src/test/e2e/pruning/mforeach3.pig
deleted file mode 100644
index 23643a783..000000000
--- a/contrib/zebra/src/test/e2e/pruning/mforeach3.pig
+++ /dev/null
@@ -1,36 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-a2 = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-n1 = foreach a1 generate a,c,e,m1#'a' as ms1, m1#'b' as ms2;
-n2 = foreach a2 generate b,d,f,m1#'a' as ms1,m1#'c' as ms3;
-
-
---this store will pass
-store n1 into 'mforeach31' using org.apache.hadoop.zebra.pig.TableStorer('');
-store n2 into 'mforeach32' using org.apache.hadoop.zebra.pig.TableStorer('');
-
-a3 = load 'mforeach31' using org.apache.hadoop.zebra.pig.TableLoader();
-dump a3;
-
-a4 = load 'mforeach32' using org.apache.hadoop.zebra.pig.TableLoader();
-dump a4;
diff --git a/contrib/zebra/src/test/e2e/pruning/orderby.pig b/contrib/zebra/src/test/e2e/pruning/orderby.pig
deleted file mode 100644
index c87024e8a..000000000
--- a/contrib/zebra/src/test/e2e/pruning/orderby.pig
+++ /dev/null
@@ -1,25 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-b = order a by a;
-c = foreach b generate a as a, b as b;
-describe c;
---There should be 2 columns in orderby1 table
-store c into 'orderby1' using org.apache.hadoop.zebra.pig.TableStorer('');
diff --git a/contrib/zebra/src/test/e2e/pruning/orderby2.pig b/contrib/zebra/src/test/e2e/pruning/orderby2.pig
deleted file mode 100644
index 66b96bce4..000000000
--- a/contrib/zebra/src/test/e2e/pruning/orderby2.pig
+++ /dev/null
@@ -1,27 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-b = order a by m1#'a';
-
-c = foreach b generate a as a, m1#'a' as ms1;
-describe c;
---dump c;
---There should be 2 columns in orderby1 table
-store c into 'orderby2' using org.apache.hadoop.zebra.pig.TableStorer('');
diff --git a/contrib/zebra/src/test/e2e/pruning/readme b/contrib/zebra/src/test/e2e/pruning/readme
deleted file mode 100644
index d06062422..000000000
--- a/contrib/zebra/src/test/e2e/pruning/readme
+++ /dev/null
@@ -1,5 +0,0 @@
-1.put 1.txt and 2.txt to hdf /user/hadoopqa/
-2.run each pig script
-java -cp /grid/0/dev/hadoopqa/jing1234/conf:/grid/0/dev/hadoopqa/jars/pig.jar:/grid/0/dev/hadoopqa/jars/tfile.jar:/grid/0/dev/hadoopqa/jars/zebra.jar org.apache.pig.Main -M <my.pig>
-3. check output table
-$HADOOP_HOME/bin/hadoop fs -cat <table>/.btschema
diff --git a/contrib/zebra/src/test/e2e/pruning/union.pig b/contrib/zebra/src/test/e2e/pruning/union.pig
deleted file mode 100644
index 933a2a374..000000000
--- a/contrib/zebra/src/test/e2e/pruning/union.pig
+++ /dev/null
@@ -1,38 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-a2 = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-                      
-a1order = order a1 by a;  
-a2order = order a2 by a;   
-       
-b =  foreach a1order  generate m1#'a' as ms1;
-c =  foreach a2order generate m1#'b' as ms2;
-
---store b  into 'u1' using org.apache.hadoop.zebra.pig.TableStorer('');    
---store c  into 'u2' using org.apache.hadoop.zebra.pig.TableStorer('');    
-
--- records1 should have only two columns, ms1 and ms2, for column a will return empty
---records1 = LOAD 'u1,u2' USING org.apache.hadoop.zebra.pig.TableLoader ('source_table,ms1,a, ms2','sorted');
-records1 = LOAD 'u1,u2' USING org.apache.hadoop.zebra.pig.TableLoader ('ms1,a, ms2');
---joina = join rec1 by a, rec2 by a2 using "merge" ;     
-dump records1;
-             
diff --git a/contrib/zebra/src/test/e2e/pruning/union1.pig b/contrib/zebra/src/test/e2e/pruning/union1.pig
deleted file mode 100644
index 9f792029f..000000000
--- a/contrib/zebra/src/test/e2e/pruning/union1.pig
+++ /dev/null
@@ -1,32 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-a2 = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-       
-b =  foreach a1  generate m1#'a' as ms1;
-c =  foreach a2 generate m1#'b' as ms2;
-
---store b  into 'u11' using org.apache.hadoop.zebra.pig.TableStorer('');    
---store c  into 'u12' using org.apache.hadoop.zebra.pig.TableStorer('');    
-
-records1 = LOAD 'u11,u12' USING org.apache.hadoop.zebra.pig.TableLoader ('ms1,a, ms2');
-dump records1;
-             
diff --git a/contrib/zebra/src/test/e2e/pruning/union2.pig b/contrib/zebra/src/test/e2e/pruning/union2.pig
deleted file mode 100644
index 9b62aa07f..000000000
--- a/contrib/zebra/src/test/e2e/pruning/union2.pig
+++ /dev/null
@@ -1,32 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-a2 = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-       
-b =  foreach a1  generate a,b,c;
-c =  foreach a2 generate d,e,f;
-
-store b  into 'u21' using org.apache.hadoop.zebra.pig.TableStorer('');    
-store c  into 'u22' using org.apache.hadoop.zebra.pig.TableStorer('');    
-
-records1 = LOAD 'u21,u22' USING org.apache.hadoop.zebra.pig.TableLoader ();
-dump records1;
-             
diff --git a/contrib/zebra/src/test/e2e/pruning/union3.pig b/contrib/zebra/src/test/e2e/pruning/union3.pig
deleted file mode 100644
index 2df794583..000000000
--- a/contrib/zebra/src/test/e2e/pruning/union3.pig
+++ /dev/null
@@ -1,31 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-
-a2 = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
-       
-c =  foreach a2 generate d,e,f;
-
-store a1  into 'u31' using org.apache.hadoop.zebra.pig.TableStorer('');    
-store c  into 'u32' using org.apache.hadoop.zebra.pig.TableStorer('');    
-
-records1 = LOAD 'u31,u32' USING org.apache.hadoop.zebra.pig.TableLoader ();
-dump records1;
-             
diff --git a/contrib/zebra/src/test/e2e/streaming/1.txt b/contrib/zebra/src/test/e2e/streaming/1.txt
deleted file mode 100644
index 8b96da90f..000000000
--- a/contrib/zebra/src/test/e2e/streaming/1.txt
+++ /dev/null
@@ -1,4 +0,0 @@
-100	100.1	100	50e+2	something	someting	(0,0)	[a#0,b#0,c#0]
-1	1.1	11	1.1	some	some	(1,1)	[a#1,b#1,c#1]
--100	-100.1	-100	-50e+2	so	so	(2,2)	[a#2,b#2,c#2]
-3	1.1	11	1.1	some	some	(3,3)	[a#3,b#3,c#3]
diff --git a/contrib/zebra/src/test/e2e/streaming/complex.txt b/contrib/zebra/src/test/e2e/streaming/complex.txt
deleted file mode 100644
index d5e5d3e9c..000000000
--- a/contrib/zebra/src/test/e2e/streaming/complex.txt
+++ /dev/null
@@ -1,4 +0,0 @@
-(0,0	[a#0,b#0,c#0]
-(1,1)	[a#1,b#1,c#1]
-(2,2)	[a#2,b#2,c#2]
-(3,3)	[a#3,b#3,c#3]
diff --git a/contrib/zebra/src/test/e2e/streaming/load.pig b/contrib/zebra/src/test/e2e/streaming/load.pig
deleted file mode 100644
index bfab7e575..000000000
--- a/contrib/zebra/src/test/e2e/streaming/load.pig
+++ /dev/null
@@ -1,29 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
--- The script converts simple.txt to simple table 
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load 'simple.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray);
-
-dump a1;
-
-store a1 into 'simple-table' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c];[d,e,f]');
-a2 = load 'simple-table' using org.apache.hadoop.zebra.pig.TableLoader();
-dump a2;
-
diff --git a/contrib/zebra/src/test/e2e/streaming/map.txt b/contrib/zebra/src/test/e2e/streaming/map.txt
deleted file mode 100644
index 4cdc210ee..000000000
--- a/contrib/zebra/src/test/e2e/streaming/map.txt
+++ /dev/null
@@ -1,2 +0,0 @@
-[aaa#100]
-[bbb#200]
diff --git a/contrib/zebra/src/test/e2e/streaming/nested_map.txt b/contrib/zebra/src/test/e2e/streaming/nested_map.txt
deleted file mode 100644
index 87e7fa550..000000000
--- a/contrib/zebra/src/test/e2e/streaming/nested_map.txt
+++ /dev/null
@@ -1,2 +0,0 @@
-[aaa#[aaa#100]]
-[bbb#[bbb#200]]
diff --git a/contrib/zebra/src/test/e2e/streaming/readme b/contrib/zebra/src/test/e2e/streaming/readme
deleted file mode 100644
index 476d2ecf2..000000000
--- a/contrib/zebra/src/test/e2e/streaming/readme
+++ /dev/null
@@ -1,12 +0,0 @@
-1. simple.txt (this txt does not have boolean type, because pig doesn't support bool type)
-
-2. run table-creator-simple.pig
-
-Use the already generated simple table (IO layer, TestSchema,located at /homes/hadoopqa/jing1234/simple-table
-3. 
-$HADOOP_HOME/bin/hadoop jar /grid/0/dev/hadoopqa/hadoop/hadoop-streaming.jar -libjars /grid/0/dev/hadoopqa/jars/pig.jar,/grid/0/dev/hadoopqa/jars/zebra.jar -D mapred.lib.table.input.projection="s1,s2,s3,s4,s5,s6" -input simple-table -output simple-stream-all-fields -mapper 'cat' -inputformat org.apache.hadoop.zebra.mapred.TableInputFormat
-4.
-$HADOOP_HOME/bin/hadoop fs -tail simple-stream-all-fields/part-00000 
-
-For details, please refer to 
-http://twiki.corp.yahoo.com/pub/Grid/Release2TestPlan/zebra_streaming_test.html
diff --git a/contrib/zebra/src/test/e2e/streaming/simple.txt b/contrib/zebra/src/test/e2e/streaming/simple.txt
deleted file mode 100644
index e0cb96d9b..000000000
--- a/contrib/zebra/src/test/e2e/streaming/simple.txt
+++ /dev/null
@@ -1,4 +0,0 @@
-100	100.1	100	50e+2	something	someting
-1	1.1	11	1.1	some	some
--100	-100.1	-100	-50e+2	so	so
-3	1.1	11	1.1	some	some
diff --git a/contrib/zebra/src/test/e2e/streaming/table-creator-complex.pig b/contrib/zebra/src/test/e2e/streaming/table-creator-complex.pig
deleted file mode 100644
index ec8629dd6..000000000
--- a/contrib/zebra/src/test/e2e/streaming/table-creator-complex.pig
+++ /dev/null
@@ -1,28 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
--- The script converts simple.txt to simple table 
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load 'complex.txt' as (r1(f1:chararray,f2:chararray),m1:map[]);
-
-dump a1;
-
-store a1 into 'complex-table' using org.apache.hadoop.zebra.pig.TableStorer('[r1];[m1]');
-
-
diff --git a/contrib/zebra/src/test/e2e/streaming/table-creator-simple.pig b/contrib/zebra/src/test/e2e/streaming/table-creator-simple.pig
deleted file mode 100644
index fa5aac1cf..000000000
--- a/contrib/zebra/src/test/e2e/streaming/table-creator-simple.pig
+++ /dev/null
@@ -1,28 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
--- The script converts simple.txt to simple table 
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-a1 = load 'simple.txt' as (a2:int, b2:float,c2:long,d2:double,e2:chararray,f2:bytearray);
-
-dump a1;
-
-store a1 into 'simple-table2' using org.apache.hadoop.zebra.pig.TableStorer('[a2,b2,c2];[d2,e2,f2]');
-
-
diff --git a/contrib/zebra/src/test/e2e/streaming/table-creator.pig b/contrib/zebra/src/test/e2e/streaming/table-creator.pig
deleted file mode 100644
index c14a363f0..000000000
--- a/contrib/zebra/src/test/e2e/streaming/table-creator.pig
+++ /dev/null
@@ -1,28 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
--- The script converts wikipedia pagecounts file into a 
--- zebra table
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-pagecounts = LOAD 'pagecounts-20090922-100000' USING PigStorage(' ') AS (project, page, count, size);
-
--- covert to a zebra table
-
-STORE pagecounts INTO 'pagecounts-table' USING org.apache.hadoop.zebra.pig.TableStorer('');
-
diff --git a/contrib/zebra/src/test/e2e/streaming/table-mapper.pl b/contrib/zebra/src/test/e2e/streaming/table-mapper.pl
deleted file mode 100644
index 9da869ccb..000000000
--- a/contrib/zebra/src/test/e2e/streaming/table-mapper.pl
+++ /dev/null
@@ -1,9 +0,0 @@
-#!/usr/bin/env perl
-
-#print total line number of the file
-$n=0;
-while ($line = <>) {
-$n++;
-print "$n). $line>\n";
-}
-print "There are ($n) lines in the file\n";
diff --git a/contrib/zebra/src/test/e2e/streaming/tuple.txt b/contrib/zebra/src/test/e2e/streaming/tuple.txt
deleted file mode 100644
index 103e5d529..000000000
--- a/contrib/zebra/src/test/e2e/streaming/tuple.txt
+++ /dev/null
@@ -1,3 +0,0 @@
-(3,8,9)	(4,5,[aaa#100])
-(1,4,7)	(3,7,[bbb#200])
-(2,5,8)	(9,5,[ccc#1255])
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/BaseTestCase.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/BaseTestCase.java
deleted file mode 100644
index a1154fa88..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/BaseTestCase.java
+++ /dev/null
@@ -1,185 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra;
-
-import java.io.File;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.Iterator;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.conf.Configured;
-import org.apache.pig.ExecType;
-import org.apache.pig.PigServer;
-import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
-import org.apache.pig.data.Tuple;
-
-public class BaseTestCase extends Configured {
-  protected static PigServer pigServer = null;
-  protected static Configuration conf = null;
-  protected static FileSystem fs = null;
-  protected static String zebraJar = null;
-  
-  // by default, we use local model for testing
-  protected static TestMode mode = TestMode.local;
-  
-  protected enum TestMode {
-    local, cluster
-  };
-  
-  /*
-   * Initialized for the test case;
-   */
-  public static void init() throws Exception {
-    if (conf == null) {
-      conf = new Configuration();
-      if (System.getProperty("mapred.job.queue.name") != null) {
-        //conf.set("mapred.job.queue.name", "grideng");
-        conf.set("mapred.job.queue.name", System.getProperty("mapred.job.queue.name"));
-      }
-    }
-    
-    if (System.getenv("hadoop.log.dir") == null) {
-      String base = new File(".").getPath(); // getAbsolutePath();
-      System.setProperty("hadoop.log.dir", new Path(base).toString() + "./logs");
-    }
-   
-    String str = System.getenv("ZebraTestMode");
-    if (str != null && str.equals("cluster")) {
-      mode = TestMode.cluster;
-    }
-
-    if (mode == TestMode.cluster) {
-      System.out.println(" get env hadoop home: " + System.getenv("HADOOP_HOME"));
-      System.out.println(" get env user name: " + System.getenv("USER"));
-
-      if (System.getenv("HADOOP_HOME") == null) {
-        System.out.println("Please set HADOOP_HOME for cluster testing mode");
-        System.exit(0);
-      }
-
-      if (System.getenv("USER") == null) {
-        System.out.println("Please set USER for cluster testing mode");
-        System.exit(0);
-      }
-
-      String zebraJar = System.getenv("HADOOP_HOME") + "/lib/zebra.jar";
-      File file = new File(zebraJar);
-      if (!file.exists()) {
-        System.out.println("Please place zebra.jar at $HADOOP_HOME/lib");
-        System.exit(0);
-      }
-      
-      pigServer = new PigServer(ExecType.MAPREDUCE, ConfigurationUtil.toProperties(conf));
-      pigServer.registerJar(zebraJar);
-      
-      Path path = new Path("/user/" + System.getenv("USER"));
-      fs = path.getFileSystem(conf);
-    } else {
-      pigServer = new PigServer(ExecType.LOCAL);
-      fs = LocalFileSystem.get(conf);
-    }
-  }
-  
-  protected static void removeDir(Path path) throws IOException {
-    String command = null;
-    
-    if (mode == TestMode.cluster) {
-      command = System.getenv("HADOOP_HOME") +"/bin/hadoop fs -rmr " + path.toString();
-    } else{
-      command = "rm -rf " + path.toString();
-    }
-    
-    Runtime runtime = Runtime.getRuntime();
-    Process proc = runtime.exec(command);
-    
-    try {
-      proc.waitFor();
-    } catch (InterruptedException e) {
-      System.err.println(e);
-    }
-  } 
-  
-  protected static Path getTableFullPath(String tableName) {
-    if (mode == TestMode.cluster) {
-      return new Path("/user/" + System.getenv("USER") + "/" + tableName);
-    } else {
-      return new Path(fs.getWorkingDirectory().toString().split(":")[1] + "/" + tableName);    
-    }
-  }
-  
-  /**
-   * Verify union output table with expected results
-   * 
-   */
-  protected int verifyTable(HashMap<Integer, ArrayList<ArrayList<Object>>> resultTable,
-      int keyColumn, int tblIdxCol, Iterator<Tuple> it) throws IOException {
-    int numbRows = 0;
-    int index = 0, rowIndex = -1, rowCount = -1, prevIndex = -1;
-    Object value;
-    boolean first = true;
-    ArrayList<ArrayList<Object>> rows = null;
-    
-    while (it.hasNext()) {
-      Tuple rowValues = it.next();
-      
-      if (first) {
-        index = (Integer) rowValues.get(tblIdxCol);
-        Assert.assertNotSame(prevIndex, index);
-        rows = resultTable.get(index);
-        rowIndex = 0;
-        rowCount = rows.size();
-        first = false;
-      }
-      value = rows.get(rowIndex++).get(keyColumn);
-      Assert.assertEquals("Table comparison error for row : " + numbRows + " - no key found for : "
-          + rowValues.get(keyColumn), value, rowValues.get(keyColumn));
-      
-      if (rowIndex == rowCount)
-      {
-        // current table is run out; start on a new table for next iteration
-        first = true;
-        prevIndex = index;
-      }
-      
-      ++numbRows;
-    }
-    return numbRows;
-  }
-  
-  public static void checkTableExists(boolean expected, String strDir) throws IOException {  
-    File theDir = null; 
-    boolean actual = false;
-
-    theDir = new File(strDir.split(":")[0]);
-    actual = fs.exists(new Path (theDir.toString()));
-    
-    System.out.println("the dir : "+ theDir.toString());
-   
-    if (actual != expected){
-      Assert.fail("dir exists or not is different from what expected.");
-    }
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestBasicTable.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestBasicTable.java
deleted file mode 100644
index 133d02d2b..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestBasicTable.java
+++ /dev/null
@@ -1,362 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.util.List;
-import java.util.Random;
-import java.util.Set;
-import java.util.TreeSet;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.tfile.RawComparable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.BasicTableStatus;
-import org.apache.hadoop.zebra.io.KeyDistribution;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Projection;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-public class TestBasicTable {
-  public static Configuration conf;
-  public static Random random;
-  public static Path rootPath;
-  public static FileSystem fs;
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException {
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-    random = new Random(System.nanoTime());
-    rootPath = new Path(System.getProperty("test.build.data",
-        "build/test/data/work-dir"));
-    fs = rootPath.getFileSystem(conf);
-  }
-
-  @BeforeClass
-  public static void tearDownOnce() throws IOException {
-  }
-
-  static BytesWritable makeRandomKey(int max) {
-    return makeKey(random.nextInt(max));
-  }
-
-  static BytesWritable makeKey(int i) {
-    return new BytesWritable(String.format("key%09d", i).getBytes());
-  }
-
-  static String makeString(String prefix, int max) {
-    return String.format("%s%09d", prefix, random.nextInt(max));
-  }
-
-  public static int createBasicTable(int parts, int rows, String strSchema, String storage, String sortColumns,
-      Path path, boolean properClose) throws IOException {
-    if (fs.exists(path)) {
-      BasicTable.drop(path, conf);
-    }
-
-    BasicTable.Writer writer = new BasicTable.Writer(path, strSchema, storage, sortColumns, null, conf);
-    writer.finish();
-
-    int total = 0;
-    Schema schema = writer.getSchema();
-    String colNames[] = schema.getColumns();
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    boolean sorted = writer.isSorted();
-    for (int i = 0; i < parts; ++i) {
-      writer = new BasicTable.Writer(path, conf);
-      TableInserter inserter = writer.getInserter(
-          String.format("part-%06d", i), true);
-      if (rows > 0) {
-        int actualRows = random.nextInt(rows) + rows / 2;
-        for (int j = 0; j < actualRows; ++j, ++total) {
-          BytesWritable key;
-          if (!sorted) {
-            key = makeRandomKey(rows * 10);
-          } else {
-            key = makeKey(total);
-          }
-          TypesUtils.resetTuple(tuple);
-          for (int k = 0; k < tuple.size(); ++k) {
-            try {
-              tuple.set(k, new DataByteArray(makeString("col-" + colNames[k], rows * 10).getBytes()));
-            } catch (ExecException e) {
-              e.printStackTrace();
-            }
-          }
-          inserter.insert(key, tuple);
-        }
-      }
-      inserter.close();
-    }
-
-    if (properClose) {
-      writer = new BasicTable.Writer(path, conf);
-      writer.close();
-      /* We can only test number of rows on sorted tables.*/
-      if (sorted) {
-        BasicTableStatus status = getStatus(path);
-        Assert.assertEquals(total, status.getRows());
-      }
-    }
-
-    return total;
-  }
-
-  static void rangeSplitBasicTable(int numSplits, int totalRows, String strProjection,
-      Path path) throws IOException, ParseException {
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(strProjection);
-    long totalBytes = reader.getStatus().getSize();
-
-    List<RangeSplit> splits = reader.rangeSplit(numSplits);
-    reader.close();
-    int total = 0;
-    for (int i = 0; i < splits.size(); ++i) {
-      reader = new BasicTable.Reader(path, conf);
-      reader.setProjection(strProjection);
-      total += doReadOnly(reader.getScanner(splits.get(i), true));
-      totalBytes -= reader.getBlockDistribution(splits.get(i)).getLength();
-    }
-    Assert.assertEquals(total, totalRows);
-    Assert.assertEquals(0L, totalBytes);
-    // TODO: verify tuples contains the right projected values
-  }
-
-  static void doRangeSplit(int[] numSplits, int totalRows, String projection, Path path)
-      throws IOException, ParseException {
-    for (int i : numSplits) {
-      if (i > 0) {
-        rangeSplitBasicTable(i, totalRows, projection, path);
-      }
-    }
-  }
-
-  static void keySplitBasicTable(int numSplits, int totalRows, String strProjection,
-      Path path) throws IOException, ParseException {
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(strProjection);
-    long totalBytes = reader.getStatus().getSize();
-    BlockDistribution lastBd = new BlockDistribution();
-    KeyDistribution keyDistri = reader.getKeyDistribution(numSplits * 10, 1, lastBd);
-    Assert.assertEquals(totalBytes, keyDistri.length()+lastBd.getLength());
-    reader.close();
-    BytesWritable[] keys = null;
-    if (keyDistri.size() >= numSplits) {
-      keyDistri.resize(lastBd);
-      Assert.assertEquals(totalBytes, keyDistri.length()+lastBd.getLength());
-      RawComparable[] rawComparables = keyDistri.getKeys();
-      keys = new BytesWritable[rawComparables.length];
-      for (int i = 0; i < keys.length; ++i) {
-        keys[i] = new BytesWritable();
-        keys[i].setSize(rawComparables[i].size());
-        System.arraycopy(rawComparables[i].buffer(),
-            rawComparables[i].offset(), keys[i].get(), 0, rawComparables[i]
-                .size());
-      }
-    } else {
-      int targetSize = Math.min(totalRows / 10, numSplits);
-      // revert to manually cooked up keys.
-      Set<Integer> keySets = new TreeSet<Integer>();
-      while (keySets.size() < targetSize) {
-        keySets.add(random.nextInt(totalRows));
-      }
-      keys = new BytesWritable[targetSize];
-      if (!keySets.isEmpty()) {
-        int j = 0;
-        for (int i : keySets.toArray(new Integer[keySets.size()])) {
-          keys[j] = makeKey(i);
-          ++j;
-        }
-      }
-    }
-
-    int total = 0;
-    for (int i = 0; i < keys.length; ++i) {
-      reader = new BasicTable.Reader(path, conf);
-      reader.setProjection(strProjection);
-      BytesWritable begin = (i == 0) ? null : keys[i - 1];
-      BytesWritable end = (i == keys.length - 1) ? null : keys[i];
-      total += doReadOnly(reader.getScanner(begin, end, true));
-    }
-    Assert.assertEquals(total, totalRows);
-  }
-
-  static void doKeySplit(int[] numSplits, int totalRows, String projection, Path path)
-      throws IOException, ParseException {
-    for (int i : numSplits) {
-      if (i > 0) {
-        keySplitBasicTable(i, totalRows, projection, path);
-      }
-    }
-  }
-
-  static BasicTableStatus getStatus(Path path) throws IOException {
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    try {
-      return reader.getStatus();
-    } finally {
-      reader.close();
-    }
-  }
-
-  static void doReadWrite(Path path, int parts, int rows, String schema,
-      String storage, String sortColumns, String projection, boolean properClose, boolean sorted)
-      throws IOException, ParseException {
-    int totalRows = createBasicTable(parts, rows, schema, storage, sortColumns, path,
-        properClose);
-    if (rows == 0) {
-      Assert.assertEquals(rows, 0);
-    }
-
-    doRangeSplit(new int[] { 1, 2, parts / 2, parts, 2 * parts }, totalRows,
-        projection, path);
-    if (sorted) {
-      doKeySplit(new int[] { 1, 2, parts / 2, parts, 2 * parts, 10 * parts },
-          totalRows, projection, path);
-    }
-  }
-
-  public void testMultiCGs() throws IOException, ParseException {
-    Path path = new Path(rootPath, "TestBasicTableMultiCGs");
-    doReadWrite(path, 2, 100, "SF_a,SF_b,SF_c,SF_d,SF_e", "[SF_a,SF_b,SF_c];[SF_d,SF_e]", null, "SF_f,SF_a,SF_c,SF_d", true, false);
-  }
-
-  public void testCornerCases() throws IOException, ParseException {
-    Path path = new Path(rootPath, "TestBasicTableCornerCases");
-    doReadWrite(path, 0, 0, "a, b, c", "", null, "a, d, c, f", false, false);
-    doReadWrite(path, 0, 0, "a, b, c", "", null, "a, d, c, f", true, false);
-    doReadWrite(path, 0, 0, "a, b, c", "", "a", "a, d, c, f", true, true);
-    doReadWrite(path, 2, 0, "a, b, c", "", null, "a, d, c, f", false, false);
-    doReadWrite(path, 2, 0, "a, b, c", "", null, "a, d, c, f", true, false);
-    doReadWrite(path, 2, 0, "a, b, c", "", "a", "a, d, c, f", true, true);
-  }
-
-  static int doReadOnly(TableScanner scanner) throws IOException, ParseException {
-    int total = 0;
-    BytesWritable key = new BytesWritable();
-    Tuple value = TypesUtils.createTuple(scanner.getSchema());
-    for (; !scanner.atEnd(); scanner.advance()) {
-      ++total;
-      switch (random.nextInt() % 4) {
-      case 0:
-        scanner.getKey(key);
-        break;
-      case 1:
-        scanner.getValue(value);
-        break;
-      case 2:
-        scanner.getKey(key);
-        scanner.getValue(value);
-        break;
-      default: // no-op.
-      }
-    }
-    scanner.close();
-
-    return total;
-  }
-
-  @Test
-  public void testNullSplits() throws IOException, ParseException {
-    Path path = new Path(rootPath, "TestBasicTableNullSplits");
-    int totalRows = createBasicTable(2, 250, "a, b, c", "", "a", path, true);
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection("a,d,c,f");
-    Assert.assertEquals(totalRows, doReadOnly(reader.getScanner(null, false)));
-    Assert.assertEquals(totalRows, doReadOnly(reader.getScanner(null, null,
-        false)));
-    reader.close();
-  }
-
-  @Test
-  public void testNegativeSplits() throws IOException, ParseException {
-    Path path = new Path(rootPath, "TestNegativeSplits");
-    int totalRows = createBasicTable(2, 250, "a, b, c", "", "", path, true);
-    rangeSplitBasicTable(-1, totalRows, "a,d,c,f", path);
-  }
-
-  @Test
-  public void testMetaBlocks() throws IOException, ParseException {
-    Path path = new Path(rootPath, "TestBasicTableMetaBlocks");
-    createBasicTable(3, 100, "a, b, c", "", null, path, false);
-    BasicTable.Writer writer = new BasicTable.Writer(path, conf);
-    BytesWritable meta1 = makeKey(1234);
-    BytesWritable meta2 = makeKey(9876);
-    DataOutputStream dos = writer.createMetaBlock("testMetaBlocks.meta1");
-    try {
-      meta1.write(dos);
-    } finally {
-      dos.close();
-    }
-    dos = writer.createMetaBlock("testMetaBlocks.meta2");
-    try {
-      meta2.write(dos);
-    } finally {
-      dos.close();
-    }
-    writer.close();
-
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection("a,d,c,f");
-    BytesWritable tmp = new BytesWritable();
-    DataInputStream dis = reader.getMetaBlock("testMetaBlocks.meta1");
-    try {
-      tmp.readFields(dis);
-      Assert.assertTrue(tmp.compareTo(meta1) == 0);
-    } finally {
-      dis.close();
-    }
-
-    dis = reader.getMetaBlock("testMetaBlocks.meta2");
-    try {
-      tmp.readFields(dis);
-      Assert.assertTrue(tmp.compareTo(meta2) == 0);
-    } finally {
-      dis.close();
-    }
-    reader.close();
-  }
-
-  @Test
-  public void testNormalCases() throws IOException, ParseException {
-    Path path = new Path(rootPath, "TestBasicTableNormal");
-    doReadWrite(path, 2, 250, "a, b, c", "", null, "a, d, c, f", true, false);
-    doReadWrite(path, 2, 250, "a, b, c", "", null, "a, d, c, f", true, false);
-    doReadWrite(path, 2, 250, "a, b, c", "", "a", "a, d, c, f", true, true);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestBasicTableMapSplits.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestBasicTableMapSplits.java
deleted file mode 100644
index c112dfefb..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestBasicTableMapSplits.java
+++ /dev/null
@@ -1,242 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.IOException;
-import java.io.PrintStream;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * 
- * Test projections on complicated column types.
- * 
- */
-public class TestBasicTableMapSplits {
-  final static String STR_SCHEMA = "f1:bool, r:record(f11:int, f12:long), m:map(string), c:collection(record(f13:double, f14:double, f15:bytes))";
-  final static String STR_STORAGE = "[r.f12, f1, m#{b}]; [m#{a}, r.f11]";
-  private static Configuration conf;
-  private static Path path;
-  private static FileSystem fs;
-
-  @BeforeClass
-  public static void setUp() throws IOException {
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), "TestBasicTableMapSplits");
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-
-    BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-        STR_STORAGE, conf);
-    writer.finish();
-
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-    TypesUtils.resetTuple(tuple);
-
-    tuple.set(0, true);
-
-    Tuple tupRecord;
-    try {
-      tupRecord = TypesUtils.createTuple(schema.getColumnSchema("r")
-          .getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-    tupRecord.set(0, 1);
-    tupRecord.set(1, 1001L);
-    tuple.set(1, tupRecord);
-
-    Map<String, String> map = new HashMap<String, String>();
-    map.put("a", "x");
-    map.put("b", "y");
-    map.put("c", "z");
-    tuple.set(2, map);
-
-    DataBag bagColl = TypesUtils.createBag();
-    Schema schColl = schema.getColumn(3).getSchema().getColumn(0).getSchema();
-    Tuple tupColl1 = TypesUtils.createTuple(schColl);
-    Tuple tupColl2 = TypesUtils.createTuple(schColl);
-    byte[] abs1 = new byte[3];
-    byte[] abs2 = new byte[4];
-    tupColl1.set(0, 3.1415926);
-    tupColl1.set(1, 1.6);
-    abs1[0] = 11;
-    abs1[1] = 12;
-    abs1[2] = 13;
-    tupColl1.set(2, new DataByteArray(abs1));
-    bagColl.add(tupColl1);
-    tupColl2.set(0, 123.456789);
-    tupColl2.set(1, 100);
-    abs2[0] = 21;
-    abs2[1] = 22;
-    abs2[2] = 23;
-    abs2[3] = 24;
-    tupColl2.set(2, new DataByteArray(abs2));
-    bagColl.add(tupColl2);
-    tuple.set(3, bagColl);
-
-    int row = 0;
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-    inserter.close();
-    writer1.finish();
-
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-  }
-
-  public void testDescribse() throws IOException {
-
-    BasicTable.dumpInfo(path.toString(), System.out, conf);
-
-  }
-
-  @Test
-  public void test1() throws IOException, ParseException {
-    String projection = new String("r.f12, f1");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    // long totalBytes = reader.getStatus().getSize();
-
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    reader.close();
-    reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple value = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(value);
-
-    Assert.assertEquals(1001L, value.get(0));
-    Assert.assertEquals(true, value.get(1));
-    reader.close();
-  }
-
-  @Test
-  public void testStitch1() throws IOException, ParseException {
-    String projection = new String("f1, m#{a|b}, r, m#{c}");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    // long totalBytes = reader.getStatus().getSize();
-
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    reader.close();
-    reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple value = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(value);
-
-    Tuple recordTuple = (Tuple) value.get(2);
-    Assert.assertEquals(1, recordTuple.get(0));
-    Assert.assertEquals(1001L, recordTuple.get(1));
-    Assert.assertEquals(true, value.get(0));
-
-    HashMap<String, Object> mapval = (HashMap<String, Object>) value.get(1);
-    Assert.assertEquals("x", mapval.get("a"));
-    Assert.assertEquals("y", mapval.get("b"));
-    Assert.assertEquals(null, mapval.get("c"));
-    mapval = (HashMap<String, Object>) value.get(3);
-    Assert.assertEquals("z", mapval.get("c"));
-    Assert.assertEquals(null, mapval.get("a"));
-    Assert.assertEquals(null, mapval.get("b"));
-    reader.close();
-  }
-
-  @Test
-  public void testStitch2() throws IOException, ParseException {
-    String projection = new String("f1, m#{a}, r, m#{c}, m");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    // long totalBytes = reader.getStatus().getSize();
-
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    reader.close();
-    reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple value = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(value);
-
-    Tuple recordTuple = (Tuple) value.get(2);
-    Assert.assertEquals(1, recordTuple.get(0));
-    Assert.assertEquals(1001L, recordTuple.get(1));
-    Assert.assertEquals(true, value.get(0));
-
-    Map<String, Object> mapval = (Map<String, Object>) value.get(1);
-    Assert.assertEquals("x", mapval.get("a"));
-    mapval = (Map<String, Object>) value.get(3);
-    Assert.assertEquals("z", mapval.get("c"));
-
-    Map<String, Object> map = (Map<String, Object>) (value.get(4));
-    Assert.assertEquals("x", (String) map.get("a"));
-    Assert.assertEquals("y", (String) map.get("b"));
-    Assert.assertEquals("z", (String) map.get("c"));
-    reader.close();
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestBasicTableProjections.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestBasicTableProjections.java
deleted file mode 100644
index d5e10f714..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestBasicTableProjections.java
+++ /dev/null
@@ -1,152 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.IOException;
-import java.util.List;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Projection;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-public class TestBasicTableProjections {
-  private static Configuration conf;
-  private static Path path;
-  private static FileSystem fs;
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException {
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), "TestBasicTableProjections");
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-
-    BasicTable.Writer writer = new BasicTable.Writer(path, "a,b,c,d,e,f,g",
-        "[a,b,c];[d,e,f,g]", conf);
-    writer.finish();
-
-    Schema schema = writer.getSchema();
-    // String[] colNames = schema.getColumns();
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    // BytesWritable key;
-    int parts = 2;
-    for (int part = 0; part < parts; part++) {
-      writer = new BasicTable.Writer(path, conf);
-      TableInserter inserter = writer.getInserter("part" + part, true);
-      TypesUtils.resetTuple(tuple);
-      for (int row = 0; row < 2; row++) {
-        try {
-          for (int nx = 0; nx < tuple.size(); nx++)
-            tuple.set(nx, new DataByteArray(String.format("%c%d%d", 'a' + nx, part + 1, row + 1).getBytes()));
-        } catch (ExecException e) {
-          e.printStackTrace();
-        }
-        inserter.insert(new BytesWritable(String.format("k%d%d", part + 1,
-            row + 1).getBytes()), tuple);
-      }
-      inserter.close();
-      writer.finish();
-    }
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-  }
-
-  @Test
-  public void test1() throws IOException, ParseException {
-    String projection = new String("f,a");
-    Configuration conf = new Configuration();
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    // long totalBytes = reader.getStatus().getSize();
-
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    reader.close();
-    reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple value = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(value);
-    Assert.assertEquals("f11", value.get(0).toString());
-    Assert.assertEquals("a11", value.get(1).toString());
-    try {
-      value.get(2);
-      Assert.fail("Failed to catch out of boundary exceptions.");
-    } catch (IndexOutOfBoundsException e) {
-      // no op, expecting out of bounds exceptions
-    }
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    TypesUtils.resetTuple(value);
-    scanner.getValue(value);
-    Assert.assertEquals("f12", value.get(0).toString());
-    Assert.assertEquals("a12", value.get(1).toString());
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k21".getBytes()));
-    TypesUtils.resetTuple(value);
-    scanner.getValue(value);
-    Assert.assertEquals("f21", value.get(0).toString());
-    Assert.assertEquals("a21", value.get(1).toString());
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k22".getBytes()));
-    TypesUtils.resetTuple(value);
-    scanner.getValue(value);
-    Assert.assertEquals("f22", value.get(0).toString());
-    Assert.assertEquals("a22", value.get(1).toString());
-  }
-
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestBasicTableSplits.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestBasicTableSplits.java
deleted file mode 100644
index 6691e1c31..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestBasicTableSplits.java
+++ /dev/null
@@ -1,238 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * 
- * Test projections on complicated column types.
- * 
- */
-public class TestBasicTableSplits {
-  final static String STR_SCHEMA = "f1:bool, r:record(f11:int, f12:long), m:map(string), c:collection(record(f13:double, f14:double, f15:bytes))";
-  // TODO: try map hash split later
-  final static String STR_STORAGE = "[r.f12, f1]; [m]";
-  private static Configuration conf;
-  private static Path path;
-  private static FileSystem fs;
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException {
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), "TestBasicTableSplits");
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-
-    BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-        STR_STORAGE, conf);
-    writer.finish();
-
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-    TypesUtils.resetTuple(tuple);
-
-    tuple.set(0, true);
-
-    Tuple tupRecord;
-    try {
-      tupRecord = TypesUtils.createTuple(schema.getColumnSchema("r")
-          .getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    // row 1
-    tupRecord.set(0, 1);
-    tupRecord.set(1, 1001L);
-    tuple.set(1, tupRecord);
-
-    Map<String, String> map = new HashMap<String, String>();
-    map.put("a", "x");
-    map.put("b", "y");
-    map.put("c", "z");
-    tuple.set(2, map);
-
-    DataBag bagColl = TypesUtils.createBag();
-    Schema schColl = schema.getColumn(3).getSchema().getColumn(0).getSchema();
-    Tuple tupColl1 = TypesUtils.createTuple(schColl);
-    Tuple tupColl2 = TypesUtils.createTuple(schColl);
-    byte[] abs1 = new byte[3];
-    byte[] abs2 = new byte[4];
-    tupColl1.set(0, 3.1415926);
-    tupColl1.set(1, 1.6);
-    abs1[0] = 11;
-    abs1[1] = 12;
-    abs1[2] = 13;
-    tupColl1.set(2, new DataByteArray(abs1));
-    bagColl.add(tupColl1);
-    tupColl2.set(0, 123.456789);
-    tupColl2.set(1, 100);
-    abs2[0] = 21;
-    abs2[1] = 22;
-    abs2[2] = 23;
-    abs2[3] = 24;
-    tupColl2.set(2, new DataByteArray(abs2));
-    bagColl.add(tupColl2);
-    tuple.set(3, bagColl);
-
-    int row = 0;
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // row 2
-    row++;
-    TypesUtils.resetTuple(tuple);
-    TypesUtils.resetTuple(tupRecord);
-    map.clear();
-    tuple.set(0, false);
-    tupRecord.set(0, 2);
-    tupRecord.set(1, 1002L);
-    tuple.set(1, tupRecord);
-    map.put("boy", "girl");
-    map.put("adam", "amy");
-    map.put("bob", "becky");
-    map.put("carl", "cathy");
-    tuple.set(2, map);
-    bagColl.clear();
-    TypesUtils.resetTuple(tupColl1);
-    TypesUtils.resetTuple(tupColl2);
-    tupColl1.set(0, 7654.321);
-    tupColl1.set(1, 0.0001);
-    abs1[0] = 31;
-    abs1[1] = 32;
-    abs1[2] = 33;
-    tupColl1.set(2, new DataByteArray(abs1));
-    bagColl.add(tupColl1);
-    tupColl2.set(0, 0.123456789);
-    tupColl2.set(1, 0.3333);
-    abs2[0] = 41;
-    abs2[1] = 42;
-    abs2[2] = 43;
-    abs2[3] = 44;
-    tupColl2.set(2, new DataByteArray(abs2));
-    bagColl.add(tupColl2);
-    tuple.set(3, bagColl);
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    inserter.close();
-    writer1.finish();
-
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-  }
-
-  @Test
-  public void test1() throws IOException, ParseException {
-    String projection = new String("r.f12, f1");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    // long totalBytes = reader.getStatus().getSize();
-
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    reader.close();
-    reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple value = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(value);
-
-    Assert.assertEquals(1001L, value.get(0));
-    Assert.assertEquals(true, value.get(1));
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(value);
-    Assert.assertEquals(1002L, value.get(0));
-    Assert.assertEquals(false, value.get(1));
-
-    reader.close();
-  }
-
-  @Test
-  public void testStitch() throws IOException, ParseException {
-    String projection = new String("f1, r");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    // long totalBytes = reader.getStatus().getSize();
-
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    reader.close();
-    reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple value = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(value);
-
-    Tuple recordTuple = (Tuple) value.get(1);
-    Assert.assertEquals(1, recordTuple.get(0));
-    Assert.assertEquals(1001L, recordTuple.get(1));
-    Assert.assertEquals(true, value.get(0));
-    reader.close();
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestCheckin.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestCheckin.java
deleted file mode 100644
index 2596e3a30..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestCheckin.java
+++ /dev/null
@@ -1,55 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.io;
-
-import org.junit.runner.RunWith;
-import org.junit.runners.Suite;
-
-@RunWith(Suite.class)
-@Suite.SuiteClasses({
-  TestBasicTable.class,
-  TestBasicTableMapSplits.class,
-  TestBasicTableProjections.class,
-  TestBasicTableSplits.class,
-  TestCollection.class,
-  TestColumnGroupInserters.class,
-  TestColumnGroup.class,
-  TestColumnGroupName1.class,
-  TestColumnGroupOpen.class,
-  TestColumnGroupProjections.class,
-  TestColumnGroupReaders.class,
-  TestColumnGroupSchemas.class,
-  TestColumnGroupSplits.class,
-  TestDropColumnGroup.class,
-  TestMap.class,
-  TestMapOfRecord.class,
-  TestMixedType1.class,
-  TestNegative.class,
-  TestRecord2Map.class,
-  TestRecord3Map.class,
-  TestRecord.class,
-  TestRecordMap.class,
-  TestSchema.class,
-  TestWrite.class
-})
-
-public class TestCheckin {
-  // the class remains completely empty, 
-  // being used only as a holder for the above annotations
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestCollection.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestCollection.java
deleted file mode 100644
index 1e17109e7..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestCollection.java
+++ /dev/null
@@ -1,717 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.IOException;
-import java.util.Iterator;
-import java.util.List;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * 
- * Test projections on complicated column types.
- * 
- */
-public class TestCollection {
-  final static String STR_SCHEMA = "c:collection(record(a:double, b:double, c:bytes)),c2:collection(record(r1:record(f1:int, f2:string), d:string)),c3:collection(record(e:int,f:bool))";
-
-  final static String STR_STORAGE = "[c]";
-  private static Configuration conf;
-  private static Path path;
-  private static FileSystem fs;
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException {
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), "TestCollection");
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-        STR_STORAGE, conf);
-
-    writer.finish();
-
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-    TypesUtils.resetTuple(tuple);
-    DataBag bag1 = TypesUtils.createBag();
-    Schema schColl = schema.getColumn(0).getSchema().getColumn(0).getSchema();
-    Tuple tupColl1 = TypesUtils.createTuple(schColl);
-    Tuple tupColl2 = TypesUtils.createTuple(schColl);
-
-    DataBag bag2 = TypesUtils.createBag();
-    Schema schColl2 = schema.getColumn(1).getSchema().getColumn(0).getSchema();
-    Tuple tupColl2_1 = TypesUtils.createTuple(schColl2);
-    Tuple tupColl2_2 = TypesUtils.createTuple(schColl2);
-    Tuple collRecord1;
-    try {
-      collRecord1 = TypesUtils.createTuple(new Schema("f1:int, f2:string"));
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-    Tuple collRecord2;
-    try {
-      collRecord2 = TypesUtils.createTuple(new Schema("f1:int, f2:string"));
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    // c3:collection(c3_1:collection(e:int,f:bool))
-    DataBag bag3 = TypesUtils.createBag();
-    DataBag bag3_1 = TypesUtils.createBag();
-    DataBag bag3_2 = TypesUtils.createBag();
-
-    Tuple tupColl3_1 = null;
-    try {
-      tupColl3_1 = TypesUtils.createTuple(new Schema("e:int,f:bool"));
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-    Tuple tupColl3_2;
-    try {
-      tupColl3_2 = TypesUtils.createTuple(new Schema("e:int,f:bool"));
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    Tuple tupColl3_3 = null;
-    try {
-      tupColl3_3 = TypesUtils.createTuple(new Schema("e:int,f:bool"));
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-    Tuple tupColl3_4;
-    try {
-      tupColl3_4 = TypesUtils.createTuple(new Schema("e:int,f:bool"));
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    byte[] abs1 = new byte[3];
-    byte[] abs2 = new byte[4];
-    tupColl1.set(0, 3.1415926);
-    tupColl1.set(1, 1.6);
-    abs1[0] = 11;
-    abs1[1] = 12;
-    abs1[2] = 13;
-    tupColl1.set(2, new DataByteArray(abs1));
-    bag1.add(tupColl1);
-    tupColl2.set(0, 123.456789);
-    tupColl2.set(1, 100);
-    abs2[0] = 21;
-    abs2[1] = 22;
-    abs2[2] = 23;
-    abs2[3] = 24;
-    tupColl2.set(2, new DataByteArray(abs2));
-    bag1.add(tupColl2);
-    tuple.set(0, bag1);
-
-    collRecord1.set(0, 1);
-    collRecord1.set(1, "record1_string1");
-    tupColl2_1.set(0, collRecord1);
-    tupColl2_1.set(1, "hello1");
-    bag2.add(tupColl2_1);
-
-    collRecord2.set(0, 2);
-    collRecord2.set(1, "record2_string1");
-    tupColl2_2.set(0, collRecord2);
-    tupColl2_2.set(1, "hello2");
-    bag2.add(tupColl2_2);
-    tuple.set(1, bag2);
-
-    TypesUtils.resetTuple(tupColl3_1);
-    TypesUtils.resetTuple(tupColl3_2);
-    tupColl3_1.set(0, 1);
-    tupColl3_1.set(1, true);
-    tupColl3_2.set(0, 2);
-    tupColl3_2.set(1, false);
-    bag3_1.add(tupColl3_1);
-    bag3_1.add(tupColl3_2);
-    bag3.addAll(bag3_1);
-
-    tupColl3_3.set(0, 3);
-    tupColl3_3.set(1, true);
-    tupColl3_4.set(0, 4);
-    tupColl3_4.set(1, false);
-    bag3_2.add(tupColl3_3);
-    bag3_2.add(tupColl3_4);
-    bag3.addAll(bag3_2);
-    tuple.set(2, bag3);
-
-    int row = 0;
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1).getBytes()), tuple);
-
-    row++;
-
-    bag1.clear();
-    bag2.clear();
-    bag3.clear();
-    bag3_1.clear();
-    bag3_2.clear();
-    TypesUtils.resetTuple(tupColl1);
-    TypesUtils.resetTuple(tupColl2);
-    TypesUtils.resetTuple(tupColl2_1);
-    TypesUtils.resetTuple(tupColl2_2);
-    TypesUtils.resetTuple(collRecord1);
-    TypesUtils.resetTuple(collRecord2);
-    TypesUtils.resetTuple(tupColl3_1);
-    TypesUtils.resetTuple(tupColl3_2);
-    TypesUtils.resetTuple(tupColl3_3);
-    TypesUtils.resetTuple(tupColl3_4);
-
-    tupColl1.set(0, 7654.321);
-    tupColl1.set(1, 0.0001);
-    abs1[0] = 31;
-    abs1[1] = 32;
-    abs1[2] = 33;
-    tupColl1.set(2, new DataByteArray(abs1));
-    bag1.add(tupColl1);
-    tupColl2.set(0, 0.123456789);
-    tupColl2.set(1, 0.3333);
-    abs2[0] = 41;
-    abs2[1] = 42;
-    abs2[2] = 43;
-    abs2[3] = 44;
-    tupColl2.set(2, new DataByteArray(abs2));
-    bag1.add(tupColl2);
-    tuple.set(0, bag1);
-
-    collRecord1.set(0, 3);
-    collRecord1.set(1, "record1_string2");
-    tupColl2_1.set(0, collRecord1);
-    tupColl2_1.set(1, "hello1_2");
-    bag2.add(tupColl2_1);
-
-    collRecord2.set(0, 4);
-    collRecord2.set(1, "record2_string2");
-    tupColl2_2.set(0, collRecord2);
-    tupColl2_2.set(1, "hello2_2");
-    bag2.add(tupColl2_2);
-    tuple.set(1, bag2);
-
-    tupColl3_1.set(0, 5);
-    tupColl3_1.set(1, true);
-    tupColl3_2.set(0, 6);
-    tupColl3_2.set(1, false);
-    bag3_1.add(tupColl3_1);
-    bag3_1.add(tupColl3_2);
-    bag3.addAll(bag3_1);
-
-    tupColl3_3.set(0, 7);
-    tupColl3_3.set(1, true);
-    tupColl3_4.set(0, 8);
-    tupColl3_4.set(1, false);
-    bag3_2.add(tupColl3_3);
-    bag3_2.add(tupColl3_4);
-    bag3.addAll(bag3_2);
-    tuple.set(2, bag3);
-
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    inserter.close();
-    writer1.finish();
-
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDown() throws IOException {
-    BasicTable.drop(path, conf);
-  }
-
-  // read one collection
-  // final static String STR_SCHEMA =
-  // "c:collection(a:double, b:float, c:bytes),c2:collection(r1:record(f1:int, f2:string), d:string),c3:collection(c3_1:collection(e:int,f:bool))";
-  @Test
-  public void testRead1() throws IOException, ParseException {
-    String projection = new String("c");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    // Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    System.out.println("test read 1: row: " + RowValue.toString());
-    // test read 1: row: ({(3.1415926,1.6, ),(123.456789,100,)})
-    Iterator<Tuple> it = ((DataBag) RowValue.get(0)).iterator();
-    int list = 0;
-    while (it.hasNext()) {
-      Tuple cur = it.next();
-      System.out.println(cur.get(0));
-      list++;
-      if (list == 1) {
-        Assert.assertEquals(3.1415926, cur.get(0));
-        Assert.assertEquals(1.6, cur.get(1));
-        System.out
-            .println("byte 0: " + ((DataByteArray) cur.get(2)).toString());
-
-      }
-      if (list == 2) {
-        Assert.assertEquals(123.456789, cur.get(0));
-        Assert.assertEquals(100, cur.get(1));
-        // Assert.assertEquals(3.1415926, cur.get(2));
-      }
-    }
-    scanner.advance();
-    scanner.getValue(RowValue);
-    Iterator<Tuple> it2 = ((DataBag) RowValue.get(0)).iterator();
-    int list2 = 0;
-    while (it2.hasNext()) {
-      Tuple cur = it2.next();
-      System.out.println(cur.get(0));
-      list2++;
-      if (list2 == 1) {
-        Assert.assertEquals(7654.321, cur.get(0));
-        Assert.assertEquals(0.0001, cur.get(1));
-        // Assert.assertEquals(3.1415926, cur.get(2));
-      }
-      if (list2 == 2) {
-        Assert.assertEquals(0.123456789, cur.get(0));
-        Assert.assertEquals(0.3333, cur.get(1));
-        // Assert.assertEquals(3.1415926, cur.get(2));
-      }
-    }
-
-    reader.close();
-  }
-
-  // read second collection
-  // final static String STR_SCHEMA =
-  // "c:collection(a:double, b:float, c:bytes),c2:collection(r1:record(f1:int, f2:string), d:string),c3:collection(c3_1:collection(e:int,f:bool))";
-  @Test
-  public void testRead2() throws IOException, ParseException {
-    String projection = new String("c2");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    // Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    System.out.println("test read 2:row: " + RowValue.toString());
-    // test read 2:row:
-    // ({((1,record1_string1),hello1),((2,record2_string1),hello2)})
-    Iterator<Tuple> it = ((DataBag) RowValue.get(0)).iterator();
-    int list = 0;
-    while (it.hasNext()) {
-      Tuple cur = it.next();
-      System.out.println(cur.get(0)); // (1,record1_string1)
-      list++;
-      if (list == 1) {
-        Assert.assertEquals(1, ((Tuple) cur.get(0)).get(0));
-        Assert.assertEquals("record1_string1", ((Tuple) cur.get(0)).get(1));
-        Assert.assertEquals("hello1", cur.get(1));
-
-      }
-      if (list == 2) {
-        Assert.assertEquals(2, ((Tuple) cur.get(0)).get(0));
-        Assert.assertEquals("record2_string1", ((Tuple) cur.get(0)).get(1));
-        Assert.assertEquals("hello2", cur.get(1));
-
-      }
-    }
-    scanner.advance();
-    scanner.getValue(RowValue);
-    Iterator<Tuple> it2 = ((DataBag) RowValue.get(0)).iterator();
-    int list2 = 0;
-    while (it2.hasNext()) {
-      Tuple cur = it2.next();
-      System.out.println(cur.get(0));
-      list2++;
-      if (list2 == 1) {
-        Assert.assertEquals(3, ((Tuple) cur.get(0)).get(0));
-        Assert.assertEquals("record1_string2", ((Tuple) cur.get(0)).get(1));
-        Assert.assertEquals("hello1_2", cur.get(1));
-      }
-      if (list2 == 2) {
-        Assert.assertEquals(4, ((Tuple) cur.get(0)).get(0));
-        Assert.assertEquals("record2_string2", ((Tuple) cur.get(0)).get(1));
-        Assert.assertEquals("hello2_2", cur.get(1));
-      }
-    }
-
-    reader.close();
-  }
-
-  // read 3rd column
-  // final static String STR_SCHEMA =
-  // "c:collection(a:double, b:float, c:bytes),c2:collection(r1:record(f1:int, f2:string), d:string),c3:collection(c3_1:collection(e:int,f:bool))";
-  @Test
-  public void testRead3() throws IOException, ParseException {
-    String projection = new String("c3");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    System.out.println("test record 3: row: " + RowValue.toString());
-    // test record 3: row: ({(1,true),(2,false),(3,true),(4,false)})
-    Iterator<Tuple> it = ((DataBag) RowValue.get(0)).iterator();
-    int list = 0;
-    while (it.hasNext()) {
-      Tuple cur = it.next();
-      System.out.println(cur.get(0)); // 3
-      list++;
-      if (list == 1) {
-        Assert.assertEquals(1, cur.get(0));
-        Assert.assertEquals(true, cur.get(1));
-      }
-      if (list == 2) {
-        Assert.assertEquals(2, cur.get(0));
-        Assert.assertEquals(false, cur.get(1));
-      }
-      if (list == 3) {
-        Assert.assertEquals(3, cur.get(0));
-        Assert.assertEquals(true, cur.get(1));
-      }
-      if (list == 4) {
-        Assert.assertEquals(4, cur.get(0));
-        Assert.assertEquals(false, cur.get(1));
-      }
-    }
-    scanner.advance();
-    scanner.getValue(RowValue);
-    System.out.println("row: " + RowValue.toString());
-    // row: ({(5,true),(6,false),(7,true),(8,false)})
-    Iterator<Tuple> it2 = ((DataBag) RowValue.get(0)).iterator();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    int list2 = 0;
-    while (it2.hasNext()) {
-      Tuple cur = it2.next();
-      System.out.println(cur.get(0));
-      list2++;
-      if (list2 == 1) {
-        Assert.assertEquals(5, cur.get(0));
-        Assert.assertEquals(true, cur.get(1));
-      }
-      if (list2 == 2) {
-        Assert.assertEquals(6, cur.get(0));
-        Assert.assertEquals(false, cur.get(1));
-      }
-      if (list2 == 3) {
-        Assert.assertEquals(7, cur.get(0));
-        Assert.assertEquals(true, cur.get(1));
-      }
-      if (list2 == 4) {
-        Assert.assertEquals(8, cur.get(0));
-        Assert.assertEquals(false, cur.get(1));
-      }
-    }
-
-    reader.close();
-  }
-
-  // Negative none exist column
-  @Test
-  public void xtestReadNeg1() throws IOException, ParseException {
-    String projection = new String("d");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    // Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    System.out.println("row: " + RowValue.toString());
-    System.out.println("row1: " + RowValue.get(0));
-    Assert.assertEquals(null, RowValue.get(0));
-    Assert.assertEquals(1, RowValue.size());
-    reader.close();
-  }
-
-  // Negative test case. Projection on fields in collection is not supported.
-  @Test
-  public void testRead5() throws IOException {
-	  try {
-		  String projection = new String("c.a");
-		  BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-		  reader.setProjection(projection);
-	  } catch(ParseException ex) {
-		  System.out.println( "caught expected exception: " + ex );
-		  return;
-	  }
-	  
-	  Assert.fail( "Test case failure: projection on collection field or record field is not supported." );
-  }
-
-  // read, should support project to 3rd level TODO: construct scanner failed
-  // final static String STR_SCHEMA =
-  // "c:collection(a:double, b:float, c:bytes),c2:collection(r1:record(f1:int, f2:string), d:string),c3:collection(c3_1:collection(e:int,f:bool))";
-
-  public void xtestRead7() throws IOException, ParseException {
-    String projection = new String("c2.r1.f1");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    scanner = reader.getScanner(splits.get(0), true);
-
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    // Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    System.out.println("row: " + RowValue.toString());
-    Iterator<Tuple> it = ((DataBag) RowValue.get(0)).iterator();
-    int list = 0;
-    while (it.hasNext()) {
-      Tuple cur = it.next();
-      System.out.println(cur.get(0));
-      list++;
-      if (list == 1) {
-        Assert.assertEquals(1, ((Tuple) cur.get(0)).get(0));
-        Assert.assertEquals("record1_string1", ((Tuple) cur.get(0)).get(1));
-        try {
-          cur.get(1);
-          Assert.fail("Should throw index out of bounds exception");
-        } catch (Exception e) {
-          System.out.println(e);
-        }
-
-      }
-      if (list == 2) {
-        Assert.assertEquals(2, ((Tuple) cur.get(0)).get(0));
-        Assert.assertEquals("record2_string1", ((Tuple) cur.get(0)).get(1));
-        try {
-          cur.get(1);
-          Assert.fail("Should throw index out of bounds exception");
-        } catch (Exception e) {
-          System.out.println(e);
-        }
-      }
-    }
-    scanner.advance();
-    scanner.getValue(RowValue);
-    Iterator<Tuple> it2 = ((DataBag) RowValue.get(0)).iterator();
-    int list2 = 0;
-    while (it2.hasNext()) {
-      Tuple cur = it2.next();
-      System.out.println(cur.get(0));
-      list2++;
-      if (list2 == 1) {
-        Assert.assertEquals(3, ((Tuple) cur.get(0)).get(0));
-        Assert.assertEquals("record1_string2", ((Tuple) cur.get(0)).get(1));
-        try {
-          cur.get(1);
-          Assert.fail("Should throw index out of bounds exception");
-        } catch (Exception e) {
-          System.out.println(e);
-        }
-      }
-      if (list2 == 2) {
-        Assert.assertEquals(4, ((Tuple) cur.get(0)).get(0));
-        Assert.assertEquals("record2_string2", ((Tuple) cur.get(0)).get(1));
-        try {
-          cur.get(1);
-          Assert.fail("Should throw index out of bounds exception");
-        } catch (Exception e) {
-          System.out.println(e);
-        }
-      }
-    }
-
-    reader.close();
-  }
-
-  // read, should support project to 3rd level TODO: construct scanner failed
-  // final static String STR_SCHEMA =
-  // "c:collection(a:double, b:float, c:bytes),c2:collection(r1:record(f1:int, f2:string), d:string),c3:collection(c3_1:collection(e:int,f:bool))";
-
-  public void xtestRead8() throws IOException, ParseException {
-    String projection = new String("c3.c3_1.e");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = null;
-    scanner = reader.getScanner(splits.get(0), true);
-
-    scanner = reader.getScanner(splits.get(0), true);
-
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    // Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    System.out.println("row: " + RowValue.toString());
-    Iterator<Tuple> it = ((DataBag) RowValue.get(0)).iterator();
-    int list = 0;
-    while (it.hasNext()) {
-      Tuple cur = it.next();
-      System.out.println(cur.get(0));
-      list++;
-      if (list == 1) {
-        Assert.assertEquals(1, ((Tuple) cur.get(0)).get(0));
-        Assert.assertEquals("record1_string1", ((Tuple) cur.get(0)).get(1));
-        try {
-          cur.get(1);
-          Assert.fail("Should throw index out of bounds exception");
-        } catch (Exception e) {
-          System.out.println(e);
-        }
-
-      }
-      if (list == 2) {
-        Assert.assertEquals(2, ((Tuple) cur.get(0)).get(0));
-        Assert.assertEquals("record2_string1", ((Tuple) cur.get(0)).get(1));
-        try {
-          cur.get(1);
-          Assert.fail("Should throw index out of bounds exception");
-        } catch (Exception e) {
-          System.out.println(e);
-        }
-      }
-    }
-    scanner.advance();
-    scanner.getValue(RowValue);
-    Iterator<Tuple> it2 = ((DataBag) RowValue.get(0)).iterator();
-    int list2 = 0;
-    while (it2.hasNext()) {
-      Tuple cur = it2.next();
-      System.out.println(cur.get(0));
-      list2++;
-      if (list2 == 1) {
-        Assert.assertEquals(3, ((Tuple) cur.get(0)).get(0));
-        Assert.assertEquals("record1_string2", ((Tuple) cur.get(0)).get(1));
-        try {
-          cur.get(1);
-          Assert.fail("Should throw index out of bounds exception");
-        } catch (Exception e) {
-          System.out.println(e);
-        }
-      }
-      if (list2 == 2) {
-        Assert.assertEquals(4, ((Tuple) cur.get(0)).get(0));
-        Assert.assertEquals("record2_string2", ((Tuple) cur.get(0)).get(1));
-        try {
-          cur.get(1);
-          Assert.fail("Should throw index out of bounds exception");
-        } catch (Exception e) {
-          System.out.println(e);
-        }
-      }
-    }
-
-    reader.close();
-  }
-
-  // Negative should not support 2nd level collection split
-  @Test
-  public void testSplit1() throws IOException, ParseException {
-    String STR_SCHEMA = "c:collection(a:double, b:double, c:bytes),c2:collection(r1:record(f1:int, f2:string), d:string),c3:collection(c3_1:collection(e:int,f:bool))";
-    String STR_STORAGE = "[c.a]";
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), this.getClass().getSimpleName());
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    try {
-      BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-          STR_STORAGE, conf);
-      Assert.fail("should throw exception");
-    } catch (Exception e) {
-      System.out.println(e);
-    }
-  }
-
-  // Negative should not support none_existent column split
-  @Test
-  public void testSplit2() throws IOException, ParseException {
-    String STR_SCHEMA = "c:collection(a:double, b:double, c:bytes),c2:collection(r1:record(f1:int, f2:string), d:string),c3:collection(c3_1:collection(e:int,f:bool))";
-    String STR_STORAGE = "[d]";
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), this.getClass().getSimpleName());
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    try {
-      BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-          STR_STORAGE, conf);
-      Assert.fail("should throw exception");
-    } catch (Exception e) {
-      System.out.println(e);
-    }
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroup.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroup.java
deleted file mode 100644
index 8f96b12f1..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroup.java
+++ /dev/null
@@ -1,483 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.IOException;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Random;
-import java.util.Set;
-import java.util.TreeSet;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.tfile.RawComparable;
-import org.apache.hadoop.zebra.io.BasicTableStatus;
-import org.apache.hadoop.zebra.io.ColumnGroup;
-import org.apache.hadoop.zebra.io.KeyDistribution;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.io.ColumnGroup.Reader.CGRangeSplit;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * Testing ColumnGroup APIs called as if in MapReduce Jobs
- */
-public class TestColumnGroup {
-  static Configuration conf;
-  static Random random;
-  static Path rootPath;
-  static FileSystem fs;
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException {
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-    conf.set("io.compression.codec.lzo.class", "no");
-    random = new Random(System.nanoTime());
-    rootPath = new Path(System.getProperty("test.build.data",
-        "build/test/data/workdir3"));
-    fs = rootPath.getFileSystem(conf);
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-  }
-
-  BytesWritable makeRandomKey(int max) {
-    return makeKey(random.nextInt(max));
-  }
-
-  static BytesWritable makeKey(int i) {
-    return new BytesWritable(String.format("key%09d", i).getBytes());
-  }
-
-  String makeString(String prefix, int max) {
-    return String.format("%s%09d", prefix, random.nextInt(max));
-  }
-
-  int createCG(int parts, int rows, String strSchema, Path path,
-      boolean properClose, boolean sorted, int[] emptyTFiles)
-      throws IOException, ParseException {
-    if (fs.exists(path)) {
-      ColumnGroup.drop(path, conf);
-    }
-
-    Set<Integer> emptyTFileSet = new HashSet<Integer>();
-    if (emptyTFiles != null) {
-      for (int i = 0; i < emptyTFiles.length; ++i) {
-        emptyTFileSet.add(emptyTFiles[i]);
-      }
-    }
-
-    ColumnGroup.Writer writer = new ColumnGroup.Writer(path, strSchema, sorted, path.getName(),
-        "pig", "gz", "root", null, (short) Short.parseShort("755", 8), false, conf);
-
-    writer.finish();
-
-    int total = 0;
-    Schema schema = new Schema(strSchema);
-    String colNames[] = schema.getColumns();
-    Tuple tuple = TypesUtils.createTuple(schema);
-    int[] permutation = new int[parts];
-    for (int i = 0; i < parts; ++i) {
-      permutation[i] = i;
-    }
-
-    for (int i = parts - 1; i > 0; --i) {
-      int targetIndex = random.nextInt(i + 1);
-      int tmp = permutation[i];
-      permutation[i] = permutation[targetIndex];
-      permutation[targetIndex] = tmp;
-    }
-
-    for (int i = 0; i < parts; ++i) {
-      writer = new ColumnGroup.Writer(path, conf);
-      TableInserter inserter = writer.getInserter(String.format("part-%06d",
-          permutation[i]), true);
-      if ((rows > 0) && !emptyTFileSet.contains(permutation[i])) {
-        int actualRows = random.nextInt(rows) + rows / 2;
-        for (int j = 0; j < actualRows; ++j, ++total) {
-          BytesWritable key;
-          if (!sorted) {
-            key = makeRandomKey(rows * 10);
-          } else {
-            key = makeKey(total);
-          }
-          TypesUtils.resetTuple(tuple);
-          for (int k = 0; k < tuple.size(); ++k) {
-            try {
-              tuple.set(k, new DataByteArray(makeString("col-" + colNames[k], rows * 10).getBytes()));
-            } catch (ExecException e) {
-              e.printStackTrace();
-            }
-          }
-          inserter.insert(key, tuple);
-        }
-      }
-      inserter.close();
-    }
-
-    if (properClose) {
-      writer = new ColumnGroup.Writer(path, conf);
-      writer.close();
-      /* We can only test number of rows on sorted tables.*/
-      if (sorted) {
-        BasicTableStatus status = getStatus(path);
-        Assert.assertEquals(total, status.getRows());
-      }
-    }
-
-    return total;
-  }
-
-  static class DupKeyGen {
-    int low, high;
-    int current;
-    boolean grow = true;
-    int index = 0;
-    int count = 0;
-
-    DupKeyGen(int low, int high) {
-      this.low = Math.max(10, low);
-      this.high = Math.max(this.low * 2, high);
-      current = this.low;
-    }
-
-    BytesWritable next() {
-      if (count == 0) {
-        count = nextCount();
-        ++index;
-      }
-      --count;
-      return makeKey(index);
-    }
-
-    int nextCount() {
-      int ret = current;
-      if ((grow && current > high) || (!grow && current < low)) {
-        grow = !grow;
-      }
-      if (grow) {
-        current *= 2;
-      } else {
-        current /= 2;
-      }
-      return ret;
-    }
-  }
-
-  int createCGDupKeys(int parts, int rows, String strSchema, Path path)
-      throws IOException, ParseException {
-    if (fs.exists(path)) {
-      ColumnGroup.drop(path, conf);
-    }
-
-    ColumnGroup.Writer writer = new ColumnGroup.Writer(path, strSchema, true, path.getName(),
-        "pig", "gz", "root", null, (short) Short.parseShort("777", 8), false, conf);
-    writer.finish();
-
-    int total = 0;
-    DupKeyGen keyGen = new DupKeyGen(10, rows * 3);
-    Schema schema = new Schema(strSchema);
-    String colNames[] = schema.getColumns();
-    Tuple tuple = TypesUtils.createTuple(schema);
-    int[] permutation = new int[parts];
-    for (int i = 0; i < parts; ++i) {
-      permutation[i] = i;
-    }
-
-    for (int i = parts - 1; i > 0; --i) {
-      int targetIndex = random.nextInt(i + 1);
-      int tmp = permutation[i];
-      permutation[i] = permutation[targetIndex];
-      permutation[targetIndex] = tmp;
-    }
-
-    for (int i = 0; i < parts; ++i) {
-      writer = new ColumnGroup.Writer(path, conf);
-      TableInserter inserter = writer.getInserter(String.format("part-%06d",
-          permutation[i]), true);
-      if (rows > 0) {
-        int actualRows = random.nextInt(rows * 2 / 3) + rows * 2 / 3;
-        for (int j = 0; j < actualRows; ++j, ++total) {
-          BytesWritable key = keyGen.next();
-          TypesUtils.resetTuple(tuple);
-          for (int k = 0; k < tuple.size(); ++k) {
-            try {
-              tuple.set(k, new DataByteArray(makeString("col-" + colNames[k], rows * 10).getBytes()));
-            } catch (ExecException e) {
-              e.printStackTrace();
-            }
-          }
-          inserter.insert(key, tuple);
-        }
-      }
-      inserter.close();
-    }
-
-    writer = new ColumnGroup.Writer(path, conf);
-    writer.close();
-    BasicTableStatus status = getStatus(path);
-    Assert.assertEquals(total, status.getRows());
-
-    return total;
-  }
-
-  void rangeSplitCG(int numSplits, int totalRows, String strProjection,
-      Path path) throws IOException, ParseException {
-    ColumnGroup.Reader reader = new ColumnGroup.Reader(path, conf);
-    reader.setProjection(strProjection);
-    long totalBytes = reader.getStatus().getSize();
-
-    List<CGRangeSplit> splits = reader.rangeSplit(numSplits);
-    reader.close();
-    int total = 0;
-    for (int i = 0; i < splits.size(); ++i) {
-      reader = new ColumnGroup.Reader(path, conf);
-      reader.setProjection(strProjection);
-      total += doReadOnly(reader.getScanner(splits.get(i), true));
-      totalBytes -= reader.getBlockDistribution(splits.get(i)).getLength();
-    }
-    Assert.assertEquals(total, totalRows);
-    Assert.assertEquals(totalBytes, 0L);
-  }
-
-  void doRangeSplit(int[] numSplits, int totalRows, String projection, Path path)
-      throws IOException, ParseException {
-    for (int i : numSplits) {
-      if (i > 0) {
-        rangeSplitCG(i, totalRows, projection, path);
-      }
-    }
-  }
-
-  void keySplitCG(int numSplits, int totalRows, String strProjection, Path path)
-      throws IOException, ParseException {
-    ColumnGroup.Reader reader = new ColumnGroup.Reader(path, conf);
-    reader.setProjection(strProjection);
-    long totalBytes = reader.getStatus().getSize();
-    BlockDistribution lastBd = new BlockDistribution();
-    KeyDistribution keyDistri = reader.getKeyDistribution(numSplits * 10, 1, lastBd);
-    Assert.assertEquals(totalBytes, keyDistri.length()+lastBd.getLength());
-    reader.close();
-    BytesWritable[] keys = null;
-    if (keyDistri.size() >= numSplits) {
-      keyDistri.resize(lastBd);
-      Assert.assertEquals(totalBytes, keyDistri.length()+lastBd.getLength());
-      RawComparable[] rawComparables = keyDistri.getKeys();
-      keys = new BytesWritable[rawComparables.length];
-      for (int i = 0; i < keys.length; ++i) {
-        keys[i] = new BytesWritable();
-        keys[i].setSize(rawComparables[i].size());
-        System.arraycopy(rawComparables[i].buffer(),
-            rawComparables[i].offset(), keys[i].get(), 0, rawComparables[i]
-                .size());
-      }
-    } else {
-      int targetSize = Math.min(totalRows / 10, numSplits);
-      // revert to manually cooked up keys.
-      Set<Integer> keySets = new TreeSet<Integer>();
-      while (keySets.size() < targetSize) {
-        keySets.add(random.nextInt(totalRows));
-      }
-      keys = new BytesWritable[targetSize];
-      if (!keySets.isEmpty()) {
-        int j = 0;
-        for (int i : keySets.toArray(new Integer[keySets.size()])) {
-          keys[j] = makeKey(i);
-          ++j;
-        }
-      }
-    }
-
-    int total = 0;
-    for (int i = 0; i < keys.length; ++i) {
-      reader = new ColumnGroup.Reader(path, conf);
-      reader.setProjection(strProjection);
-      BytesWritable begin = (i == 0) ? null : keys[i - 1];
-      BytesWritable end = (i == keys.length - 1) ? null : keys[i];
-      total += doReadOnly(reader.getScanner(begin, end, true));
-    }
-    Assert.assertEquals(total, totalRows);
-  }
-
-  void doKeySplit(int[] numSplits, int totalRows, String projection, Path path)
-      throws IOException, ParseException {
-    for (int i : numSplits) {
-      if (i > 0) {
-        keySplitCG(i, totalRows, projection, path);
-      }
-    }
-  }
-
-  BasicTableStatus getStatus(Path path) throws IOException, ParseException {
-    ColumnGroup.Reader reader = new ColumnGroup.Reader(path, conf);
-    try {
-      return reader.getStatus();
-    } finally {
-      reader.close();
-    }
-  }
-
-  void doReadWrite(Path path, int parts, int rows, String schema,
-      String projection, boolean properClose, boolean sorted, int[] emptyTFiles)
-      throws IOException, ParseException {
-    int totalRows = createCG(parts, rows, schema, path, properClose, sorted,
-        emptyTFiles);
-    if (rows == 0) {
-      Assert.assertEquals(rows, 0);
-    }
-
-    doRangeSplit(new int[] { 1, 2, parts / 2, parts, 2 * parts }, totalRows,
-        projection, path);
-    if (sorted) {
-      doKeySplit(new int[] { 1, 2, parts / 2, parts, 2 * parts, 10 * parts },
-          totalRows, projection, path);
-    }
-  }
-
-  int doReadOnly(TableScanner scanner) throws IOException, ParseException {
-    int total = 0;
-    BytesWritable key = new BytesWritable();
-    Tuple value = TypesUtils.createTuple(scanner.getSchema());
-    for (; !scanner.atEnd(); scanner.advance()) {
-      ++total;
-      switch (random.nextInt() % 4) {
-      case 0:
-        scanner.getKey(key);
-        break;
-      case 1:
-        scanner.getValue(value);
-        break;
-      case 2:
-        scanner.getKey(key);
-        scanner.getValue(value);
-        break;
-      default: // no-op.
-      }
-    }
-    scanner.close();
-
-    return total;
-  }
-
-  @Test
-  public void testNullSplits() throws IOException, ParseException {
-    Path path = new Path(rootPath, "TestColumnGroupNullSplits");
-    int totalRows = createCG(2, 10, "a, b, c", path, true, true, null);
-    ColumnGroup.Reader reader = new ColumnGroup.Reader(path, conf);
-    reader.setProjection("a,d,c,f");
-    Assert.assertEquals(totalRows, doReadOnly(reader.getScanner(null, false)));
-    Assert.assertEquals(totalRows, doReadOnly(reader.getScanner(null, null,
-        false)));
-    reader.close();
-  }
-
-  @Test
-  public void testNegativeSplits() throws IOException, ParseException {
-    Path path = new Path(rootPath, "TestNegativeSplits");
-    int totalRows = createCG(2, 100, "a, b, c", path, true, true, null);
-    rangeSplitCG(-1, totalRows, "a,d,c,f", path);
-  }
-
-  @Test
-  public void testEmptyCG() throws IOException, ParseException {
-    Path path = new Path(rootPath, "TestColumnGroupEmptyCG");
-    doReadWrite(path, 0, 0, "a, b, c", "a, d, c, f", true, false, null);
-    doReadWrite(path, 0, 0, "a, b, c", "a, d, c, f", true, true, null);
-  }
-
-  @Test
-  public void testEmptyTFiles() throws IOException, ParseException {
-    Path path = new Path(rootPath, "TestColumnGroupEmptyTFile");
-    doReadWrite(path, 2, 0, "a, b, c", "a, d, c, f", true, false, null);
-    doReadWrite(path, 2, 0, "a, b, c", "a, d, c, f", true, true, null);    
-  }
-
-  public void testNormalCases() throws IOException, ParseException {
-    Path path = new Path(rootPath, "TestColumnGroupNormal");
-    doReadWrite(path, 2, 500, "a, b, c", "a, d, c, f", true, false, null);
-    doReadWrite(path, 2, 500, "a, b, c", "a, d, c, f", true, true, null);
-  }
-
-  @Test
-  public void testSomeEmptyTFiles() throws IOException, ParseException {
-    Path path = new Path(rootPath, "TestColumnGroupSomeEmptyTFile");
-		for (int[] emptyTFiles : new int[][] { { 1, 2 }}) {
-      doReadWrite(path, 2, 250, "a, b, c", "a, d, c, f", true, false,
-          emptyTFiles);
-      doReadWrite(path, 2, 250, "a, b, c", "a, d, c, f", true, true,
-          emptyTFiles);    
-    }
-  }
-
-  int countRows(Path path, String projection) throws IOException,
-    ParseException {
-    ColumnGroup.Reader reader = new ColumnGroup.Reader(path, conf);
-    if (projection != null) {
-      reader.setProjection(projection);
-    }
-    int totalRows = 0;
-    TableScanner scanner = reader.getScanner(null, true);
-    for (; !scanner.atEnd(); scanner.advance()) {
-      ++totalRows;
-    }
-    scanner.close();
-    return totalRows;
-  }
-
-  @Test
-  public void testProjection() throws IOException, ParseException {
-    Path path = new Path(rootPath, "TestColumnGroupProjection");
-    int totalRows = createCG(2, 250, "a, b, c", path, true, true, null);
-    Assert.assertEquals(totalRows, countRows(path, null));
-    Assert.assertEquals(totalRows, countRows(path, ""));
-  }
-
-  @Test
-  public void testDuplicateKeys() throws IOException, ParseException {
-    Path path = new Path(rootPath, "TestColumnGroupDuplicateKeys");
-    int totalRows = createCGDupKeys(2, 250, "a, b, c", path);
-    doKeySplit(new int[] { 1, 5 }, totalRows, "a, d, c, f",
-        path);
-  }
-
-  @Test
-  public void testSortedCGKeySplit() throws IOException, ParseException {
-    conf.setInt("table.output.tfile.minBlock.size", 640 * 1024);
-    Path path = new Path(rootPath, "TestSortedCGKeySplit");
-    int totalRows = createCG(2, 250, "a, b, c", path, true, true, null);
-    doKeySplit(new int[] { 1, 5 }, totalRows, "a, d, c, f",
-        path);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupInserters.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupInserters.java
deleted file mode 100644
index a4174dc65..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupInserters.java
+++ /dev/null
@@ -1,327 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.IOException;
-import java.util.SortedMap;
-import java.util.TreeMap;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.ColumnGroup;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-public class TestColumnGroupInserters {
-  final static String outputFile = "TestColumnGroupInserters";
-  final static private Configuration conf = new Configuration();
-  private static FileSystem fs;
-  private static Path path;
-  private static ColumnGroup.Writer writer;
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException {
-    // set default file system to local file system
-    conf.set("fs.file.impl", "org.apache.hadoop.fs.LocalFileSystem");
-
-    // must set a conf here to the underlying FS, or it barks
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    rawLFS.setConf(conf);
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), outputFile);
-    System.out.println("output file: " + path);
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-    finish();
-  }
-
-  @Test
-  public void testInsertNullValues() throws IOException, ParseException {
-    fs.delete(path, true);
-    System.out.println("testInsertNullValues");
-    writer = new ColumnGroup.Writer(path, "abc, def", false, path.getName(), "pig", "gz",
-        null, null, (short) -1, true, conf);
-    TableInserter ins = writer.getInserter("part1", true);
-    // Tuple row = TypesUtils.createTuple(writer.getSchema());
-    // ins.insert(new BytesWritable("key".getBytes()), row);
-    ins.close();
-    close();
-  }
-
-  @Test
-  public void testFailureInvalidSchema() throws IOException, ParseException {
-    fs.delete(path, true);
-    System.out.println("testFailureInvalidSchema");
-    writer = new ColumnGroup.Writer(path, "abc, def", false, path.getName(), "pig", "gz",
-        null, null, (short) -1, true, conf);
-    TableInserter ins = writer.getInserter("part1", true);
-    Tuple row = TypesUtils.createTuple(Schema.parse("xyz, ijk, def"));
-    try {
-      ins.insert(new BytesWritable("key".getBytes()), row);
-      Assert.fail("Failed to catch diff schemas.");
-    } catch (IOException e) {
-      // noop, expecting exceptions
-    } finally {
-      ins.close();
-      close();
-    }
-  }
-
-  @Test
-  public void testFailureGetInserterAfterWriterClosed() throws IOException,
-    ParseException {
-    fs.delete(path, true);
-    System.out.println("testFailureGetInserterAfterWriterClosed");
-    writer = new ColumnGroup.Writer(path, "abc, def", false, path.getName(), "pig", "gz",
-        null, null, (short) -1, true, conf);
-    try {
-      writer.close();
-      TableInserter ins = writer.getInserter("part1", true);
-      Assert.fail("Failed to catch getInsertion after writer closure.");
-    } catch (IOException e) {
-      // noop, expecting exceptions
-    } finally {
-      close();
-    }
-  }
-
-  @Test
-  public void testFailureInsertAfterClose() throws IOException, ExecException,
-    ParseException {
-    fs.delete(path, true);
-    System.out.println("testFailureInsertAfterClose");
-    writer = new ColumnGroup.Writer(path, "abc:string, def:map(string)", false, path.getName(), "pig", "gz",
-        null, null, (short) -1, true, conf);
-    TableInserter ins = writer.getInserter("part1", true);
-
-    Tuple row = TypesUtils.createTuple(writer.getSchema());
-    row.set(0, new String("val1"));
-
-    SortedMap<String, String> map = new TreeMap<String, String>();
-    map.put("john", "boy");
-    row.set(1, map);
-
-    ins.insert(new BytesWritable("key".getBytes()), row);
-
-    ins.close();
-    writer.close();
-
-    try {
-      TableInserter ins2 = writer.getInserter("part2", true);
-      Assert.fail("Failed to catch insertion after closure.");
-    } catch (IOException e) {
-      // noop, expecting exceptions
-    } finally {
-      close();
-    }
-  }
-
-  @Test
-  public void testFailureInsertXtraColumn() throws IOException, ExecException,
-    ParseException {
-    fs.delete(path, true);
-    System.out.println("testFailureInsertXtraColumn");
-    writer = new ColumnGroup.Writer(path, "abc ", false, path.getName(), "pig", "gz", null, null, (short) -1, true,
-        conf);
-    TableInserter ins = writer.getInserter("part1", true);
-
-    try {
-      Tuple row = TypesUtils.createTuple(writer.getSchema());
-      row.set(0, new String("val1"));
-
-      SortedMap<String, String> map = new TreeMap<String, String>();
-      map.put("john", "boy");
-      row.set(1, map);
-      Assert
-          .fail("Failed to catch insertion an extra column not defined in schema.");
-    } catch (IndexOutOfBoundsException e) {
-      // noop, expecting exceptions
-    } finally {
-      ins.close();
-      close();
-    }
-  }
-
-  @Test
-  public void testInsertOneRow() throws IOException, ExecException,
-    ParseException {
-    fs.delete(path, true);
-    System.out.println("testInsertOneRow");
-    writer = new ColumnGroup.Writer(path, "abc:string, def:map(string)", false, path.getName(), "pig", "gz",
-        null, null, (short) -1, true, conf);
-    TableInserter ins = writer.getInserter("part1", true);
-
-    Tuple row = TypesUtils.createTuple(writer.getSchema());
-    row.set(0, new String("val1"));
-
-    SortedMap<String, String> map = new TreeMap<String, String>();
-    map.put("john", "boy");
-    row.set(1, map);
-
-    ins.insert(new BytesWritable("key".getBytes()), row);
-
-    ins.close();
-    close();
-  }
-
-  @Test
-  public void testInsert2Rows() throws IOException, ExecException,
-    ParseException {
-    fs.delete(path, true);
-    System.out.println("testInsert2Rows");
-    writer = new ColumnGroup.Writer(path, "abc:string, def:map(string)", false, path.getName(), "pig", "gz",
-        null, null, (short) -1, true, conf);
-    TableInserter ins = writer.getInserter("part1", true);
-
-    // row 1
-    Tuple row = TypesUtils.createTuple(writer.getSchema());
-    row.set(0, new String("val1"));
-
-    SortedMap<String, String> map = new TreeMap<String, String>();
-    map.put("john", "boy");
-    row.set(1, map);
-
-    ins.insert(new BytesWritable("key".getBytes()), row);
-
-    // row 2
-    TypesUtils.resetTuple(row);
-    row.set(0, new String("val2"));
-    map.put("joe", "boy");
-    map.put("jane", "girl");
-    // map should contain 3 k->v pairs
-    row.set(1, map);
-
-    ins.insert(new BytesWritable("key".getBytes()), row);
-    ins.close();
-
-    ins.close();
-    close();
-  }
-
-  @Test
-  public void testInsert2Inserters() throws IOException, ExecException,
-    ParseException {
-    fs.delete(path, true);
-    System.out.println("testInsert2Inserters");
-    writer = new ColumnGroup.Writer(path, "abc:string, def:map(string)", false, path.getName(), "pig", "gz",
-        null, null, (short) -1, true, conf);
-    TableInserter ins1 = writer.getInserter("part1", true);
-    TableInserter ins2 = writer.getInserter("part2", true);
-
-    // row 1
-    Tuple row = TypesUtils.createTuple(writer.getSchema());
-    row.set(0, new String("val1"));
-
-    SortedMap<String, String> map = new TreeMap<String, String>();
-    map.put("john", "boy");
-
-    ins1.insert(new BytesWritable("key11".getBytes()), row);
-    ins2.insert(new BytesWritable("key21".getBytes()), row);
-
-    // row 2
-    TypesUtils.resetTuple(row);
-    row.set(0, new String("val2"));
-    map.put("joe", "boy");
-    map.put("jane", "girl");
-    // map should contain 3 k->v pairs
-    row.set(1, map);
-
-    ins2.insert(new BytesWritable("key22".getBytes()), row);
-    // ins2.close();
-    ins1.insert(new BytesWritable("key12".getBytes()), row);
-
-    ins1.close();
-    ins2.close();
-    close();
-  }
-
-  @Test
-  public void testFailureOverlappingKeys() throws IOException, ExecException,
-    ParseException {
-    fs.delete(path, true);
-    System.out.println("testFailureOverlappingKeys");
-    writer = new ColumnGroup.Writer(path, "abc:string, def:map(string)", true, path.getName(), "pig", "gz",
-        null, null, (short) -1, true, conf);
-    TableInserter ins1 = writer.getInserter("part1", false);
-    TableInserter ins2 = writer.getInserter("part2", false);
-
-    // row 1
-
-    Tuple row = TypesUtils.createTuple(writer.getSchema());
-    row.set(0, new String("val1"));
-
-    SortedMap<String, String> map = new TreeMap<String, String>();
-    map.put("john", "boy");
-    row.set(1, map);
-
-    ins1.insert(new BytesWritable("key1".getBytes()), row);
-    ins2.insert(new BytesWritable("key2".getBytes()), row);
-
-    // row 2
-    TypesUtils.resetTuple(row);
-    row.set(0, new String("val2"));
-    map.put("joe", "boy");
-    map.put("jane", "girl");
-    // map should contain 3 k->v pairs
-    row.set(1, map);
-
-    ins2.insert(new BytesWritable("key3".getBytes()), row);
-    // ins2.close();
-    ins1.insert(new BytesWritable("key4".getBytes()), row);
-    try {
-      ins1.close();
-      ins2.close();
-      close();
-      Assert.fail("Failed to detect overlapping keys.");
-    } catch (IOException e) {
-      // noop, exceptions expected
-    } finally {
-      ColumnGroup.drop(path, conf);
-    }
-  }
-
-  private static void finish() throws IOException {
-    if (writer != null) {
-      writer.finish();
-    }
-  }
-
-  private static void close() throws IOException {
-    if (writer != null) {
-      writer.close();
-      writer = null;
-      ColumnGroup.drop(path, conf);
-    }
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupName1.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupName1.java
deleted file mode 100644
index 1dabaa216..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupName1.java
+++ /dev/null
@@ -1,173 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-
-import java.io.IOException;
-import java.util.List;
-import junit.framework.Assert;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * 
- * Test projections on complicated column types.
- * 
- */
-public class TestColumnGroupName1 {
-
-  final static String STR_SCHEMA = "s1:bool, s2:int, s3:long, s4:double, s5:string, s6:bytes";
-  final static String STR_STORAGE = "[s1, s2] as PI; [s3, s4] as General; [s5, s6] as ULT";
-  private static Configuration conf;
-  private static Path path;
-  private static FileSystem fs;
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException {
-
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), "TestSimple");
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-        STR_STORAGE, conf);
-    writer.finish();
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-    BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-    TypesUtils.resetTuple(tuple);
-
-    // insert data in row 1
-    int row = 0;
-    tuple.set(0, true); // bool
-    tuple.set(1, 1); // int
-    tuple.set(2, 1001L); // long
-    tuple.set(3, 1.1); // float
-    tuple.set(4, "hello world 1"); // string
-    tuple.set(5, new DataByteArray("hello byte 1")); // byte
-
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // insert data in row 2
-    row++;
-    tuple.set(0, false);
-    tuple.set(1, 2); // int
-    tuple.set(2, 1002L); // long
-    tuple.set(3, 3.1); // float
-    tuple.set(4, "hello world 2"); // string
-    tuple.set(5, new DataByteArray("hello byte 2")); // byte
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // finish building table, closing out the inserter, writer, writer1
-    inserter.close();
-    writer1.finish();
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-    BasicTable.drop(path, conf);
-  }
-
-  // Test simple projection
-  @Test
-  public void testReadSimple1() throws IOException, 
-    ParseException {
-    String projection = new String("s6,s5,s4,s3,s2,s1");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(true, RowValue.get(5));
-    Assert.assertEquals(1, RowValue.get(4));
-    Assert.assertEquals(1001L, RowValue.get(3));
-    Assert.assertEquals(1.1, RowValue.get(2));
-    Assert.assertEquals("hello world 1", RowValue.get(1));
-    Assert.assertEquals("hello byte 1", RowValue.get(0).toString());
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(false, RowValue.get(5));
-    Assert.assertEquals(2, RowValue.get(4));
-    Assert.assertEquals(1002L, RowValue.get(3));
-    Assert.assertEquals(3.1, RowValue.get(2));
-    Assert.assertEquals("hello world 2", RowValue.get(1));
-    Assert.assertEquals("hello byte 2", RowValue.get(0).toString());
-  }
-
-  // test stitch,
-  @Test
-  public void testReadSimpleStitch() throws IOException, 
-    ParseException {
-    String projection2 = new String("s5, s1");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection2);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals("hello world 1", RowValue.get(0));
-    Assert.assertEquals(true, RowValue.get(1));
-
-    scanner.advance();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals("hello world 2", RowValue.get(0));
-    Assert.assertEquals(false, RowValue.get(1));
-
-    reader.close();
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupName2.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupName2.java
deleted file mode 100644
index 00c7e5894..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupName2.java
+++ /dev/null
@@ -1,113 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-
-import java.io.IOException;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * 
- * Test projections on complicated column types.
- * 
- */
-public class TestColumnGroupName2 {
-
-  final static String STR_SCHEMA = "s1:bool, s2:int, s3:long, s4:double, s5:string, s6:bytes";
-  final static String STR_STORAGE = "[s1, s2] as PI secure by uid:users perm:777; [s3, s4] as General; [s5, s6] as ULT";
-  private static Configuration conf;
-  private static Path path;
-  private static FileSystem fs;
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException {
-
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), "TestColumnGroupName");
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-        STR_STORAGE, conf);
-    writer.finish();
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-    BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-    TypesUtils.resetTuple(tuple);
-
-    // insert data in row 1
-    int row = 0;
-    tuple.set(0, true); // bool
-    tuple.set(1, 1); // int
-    tuple.set(2, 1001L); // long
-    tuple.set(3, 1.1); // float
-    tuple.set(4, "hello world 1"); // string
-    tuple.set(5, new DataByteArray("hello byte 1")); // byte
-
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // insert data in row 2
-    row++;
-    tuple.set(0, false);
-    tuple.set(1, 2); // int
-    tuple.set(2, 1002L); // long
-    tuple.set(3, 3.1); // float
-    tuple.set(4, "hello world 2"); // string
-    tuple.set(5, new DataByteArray("hello byte 2")); // byte
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // finish building table, closing out the inserter, writer, writer1
-    inserter.close();
-    writer1.finish();
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-   // BasicTable.drop(path, conf);
-  }
-
-  // Test simple projection
-  @Test
-  public void test1() throws IOException, Exception {
-    BasicTable.dumpInfo(path.toString(), System.out, conf);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupName3.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupName3.java
deleted file mode 100644
index 894a3ada0..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupName3.java
+++ /dev/null
@@ -1,113 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-
-import java.io.IOException;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * 
- * Test projections on complicated column types.
- * 
- */
-public class TestColumnGroupName3 {
-
-  final static String STR_SCHEMA = "s1:bool, s2:int, s3:long, s4:double, s5:string, s6:bytes";
-  final static String STR_STORAGE = "[s1, s2]; [s3, s4]; [s5, s6]";
-  private static Configuration conf;
-  private static Path path;
-  private static FileSystem fs;
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException {
-
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), "TestColumnGroupName");
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-        STR_STORAGE, conf);
-    writer.finish();
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-    BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-    TypesUtils.resetTuple(tuple);
-
-    // insert data in row 1
-    int row = 0;
-    tuple.set(0, true); // bool
-    tuple.set(1, 1); // int
-    tuple.set(2, 1001L); // long
-    tuple.set(3, 1.1); // float
-    tuple.set(4, "hello world 1"); // string
-    tuple.set(5, new DataByteArray("hello byte 1")); // byte
-
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // insert data in row 2
-    row++;
-    tuple.set(0, false);
-    tuple.set(1, 2); // int
-    tuple.set(2, 1002L); // long
-    tuple.set(3, 3.1); // float
-    tuple.set(4, "hello world 2"); // string
-    tuple.set(5, new DataByteArray("hello byte 2")); // byte
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // finish building table, closing out the inserter, writer, writer1
-    inserter.close();
-    writer1.finish();
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-    BasicTable.drop(path, conf);
-  }
-
-  // Test simple projection
-  @Test
-  public void test1() throws IOException, Exception {
-    BasicTable.dumpInfo(path.toString(), System.out, conf);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupName4.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupName4.java
deleted file mode 100644
index bc373ecbb..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupName4.java
+++ /dev/null
@@ -1,113 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-
-import java.io.IOException;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * 
- * Test projections on complicated column types.
- * 
- */
-public class TestColumnGroupName4 {
-
-  final static String STR_SCHEMA = "s1:bool, s2:int, s3:long, s4:double, s5:string, s6:bytes";
-  final static String STR_STORAGE = "[s1, s2] as CG1; [s3, s4]";
-  private static Configuration conf;
-  private static Path path;
-  private static FileSystem fs;
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException {
-
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), "TestColumnGroupName");
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-        STR_STORAGE, conf);
-    writer.finish();
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-    BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-    TypesUtils.resetTuple(tuple);
-
-    // insert data in row 1
-    int row = 0;
-    tuple.set(0, true); // bool
-    tuple.set(1, 1); // int
-    tuple.set(2, 1001L); // long
-    tuple.set(3, 1.1); // float
-    tuple.set(4, "hello world 1"); // string
-    tuple.set(5, new DataByteArray("hello byte 1")); // byte
-
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // insert data in row 2
-    row++;
-    tuple.set(0, false);
-    tuple.set(1, 2); // int
-    tuple.set(2, 1002L); // long
-    tuple.set(3, 3.1); // float
-    tuple.set(4, "hello world 2"); // string
-    tuple.set(5, new DataByteArray("hello byte 2")); // byte
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // finish building table, closing out the inserter, writer, writer1
-    inserter.close();
-    writer1.finish();
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-    BasicTable.drop(path, conf);
-  }
-
-  // Test simple projection
-  @Test
-  public void test1() throws IOException, Exception {
-    BasicTable.dumpInfo(path.toString(), System.out, conf);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupName5.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupName5.java
deleted file mode 100644
index 7281a7244..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupName5.java
+++ /dev/null
@@ -1,126 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-
-import java.io.IOException;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * 
- * Test projections on complicated column types.
- * 
- */
-public class TestColumnGroupName5 {
-
-  final static String STR_SCHEMA = "s1:bool, s2:int, s3:long, s4:double, s5:string, s6:bytes, s7:int, s8: int, s9:int, s10:int, s11:int, s12:int";
-  final static String STR_STORAGE = "[s1]; [s2]; [s3]; [s4]; [s5]; [s6]; [s7]; [s8]; [s9]; [s10]; [s11]; [s12]";
-  private static Configuration conf;
-  private static Path path;
-  private static FileSystem fs;
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException {
-
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), "TestColumnGroupName");
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-        STR_STORAGE, conf);
-    writer.finish();
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-    BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-    TypesUtils.resetTuple(tuple);
-
-    // insert data in row 1
-    int row = 0;
-    tuple.set(0, true); // bool
-    tuple.set(1, 1); // int
-    tuple.set(2, 1001L); // long
-    tuple.set(3, 1.1); // float
-    tuple.set(4, "hello world 1"); // string
-    tuple.set(5, new DataByteArray("hello byte 1")); // byte
-    tuple.set(6, 1); // int
-    tuple.set(7, 1); // int
-    tuple.set(8, 1); // int
-    tuple.set(9, 1); // int
-    tuple.set(10, 1); // int
-    tuple.set(11, 1); // int
-
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // insert data in row 2
-    row++;
-    tuple.set(0, false);
-    tuple.set(1, 2); // int
-    tuple.set(2, 1002L); // long
-    tuple.set(3, 3.1); // float
-    tuple.set(4, "hello world 2"); // string
-    tuple.set(5, new DataByteArray("hello byte 2")); // byte
-    tuple.set(6, 2); // int
-    tuple.set(7, 2); // int
-    tuple.set(8, 2); // int
-    tuple.set(9, 2); // int
-    tuple.set(10, 2); // int
-    tuple.set(11, 2); // int
-
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // finish building table, closing out the inserter, writer, writer1
-    inserter.close();
-    writer1.finish();
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-    BasicTable.drop(path, conf);
-  }
-
-  // Test simple projection
-  @Test
-  public void test1() throws IOException, Exception {
-    BasicTable.dumpInfo(path.toString(), System.out, conf);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupName6.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupName6.java
deleted file mode 100644
index b21627a6d..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupName6.java
+++ /dev/null
@@ -1,126 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-
-import java.io.IOException;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * 
- * Test projections on complicated column types.
- * 
- */
-public class TestColumnGroupName6 {
-
-  final static String STR_SCHEMA = "s1:bool, s2:int, s3:long, s4:double, s5:string, s6:bytes, s7:int, s8: int, s9:int, s10:int, s11:int, s12:int";
-  final static String STR_STORAGE = "[s1] as CG0; [s2] as CG1; [s3] as CG2; [s4] as CG3; [s5] as CG4; [s6] as CG5; [s7] as CG6; [s8] as CG7; [s9] as CG8; [s10] as CG9; [s11] as CG10; [s12]";
-  private static Configuration conf;
-  private static Path path;
-  private static FileSystem fs;
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException {
-
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), "TestColumnGroupName");
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-        STR_STORAGE, conf);
-    writer.finish();
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-    BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-    TypesUtils.resetTuple(tuple);
-
-    // insert data in row 1
-    int row = 0;
-    tuple.set(0, true); // bool
-    tuple.set(1, 1); // int
-    tuple.set(2, 1001L); // long
-    tuple.set(3, 1.1); // float
-    tuple.set(4, "hello world 1"); // string
-    tuple.set(5, new DataByteArray("hello byte 1")); // byte
-    tuple.set(6, 1); // int
-    tuple.set(7, 1); // int
-    tuple.set(8, 1); // int
-    tuple.set(9, 1); // int
-    tuple.set(10, 1); // int
-    tuple.set(11, 1); // int
-
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // insert data in row 2
-    row++;
-    tuple.set(0, false);
-    tuple.set(1, 2); // int
-    tuple.set(2, 1002L); // long
-    tuple.set(3, 3.1); // float
-    tuple.set(4, "hello world 2"); // string
-    tuple.set(5, new DataByteArray("hello byte 2")); // byte
-    tuple.set(6, 2); // int
-    tuple.set(7, 2); // int
-    tuple.set(8, 2); // int
-    tuple.set(9, 2); // int
-    tuple.set(10, 2); // int
-    tuple.set(11, 2); // int
-
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // finish building table, closing out the inserter, writer, writer1
-    inserter.close();
-    writer1.finish();
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-    BasicTable.drop(path, conf);
-  }
-
-  // Test simple projection
-  @Test
-  public void test1() throws IOException, Exception {
-    BasicTable.dumpInfo(path.toString(), System.out, conf);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupOpen.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupOpen.java
deleted file mode 100644
index 6938f0a66..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupOpen.java
+++ /dev/null
@@ -1,200 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.IOException;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.zebra.io.ColumnGroup;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-public class TestColumnGroupOpen {
-  final static String outputFile = "TestColumnGroupOpen";
-  final private static Configuration conf = new Configuration();
-  private static FileSystem fs;
-  private static Path path;
-  private static ColumnGroup.Writer writer;
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException {
-    // set default file system to local file system
-    conf.set("fs.file.impl", "org.apache.hadoop.fs.LocalFileSystem");
-
-    // must set a conf here to the underlying FS, or it barks
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    rawLFS.setConf(conf);
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), outputFile);
-    System.out.println("output file: " + path);
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-    finish();
-  }
-
-  @Test
-  public void testNew() throws IOException, ParseException {
-    System.out.println("testNew");
-    writer = new ColumnGroup.Writer(path, "abc, def ", false, path.getName(), "pig", "gz",
-        null, null, (short) -1, true, conf);
-    // NOTE: don't call writer.close() here
-    close();
-  }
-
-  @Test
-  public void testFailureExistingSortedDiff() throws IOException,
-    ParseException {
-    System.out.println("testFailureExistingSortedDiff");
-    try {
-      writer = new ColumnGroup.Writer(path, "abc, def ", false, path.getName(), "pig", "gz",
-          null, null, (short) -1, true, conf);
-      finish();
-      writer = new ColumnGroup.Writer(path, "abc, def", true, path.getName(), "pig", "gz",
-          null, null, (short) -1, false, conf);
-      Assert.fail("Failed to catch sorted flag alteration.");
-    } catch (IOException e) {
-      // noop, expecting exceptions
-    } finally {
-      close();
-    }
-  }
-
-  @Test
-  public void testExisting() throws IOException, ParseException {
-    System.out.println("testExisting");
-    writer = new ColumnGroup.Writer(path, " abc ,  def ", false, path.getName(), "pig", "gz",
-        null, null, (short) -1, false, conf);
-    writer.close();
-    close();
-  }
-
-  @Test
-  public void testFailurePathNotDir() throws IOException, ParseException {
-    System.out.println("testFailurePathNotDir");
-    try {
-      // fs.delete(path, true);
-      ColumnGroup.drop(path, conf);
-
-      FSDataOutputStream in = fs.create(path);
-      in.close();
-      writer = new ColumnGroup.Writer(path, "   abc ,  def   ", false, path.getName(), "pig",
-          "gz", null, null, (short) -1, false, conf);
-      Assert.fail("Failed to catch path not a directory.");
-    } catch (IOException e) {
-      // noop, expecting exceptions
-    } finally {
-      close();
-    }
-  }
-
-  @Test
-  public void testFailureMetaFileExists() throws IOException, ParseException {
-    System.out.println("testFailureMetaFileExists");
-    try {
-      fs.delete(path, true);
-      FSDataOutputStream in = fs.create(new Path(path, ColumnGroup.META_FILE));
-      in.close();
-      writer = new ColumnGroup.Writer(path, "abc", false, path.getName(), "pig", "gz", null, null, (short) -1, false,
-          conf);
-      Assert.fail("Failed to catch meta file existence.");
-    } catch (IOException e) {
-      // noop, expecting exceptions
-    } finally {
-      close();
-    }
-  }
-
-  @Test
-  public void testFailureDiffSchema() throws IOException, ParseException {
-    System.out.println("testFailureDiffSchema");
-    try {
-      writer = new ColumnGroup.Writer(path, "abc", false, path.getName(), "pig", "gz", null, null, (short) -1, false,
-          conf);
-      writer.finish();
-      writer = new ColumnGroup.Writer(path, "efg", false, path.getName(), "pig", "gz", null, null, (short) -1, false,
-          conf);
-      Assert.fail("Failed to catch schema differences.");
-    } catch (IOException e) {
-      // noop, expecting exceptions
-    } finally {
-      close();
-    }
-  }
-
-  @Test
-  public void testMultiWriters() throws IOException, ParseException {
-    System.out.println("testMultiWriters");
-    ColumnGroup.Writer writer1 = null;
-    ColumnGroup.Writer writer2 = null;
-    ColumnGroup.Writer writer3 = null;
-    try {
-      writer1 = new ColumnGroup.Writer(path, "abc", false, path.getName(), "pig", "gz", null, null, (short) -1, true,
-          conf);
-      writer2 = new ColumnGroup.Writer(path, conf);
-      writer3 = new ColumnGroup.Writer(path, conf);
-
-      TableInserter ins1 = writer1.getInserter("part1", false);
-      TableInserter ins2 = writer2.getInserter("part2", false);
-      TableInserter ins3 = writer3.getInserter("part3", false);
-      ins1.close();
-      ins2.close();
-      ins3.close();
-      // }
-      // catch (IOException e) {
-      // // noop, expecting exceptions
-      // throw e;
-    } finally {
-      if (writer1 != null) {
-        writer1.finish();
-      }
-      if (writer2 != null) {
-        writer2.finish();
-      }
-      if (writer3 != null) {
-        writer3.finish();
-      }
-      close();
-    }
-  }
-
-  private static void finish() throws IOException {
-    if (writer != null) {
-      writer.finish();
-    }
-  }
-
-  private static void close() throws IOException {
-    if (writer != null) {
-      writer.close();
-      writer = null;
-    }
-    ColumnGroup.drop(path, conf);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupProjections.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupProjections.java
deleted file mode 100644
index 826fcee7d..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupProjections.java
+++ /dev/null
@@ -1,326 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.ColumnGroup;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-public class TestColumnGroupProjections {
-  final static String outputFile = "TestColumnGroupProjections";
-  final static private Configuration conf = new Configuration();
-  private static FileSystem fs;
-  private static Path path;
-  private static Schema schema;
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException, ParseException {
-    // set default file system to local file system
-    conf.set("fs.file.impl", "org.apache.hadoop.fs.LocalFileSystem");
-
-    // must set a conf here to the underlying FS, or it barks
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    rawLFS.setConf(conf);
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), outputFile);
-    System.out.println("output file: " + path);
-    
-    if (fs.exists(path)) {
-        ColumnGroup.drop(path, conf);
-    }
-
-    schema = new Schema("a:string,b:string,c:string,d:string,e:string,f:string,g:string");
-
-    ColumnGroup.Writer writer = new ColumnGroup.Writer(path, schema, false, path.getName(),
-        "pig", "gz", null, null, (short) -1, true, conf);
-    TableInserter ins = writer.getInserter("part0", true);
-
-    // row 1
-    Tuple row = TypesUtils.createTuple(writer.getSchema());
-    row.set(0, "a1");
-    row.set(1, "b1");
-    row.set(2, "c1");
-    row.set(3, "d1");
-    row.set(4, "e1");
-    row.set(5, "f1");
-    row.set(6, "g1");
-    ins.insert(new BytesWritable("k1".getBytes()), row);
-
-    // row 2
-    TypesUtils.resetTuple(row);
-    row.set(0, "a2");
-    row.set(1, "b2");
-    row.set(2, "c2");
-    row.set(3, "d2");
-    row.set(4, "e2");
-    row.set(5, "f2");
-    row.set(6, "g2");
-    ins.insert(new BytesWritable("k2".getBytes()), row);
-    ins.close();
-
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-  }
-
-  @Test
-  // default projection (without any projection) should return every column
-  public void testDefaultProjection() throws IOException, ExecException,
-      ParseException {
-    System.out.println("testDefaultProjection");
-    // test without beginKey/endKey
-    ColumnGroup.Reader reader = new ColumnGroup.Reader(path, conf);
-    TableScanner scanner = reader.getScanner(null, false);
-
-    defTest(reader, scanner);
-
-    scanner.close();
-    reader.close();
-  }
-
-  private void defTest(ColumnGroup.Reader reader, TableScanner scanner)
-      throws IOException, ParseException {
-    BytesWritable key = new BytesWritable();
-    Tuple row = TypesUtils.createTuple(reader.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k1".getBytes()));
-    scanner.getValue(row);
-    Assert.assertEquals("a1", row.get(0));
-    Assert.assertEquals("b1", row.get(1));
-    Assert.assertEquals("g1", row.get(6));
-
-    // move to next row
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k2".getBytes()));
-    TypesUtils.resetTuple(row);
-    scanner.getValue(row);
-    Assert.assertEquals("a2", row.get(0));
-    Assert.assertEquals("b2", row.get(1));
-    Assert.assertEquals("g2", row.get(6));
-  }
-
-  @Test
-  // null projection should be same as default (fully projected) projection
-  public void testNullProjection() throws IOException, ExecException,
-      ParseException {
-    System.out.println("testNullProjection");
-    // test without beginKey/endKey
-    ColumnGroup.Reader reader = new ColumnGroup.Reader(path, conf);
-    reader.setProjection(null);
-    TableScanner scanner = reader.getScanner(null, false);
-
-    defTest(reader, scanner);
-
-    scanner.close();
-    reader.close();
-  }
-
-  @Test
-  // empty projection should project no columns at all
-  public void testEmptyProjection() throws Exception {
-    System.out.println("testEmptyProjection");
-    // test without beginKey/endKey
-    ColumnGroup.Reader reader = new ColumnGroup.Reader(path, conf);
-    reader.setProjection("");
-
-    Tuple row = TypesUtils.createTuple(reader.getSchema());
-    TableScanner scanner = reader.getScanner(null, false);
-    BytesWritable key = new BytesWritable();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k1".getBytes()));
-    scanner.getValue(row);
-    try {
-      row.get(0);
-      Assert.fail("Failed to catch out of boundary exceptions.");
-    } catch (IndexOutOfBoundsException e) {
-      // no op, expecting out of bounds exceptions
-    }
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k2".getBytes()));
-    TypesUtils.resetTuple(row);
-    scanner.getValue(row);
-    // Assert.assertEquals("c2", row.get(0));
-
-    scanner.close();
-    reader.close();
-  }
-
-  @Test
-  // a column name that does not exist in the column group
-  public void testOneNonExistentProjection() throws Exception {
-    System.out.println("testOneNonExistentProjection");
-    // test without beginKey/endKey
-    ColumnGroup.Reader reader = new ColumnGroup.Reader(path, conf);
-    reader.setProjection("X");
-
-    Tuple row = TypesUtils.createTuple(1);
-    TableScanner scanner = reader.getScanner(null, false);
-    BytesWritable key = new BytesWritable();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k1".getBytes()));
-    scanner.getValue(row);
-    Assert.assertNull(row.get(0));
-    try {
-      row.get(1);
-      Assert.fail("Failed to catch out of boundary exceptions.");
-    } catch (IndexOutOfBoundsException e) {
-      // no op, expecting out of bounds exceptions
-    }
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k2".getBytes()));
-    TypesUtils.resetTuple(row);
-    scanner.getValue(row);
-    // Assert.assertEquals("c2", row.get(0));
-
-    scanner.close();
-    reader.close();
-  }
-
-  @Test
-  // normal one column projection
-  public void testOneColumnProjection() throws Exception {
-    System.out.println("testOneColumnProjection");
-    // test without beginKey/endKey
-    ColumnGroup.Reader reader = new ColumnGroup.Reader(path, conf);
-    reader.setProjection("c");
-
-    Tuple row = TypesUtils.createTuple(reader.getSchema());
-    TableScanner scanner = reader.getScanner(null, false);
-    BytesWritable key = new BytesWritable();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k1".getBytes()));
-    scanner.getValue(row);
-    Assert.assertEquals("c1", row.get(0));
-    try {
-      row.get(1);
-      Assert.fail("Failed to catch 'out of boundary' exceptions.");
-    } catch (IndexOutOfBoundsException e) {
-      // no op, expecting out of bounds exceptions
-    }
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k2".getBytes()));
-    TypesUtils.resetTuple(row);
-    scanner.getValue(row);
-    Assert.assertEquals("c2", row.get(0));
-
-    scanner.close();
-    reader.close();
-  }
-
-  @Test
-  // normal one column plus a non-existent column projection
-  public void testOnePlusNonProjection() throws Exception {
-    System.out.println("testOnePlusNonProjection");
-    ColumnGroup.Reader reader = new ColumnGroup.Reader(path, conf);
-    reader.setProjection(",f");
-
-    Tuple row = TypesUtils.createTuple(reader.getSchema());
-    TableScanner scanner = reader.getScanner(null, false);
-    BytesWritable key = new BytesWritable();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k1".getBytes()));
-    scanner.getValue(row);
-    Assert.assertNull(row.get(0));
-    Assert.assertEquals("f1", row.get(1));
-    try {
-      row.get(2);
-      Assert.fail("Failed to catch 'out of boundary' exceptions.");
-    } catch (IndexOutOfBoundsException e) {
-      // no op, expecting out of bounds exceptions
-    }
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k2".getBytes()));
-    TypesUtils.resetTuple(row);
-    scanner.getValue(row);
-    Assert.assertEquals("f2", row.get(1));
-
-    scanner.close();
-    reader.close();
-  }
-
-  @Test
-  // two normal columns projected
-  public void testTwoColumnsProjection() throws Exception {
-    System.out.println("testTwoColumnsProjection");
-    ColumnGroup.Reader reader = new ColumnGroup.Reader(path, conf);
-    reader.setProjection("f,a");
-
-    Tuple row = TypesUtils.createTuple(reader.getSchema());
-    TableScanner scanner = reader.getScanner(null, false);
-    BytesWritable key = new BytesWritable();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k1".getBytes()));
-    scanner.getValue(row);
-    Assert.assertEquals("f1", row.get(0));
-    Assert.assertEquals("a1", row.get(1));
-    try {
-      row.get(2);
-      Assert.fail("Failed to catch 'out of boundary' exceptions.");
-    } catch (IndexOutOfBoundsException e) {
-      // no op, expecting out of bounds exceptions
-    }
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k2".getBytes()));
-    TypesUtils.resetTuple(row);
-    scanner.getValue(row);
-    Assert.assertEquals("f2", row.get(0));
-    Assert.assertEquals("a2", row.get(1));
-
-    scanner.close();
-    reader.close();
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupReaders.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupReaders.java
deleted file mode 100644
index b67ca8689..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupReaders.java
+++ /dev/null
@@ -1,339 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-import java.util.SortedMap;
-import java.util.TreeMap;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BlockDistribution;
-import org.apache.hadoop.zebra.io.ColumnGroup;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.io.ColumnGroup.Reader.CGRangeSplit;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-public class TestColumnGroupReaders {
-  final static String outputFile = "TestColumnGroupReaders";
-  final static private Configuration conf = new Configuration();
-  private static FileSystem fs;
-  private static Path path;
-  private static ColumnGroup.Writer writer;
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException {
-    // set default file system to local file system
-    conf.set("fs.file.impl", "org.apache.hadoop.fs.LocalFileSystem");
-
-    // must set a conf here to the underlying FS, or it barks
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    rawLFS.setConf(conf);
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), outputFile);
-    System.out.println("output file: " + path);
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-    close();
-  }
-
-  @SuppressWarnings("unchecked")
-  @Test
-  public void testInsert2Inserters() throws ParseException, ExecException, IOException,
-      ParseException {
-    System.out.println("testInsert2Inserters");
-    boolean sorted = false; // true;
-    writer = new ColumnGroup.Writer(path, "col1:string, colTWO:map(string)", sorted, path.getName(), "pig",
-        "gz", null, null, (short) -1, true, conf);
-    TableInserter ins1 = writer.getInserter("part1", false);
-    TableInserter ins2 = writer.getInserter("part2", false);
-
-    // row 1
-    Tuple row = TypesUtils.createTuple(writer.getSchema());
-    row.set(0, "val1");
-
-    SortedMap<String, String> map = new TreeMap<String, String>();
-    map.put("john", "boy");
-    row.set(1, map);
-
-    ins1.insert(new BytesWritable("key11".getBytes()), row);
-    ins2.insert(new BytesWritable("key21".getBytes()), row);
-
-    // row 2
-    TypesUtils.resetTuple(row);
-    row.set(0, "val2");
-    map.put("joe", "boy");
-    map.put("jane", "girl");
-    // map should contain 3 k->v pairs
-    row.set(1, map);
-
-    ins2.insert(new BytesWritable("key22".getBytes()), row);
-    ins2.insert(new BytesWritable("key23".getBytes()), row);
-    // ins2.close();
-    BytesWritable key12 = new BytesWritable("key12".getBytes());
-    ins1.insert(key12, row);
-
-    ins1.close();
-    ins2.close();
-    finish();
-
-    // test without beginKey/endKey
-    ColumnGroup.Reader reader = new ColumnGroup.Reader(path, conf);
-    List<CGRangeSplit> listRanges = reader.rangeSplit(2);
-    // TableScanner scanner = reader.getScanner(null, null, false);
-    TableScanner scanner = reader.getScanner(listRanges.get(0), false);
-    BytesWritable key = new BytesWritable();
-    scanner.getKey(key);
-    Assert
-        .assertTrue(key.compareTo(new BytesWritable("key11".getBytes())) == 0);
-    TypesUtils.resetTuple(row);
-    scanner.getValue(row);
-    Assert.assertEquals("val1", row.get(0));
-    Map<String, String> mapFld2 = (Map<String, String>) row.get(1);
-    Assert.assertEquals("boy", mapFld2.get("john"));
-    scanner.close();
-    scanner = null;
-
-    // test with beginKey
-    BytesWritable key22 = new BytesWritable("key22".getBytes());
-    // TableScanner scanner2 = reader.getScanner(key22, null, true); // for
-    // sorted
-    TableScanner scanner2 = reader.getScanner(listRanges.get(1), true);
-    BytesWritable key2 = new BytesWritable();
-    scanner2.advance();// not for sorted
-    scanner2.getKey(key2);
-    Assert.assertEquals(key2, key22);
-    TypesUtils.resetTuple(row);
-    scanner2.getValue(row);
-    Assert.assertEquals("val2", row.get(0));
-
-    // test advance
-    scanner2.advance();
-    BytesWritable key4 = new BytesWritable();
-    scanner2.getKey(key4);
-    Assert
-        .assertTrue(key4.compareTo(new BytesWritable("key23".getBytes())) == 0);
-    TypesUtils.resetTuple(row);
-    scanner2.getValue(row);
-    Assert.assertEquals("val2", row.get(0));
-
-    Map<String, String> mapFld5 = (Map<String, String>) row.get(1);
-    Assert.assertEquals("girl", mapFld5.get("jane"));
-
-    // test seekTo
-    // Assert.assertTrue("failed to locate key: " + key22,
-    // scanner2.seekTo(key22));
-    // BytesWritable key22Read = new BytesWritable();
-    // scanner2.getKey(key22Read);
-    // Assert.assertEquals(new BytesWritable("key22".getBytes()), key22Read);
-
-    // Map<String, Long> map;
-    BlockDistribution dist = reader.getBlockDistribution(listRanges.get(0));
-    long n = dist.getLength();
-
-    reader.close();
-    close();
-  }
-
-  @Test
-  public void testMultiWriters() throws ExecException, Exception {
-    System.out.println("testMultiWriters");
-    ColumnGroup.Writer writer1 = writeOnePart("col1:string, col2:map(string), col3:string", 1);
-    ColumnGroup.Writer writer2 = writeOnePart(null, 2);
-    ColumnGroup.Writer writer3 = writeOnePart(null, 3);
-
-    writer3.close();
-
-    // read in parts
-    readOnePart(1);
-    readOnePart(2);
-    readOnePart(3);
-
-    // read in one big sequence
-    readInSequence(3);
-
-    readProjection(1);
-
-    close();
-
-  }
-
-  private static ColumnGroup.Writer writeOnePart(String schemaString, int count)
-      throws IOException, ExecException, ParseException {
-    ColumnGroup.Writer writer = null;
-    if (schemaString != null) {
-      writer = new ColumnGroup.Writer(path, schemaString, false, path.getName(), "pig", "gz",
-          null, null, (short) -1, true, conf);
-    } else {
-      writer = new ColumnGroup.Writer(path, conf);
-    }
-    TableInserter ins = writer.getInserter(String.format("part-%06d", count),
-        false);
-    Tuple row = TypesUtils.createTuple(writer.getSchema());
-    SortedMap<String, String> map = new TreeMap<String, String>();
-    for (int nx = 0; nx < count; nx++) {
-      row.set(0, String.format("smpcol%02d%02d", count, nx + 1));
-
-      map.put(String.format("mapcolkey%02d%02d", count, nx + 1), String.format(
-          "mapcolvalue%02d%02d", count, nx + 1));
-      row.set(1, map);
-
-      row.set(2, String.format("col3_%02d%02d", count, nx + 1));
-
-      ins.insert(new BytesWritable(String.format("key%02d%02d", count, nx + 1)
-          .getBytes()), row);
-    }
-    ins.close();
-    return writer;
-  }
-
-  private static void readInSequence(int count) throws IOException,
-      ExecException, ParseException {
-    ColumnGroup.Reader reader = new ColumnGroup.Reader(path, conf);
-    TableScanner scanner = reader.getScanner(null, true);
-
-    Tuple row = TypesUtils.createTuple(reader.getSchema());
-    BytesWritable key = new BytesWritable();
-    for (int partIdx = 0; partIdx < count; ++partIdx) {
-      for (int keyIdx = 0; keyIdx < partIdx + 1; keyIdx++) {
-        scanner.getKey(key);
-        String keyString = String
-            .format("key%02d%02d", partIdx + 1, keyIdx + 1);
-        Assert.assertTrue(new String(key.get()) + " != " + keyString, key
-            .equals(new BytesWritable(keyString.getBytes())));
-        scanner.advance();
-      }
-    }
-
-    scanner.close();
-    scanner = null;
-    reader.close();
-  }
-
-  private static void readOnePart(int count) throws IOException, ExecException,
-    ParseException {
-    ColumnGroup.Reader reader = new ColumnGroup.Reader(path, conf);
-    TableScanner scanner = reader.getScanner(reader.rangeSplit(5)
-        .get(count - 1), true);
-
-    Tuple row = TypesUtils.createTuple(reader.getSchema());
-    BytesWritable key = new BytesWritable();
-    for (int nx = 0; nx < count; ++nx) {
-      scanner.getKey(key);
-      String keyString = String.format("key%02d%02d", count, nx + 1);
-      Assert.assertTrue("Not equal to " + keyString, key
-          .equals(new BytesWritable(keyString.getBytes())));
-      TypesUtils.resetTuple(row);
-      scanner.getValue(row);
-
-      String simpVal = String.format("smpcol%02d%02d", count, nx + 1);
-      Assert.assertEquals(simpVal, row.get(0));
-
-      String mapKey = String.format("mapcolkey%02d%02d", count, nx + 1);
-      String mapVal = String.format("mapcolvalue%02d%02d", count, nx + 1);
-      Map<String, String> mapFld2 = (Map<String, String>) row.get(1);
-      Assert.assertEquals(mapVal, mapFld2.get(mapKey));
-
-      String val3 = String.format("col3_%02d%02d", count, nx + 1);
-      Assert.assertEquals(val3, row.get(2));
-
-      if (nx < count - 1) {
-        scanner.advance();
-      }
-    }
-    scanner.close();
-    scanner = null;
-    reader.close();
-  }
-
-  private static void readProjection(int count) throws IOException,
-      ExecException, ParseException {
-    ColumnGroup.Reader reader = new ColumnGroup.Reader(path, conf);
-    reader.setProjection("col2, col3, foo, col1");
-    TableScanner scanner = reader.getScanner(null, true);
-
-    Tuple row = TypesUtils.createTuple(reader.getSchema());
-    BytesWritable key = new BytesWritable();
-    scanner.getKey(key);
-
-    String keyString = String.format("key%02d%02d", 1, 1);
-    Assert.assertTrue("Not equal to " + keyString, key
-        .equals(new BytesWritable(keyString.getBytes())));
-    TypesUtils.resetTuple(row);
-    scanner.getValue(row);
-
-    // physical 3, logical 2
-    String val3 = String.format("col3_%02d%02d", 1, 1);
-    Assert.assertEquals(val3, row.get(1));
-
-    // physical 2, logical 1
-    String mapKey = String.format("mapcolkey%02d%02d", 1, 1);
-    String mapVal = String.format("mapcolvalue%02d%02d", 1, 1);
-    Map<String, String> mapFld2 = (Map<String, String>) row.get(0);
-    Assert.assertEquals(mapVal, mapFld2.get(mapKey));
-
-    // physical NONE, logical 3
-    Assert.assertNull(row.get(2));
-
-    String simpVal = String.format("smpcol%02d%02d", 1, 1);
-    Assert.assertEquals(simpVal, row.get(3));
-
-    // move to next row
-    scanner.advance();
-    scanner.getValue(row);
-
-    simpVal = String.format("smpcol%02d%02d", 2, 1);
-    Assert.assertEquals(simpVal, row.get(3));
-
-    scanner.close();
-    scanner = null;
-    reader.close();
-  }
-
-  private static void finish() throws IOException {
-    if (writer != null) {
-      writer.close();
-    }
-  }
-
-  private static void close() throws IOException {
-    if (writer != null) {
-      writer.close();
-      writer = null;
-    }
-    ColumnGroup.drop(path, conf);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupSchemas.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupSchemas.java
deleted file mode 100644
index 452dece1a..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupSchemas.java
+++ /dev/null
@@ -1,80 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.IOException;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-public class TestColumnGroupSchemas {
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException {
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-  }
-
-  @Test
-  public void testBlankSchema() throws IOException, ParseException {
-    System.out.println("testBlankSchema");
-    Schema schema = Schema.parse(" ");
-    Assert.assertEquals(0, schema.getColumns().length);
-  }
-
-  @Test
-  public void test1Column() throws IOException, ParseException {
-    System.out.println("test1SimpleColumn");
-    Schema schema = Schema.parse("  abc");
-    Assert.assertEquals(1, schema.getColumns().length);
-    Assert.assertEquals("abc", schema.getColumns()[0]);
-  }
-
-  @Test
-  public void test2Columns() throws IOException, ParseException {
-    System.out.println("test2SimpleColumns");
-    Schema schema = Schema.parse("abc, def ");
-    Assert.assertEquals(2, schema.getColumns().length);
-    Assert.assertEquals("abc", schema.getColumns()[0]);
-    Assert.assertEquals("def", schema.getColumns()[1]);
-  }
-
-  @Test
-  public void test3Columns() throws IOException, ParseException {
-    System.out.println("test3SimpleColumns");
-    Schema schema = Schema.parse(" abc, def   , ghijk");
-    Assert.assertEquals(3, schema.getColumns().length);
-    Assert.assertEquals("abc", schema.getColumns()[0]);
-    Assert.assertEquals("def", schema.getColumns()[1]);
-    Assert.assertEquals("ghijk", schema.getColumns()[2]);
-  }
-
-  @Test
-  public void testNormalizeSchema() throws IOException, ParseException {
-    System.out.println("testNormalizeSchema");
-    String norm = Schema.normalize(" abc ,def ,   ghijk ");
-    Assert.assertEquals("abc,def,ghijk", norm);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupSplits.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupSplits.java
deleted file mode 100644
index 2cfcd155d..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupSplits.java
+++ /dev/null
@@ -1,445 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.Map;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.ColumnGroup;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * 
- * Test projections on complicated(nested: record, collection, map) column
- * types.
- * 
- */
-public class TestColumnGroupSplits {
-  final static String outputFile = "TestColumnGroupSplits";
-  final static String STR_SCHEMA = "f1:bool, r:record(f11:int, f12:long), m:map(string), c:collection(record(f13:double, f14:double, f15:bytes))";
-  final static private Configuration conf = new Configuration();
-  static private FileSystem fs;
-  static private Path path;
-  static private Schema schema;
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException,
-    ParseException {
-    // set default file system to local file system
-    conf.set("fs.file.impl", "org.apache.hadoop.fs.LocalFileSystem");
-
-    // must set a conf here to the underlying FS, or it barks
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    rawLFS.setConf(conf);
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), outputFile);
-    System.out.println("output file: " + path);
-    
-    if (fs.exists(path)) {
-        ColumnGroup.drop(path, conf);
-    }
-
-    schema = new Schema(STR_SCHEMA);
-
-    ColumnGroup.Writer writer = new ColumnGroup.Writer(path, schema, false, path.getName(),
-        "pig", "gz", null, null, (short) -1, true, conf);
-    TableInserter ins = writer.getInserter("part0", true);
-
-    Tuple row = TypesUtils.createTuple(schema);
-    // schema for "r:record..."
-    Schema schRecord = writer.getSchema().getColumn(1).getSchema();
-    Tuple tupRecord = TypesUtils.createTuple(schRecord);
-    Schema schColl = schema.getColumn(3).getSchema().getColumn(0).getSchema();
-    DataBag bagColl = TypesUtils.createBag();
-    Tuple tupColl1 = TypesUtils.createTuple(schColl);
-    Tuple tupColl2 = TypesUtils.createTuple(schColl);
-    Map<String, String> map = new HashMap<String, String>();
-    byte[] abs1 = new byte[3];
-    byte[] abs2 = new byte[4];
-
-    // row 1
-    tupRecord.set(0, 1);
-    tupRecord.set(1, 1001L);
-    map.put("a", "x");
-    map.put("b", "y");
-    map.put("c", "z");
-    tupColl1.set(0, 3.1415926);
-    tupColl1.set(1, 1.6);
-    abs1[0] = 11;
-    abs1[1] = 12;
-    abs1[2] = 13;
-    tupColl1.set(2, new DataByteArray(abs1));
-    bagColl.add(tupColl1);
-    tupColl2.set(0, 123.456789);
-    tupColl2.set(1, 100);
-    abs2[0] = 21;
-    abs2[1] = 22;
-    abs2[2] = 23;
-    abs2[3] = 24;
-    tupColl2.set(2, new DataByteArray(abs2));
-    bagColl.add(tupColl2);
-    row.set(0, true);
-    row.set(1, tupRecord);
-    row.set(2, map);
-    row.set(3, bagColl);
-    ins.insert(new BytesWritable("k1".getBytes()), row);
-
-    // row 2
-    TypesUtils.resetTuple(row);
-    TypesUtils.resetTuple(tupRecord);
-    map.clear();
-    tupRecord.set(0, 2);
-    tupRecord.set(1, 1002L);
-    map.put("boy", "girl");
-    map.put("adam", "amy");
-    map.put("bob", "becky");
-    map.put("carl", "cathy");
-    bagColl.clear();
-    TypesUtils.resetTuple(tupColl1);
-    TypesUtils.resetTuple(tupColl2);
-    tupColl1.set(0, 7654.321);
-    tupColl1.set(1, 0.0001);
-    abs1[0] = 31;
-    abs1[1] = 32;
-    abs1[2] = 33;
-    tupColl1.set(2, new DataByteArray(abs1));
-    bagColl.add(tupColl1);
-    tupColl2.set(0, 0.123456789);
-    tupColl2.set(1, 0.3333);
-    abs2[0] = 41;
-    abs2[1] = 42;
-    abs2[2] = 43;
-    abs2[3] = 44;
-    tupColl2.set(2, new DataByteArray(abs2));
-    bagColl.add(tupColl2);
-    row.set(0, false);
-    row.set(1, tupRecord);
-    row.set(2, map);
-    row.set(3, bagColl);
-    ins.insert(new BytesWritable("k2".getBytes()), row);
-    ins.close();
-
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-  }
-
-  @Test
-  public void testDescribe() throws IOException, Exception {
-    ColumnGroup.dumpInfo(path, System.out, conf);
-  }
-
-  @Test
-  // default projection (without any projection) should return every column
-  public void testDefaultProjection() throws IOException, ExecException,
-    ParseException {
-    System.out.println("testDefaultProjection");
-    ColumnGroup.Reader reader = new ColumnGroup.Reader(path, conf);
-    TableScanner scanner = reader.getScanner(null, false);
-
-    defTest(reader, scanner);
-
-    scanner.close();
-    reader.close();
-  }
-
-  @SuppressWarnings("unchecked")
-  private void defTest(ColumnGroup.Reader reader, TableScanner scanner)
-      throws IOException, ParseException {
-    BytesWritable key = new BytesWritable();
-    Tuple row = TypesUtils.createTuple(reader.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k1".getBytes()));
-    scanner.getValue(row);
-    Assert.assertEquals(true, row.get(0));
-    Tuple tupRecord1 = (Tuple) row.get(1);
-    Assert.assertEquals(1, tupRecord1.get(0));
-    Assert.assertEquals(1001L, tupRecord1.get(1));
-    Map<String, String> map1 = (Map<String, String>) row.get(2);
-    Assert.assertEquals("x", map1.get("a"));
-    Assert.assertEquals("y", map1.get("b"));
-    Assert.assertEquals("z", map1.get("c"));
-    DataBag bagColl = (DataBag) row.get(3);
-    Iterator<Tuple> itorColl = bagColl.iterator();
-    Tuple tupColl = itorColl.next();
-    Assert.assertEquals(3.1415926, tupColl.get(0));
-    Assert.assertEquals(1.6, tupColl.get(1));
-    DataByteArray dba = (DataByteArray) tupColl.get(2);
-    Assert.assertEquals(11, dba.get()[0]);
-    Assert.assertEquals(12, dba.get()[1]);
-    Assert.assertEquals(13, dba.get()[2]);
-    tupColl = itorColl.next();
-    Assert.assertEquals(123.456789, tupColl.get(0));
-    Assert.assertEquals(100, tupColl.get(1));
-    dba = (DataByteArray) tupColl.get(2);
-    Assert.assertEquals(21, dba.get()[0]);
-    Assert.assertEquals(22, dba.get()[1]);
-    Assert.assertEquals(23, dba.get()[2]);
-    Assert.assertEquals(24, dba.get()[3]);
-
-    // move to next row
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k2".getBytes()));
-    TypesUtils.resetTuple(row);
-    scanner.getValue(row);
-    Assert.assertEquals(false, row.get(0));
-    Tuple tupRecord2 = (Tuple) row.get(1);
-    Assert.assertEquals(2, tupRecord2.get(0));
-    Assert.assertEquals(1002L, tupRecord2.get(1));
-    Map<String, String> map2 = (Map<String, String>) row.get(2);
-    Assert.assertEquals("girl", map2.get("boy"));
-    Assert.assertEquals("amy", map2.get("adam"));
-    Assert.assertEquals("cathy", map2.get("carl"));
-    bagColl = (DataBag) row.get(3);
-    itorColl = bagColl.iterator();
-    tupColl = itorColl.next();
-    Assert.assertEquals(7654.321, tupColl.get(0));
-    Assert.assertEquals(0.0001, tupColl.get(1));
-    tupColl = itorColl.next();
-    Assert.assertEquals(0.123456789, tupColl.get(0));
-    Assert.assertEquals(0.3333, tupColl.get(1));
-    dba = (DataByteArray) tupColl.get(2);
-    Assert.assertEquals(41, dba.get()[0]);
-    Assert.assertEquals(42, dba.get()[1]);
-    Assert.assertEquals(43, dba.get()[2]);
-    Assert.assertEquals(44, dba.get()[3]);
-  }
-
-  @Test
-  // null projection should be same as default (fully projected) projection
-  public void testNullProjection() throws IOException, ExecException,
-    ParseException {
-    System.out.println("testNullProjection");
-    ColumnGroup.Reader reader = new ColumnGroup.Reader(path, conf);
-    reader.setProjection(null);
-    TableScanner scanner = reader.getScanner(null, false);
-
-    defTest(reader, scanner);
-
-    scanner.close();
-    reader.close();
-  }
-
-  @Test
-  // empty projection should project no columns at all
-  public void testEmptyProjection() throws Exception {
-    System.out.println("testEmptyProjection");
-    // test without beginKey/endKey
-    ColumnGroup.Reader reader = new ColumnGroup.Reader(path, conf);
-    reader.setProjection("");
-
-    Tuple row = TypesUtils.createTuple(reader.getSchema());
-    TableScanner scanner = reader.getScanner(null, false);
-    BytesWritable key = new BytesWritable();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k1".getBytes()));
-    scanner.getValue(row);
-    try {
-      row.get(0);
-      Assert.fail("Failed to catch out of boundary exceptions.");
-    } catch (IndexOutOfBoundsException e) {
-      // no op, expecting out of bounds exceptions
-    }
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k2".getBytes()));
-    TypesUtils.resetTuple(row);
-    scanner.getValue(row);
-    // Assert.assertEquals("c2", row.get(0));
-
-    scanner.close();
-    reader.close();
-  }
-
-  @Test
-  // a column name that does not exist in the column group
-  public void testOneNonExistentProjection() throws Exception {
-    System.out.println("testOneNonExistentProjection");
-    ColumnGroup.Reader reader = new ColumnGroup.Reader(path, conf);
-    reader.setProjection("X");
-
-    Tuple row = TypesUtils.createTuple(reader.getSchema());
-    TableScanner scanner = reader.getScanner(null, false);
-    BytesWritable key = new BytesWritable();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k1".getBytes()));
-    scanner.getValue(row);
-    Assert.assertNull(row.get(0));
-    try {
-      row.get(1);
-      Assert.fail("Failed to catch out of boundary exceptions.");
-    } catch (IndexOutOfBoundsException e) {
-      // no op, expecting out of bounds exceptions
-    }
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k2".getBytes()));
-    TypesUtils.resetTuple(row);
-    scanner.getValue(row);
-    // Assert.assertEquals("c2", row.get(0));
-
-    scanner.close();
-    reader.close();
-  }
-
-  @Test
-  // normal one simple column projection
-  public void testOneSimpleColumnProjection() throws Exception {
-    System.out.println("testOneSimpleColumnProjection");
-    // test without beginKey/endKey
-    ColumnGroup.Reader reader = new ColumnGroup.Reader(path, conf);
-    reader.setProjection("f1");
-
-    Tuple row = TypesUtils.createTuple(reader.getSchema());
-    TableScanner scanner = reader.getScanner(null, false);
-    BytesWritable key = new BytesWritable();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k1".getBytes()));
-    scanner.getValue(row);
-    Assert.assertEquals(true, row.get(0));
-    try {
-      row.get(1);
-      Assert.fail("Failed to catch 'out of boundary' exceptions.");
-    } catch (IndexOutOfBoundsException e) {
-      // no op, expecting out of bounds exceptions
-    }
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k2".getBytes()));
-    TypesUtils.resetTuple(row);
-    scanner.getValue(row);
-    Assert.assertEquals(false, row.get(0));
-
-    scanner.close();
-    reader.close();
-  }
-
-  @Test
-  // normal one record column projection
-  public void testOneRecordColumnProjection() throws Exception {
-    System.out.println("testOneRecordColumnProjection");
-    ColumnGroup.Reader reader = new ColumnGroup.Reader(path, conf);
-    reader.setProjection("r.f12, r.XYZ");
-
-    Tuple row = TypesUtils.createTuple(reader.getSchema());
-    TableScanner scanner = reader.getScanner(null, false);
-    BytesWritable key = new BytesWritable();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k1".getBytes()));
-    scanner.getValue(row);
-    Assert.assertEquals(1001L, row.get(0));
-    Assert.assertNull(row.get(1));
-    try {
-      row.get(2);
-      Assert.fail("Failed to catch 'out of boundary' exceptions.");
-    } catch (IndexOutOfBoundsException e) {
-      // no op, expecting out of bounds exceptions
-    }
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k2".getBytes()));
-    TypesUtils.resetTuple(row);
-    scanner.getValue(row);
-    Assert.assertEquals(1002L, row.get(0));
-    Assert.assertNull(row.get(1));
-
-    scanner.close();
-    reader.close();
-  }
-
-  @Test
-  @SuppressWarnings("unchecked")
-  // normal one collection column projection
-  public void testMapColumnProjection() throws Exception {
-    System.out.println("testMapColumnProjection");
-    // test without beginKey/endKey
-    ColumnGroup.Reader reader = new ColumnGroup.Reader(path, conf);
-    reader.setProjection("m#{c|b}, f1, m#{a}");
-
-    Tuple row = TypesUtils.createTuple(reader.getSchema());
-    TableScanner scanner = reader.getScanner(null, false);
-    BytesWritable key = new BytesWritable();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k1".getBytes()));
-    scanner.getValue(row);
-    String mapVal = (String) (((Map<String, Object>) (row.get(0))).get("c"));
-
-    boolean f1 = (Boolean) (row.get(1));
-    Assert.assertEquals(true, f1);
-
-    Assert.assertEquals("z", mapVal);
-    mapVal = (String) (((Map<String, Object>) (row.get(0))).get("b"));
-    Assert.assertEquals("y", mapVal);
-    mapVal = (String) (((Map<String, Object>) (row.get(2))).get("a"));
-    Assert.assertEquals("x", mapVal);
-    try {
-      row.get(3);
-      Assert.fail("Failed to catch 'out of boundary' exceptions.");
-    } catch (IndexOutOfBoundsException e) {
-      // no op, expecting out of bounds exceptions
-    }
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k2".getBytes()));
-    TypesUtils.resetTuple(row);
-    scanner.getValue(row);
-
-    f1 = (Boolean) row.get(1);
-    Assert.assertEquals(false, f1);
-
-    mapVal = (String) (((Map<String, Object>) (row.get(0))).get("c"));
-    Assert.assertEquals(null, mapVal);
-
-    scanner.close();
-    reader.close();
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupWithWorkPath.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupWithWorkPath.java
deleted file mode 100644
index eb1ae28a3..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnGroupWithWorkPath.java
+++ /dev/null
@@ -1,483 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.IOException;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Random;
-import java.util.Set;
-import java.util.TreeSet;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.tfile.RawComparable;
-import org.apache.hadoop.zebra.io.BasicTableStatus;
-import org.apache.hadoop.zebra.io.ColumnGroup;
-import org.apache.hadoop.zebra.io.KeyDistribution;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.io.ColumnGroup.Reader.CGRangeSplit;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * Testing ColumnGroup APIs called as if in MapReduce Jobs
- */
-public class TestColumnGroupWithWorkPath {
-  static Configuration conf;
-  static Random random;
-  static Path rootPath;
-  static FileSystem fs;
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException {
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-    conf.set("io.compression.codec.lzo.class", "no");
-    random = new Random(System.nanoTime());
-    rootPath = new Path(System.getProperty("test.build.data",
-        "build/test/data/workdir3"));
-    fs = rootPath.getFileSystem(conf);
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-  }
-
-  BytesWritable makeRandomKey(int max) {
-    return makeKey(random.nextInt(max));
-  }
-
-  static BytesWritable makeKey(int i) {
-    return new BytesWritable(String.format("key%09d", i).getBytes());
-  }
-
-  String makeString(String prefix, int max) {
-    return String.format("%s%09d", prefix, random.nextInt(max));
-  }
-
-  int createCG(int parts, int rows, String strSchema, Path path,
-      boolean properClose, boolean sorted, int[] emptyTFiles)
-      throws IOException, ParseException {
-    if (fs.exists(path)) {
-      ColumnGroup.drop(path, conf);
-    }
-
-    Set<Integer> emptyTFileSet = new HashSet<Integer>();
-    if (emptyTFiles != null) {
-      for (int i = 0; i < emptyTFiles.length; ++i) {
-        emptyTFileSet.add(emptyTFiles[i]);
-      }
-    }
-
-    ColumnGroup.Writer writer = new ColumnGroup.Writer(path, strSchema, sorted, path.getName(),
-        "pig", "gz", "root", null, (short) Short.parseShort("755", 8), false, conf);
-
-    writer.finish();
-
-    int total = 0;
-    Schema schema = new Schema(strSchema);
-    String colNames[] = schema.getColumns();
-    Tuple tuple = TypesUtils.createTuple(schema);
-    int[] permutation = new int[parts];
-    for (int i = 0; i < parts; ++i) {
-      permutation[i] = i;
-    }
-
-    for (int i = parts - 1; i > 0; --i) {
-      int targetIndex = random.nextInt(i + 1);
-      int tmp = permutation[i];
-      permutation[i] = permutation[targetIndex];
-      permutation[targetIndex] = tmp;
-    }
-
-    for (int i = 0; i < parts; ++i) {
-      Path workPath = new Path(path.getParent(), "_temporary");
-      writer = new ColumnGroup.Writer(path, workPath, conf);
-      TableInserter inserter = writer.getInserter(String.format("part-%06d",
-          permutation[i]), true);
-      if ((rows > 0) && !emptyTFileSet.contains(permutation[i])) {
-        int actualRows = random.nextInt(rows) + rows / 2;
-        for (int j = 0; j < actualRows; ++j, ++total) {
-          BytesWritable key;
-          if (!sorted) {
-            key = makeRandomKey(rows * 10);
-          } else {
-            key = makeKey(total);
-          }
-          TypesUtils.resetTuple(tuple);
-          for (int k = 0; k < tuple.size(); ++k) {
-            try {
-              tuple.set(k, makeString("col-" + colNames[k], rows * 10));
-            } catch (ExecException e) {
-              e.printStackTrace();
-            }
-          }
-          inserter.insert(key, tuple);
-        }
-      }
-      inserter.close();
-    }
-
-    if (properClose) {
-      writer = new ColumnGroup.Writer(path, conf);
-      writer.close();
-      /* We can only test number of rows on sorted tables.*/
-      if (sorted) {
-        BasicTableStatus status = getStatus(path);
-        Assert.assertEquals(total, status.getRows());
-      }
-    }
-
-    return total;
-  }
-
-  static class DupKeyGen {
-    int low, high;
-    int current;
-    boolean grow = true;
-    int index = 0;
-    int count = 0;
-
-    DupKeyGen(int low, int high) {
-      this.low = Math.max(10, low);
-      this.high = Math.max(this.low * 2, high);
-      current = this.low;
-    }
-
-    BytesWritable next() {
-      if (count == 0) {
-        count = nextCount();
-        ++index;
-      }
-      --count;
-      return makeKey(index);
-    }
-
-    int nextCount() {
-      int ret = current;
-      if ((grow && current > high) || (!grow && current < low)) {
-        grow = !grow;
-      }
-      if (grow) {
-        current *= 2;
-      } else {
-        current /= 2;
-      }
-      return ret;
-    }
-  }
-
-  int createCGDupKeys(int parts, int rows, String strSchema, Path path)
-      throws IOException, ParseException {
-    if (fs.exists(path)) {
-      ColumnGroup.drop(path, conf);
-    }
-
-    ColumnGroup.Writer writer = new ColumnGroup.Writer(path, strSchema, true, path.getName(),
-        "pig", "gz", "root", null, (short) Short.parseShort("777", 8), false, conf);
-    writer.finish();
-
-    int total = 0;
-    DupKeyGen keyGen = new DupKeyGen(10, rows * 3);
-    Schema schema = new Schema(strSchema);
-    String colNames[] = schema.getColumns();
-    Tuple tuple = TypesUtils.createTuple(schema);
-    int[] permutation = new int[parts];
-    for (int i = 0; i < parts; ++i) {
-      permutation[i] = i;
-    }
-
-    for (int i = parts - 1; i > 0; --i) {
-      int targetIndex = random.nextInt(i + 1);
-      int tmp = permutation[i];
-      permutation[i] = permutation[targetIndex];
-      permutation[targetIndex] = tmp;
-    }
-
-    for (int i = 0; i < parts; ++i) {
-      writer = new ColumnGroup.Writer(path, conf);
-      TableInserter inserter = writer.getInserter(String.format("part-%06d",
-          permutation[i]), true);
-      if (rows > 0) {
-        int actualRows = random.nextInt(rows * 2 / 3) + rows * 2 / 3;
-        for (int j = 0; j < actualRows; ++j, ++total) {
-          BytesWritable key = keyGen.next();
-          TypesUtils.resetTuple(tuple);
-          for (int k = 0; k < tuple.size(); ++k) {
-            try {
-              tuple.set(k, makeString("col-" + colNames[k], rows * 10));
-            } catch (ExecException e) {
-              e.printStackTrace();
-            }
-          }
-          inserter.insert(key, tuple);
-        }
-      }
-      inserter.close();
-    }
-
-    writer = new ColumnGroup.Writer(path, conf);
-    writer.close();
-    BasicTableStatus status = getStatus(path);
-    Assert.assertEquals(total, status.getRows());
-
-    return total;
-  }
-
-  void rangeSplitCG(int numSplits, int totalRows, String strProjection,
-      Path path) throws IOException, ParseException {
-    ColumnGroup.Reader reader = new ColumnGroup.Reader(path, conf);
-    reader.setProjection(strProjection);
-    long totalBytes = reader.getStatus().getSize();
-
-    List<CGRangeSplit> splits = reader.rangeSplit(numSplits);
-    reader.close();
-    int total = 0;
-    for (int i = 0; i < splits.size(); ++i) {
-      reader = new ColumnGroup.Reader(path, conf);
-      reader.setProjection(strProjection);
-      total += doReadOnly(reader.getScanner(splits.get(i), true));
-      totalBytes -= reader.getBlockDistribution(splits.get(i)).getLength();
-    }
-    Assert.assertEquals(total, totalRows);
-    Assert.assertEquals(totalBytes, 0L);
-  }
-
-  void doRangeSplit(int[] numSplits, int totalRows, String projection, Path path)
-      throws IOException, ParseException {
-    for (int i : numSplits) {
-      if (i > 0) {
-        rangeSplitCG(i, totalRows, projection, path);
-      }
-    }
-  }
-
-  void keySplitCG(int numSplits, int totalRows, String strProjection, Path path)
-      throws IOException, ParseException {
-    ColumnGroup.Reader reader = new ColumnGroup.Reader(path, conf);
-    reader.setProjection(strProjection);
-    long totalBytes = reader.getStatus().getSize();
-    BlockDistribution lastBd = new BlockDistribution();
-    KeyDistribution keyDistri = reader.getKeyDistribution(numSplits * 10, 1, lastBd);
-    Assert.assertEquals(totalBytes, keyDistri.length()+lastBd.getLength());
-    reader.close();
-    BytesWritable[] keys = null;
-    if (keyDistri.size() >= numSplits) {
-      keyDistri.resize(lastBd);
-      Assert.assertEquals(totalBytes, keyDistri.length()+lastBd.getLength());
-      RawComparable[] rawComparables = keyDistri.getKeys();
-      keys = new BytesWritable[rawComparables.length];
-      for (int i = 0; i < keys.length; ++i) {
-        keys[i] = new BytesWritable();
-        keys[i].setSize(rawComparables[i].size());
-        System.arraycopy(rawComparables[i].buffer(),
-            rawComparables[i].offset(), keys[i].get(), 0, rawComparables[i]
-                .size());
-      }
-    } else {
-      int targetSize = Math.min(totalRows / 10, numSplits);
-      // revert to manually cooked up keys.
-      Set<Integer> keySets = new TreeSet<Integer>();
-      while (keySets.size() < targetSize) {
-        keySets.add(random.nextInt(totalRows));
-      }
-      keys = new BytesWritable[targetSize];
-      if (!keySets.isEmpty()) {
-        int j = 0;
-        for (int i : keySets.toArray(new Integer[keySets.size()])) {
-          keys[j] = makeKey(i);
-          ++j;
-        }
-      }
-    }
-
-    int total = 0;
-    for (int i = 0; i < keys.length; ++i) {
-      reader = new ColumnGroup.Reader(path, conf);
-      reader.setProjection(strProjection);
-      BytesWritable begin = (i == 0) ? null : keys[i - 1];
-      BytesWritable end = (i == keys.length - 1) ? null : keys[i];
-      total += doReadOnly(reader.getScanner(begin, end, true));
-    }
-    Assert.assertEquals(total, totalRows);
-  }
-
-  void doKeySplit(int[] numSplits, int totalRows, String projection, Path path)
-      throws IOException, ParseException {
-    for (int i : numSplits) {
-      if (i > 0) {
-        keySplitCG(i, totalRows, projection, path);
-      }
-    }
-  }
-
-  BasicTableStatus getStatus(Path path) throws IOException, ParseException {
-    ColumnGroup.Reader reader = new ColumnGroup.Reader(path, conf);
-    try {
-      return reader.getStatus();
-    } finally {
-      reader.close();
-    }
-  }
-
-  void doReadWrite(Path path, int parts, int rows, String schema,
-      String projection, boolean properClose, boolean sorted, int[] emptyTFiles)
-      throws IOException, ParseException {
-    int totalRows = createCG(parts, rows, schema, path, properClose, sorted,
-        emptyTFiles);
-    if (rows == 0) {
-      Assert.assertEquals(rows, 0);
-    }
-
-    doRangeSplit(new int[] { 1, 2, parts / 2, parts, 2 * parts }, totalRows,
-        projection, path);
-    if (sorted) {
-      doKeySplit(new int[] { 1, 2, parts / 2, parts, 2 * parts, 10 * parts },
-          totalRows, projection, path);
-    }
-  }
-
-  int doReadOnly(TableScanner scanner) throws IOException, ParseException {
-    int total = 0;
-    BytesWritable key = new BytesWritable();
-    Tuple value = TypesUtils.createTuple(scanner.getSchema());
-    for (; !scanner.atEnd(); scanner.advance()) {
-      ++total;
-      switch (random.nextInt() % 4) {
-      case 0:
-        scanner.getKey(key);
-        break;
-      case 1:
-        scanner.getValue(value);
-        break;
-      case 2:
-        scanner.getKey(key);
-        scanner.getValue(value);
-        break;
-      default: // no-op.
-      }
-    }
-    scanner.close();
-
-    return total;
-  }
-
-  @Test
-  public void testNullSplits() throws IOException, ParseException {
-    Path path = new Path(rootPath, "TestColumnGroupNullSplits");
-    int totalRows = createCG(2, 10, "a:string, b:string, c:string", path, true, true, null);
-    ColumnGroup.Reader reader = new ColumnGroup.Reader(path, conf);
-    reader.setProjection("a,d,c,f");
-    Assert.assertEquals(totalRows, doReadOnly(reader.getScanner(null, false)));
-    Assert.assertEquals(totalRows, doReadOnly(reader.getScanner(null, null,
-        false)));
-    reader.close();
-  }
-
-  @Test
-  public void testNegativeSplits() throws IOException, ParseException {
-    Path path = new Path(rootPath, "TestNegativeSplits");
-    int totalRows = createCG(2, 100, "a:string, b:string, c:string", path, true, true, null);
-    rangeSplitCG(-1, totalRows, "a,d,c,f", path);
-  }
-
-  @Test
-  public void testEmptyCG() throws IOException, ParseException {
-    Path path = new Path(rootPath, "TestColumnGroupEmptyCG");
-    doReadWrite(path, 0, 0, "a:string, b:string, c:string", "a, d, c, f", true, false, null);
-    doReadWrite(path, 0, 0, "a:string, b:string, c:string", "a, d, c, f", true, true, null);
-  }
-
-  @Test
-  public void testEmptyTFiles() throws IOException, ParseException {
-    Path path = new Path(rootPath, "TestColumnGroupEmptyTFile");
-    doReadWrite(path, 2, 0, "a:string, b:string, c:string", "a, d, c, f", true, false, null);
-    doReadWrite(path, 2, 0, "a:string, b:string, c:string", "a, d, c, f", true, true, null);    
-  }
-
-  public void testNormalCases() throws IOException, ParseException {
-    Path path = new Path(rootPath, "TestColumnGroupNormal");
-    doReadWrite(path, 2, 500, "a:string, b:string, c:string", "a, d, c, f", true, false, null);
-    doReadWrite(path, 2, 500, "a:string, b:string, c:string", "a, d, c, f", true, true, null);
-  }
-
-  @Test
-  public void testSomeEmptyTFiles() throws IOException, ParseException {
-    Path path = new Path(rootPath, "TestColumnGroupSomeEmptyTFile");
-		for (int[] emptyTFiles : new int[][] { { 1, 2 }}) {
-      doReadWrite(path, 2, 250, "a:string, b:string, c:string", "a, d, c, f", true, false,
-          emptyTFiles);
-      doReadWrite(path, 2, 250, "a:string, b:string, c:string", "a, d, c, f", true, true,
-          emptyTFiles);    
-    }
-  }
-
-  int countRows(Path path, String projection) throws IOException,
-    ParseException {
-    ColumnGroup.Reader reader = new ColumnGroup.Reader(path, conf);
-    if (projection != null) {
-      reader.setProjection(projection);
-    }
-    int totalRows = 0;
-    TableScanner scanner = reader.getScanner(null, true);
-    for (; !scanner.atEnd(); scanner.advance()) {
-      ++totalRows;
-    }
-    scanner.close();
-    return totalRows;
-  }
-
-  @Test
-  public void testProjection() throws IOException, ParseException {
-    Path path = new Path(rootPath, "TestColumnGroupProjection");
-    int totalRows = createCG(2, 250, "a:string, b:string, c:string", path, true, true, null);
-    Assert.assertEquals(totalRows, countRows(path, null));
-    Assert.assertEquals(totalRows, countRows(path, ""));
-  }
-
-  @Test
-  public void testDuplicateKeys() throws IOException, ParseException {
-    Path path = new Path(rootPath, "TestColumnGroupDuplicateKeys");
-    int totalRows = createCGDupKeys(2, 250, "a:string, b:string, c:string", path);
-    doKeySplit(new int[] { 1, 5 }, totalRows, "a, d, c, f",
-        path);
-  }
-
-  @Test
-  public void testSortedCGKeySplit() throws IOException, ParseException {
-    conf.setInt("table.output.tfile.minBlock.size", 640 * 1024);
-    Path path = new Path(rootPath, "TestSortedCGKeySplit");
-    int totalRows = createCG(2, 250, "a:string, b:string, c:string", path, true, true, null);
-    doKeySplit(new int[] { 1, 5 }, totalRows, "a, d, c, f",
-        path);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnName.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnName.java
deleted file mode 100644
index 33d562c26..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnName.java
+++ /dev/null
@@ -1,208 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * 
- * Test conventions for column names. Specifically, '_' is allowed as leading character for map keys,
- * but it's disallowed for other fields.
- * 
- */
-public class TestColumnName {
-	final static String STR_SCHEMA = 
-		"f1:bool, r:record(f11:int, f12:long), m:map(string), c:collection(record(f13:double, f14:double, f15:bytes))";
-	final static String STR_STORAGE = "[r.f12, f1, m#{b}]; [m#{_a}, r.f11]";
-
-	final static String INVALID_STR_SCHEMA = 
-		"_f1:bool, _r:record(f11:int, _f12:long), _m:map(string), _c:collection(record(_f13:double, _f14:double, _f15:bytes))";
-	final static String INVALID_STR_STORAGE = "[_r.f12, _f1, _m#{b}]; [_m#{_a}, _r.f11]";
-
-	private static Configuration conf = new Configuration();
-	private static FileSystem fs = new LocalFileSystem( new RawLocalFileSystem() );
-	private static Path path = new Path( fs.getWorkingDirectory(), "TestColumnName" );
-	static {
-		conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-		conf.setInt("table.input.split.minSize", 64 * 1024);
-		conf.set("table.output.tfile.compression", "none");
-	}
-
-	@BeforeClass
-	public static void setUp() throws IOException {
-		// drop any previous tables
-		BasicTable.drop( path, conf );
-
-		BasicTable.Writer writer = new BasicTable.Writer( path, STR_SCHEMA, STR_STORAGE, conf );
-		writer.finish();
-
-		Schema schema = writer.getSchema();
-		Tuple tuple = TypesUtils.createTuple( schema );
-
-		BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-		int part = 0;
-		TableInserter inserter = writer1.getInserter("part" + part, true);
-		TypesUtils.resetTuple(tuple);
-
-		tuple.set(0, true);
-
-		Tuple tupRecord;
-		try {
-			tupRecord = TypesUtils.createTuple(schema.getColumnSchema("r")
-					.getSchema());
-		} catch (ParseException e) {
-			e.printStackTrace();
-			throw new IOException(e);
-		}
-		tupRecord.set(0, 1);
-		tupRecord.set(1, 1001L);
-		tuple.set(1, tupRecord);
-
-		Map<String, String> map = new HashMap<String, String>();
-		map.put("_a", "x");
-		map.put("b", "y");
-		map.put("c", "z");
-		tuple.set(2, map);
-
-		DataBag bagColl = TypesUtils.createBag();
-		Schema schColl = schema.getColumn(3).getSchema().getColumn(0).getSchema();
-		Tuple tupColl1 = TypesUtils.createTuple(schColl);
-		Tuple tupColl2 = TypesUtils.createTuple(schColl);
-		byte[] abs1 = new byte[3];
-		byte[] abs2 = new byte[4];
-		tupColl1.set(0, 3.1415926);
-		tupColl1.set(1, 1.6);
-		abs1[0] = 11;
-		abs1[1] = 12;
-		abs1[2] = 13;
-		tupColl1.set(2, new DataByteArray(abs1));
-		bagColl.add(tupColl1);
-		tupColl2.set(0, 123.456789);
-		tupColl2.set(1, 100);
-		abs2[0] = 21;
-		abs2[1] = 22;
-		abs2[2] = 23;
-		abs2[3] = 24;
-		tupColl2.set(2, new DataByteArray(abs2));
-		bagColl.add(tupColl2);
-		tuple.set(3, bagColl);
-
-		int row = 0;
-		inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-				.getBytes()), tuple);
-		inserter.close();
-		writer1.finish();
-
-		writer.close();
-	}
-
-	@AfterClass
-	public static void tearDownOnce() throws IOException {
-	}
-
-	@Test
-	public void testInvalidCase() throws IOException {
-		Path p = new Path( fs.getWorkingDirectory(), "TestColumnNameInvalid" );
-		BasicTable.drop( p, conf );
-
-		try {
-			BasicTable.Writer writer = new BasicTable.Writer( p, INVALID_STR_SCHEMA, INVALID_STR_STORAGE, conf );
-			writer.finish();
-		} catch(IOException ex) {
-			// Do nothing. This is expected.
-			return;
-		}
-
-		Assert.assertTrue( false ); // Test failure.
-	}
-
-	@Test
-	public void testRead() throws IOException, ParseException {
-		String projection = new String("f1, m#{_a|b}, r, m#{c}");
-		BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-		reader.setProjection(projection);
-		// long totalBytes = reader.getStatus().getSize();
-
-		List<RangeSplit> splits = reader.rangeSplit(1);
-		reader.close();
-		reader = new BasicTable.Reader(path, conf);
-		reader.setProjection(projection);
-		TableScanner scanner = reader.getScanner(splits.get(0), true);
-		BytesWritable key = new BytesWritable();
-		Tuple value = TypesUtils.createTuple(scanner.getSchema());
-
-		scanner.getKey(key);
-		Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-		scanner.getValue(value);
-
-		Tuple recordTuple = (Tuple) value.get(2);
-		Assert.assertEquals(1, recordTuple.get(0));
-		Assert.assertEquals(1001L, recordTuple.get(1));
-		Assert.assertEquals(true, value.get(0));
-
-		HashMap<String, Object> mapval = (HashMap<String, Object>) value.get(1);
-		Assert.assertEquals("x", mapval.get("_a"));
-		Assert.assertEquals("y", mapval.get("b"));
-		Assert.assertEquals(null, mapval.get("c"));
-		mapval = (HashMap<String, Object>) value.get(3);
-		Assert.assertEquals("z", mapval.get("c"));
-		Assert.assertEquals(null, mapval.get("_a"));
-		Assert.assertEquals(null, mapval.get("b"));
-		reader.close();
-	}
-
-	@Test
-	public void testProjectionParsing() throws IOException, ParseException {
-		String projection = new String( "f1, m#{_a}, _r, m#{c}, m" );
-		BasicTable.Reader reader = new BasicTable.Reader( path, conf );
-		try {
-			reader.setProjection( projection );
-			reader.close();
-		} catch(ParseException ex) {
-			// Expected.
-			return;
-		}
-		
-		Assert.assertTrue( false );
-	}
-	
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnNameGroup.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnNameGroup.java
deleted file mode 100644
index a323098bf..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestColumnNameGroup.java
+++ /dev/null
@@ -1,172 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Random;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Projection;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * 
- * Test projections on complicated column types.
- * 
- */
-public class TestColumnNameGroup {
-  // final static String STR_SCHEMA =
-  // "s1:bool, s2:int, s3:long, s4:float, s5:string, s6:bytes, r1:record(f1:int, f2:long),  m1:map(string), c:collection(f13:double, f14:float, f15:bytes)";
-  // final static String STR_STORAGE =
-  // "[s1, s2]; [r1.f1]; [s3, s4]; [s5, s6]; [r1.f2]; [c.f13]";
-  private static Configuration conf;
-  private static Path path;
-  private static FileSystem fs;
-
-  @BeforeClass
-  public static void setUp() throws IOException {
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), "TestColumnNameGroup");
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-  }
-
-  /**
-   * Return the name of the routine that called getCurrentMethodName
-   * 
-   */
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    return t;
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-  //  BasicTable.drop(path, conf);
-  }
-
-  @Test
-  public void testSimple() throws IOException, ParseException {
-    String STR_SCHEMA = "group:string, s6:bytes";
-    String STR_STORAGE = "[group]; [s6]";
-
-    // Build Table and column groups
-    BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-        STR_STORAGE, conf);
-    writer.finish();
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-    BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-    TypesUtils.resetTuple(tuple);
-
-    // insert data in row 1
-    int row = 0;
-   
-    tuple.set(0, "hello world 1"); // string
-    tuple.set(1, new DataByteArray("hello byte 1")); // byte
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // insert data in row 2
-    row++;
-   
-    tuple.set(0, "hello world 2"); // string
-    tuple.set(1, new DataByteArray("hello byte 2")); // byte
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // finish building table, closing out the inserter, writer, writer1
-    inserter.close();
-    writer1.finish();
-    writer.close();
-
-    // Starting read
-    String projection = new String("group");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-   
-    Assert.assertEquals("hello world 1", RowValue.get(0));
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-   
-    Assert.assertEquals("hello world 2", RowValue.get(0));
-
-
-    reader.close();
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestDropColumnGroup.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestDropColumnGroup.java
deleted file mode 100644
index c77a878ae..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestDropColumnGroup.java
+++ /dev/null
@@ -1,624 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.IOException;
-import java.util.HashMap;
-
-import java.util.List;
-
-import java.util.Map;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.parser.ParseException;
-
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-
-import org.junit.AfterClass;
-
-import org.junit.Assert;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-public class TestDropColumnGroup {
-  Log LOG = LogFactory.getLog(TestDropColumnGroup.class);
-  private static Path path;
-  private static Configuration conf;
-  private static FileSystem fs;
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException {
-    TestBasicTable.setUpOnce();
-    path = new Path(TestBasicTable.rootPath, "DropCGTest");
-    conf = TestBasicTable.conf;
-    Log LOG = LogFactory.getLog(TestDropColumnGroup.class);
-    fs = path.getFileSystem(conf);
-  }
-
-  @AfterClass
-  public static void tearDown() throws IOException {
-    BasicTable.drop(path, conf);
-  }
-
-  /**
-   * Utitility function to open a table with a given projection and verify that
-   * certain fields in the returned tuple are null and certain fields are not.
-   */
-  void verifyScanner(Path path, Configuration conf, String projection,
-      boolean isNullExpected[], int numRowsToRead) throws IOException,
-      ParseException {
-
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    TableScanner scanner = reader.getScanner(null, true);
-
-    Tuple row = TypesUtils.createTuple(reader.getSchema());
-
-    for (int i = 0; i < numRowsToRead; i++) {
-      scanner.getValue(row);
-      for (int f = 0; f < isNullExpected.length; f++) {
-        if (isNullExpected[f] ^ row.get(f) == null) {
-          throw new IOException("Verification failure at field " + f + " row "
-              + i + " : expected " + (isNullExpected[f] ? "NULL" : "nonNULL")
-              + " but got opposite.");
-
-        }
-      }
-      scanner.advance();
-    }
-
-    scanner.close();
-  }
-
-  int countRows(Path path, Configuration conf, String projection)
-      throws IOException, ParseException {
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    TableScanner scanner = reader.getScanner(null, true);
-    int count = 0;
-    while (!scanner.atEnd()) {
-      count++;
-      scanner.advance();
-    }
-    scanner.close();
-    return count;
-  }
-
-  @Test
-  public void testDropColumnGroup() throws IOException, ParseException {
-    /*
-     * Tests basic drop columns feature. Also tests that fields in dropped
-     * columns can be read the value returned is null.
-     */
-
-    if (fs.exists(path)) {
-      BasicTable.drop(path, conf);
-    }
-    
-    int numRows = TestBasicTable.createBasicTable(1, 10, "a, b, c, d, e, f",
-                                                  "[a, b]; [c, d]", null,
-                                                  path, true);
-
-    int rowsToRead = Math.min(10, numRows);
-
-    // normal table.
-    verifyScanner(path, conf, "a, c, x", new boolean[] { false, false, true },
-        rowsToRead);
-
-    // Now delete ([c, d)
-    BasicTable.dropColumnGroup(path, conf, "CG1");
-
-    // check various read cases.
-    verifyScanner(path, conf, "c, a", new boolean[] { true, false }, rowsToRead);
-    verifyScanner(path, conf, "c, a", new boolean[] { true, false }, rowsToRead);
-
-    verifyScanner(path, conf, "c, a, b, f, d, e", new boolean[] { true, false,
-        false, false, true, false }, rowsToRead);
-
-    BasicTable.dumpInfo(path.toString(), System.err, conf);
-
-    // Drop CG0 ([a, b])
-    BasicTable.dropColumnGroup(path, conf, "CG0");
-
-    verifyScanner(path, conf, "a, b", new boolean[] { true, true }, rowsToRead);
-
-    // Drop remaining CG2
-    BasicTable.dropColumnGroup(path, conf, "CG2");
-
-    verifyScanner(path, conf, "a, b, c, d, e, f", new boolean[] { true, true,
-        true, true, true, true }, rowsToRead);
-
-    // Now make sure the reader reports zero rows.
-    Assert.assertTrue(countRows(path, conf, "c, e, b") == 0);
-
-    // delete the table
-    BasicTable.drop(path, conf);
-
-    /*
-     * Try similar tests with range splits.
-     */
-
-    // 5 splits and 50 rows
-    numRows =  TestBasicTable.createBasicTable(5, 50, "a, b, c, d, e, f",
-                                               "[a, b]; [c, d]; [e] as myCG",
-                                               null, path, true);
-
-    BasicTable.dropColumnGroup(path, conf, "myCG");
-
-    verifyScanner(path, conf, "e, c, g, b", new boolean[] { true, false, true,
-        false }, numRows);
-
-    TestBasicTable.doRangeSplit(new int[] { 4, 0, 2 }, numRows,
-        "a, b, c, e, f, x", path);
-
-    // Remove another CG.
-    BasicTable.dropColumnGroup(path, conf, "CG0");
-
-    TestBasicTable.doRangeSplit(new int[] { 4, 0, 2, 3, 1 }, numRows,
-        "a, y, c, e, f, x", path);
-
-    BasicTable.drop(path, conf);
-  }
-
-  @Test
-  public void test2() throws IOException, ParseException {
-    /*
-     * Tests concurrent drop CGs
-     */
-    if (fs.exists(path)) {
-      BasicTable.drop(path, conf);
-    }
-
-    int numRows = TestBasicTable.createBasicTable(1, 10, "f1,f2,f3,f4,f5,f6,f7,f8,f9,f10," +
-    		                                                 "f11,f12,f13,f14,f15,f16,f17,f18,f19,f20," +
-    		                                                 "f21,f22,f23,f24,f25,f26,f27,f28,f29,f30," +
-    		                                                 "f31,f32,f33,f34,f35,f36,f37,f38,f39,f40," +
-    		                                                 "f41,f42,f43,f44,f45,f46,f47,f48,f49,f50",
-      "[f1];[f2];[f3];[f4];[f5];[f6];[f7];[f8];[f9];[f10];" +
-      "[f11];[f12];[f13];[f14];[f15];[f16];[f17];[f18];[f19];[f20];" +
-      "[f21];[f22];[f23];[f24];[f25];[f26];[f27];[f28];[f29];[f30];" +
-      "[f31];[f32];[f33];[f34];[f35];[f36];[f37];[f38];[f39];[f40];" +
-      "[f41];[f42];[f43];[f44];[f45];[f46];[f47];[f48];[f49];[f50]",
-      null, path, true);
-    
-    System.out.println("First dump:");
-    BasicTable.dumpInfo(path.toString(), System.out, conf);
-    int rowsToRead = Math.min(10, numRows);
-
-    // normal table.
-    verifyScanner(path, conf, "f1, f3, xx", new boolean[] { false, false, true },
-        rowsToRead);
-
-    // create a thread for each dropCG
-    DropThread[] threads = new DropThread[50];
-
-    for (int i = 0; i < threads.length; i++) {
-      threads[i] = new DropThread(i, 50);
-    }
-
-    // start the threads
-    for (int j = 0; j < threads.length; j++) {
-      threads[j].start();
-    }
-
-    for (Thread thr : threads) {
-      try {
-        thr.join();
-      } catch (InterruptedException e) {
-        e.printStackTrace();
-      }
-    }
-
-    // check various read cases.
-
-    verifyScanner(path, conf, "f3, f1, f2, f6, f4, f5", new boolean[] { true, true,
-        true, true, true, true }, rowsToRead);
-    System.out.println("second dump");
-    BasicTable.dumpInfo(path.toString(), System.out, conf);
-
-    // Now make sure the reader reports zero rows.
-    Assert.assertTrue(countRows(path, conf, "f3, f5, f2") == 0);
-
-    // delete the table
-    BasicTable.drop(path, conf);
-  }
-
-  @Test
-  public void test3() throws IOException, ParseException {
-    /*
-     * Tests concurrrent drop CGs while one fails
-     */
-    if (fs.exists(path)) {
-      BasicTable.drop(path, conf);
-    }
-
-    int numRows = TestBasicTable.createBasicTable(1, 10, "f1,f2,f3,f4,f5,f6,f7,f8,f9,f10," +
-        "f11,f12,f13,f14,f15,f16,f17,f18,f19,f20," +
-        "f21,f22,f23,f24,f25,f26,f27,f28,f29,f30," +
-        "f31,f32,f33,f34,f35,f36,f37,f38,f39,f40," +
-        "f41,f42,f43,f44,f45,f46,f47,f48,f49,f50",
-        "[f1];[f2];[f3];[f4];[f5];[f6];[f7];[f8];[f9];[f10];" +
-        "[f11];[f12];[f13];[f14];[f15];[f16];[f17];[f18];[f19];[f20];" +
-        "[f21];[f22];[f23];[f24];[f25];[f26];[f27];[f28];[f29];[f30];" +
-        "[f31];[f32];[f33];[f34];[f35];[f36];[f37];[f38];[f39];[f40];" +
-        "[f41];[f42];[f43];[f44];[f45];[f46];[f47];[f48];[f49];[f50]",
-        null, path, true);
-    
-    System.out.println("First dump:");
-    BasicTable.dumpInfo(path.toString(), System.out, conf);
-    int rowsToRead = Math.min(10, numRows);
-
-    // normal table.
-    verifyScanner(path, conf, "f1, f3, xx", new boolean[] { false, false, true },
-        rowsToRead);
-
-    // create a thread for each dropCG
-    DropThread[] threads = new DropThread[60];
-
-    for (int i = 0; i < threads.length; i++) {
-      threads[i] = new DropThread(i, 50);
-    }
-
-    // start the threads
-    for (int j = 0; j < threads.length; j++) {
-      threads[j].start();
-    }
-
-    for (Thread thr : threads) {
-      try {
-        thr.join();
-      } catch (InterruptedException e) {
-        e.printStackTrace();
-      }
-    }
-
-    // check various read cases.
-
-    verifyScanner(path, conf, "f3, f1, f2, f6, f4, f5", new boolean[] { true, true,
-        true, true, true, true }, rowsToRead);
-    System.out.println("second dump");
-    BasicTable.dumpInfo(path.toString(), System.out, conf);
-
-    // Now make sure the reader reports zero rows.
-    Assert.assertTrue(countRows(path, conf, "f3, f5, f2") == 0);
-
-    // delete the table
-    BasicTable.drop(path, conf);
-  }
-
-  @Test
-  public void test5() throws IOException, ParseException {
-    /*
-     * Tests drop CGs while reading the same CGs
-     */
-
-    System.out.println("######int test 5");
-
-    if (fs.exists(path)) {
-      BasicTable.drop(path, conf);
-    }
-
-    int numRows = TestBasicTable.createBasicTable(1, 100000,
-        "a, b, c, d, e, f, g, h, i, j, k, l, m, n", "[a, b]; [c, d]; [e]; [f]; [g]; [h]; [i]; [j]; [k]; [l]; [m]; [n]", null, path, true);
-
-    System.out.println("in test5 , dump infor 1");
-    BasicTable.dumpInfo(path.toString(), System.out, conf);
-
-    int minRowsToRead = 10000;
-    int numOfReadThreads = 20;
-    int rowsToRead = Math.min(minRowsToRead, numRows);
-
-    // normal table.
-    verifyScanner(path, conf, "a, c, x", new boolean[] { false, false, true },
-        rowsToRead);
-
-    // create a thread for each dropCG
-    DropThread[] dropThreads = new DropThread[12];
-
-    for (int i = 0; i < dropThreads.length; i++) {
-      dropThreads[i] = new DropThread(i, 12);
-    }
-
-    // start the threads
-    for (int j = 0; j < dropThreads.length; j++) {
-      dropThreads[j].start();
-    }
-    
-    // create read threads
-    ReadThread[] readThreads = new ReadThread[numOfReadThreads];
-
-    for (int i = 0; i < readThreads.length; i++) {
-      readThreads[i] = new ReadThread(i, "a, b, c, d, e, f", 1000);
-    }
-
-    // start the threads
-    for (int j = 0; j < readThreads.length; j++) {
-      readThreads[j].start();
-    }
-
-    for (Thread thr : dropThreads) {
-      try {
-        thr.join();
-      } catch (InterruptedException e) {
-        e.printStackTrace();
-      }
-    }
-    
-    for (Thread thr : readThreads) {
-      try {
-        thr.join();
-      } catch (InterruptedException e) {
-        e.printStackTrace();
-      }
-    }
-
-    verifyScanner(path, conf, "c, a, b, f, d, e", new boolean[] { true, true,
-        true, true, true, true }, rowsToRead);
-    System.out.println("second dump");
-    BasicTable.dumpInfo(path.toString(), System.out, conf);
-
-    // Now make sure the reader reports zero rows.
-    Assert.assertTrue(countRows(path, conf, "c, e, b") == 0);
-
-    // delete the table
-    BasicTable.drop(path, conf);
-  }
-
-  @Test
-  public void test11() throws IOException, ParseException {
-
-    /*
-     * Tests test open non-existing table.
-     */
-
-    try {
-      new BasicTable.Reader(new Path(path.toString(), "non-existing"), conf);
-      Assert.fail("read none existing table should fail");
-    } catch (Exception e) {
-
-    }
-
-  }
-
-  @Test
-  public void test12() throws IOException, ParseException {
-    /*
-     * Tests API, path is wrong
-     */
-    if (fs.exists(path)) {
-      BasicTable.drop(path, conf);
-    }
-    
-    TestBasicTable.createBasicTable(1, 10, "a, b, c, d, e, f",
-        "[a];[b];[c];[d];[e];[f]", null, path, true);
-    Path wrongPath = new Path(path.toString() + "non-existing");
-    try {
-      BasicTable.dropColumnGroup(wrongPath, conf, "CG0");
-      Assert.fail("should throw excepiton");
-    } catch (Exception e) {
-
-    }
-    BasicTable.drop(path, conf);
-  }
-
-  @Test
-  public void test13() throws IOException, ParseException {
-    /*
-     * Tests API, conf is null
-     */
-
-    Path path1 = new Path(path.toString() + "13");
-    TestBasicTable.createBasicTable(1, 10, "a, b, c, d, e, f",
-        "[a];[b];[c];[d];[e];[f]", null, path1, true);
-    try {
-      BasicTable.dropColumnGroup(path1, null, "CG0");
-      Assert.fail("should throw excepiton");
-    } catch (Exception e) {
-
-    }
-    BasicTable.drop(path1, conf);
-  }
-
-  @Test
-  public void test14() throws IOException, ParseException {
-    /*
-     * Tests API, CG name is empty string
-     */
-
-    Path path1 = new Path(path.toString() + "14");
-    TestBasicTable.createBasicTable(1, 10, "a, b, c, d, e, f",
-        "[a];[b];[c];[d];[e];[f]", null, path1, true);
-    try {
-      BasicTable.dropColumnGroup(path1, conf, "");
-      Assert.fail("should throw excepiton");
-    } catch (Exception e) {
-
-    }
-    BasicTable.drop(path1, conf);
-  }
-
-  @Test
-  public void test15() throws IOException, ParseException {
-    /*
-     * Tests API, CG name is null
-     */
-
-    Path path1 = new Path(path.toString() + "15");
-
-    TestBasicTable.createBasicTable(1, 10, "a, b, c, d, e, f",
-        "[a];[b];[c];[d];[e];[f]", null, path1, true);
-    try {
-      BasicTable.dropColumnGroup(path1, conf, null);
-      Assert.fail("should throw excepiton");
-    } catch (Exception e) {
-
-    }
-    BasicTable.drop(path1, conf);
-  }
-
-  @Test
-  public void test16() throws IOException, ParseException {
-    /*
-     * Tests delete same CG multiple times
-     */
-
-    Path path1 = new Path(path.toString() + "16");
-
-    int numRows = TestBasicTable.createBasicTable(1, 10, "a, b, c, d, e, f",
-        "[a, b]; [c, d]", null, path1, true);
-
-    int rowsToRead = Math.min(10, numRows);
-
-    // normal table.
-    verifyScanner(path1, conf, "a, c, x", new boolean[] { false, false, true },
-        rowsToRead);
-
-    // Now delete ([c, d)
-    BasicTable.dropColumnGroup(path1, conf, "CG1");
-
-    // check various read cases.
-    verifyScanner(path1, conf, "c, a", new boolean[] { true, false },
-        rowsToRead);
-
-    // Now delete ([c, d)again
-    BasicTable.dropColumnGroup(path1, conf, "CG1");
-
-    verifyScanner(path1, conf, "c, a", new boolean[] { true, false },
-        rowsToRead);
-    BasicTable.drop(path1, conf);
-  }
-
-  @Test
-  public void test17() throws IOException, ParseException {
-    /*
-     * test rangesplit
-     */
-    System.out.println("test 17");
-
-    Path path1 = new Path(path.toString() + "17");
-    TestBasicTable.createBasicTable(1, 10, "a, b, c, d, e, f", "[a,b,c,d,e,f]",
-        null, path1, true);
-
-    BasicTable.dropColumnGroup(path1, conf, "CG0");
-
-    BasicTable.Reader reader = new BasicTable.Reader(path1, conf);
-    reader.setProjection("a, b, c, d, e, f");
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = null;
-    try {
-      scanner = reader.getScanner(splits.get(0), true);
-    } catch (Exception e) {
-      System.out.println("in test 17, getScanner");
-      e.printStackTrace();
-    }
-
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getValue(RowValue);
-
-    Assert.assertEquals(null, RowValue.get(0));
-    Assert.assertFalse(scanner.advance());
-    scanner.getValue(RowValue);
-    Assert.assertEquals(null, RowValue.get(0));
-    BasicTable.drop(path1, conf);
-  }
-
-/**
-   * A thread that performs a DropColumnGroup.
-   */
-  class DropThread extends Thread {
-
-    private int id;
-    private int cntCGs;
-
-    public DropThread(int id, int cntCGs) {
-      this.id = id;
-      this.cntCGs = cntCGs;
-    }
-
-    /**
-     * Executes DropColumnGroup.
-     */
-    public void run() {
-      try {
-        int total = cntCGs;
-        int digits = 1;
-        while (total >= 10) {
-          ++ digits;
-          total /= 10;
-        }
-        String formatString = "%0" + digits + "d";
-        String str = "CG"  + String.format(formatString, id);
-
-        System.out.println(id + ": Droping CG: " + str);
-        BasicTable.dropColumnGroup(path, conf, str);
-      } catch (Exception e) {
-        System.out.println(id + " - error: " + e);
-        e.printStackTrace();
-      }
-    }
-  }
-
-  /**
-   * A thread that performs a ReadColumnGroup.
-   */
-  class ReadThread extends Thread {
-
-    private int id;
-    private String projection;
-    private int numRowsToRead;
-
-    public ReadThread(int id, String projection, int numRowsToRead) {
-      this.id = id;
-      this.projection = projection;
-      this.numRowsToRead = numRowsToRead;
-
-    }
-
-    /**
-     * Executes DropColumnGroup.
-     */
-    public void run() {
-      BasicTable.Reader reader = null;
-      try {
-        reader = new BasicTable.Reader(path, conf);
-        reader.setProjection(projection);
-        TableScanner scanner = reader.getScanner(null, true);
-        Tuple row = TypesUtils.createTuple(reader.getSchema());
-        for (int i = 0; i < numRowsToRead; i++) {
-          scanner.getValue(row);
-        }
-        scanner.advance();
-        scanner.close();
-      } catch (Exception e) {
-        e.printStackTrace();
-      }
-
-    }
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestDuplicateMapKeyInDifferentCGs.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestDuplicateMapKeyInDifferentCGs.java
deleted file mode 100644
index f2b1f235b..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestDuplicateMapKeyInDifferentCGs.java
+++ /dev/null
@@ -1,85 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import junit.framework.Assert;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * 
- * Test projections on complicated column types.
- * 
- */
-public class TestDuplicateMapKeyInDifferentCGs {
-
-  final static String STR_SCHEMA = "m1:map(string),m2:map(map(int))";
-  final static String STR_STORAGE = "[m1#{a}, m2#{x}];[m2#{x|y}]; [m1#{b}, m2#{z}];[m1,m2]";
-  private static Configuration conf;
-  private static Path path;
-  private static FileSystem fs;
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException {
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), "TestMap");
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-  }
-
- 
-  @Test
-  public void testRead1() throws IOException, ParseException {
-    try {
-      BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-            STR_STORAGE, conf);
-      Assert.fail("duplicate keys in different column groups should throw an exception!");
-    } catch (IOException e) {
-      return;
-    }
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestMap.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestMap.java
deleted file mode 100644
index ca1a93794..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestMap.java
+++ /dev/null
@@ -1,281 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Random;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Projection;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * 
- * Test projections on complicated column types.
- * 
- */
-public class TestMap {
-
-  final static String STR_SCHEMA = "m1:map(string),m2:map(map(int))";
-  final static String STR_STORAGE = "[m1#{a}];[m2#{x|y}]; [m1#{b}, m2#{z}]";
-  private static Configuration conf;
-  private static Path path;
-  private static FileSystem fs;
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException {
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), "TestMap");
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-        STR_STORAGE, conf);
-    writer.finish();
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-    BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-    TypesUtils.resetTuple(tuple);
-
-    // add data to row 1
-    // m1:map(string)
-    Map<String, String> m1 = new HashMap<String, String>();
-    m1.put("a", "A");
-    m1.put("b", "B");
-    m1.put("c", "C");
-    tuple.set(0, m1);
-
-    // m2:map(map(int))
-    HashMap<String, Map> m2 = new HashMap<String, Map>();
-    Map<String, Integer> m3 = new HashMap<String, Integer>();
-    m3.put("m311", 311);
-    m3.put("m321", 321);
-    m3.put("m331", 331);
-    Map<String, Integer> m4 = new HashMap<String, Integer>();
-    m4.put("m411", 411);
-    m4.put("m421", 421);
-    m4.put("m431", 431);
-    m2.put("x", m3);
-    m2.put("y", m4);
-    tuple.set(1, m2);
-    int row = 0;
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // row 2
-    row++;
-    TypesUtils.resetTuple(tuple);
-    m1.clear();
-    m2.clear();
-    m3.clear();
-    m4.clear();
-    // m1:map(string)
-    m1.put("a", "A2");
-    m1.put("b2", "B2");
-    m1.put("c2", "C2");
-    tuple.set(0, m1);
-
-    // m2:map(map(int))
-    m3.put("m321", 321);
-    m3.put("m322", 322);
-    m3.put("m323", 323);
-    m2.put("z", m3);
-    tuple.set(1, m2);
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // finish building table, closing out the inserter, writer, writer1
-    inserter.close();
-    writer1.finish();
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-    BasicTable.drop(path, conf);
-  }
-
-  // read one map
-  @Test
-  public void testRead1() throws IOException, ParseException {
-    String projection = new String("m1#{a}");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    System.out.println("read1 : " + RowValue.toString());
-    Assert.assertEquals("{a=A}", RowValue.get(0).toString());
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    System.out.println(RowValue.get(0).toString());
-    Assert.assertEquals("{a=A2}", RowValue.get(0).toString());
-
-    reader.close();
-  }
-
-  // m1:map(string),m2:map(map(int))";
-  // String STR_STORAGE = "[m1#{a}];[m2#{x|y}]; [m1#{b}, m2#{z}]";
-  // read map of map, stitch
-  @Test
-  public void testRead2() throws IOException, ParseException {
-    String projection2 = new String("m1#{b}, m2#{x|z}");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection2);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    System.out.println("map of map: " + RowValue.toString());
-    // map of map: ([b#B],[z#,x#{m311=311, m321=321, m331=331}])
-    Assert.assertEquals("B", ((Map) RowValue.get(0)).get("b"));
-    Assert.assertEquals(321, ((Map) ((Map) RowValue.get(1)).get("x"))
-        .get("m321"));
-    Assert.assertEquals(311, ((Map) ((Map) RowValue.get(1)).get("x"))
-        .get("m311"));
-    Assert.assertEquals(331, ((Map) ((Map) RowValue.get(1)).get("x"))
-        .get("m331"));
-    Assert.assertEquals(null, ((Map) ((Map) RowValue.get(1)).get("x"))
-        .get("m341"));
-    Assert.assertEquals(null, ((Map) ((Map) RowValue.get(1)).get("z")));
-    Assert.assertEquals(null, ((Map) ((Map) RowValue.get(0)).get("a")));
-    Assert.assertEquals(null, ((Map) ((Map) RowValue.get(0)).get("c")));
-
-    System.out.println("rowValue.get)1): " + RowValue.get(1).toString());
-    // rowValue.get)1): {z=null, x={m311=311, m321=321, m331=331}}
-
-    scanner.advance();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(null, ((Map) RowValue.get(0)).get("b"));
-    Assert.assertEquals(null, ((Map) ((Map) RowValue.get(1)).get("x")));
-    Assert.assertEquals(323, ((Map) ((Map) RowValue.get(1)).get("z"))
-        .get("m323"));
-    Assert.assertEquals(322, ((Map) ((Map) RowValue.get(1)).get("z"))
-        .get("m322"));
-    Assert.assertEquals(321, ((Map) ((Map) RowValue.get(1)).get("z"))
-        .get("m321"));
-    Assert.assertEquals(null, ((Map) ((Map) RowValue.get(0)).get("a")));
-    Assert.assertEquals(null, ((Map) ((Map) RowValue.get(0)).get("b")));
-    Assert.assertEquals(null, ((Map) ((Map) RowValue.get(1)).get("a")));
-
-    reader.close();
-
-  }
-
-  // read one map negative non-exist column
-  @Test
-  public void testRead3() throws IOException, ParseException {
-    String projection = new String("m5");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(null, RowValue.get(0));
-    Assert.assertEquals(1, RowValue.size());
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(null, RowValue.get(0));
-    Assert.assertEquals(1, RowValue.size());
-    reader.close();
-  }
-
-  // Doesn't exist key for all rows
-  @Test
-  public void testRead4() throws IOException, ParseException {
-    String projection = new String("m1#{nonexist}");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    System.out.println("read1 : " + RowValue.toString());
-    Assert.assertEquals("{}", RowValue.get(0).toString());
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    System.out.println(RowValue.get(0).toString());
-    Assert.assertEquals("{}", RowValue.get(0).toString());
-
-    reader.close();
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestMapOfRecord.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestMapOfRecord.java
deleted file mode 100644
index 5fac848f0..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestMapOfRecord.java
+++ /dev/null
@@ -1,391 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Random;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.record.Record;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Projection;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.ExecType;
-import org.apache.pig.PigServer;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.test.MiniCluster;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * 
- * Test projections on complicated column types.
- * 
- */
-public class TestMapOfRecord {
-  final static String STR_SCHEMA = "m1:map(string),m2:map(map(int)), m4:map(map(record(f1:int,f2:string)))";
-  final static String STR_STORAGE = "[m1#{a}];[m2#{x|y}]; [m1#{b}, m2#{z}]; [m4#{a4}]; [m4#{b4|c4}]";
-
-  private static Configuration conf;
-  private static FileSystem fs;
-
-  protected static ExecType execType = ExecType.MAPREDUCE;
-  // private MiniCluster cluster;
-  // protected PigServer pigServer;
-  private static Path path;
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException {
-
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("tabaale.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), "TestMapOfRecord");
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-        STR_STORAGE, conf);
-    writer.finish();
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-    BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-    TypesUtils.resetTuple(tuple);
-
-    Tuple record1;
-    try {
-      record1 = TypesUtils.createTuple(new Schema("f1:int, f2:string"));
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    } 
-
-    Tuple record2;
-    try {
-      record2 = TypesUtils.createTuple(new Schema("f1:int, f2:string"));
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    Tuple record3;
-    try {
-      record3 = TypesUtils.createTuple(new Schema("f1:int, f2:string"));
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    // add data to row 1
-    // m1:map(string)
-    Map<String, String> m1 = new HashMap<String, String>();
-    m1.put("a", "A");
-    m1.put("b", "B");
-    m1.put("c", "C");
-    tuple.set(0, m1);
-
-    // m2:map(map(int))
-    HashMap<String, Map> m2 = new HashMap<String, Map>();
-    Map<String, Integer> m31 = new HashMap<String, Integer>();
-    m31.put("m311", 311);
-    m31.put("m321", 321);
-    m31.put("m331", 331);
-    Map<String, Integer> m32 = new HashMap<String, Integer>();
-    m32.put("m411", 411);
-    m32.put("m421", 421);
-    m32.put("m431", 431);
-    m2.put("x", m31);
-    m2.put("y", m32);
-    tuple.set(1, m2);
-
-    // m4:map(map(record(f1:int,f2:string)))
-    record1.set(0, 11);
-    record1.set(1, "record row 1.1");
-    Map<String, Tuple> m51 = new HashMap<String, Tuple>();
-    Map<String, Tuple> m52 = new HashMap<String, Tuple>();
-    Map<String, Tuple> m53 = new HashMap<String, Tuple>();
-    m51.put("ma4", (Tuple) record1);
-    m52.put("ma41", (Tuple) record1);
-    m53.put("ma43", (Tuple) record1);
-
-    record2.set(0, 12);
-    record2.set(1, "record row 1.2");
-    m51.put("mb4", (Tuple) record2);
-    m52.put("mb42", (Tuple) record2);
-    m53.put("ma43", (Tuple) record2);
-    System.out.println("record1-1: " + record1.toString());
-
-    record3.set(0, 13);
-    record3.set(1, "record row 1.3");
-    System.out.println("record1-3: " + record1.toString());
-
-    m51.put("mc4", (Tuple) record3);
-    m52.put("mc42", (Tuple) record3);
-    m53.put("ma43", (Tuple) record3);
-
-    Map<String, Map> m4 = new HashMap<String, Map>();
-    m4.put("a4", m51);
-    m4.put("b4", m52);
-    m4.put("c4", m53);
-    m4.put("d4", m53);
-    m4.put("ma43", m53);
-
-    tuple.set(2, m4);
-
-    int row = 0;
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // row 2
-    row++;
-    TypesUtils.resetTuple(tuple);
-    TypesUtils.resetTuple(record1);
-    TypesUtils.resetTuple(record2);
-    TypesUtils.resetTuple(record3);
-    m1.clear();
-    m2.clear();
-    m31.clear();
-    m32.clear();
-    m4.clear();
-    m51.clear();
-    m52.clear();
-    m53.clear();
-    // m1:map(string)
-    m1.put("a", "A2");
-    m1.put("b2", "B2");
-    m1.put("c2", "C2");
-    tuple.set(0, m1);
-
-    // m2:map(map(int))
-    m31.put("m321", 321);
-    m31.put("m322", 322);
-    m31.put("m323", 323);
-    m2.put("z", m31);
-    tuple.set(1, m2);
-
-    // m4:map(map(record(f1:int,f2:string)))
-    record1.set(0, 21);
-    record1.set(1, "record row 2.1");
-    m51.put("ma4", (Tuple) record1);
-    m52.put("ma41", (Tuple) record1);
-    m53.put("ma43", (Tuple) record1);
-
-    record2.set(0, 22);
-    record2.set(1, "record row 2.2");
-    m51.put("mb4", (Tuple) record2);
-    m52.put("mb42", (Tuple) record2);
-    m53.put("ma43", (Tuple) record2);
-
-    record3.set(0, 33);
-    record3.set(1, "record row 3.3");
-    m51.put("mc4", (Tuple) record3);
-    m52.put("mc42", (Tuple) record3);
-    m53.put("ma43", (Tuple) record3);
-
-    m4.put("a4", m51);
-    m4.put("b4", m52);
-
-    m4.put("ma43", m53);
-
-    tuple.set(2, m4);
-
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // finish building table, closing out the inserter, writer, writer1
-    inserter.close();
-    writer1.finish();
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-    BasicTable.drop(path, conf);
-  }
-
-  @Test
-  // read one map
-  public void testReadSimpleMap() throws IOException, ParseException {
-    String projection = new String("m1#{a}");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals("{a=A}", RowValue.get(0).toString());
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals("{a=A2}", RowValue.get(0).toString());
-
-    reader.close();
-  }
-
-  @Test
-  // m1:map(string),m2:map(map(int))";
-  // String STR_STORAGE = "[m1#{a}];[m2#{x|y}]; [m1#{b}, m2#{z}]";
-  // read map of map, stitch
-  public void testReadMapOfMap() throws IOException, ParseException {
-    String projection2 = new String("m1#{b}, m2#{x|z}");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection2);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals("B", ((Map) RowValue.get(0)).get("b"));
-    Assert.assertEquals(321, ((Map) ((Map) RowValue.get(1)).get("x"))
-        .get("m321"));
-    Assert.assertEquals(311, ((Map) ((Map) RowValue.get(1)).get("x"))
-        .get("m311"));
-    Assert.assertEquals(331, ((Map) ((Map) RowValue.get(1)).get("x"))
-        .get("m331"));
-    Assert.assertEquals(null, ((Map) ((Map) RowValue.get(1)).get("x"))
-        .get("m341"));
-    Assert.assertEquals(null, ((Map) ((Map) RowValue.get(1)).get("z")));
-    Assert.assertEquals(null, ((Map) ((Map) RowValue.get(0)).get("a")));
-    Assert.assertEquals(null, ((Map) ((Map) RowValue.get(0)).get("c")));
-
-    System.out.println(RowValue.get(1).toString());
-
-    scanner.advance();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(null, ((Map) RowValue.get(0)).get("b"));
-    Assert.assertEquals(null, ((Map) ((Map) RowValue.get(1)).get("x")));
-    Assert.assertEquals(323, ((Map) ((Map) RowValue.get(1)).get("z"))
-        .get("m323"));
-    Assert.assertEquals(322, ((Map) ((Map) RowValue.get(1)).get("z"))
-        .get("m322"));
-    Assert.assertEquals(321, ((Map) ((Map) RowValue.get(1)).get("z"))
-        .get("m321"));
-    Assert.assertEquals(null, ((Map) ((Map) RowValue.get(0)).get("a")));
-    Assert.assertEquals(null, ((Map) ((Map) RowValue.get(0)).get("b")));
-    Assert.assertEquals(null, ((Map) ((Map) RowValue.get(1)).get("a")));
-
-    reader.close();
-
-  }
-
-  // final static String STR_SCHEMA =
-  // "m1:map(string),m2:map(map(int)), m4:map(map(record(f1:int,f2:string)))";
-  // final static String STR_STORAGE =
-  // "[m1#{a}];[m2#{x|y}]; [m1#{b}, m2#{z}]; [m4#{a4}]; [m4#{b4|c4}]";
-  @Test
-  // read map of map, stitch
-  public void testReadMapOfRecord1() throws IOException, ParseException {
-    String projection2 = new String("m1#{b}, m4#{a4|c4}");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection2);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals("B", ((Map) RowValue.get(0)).get("b"));
-    System.out.println(RowValue.get(1).toString());
-
-    Assert.assertEquals(11, ((Tuple) ((Map) ((Map) RowValue.get(1)).get("a4"))
-        .get("ma4")).get(0));
-    Assert.assertEquals("record row 1.1", ((Tuple) ((Map) ((Map) RowValue
-        .get(1)).get("a4")).get("ma4")).get(1));
-    Assert.assertEquals(12, ((Tuple) ((Map) ((Map) RowValue.get(1)).get("a4"))
-        .get("mb4")).get(0));
-    Assert.assertEquals("record row 1.2", ((Tuple) ((Map) ((Map) RowValue
-        .get(1)).get("a4")).get("mb4")).get(1));
-    Assert.assertEquals(13, ((Tuple) ((Map) ((Map) RowValue.get(1)).get("a4"))
-        .get("mc4")).get(0));
-    Assert.assertEquals("record row 1.3", ((Tuple) ((Map) ((Map) RowValue
-        .get(1)).get("a4")).get("mc4")).get(1));
-
-    Assert.assertEquals(13, ((Tuple) ((Map) ((Map) RowValue.get(1)).get("c4"))
-        .get("ma43")).get(0));
-    Assert.assertEquals("record row 1.3", ((Tuple) ((Map) ((Map) RowValue
-        .get(1)).get("c4")).get("ma43")).get(1));
-
-    Assert.assertEquals(null, (((Map) ((Map) RowValue.get(1)).get("c4"))
-        .get("mc4")));
-    Assert.assertEquals(null, (((Map) ((Map) RowValue.get(1)).get("c4"))
-        .get("mb4")));
-
-    scanner.advance();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    System.out.println(RowValue.get(1).toString());
-    Assert.assertEquals(21, ((Tuple) ((Map) ((Map) RowValue.get(1)).get("a4"))
-        .get("ma4")).get(0));
-    Assert.assertEquals("record row 2.1", ((Tuple) ((Map) ((Map) RowValue
-        .get(1)).get("a4")).get("ma4")).get(1));
-    Assert.assertEquals(22, ((Tuple) ((Map) ((Map) RowValue.get(1)).get("a4"))
-        .get("mb4")).get(0));
-    Assert.assertEquals("record row 2.2", ((Tuple) ((Map) ((Map) RowValue
-        .get(1)).get("a4")).get("mb4")).get(1));
-    Assert.assertEquals(33, ((Tuple) ((Map) ((Map) RowValue.get(1)).get("a4"))
-        .get("mc4")).get(0));
-    Assert.assertEquals("record row 3.3", ((Tuple) ((Map) ((Map) RowValue
-        .get(1)).get("a4")).get("mc4")).get(1));
-
-    reader.close();
-
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestMapSplitSchemaStorageColumnOutOfOrder.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestMapSplitSchemaStorageColumnOutOfOrder.java
deleted file mode 100644
index 50eb94d72..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestMapSplitSchemaStorageColumnOutOfOrder.java
+++ /dev/null
@@ -1,319 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownershcolumn3. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import junit.framework.Assert;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * 
- * Test projections on complicated column types: Jira 1026
- * 
- */
-public class TestMapSplitSchemaStorageColumnOutOfOrder {
-  final static String STR_SCHEMA = "column1:bytes,column2:bytes, column3:bytes,column4:bytes,column5:map(String),column6:map(String),column7:map(String),column8:collection(record(f1:map(String)))";   
-  final static String STR_STORAGE = "[column1,column2,column3,column4];[column5#{key51|key52|key53|key54|key55|key56},column7#{key71|key72|key73|key74|key75}];[column5,column7,column6];[column8]";
-  private static Configuration conf;
-  private static Path path;
-  private static FileSystem fs;
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException {
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), "TestMapSplitSchemaStorageColumnOutOfOrder");
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-        STR_STORAGE, conf);
-    writer.finish();
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-    BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-    TypesUtils.resetTuple(tuple);
-
-    DataBag bag2 = TypesUtils.createBag();
-    Schema schColl2 = schema.getColumn(7).getSchema();
-    Tuple tupColl2_1 = TypesUtils.createTuple(schColl2);
-    Tuple tupColl2_2 = TypesUtils.createTuple(schColl2);
-    // add data to row 1
-    tuple.set(0, new DataByteArray("column1 row 1 ")); // byte
-    tuple.set(1,new DataByteArray("column2 row 1"));
-    tuple.set(2, new DataByteArray("column3 row 1"));
-    tuple.set(3, new DataByteArray("column4 row 1"));
-    
-    // column5
-    Map<String, String> column5 = new HashMap<String, String>();
-    column5.put("key51", "key511");
-    column5.put("key52", "key521");
-    column5.put("key53", "key531");
-    column5.put("key54", "key541");
-    column5.put("key55", "key551");
-    column5.put("key56", "key561");
-    column5.put("key57", "key571");
-   
-    tuple.set(4, column5);
-
-    //column5:map(bytes),column6:map(bytes),column7:map(bytes),column8:collection(f1:map(bytes)
-    //column7:map(String, column6:map(String)
-    HashMap<String, String> column7 = new HashMap<String, String>();
-    HashMap<String, String> column6 = new HashMap<String, String>();
-    column6.put("column61", "column61");
-    column7.put("key71", "key711");
-    column7.put("key72", "key721");
-    column7.put("key73", "key731");
-    column7.put("key74", "key741");
-    column7.put("key75", "key751");
-    tuple.set(6, column7);
-    tuple.set(5, column6);
-    
-    //column8:collection(f1:map(bytes))
-    HashMap<String, String> mapInCollection = new HashMap<String, String>();
-    mapInCollection.put("mc", "mc1");
-    tupColl2_1.set(0, mapInCollection);
-    bag2.add(tupColl2_1);
-
-    tupColl2_2.set(0, mapInCollection);
-    bag2.add(tupColl2_2);
-    tuple.set(7, bag2);
-
-    int row = 0;
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // row 2
-    row++;
-    TypesUtils.resetTuple(tuple);
-    column5.clear();
-    column7.clear();
-    column6.clear();
-    mapInCollection.clear();
-    bag2.clear();
-    TypesUtils.resetTuple(tupColl2_1);
-    TypesUtils.resetTuple(tupColl2_2);
-    
-    tuple.set(0, new DataByteArray("column1 row 2 ")); // byte
-    tuple.set(1,new DataByteArray("column2 row 2"));
-    tuple.set(2, new DataByteArray("column3 row 2"));
-    tuple.set(3, new DataByteArray("column4 row 2"));
-    
-    // column5
-    column5.put("key51", "key512");
-    column5.put("key52", "key522");
-    column5.put("key53", "key532");
-    column5.put("key54", "key542");
-    column5.put("key55", "key552");
-    column5.put("key56", "key562");
-    column5.put("key57", "key572");
-    tuple.set(4, column5);
-
-    // column6
-  
-    column6.put("column6", "column62");
-    column7.put("key71", "key712");
-    column7.put("key72", "key722");
-    column7.put("key73", "key732");
-    column7.put("key74", "key742");
-    column7.put("key75", "key752");
-    tuple.set(6, column7);
-    tuple.set(5, column6);
-    
-    
-    //column8
-    //column8:collection(f1:map(bytes))
-    mapInCollection.put("mc", "mc2");
-    tupColl2_1.set(0, mapInCollection);
-    bag2.add(tupColl2_1);
-
-    tupColl2_2.set(0, mapInCollection);
-    bag2.add(tupColl2_2);
-    tuple.set(7, bag2);
-
-    
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // finish building table, closing out the inserter, writer, writer1
-    inserter.close();
-    writer1.finish();
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-    BasicTable.drop(path, conf);
-  }
-
- 
-  @Test
-  public void testRead1() throws IOException, ParseException {
-    /*
-     * read one map
-     *  column5.put("key51", "key511");
-     */
-    String projection = new String("column5#{key51}");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    System.out.println("read1 : " + RowValue.toString());
-    Assert.assertEquals("{key51=key511}", RowValue.get(0).toString());
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    System.out.println(RowValue.get(0).toString());
-    Assert.assertEquals("{key51=key512}", RowValue.get(0).toString());
-
-    reader.close();
-  }
-
- 
-  @Test
-  public void testRead2() throws IOException, ParseException {
-    /*
-     * read map , stitch
-     * [column5#{key51|key52|key53|key54|key55|key56},column7#{key71|key72|key73|key74|key75}];[column5,column7,column6]
-     */
-    String projection2 = new String("column5#{new}, column7#{key71|ytestid}");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-//    reader.setProjection(projection2);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    System.out.println("map of map: " + RowValue.toString());
-    Assert.assertEquals("key571", ((Map) RowValue.get(4)).get("key57"));
-    Assert.assertEquals("key711", ((Map) RowValue.get(6)).get("key71"));
-       
-    Assert.assertEquals(null, (((Map) RowValue.get(6)).get("ytestid")));
-    Assert.assertEquals(null, ((Map) ((Map) RowValue.get(6)).get("x")));
-    System.out.println("rowValue.get)1): " + RowValue.get(6).toString());
-    // rowValue.get)1): {z=null, x={m311=311, m321=321, m331=331}}
-
-    scanner.advance();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals("key572", ((Map) RowValue.get(4)).get("key57"));
-    Assert.assertEquals("key712", ((Map) RowValue.get(6)).get("key71"));
-       
-    Assert.assertEquals("key722", ((Map) RowValue.get(6)).get("key72"));
-    Assert.assertEquals(null, ((Map) ((Map) RowValue.get(6)).get("x")));
-    reader.close();
-
-  }
-
-  @Test
-  public void testRead3() throws IOException, ParseException {
-    /*
-     * negative , read one map who is non-exist 
-     */
-    String projection = new String("m5");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(null, RowValue.get(0));
-    Assert.assertEquals(1, RowValue.size());
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(null, RowValue.get(0));
-    Assert.assertEquals(1, RowValue.size());
-    reader.close();
-  }
-
-  @Test
-  public void testRead4() throws IOException, ParseException {
-    /*
-     *  Not exist key for all rows
-     */
-    String projection = new String("column5#{nonexist}");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    System.out.println("read1 : " + RowValue.toString());
-    Assert.assertEquals("{}", RowValue.get(0).toString());
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    System.out.println(RowValue.get(0).toString());
-    Assert.assertEquals("{}", RowValue.get(0).toString());
-
-    reader.close();
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestMixedType1.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestMixedType1.java
deleted file mode 100644
index 1a57c39be..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestMixedType1.java
+++ /dev/null
@@ -1,313 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Random;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Projection;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * 
- * Test projections on complicated column types.
- * 
- */
-public class TestMixedType1 {
-  final static String STR_SCHEMA = "s1:bool, s2:int, s3:long, s4:double, s5:string, s6:bytes, r1:record(f1:int, f2:long), r2:record(r3:record(f3:double, f4)), m1:map(string),m2:map(map(int)), c:collection(record(f13:double, f14:double, f15:bytes))";
-  final static String STR_STORAGE = "[s1, s2]; [m1#{a}]; [r1.f1]; [s3, s4, r2.r3.f3]; [s5, s6, m2#{x|y}]; [r1.f2, m1#{b}]; [r2.r3.f4, m2#{z}]";
-  private static Configuration conf;
-  private static Path path;
-  private static FileSystem fs;
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException {
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), "TestMixedType1");
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-
-    BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-        STR_STORAGE, conf);
-    writer.finish();
-
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-    BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-    TypesUtils.resetTuple(tuple);
-
-    Tuple tupRecord1;
-    try {
-      tupRecord1 = TypesUtils.createTuple(schema.getColumnSchema("r1")
-          .getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    Tuple tupRecord2;
-    try {
-      tupRecord2 = TypesUtils.createTuple(schema.getColumnSchema("r2")
-          .getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    Tuple tupRecord3;
-    try {
-      tupRecord3 = TypesUtils.createTuple(new Schema("f3:float, f4"));
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    // row 1
-    tuple.set(0, true); // bool
-    tuple.set(1, 1); // int
-    tuple.set(2, 1001L); // long
-    tuple.set(3, 1.1); // float
-    tuple.set(4, "hello world 1"); // string
-    tuple.set(5, new DataByteArray("hello byte 1")); // byte
-
-    // r1:record(f1:int, f2:long
-    tupRecord1.set(0, 1);
-    tupRecord1.set(1, 1001L);
-    tuple.set(6, tupRecord1);
-
-    // r2:record(r3:record(f3:float, f4))
-    tupRecord2.set(0, tupRecord3);
-    tupRecord3.set(0, 1.3);
-    tupRecord3.set(1, new DataByteArray("r3 row 1 byte array "));
-    tuple.set(7, tupRecord2);
-
-    // m1:map(string)
-    Map<String, String> m1 = new HashMap<String, String>();
-    m1.put("a", "A");
-    m1.put("b", "B");
-    m1.put("c", "C");
-    tuple.set(8, m1);
-
-    // m2:map(map(int))
-    HashMap<String, Map> m2 = new HashMap<String, Map>();
-    Map<String, Integer> m3 = new HashMap<String, Integer>();
-    m3.put("m311", 311);
-    m3.put("m321", 321);
-    m3.put("m331", 331);
-    Map<String, Integer> m4 = new HashMap<String, Integer>();
-    m4.put("m411", 411);
-    m4.put("m421", 421);
-    m4.put("m431", 431);
-    m2.put("x", m3);
-    m2.put("y", m4);
-    tuple.set(9, m2);
-
-    // c:collection(f13:double, f14:float, f15:bytes)
-    DataBag bagColl = TypesUtils.createBag();
-    Schema schColl = schema.getColumn(10).getSchema().getColumn(0).getSchema();
-    Tuple tupColl1 = TypesUtils.createTuple(schColl);
-    Tuple tupColl2 = TypesUtils.createTuple(schColl);
-    byte[] abs1 = new byte[3];
-    byte[] abs2 = new byte[4];
-    tupColl1.set(0, 3.1415926);
-    tupColl1.set(1, 1.6);
-    abs1[0] = 11;
-    abs1[1] = 12;
-    abs1[2] = 13;
-    tupColl1.set(2, new DataByteArray(abs1));
-    bagColl.add(tupColl1);
-    tupColl2.set(0, 123.456789);
-    tupColl2.set(1, 100);
-    abs2[0] = 21;
-    abs2[1] = 22;
-    abs2[2] = 23;
-    abs2[3] = 24;
-    tupColl2.set(2, new DataByteArray(abs2));
-    bagColl.add(tupColl2);
-    tuple.set(10, bagColl);
-
-    int row = 0;
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // row 2
-    row++;
-    TypesUtils.resetTuple(tuple);
-    TypesUtils.resetTuple(tupRecord1);
-    TypesUtils.resetTuple(tupRecord2);
-    TypesUtils.resetTuple(tupRecord3);
-    m1.clear();
-    m2.clear();
-    m3.clear();
-    m4.clear();
-    tuple.set(0, false);
-    tuple.set(1, 2); // int
-    tuple.set(2, 1002L); // long
-    tuple.set(3, 3.1); // float
-    tuple.set(4, "hello world 2"); // string
-    tuple.set(5, new DataByteArray("hello byte 2")); // byte
-
-    // r1:record(f1:int, f2:long
-    tupRecord1.set(0, 2);
-    tupRecord1.set(1, 1002L);
-    tuple.set(6, tupRecord1);
-
-    // r2:record(r3:record(f3:float, f4))
-    tupRecord2.set(0, tupRecord3);
-    tupRecord3.set(0, 2.3);
-    tupRecord3.set(1, new DataByteArray("r3 row2  byte array"));
-    tuple.set(7, tupRecord2);
-
-    // m1:map(string)
-    m1.put("a2", "A2");
-    m1.put("b2", "B2");
-    m1.put("c2", "C2");
-    tuple.set(8, m1);
-
-    // m2:map(map(int))
-    m3.put("m321", 321);
-    m3.put("m322", 322);
-    m3.put("m323", 323);
-    m2.put("z", m3);
-    tuple.set(9, m2);
-
-    // c:collection(f13:double, f14:float, f15:bytes)
-    bagColl.clear();
-    TypesUtils.resetTuple(tupColl1);
-    TypesUtils.resetTuple(tupColl2);
-    tupColl1.set(0, 7654.321);
-    tupColl1.set(1, 0.0001);
-    abs1[0] = 31;
-    abs1[1] = 32;
-    abs1[2] = 33;
-    tupColl1.set(2, new DataByteArray(abs1));
-    bagColl.add(tupColl1);
-    tupColl2.set(0, 0.123456789);
-    tupColl2.set(1, 0.3333);
-    abs2[0] = 41;
-    abs2[1] = 42;
-    abs2[2] = 43;
-    abs2[3] = 44;
-    tupColl2.set(2, new DataByteArray(abs2));
-    bagColl.add(tupColl2);
-    tuple.set(10, bagColl);
-
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    inserter.close();
-    writer1.finish();
-
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-  }
-
-  @Test
-  public void test1() throws IOException, ParseException {
-    String projection = new String("r1.f2, s1");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    // long totalBytes = reader.getStatus().getSize();
-
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    reader.close();
-    reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple value = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(value);
-
-    Assert.assertEquals(1001L, value.get(0));
-    Assert.assertEquals(true, value.get(1));
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(value);
-    Assert.assertEquals(1002L, value.get(0));
-    Assert.assertEquals(false, value.get(1));
-
-    reader.close();
-  }
-
-  @Test
-  public void testStitch() throws IOException, ParseException {
-    String projection = new String("s1, r1");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    // long totalBytes = reader.getStatus().getSize();
-
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    reader.close();
-    reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple value = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(value);
-
-    Tuple recordTuple = (Tuple) value.get(1);
-    Assert.assertEquals(1, recordTuple.get(0));
-    Assert.assertEquals(1001L, recordTuple.get(1));
-    Assert.assertEquals(true, value.get(0));
-    reader.close();
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestNegative.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestNegative.java
deleted file mode 100644
index c36ad5017..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestNegative.java
+++ /dev/null
@@ -1,646 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-import java.util.Random;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Projection;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * 
- * Test projections on complicated column types.
- * 
- */
-public class TestNegative {
-  private static Configuration conf;
-  private static Path path;
-  private static FileSystem fs;
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException {
-
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-    BasicTable.drop(path, conf);
-  }
-
-  // Negative test case. For record split, we should not try to store same
-  // record field on different column groups.
-  @Test
-  public void testWriteRecord5() throws IOException, ParseException {
-    String STR_SCHEMA = "r1:record(f1:int, f2:long), r2:record(r3:record(f3:float, f4))";
-    String STR_STORAGE = "[r1.f1]; [r2.r3]; [r1.f2, r2.r3.f3]";
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), this.getClass().getSimpleName());
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    // Build Table and column groups
-    BasicTable.Writer writer = null;
-    try {
-      writer = new BasicTable.Writer(path, STR_SCHEMA, STR_STORAGE, conf);
-      Assert.fail("Should throw exception");
-    } catch (Exception e) {
-      System.out.println(e);
-    }
-  }
-
-//Negative test case. For record split, we should not try to store same
-  // record field on different column groups.
-  @Test
-  public void testWriteRecord6() throws IOException, ParseException {
-    String STR_SCHEMA = "r1:record(f1:int, f2:long), r2:record(r3:record(f3:float, f4))";
-    String STR_STORAGE = "[r1.f1]; [r1.f2, r2.r3.f3]; [r2.r3]";
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), this.getClass().getSimpleName());
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    // Build Table and column groups
-    BasicTable.Writer writer = null;
-    try {
-      writer = new BasicTable.Writer(path, STR_SCHEMA, STR_STORAGE, conf);
-      Assert.fail("Should throw exception");
-    } catch (Exception e) {
-      System.out.println(e);
-    }
-  }
-  
-  // Negative test case. map storage syntax is wrong
-  @Test
-  public void testWriteMap1() throws IOException, ParseException {
-    String STR_SCHEMA = " m2:map(map(map(string)))";
-    String STR_STORAGE = "[m2#{k}#{j}]";
-
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), this.getClass().getSimpleName());
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    // Build Table and column groups
-    BasicTable.Writer writer = null;
-    try {
-      writer = new BasicTable.Writer(path, STR_SCHEMA, STR_STORAGE, conf);
-      Assert.fail("Should throw exception");
-    } catch (Exception e) {
-      System.out.println(e);
-    }
-  }
-
-  // Negative test case. map storage syntax is wrong
-  @Test
-  public void testWriteMap2() throws IOException, ParseException {
-    String STR_SCHEMA = " m2:map(map(map(string)))";
-    String STR_STORAGE = "[m2.{k}]";
-
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), this.getClass().getSimpleName());
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    // Build Table and column groups
-    BasicTable.Writer writer = null;
-    try {
-      writer = new BasicTable.Writer(path, STR_SCHEMA, STR_STORAGE, conf);
-      Assert.fail("Should throw exception");
-    } catch (Exception e) {
-      System.out.println(e);
-    }
-  }
-
-  // Negative test case. map storage syntax is wrong
-  @Test
-  public void testWriteMap3() throws IOException, ParseException {
-    String STR_SCHEMA = " m2:map(map(map(string)))";
-    String STR_STORAGE = "[m2{k}]";
-
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), this.getClass().getSimpleName());
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    // Build Table and column groups
-    BasicTable.Writer writer = null;
-    try {
-      writer = new BasicTable.Writer(path, STR_SCHEMA, STR_STORAGE, conf);
-      Assert.fail("Should throw exception");
-    } catch (Exception e) {
-      System.out.println(e);
-    }
-  }
-
-  // Negative test case. map storage syntax is wrong
-  @Test
-  public void testWriteMap4() throws IOException, ParseException {
-    String STR_SCHEMA = " m2:map(map(map(string)))";
-    String STR_STORAGE = "[m2#{k}";
-
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), this.getClass().getSimpleName());
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    // Build Table and column groups
-    BasicTable.Writer writer = null;
-    try {
-      writer = new BasicTable.Writer(path, STR_SCHEMA, STR_STORAGE, conf);
-      Assert.fail("Should throw exception");
-    } catch (Exception e) {
-      System.out.println(e);
-    }
-  }
-
-  // Negative test case. map schema syntax is wrong
-  @Test
-  public void testWriteMap5() throws IOException, ParseException {
-    String STR_SCHEMA = " m2:map(map(map(string,string,string)))";
-    String STR_STORAGE = "[m2#{k}]";
-
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), this.getClass().getSimpleName());
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    // Build Table and column groups
-    BasicTable.Writer writer = null;
-    try {
-      writer = new BasicTable.Writer(path, STR_SCHEMA, STR_STORAGE, conf);
-      Assert.fail("Should throw exception");
-    } catch (Exception e) {
-      System.out.println(e);
-    }
-  }
-
-  // Negative test case. map storage syntax is wrong.
-  @Test
-  public void testWriteMap6() throws IOException, ParseException {
-    String STR_SCHEMA = " m2:map(map(map(string)))";
-    String STR_STORAGE = "[m2#k#k1]";
-
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), this.getClass().getSimpleName());
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    // Build Table and column groups
-    BasicTable.Writer writer = null;
-    try {
-      writer = new BasicTable.Writer(path, STR_SCHEMA, STR_STORAGE, conf);
-      Assert.fail("Should throw exception");
-    } catch (Exception e) {
-      System.out.println(e);
-    }
-  }
-
-  // negative, map storage syntax is wrong
-  @Test
-  public void testWriteMap7() throws IOException, ParseException {
-    String STR_SCHEMA = " m2:map(map(map(string)))";
-    String STR_STORAGE = "[m2#k]";
-
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), this.getClass().getSimpleName());
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    // Build Table and column groups
-    BasicTable.Writer writer = null;
-    try {
-      writer = new BasicTable.Writer(path, STR_SCHEMA, STR_STORAGE, conf);
-      Assert.fail("Should throw exception");
-    } catch (Exception e) {
-      System.out.println(e);
-    }
-  }
-
-  // negative, should not take same field name
-  @Test
-  public void testWriteRecord1() throws IOException, ParseException {
-    String STR_SCHEMA = " r1:record(f1,f2), r1:record(f1,f2)";
-    String STR_STORAGE = "[r1]";
-
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), this.getClass().getSimpleName());
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    // Build Table and column groups
-    BasicTable.Writer writer = null;
-    try {
-      writer = new BasicTable.Writer(path, STR_SCHEMA, STR_STORAGE, conf);
-      Assert.fail("Should throw exception");
-    } catch (Exception e) {
-      System.out.println(e);
-    }
-  }
-
-  // negative, duplicate column storage
-  @Test
-  public void testWriteRecord2() throws IOException, ParseException {
-    String STR_SCHEMA = " r1:record(f1,f2), r2:record(f1,f2)";
-    String STR_STORAGE = "[r1,r1]";
-
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), this.getClass().getSimpleName());
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    // Build Table and column groups
-    BasicTable.Writer writer = null;
-    try {
-      writer = new BasicTable.Writer(path, STR_SCHEMA, STR_STORAGE, conf);
-      Assert.fail("Should throw exception");
-    } catch (Exception e) {
-      System.out.println(e);
-    }
-  }
-
-  // negative, duplicate column storage
-  @Test
-  public void testWriteRecord3() throws IOException, ParseException {
-    String STR_SCHEMA = " r1:record(f1,f2), r2:record(f1,f2)";
-    String STR_STORAGE = "[r1.f1, r2]; [r1.f1,r2]";
-
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), this.getClass().getSimpleName());
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    // Build Table and column groups
-    BasicTable.Writer writer = null;
-    try {
-      writer = new BasicTable.Writer(path, STR_SCHEMA, STR_STORAGE, conf);
-      Assert.fail("Should throw exception");
-    } catch (Exception e) {
-      System.out.println(e);
-    }
-  }
-
-  // negative, duplicate column storage
-  @Test
-  public void testWriteRecord4() throws IOException, ParseException {
-    String STR_SCHEMA = " r1:record(f1,f2), r2:record(f1,f2)";
-    String STR_STORAGE = "[r1.f1]; [r1.f1,r2]";
-
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), this.getClass().getSimpleName());
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    // Build Table and column groups
-    BasicTable.Writer writer = null;
-    try {
-      writer = new BasicTable.Writer(path, STR_SCHEMA, STR_STORAGE, conf);
-      Assert.fail("Should throw exception");
-    } catch (Exception e) {
-      System.out.println(e);
-    }
-  }
-
-  // negative, null column storage
-  @Test
-  public void testWriteNull5() throws IOException, ParseException {
-    String STR_SCHEMA = " r1:record(f1,f2), r2:record(f1,f2)";
-
-    String STR_STORAGE = null;
-
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), this.getClass().getSimpleName());
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    // Build Table and column groups
-    BasicTable.Writer writer = null;
-    try {
-      writer = new BasicTable.Writer(path, STR_SCHEMA, STR_STORAGE, conf);
-      System.out.println("HERE HERE");
-      Assert.fail("Should throw exception");
-    } catch (Exception e) {
-      e.printStackTrace();
-    }
-  }
-
-  // empty column group
-  @Test
-  public void testWriteEmpty6() throws IOException, ParseException {
-    String STR_SCHEMA = "f1:int, f2:string";
-
-    String STR_STORAGE = "";
-
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), this.getClass().getSimpleName());
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    // Build Table and column groups
-    BasicTable.Writer writer = null;
-    try {
-      writer = new BasicTable.Writer(path, STR_SCHEMA, STR_STORAGE, conf);
-    } catch (Exception e) {
-      e.printStackTrace();
-      Assert.fail("Should Not throw exception");
-    }
-    writer.finish();
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-    BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-    TypesUtils.resetTuple(tuple);
-
-    // insert data in row 1
-    int row = 0;
-    tuple.set(0, 1);
-    tuple.set(1, "hello1");
-
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // row 2
-    row++;
-    TypesUtils.resetTuple(tuple);
-
-    tuple.set(0, 2);
-    tuple.set(1, "hello2");
-
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // finish building table, closing out the inserter, writer, writer1
-    inserter.close();
-    writer1.finish();
-    writer.close();
-
-    String projection3 = new String("f1,f2");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection3);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-
-    Assert.assertEquals(1, RowValue.get(0));
-    Assert.assertEquals("hello1", RowValue.get(1));
-
-    scanner.advance();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(2, RowValue.get(0));
-    Assert.assertEquals("hello2", RowValue.get(1));
-
-    reader.close();
-
-  }
-
-  // Positive test case. map storage , [m2] will storage everything besides k1
-  @Test
-  public void testMapWrite8() throws IOException, ParseException {
-    String STR_SCHEMA = " m2:map(map(map(string))), m1:map(int)";
-    String STR_STORAGE = "[m2#{k1}];[m2]";
-
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), this.getClass().getSimpleName());
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    // Build Table and column groups
-    BasicTable.Writer writer = null;
-    try {
-      writer = new BasicTable.Writer(path, STR_SCHEMA, STR_STORAGE, conf);
-    } catch (Exception e) {
-      System.out.println(e);
-      Assert.fail("Should throw exception");
-    }
-  }
-
-  // Negative test case.duplicate map storage
-  @Test
-  public void testMapWrite9() throws IOException, ParseException {
-    String STR_SCHEMA = " m2:map(map(map(string))), m1:map(int)";
-    String STR_STORAGE = "[m2#{k1}], [m2#{k1}]";
-
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), this.getClass().getSimpleName());
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    // Build Table and column groups
-    BasicTable.Writer writer = null;
-    try {
-      writer = new BasicTable.Writer(path, STR_SCHEMA, STR_STORAGE, conf);
-      Assert.fail("Should throw exception");
-    } catch (Exception e) {
-      System.out.println(e);
-    }
-  }
-
-  // Positive test case.duplicate map storage, TODO: why failed?
-
-  public void xtestMapWrite10() throws IOException, ParseException {
-    String STR_SCHEMA = " m2:map(map(map(string))), m1:map(int)";
-    String STR_STORAGE = "[m2#{k1}]; [m2#{k2}]";
-
-    Configuration conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    FileSystem fs = new LocalFileSystem(rawLFS);
-    Path path = new Path(fs.getWorkingDirectory(), this.getClass()
-        .getSimpleName());
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    // Build Table and column groups
-    BasicTable.Writer writer = null;
-    try {
-      writer = new BasicTable.Writer(path, STR_SCHEMA, STR_STORAGE, conf);
-    } catch (Exception e) {
-      e.printStackTrace();
-      Assert.fail("Should Not throw exception");
-    }
-
-  }
-
-  // negative, schema, same field name, different type r1:record(f1:int,
-  // f2:long).
-  @Test
-  public void testColumnField5() throws IOException, ParseException {
-    String STR_SCHEMA = "r1:int, r1:float";
-    String STR_STORAGE = "[r1]";
-
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), this.getClass().getSimpleName());
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    // Build Table and column groups
-    BasicTable.Writer writer = null;
-    try {
-      writer = new BasicTable.Writer(path, STR_SCHEMA, STR_STORAGE, conf);
-      Assert.fail("Should throw exception");
-    } catch (Exception e) {
-      System.out.println(e);
-    }
-  }
-
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestNonDefaultWholeMapSplit.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestNonDefaultWholeMapSplit.java
deleted file mode 100644
index 8ea28e565..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestNonDefaultWholeMapSplit.java
+++ /dev/null
@@ -1,281 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import junit.framework.Assert;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * 
- * Test projections on complicated column types.
- * 
- */
-public class TestNonDefaultWholeMapSplit {
-
-  final static String STR_SCHEMA = "m1:map(string),m2:map(map(int))";
-  //the comment STR_STORAGE is the one for jira 949 
-  final static String STR_STORAGE = "[m1#{a}];[m2#{x|y}]; [m1#{b}, m2#{z}];[m1]";
-  private static Configuration conf;
-  private static Path path;
-  private static FileSystem fs;
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException {
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), "TestMap");
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-        STR_STORAGE, conf);
-    writer.finish();
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-    BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-    TypesUtils.resetTuple(tuple);
-
-    // add data to row 1
-    // m1:map(string)
-    Map<String, String> m1 = new HashMap<String, String>();
-    m1.put("a", "A");
-    m1.put("b", "B");
-    m1.put("c", "C");
-    tuple.set(0, m1);
-
-    // m2:map(map(int))
-    HashMap<String, Map> m2 = new HashMap<String, Map>();
-    Map<String, Integer> m3 = new HashMap<String, Integer>();
-    m3.put("m311", 311);
-    m3.put("m321", 321);
-    m3.put("m331", 331);
-    Map<String, Integer> m4 = new HashMap<String, Integer>();
-    m4.put("m411", 411);
-    m4.put("m421", 421);
-    m4.put("m431", 431);
-    m2.put("x", m3);
-    m2.put("y", m4);
-    tuple.set(1, m2);
-    int row = 0;
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // row 2
-    row++;
-    TypesUtils.resetTuple(tuple);
-    m1.clear();
-    m2.clear();
-    m3.clear();
-    m4.clear();
-    // m1:map(string)
-    m1.put("a", "A2");
-    m1.put("b2", "B2");
-    m1.put("c2", "C2");
-    tuple.set(0, m1);
-
-    // m2:map(map(int))
-    m3.put("m321", 321);
-    m3.put("m322", 322);
-    m3.put("m323", 323);
-    m2.put("z", m3);
-    tuple.set(1, m2);
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // finish building table, closing out the inserter, writer, writer1
-    inserter.close();
-    writer1.finish();
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-    BasicTable.drop(path, conf);
-  }
-
- 
-  @Test
-  public void testRead1() throws IOException, ParseException {
-    /*
-     * read one map
-     */
-    String projection = new String("m1#{a}");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    System.out.println("read1 : " + RowValue.toString());
-    Assert.assertEquals("{a=A}", RowValue.get(0).toString());
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    System.out.println(RowValue.get(0).toString());
-    Assert.assertEquals("{a=A2}", RowValue.get(0).toString());
-
-    reader.close();
-  }
-
- 
-  @Test
-  public void testRead2() throws IOException, ParseException {
-    /*
-     * read map of map, stitch
-     */
-    String projection2 = new String("m1#{b}, m2#{x|z}");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection2);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    System.out.println("map of map: " + RowValue.toString());
-    // map of map: ([b#B],[z#,x#{m311=311, m321=321, m331=331}])
-    Assert.assertEquals("B", ((Map) RowValue.get(0)).get("b"));
-    Assert.assertEquals(321, ((Map) ((Map) RowValue.get(1)).get("x"))
-        .get("m321"));
-    Assert.assertEquals(311, ((Map) ((Map) RowValue.get(1)).get("x"))
-        .get("m311"));
-    Assert.assertEquals(331, ((Map) ((Map) RowValue.get(1)).get("x"))
-        .get("m331"));
-    Assert.assertEquals(null, ((Map) ((Map) RowValue.get(1)).get("x"))
-        .get("m341"));
-    Assert.assertEquals(null, ((Map) ((Map) RowValue.get(1)).get("z")));
-    Assert.assertEquals(null, ((Map) ((Map) RowValue.get(0)).get("a")));
-    Assert.assertEquals(null, ((Map) ((Map) RowValue.get(0)).get("c")));
-
-    System.out.println("rowValue.get)1): " + RowValue.get(1).toString());
-    // rowValue.get)1): {z=null, x={m311=311, m321=321, m331=331}}
-
-    scanner.advance();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(null, ((Map) RowValue.get(0)).get("b"));
-    Assert.assertEquals(null, ((Map) ((Map) RowValue.get(1)).get("x")));
-    Assert.assertEquals(323, ((Map) ((Map) RowValue.get(1)).get("z"))
-        .get("m323"));
-    Assert.assertEquals(322, ((Map) ((Map) RowValue.get(1)).get("z"))
-        .get("m322"));
-    Assert.assertEquals(321, ((Map) ((Map) RowValue.get(1)).get("z"))
-        .get("m321"));
-    Assert.assertEquals(null, ((Map) ((Map) RowValue.get(0)).get("a")));
-    Assert.assertEquals(null, ((Map) ((Map) RowValue.get(0)).get("b")));
-    Assert.assertEquals(null, ((Map) ((Map) RowValue.get(1)).get("a")));
-
-    reader.close();
-
-  }
-
-  
-  @Test
-  public void testRead3() throws IOException, ParseException {
-    /*
-     * negative , read one map who is non-exist 
-     */
-    String projection = new String("m5");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(null, RowValue.get(0));
-    Assert.assertEquals(1, RowValue.size());
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(null, RowValue.get(0));
-    Assert.assertEquals(1, RowValue.size());
-    reader.close();
-  }
-
- 
-  @Test
-  public void testRead4() throws IOException, ParseException {
-    /*
-     *  Not exist key for all rows
-     */
-    String projection = new String("m1#{nonexist}");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    System.out.println("read1 : " + RowValue.toString());
-    Assert.assertEquals("{}", RowValue.get(0).toString());
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    System.out.println(RowValue.get(0).toString());
-    Assert.assertEquals("{}", RowValue.get(0).toString());
-
-    reader.close();
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestProjectionOnFullMap.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestProjectionOnFullMap.java
deleted file mode 100644
index 4f4908cff..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestProjectionOnFullMap.java
+++ /dev/null
@@ -1,137 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.IOException;
-import java.io.PrintStream;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * 
- * Test projections on complicated column types.
- * 
- */
-public class TestProjectionOnFullMap {
-  final static String STR_SCHEMA = "f1:string, f2:map(string)";
-  final static String STR_STORAGE = "[f1]; [f2]";
-  private static Configuration conf;
-  private static Path path;
-  private static FileSystem fs;
-
-  @BeforeClass
-  public static void setUp() throws IOException {
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), "TestBasicTableMapSplits");
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-
-    BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-        STR_STORAGE, conf);
-    writer.finish();
-
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-    TypesUtils.resetTuple(tuple);
-
-    tuple.set(0, "Kitty");
-
-   
-    Map<String, String> map = new HashMap<String, String>();
-    map.put("a", "x");
-    map.put("b", "y");
-    map.put("c", "z");
-    tuple.set(1, map);
-
-    int row = 0;
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-    inserter.close();
-    writer1.finish();
-
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-  }
-
-  public void testDescribse() throws IOException {
-
-    BasicTable.dumpInfo(path.toString(), System.out, conf);
-
-  }
-
-  @Test
-  public void test1() throws IOException, ParseException {
-    String projection = new String("f2#{a}");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    // long totalBytes = reader.getStatus().getSize();
-
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    reader.close();
-    reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple value = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(value);
-
-    Assert.assertEquals("x", ((Map)value.get(0)).get("a"));
-    reader.close();
-  }
-
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestRecord.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestRecord.java
deleted file mode 100644
index 94e5b9f53..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestRecord.java
+++ /dev/null
@@ -1,415 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-import java.util.Random;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Projection;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * 
- * Test projections on complicated column types.
- * 
- */
-public class TestRecord {
-  private static Configuration conf;
-  private static Path path;
-  private static FileSystem fs;
-  static int count;
-  final static String STR_SCHEMA = "r1:record(f1:int, f2:long), r2:record(f5:string, r3:record(f3:double, f4))";
-  final static String STR_STORAGE = "[r1.f1]; [r2.r3.f4]; [r1.f2, r2.r3.f3]";
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException {
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), "TestRecord");
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    // Build Table and column groups
-    BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-        STR_STORAGE, conf);
-    writer.finish();
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-    BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-    TypesUtils.resetTuple(tuple);
-
-    Tuple tupRecord1;
-    try {
-      tupRecord1 = TypesUtils.createTuple(schema.getColumnSchema("r1")
-          .getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    Tuple tupRecord2;
-    try {
-      tupRecord2 = TypesUtils.createTuple(schema.getColumnSchema("r2")
-          .getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    Tuple tupRecord3;
-    try {
-      tupRecord3 = TypesUtils.createTuple(new Schema("f3:float, f4"));
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    // insert data in row 1
-    int row = 0;
-    // r1:record(f1:int, f2:long
-    tupRecord1.set(0, 1);
-    tupRecord1.set(1, 1001L);
-    tuple.set(0, tupRecord1);
-
-    // r2:record(r3:record(f3:float, f4))
-    tupRecord2.set(0, "f5 row1 byte array");
-
-    tupRecord3.set(0, 1.3);
-    tupRecord3.set(1, new DataByteArray("r3 row1 byte array"));
-    tupRecord2.set(1, tupRecord3);
-    tuple.set(1, tupRecord2);
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // row 2
-    row++;
-    TypesUtils.resetTuple(tuple);
-    TypesUtils.resetTuple(tupRecord1);
-    TypesUtils.resetTuple(tupRecord2);
-    TypesUtils.resetTuple(tupRecord3);
-    // r1:record(f1:int, f2:long
-    tupRecord1.set(0, 2);
-    tupRecord1.set(1, 1002L);
-    tuple.set(0, tupRecord1);
-
-    // r2:record(r3:record(f3:float, f4))
-    tupRecord2.set(0, "f5 row2 byte array");
-    tupRecord3.set(0, 2.3);
-    tupRecord3.set(1, new DataByteArray("r3 row2 byte array"));
-    tupRecord2.set(1, tupRecord3);
-    tuple.set(1, tupRecord2);
-
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // finish building table, closing out the inserter, writer, writer1
-    inserter.close();
-    writer1.finish();
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-    BasicTable.drop(path, conf);
-  }
-
-  @Test
-  // Test read , simple projection 0, not record of record level.
-  public void testRead1() throws IOException, ParseException {
-    String projection0 = new String("r1.f1, r1.f2");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection0);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    System.out.println("read 1:" + RowValue.toString());
-    // read 1:(1,1001L)
-    Assert.assertEquals(1, RowValue.get(0));
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(2, RowValue.get(0));
-    reader.close();
-
-  }
-
-  /*
-   * String STR_SCHEMA =
-   * "r1:record(f1:int, f2:long), r2:record(f5, r3:record(f3:float, f4))";
-   * String STR_STORAGE = "[r1.f1]; [r2.r3.f4]; [r1.f2, r2.r3.f3]";
-   */
-  @Test
-  // Test read , record of record level,
-  public void testRead2() throws IOException, ParseException {
-    String projection1 = new String("r2.r3.f4, r2.r3.f3");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection1);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    System.out.println("read 2:" + RowValue.toString());
-    // read 2:(r3 row1 byte array,1.3)
-    Assert.assertEquals("r3 row1 byte array", RowValue.get(0).toString());
-    Assert.assertEquals(1.3, RowValue.get(1));
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals("r3 row2 byte array", RowValue.get(0).toString());
-    Assert.assertEquals(2.3, RowValue.get(1));
-    reader.close();
-  }
-
-  /*
-   * String STR_SCHEMA =
-   * "r1:record(f1:int, f2:long), r2:record(f5, r3:record(f3:float, f4))";
-   * String STR_STORAGE = "[r1.f1]; [r2.r3.f4]; [r1.f2, r2.r3.f3]";
-   */
-  @Test
-  // test stitch, r1.f1, r2.r3.f3
-  public void testRead3() throws IOException, ParseException {
-    String projection2 = new String("r1.f1, r2.r3.f3");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection2);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    System.out.println("read 3:" + RowValue.toString());
-    // read 3:(1,1.3)
-    Assert.assertEquals(1, RowValue.get(0));
-    Assert.assertEquals(1.3, RowValue.get(1));
-
-    scanner.advance();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(2, RowValue.get(0));
-    Assert.assertEquals(2.3, RowValue.get(1));
-    reader.close();
-  }
-
-  @Test
-  public void testRead4() throws IOException, ParseException {
-    String projection3 = new String("r1, r2");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection3);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    System.out.println("read 4:" + RowValue.toString());
-    // read 4:((1,1001L),(f5 row1 byte array,(1.3,r3 row1 byte array)))
-
-    Assert.assertEquals(1, ((Tuple) RowValue.get(0)).get(0));
-    Assert.assertEquals(1001L, ((Tuple) RowValue.get(0)).get(1));
-    Assert.assertEquals("f5 row1 byte array", ((Tuple) RowValue.get(1)).get(0)
-        .toString());
-
-    Assert.assertEquals(1.3, ((Tuple) ((Tuple) RowValue.get(1)).get(1)).get(0));
-    Assert.assertEquals("r3 row1 byte array",
-        ((Tuple) ((Tuple) RowValue.get(1)).get(1)).get(1).toString());
-
-    scanner.advance();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(2, ((Tuple) RowValue.get(0)).get(0));
-    Assert.assertEquals(1002L, ((Tuple) RowValue.get(0)).get(1));
-    Assert.assertEquals("f5 row2 byte array", ((Tuple) RowValue.get(1)).get(0)
-        .toString());
-
-    Assert.assertEquals(2.3, ((Tuple) ((Tuple) RowValue.get(1)).get(1)).get(0));
-    Assert.assertEquals("r3 row2 byte array",
-        ((Tuple) ((Tuple) RowValue.get(1)).get(1)).get(1).toString());
-
-    reader.close();
-  }
-
-  @Test
-  // test stitch, r2.f5.f3
-  public void testRead5() throws IOException, ParseException {
-    String projection3 = new String("r2.r3.f3");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection3);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    System.out.println("read 5:" + RowValue.toString());
-    // read 5:(1.3)
-    Assert.assertEquals(1.3, RowValue.get(0));
-
-    scanner.advance();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(2.3, RowValue.get(0));
-
-    reader.close();
-  }
-
-  /*
-   * String STR_SCHEMA =
-   * "r1:record(f1:int, f2:long), r2:record(f5, r3:record(f3:float, f4))";
-   * String STR_STORAGE = "[r1.f1]; [r2.r3.f4]; [r1.f2, r2.r3.f3]";
-   */
-  @Test
-  // test stitch,(r1.f2, r1.f1)
-  public void testRead6() throws IOException, ParseException {
-    String projection3 = new String("r1.f2,r1.f1");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection3);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    System.out.println("read 6:" + RowValue.toString());
-    // read 6:(1001L,1)
-
-    Assert.assertEquals(1001L, RowValue.get(0));
-    Assert.assertEquals(1, RowValue.get(1));
-    try {
-      RowValue.get(2);
-      Assert.fail("Should throw index out of bound exception ");
-    } catch (Exception e) {
-      e.printStackTrace();
-    }
-
-    scanner.advance();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(1002L, RowValue.get(0));
-    Assert.assertEquals(2, RowValue.get(1));
-    try {
-      RowValue.get(2);
-      Assert.fail("Should throw index out of bound exception ");
-    } catch (Exception e) {
-      e.printStackTrace();
-    }
-    reader.close();
-  }
-
-  @Test
-  // test projection, negative, none-exist column name.
-  public void testReadNegative1() throws IOException, ParseException {
-    String projection4 = new String("r3");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection4);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(null, RowValue.get(0));
-    reader.close();
-
-  }
-
-  @Test
-  // test projection, negative, none-exist column name.
-  public void testReadNegative2() throws IOException, ParseException {
-    String projection4 = new String("NO");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection4);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(null, RowValue.get(0));
-    reader.close();
-
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestRecord2Map.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestRecord2Map.java
deleted file mode 100644
index bc1c209ba..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestRecord2Map.java
+++ /dev/null
@@ -1,411 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Random;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Projection;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * 
- * Test projections on complicated column types.
- * 
- */
-public class TestRecord2Map {
-  private static Configuration conf;
-  private static Path path;
-  private static FileSystem fs;
-  static int count;
-  static String STR_SCHEMA = "r1:record(f1:int, f2:map(long)), r2:record(r3:record(f3:double, f4:map(int)))";
-  static String STR_STORAGE = "[r1.f1, r1.f2#{x|y}]; [r2.r3.f4]; [r1.f2, r2.r3.f3, r2.r3.f4#{a|c}]";
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException {
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), "TestRecord2Map");
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    // Build Table and column groups
-    BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-        STR_STORAGE, conf);
-    writer.finish();
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-    BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-    TypesUtils.resetTuple(tuple);
-
-    Tuple tupRecord1;
-    try {
-      tupRecord1 = TypesUtils.createTuple(schema.getColumnSchema("r1")
-          .getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    Tuple tupRecord2;
-    try {
-      tupRecord2 = TypesUtils.createTuple(schema.getColumnSchema("r2")
-          .getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    Tuple tupRecord3;
-    try {
-      tupRecord3 = TypesUtils.createTuple(new Schema("f3:float, f4:map(int)"));
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-    HashMap<String, Long> map1 = new HashMap<String, Long>();
-    // insert data in row 1
-    int row = 0;
-    // r1:record(f1:int, f2:map(long))
-    tupRecord1.set(0, 1);
-    map1.put("x", 1001L);
-    tupRecord1.set(1, map1);
-    tuple.set(0, tupRecord1);
-
-    // r2:record(r3:record(f3:float, map(int)))
-    HashMap<String, Integer> map = new HashMap<String, Integer>();
-    tupRecord2.set(0, tupRecord3);
-    tupRecord3.set(0, 1.3);
-    map.put("a", 1);
-    map.put("b", 2);
-    map.put("c", 3);
-    tupRecord3.set(1, map);
-    tuple.set(1, tupRecord2);
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // row 2
-    map1.clear();
-    row++;
-    TypesUtils.resetTuple(tuple);
-    TypesUtils.resetTuple(tupRecord1);
-    TypesUtils.resetTuple(tupRecord2);
-    TypesUtils.resetTuple(tupRecord3);
-    // r1:record(f1:int, f2:map(long))
-    tupRecord1.set(0, 2);
-    map1.put("y", 1002L);
-    tupRecord1.set(1, map1);
-    tuple.set(0, tupRecord1);
-
-    // r2:record(r3:record(f3:float, f4))
-    tupRecord2.set(0, tupRecord3);
-    map.clear();
-    map.put("x", 11);
-    map.put("y", 12);
-    map.put("c", 13);
-
-    tupRecord3.set(0, 2.3);
-    tupRecord3.set(1, map);
-    tuple.set(1, tupRecord2);
-
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // finish building table, closing out the inserter, writer, writer1
-    inserter.close();
-    writer1.finish();
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-    BasicTable.drop(path, conf);
-  }
-
-  @Test
-  public void testReadSimpleRecord() throws IOException, ParseException {
-    // Starting read , simple projection 0, not record of record level. PASS
-    String projection0 = new String("r1.f1, r1.f2");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection0);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(1, RowValue.get(0));
-    Long expected = 1001L;
-    Assert.assertEquals(expected, ((Map<String, Long>) RowValue.get(1))
-        .get("x"));
-    Assert.assertEquals(null, ((Map<String, Long>) RowValue.get(1)).get("y"));
-    Assert.assertEquals(null, ((Map<String, Long>) RowValue.get(1)).get("a"));
-    Assert.assertEquals(null, ((Map<String, Long>) RowValue.get(1)).get("z"));
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(2, RowValue.get(0));
-    expected = 1002L;
-    Assert.assertEquals(expected, ((Map<String, Long>) RowValue.get(1))
-        .get("y"));
-    Assert.assertEquals(null, ((Map<String, Long>) RowValue.get(1)).get("x"));
-    Assert.assertEquals(null, ((Map<String, Long>) RowValue.get(1)).get("a"));
-    Assert.assertEquals(null, ((Map<String, Long>) RowValue.get(1)).get("z"));
-
-    reader.close();
-
-  }
-
-  /*
-   * String STR_SCHEMA =
-   * "r1:record(f1:int, f2:long), r2:record(r3:record(f3:float, map(int))";
-   * String STR_STORAGE =
-   * "[r1.f1]; [r2.r3.f4]; [r1.f2, r2.r3.f3, r2.r3.f4#{a|c}]";
-   */
-  @Test
-  public void testReadRecordOfMap1() throws IOException, ParseException {
-    String projection1 = new String("r2.r3.f4#{a|b}, r2.r3.f3");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection1);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(1, ((Map) (RowValue.get(0))).get("a"));
-    Assert.assertEquals(2, ((Map) RowValue.get(0)).get("b"));
-    Assert.assertEquals(null, ((Map) (RowValue.get(0))).get("x"));
-    Assert.assertEquals(null, ((Map) RowValue.get(0)).get("y"));
-    Assert.assertEquals(1.3, RowValue.get(1));
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(null, ((Map) RowValue.get(0)).get("a"));
-    Assert.assertEquals(null, ((Map) RowValue.get(0)).get("b"));
-    Assert.assertEquals(null, ((Map) RowValue.get(0)).get("x"));
-    Assert.assertEquals(null, ((Map) RowValue.get(0)).get("y"));
-    Assert.assertEquals(null, ((Map) RowValue.get(0)).get("c"));
-    Assert.assertEquals(2.3, RowValue.get(1));
-    reader.close();
-  }
-
-  /*
-   * String STR_SCHEMA =
-   * "r1:record(f1:int, f2:long), r2:record(r3:record(f3:float, f4:map(int))";
-   * String STR_STORAGE =
-   * "[r1.f1]; [r2.r3.f4]; [r1.f2, r2.r3.f3, r2.r3.f4#{a|c}]";
-   */
-  @Test
-  // test stitch, [r1.f1]; [r2.r3.f4]; [r1.f2, r2.r3.f3]
-  public void testReadRecordOfRecord2() throws IOException, ParseException {
-    String projection2 = new String("r1.f1, r2.r3.f3");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection2);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(1, RowValue.get(0));
-    Assert.assertEquals(1.3, RowValue.get(1));
-    scanner.advance();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(2, RowValue.get(0));
-    Assert.assertEquals(2.3, RowValue.get(1));
-    reader.close();
-  }
-
-  /*
-   * String STR_SCHEMA =
-   * "r1:record(f1:int, f2:long), r2:record(r3:record(f3:float, f4:map(int)))" ;
-   * String STR_STORAGE =
-   * "[r1.f1]; [r2.r3.f4]; [r1.f2, r2.r3.f3, r2.r3.f4#{a|c}]";
-   */
-  // test stitch, [r1.f1]; [r2.r3.f4]; [r1.f2, r2.r3.f3]
-  public void testReadRecordOfRecord3() throws IOException, ParseException {
-    String projection3 = new String("r1, r2, r1.f2#{x|z}");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection3);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-
-    Assert.assertEquals(1, ((Tuple) RowValue.get(0)).get(0));
-    Long expected = 1001L;
-    Assert.assertEquals(expected, ((Map<String, Long>) RowValue.get(2))
-        .get("x"));
-    Assert.assertEquals(null, ((Map<String, Long>) RowValue.get(2)).get("y"));
-    Assert.assertEquals(null, ((Map<String, Long>) RowValue.get(2)).get("z"));
-    Tuple r2 = (Tuple) RowValue.get(1);
-    Tuple r3 = (Tuple) r2.get(0);
-    Map<String, Integer> f4 = (Map<String, Integer>) r3.get(1);
-    Integer tmp = 1;
-    Assert.assertEquals(tmp, f4.get("a"));
-    tmp = 2;
-    Assert.assertEquals(tmp, f4.get("b"));
-    tmp = 3;
-    Assert.assertEquals(tmp, f4.get("c"));
-    Assert.assertEquals(1.3, ((Tuple) ((Tuple) RowValue.get(1)).get(0)).get(0));
-
-    scanner.advance();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(2, ((Tuple) RowValue.get(0)).get(0));
-    expected = 1002L;
-    Assert.assertEquals(null, ((Map<String, Long>) RowValue.get(2)).get("x"));
-    Assert.assertEquals(null, ((Map<String, Long>) RowValue.get(2)).get("y"));
-    Assert.assertEquals(null, ((Map<String, Long>) RowValue.get(2)).get("z"));
-    r2 = (Tuple) RowValue.get(1);
-    r3 = (Tuple) r2.get(0);
-    f4 = (Map<String, Integer>) r3.get(1);
-    tmp = 11;
-    Assert.assertEquals(tmp, f4.get("x"));
-    tmp = 12;
-    Assert.assertEquals(tmp, f4.get("y"));
-    tmp = 13;
-    Assert.assertEquals(tmp, f4.get("c"));
-    Assert.assertEquals(2.3, ((Tuple) ((Tuple) RowValue.get(1)).get(0)).get(0));
-
-    reader.close();
-  }
-
-  /*
-   * String STR_SCHEMA =
-   * "r1:record(f1:int, f2:long), r2:record(r3:record(f3:float, f4:map(int)))" ;
-   * String STR_STORAGE =
-   * "[r1.f1]; [r2.r3.f4]; [r1.f2, r2.r3.f3, r2.r3.f4#{a|c}]";
-   */
-  @Test
-  // test stitch, [r1.f1]; [r2.r3.f4]; [r1.f2, r2.r3.f3]
-  public void testReadRecordOfRecord4() throws IOException, ParseException {
-    String projection3 = new String("r2.r3.f4");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection3);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-
-    Map<String, Integer> f4 = (Map<String, Integer>) RowValue.get(0);
-    Integer tmp = 1;
-    Assert.assertEquals(tmp, f4.get("a"));
-    tmp = 2;
-    Assert.assertEquals(tmp, f4.get("b"));
-    tmp = 3;
-    Assert.assertEquals(tmp, f4.get("c"));
-    Assert.assertEquals(null, f4.get("x"));
-
-    scanner.advance();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-
-    f4 = (Map<String, Integer>) RowValue.get(0);
-    tmp = 11;
-    Assert.assertEquals(tmp, f4.get("x"));
-    tmp = 12;
-    Assert.assertEquals(tmp, f4.get("y"));
-    tmp = 13;
-    Assert.assertEquals(tmp, f4.get("c"));
-    Assert.assertEquals(null, f4.get("a"));
-    reader.close();
-  }
-
-  @Test
-  public void testReadNegative1() throws IOException, ParseException {
-    String projection4 = new String("r3");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection4);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(null, RowValue.get(0));
-    reader.close();
-
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestRecord3Map.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestRecord3Map.java
deleted file mode 100644
index 9e98958a1..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestRecord3Map.java
+++ /dev/null
@@ -1,403 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Random;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Projection;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * 
- * Test projections on complicated column types.
- * 
- */
-public class TestRecord3Map {
-  static private Configuration conf;
-  static private Path path;
-  static private FileSystem fs;
-  static String STR_SCHEMA = "r1:record(f1:int, f2:long), r2:record(r3:record(f3:map(float), f4:map(int)))";
-  static String STR_STORAGE = "[r1.f1, r1.f2]; [r2.r3.f4]; [r2.r3.f3#{x|y}, r2.r3.f4#{a|c}]";
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException {
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), "TestRecord3Map");
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    // Build Table and column groups
-    BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-        STR_STORAGE, conf);
-    writer.finish();
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-    BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-    TypesUtils.resetTuple(tuple);
-
-    Tuple tupRecord1;
-    try {
-      tupRecord1 = TypesUtils.createTuple(schema.getColumnSchema("r1")
-          .getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    Tuple tupRecord2;
-    try {
-      tupRecord2 = TypesUtils.createTuple(schema.getColumnSchema("r2")
-          .getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    Tuple tupRecord3;
-    try {
-      tupRecord3 = TypesUtils.createTuple(new Schema("f3:float, f4:map(int)"));
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-    HashMap<String, Float> map1 = new HashMap<String, Float>();
-    // insert data in row 1
-    int row = 0;
-    // r1:record(f1:int, f2:long)
-    tupRecord1.set(0, 1);
-    tupRecord1.set(1, 1001L);
-    tuple.set(0, tupRecord1);
-
-    // r2:record(r3:record(f3:map(float), map(int)))
-    HashMap<String, Integer> map = new HashMap<String, Integer>();
-    tupRecord2.set(0, tupRecord3);
-    map1.put("x", 1.3F);
-    tupRecord3.set(0, map1);
-    map.put("a", 1);
-    map.put("b", 2);
-    map.put("c", 3);
-    tupRecord3.set(1, map);
-    tuple.set(1, tupRecord2);
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // row 2
-    map1.clear();
-    row++;
-    TypesUtils.resetTuple(tuple);
-    TypesUtils.resetTuple(tupRecord1);
-    TypesUtils.resetTuple(tupRecord2);
-    TypesUtils.resetTuple(tupRecord3);
-    // r1:record(f1:int, f2:map(long))
-    tupRecord1.set(0, 2);
-    tupRecord1.set(1, 1002L);
-    tuple.set(0, tupRecord1);
-
-    // r2:record(r3:record(f3:float, f4))
-    tupRecord2.set(0, tupRecord3);
-    map.clear();
-    map.put("x", 11);
-    map.put("y", 12);
-    map.put("c", 13);
-
-    map1.put("y", 2.3F);
-    tupRecord3.set(0, map1);
-    tupRecord3.set(1, map);
-    tuple.set(1, tupRecord2);
-
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // finish building table, closing out the inserter, writer, writer1
-    inserter.close();
-    writer1.finish();
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-    BasicTable.drop(path, conf);
-  }
-
-  @Test
-  public void testReadSimpleRecord() throws IOException, ParseException {
-    // Starting read , simple projection 0, not record of record level. PASS
-    String projection0 = new String("r1.f1, r1.f2");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection0);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(1, RowValue.get(0));
-    Long expected = 1001L;
-    Assert.assertEquals(expected, RowValue.get(1));
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(2, RowValue.get(0));
-    expected = 1002L;
-    Assert.assertEquals(expected, RowValue.get(1));
-
-    reader.close();
-
-  }
-
-  /*
-   * String STR_SCHEMA =
-   * "r1:record(f1:int, f2:long), r2:record(r3:record(f3:map(float), f4:map(int)))"
-   * ; String STR_STORAGE =
-   * "[r1.f1, r1.f2]; [r2.r3.f4]; [r2.r3.f3#{x|y}, r2.r3.f4#{a|c}]";
-   */
-  @Test
-  public void testReadRecordOfMap1() throws IOException, ParseException {
-    String projection1 = new String("r2.r3.f4#{a|b}, r2.r3.f3");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection1);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(1, ((Map) (RowValue.get(0))).get("a"));
-    Assert.assertEquals(2, ((Map) RowValue.get(0)).get("b"));
-    Assert.assertEquals(1.3F, ((Map<String, Float>) RowValue.get(1)).get("x"));
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(null, ((Map) RowValue.get(0)).get("a"));
-    Assert.assertEquals(null, ((Map) RowValue.get(0)).get("b"));
-    Assert.assertEquals(null, ((Map) RowValue.get(0)).get("x"));
-    Assert.assertEquals(null, ((Map) RowValue.get(0)).get("y"));
-    Assert.assertEquals(null, ((Map) RowValue.get(0)).get("c"));
-    Assert.assertEquals(2.3F, ((Map<String, Float>) RowValue.get(1)).get("y"));
-    reader.close();
-  }
-
-  /*
-   * String STR_SCHEMA =
-   * "r1:record(f1:int, f2:long), r2:record(r3:record(f3:float, f4:map(int))";
-   * String STR_STORAGE =
-   * "[r1.f1]; [r2.r3.f4]; [r1.f2, r2.r3.f3, r2.r3.f4#{a|c}]";
-   */
-  // test stitch, [r1.f1]; [r2.r3.f4]; [r1.f2, r2.r3.f3]
-  @Test
-  public void testReadRecordOfRecord2() throws IOException, ParseException {
-    String projection2 = new String("r1.f1, r2.r3.f3");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection2);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(1, RowValue.get(0));
-    Assert.assertEquals(1.3F, ((Map<String, Float>) RowValue.get(1)).get("x"));
-
-    scanner.advance();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(2, RowValue.get(0));
-    Assert.assertEquals(2.3F, ((Map<String, Float>) RowValue.get(1)).get("y"));
-    reader.close();
-  }
-
-  /*
-   * String STR_SCHEMA =
-   * "r1:record(f1:int, f2:long), r2:record(r3:record(f3:map(float), f4:map(int)))"
-   * ; String STR_STORAGE =
-   * "[r1.f1, r1.f2]; [r2.r3.f4]; [r2.r3.f3#{x|y}, r2.r3.f4#{a|c}]";
-   */
-  // test stitch, [r1.f1]; [r2.r3.f4]; [r1.f2, r2.r3.f3]
-  @Test
-  public void testReadRecordOfRecord3() throws IOException, ParseException {
-    String projection3 = new String("r1, r2, r2.r3.f3#{y|x}");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection3);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-
-    Assert.assertEquals(1, ((Tuple) RowValue.get(0)).get(0));
-    Assert.assertEquals(1.3F, ((Map<String, Float>) RowValue.get(2)).get("x"));
-    // Assert.assertEquals(null, ((Map<String,
-    // Float>)RowValue.get(2)).get("x"));
-    Assert.assertEquals(null, ((Map<String, Float>) RowValue.get(2)).get("y"));
-    Assert.assertEquals(null, ((Map<String, Float>) RowValue.get(2)).get("z"));
-    Tuple r2 = (Tuple) RowValue.get(1);
-    Tuple r3 = (Tuple) r2.get(0);
-    Map<String, Integer> f4 = (Map<String, Integer>) r3.get(1);
-    Integer tmp = 1;
-    Assert.assertEquals(tmp, f4.get("a"));
-    tmp = 2;
-    Assert.assertEquals(tmp, f4.get("b"));
-    tmp = 3;
-    Assert.assertEquals(tmp, f4.get("c"));
-
-    scanner.advance();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(2, ((Tuple) RowValue.get(0)).get(0));
-    Assert.assertEquals(null, ((Map<String, Long>) RowValue.get(2)).get("x"));
-    // Assert.assertEquals(null, ((Map<String,
-    // Float>)RowValue.get(2)).get("y"));
-    Assert.assertEquals(2.3F, ((Map<String, Long>) RowValue.get(2)).get("y"));
-    Assert.assertEquals(null, ((Map<String, Long>) RowValue.get(2)).get("z"));
-    r2 = (Tuple) RowValue.get(1);
-    r3 = (Tuple) r2.get(0);
-    f4 = (Map<String, Integer>) r3.get(1);
-    tmp = 11;
-    Assert.assertEquals(tmp, f4.get("x"));
-    tmp = 12;
-    Assert.assertEquals(tmp, f4.get("y"));
-    tmp = 13;
-    Assert.assertEquals(tmp, f4.get("c"));
-
-    reader.close();
-  }
-
-  /*
-   * String STR_SCHEMA =
-   * "r1:record(f1:int, f2:long), r2:record(r3:record(f3:map(float), f4:map(int)))"
-   * ; String STR_STORAGE =
-   * "[r1.f1, r1.f2]; [r2.r3.f4]; [r2.r3.f3#{x|y}, r2.r3.f4#{a|c}]";
-   */
-  // test stitch, [r1.f1]; [r2.r3.f4]; [r1.f2, r2.r3.f3]
-  @Test
-  public void testReadRecordOfRecord4() throws IOException, ParseException {
-    String projection3 = new String("r2.r3.f4");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection3);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-
-    Map<String, Integer> f4 = (Map<String, Integer>) RowValue.get(0);
-    Integer tmp = 1;
-    Assert.assertEquals(tmp, f4.get("a"));
-    tmp = 2;
-    Assert.assertEquals(tmp, f4.get("b"));
-    tmp = 3;
-    Assert.assertEquals(tmp, f4.get("c"));
-    Assert.assertEquals(null, f4.get("x"));
-
-    scanner.advance();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-
-    f4 = (Map<String, Integer>) RowValue.get(0);
-    tmp = 11;
-    Assert.assertEquals(tmp, f4.get("x"));
-    tmp = 12;
-    Assert.assertEquals(tmp, f4.get("y"));
-    tmp = 13;
-    Assert.assertEquals(tmp, f4.get("c"));
-    Assert.assertEquals(null, f4.get("a"));
-    reader.close();
-  }
-
-  // test projection, negative, none-exist column name. FAILED, It crashes
-  // here
-  @Test
-  public void testReadNegative1() throws IOException, ParseException {
-    String projection4 = new String("r3");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection4);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(null, RowValue.get(0));
-    reader.close();
-
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestRecordMap.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestRecordMap.java
deleted file mode 100644
index f817c557f..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestRecordMap.java
+++ /dev/null
@@ -1,391 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Random;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Projection;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * 
- * Test projections on complicated column types.
- * 
- */
-public class TestRecordMap {
-  static private Configuration conf;
-  static private Path path;
-  static private FileSystem fs;
-  static int count;
-  static String STR_SCHEMA = "r1:record(f1:int, f2:long), r2:record(r3:record(f3:double, f4:map(int)))";
-  static String STR_STORAGE = "[r1.f1]; [r2.r3.f4]; [r1.f2, r2.r3.f3, r2.r3.f4#{a|c}]";
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException {
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), "TestRecordMap");
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    // Build Table and column groups
-    BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-        STR_STORAGE, conf);
-    writer.finish();
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-    BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-    TypesUtils.resetTuple(tuple);
-
-    Tuple tupRecord1;
-    try {
-      tupRecord1 = TypesUtils.createTuple(schema.getColumnSchema("r1")
-          .getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    Tuple tupRecord2;
-    try {
-      tupRecord2 = TypesUtils.createTuple(schema.getColumnSchema("r2")
-          .getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    Tuple tupRecord3;
-    try {
-      tupRecord3 = TypesUtils.createTuple(new Schema("f3:float, f4:map(int)"));
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-    // insert data in row 1
-    int row = 0;
-    // r1:record(f1:int, f2:long
-    tupRecord1.set(0, 1);
-    tupRecord1.set(1, 1001L);
-    tuple.set(0, tupRecord1);
-
-    // r2:record(r3:record(f3:float, map(int)))
-    HashMap<String, Integer> map = new HashMap<String, Integer>();
-    tupRecord2.set(0, tupRecord3);
-    tupRecord3.set(0, 1.3);
-    map.put("a", 1);
-    map.put("b", 2);
-    map.put("c", 3);
-    tupRecord3.set(1, map);
-    tuple.set(1, tupRecord2);
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // row 2
-    row++;
-    TypesUtils.resetTuple(tuple);
-    TypesUtils.resetTuple(tupRecord1);
-    TypesUtils.resetTuple(tupRecord2);
-    TypesUtils.resetTuple(tupRecord3);
-    // r1:record(f1:int, f2:long
-    tupRecord1.set(0, 2);
-    tupRecord1.set(1, 1002L);
-    tuple.set(0, tupRecord1);
-
-    // r2:record(r3:record(f3:float, f4))
-    tupRecord2.set(0, tupRecord3);
-    map.clear();
-    map.put("x", 11);
-    map.put("y", 12);
-    map.put("c", 13);
-
-    tupRecord3.set(0, 2.3);
-    tupRecord3.set(1, map);
-    tuple.set(1, tupRecord2);
-
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // finish building table, closing out the inserter, writer, writer1
-    inserter.close();
-    writer1.finish();
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-    BasicTable.drop(path, conf);
-  }
-
-  @Test
-  public void testReadSimpleRecord() throws IOException, ParseException {
-    // Starting read , simple projection 0, not record of record level. PASS
-    String projection0 = new String("r1.f1, r1.f2");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection0);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(1, RowValue.get(0));
-    Assert.assertEquals(1001L, RowValue.get(1));
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(2, RowValue.get(0));
-    Assert.assertEquals(1002L, RowValue.get(1));
-
-    reader.close();
-
-  }
-
-  /*
-   * String STR_SCHEMA =
-   * "r1:record(f1:int, f2:long), r2:record(r3:record(f3:float, map(int))";
-   * String STR_STORAGE =
-   * "[r1.f1]; [r2.r3.f4]; [r1.f2, r2.r3.f3, r2.r3.f4#{a|c}]";
-   */
-  @Test
-  public void testReadRecordOfMap1() throws IOException, ParseException {
-    String projection1 = new String("r2.r3.f4#{a|b}, r2.r3.f3");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection1);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(1, ((Map) (RowValue.get(0))).get("a"));
-    Assert.assertEquals(2, ((Map) RowValue.get(0)).get("b"));
-    Assert.assertEquals(1.3, RowValue.get(1));
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(null, ((Map) RowValue.get(0)).get("a"));
-    Assert.assertEquals(null, ((Map) RowValue.get(0)).get("b"));
-    Assert.assertEquals(null, ((Map) RowValue.get(0)).get("x"));
-    Assert.assertEquals(null, ((Map) RowValue.get(0)).get("y"));
-    Assert.assertEquals(null, ((Map) RowValue.get(0)).get("c"));
-    Assert.assertEquals(2.3, RowValue.get(1));
-    reader.close();
-  }
-
-  /*
-   * String STR_SCHEMA =
-   * "r1:record(f1:int, f2:long), r2:record(r3:record(f3:float, f4:map(int))";
-   * String STR_STORAGE =
-   * "[r1.f1]; [r2.r3.f4]; [r1.f2, r2.r3.f3, r2.r3.f4#{a|c}]";
-   */
-  // test stitch, [r1.f1]; [r2.r3.f4]; [r1.f2, r2.r3.f3]
-  @Test
-  public void testReadRecordOfRecord2() throws IOException, ParseException {
-    String projection2 = new String("r1.f1, r2.r3.f3");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection2);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(1, RowValue.get(0));
-    Assert.assertEquals(1.3, RowValue.get(1));
-
-    scanner.advance();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(2, RowValue.get(0));
-    Assert.assertEquals(2.3, RowValue.get(1));
-    reader.close();
-  }
-
-  /*
-   * String STR_SCHEMA =
-   * "r1:record(f1:int, f2:long), r2:record(r3:record(f3:float, f4:map(int)))" ;
-   * String STR_STORAGE =
-   * "[r1.f1]; [r2.r3.f4]; [r1.f2, r2.r3.f3, r2.r3.f4#{a|c}]";
-   */
-  // test stitch, [r1.f1]; [r2.r3.f4]; [r1.f2, r2.r3.f3]
-  @Test
-  public void testReadRecordOfRecord3() throws IOException, ParseException {
-    String projection3 = new String("r1, r2");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection3);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-
-    Assert.assertEquals(1, ((Tuple) RowValue.get(0)).get(0));
-    Assert.assertEquals(1001L, ((Tuple) RowValue.get(0)).get(1));
-    Tuple r2 = (Tuple) RowValue.get(1);
-    Tuple r3 = (Tuple) r2.get(0);
-    Map<String, Integer> f4 = (Map<String, Integer>) r3.get(1);
-    Integer tmp = 1;
-    Assert.assertEquals(tmp, f4.get("a"));
-    tmp = 2;
-    Assert.assertEquals(tmp, f4.get("b"));
-    tmp = 3;
-    Assert.assertEquals(tmp, f4.get("c"));
-    Assert.assertEquals(1.3, ((Tuple) ((Tuple) RowValue.get(1)).get(0)).get(0));
-
-    scanner.advance();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(2, ((Tuple) RowValue.get(0)).get(0));
-    Assert.assertEquals(1002L, ((Tuple) RowValue.get(0)).get(1));
-    r2 = (Tuple) RowValue.get(1);
-    r3 = (Tuple) r2.get(0);
-    f4 = (Map<String, Integer>) r3.get(1);
-    tmp = 11;
-    Assert.assertEquals(tmp, f4.get("x"));
-    tmp = 12;
-    Assert.assertEquals(tmp, f4.get("y"));
-    tmp = 13;
-    Assert.assertEquals(tmp, f4.get("c"));
-    Assert.assertEquals(2.3, ((Tuple) ((Tuple) RowValue.get(1)).get(0)).get(0));
-
-    reader.close();
-  }
-
-  /*
-   * String STR_SCHEMA =
-   * "r1:record(f1:int, f2:long), r2:record(r3:record(f3:float, f4:map(int)))" ;
-   * String STR_STORAGE =
-   * "[r1.f1]; [r2.r3.f4]; [r1.f2, r2.r3.f3, r2.r3.f4#{a|c}]";
-   */
-  // test stitch, [r1.f1]; [r2.r3.f4]; [r1.f2, r2.r3.f3]
-  @Test
-  public void testReadRecordOfRecord4() throws IOException, ParseException {
-    String projection3 = new String("r2.r3.f4");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection3);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-
-    Map<String, Integer> f4 = (Map<String, Integer>) RowValue.get(0);
-    Integer tmp = 1;
-    Assert.assertEquals(tmp, f4.get("a"));
-    tmp = 2;
-    Assert.assertEquals(tmp, f4.get("b"));
-    tmp = 3;
-    Assert.assertEquals(tmp, f4.get("c"));
-    Assert.assertEquals(null, f4.get("x"));
-
-    scanner.advance();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-
-    f4 = (Map<String, Integer>) RowValue.get(0);
-    tmp = 11;
-    Assert.assertEquals(tmp, f4.get("x"));
-    tmp = 12;
-    Assert.assertEquals(tmp, f4.get("y"));
-    tmp = 13;
-    Assert.assertEquals(tmp, f4.get("c"));
-    Assert.assertEquals(null, f4.get("a"));
-    reader.close();
-  }
-
-  // test projection, negative, none-exist column name.
-  @Test
-  public void testReadNegative1() throws IOException, ParseException {
-    String projection4 = new String("r3");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection4);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(null, RowValue.get(0));
-    reader.close();
-
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestSchema.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestSchema.java
deleted file mode 100644
index fa5904a4a..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestSchema.java
+++ /dev/null
@@ -1,536 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Random;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Projection;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * 
- * Test projections on complicated column types.
- * 
- */
-public class TestSchema {
-  // final static String STR_SCHEMA =
-  // "s1:bool, s2:int, s3:long, s4:float, s5:string, s6:bytes, r1:record(f1:int, f2:long),  m1:map(string), c:collection(f13:double, f14:float, f15:bytes)";
-  // final static String STR_STORAGE =
-  // "[s1, s2]; [r1.f1]; [s3, s4]; [s5, s6]; [r1.f2]; [c.f13]";
-  private static Configuration conf;
-  private static Path path;
-  private static FileSystem fs;
-
-  @BeforeClass
-  public static void setUp() throws IOException {
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), "TestSchema");
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-  }
-
-  /**
-   * Return the name of the routine that called getCurrentMethodName
-   * 
-   */
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    return t;
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-    BasicTable.drop(path, conf);
-  }
-
-  @Test
-  public void testSimple() throws IOException, ParseException {
-    String STR_SCHEMA = "s1:bool, s2:int, s3:long, s4:double, s5:string, s6:bytes";
-    String STR_STORAGE = "[s1, s2]; [s3, s4]; [s5, s6]";
-
-    // Build Table and column groups
-    BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-        STR_STORAGE, conf);
-    writer.finish();
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-    BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-    TypesUtils.resetTuple(tuple);
-
-    // insert data in row 1
-    int row = 0;
-    tuple.set(0, true); // bool
-    tuple.set(1, 1); // int
-    tuple.set(2, 1001L); // long
-    tuple.set(3, 1.1); // float
-    tuple.set(4, "hello world 1"); // string
-    tuple.set(5, new DataByteArray("hello byte 1")); // byte
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // insert data in row 2
-    row++;
-    tuple.set(0, false);
-    tuple.set(1, 2); // int
-    tuple.set(2, 1002L); // long
-    tuple.set(3, 3.1); // float
-    tuple.set(4, "hello world 2"); // string
-    tuple.set(5, new DataByteArray("hello byte 2")); // byte
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // finish building table, closing out the inserter, writer, writer1
-    inserter.close();
-    writer1.finish();
-    writer.close();
-
-    // Starting read
-    String projection = new String("s6,s5,s4,s3,s2,s1");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(true, RowValue.get(5));
-    Assert.assertEquals(1, RowValue.get(4));
-    Assert.assertEquals(1.1, RowValue.get(2));
-    Assert.assertEquals("hello world 1", RowValue.get(1));
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(false, RowValue.get(5));
-    Assert.assertEquals(2, RowValue.get(4));
-    Assert.assertEquals(3.1, RowValue.get(2));
-    Assert.assertEquals("hello world 2", RowValue.get(1));
-
-    // test stitch, I can re_use reader, split, but NOT scanner
-    String projection2 = new String("s5, s1");
-    reader.setProjection(projection2);
-    scanner = reader.getScanner(splits.get(0), true);
-    RowValue = TypesUtils.createTuple(scanner.getSchema());
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals("hello world 1", RowValue.get(0));
-    Assert.assertEquals(true, RowValue.get(1));
-
-    scanner.advance();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals("hello world 2", RowValue.get(0));
-    Assert.assertEquals(false, RowValue.get(1));
-
-    reader.close();
-  }
-
-  @Test
-  public void testRecord() throws IOException, ParseException {
-    BasicTable.drop(path, conf);
-    String STR_SCHEMA = "r1:record(f1:int, f2:long), r2:record(r3:record(f3:double, f4))";
-    String STR_STORAGE = "[r1.f1]; [r2.r3.f4]; [r1.f2, r2.r3.f3]";
-
-    path = new Path(getCurrentMethodName());
-    // in case BasicTable exists
-    BasicTable.drop(path, conf);
-    System.out.println("in testRecord, get path: " + path.toString());
-    // Build Table and column groups
-    BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-        STR_STORAGE, conf);
-    writer.finish();
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-    BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-    TypesUtils.resetTuple(tuple);
-
-    Tuple tupRecord1;
-    try {
-      tupRecord1 = TypesUtils.createTuple(schema.getColumnSchema("r1")
-          .getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    Tuple tupRecord2;
-    try {
-      tupRecord2 = TypesUtils.createTuple(schema.getColumnSchema("r2")
-          .getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    Tuple tupRecord3;
-    try {
-      tupRecord3 = TypesUtils.createTuple(new Schema("f3:float, f4"));
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-    // insert data in row 1
-    int row = 0;
-    // r1:record(f1:int, f2:long
-    tupRecord1.set(0, 1);
-    tupRecord1.set(1, 1001L);
-    tuple.set(0, tupRecord1);
-
-    // r2:record(r3:record(f3:float, f4))
-    tupRecord2.set(0, tupRecord3);
-    tupRecord3.set(0, 1.3);
-    tupRecord3.set(1, new DataByteArray("r3 row1 byte array"));
-    tuple.set(1, tupRecord2);
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // row 2
-    row++;
-    TypesUtils.resetTuple(tuple);
-    TypesUtils.resetTuple(tupRecord1);
-    TypesUtils.resetTuple(tupRecord2);
-    TypesUtils.resetTuple(tupRecord3);
-    // r1:record(f1:int, f2:long
-    tupRecord1.set(0, 2);
-    tupRecord1.set(1, 1002L);
-    tuple.set(0, tupRecord1);
-
-    // r2:record(r3:record(f3:float, f4))
-    tupRecord2.set(0, tupRecord3);
-    tupRecord3.set(0, 2.3);
-    tupRecord3.set(1, new DataByteArray("r3 row2 byte array"));
-    tuple.set(1, tupRecord2);
-
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // finish building table, closing out the inserter, writer, writer1
-    inserter.close();
-    writer1.finish();
-    writer.close();
-
-    // Starting read , simple projection 0, not record of record level. PASS
-    String projection0 = new String("r1.f1, r1.f2");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection0);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(1, RowValue.get(0));
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(2, RowValue.get(0));
-
-    /*
-     * String STR_SCHEMA =
-     * "r1:record(f1:int, f2:long), r2:record(r3:record(f3:float, f4))"; String
-     * STR_STORAGE = "[r1.f1]; [r2.r3.f4]; [r1.f2, r2.r3.f3]";
-     */
-    // Starting read , simple projection 1 , record of record level, FIXED,
-    // PASS on advance
-    String projection1 = new String("r2.r3.f4, r2.r3.f3");
-    reader.setProjection(projection1);
-    scanner = reader.getScanner(splits.get(0), true);
-    RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals("r3 row1 byte array", RowValue.get(0).toString());
-    Assert.assertEquals(1.3, RowValue.get(1));
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals("r3 row2 byte array", RowValue.get(0).toString());
-    Assert.assertEquals(2.3, RowValue.get(1));
-
-    /*
-     * String STR_SCHEMA =
-     * "r1:record(f1:int, f2:long), r2:record(r3:record(f3:float, f4))"; String
-     * STR_STORAGE = "[r1.f1]; [r2.r3.f4]; [r1.f2, r2.r3.f3]";
-     */
-    // test stitch, [r1.f1]; [r2.r3.f4]; [r1.f2, r2.r3.f3]
-    String projection2 = new String("r1.f1, r2.r3.f3");
-    reader.setProjection(projection2);
-    scanner = reader.getScanner(splits.get(0), true);
-    RowValue = TypesUtils.createTuple(scanner.getSchema());
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(1, RowValue.get(0));
-    Assert.assertEquals(1.3, RowValue.get(1));
-
-    scanner.advance();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(2, RowValue.get(0));
-    Assert.assertEquals(2.3, RowValue.get(1));
-
-    /*
-     * String STR_SCHEMA =
-     * "r1:record(f1:int, f2:long), r2:record(r3:record(f3:float, f4))"; String
-     * STR_STORAGE = "[r1.f1]; [r2.r3.f4]; [r1.f2, r2.r3.f3]";
-     */
-    // test stitch, [r1.f1]; [r2.r3.f4]; [r1.f2, r2.r3.f3]
-    String projection3 = new String("r1, r2");
-    reader.setProjection(projection3);
-    scanner = reader.getScanner(splits.get(0), true);
-    RowValue = TypesUtils.createTuple(scanner.getSchema());
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-
-    Assert.assertEquals(1, ((Tuple) RowValue.get(0)).get(0));
-    Assert.assertEquals(1001L, ((Tuple) RowValue.get(0)).get(1));
-    Assert.assertEquals("1.3,#r3 row1 byte array", RowValue.get(1).toString());
-    Assert.assertEquals(1.3, ((Tuple) ((Tuple) RowValue.get(1)).get(0)).get(0));
-    Assert.assertEquals("r3 row1 byte array",
-        ((Tuple) ((Tuple) RowValue.get(1)).get(0)).get(1).toString());
-
-    scanner.advance();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(2, ((Tuple) RowValue.get(0)).get(0));
-    Assert.assertEquals(1002L, ((Tuple) RowValue.get(0)).get(1));
-    Assert.assertEquals("2.3,#r3 row2 byte array", RowValue.get(1).toString());
-    Assert.assertEquals(2.3, ((Tuple) ((Tuple) RowValue.get(1)).get(0)).get(0));
-    Assert.assertEquals("r3 row2 byte array",
-        ((Tuple) ((Tuple) RowValue.get(1)).get(0)).get(1).toString());
-
-    // test projection, negative, none-exist column name. FAILED, It crashes
-    // here
-    String projection4 = new String("r3");
-    reader.setProjection(projection4);
-    scanner = reader.getScanner(splits.get(0), true);
-    RowValue = TypesUtils.createTuple(scanner.getSchema());
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    // scanner.getValue(RowValue); //FAILED, It crashes here
-
-    reader.close();
-
-  }
-
-  @Test
-  public void testMap() throws IOException, ParseException {
-    String STR_SCHEMA = "m1:map(string),m2:map(map(int))";
-    String STR_STORAGE = "[m1#{a}];[m2#{x|y}]; [m1#{b}, m2#{z}]";
-
-    // String STR_SCHEMA = "m1:map(string)";
-    // String STR_STORAGE = "[m1#{a}]";
-    // Build Table and column groups
-    path = new Path(getCurrentMethodName());
-    BasicTable.drop(path, conf);
-    BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-        STR_STORAGE, conf);
-    writer.finish();
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-    BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-    TypesUtils.resetTuple(tuple);
-
-    // m1:map(string)
-    Map<String, String> m1 = new HashMap<String, String>();
-    m1.put("a", "A");
-    m1.put("b", "B");
-    m1.put("c", "C");
-    tuple.set(0, m1);
-
-    // m2:map(map(int))
-    HashMap<String, Map> m2 = new HashMap<String, Map>();
-    Map<String, Integer> m3 = new HashMap<String, Integer>();
-    m3.put("m311", 311);
-    m3.put("m321", 321);
-    m3.put("m331", 331);
-    Map<String, Integer> m4 = new HashMap<String, Integer>();
-    m4.put("m411", 411);
-    m4.put("m421", 421);
-    m4.put("m431", 431);
-    m2.put("x", m3);
-    m2.put("y", m4);
-    tuple.set(1, m2);
-    int row = 0;
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // row 2
-    row++;
-    TypesUtils.resetTuple(tuple);
-    m1.clear();
-    m2.clear();
-    m3.clear();
-    m4.clear();
-    // m1:map(string)
-    m1.put("a", "A2");
-    m1.put("b2", "B2");
-    m1.put("c2", "C2");
-    tuple.set(0, m1);
-
-    // m2:map(map(int))
-    m3.put("m321", 321);
-    m3.put("m322", 322);
-    m3.put("m323", 323);
-    m2.put("z", m3);
-    tuple.set(1, m2);
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // finish building table, closing out the inserter, writer, writer1
-    inserter.close();
-    writer1.finish();
-    writer.close();
-
-    // Starting read Simple read one map PASS
-    String projection = new String("m1#{a}");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    reader.close();
-    reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals("{a=A}", RowValue.get(0).toString());
-
-    // If row 1 has a==>A, and row 2 doesn't have key a, scanner advance
-    // will still return a==>A
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    System.out.println(RowValue.get(0).toString());
-    Assert.assertEquals("{a=A2}", RowValue.get(0).toString());
-
-    // m1:map(string),m2:map(map(int))";
-    // String STR_STORAGE = "[m1#{a}];[m2#{x|y}]; [m1#{b}, m2#{z}]";
-
-    // test stitch, I can re_use reader, split, but NOT scanner
-    String projection2 = new String("m1#{b}, m2#{x|z}");
-    reader.setProjection(projection2);
-    scanner = reader.getScanner(splits.get(0), true);
-    RowValue = TypesUtils.createTuple(scanner.getSchema());
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    // TODO Assert.assertEquals("{a=A}", RowValue.get(0).toString());
-    Assert.assertEquals("B", ((Map) RowValue.get(0)).get("b"));
-    Assert.assertEquals(321, ((Map) ((Map) RowValue.get(1)).get("x"))
-        .get("m321"));
-    Assert.assertEquals(null, ((Map) ((Map) RowValue.get(1)).get("z")));
-    // m2's x key:
-    /*
-     * m3.put("m311", 311); m3.put("m321", 321); m3.put("m331", 331);
-     */
-    System.out.println(RowValue.get(1).toString());
-
-    TypesUtils.resetTuple(RowValue);
-    scanner.advance();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(null, ((Map) RowValue.get(0)).get("b"));
-    Assert.assertEquals(null, ((Map) ((Map) RowValue.get(1)).get("x")));
-    Assert.assertEquals(323, ((Map) ((Map) RowValue.get(1)).get("z"))
-        .get("m323"));
-
-    reader.close();
-
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestSimple.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestSimple.java
deleted file mode 100644
index ced6a2a73..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestSimple.java
+++ /dev/null
@@ -1,187 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-import java.util.Random;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Projection;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.ExecType;
-import org.apache.pig.PigServer;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.test.MiniCluster;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * 
- * Test projections on complicated column types.
- * 
- */
-public class TestSimple {
-
-  final static String STR_SCHEMA = "s1:bool, s2:int, s3:long, s4:float, s5:string, s6:bytes";
-  final static String STR_STORAGE = "[s1, s2]; [s3, s4]; [s5, s6]";
-  private static Configuration conf;
-  private static Path path;
-  private static FileSystem fs;
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException {
-
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), "TestSimple");
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-    BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-        STR_STORAGE, conf);
-    writer.finish();
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-    BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-    TypesUtils.resetTuple(tuple);
-
-    // insert data in row 1
-    int row = 0;
-    tuple.set(0, true); // bool
-    tuple.set(1, 1); // int
-    tuple.set(2, 1001L); // long
-    tuple.set(3, 1.1f); // float
-    tuple.set(4, "hello world 1"); // string
-    tuple.set(5, new DataByteArray("hello byte 1")); // byte
-
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // insert data in row 2
-    row++;
-    tuple.set(0, false);
-    tuple.set(1, 2); // int
-    tuple.set(2, 1002L); // long
-    tuple.set(3, 3.1f); // float
-    tuple.set(4, "hello world 2"); // string
-    tuple.set(5, new DataByteArray("hello byte 2")); // byte
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // finish building table, closing out the inserter, writer, writer1
-    inserter.close();
-    writer1.finish();
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-    BasicTable.drop(path, conf);
-  }
-
-  // Test simple projection
-  @Test
-  public void testReadSimple1() throws IOException, ParseException {
-    String projection = new String("s6,s5,s4,s3,s2,s1");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(true, RowValue.get(5));
-    Assert.assertEquals(1, RowValue.get(4));
-    Assert.assertEquals(1001L, RowValue.get(3));
-    Assert.assertEquals(1.1f, RowValue.get(2));
-    Assert.assertEquals("hello world 1", RowValue.get(1));
-    Assert.assertEquals("hello byte 1", RowValue.get(0).toString());
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(false, RowValue.get(5));
-    Assert.assertEquals(2, RowValue.get(4));
-    Assert.assertEquals(1002L, RowValue.get(3));
-    Assert.assertEquals(3.1f, RowValue.get(2));
-    Assert.assertEquals("hello world 2", RowValue.get(1));
-    Assert.assertEquals("hello byte 2", RowValue.get(0).toString());
-  }
-
-  // test stitch,
-  @Test
-  public void testReadSimpleStitch() throws IOException, ParseException {
-    String projection2 = new String("s5, s1");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection2);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals("hello world 1", RowValue.get(0));
-    Assert.assertEquals(true, RowValue.get(1));
-
-    scanner.advance();
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals("hello world 2", RowValue.get(0));
-    Assert.assertEquals(false, RowValue.get(1));
-
-    reader.close();
-  }
-
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestSortedBasicTableSplits.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestSortedBasicTableSplits.java
deleted file mode 100644
index fe38f6960..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestSortedBasicTableSplits.java
+++ /dev/null
@@ -1,238 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * 
- * Test projections on complicated column types.
- * 
- */
-public class TestSortedBasicTableSplits {
-  final static String STR_SCHEMA = "f1:bool, r:record(f11:int, f12:long), m:map(string), c:collection(record(f13:double, f14:double, f15:bytes))";
-  // TODO: try map hash split later
-  final static String STR_STORAGE = "[r.f12, f1]; [m]";
-  private static Configuration conf;
-  private static Path path;
-  private static FileSystem fs;
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException {
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), "TestBasicTableSplits");
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-
-    BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-        STR_STORAGE, "r, c", null, conf);
-    writer.finish();
-
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-    TypesUtils.resetTuple(tuple);
-
-    tuple.set(0, true);
-
-    Tuple tupRecord;
-    try {
-      tupRecord = TypesUtils.createTuple(schema.getColumnSchema("r")
-          .getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    // row 1
-    tupRecord.set(0, 1);
-    tupRecord.set(1, 1001L);
-    tuple.set(1, tupRecord);
-
-    Map<String, String> map = new HashMap<String, String>();
-    map.put("a", "x");
-    map.put("b", "y");
-    map.put("c", "z");
-    tuple.set(2, map);
-
-    DataBag bagColl = TypesUtils.createBag();
-    Schema schColl = schema.getColumn(3).getSchema().getColumn(0).getSchema();
-    Tuple tupColl1 = TypesUtils.createTuple(schColl);
-    Tuple tupColl2 = TypesUtils.createTuple(schColl);
-    byte[] abs1 = new byte[3];
-    byte[] abs2 = new byte[4];
-    tupColl1.set(0, 3.1415926);
-    tupColl1.set(1, 1.6);
-    abs1[0] = 11;
-    abs1[1] = 12;
-    abs1[2] = 13;
-    tupColl1.set(2, new DataByteArray(abs1));
-    bagColl.add(tupColl1);
-    tupColl2.set(0, 123.456789);
-    tupColl2.set(1, 100);
-    abs2[0] = 21;
-    abs2[1] = 22;
-    abs2[2] = 23;
-    abs2[3] = 24;
-    tupColl2.set(2, new DataByteArray(abs2));
-    bagColl.add(tupColl2);
-    tuple.set(3, bagColl);
-
-    int row = 0;
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // row 2
-    row++;
-    TypesUtils.resetTuple(tuple);
-    TypesUtils.resetTuple(tupRecord);
-    map.clear();
-    tuple.set(0, false);
-    tupRecord.set(0, 2);
-    tupRecord.set(1, 1002L);
-    tuple.set(1, tupRecord);
-    map.put("boy", "girl");
-    map.put("adam", "amy");
-    map.put("bob", "becky");
-    map.put("carl", "cathy");
-    tuple.set(2, map);
-    bagColl.clear();
-    TypesUtils.resetTuple(tupColl1);
-    TypesUtils.resetTuple(tupColl2);
-    tupColl1.set(0, 7654.321);
-    tupColl1.set(1, 0.0001);
-    abs1[0] = 31;
-    abs1[1] = 32;
-    abs1[2] = 33;
-    tupColl1.set(2, new DataByteArray(abs1));
-    bagColl.add(tupColl1);
-    tupColl2.set(0, 0.123456789);
-    tupColl2.set(1, 0.3333);
-    abs2[0] = 41;
-    abs2[1] = 42;
-    abs2[2] = 43;
-    abs2[3] = 44;
-    tupColl2.set(2, new DataByteArray(abs2));
-    bagColl.add(tupColl2);
-    tuple.set(3, bagColl);
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    inserter.close();
-    writer1.finish();
-
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-  }
-
-  @Test
-  public void test1() throws IOException, ParseException {
-    String projection = new String("r.f12, f1");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    // long totalBytes = reader.getStatus().getSize();
-
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    reader.close();
-    reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple value = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(value);
-
-    Assert.assertEquals(1001L, value.get(0));
-    Assert.assertEquals(true, value.get(1));
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(value);
-    Assert.assertEquals(1002L, value.get(0));
-    Assert.assertEquals(false, value.get(1));
-
-    reader.close();
-  }
-
-  @Test
-  public void testStitch() throws IOException, ParseException {
-    String projection = new String("f1, r");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    // long totalBytes = reader.getStatus().getSize();
-
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    reader.close();
-    reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple value = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(value);
-
-    Tuple recordTuple = (Tuple) value.get(1);
-    Assert.assertEquals(1, recordTuple.get(0));
-    Assert.assertEquals(1001L, recordTuple.get(1));
-    Assert.assertEquals(true, value.get(0));
-    reader.close();
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestTypeCheck.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestTypeCheck.java
deleted file mode 100644
index 5429a03aa..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestTypeCheck.java
+++ /dev/null
@@ -1,266 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * This is to test type check for writes in Zebra at IO level.
- */
-public class TestTypeCheck extends BaseTestCase {
-  final static String STR_STORAGE = "[r.f12, f1]; [m]";
-  private static Path path;
-  
-
-  @BeforeClass
-  public static void setUp() throws Exception {
-    init();
-  }
-  
-  @Test
-  public void testPositive1() throws IOException, ParseException {
-    path = getTableFullPath("TestTypeCheck");
-    removeDir(path);
-    
-    String STR_SCHEMA = "f1:bool, r:record(f11:int, f12:long), m:map(string), c:collection(record(f13:double, f14:double, f15:bytes))";
-    BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-        STR_STORAGE, conf);
-    writer.finish();
-
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-    TypesUtils.resetTuple(tuple);
-
-    tuple.set(0, true);
-
-    Tuple tupRecord;
-    try {
-      tupRecord = TypesUtils.createTuple(schema.getColumnSchema("r")
-          .getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-    tupRecord.set(0, 1);
-    tupRecord.set(1, 1001L);
-    tuple.set(1, tupRecord);
-
-    Map<String, String> map = new HashMap<String, String>();
-    map.put("a", "x");
-    map.put("b", "y");
-    map.put("c", "z");
-    tuple.set(2, map);
-
-    DataBag bagColl = TypesUtils.createBag();
-    Schema schColl = schema.getColumn(3).getSchema().getColumn(0).getSchema();
-    Tuple tupColl1 = TypesUtils.createTuple(schColl);
-    Tuple tupColl2 = TypesUtils.createTuple(schColl);
-    byte[] abs1 = new byte[3];
-    byte[] abs2 = new byte[4];
-    tupColl1.set(0, 3.1415926);
-    tupColl1.set(1, 1.6);
-    abs1[0] = 11;
-    abs1[1] = 12;
-    abs1[2] = 13;
-    tupColl1.set(2, new DataByteArray(abs1));
-    bagColl.add(tupColl1);
-    tupColl2.set(0, 123.456789);
-    tupColl2.set(1, 100);
-    abs2[0] = 21;
-    abs2[1] = 22;
-    abs2[2] = 23;
-    abs2[3] = 24;
-    tupColl2.set(2, new DataByteArray(abs2));
-    bagColl.add(tupColl2);
-    tuple.set(3, bagColl);
-
-    int row = 0;
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-    inserter.close();
-    writer1.finish();
-
-    writer.close();
-  }
-  
-  // float column sees double value;
-  @Test (expected = IOException.class)
-  public void testNegative1() throws IOException, ParseException {
-    path = getTableFullPath("TestTypeCheck");
-    removeDir(path);
-    String STR_SCHEMA = "f1:bool, r:record(f11:int, f12:long), m:map(string), c:collection(record(f13:double, f14:float, f15:bytes))";
-    BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-        STR_STORAGE, conf);
-    writer.finish();
-
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-    TypesUtils.resetTuple(tuple);
-
-    tuple.set(0, true);
-
-    Tuple tupRecord;
-    try {
-      tupRecord = TypesUtils.createTuple(schema.getColumnSchema("r")
-          .getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-    tupRecord.set(0, 1);
-    tupRecord.set(1, 1001L);
-    tuple.set(1, tupRecord);
-
-    Map<String, String> map = new HashMap<String, String>();
-    map.put("a", "x");
-    map.put("b", "y");
-    map.put("c", "z");
-    tuple.set(2, map);
-
-    DataBag bagColl = TypesUtils.createBag();
-    Schema schColl = schema.getColumn(3).getSchema().getColumn(0).getSchema();
-    Tuple tupColl1 = TypesUtils.createTuple(schColl);
-    Tuple tupColl2 = TypesUtils.createTuple(schColl);
-    byte[] abs1 = new byte[3];
-    byte[] abs2 = new byte[4];
-    tupColl1.set(0, 3.1415926);
-    tupColl1.set(1, 1.6);
-    abs1[0] = 11;
-    abs1[1] = 12;
-    abs1[2] = 13;
-    tupColl1.set(2, new DataByteArray(abs1));
-    bagColl.add(tupColl1);
-    tupColl2.set(0, 123.456789);
-    tupColl2.set(1, 100);
-    abs2[0] = 21;
-    abs2[1] = 22;
-    abs2[2] = 23;
-    abs2[3] = 24;
-    tupColl2.set(2, new DataByteArray(abs2));
-    bagColl.add(tupColl2);
-    tuple.set(3, bagColl);
-
-    int row = 0;
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-    inserter.close();
-    writer1.finish();
-
-    writer.close();
-  }
-
-  // int column sees a string value;
-  @Test (expected = IOException.class)
-  public void testNegative2() throws IOException, ParseException {
-    path = getTableFullPath("TestTypeCheck");
-    removeDir(path);
-
-    String STR_SCHEMA = "f1:bool, r:record(f11:int, f12:long), m:map(string), c:collection(record(f13:double, f14:double, f15:bytes))";
-    BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-        STR_STORAGE, conf);
-    writer.finish();
-
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-    TypesUtils.resetTuple(tuple);
-
-    tuple.set(0, true);
-
-    Tuple tupRecord;
-    try {
-      tupRecord = TypesUtils.createTuple(schema.getColumnSchema("r")
-          .getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-    tupRecord.set(0, "abc");
-    tupRecord.set(1, 1001L);
-    tuple.set(1, tupRecord);
-
-    Map<String, String> map = new HashMap<String, String>();
-    map.put("a", "x");
-    map.put("b", "y");
-    map.put("c", "z");
-    tuple.set(2, map);
-
-    DataBag bagColl = TypesUtils.createBag();
-    Schema schColl = schema.getColumn(3).getSchema().getColumn(0).getSchema();
-    Tuple tupColl1 = TypesUtils.createTuple(schColl);
-    Tuple tupColl2 = TypesUtils.createTuple(schColl);
-    byte[] abs1 = new byte[3];
-    byte[] abs2 = new byte[4];
-    tupColl1.set(0, 3.1415926);
-    tupColl1.set(1, 1.6);
-    abs1[0] = 11;
-    abs1[1] = 12;
-    abs1[2] = 13;
-    tupColl1.set(2, new DataByteArray(abs1));
-    bagColl.add(tupColl1);
-    tupColl2.set(0, 123.456789);
-    tupColl2.set(1, 100);
-    abs2[0] = 21;
-    abs2[1] = 22;
-    abs2[2] = 23;
-    abs2[3] = 24;
-    tupColl2.set(2, new DataByteArray(abs2));
-    bagColl.add(tupColl2);
-    tuple.set(3, bagColl);
-
-    int row = 0;
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-    inserter.close();
-    writer1.finish();
-
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-     removeDir(path);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestWrite.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestWrite.java
deleted file mode 100644
index 49908ba9e..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/io/TestWrite.java
+++ /dev/null
@@ -1,313 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.io;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Random;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Projection;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * 
- * Test projections on complicated column types.
- * 
- */
-public class TestWrite {
-  final static String STR_SCHEMA = "s1:bool, s2:int, s3:long, s4:double, s5:string, s6:bytes, r1:record(f1:int, f2:long), r2:record(r3:record(f3:double, f4)), m1:map(string),m2:map(map(int)), c:collection(record(f13:double, f14:double, f15:bytes))";
-  final static String STR_STORAGE = "[s1, s2]; [m1#{a}]; [r1.f1]; [s3, s4, r2.r3.f3]; [s5, s6, m2#{x|y}]; [r1.f2, m1#{b}]; [r2.r3.f4, m2#{z}]";
-  private static Configuration conf;
-  private static Path path;
-  private static FileSystem fs;
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException {
-    conf = new Configuration();
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), "TestWrite");
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-
-    BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-        STR_STORAGE, conf);
-    writer.finish();
-
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-    BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-    TypesUtils.resetTuple(tuple);
-
-    Tuple tupRecord1;
-    try {
-      tupRecord1 = TypesUtils.createTuple(schema.getColumnSchema("r1")
-          .getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    Tuple tupRecord2;
-    try {
-      tupRecord2 = TypesUtils.createTuple(schema.getColumnSchema("r2")
-          .getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    Tuple tupRecord3;
-    try {
-      tupRecord3 = TypesUtils.createTuple(new Schema("f3:float, f4"));
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    // row 1
-    tuple.set(0, true); // bool
-    tuple.set(1, 1); // int
-    tuple.set(2, 1001L); // long
-    tuple.set(3, 1.1); // float
-    tuple.set(4, "hello world 1"); // string
-    tuple.set(5, new DataByteArray("hello byte 1")); // byte
-
-    // r1:record(f1:int, f2:long
-    tupRecord1.set(0, 1);
-    tupRecord1.set(1, 1001L);
-    tuple.set(6, tupRecord1);
-
-    // r2:record(r3:record(f3:float, f4))
-    tupRecord2.set(0, tupRecord3);
-    tupRecord3.set(0, 1.3);
-    tupRecord3.set(1, new DataByteArray("r3 row 1 byte array "));
-    tuple.set(7, tupRecord2);
-
-    // m1:map(string)
-    Map<String, String> m1 = new HashMap<String, String>();
-    m1.put("a", "A");
-    m1.put("b", "B");
-    m1.put("c", "C");
-    tuple.set(8, m1);
-
-    // m2:map(map(int))
-    HashMap<String, Map> m2 = new HashMap<String, Map>();
-    Map<String, Integer> m3 = new HashMap<String, Integer>();
-    m3.put("m311", 311);
-    m3.put("m321", 321);
-    m3.put("m331", 331);
-    Map<String, Integer> m4 = new HashMap<String, Integer>();
-    m4.put("m411", 411);
-    m4.put("m421", 421);
-    m4.put("m431", 431);
-    m2.put("x", m3);
-    m2.put("y", m4);
-    tuple.set(9, m2);
-
-    // c:collection(f13:double, f14:float, f15:bytes)
-    DataBag bagColl = TypesUtils.createBag();
-    Schema schColl = schema.getColumn(10).getSchema().getColumn(0).getSchema();
-    Tuple tupColl1 = TypesUtils.createTuple(schColl);
-    Tuple tupColl2 = TypesUtils.createTuple(schColl);
-    byte[] abs1 = new byte[3];
-    byte[] abs2 = new byte[4];
-    tupColl1.set(0, 3.1415926);
-    tupColl1.set(1, 1.6);
-    abs1[0] = 11;
-    abs1[1] = 12;
-    abs1[2] = 13;
-    tupColl1.set(2, new DataByteArray(abs1));
-    bagColl.add(tupColl1);
-    tupColl2.set(0, 123.456789);
-    tupColl2.set(1, 100);
-    abs2[0] = 21;
-    abs2[1] = 22;
-    abs2[2] = 23;
-    abs2[3] = 24;
-    tupColl2.set(2, new DataByteArray(abs2));
-    bagColl.add(tupColl2);
-    tuple.set(10, bagColl);
-
-    int row = 0;
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // row 2
-    row++;
-    TypesUtils.resetTuple(tuple);
-    TypesUtils.resetTuple(tupRecord1);
-    TypesUtils.resetTuple(tupRecord2);
-    TypesUtils.resetTuple(tupRecord3);
-    m1.clear();
-    m2.clear();
-    m3.clear();
-    m4.clear();
-    tuple.set(0, false);
-    tuple.set(1, 2); // int
-    tuple.set(2, 1002L); // long
-    tuple.set(3, 3.1); // float
-    tuple.set(4, "hello world 2"); // string
-    tuple.set(5, new DataByteArray("hello byte 2")); // byte
-
-    // r1:record(f1:int, f2:long
-    tupRecord1.set(0, 2);
-    tupRecord1.set(1, 1002L);
-    tuple.set(6, tupRecord1);
-
-    // r2:record(r3:record(f3:float, f4))
-    tupRecord2.set(0, tupRecord3);
-    tupRecord3.set(0, 2.3);
-    tupRecord3.set(1, new DataByteArray("r3 row2  byte array"));
-    tuple.set(7, tupRecord2);
-
-    // m1:map(string)
-    m1.put("a2", "A2");
-    m1.put("b2", "B2");
-    m1.put("c2", "C2");
-    tuple.set(8, m1);
-
-    // m2:map(map(int))
-    m3.put("m321", 321);
-    m3.put("m322", 322);
-    m3.put("m323", 323);
-    m2.put("z", m3);
-    tuple.set(9, m2);
-
-    // c:collection(f13:double, f14:float, f15:bytes)
-    bagColl.clear();
-    TypesUtils.resetTuple(tupColl1);
-    TypesUtils.resetTuple(tupColl2);
-    tupColl1.set(0, 7654.321);
-    tupColl1.set(1, 0.0001);
-    abs1[0] = 31;
-    abs1[1] = 32;
-    abs1[2] = 33;
-    tupColl1.set(2, new DataByteArray(abs1));
-    bagColl.add(tupColl1);
-    tupColl2.set(0, 0.123456789);
-    tupColl2.set(1, 0.3333);
-    abs2[0] = 41;
-    abs2[1] = 42;
-    abs2[2] = 43;
-    abs2[3] = 44;
-    tupColl2.set(2, new DataByteArray(abs2));
-    bagColl.add(tupColl2);
-    tuple.set(10, bagColl);
-
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    inserter.close();
-    writer1.finish();
-
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-  }
-
-  @Test
-  public void test1() throws IOException, ParseException {
-    String projection = new String("r1.f2, s1");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    // long totalBytes = reader.getStatus().getSize();
-
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    reader.close();
-    reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple value = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(value);
-
-    Assert.assertEquals(1001L, value.get(0));
-    Assert.assertEquals(true, value.get(1));
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(value);
-    Assert.assertEquals(1002L, value.get(0));
-    Assert.assertEquals(false, value.get(1));
-
-    reader.close();
-  }
-
-  @Test
-  public void testStitch() throws IOException, ParseException {
-    String projection = new String("s1, r1");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    // long totalBytes = reader.getStatus().getSize();
-
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    reader.close();
-    reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple value = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(value);
-
-    Tuple recordTuple = (Tuple) value.get(1);
-    Assert.assertEquals(1, recordTuple.get(0));
-    Assert.assertEquals(1001L, recordTuple.get(1));
-    Assert.assertEquals(true, value.get(0));
-    reader.close();
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/ArticleGenerator.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/ArticleGenerator.java
deleted file mode 100644
index 2e6a3d068..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/ArticleGenerator.java
+++ /dev/null
@@ -1,152 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-import java.util.Random;
-
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.zebra.tfile.RandomDistribution.DiscreteRNG;
-import org.apache.hadoop.zebra.tfile.RandomDistribution.Flat;
-
-/**
- * Generate some input text files.
- */
-class ArticleGenerator {
-  Random random;
-  Dictionary dict;
-  int pageWidth;
-  DiscreteRNG lastLineLenGen;
-  DiscreteRNG paragraphLineLenGen;
-  DiscreteRNG paragraphLenGen;
-  long wordCount;
-  long lineCount;
-
-  /**
-   * Create an article generator.
-   * 
-   * @param dictWordCnt
-   *          Number of words in the dictionary.
-   * @param minWordLen
-   *          Minimum word length
-   * @param maxWordLen
-   *          Maximum word length
-   * @param lineWidth
-   *          Line width.
-   */
-  ArticleGenerator(int dictWordCnt, int minWordLen, int maxWordLen,
-      int pageWidth) {
-    random = new Random(System.nanoTime());
-    dict = new Dictionary(random, dictWordCnt, minWordLen, maxWordLen, 100);
-    this.pageWidth = pageWidth;
-    lastLineLenGen = new Flat(random, 1, pageWidth);
-    paragraphLineLenGen = new Flat(random, pageWidth * 3 / 4, pageWidth);
-    paragraphLenGen = new Flat(random, 1, 40);
-  }
-
-  /**
-   * Create an article
-   * 
-   * @param fs
-   *          File system.
-   * @param path
-   *          path of the file
-   * @param length
-   *          Expected size of the file.
-   * @throws IOException
-   */
-  void createArticle(FileSystem fs, Path path, long length) throws IOException {
-    FSDataOutputStream fsdos = fs.create(path, false);
-    StringBuilder sb = new StringBuilder();
-    int remainLinesInParagraph = paragraphLenGen.nextInt();
-    while (fsdos.getPos() < length) {
-      if (remainLinesInParagraph == 0) {
-        remainLinesInParagraph = paragraphLenGen.nextInt();
-        fsdos.write('\n');
-      }
-      int lineLen = paragraphLineLenGen.nextInt();
-      if (--remainLinesInParagraph == 0) {
-        lineLen = lastLineLenGen.nextInt();
-      }
-      sb.setLength(0);
-      while (sb.length() < lineLen) {
-        if (sb.length() > 0) {
-          sb.append(' ');
-        }
-        sb.append(dict.nextWord());
-        ++wordCount;
-      }
-      sb.append('\n');
-      fsdos.write(sb.toString().getBytes());
-      ++lineCount;
-    }
-    fsdos.close();
-  }
-
-  /**
-   * Create a bunch of files under the same directory.
-   * 
-   * @param fs
-   *          File system
-   * @param parent
-   *          directory where files should be created
-   * @param prefix
-   *          prefix name of the files
-   * @param n
-   *          total number of files
-   * @param length
-   *          length of each file.
-   * @throws IOException
-   */
-  void batchArticalCreation(FileSystem fs, Path parent, String prefix, int n,
-      long length) throws IOException {
-    for (int i = 0; i < n; ++i) {
-      createArticle(fs, new Path(parent, String.format("%s%06d", prefix, i)),
-          length);
-    }
-  }
-
-  static class Summary {
-    long wordCount;
-    long lineCount;
-    Map<String, Long> wordCntDist;
-
-    Summary() {
-      wordCntDist = new HashMap<String, Long>();
-    }
-  }
-
-  void resetSummary() {
-    wordCount = 0;
-    lineCount = 0;
-    dict.resetWordCnts();
-  }
-
-  Summary getSummary() {
-    Summary ret = new Summary();
-    ret.wordCount = wordCount;
-    ret.lineCount = lineCount;
-    ret.wordCntDist = dict.getWordCounts();
-    return ret;
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/Dictionary.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/Dictionary.java
deleted file mode 100644
index 3d686fa0c..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/Dictionary.java
+++ /dev/null
@@ -1,113 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapred;
-
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Map;
-import java.util.Random;
-import java.util.Set;
-
-import org.apache.hadoop.zebra.tfile.RandomDistribution.Binomial;
-import org.apache.hadoop.zebra.tfile.RandomDistribution.DiscreteRNG;
-import org.apache.hadoop.zebra.tfile.RandomDistribution.Zipf;
-
-/**
- * A dictionary that generates English words, whose frequency follows Zipf
- * distributions, and length follows Binomial distribution.
- */
-class Dictionary {
-  private static final double BINOMIAL_P = 0.3;
-  private static final double SIGMA = 1.1;
-  private final int lead;
-  private final Zipf zipf;
-  private final String[] dict;
-  private final long[] wordCnts;
-
-  private static String makeWord(DiscreteRNG rng, Random random) {
-    int len = rng.nextInt();
-    StringBuilder sb = new StringBuilder(len);
-    for (int i = 0; i < len; ++i) {
-      sb.append((char) ('a' + random.nextInt(26)));
-    }
-    return sb.toString();
-  }
-
-  /**
-   * Constructor
-   * 
-   * @param entries
-   *          How many words exist in the dictionary.
-   * @param minWordLen
-   *          Minimum word length.
-   * @param maxWordLen
-   *          Maximum word length.
-   * @param freqRatio
-   *          Expected ratio between the most frequent words and the least
-   *          frequent words. (e.g. 100)
-   */
-  public Dictionary(Random random, int entries, int minWordLen, int maxWordLen,
-      int freqRatio) {
-    Binomial binomial = new Binomial(random, minWordLen, maxWordLen, BINOMIAL_P);
-    lead = Math.max(0,
-        (int) (entries / (Math.exp(Math.log(freqRatio) / SIGMA) - 1)) - 1);
-    zipf = new Zipf(random, lead, entries + lead, 1.1);
-    dict = new String[entries];
-    // Use a set to ensure no dup words in dictionary
-    Set<String> dictTmp = new HashSet<String>();
-    for (int i = 0; i < entries; ++i) {
-      while (true) {
-        String word = makeWord(binomial, random);
-        if (!dictTmp.contains(word)) {
-          dictTmp.add(word);
-          dict[i] = word;
-          break;
-        }
-      }
-    }
-    wordCnts = new long[dict.length];
-  }
-
-  /**
-   * Get the next word from the dictionary.
-   * 
-   * @return The next word from the dictionary.
-   */
-  public String nextWord() {
-    int index = zipf.nextInt() - lead;
-    ++wordCnts[index];
-    return dict[index];
-  }
-
-  public void resetWordCnts() {
-    for (int i = 0; i < wordCnts.length; ++i) {
-      wordCnts[i] = 0;
-    }
-  }
-
-  public Map<String, Long> getWordCounts() {
-    Map<String, Long> ret = new HashMap<String, Long>();
-    for (int i = 0; i < dict.length; ++i) {
-      if (wordCnts[i] > 0) {
-        ret.put(dict[i], wordCnts[i]);
-      }
-    }
-    return ret;
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TableMRSample.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TableMRSample.java
deleted file mode 100644
index 7a2bc8d96..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TableMRSample.java
+++ /dev/null
@@ -1,127 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.IOException;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapred.FileInputFormat;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.Mapper;
-import org.apache.hadoop.mapred.OutputCollector;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapred.TextInputFormat;
-import org.apache.hadoop.zebra.mapred.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.Tuple;
-
-/**
- * This is a sample a complete MR sample code for Table. It doens't contain
- * 'read' part. But, it should be similar and easier to write. Refer to test
- * cases in the same directory.
- * 
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * this 2
- * is 1
- * a 4 
- * test 2 
- * hello 1 
- * world 3
- * </pre>
- * 
- */
-public class TableMRSample {
-  static class MapClass implements
-      Mapper<LongWritable, Text, BytesWritable, Tuple> {
-    private BytesWritable bytesKey;
-    private Tuple tupleRow;
-
-    @Override
-    public void map(LongWritable key, Text value,
-        OutputCollector<BytesWritable, Tuple> output, Reporter reporter)
-        throws IOException {
-
-      // value should contain "word count"
-      String[] wdct = value.toString().split(" ");
-      if (wdct.length != 2) {
-        // LOG the error
-        return;
-      }
-
-      byte[] word = wdct[0].getBytes();
-      bytesKey.set(word, 0, word.length);
-      tupleRow.set(0, new String(word));
-      tupleRow.set(1, Integer.parseInt(wdct[1]));
-
-      output.collect(bytesKey, tupleRow);
-    }
-
-    @Override
-    public void configure(JobConf job) {
-      bytesKey = new BytesWritable();
-      try {
-        Schema outSchema = BasicTableOutputFormat.getSchema(job);
-        tupleRow = TypesUtils.createTuple(outSchema);
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      } catch (ParseException e) {
-        throw new RuntimeException(e);
-      }
-    }
-
-    @Override
-    public void close() throws IOException {
-      // no-op
-    }
-  }
-
-  public static void main(String[] args) throws ParseException, IOException {
-    JobConf jobConf = new JobConf();
-    jobConf.setJobName("tableMRSample");
-    jobConf.set("table.output.tfile.compression", "gz");
-
-    // input settings
-    jobConf.setInputFormat(TextInputFormat.class);
-    jobConf.setMapperClass(TableMRSample.MapClass.class);
-    FileInputFormat.setInputPaths(jobConf, new Path(
-        "/user/joe/inputdata/input.txt"));
-    jobConf.setNumMapTasks(2);
-
-    // output settings
-    Path outPath = new Path("/user/joe/outputdata/");
-    jobConf.setOutputFormat(BasicTableOutputFormat.class);
-    BasicTableOutputFormat.setOutputPath(jobConf, outPath);
-    // set the logical schema with 2 columns
-    BasicTableOutputFormat.setSchema(jobConf, "word:string, count:int");
-    // for demo purposes, create 2 physical column groups
-    BasicTableOutputFormat.setStorageHint(jobConf, "[word];[count]");
-
-    // set map-only job.
-    jobConf.setNumReduceTasks(0);
-    JobClient.runJob(jobConf);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TableMRSample1.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TableMRSample1.java
deleted file mode 100644
index 789b4ce00..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TableMRSample1.java
+++ /dev/null
@@ -1,127 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.IOException;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapred.FileInputFormat;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.Mapper;
-import org.apache.hadoop.mapred.OutputCollector;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapred.TextInputFormat;
-import org.apache.hadoop.zebra.mapred.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.Tuple;
-
-/**
- * This is a sample a complete MR sample code for Table. It doens't contain
- * 'read' part. But, it should be similar and easier to write. Refer to test
- * cases in the same directory.
- * 
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * this 2
- * is 1
- * a 4 
- * test 2 
- * hello 1 
- * world 3
- * </pre>
- * 
- */
-public class TableMRSample1 {
-  static class MapClass implements
-      Mapper<LongWritable, Text, BytesWritable, Tuple> {
-    private BytesWritable bytesKey;
-    private Tuple tupleRow;
-
-    @Override
-    public void map(LongWritable key, Text value,
-        OutputCollector<BytesWritable, Tuple> output, Reporter reporter)
-        throws IOException {
-
-      // value should contain "word count"
-      String[] wdct = value.toString().split(" ");
-      if (wdct.length != 2) {
-        // LOG the error
-        throw new IOException("Does not contain two fields");
-      }
-
-      byte[] word = wdct[0].getBytes();
-      bytesKey.set(word, 0, word.length);
-      tupleRow.set(0, new String(word));
-      tupleRow.set(1, Integer.parseInt(wdct[1]));
-
-      output.collect(bytesKey, tupleRow);
-    }
-
-    @Override
-    public void configure(JobConf job) {
-      bytesKey = new BytesWritable();
-      try {
-        Schema outSchema = BasicTableOutputFormat.getSchema(job);
-        tupleRow = TypesUtils.createTuple(outSchema);
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      } catch (ParseException e) {
-        throw new RuntimeException(e);
-      }
-    }
-
-    @Override
-    public void close() throws IOException {
-      // no-op
-    }
-
-  }
-
-  public static void main(String[] args) throws ParseException, IOException {
-    JobConf jobConf = new JobConf();
-    jobConf.setJobName("tableMRSample");
-    jobConf.set("table.output.tfile.compression", "gz");
-
-    // input settings
-    jobConf.setInputFormat(TextInputFormat.class);
-    jobConf.setMapperClass(TableMRSample1.MapClass.class);
-    FileInputFormat.setInputPaths(jobConf, new Path("/user/joe/input.txt"));
-    jobConf.setNumMapTasks(2);
-
-    // output settings
-    Path outPath = new Path("/user/joe/tableout");
-    jobConf.setOutputFormat(BasicTableOutputFormat.class);
-    BasicTableOutputFormat.setOutputPath(jobConf, outPath);
-    // set the logical schema with 2 columns
-    BasicTableOutputFormat.setSchema(jobConf, "word:string, count:int");
-    // for demo purposes, create 2 physical column groups
-    BasicTableOutputFormat.setStorageHint(jobConf, "[word];[count]");
-
-    // set map-only job.
-    jobConf.setNumReduceTasks(0);
-    JobClient.runJob(jobConf);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TableMRSample2.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TableMRSample2.java
deleted file mode 100644
index 135052f92..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TableMRSample2.java
+++ /dev/null
@@ -1,116 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.ArrayList;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapred.FileInputFormat;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.Mapper;
-import org.apache.hadoop.mapred.OutputCollector;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapred.TextInputFormat;
-import org.apache.hadoop.zebra.mapred.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.mapred.TableInputFormat;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.Tuple;
-
-/**
- * This is a sample to show using zebra table to do a simple basic union in
- * map/reduce * To run this, we need have two basic tables ready. They contain
- * the data as in Sample 1, i.e., (word, count). In this example, they are at:
- * /homes/chaow/mapredu/t1 /homes/chaow/mapredu/t2 The resulting table is put
- * at: /homes/chaow/mapredu2/t1
- * 
- */
-public class TableMRSample2 {
-  static class MapClass implements
-      Mapper<BytesWritable, Tuple, BytesWritable, Tuple> {
-    private BytesWritable bytesKey;
-    private Tuple tupleRow;
-
-    @Override
-    public void map(BytesWritable key, Tuple value,
-        OutputCollector<BytesWritable, Tuple> output, Reporter reporter)
-        throws IOException
-
-    {
-      System.out.println(key.toString() + value.toString());
-      output.collect(key, value);
-    }
-
-    @Override
-    public void configure(JobConf job) {
-      bytesKey = new BytesWritable();
-      try {
-        Schema outSchema = BasicTableOutputFormat.getSchema(job);
-        tupleRow = TypesUtils.createTuple(outSchema);
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      } catch (ParseException e) {
-        throw new RuntimeException(e);
-      }
-    }
-
-    @Override
-    public void close() throws IOException {
-      // no-op
-    }
-
-    public static void main(String[] args) throws ParseException, IOException {
-      JobConf jobConf = new JobConf();
-      jobConf.setJobName("tableMRSample");
-      jobConf.set("table.output.tfile.compression", "gz");
-
-      // input settings
-      jobConf.setInputFormat(TableInputFormat.class);
-      jobConf.setOutputFormat(BasicTableOutputFormat.class);
-      jobConf.setMapperClass(TableMRSample2.MapClass.class);
-
-      List<Path> paths = new ArrayList<Path>(2);
-      Path p = new Path("/homes/chaow/mapredu/t1");
-      System.out.println("path = " + p);
-      paths.add(p);
-      p = new Path("/homes/chaow/mapredu/t2");
-      paths.add(p);
-
-      TableInputFormat.setInputPaths(jobConf, paths.toArray(new Path[2]));
-      TableInputFormat.setProjection(jobConf, "word");
-      BasicTableOutputFormat.setOutputPath(jobConf, new Path(
-          "/homes/chaow/mapredu2/t1"));
-
-      BasicTableOutputFormat.setSchema(jobConf, "word:string");
-      BasicTableOutputFormat.setStorageHint(jobConf, "[word]");
-
-      // set map-only job.
-      jobConf.setNumReduceTasks(0);
-      jobConf.setNumMapTasks(2);
-      JobClient.runJob(jobConf);
-    }
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TableMRSampleSortedTable.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TableMRSampleSortedTable.java
deleted file mode 100644
index 9ff6f5789..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TableMRSampleSortedTable.java
+++ /dev/null
@@ -1,192 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.TreeMap;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapred.FileInputFormat;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.Mapper;
-import org.apache.hadoop.mapred.OutputCollector;
-import org.apache.hadoop.mapred.Reducer;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapred.TextInputFormat;
-import org.apache.hadoop.zebra.mapred.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.mapred.TestBasicTableIOFormatLocalFS.InvIndex;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.SortInfo;
-import org.apache.hadoop.zebra.schema.*;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.types.*;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DefaultTuple;
-import org.apache.pig.data.Tuple;
-
-/**
- * This is a sample a complete MR sample code for Table. It doens't contain
- * 'read' part. But, it should be similar and easier to write. Refer to test
- * cases in the same directory.
- * 
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * this 2
- * is 1
- * a 4 
- * test 2 
- * hello 1 
- * world 3
- * </pre>
- * 
- */
-public class TableMRSampleSortedTable {
-	static class MapClass implements
-      Mapper<LongWritable, Text, BytesWritable, Tuple> {
-    private BytesWritable bytesKey;
-    private Tuple tupleRow;
-    private Object javaObj;
-    private byte[] types;
-    private ColumnType[] sortColTypes;
-
-    @Override
-    public void map(LongWritable key, Text value,
-        OutputCollector<BytesWritable, Tuple> output, Reporter reporter)
-        throws IOException {
-        // value should contain "word count"
-        String[] wdct = value.toString().split(" ");
-        if (wdct.length != 2) {
-          // LOG the error
-          return;
-        }
-
-        byte[] word = wdct[0].getBytes();
-
-        // This key has to be created by user
-        bytesKey.set(word, 0, word.length);
-        
-        // This tuple has to be created by user
-        tupleRow.set(0, new String(word));
-        tupleRow.set(1, Integer.parseInt(wdct[1]));
-
-        output.collect(bytesKey, tupleRow);
-    }
-
-    @Override
-    public void configure(JobConf job) {
-      bytesKey = new BytesWritable();
-      try {
-    	  
-    	/* New M/R Interface to get sort information */  
-	    SortInfo sortInfo = BasicTableOutputFormat.getSortInfo(job);
-	    
-	    /* New M/R Interface SortInfo is exposed to user 
-	     * To get types of sort columns.
-	     * Similar interface to get names and indices
-	     */
-   	    sortColTypes  = sortInfo.getSortColumnTypes();
-   	    types = new byte[sortColTypes.length];
-   	    for(int i =0 ; i < sortColTypes.length; ++i){
-   	      types[i] = sortColTypes[i].pigDataType();
-   	    }
-        Schema outSchema = BasicTableOutputFormat.getSchema(job);
-        tupleRow = TypesUtils.createTuple(outSchema);
-                
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      } catch (ParseException e) {
-        throw new RuntimeException(e);
-      }
-    }
-
-    @Override
-    public void close() throws IOException {
-      // no-op
-    }
-  }
-
-  static class ReduceClass implements
-    Reducer<BytesWritable, Tuple, BytesWritable, Tuple> {
-      Tuple outRow;
-     
-
-    @Override
-    public void configure(JobConf job) {
-    }
-
-    @Override
-    public void close() throws IOException {
-    }	
-    public void reduce(BytesWritable key, Iterator<Tuple> values,
-      OutputCollector<BytesWritable, Tuple> output, Reporter reporter)
-   	    throws IOException {
-    	  try {
-    		for(; values.hasNext();)  {
-    	      output.collect(key, values.next());
-    		}  
-    	  } catch (ExecException e) {
-    	    e.printStackTrace();
-    	  }
-    }
-  
-  }  
-  
-  public static void main(String[] args) throws ParseException, IOException, Exception {
-    JobConf jobConf = new JobConf();
-    jobConf.setJobName("tableMRSample");
-    jobConf.set("table.output.tfile.compression", "gz");
-
-    // input settings
-    jobConf.setInputFormat(TextInputFormat.class);
-    jobConf.setMapperClass(TableMRSampleSortedTable.MapClass.class);
-    jobConf.setReducerClass(TableMRSampleSortedTable.ReduceClass.class);
-    jobConf.setMapOutputKeyClass(BytesWritable.class);
-    jobConf.setMapOutputValueClass(DefaultTuple.class);
-    FileInputFormat.setInputPaths(jobConf, new Path(
-        "/home/gauravj/work/grid/myTesting/input.txt"));
-    jobConf.setNumMapTasks(1);
-
-    // output settings
-    Path outPath = new Path("/home/gauravj/work/grid/myTesting/tableOuts");
-    jobConf.setOutputFormat(BasicTableOutputFormat.class);
-    BasicTableOutputFormat.setOutputPath(jobConf, outPath);
-    // set the logical schema with 2 columns
-    BasicTableOutputFormat.setSchema(jobConf, "word:string, count:int");
-    // for demo purposes, create 2 physical column groups
-    BasicTableOutputFormat.setStorageHint(jobConf, "[word];[count]");
-    
-    /* New M/R Interface */
-    /* Set sort columns in a comma separated string */
-    /* Each sort column should belong to schema columns */
-    BasicTableOutputFormat.setSortInfo(jobConf, "word, count");
-
-    // set map-only job.
-    jobConf.setNumReduceTasks(1);
-    JobClient.runJob(jobConf);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TableMRSortedTableZebraKeyGenerator.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TableMRSortedTableZebraKeyGenerator.java
deleted file mode 100644
index c5d0ecad2..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TableMRSortedTableZebraKeyGenerator.java
+++ /dev/null
@@ -1,194 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.TreeMap;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapred.FileInputFormat;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.Mapper;
-import org.apache.hadoop.mapred.OutputCollector;
-import org.apache.hadoop.mapred.Reducer;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapred.TextInputFormat;
-import org.apache.hadoop.zebra.mapred.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.mapred.TestBasicTableIOFormatLocalFS.InvIndex;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DefaultTuple;
-import org.apache.pig.data.Tuple;
-
-/**
- * This is a sample a complete MR sample code for Table. It doens't contain
- * 'read' part. But, it should be similar and easier to write. Refer to test
- * cases in the same directory.
- * 
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * this 2
- * is 1
- * a 4 
- * test 2 
- * hello 1 
- * world 3
- * </pre>
- * 
- */
-public class TableMRSortedTableZebraKeyGenerator {
-	static class MapClass implements
-      Mapper<LongWritable, Text, BytesWritable, Tuple> {
-    private BytesWritable bytesKey;
-    private Tuple tupleRow;
-    private Object javaObj;
-
-    @Override
-    public void map(LongWritable key, Text value,
-        OutputCollector<BytesWritable, Tuple> output, Reporter reporter)
-        throws IOException {
-        // value should contain "word count"
-        String[] wdct = value.toString().split(" ");
-        if (wdct.length != 2) {
-          // LOG the error
-          return;
-        }
-
-        byte[] word = wdct[0].getBytes();
-        bytesKey.set(word, 0, word.length);
-        tupleRow.set(0, new String(word));
-        tupleRow.set(1, Integer.parseInt(wdct[1]));
-
-        // This key has to be created by user
-        Tuple userKey = new DefaultTuple();
-        userKey.append(new String(word));
-        userKey.append(Integer.parseInt(wdct[1]));
-        try {
-
-            /* New M/R Interface */
-            /* Converts user key to zebra BytesWritable key */
-            /* using sort key expr tree  */
-            /* Returns a java base object */
-            /* Done for each user key */
-        	
-          bytesKey = BasicTableOutputFormat.getSortKey(javaObj, userKey);
-        } catch(Exception e) {
-        	
-        }
-
-        output.collect(bytesKey, tupleRow);
-    }
-
-    @Override
-    public void configure(JobConf job) {
-      bytesKey = new BytesWritable();
-      try {
-        Schema outSchema = BasicTableOutputFormat.getSchema(job);
-        tupleRow = TypesUtils.createTuple(outSchema);
-        
-        /* New M/R Interface */
-        /* returns an expression tree for sort keys */
-        /* Returns a java base object */
-        /* Done once per table */
-        javaObj = BasicTableOutputFormat.getSortKeyGenerator(job);
-        
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      } catch (ParseException e) {
-        throw new RuntimeException(e);
-      }
-    }
-
-    @Override
-    public void close() throws IOException {
-      // no-op
-    }
-  }
-
-  static class ReduceClass implements
-    Reducer<BytesWritable, Tuple, BytesWritable, Tuple> {
-      Tuple outRow;
-     
-
-    @Override
-    public void configure(JobConf job) {
-    }
-
-    @Override
-    public void close() throws IOException {
-    }	
-    public void reduce(BytesWritable key, Iterator<Tuple> values,
-      OutputCollector<BytesWritable, Tuple> output, Reporter reporter)
-   	    throws IOException {
-    	  try {
-    		for(; values.hasNext();)  {
-    	      output.collect(key, values.next());
-    		}  
-    	  } catch (ExecException e) {
-    	    e.printStackTrace();
-    	  }
-    }
-  
-  }  
-  
-  public static void main(String[] args) throws ParseException, IOException, Exception {
-    JobConf jobConf = new JobConf();
-    jobConf.setJobName("tableMRSample");
-    jobConf.set("table.output.tfile.compression", "gz");
-
-    // input settings
-    jobConf.setInputFormat(TextInputFormat.class);
-    jobConf.setMapperClass(TableMRSortedTableZebraKeyGenerator.MapClass.class);
-    jobConf.setReducerClass(TableMRSortedTableZebraKeyGenerator.ReduceClass.class);
-    jobConf.setMapOutputKeyClass(BytesWritable.class);
-    jobConf.setMapOutputValueClass(DefaultTuple.class);
-    FileInputFormat.setInputPaths(jobConf, new Path(
-        "/home/gauravj/work/grid/myTesting/input.txt"));
-    jobConf.setNumMapTasks(1);
-
-    // output settings
-    Path outPath = new Path("/home/gauravj/work/grid/myTesting/tableOuts");
-    jobConf.setOutputFormat(BasicTableOutputFormat.class);
-    BasicTableOutputFormat.setOutputPath(jobConf, outPath);
-    // set the logical schema with 2 columns
-    BasicTableOutputFormat.setSchema(jobConf, "word:string, count:int");
-    // for demo purposes, create 2 physical column groups
-    BasicTableOutputFormat.setStorageHint(jobConf, "[word];[count]");
-    
-    /* New M/R Interface */
-    /* Set sort columns in a comma separated string */
-    /* Each sort column should belong to schema columns */
-    BasicTableOutputFormat.setSortInfo(jobConf, "word, count");
-
-    // set map-only job.
-    jobConf.setNumReduceTasks(1);
-    JobClient.runJob(jobConf);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TableMapReduceExample.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TableMapReduceExample.java
deleted file mode 100644
index 8952ef33b..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TableMapReduceExample.java
+++ /dev/null
@@ -1,250 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapred;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.conf.Configured;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapred.*;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.hadoop.zebra.mapred.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.mapred.TableInputFormat;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.Tuple;
-
-import java.io.IOException;
-import java.util.Iterator;
-
-/**
- * <code>TableMapReduceExample<code> is a map-reduce example for Table Input/Output Format.
- * <p/>
- * Schema for Table is set to two columns containing Word of type <i>string</i> and Count of type <i>int</i> using <code> BasicTableOutputFormat.setSchema(jobConf, "word:string, count:int"); </code>
- * <p/>
- * Hint for creation of Column Groups is specified using
- * <code>  BasicTableOutputFormat.setStorageHint(jobConf, "[word];[count]"); </code>
- * . Here we have two column groups.
- * <p/>
- * Input file should contain rows of word and count, separated by a space. For
- * example:
- * 
- * <pre>
- * this 2
- * is 1
- * a 5
- * test 2
- * hello 1
- * world 3
- * </pre>
- * <p/>
- * <p>
- * Second job reads output from the first job which is in Table Format. Here we
- * specify <i>count</i> as projection column. Table Input Format projects in put
- * row which has both word and count into a row containing only the count column
- * and hands it to map.
- * <p/>
- * Reducer sums the counts and produces a sum of counts which should match total
- * number of words in original text.
- */
-
-public class TableMapReduceExample extends Configured implements Tool {
-  private static Configuration conf = null;
-
-  static class MyMap extends MapReduceBase implements
-      Mapper<LongWritable, Text, BytesWritable, Tuple> {
-    private BytesWritable bytesKey;
-    private Tuple tupleRow;
-
-    /**
-     * Map method for reading input.
-     */
-    @Override
-    public void map(LongWritable key, Text value,
-        OutputCollector<BytesWritable, Tuple> output, Reporter reporter)
-        throws IOException {
-
-      // value should contain "word count"
-      String[] wordCount = value.toString().split(" ");
-      if (wordCount.length != 2) {
-        // LOG the error
-        throw new IOException("Value does not contain two fields:" + value);
-      }
-
-      byte[] word = wordCount[0].getBytes();
-      bytesKey.set(word, 0, word.length);
-      tupleRow.set(0, new String(word));
-      tupleRow.set(1, Integer.parseInt(wordCount[1]));
-
-      output.collect(bytesKey, tupleRow);
-    }
-
-    /**
-     * Configuration of the job. Here we create an empty Tuple Row.
-     */
-    @Override
-    public void configure(JobConf job) {
-      bytesKey = new BytesWritable();
-      try {
-        Schema outSchema = BasicTableOutputFormat.getSchema(job);
-        tupleRow = TypesUtils.createTuple(outSchema);
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      } catch (ParseException e) {
-        throw new RuntimeException(e);
-      }
-    }
-
-  }
-
-  static class ProjectionMap extends MapReduceBase implements
-      Mapper<BytesWritable, Tuple, Text, IntWritable> {
-    private final static Text all = new Text("All");
-
-    /**
-     * Map method which gets count column after projection.
-     * 
-     * @throws IOException
-     */
-    @Override
-    public void map(BytesWritable key, Tuple value,
-        OutputCollector<Text, IntWritable> output, Reporter reporter)
-        throws IOException {
-      output.collect(all, new IntWritable((Integer) value.get(0)));
-    }
-  }
-
-  public static class ProjectionReduce extends MapReduceBase implements
-      Reducer<Text, IntWritable, Text, IntWritable> {
-    /**
-     * Reduce method which implements summation. Acts as both reducer and
-     * combiner.
-     * 
-     * @throws IOException
-     */
-    public void reduce(Text key, Iterator<IntWritable> values,
-        OutputCollector<Text, IntWritable> output, Reporter reporter)
-        throws IOException {
-      int sum = 0;
-      while (values.hasNext()) {
-        sum += values.next().get();
-      }
-      output.collect(key, new IntWritable(sum));
-    }
-  }
-
-  /**
-   * Where jobs and their settings and sequence is set.
-   * 
-   * @param args
-   *          arguments with exception of Tools understandable ones.
-   */
-  @Override
-  public int run(String[] args) throws Exception {
-    if (args == null || args.length != 3) {
-      System.out
-          .println("usage: TableMapReduceExample input_path_for_text_file output_path_for_table output_path_for_text_file");
-      System.exit(-1);
-    }
-
-    /*
-     * First MR Job creating a Table with two columns
-     */
-    //Configuration conf = getConf();
-    JobConf jobConf = new JobConf(conf);
-    jobConf.setJarByClass(TableMapReduceExample.class);
-
-    jobConf.setJobName("TableMapReduceExample");
-    jobConf.set("table.output.tfile.compression", "none");
-
-    // Input settings
-    jobConf.setInputFormat(TextInputFormat.class);
-    jobConf.setMapperClass(MyMap.class);
-    FileInputFormat.setInputPaths(jobConf, new Path(args[0]));
-    //FileInputFormat.setInputPaths(jobConf, new Path("/tmp/input.txt"));
-    
-    // Output settings
-    jobConf.setOutputFormat(BasicTableOutputFormat.class);
-    BasicTableOutputFormat.setOutputPath(jobConf, new Path(args[1]));
-    //BasicTableOutputFormat.setOutputPath(jobConf, new Path("/tmp/t1"));
-
-    // set the logical schema with 2 columns
-    BasicTableOutputFormat.setSchema(jobConf, "word:string, count:int");
-
-    // for demo purposes, create 2 physical column groups
-    BasicTableOutputFormat.setStorageHint(jobConf, "[word];[count]");
-
-    // set map-only job.
-    jobConf.setNumReduceTasks(0);
-
-    // Run Job
-    JobClient.runJob(jobConf);
-    
-
-    /*
-     * Second MR Job for Table Projection of count column
-     */
-    //JobConf projectionJobConf = new JobConf();
-    JobConf projectionJobConf = new JobConf(conf);
-    projectionJobConf.setJobName("TableProjectionMapReduceExample");
-
-    // Input settings
-    projectionJobConf.setJarByClass(TableMapReduceExample.class);
-    projectionJobConf.setMapperClass(ProjectionMap.class);
-    projectionJobConf.setInputFormat(TableInputFormat.class);
-    TableInputFormat.setProjection(projectionJobConf, "count");
-    TableInputFormat.setInputPaths(projectionJobConf, new Path(args[1]));
-    projectionJobConf.setMapOutputKeyClass(Text.class);
-    projectionJobConf.setMapOutputValueClass(IntWritable.class);
-
-    // Output settings
-    projectionJobConf.setOutputFormat(TextOutputFormat.class);
-    Path p2 = new Path(args[2]);
-    FileSystem fs = p2.getFileSystem(conf);
-    if (fs.exists(p2)) {
-      fs.delete(p2, true);
-    }
-    FileOutputFormat.setOutputPath(projectionJobConf, p2);
-    
-    projectionJobConf.setReducerClass(ProjectionReduce.class);
-    projectionJobConf.setCombinerClass(ProjectionReduce.class);
-
-    // Run Job
-    JobClient.runJob(projectionJobConf);
-
-    return 0;
-  }
-
-  public static void main(String[] args) throws Exception {
-    System.out.println("*******************  this is new now");
-
-    conf = new Configuration();
-    
-    //int res = ToolRunner.run(new Configuration(), new TableMapReduceExample(), args);
-    int res = ToolRunner.run(conf, new TableMapReduceExample(), args);
-    
-    System.exit(res);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestBasicTableIOFMoreLocal.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestBasicTableIOFMoreLocal.java
deleted file mode 100644
index c5f584aac..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestBasicTableIOFMoreLocal.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.IOException;
-
-import org.apache.commons.logging.LogFactory;
-
-/**
- * Testing BasicTableOutputFormat and TableInputFormat using LocalFS with larger
- * settings.
- */
-public class TestBasicTableIOFMoreLocal extends TestBasicTableIOFormatLocalFS {
-  @Override
-  protected void setUp() throws IOException {
-    LOG = LogFactory.getLog(TestBasicTableIOFMoreLocal.class.getName());
-
-    if (options == null) {
-      options = new Options();
-      options.srcFiles = 20;
-      options.numBatches = 1;
-      options.numMapper = 5;
-      options.numReducer = 4;
-    }
-
-    super.setUp();
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestBasicTableIOFormatDFS.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestBasicTableIOFormatDFS.java
deleted file mode 100644
index ec78d0803..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestBasicTableIOFormatDFS.java
+++ /dev/null
@@ -1,40 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.IOException;
-
-import org.apache.commons.logging.LogFactory;
-
-/**
- * Testing BasicTableOutputFormat and TableInputFormat using DFS
- */
-public class TestBasicTableIOFormatDFS extends TestBasicTableIOFormatLocalFS {
-  @Override
-  protected void setUp() throws IOException {
-    LOG = LogFactory.getLog(TestBasicTableIOFormatDFS.class.getName());
-
-    if (options == null) {
-      options = new Options();
-      options.localFS = false;
-    }
-
-    super.setUp();
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestBasicTableIOFormatLocalFS.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestBasicTableIOFormatLocalFS.java
deleted file mode 100644
index 2285a6a73..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestBasicTableIOFormatLocalFS.java
+++ /dev/null
@@ -1,1002 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.File;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Comparator;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.PriorityQueue;
-import java.util.StringTokenizer;
-import java.util.TreeMap;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hdfs.MiniDFSCluster;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.RawComparator;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.zebra.tfile.Utils;
-import org.apache.hadoop.mapred.FileInputFormat;
-import org.apache.hadoop.mapred.FileSplit;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.Mapper;
-import org.apache.hadoop.mapred.MiniMRCluster;
-import org.apache.hadoop.mapred.OutputCollector;
-import org.apache.hadoop.mapred.Reducer;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapred.TextInputFormat;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.mapred.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.mapred.TableInputFormat;
-import org.apache.hadoop.zebra.mapred.ArticleGenerator.Summary;
-import org.apache.hadoop.zebra.mapred.TestBasicTableIOFormatLocalFS.FreqWordCache.Item;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Projection;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.Tuple;
-
-/**
- * Testing BasicTableOutputFormat and TableInputFormat using Local FS
- */
-public class TestBasicTableIOFormatLocalFS extends TestCase {
-  static class Options {
-    int taskTrackers = 4;
-    int dataNodes = 4;
-    int srcFiles = 7;
-    int numBatches = 1;
-    int srcFileLen = 2 * 1024; // 20KB
-    int numMapper = 3;
-    int numReducer = 2;
-    int numFreqWords = 10;
-    long minTableSplitSize = 16 * 1024L;
-    boolean localFS = true;
-    String compression = "none";
-    String rootPath = "TestBasicTableIOFormat";
-    String srcPath = "docs";
-    String fwdIndexRootPath = "fwdIndex";
-    String invIndexTablePath = "invIndex";
-    String freqWordTablePath = "freqWord";
-  }
-
-  static Log LOG = LogFactory.getLog(TestBasicTableIOFormatLocalFS.class
-      .getName());
-
-  Options options;
-  Configuration conf;
-  MiniDFSCluster dfs;
-  FileSystem fileSys;
-  Path rootPath;
-  Path srcPath;
-  Path fwdIndexRootPath;
-  Path invIndexTablePath;
-  Path freqWordTablePath;
-  MiniMRCluster mr;
-  ArticleGenerator articalGen;
-  Map<String, Summary> summary;
-
-  @Override
-  protected void setUp() throws IOException {
-    if (System.getProperty("hadoop.log.dir") == null) {
-      String base = new File(".").getPath(); // getAbsolutePath();
-      Path path = new Path(".");
-      System
-          .setProperty("hadoop.log.dir", new Path(base).toString() + "./logs");
-    }
-
-    if (options == null) {
-      options = new Options();
-    }
-
-    if (conf == null) {
-      conf = new Configuration();
-    }
-
-    articalGen = new ArticleGenerator(1000, 1, 20, 100);
-    summary = new HashMap<String, Summary>();
-
-    if (options.localFS) {
-      Path localFSRootPath = new Path(System.getProperty("test.build.data",
-          "build/test/data/work-dir"));
-      fileSys = localFSRootPath.getFileSystem(conf);
-      rootPath = new Path(localFSRootPath, options.rootPath);
-      mr = new MiniMRCluster(options.taskTrackers, "file:///", 3);
-    } else {
-      dfs = new MiniDFSCluster(conf, options.dataNodes, true, null);
-      fileSys = dfs.getFileSystem();
-      rootPath = new Path(options.rootPath);
-      mr = new MiniMRCluster(options.taskTrackers, fileSys.getUri().toString(),
-          1);
-    }
-    conf = getJobConf("TestBasicTableIOFormat");
-    srcPath = new Path(rootPath, options.srcPath);
-    fwdIndexRootPath = new Path(rootPath, options.fwdIndexRootPath);
-    invIndexTablePath = new Path(rootPath, options.invIndexTablePath);
-    freqWordTablePath = new Path(rootPath, options.freqWordTablePath);
-  }
-
-  @Override
-  protected void tearDown() throws IOException {
-    if (mr != null) {
-      mr.shutdown();
-    }
-    if (dfs != null) {
-      dfs.shutdown();
-    }
-  }
-
-  JobConf getJobConf(String name) {
-    JobConf jobConf = mr.createJobConf();
-    jobConf.setJobName(name);
-    jobConf.setInt("table.input.split.minSize", 1);
-    options.minTableSplitSize = 1; // force more splits
-    jobConf.setInt("dfs.block.size", 1024); // force multiple blocks
-    jobConf.set("table.output.tfile.compression", options.compression);
-    jobConf.setInt("mapred.app.freqWords.count", options.numFreqWords);
-    return jobConf;
-  }
-
-  /**
-   * Create a bunch of text files under a sub-directory of the srcPath. The name
-   * of the sub-directory is named after the batch name.
-   * 
-   * @param batchName
-   *          The batch name.
-   * @throws IOException
-   */
-  void createSourceFiles(String batchName) throws IOException {
-    LOG.info("Creating source data folder: " + batchName);
-    Path batchDir = new Path(srcPath, batchName);
-    LOG.info("Cleaning directory: " + batchName);
-    fileSys.delete(batchDir, true);
-    LOG.info("Generating input files: " + batchName);
-    articalGen.batchArticalCreation(fileSys, new Path(srcPath, batchName),
-        "doc-", options.srcFiles, options.srcFileLen);
-    Summary s = articalGen.getSummary();
-    // dumpSummary(s);
-    long tmp = 0;
-    for (Iterator<Long> it = s.wordCntDist.values().iterator(); it.hasNext(); tmp += it
-        .next())
-      ;
-    Assert.assertEquals(tmp, s.wordCount);
-    summary.put(batchName, s);
-    articalGen.resetSummary();
-  }
-
-  /**
-   * Generate forward index. Map-only task.
-   * 
-   * <pre>
-   * Map Input:
-   *    K = LongWritable (byte offset)
-   *    V = Text (text line)
-   * Map Output:
-   *    K = word: String.
-   *    V = Tuple of {fileName:String, wordPos:Integer, lineNo:Integer }
-   * </pre>
-   */
-  static class ForwardIndexGen {
-    static class MapClass implements
-        Mapper<LongWritable, Text, BytesWritable, Tuple> {
-      private BytesWritable outKey;
-      private Tuple outRow;
-      // index into the output tuple for fileName, wordPos, lineNo.
-      private int idxFileName, idxWordPos, idxLineNo;
-      private String filePath;
-      // maintain line number and word position.
-      private int lineNo = 0;
-      private int wordPos = 0;
-
-      @Override
-      public void map(LongWritable key, Text value,
-          OutputCollector<BytesWritable, Tuple> output, Reporter reporter)
-          throws IOException {
-        if (filePath == null) {
-          FileSplit split = (FileSplit) reporter.getInputSplit();
-          filePath = split.getPath().toString();
-        }
-        String line = value.toString();
-        StringTokenizer st = new StringTokenizer(line, " ");
-        while (st.hasMoreElements()) {
-          byte[] word = st.nextToken().getBytes();
-          outKey.set(word, 0, word.length);
-          TypesUtils.resetTuple(outRow);
-          try {
-            outRow.set(idxFileName, filePath);
-            outRow.set(idxWordPos, new Integer(wordPos));
-            outRow.set(idxLineNo, new Integer(lineNo));
-            output.collect(outKey, outRow);
-          } catch (ExecException e) {
-            e.printStackTrace();
-          }
-
-          ++wordPos;
-        }
-        ++lineNo;
-
-      }
-
-      @Override
-      public void configure(JobConf job) {
-        LOG.info("ForwardIndexGen.MapClass.configure");
-        outKey = new BytesWritable();
-        try {
-          Schema outSchema = BasicTableOutputFormat.getSchema(job);
-          outRow = TypesUtils.createTuple(outSchema);
-          idxFileName = outSchema.getColumnIndex("fileName");
-          idxWordPos = outSchema.getColumnIndex("wordPos");
-          idxLineNo = outSchema.getColumnIndex("lineNo");
-        } catch (IOException e) {
-          throw new RuntimeException("Schema parsing failed : "
-              + e.getMessage());
-        } catch (ParseException e) {
-          throw new RuntimeException("Schema parsing failed : "
-              + e.getMessage());
-        }
-      }
-
-      @Override
-      public void close() throws IOException {
-        // no-op
-      }
-    }
-  }
-
-  /**
-   * Run forward index generation.
-   * 
-   * @param batchName
-   * @throws IOException
-   */
-  void runForwardIndexGen(String batchName) throws IOException, ParseException {
-    LOG.info("Run Map-only job to convert source data to forward index: "
-        + batchName);
-
-    JobConf jobConf = getJobConf("fwdIndexGen-" + batchName);
-
-    // input-related settings
-    jobConf.setInputFormat(TextInputFormat.class);
-    jobConf.setMapperClass(ForwardIndexGen.MapClass.class);
-    FileInputFormat.setInputPaths(jobConf, new Path(srcPath, batchName));
-    jobConf.setNumMapTasks(options.numMapper);
-
-    // output related settings
-    Path outPath = new Path(fwdIndexRootPath, batchName);
-    fileSys.delete(outPath, true);
-    jobConf.setOutputFormat(BasicTableOutputFormat.class);
-    BasicTableOutputFormat.setOutputPath(jobConf, outPath);
-    BasicTableOutputFormat.setSchema(jobConf, "fileName:string, wordPos:int, lineNo:int");
-
-    // set map-only job.
-    jobConf.setNumReduceTasks(0);
-    JobClient.runJob(jobConf);
-    BasicTableOutputFormat.close(jobConf);
-  }
-
-  /**
-   * Count the # of rows of a BasicTable
-   * 
-   * @param tablePath
-   *          The path to the BasicTable
-   * @return Number of rows.
-   * @throws IOException
-   */
-  long countRows(Path tablePath) throws IOException, ParseException {
-    BasicTable.Reader reader = new BasicTable.Reader(tablePath, conf);
-    reader.setProjection("");
-    long totalRows = 0;
-    TableScanner scanner = reader.getScanner(null, true);
-    for (; !scanner.atEnd(); scanner.advance()) {
-      ++totalRows;
-    }
-    scanner.close();
-    return totalRows;
-  }
-
-  /**
-   * Given a batch ID, return the batch name.
-   * 
-   * @param i
-   *          batch ID
-   * @return Batch name.
-   */
-  static String batchName(int i) {
-    return String.format("batch-%03d", i);
-  }
-
-  /**
-   * Inverted index for one word.
-   */
-  static class InvIndex implements Writable {
-    int count = 0;
-    // a map from filePath to all occurrences of the word positions.
-    Map<String, ArrayList<Integer>> index;
-
-    InvIndex() {
-      index = new TreeMap<String, ArrayList<Integer>>();
-    }
-
-    InvIndex(String fileName, int pos) {
-      this();
-      add(fileName, pos);
-    }
-
-    void add(String fileName, int pos) {
-      ++count;
-      ArrayList<Integer> list = index.get(fileName);
-      if (list == null) {
-        list = new ArrayList<Integer>(1);
-        index.put(fileName, list);
-      }
-      list.add(pos);
-    }
-
-    void add(String fileName, ArrayList<Integer> positions) {
-      ArrayList<Integer> list = index.get(fileName);
-      if (list == null) {
-        list = new ArrayList<Integer>();
-        index.put(fileName, list);
-      }
-      count += positions.size();
-      list.ensureCapacity(list.size() + positions.size());
-      list.addAll(positions);
-    }
-
-    void reduce(InvIndex other) {
-      for (Iterator<Map.Entry<String, ArrayList<Integer>>> it = other.index
-          .entrySet().iterator(); it.hasNext();) {
-        Map.Entry<String, ArrayList<Integer>> e = it.next();
-        add(e.getKey(), e.getValue());
-      }
-    }
-
-    @Override
-    public void readFields(DataInput in) throws IOException {
-      count = 0;
-      index.clear();
-      count = Utils.readVInt(in);
-      if (count > 0) {
-        int n = Utils.readVInt(in);
-        for (int i = 0; i < n; ++i) {
-          String fileName = Utils.readString(in);
-          int m = Utils.readVInt(in);
-          ArrayList<Integer> list = new ArrayList<Integer>(m);
-          index.put(fileName, list);
-          for (int j = 0; j < m; ++j) {
-            list.add(Utils.readVInt(in));
-          }
-        }
-      }
-    }
-
-    @Override
-    public void write(DataOutput out) throws IOException {
-      Utils.writeVInt(out, count);
-      if (count > 0) {
-        Utils.writeVInt(out, index.size());
-        for (Iterator<Map.Entry<String, ArrayList<Integer>>> it = index
-            .entrySet().iterator(); it.hasNext();) {
-          Map.Entry<String, ArrayList<Integer>> e = it.next();
-          Utils.writeString(out, e.getKey());
-          ArrayList<Integer> list = e.getValue();
-          Utils.writeVInt(out, list.size());
-          for (Iterator<Integer> it2 = list.iterator(); it2.hasNext();) {
-            Utils.writeVInt(out, it2.next());
-          }
-        }
-      }
-    }
-  }
-
-  /**
-   * Generate inverted Index.
-   * 
-   * <pre>
-   * Mapper Input = 
-   *    K: BytesWritable word; 
-   *    V: Tuple { fileName:String, wordPos:Integer };
-   *    
-   * Mapper Output =
-   *    K: BytesWritable word;
-   *    V: InvIndex;
-   *   
-   * Reducer Output =
-   *    K: BytesWritable word; 
-   *    V: Tuple {count:Integer, index: Map of {fileName:String, Bag of {wordPos:Integer}}};
-   * </pre>
-   */
-  static class InvertedIndexGen {
-    static class MapClass implements
-        Mapper<BytesWritable, Tuple, BytesWritable, InvIndex> {
-      // index of fileName and wordPos fileds of the input tuple
-      int idxFileName, idxWordPos;
-
-      @Override
-      public void map(BytesWritable key, Tuple value,
-          OutputCollector<BytesWritable, InvIndex> output, Reporter reporter)
-          throws IOException {
-        try {
-          String fileName = (String) value.get(idxFileName);
-          int wordPos = (Integer) value.get(idxWordPos);
-          output.collect(key, new InvIndex(fileName, wordPos));
-        } catch (ExecException e) {
-          e.printStackTrace();
-        }
-      }
-
-      @Override
-      public void configure(JobConf job) {
-        LOG.info("InvertedIndexGen.MapClass.configure");
-        String projection;
-        try {
-          projection = TableInputFormat.getProjection(job);
-        } catch (ParseException e) {
-          throw new RuntimeException("Schema parsing failed : "
-              + e.getMessage());
-        } catch (IOException e) {
-          throw new RuntimeException("TableInputFormat.getProjection", e);
-        }
-        idxFileName = Projection.getColumnIndex(projection, "fileName");
-        idxWordPos = Projection.getColumnIndex(projection, "wordPos");
-      }
-
-      @Override
-      public void close() throws IOException {
-        // no-op
-      }
-    }
-
-    static class CombinerClass implements
-        Reducer<BytesWritable, InvIndex, BytesWritable, InvIndex> {
-
-      @Override
-      public void reduce(BytesWritable key, Iterator<InvIndex> values,
-          OutputCollector<BytesWritable, InvIndex> output, Reporter reporter)
-          throws IOException {
-        InvIndex sum = new InvIndex();
-        for (; values.hasNext();) {
-          sum.reduce(values.next());
-        }
-        output.collect(key, sum);
-      }
-
-      @Override
-      public void configure(JobConf job) {
-        LOG.info("InvertedIndexGen.CombinerClass.configure");
-      }
-
-      @Override
-      public void close() throws IOException {
-        // no-op
-      }
-    }
-
-    static class ReduceClass implements
-        Reducer<BytesWritable, InvIndex, BytesWritable, Tuple> {
-      Tuple outRow;
-      int idxCount, idxIndex;
-      Schema wordPosSchema;
-      int idxWordPos;
-
-      @Override
-      public void configure(JobConf job) {
-        LOG.info("InvertedIndexGen.ReduceClass.configure");
-        try {
-          Schema outSchema = BasicTableOutputFormat.getSchema(job);
-          outRow = TypesUtils.createTuple(outSchema);
-          idxCount = outSchema.getColumnIndex("count");
-          idxIndex = outSchema.getColumnIndex("index");
-          wordPosSchema = new Schema("wordPos");
-          idxWordPos = wordPosSchema.getColumnIndex("wordPos");
-        } catch (IOException e) {
-          throw new RuntimeException("Schema parsing failed :" + e.getMessage());
-        } catch (ParseException e) {
-          throw new RuntimeException("Schema parsing failed :" + e.getMessage());
-        }
-      }
-
-      @Override
-      public void close() throws IOException {
-        // no-op
-      }
-
-      Map<String, DataBag> convertInvIndex(Map<String, ArrayList<Integer>> index)
-          throws IOException {
-        Map<String, DataBag> ret = new TreeMap<String, DataBag>();
-        for (Iterator<Map.Entry<String, ArrayList<Integer>>> it = index
-            .entrySet().iterator(); it.hasNext();) {
-          Map.Entry<String, ArrayList<Integer>> e = it.next();
-          DataBag bag = TypesUtils.createBag();
-          for (Iterator<Integer> it2 = e.getValue().iterator(); it2.hasNext();) {
-            Tuple tuple = TypesUtils.createTuple(wordPosSchema);
-            tuple.set(idxWordPos, it2.next());
-            bag.add(tuple);
-          }
-          ret.put(e.getKey(), bag);
-        }
-
-        return ret;
-      }
-
-      @Override
-      public void reduce(BytesWritable key, Iterator<InvIndex> values,
-          OutputCollector<BytesWritable, Tuple> output, Reporter reporter)
-          throws IOException {
-        InvIndex sum = new InvIndex();
-        for (; values.hasNext();) {
-          sum.reduce(values.next());
-        }
-        try {
-          outRow.set(idxCount, sum.count);
-          outRow.set(idxIndex, convertInvIndex(sum.index));
-          output.collect(key, outRow);
-        } catch (ExecException e) {
-          e.printStackTrace();
-        }
-      }
-    }
-  }
-
-  void runInvertedIndexGen() throws IOException, ParseException {
-    LOG.info("Converting forward index to inverted index");
-    JobConf jobConf = getJobConf("runInvertedIndexGen");
-
-    // input-related settings
-    jobConf.setInputFormat(TableInputFormat.class);
-    jobConf.setMapperClass(InvertedIndexGen.MapClass.class);
-    Path[] paths = new Path[options.numBatches];
-    for (int i = 0; i < options.numBatches; ++i) {
-      paths[i] = new Path(fwdIndexRootPath, batchName(i));
-    }
-    TableInputFormat.setInputPaths(jobConf, paths);
-    // TableInputFormat.setProjection(jobConf, "fileName, wordPos");
-    TableInputFormat.setMinSplitSize(jobConf, options.minTableSplitSize);
-    jobConf.setNumMapTasks(options.numMapper);
-    jobConf.setMapOutputKeyClass(BytesWritable.class);
-    jobConf.setMapOutputValueClass(InvIndex.class);
-
-    // output related settings
-    fileSys.delete(invIndexTablePath, true);
-    jobConf.setOutputFormat(BasicTableOutputFormat.class);
-    jobConf.setReducerClass(InvertedIndexGen.ReduceClass.class);
-    jobConf.setCombinerClass(InvertedIndexGen.CombinerClass.class);
-    BasicTableOutputFormat.setOutputPath(jobConf, invIndexTablePath);
-    BasicTableOutputFormat.setSchema(jobConf, "count:int, index:map()");
-    jobConf.setNumReduceTasks(options.numReducer);
-
-    JobClient.runJob(jobConf);
-    BasicTableOutputFormat.close(jobConf);
-  }
-
-  void reduce(Summary sum, Summary delta) {
-    sum.lineCount += delta.lineCount;
-    sum.wordCount += delta.wordCount;
-    reduce(sum.wordCntDist, delta.wordCntDist);
-  }
-
-  void reduce(Map<String, Long> sum, Map<String, Long> delta) {
-    for (Iterator<Map.Entry<String, Long>> it = delta.entrySet().iterator(); it
-        .hasNext();) {
-      Map.Entry<String, Long> e = it.next();
-      String key = e.getKey();
-      Long base = sum.get(key);
-      sum.put(key, (base == null) ? e.getValue() : base + e.getValue());
-    }
-  }
-
-  void dumpSummary(Summary s) {
-    LOG.info("Dumping Summary");
-    LOG.info("Word Count: " + s.wordCount);
-    for (Iterator<Map.Entry<String, Long>> it = s.wordCntDist.entrySet()
-        .iterator(); it.hasNext();) {
-      Map.Entry<String, Long> e = it.next();
-      LOG.info(e.getKey() + "->" + e.getValue());
-    }
-  }
-
-  /**
-   * Verify the word counts from the invIndexTable is the same as collected from
-   * the ArticleGenerator.
-   * 
-   * @throws IOException
-   */
-  void verifyWordCount() throws IOException, ParseException {
-    Summary expected = new Summary();
-    for (Iterator<Summary> it = summary.values().iterator(); it.hasNext();) {
-      Summary e = it.next();
-      // dumpSummary(e);
-      reduce(expected, e);
-    }
-    // LOG.info("Dumping aggregated Summary");
-    // dumpSummary(expected);
-
-    Summary actual = new Summary();
-    BasicTable.Reader reader = new BasicTable.Reader(invIndexTablePath, conf);
-    reader.setProjection("count");
-    TableScanner scanner = reader.getScanner(null, true);
-    Tuple tuple = TypesUtils.createTuple(Projection.toSchema(scanner
-        .getProjection()));
-    BytesWritable key = new BytesWritable();
-    for (; !scanner.atEnd(); scanner.advance()) {
-      scanner.getKey(key);
-      scanner.getValue(tuple);
-      int count = 0;
-      try {
-        count = (Integer) tuple.get(0);
-      } catch (ExecException e) {
-        e.printStackTrace();
-      }
-      actual.wordCount += count;
-      String word = new String(key.get(), 0, key.getSize());
-      actual.wordCntDist.put(word, (long) count);
-    }
-    scanner.close();
-    // LOG.info("Dumping MR calculated Summary");
-    // dumpSummary(actual);
-    Assert.assertEquals(expected.wordCount, actual.wordCount);
-    Assert.assertEquals(expected.wordCntDist.size(), actual.wordCntDist.size());
-    for (Iterator<Map.Entry<String, Long>> it = expected.wordCntDist.entrySet()
-        .iterator(); it.hasNext();) {
-      Map.Entry<String, Long> e = it.next();
-      String word = e.getKey();
-      Long myCount = actual.wordCntDist.get(word);
-      Assert.assertFalse(word, myCount == null);
-      Assert.assertEquals(word, e.getValue(), myCount);
-    }
-  }
-
-  /**
-   * Caching the top K frequent words.
-   */
-  static class FreqWordCache {
-    static class Item {
-      BytesWritable word;
-      int count;
-
-      Item(BytesWritable w, int c) {
-        word = new BytesWritable();
-        word.set(w.get(), 0, w.getSize());
-        count = c;
-      }
-    }
-
-    int k;
-    PriorityQueue<Item> words;
-
-    FreqWordCache(int k) {
-      if (k <= 0) {
-        throw new IllegalArgumentException("Expecting positive int");
-      }
-      this.k = k;
-      words = new PriorityQueue<Item>(k, new Comparator<Item>() {
-        @Override
-        public int compare(Item o1, Item o2) {
-          if (o1.count != o2.count) {
-            return o1.count - o2.count;
-          }
-          return -o1.word.compareTo(o2.word);
-        }
-      });
-    }
-
-    void add(BytesWritable word, int cnt) {
-      while ((words.size() >= k) && words.peek().count < cnt) {
-        words.poll();
-      }
-      if ((words.size() < k) || words.peek().count == cnt) {
-        words.add(new Item(word, cnt));
-      }
-    }
-
-    void add(Iterator<BytesWritable> itWords, int cnt) {
-      while ((words.size() >= k) && words.peek().count < cnt) {
-        words.poll();
-      }
-      if ((words.size() < k) || words.peek().count == cnt) {
-        for (; itWords.hasNext();) {
-          words.add(new Item(itWords.next(), cnt));
-        }
-      }
-    }
-
-    Item[] toArray() {
-      Item[] ret = new Item[words.size()];
-      for (int i = 0; i < ret.length; ++i) {
-        ret[i] = words.poll();
-      }
-
-      for (int i = 0; i < ret.length / 2; ++i) {
-        Item tmp = ret[i];
-        ret[i] = ret[ret.length - i - 1];
-        ret[ret.length - i - 1] = tmp;
-      }
-
-      return ret;
-    }
-  }
-
-  /**
-   * Get the most frequent words from inverted index. The mapper uses a priority
-   * queue to keep the top frequent words in memory and output the results in
-   * close().
-   * 
-   * <pre>
-   * Mapper Input = 
-   *    K: BytesWritable word; 
-   *    V: Tuple { count:Integer }.
-   *    
-   * Mapper Output =
-   *    K: IntWritabl: count
-   *    V: BytesWritable: word
-   *   
-   * Reducer Output =
-   *    K: BytesWritable word;
-   *    V: Tuple { count:Integer }.
-   * </pre>
-   */
-  static class FreqWords {
-    static int getFreqWordsCount(Configuration conf) {
-      return conf.getInt("mapred.app.freqWords.count", 100);
-    }
-
-    static class MapClass implements
-        Mapper<BytesWritable, Tuple, IntWritable, BytesWritable> {
-      int idxCount;
-      FreqWordCache freqWords;
-      IntWritable intWritable;
-      OutputCollector<IntWritable, BytesWritable> out;
-
-      @Override
-      public void map(BytesWritable key, Tuple value,
-          OutputCollector<IntWritable, BytesWritable> output, Reporter reporter)
-          throws IOException {
-        if (out == null)
-          out = output;
-        try {
-          int count = (Integer) value.get(idxCount);
-          freqWords.add(key, count);
-          reporter.progress();
-        } catch (ExecException e) {
-          e.printStackTrace();
-        }
-      }
-
-      @Override
-      public void configure(JobConf job) {
-        LOG.info("FreqWords.MapClass.configure");
-        String inSchema;
-        try {
-          inSchema = TableInputFormat.getProjection(job);
-        } catch (ParseException e) {
-          throw new RuntimeException("Projection parsing failed : "
-              + e.getMessage());
-        } catch (IOException e) {
-          throw new RuntimeException("TableInputFormat.getprojection", e);
-        }
-        idxCount = Projection.getColumnIndex(inSchema, "count");
-        intWritable = new IntWritable();
-        freqWords = new FreqWordCache(getFreqWordsCount(job));
-      }
-
-      @Override
-      public void close() throws IOException {
-        Item[] items = freqWords.toArray();
-        for (Item i : items) {
-          intWritable.set(i.count);
-          out.collect(intWritable, i.word);
-        }
-      }
-    }
-
-    static class CombinerClass implements
-        Reducer<IntWritable, BytesWritable, IntWritable, BytesWritable> {
-      FreqWordCache freqWords;
-      OutputCollector<IntWritable, BytesWritable> out;
-      IntWritable intWritable;
-
-      @Override
-      public void configure(JobConf job) {
-        LOG.info("FreqWords.CombinerClass.configure");
-        freqWords = new FreqWordCache(getFreqWordsCount(job));
-        intWritable = new IntWritable();
-      }
-
-      @Override
-      public void close() throws IOException {
-        Item[] items = freqWords.toArray();
-        for (Item i : items) {
-          intWritable.set(i.count);
-          out.collect(intWritable, i.word);
-        }
-      }
-
-      @Override
-      public void reduce(IntWritable key, Iterator<BytesWritable> values,
-          OutputCollector<IntWritable, BytesWritable> output, Reporter reporter)
-          throws IOException {
-        if (out == null) {
-          out = output;
-        }
-        freqWords.add(values, key.get());
-        reporter.progress();
-      }
-    }
-
-    static class ReduceClass implements
-        Reducer<IntWritable, BytesWritable, BytesWritable, Tuple> {
-      FreqWordCache freqWords;
-      OutputCollector<BytesWritable, Tuple> out;
-      Tuple outRow;
-      int idxCount;
-
-      @Override
-      public void configure(JobConf job) {
-        LOG.info("FreqWords.ReduceClass.configure");
-        freqWords = new FreqWordCache(getFreqWordsCount(job));
-        try {
-          Schema outSchema = BasicTableOutputFormat.getSchema(job);
-          outRow = TypesUtils.createTuple(outSchema);
-          idxCount = outSchema.getColumnIndex("count");
-        } catch (IOException e) {
-          throw new RuntimeException("Schema parsing failed : "
-              + e.getMessage());
-        } catch (ParseException e) {
-          throw new RuntimeException("Schema parsing failed : "
-              + e.getMessage());
-        }
-      }
-
-      @Override
-      public void close() throws IOException {
-        Item[] items = freqWords.toArray();
-        for (Item i : items) {
-          try {
-            outRow.set(idxCount, new Integer(i.count));
-            out.collect(i.word, outRow);
-          } catch (ExecException e) {
-            e.printStackTrace();
-          }
-        }
-      }
-
-      @Override
-      public void reduce(IntWritable key, Iterator<BytesWritable> values,
-          OutputCollector<BytesWritable, Tuple> output, Reporter reporter)
-          throws IOException {
-        if (out == null) {
-          out = output;
-        }
-        freqWords.add(values, key.get());
-        reporter.progress();
-      }
-    }
-  }
-
-  static class InverseIntRawComparator implements RawComparator<IntWritable> {
-    IntWritable.Comparator comparator;
-
-    InverseIntRawComparator() {
-      comparator = new IntWritable.Comparator();
-    }
-
-    @Override
-    public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
-      return -comparator.compare(b1, s1, l1, b2, s2, l2);
-    }
-
-    @Override
-    public int compare(IntWritable o1, IntWritable o2) {
-      return -comparator.compare(o1, o2);
-    }
-  }
-
-  void runFreqWords() throws IOException, ParseException {
-    LOG.info("Find the most frequent words");
-    JobConf jobConf = getJobConf("runFreqWords");
-
-    // input-related settings
-    jobConf.setInputFormat(TableInputFormat.class);
-    jobConf.setMapperClass(FreqWords.MapClass.class);
-    TableInputFormat.setInputPaths(jobConf, invIndexTablePath);
-    TableInputFormat.setProjection(jobConf, "count");
-    TableInputFormat.setMinSplitSize(jobConf, options.minTableSplitSize);
-    // jobConf.setNumMapTasks(options.numMapper);
-    jobConf.setNumMapTasks(-1);
-    jobConf.setMapOutputKeyClass(IntWritable.class);
-    jobConf.setMapOutputValueClass(BytesWritable.class);
-    // Set customized output comparator.
-    jobConf.setOutputKeyComparatorClass(InverseIntRawComparator.class);
-
-    // output related settings
-    fileSys.delete(freqWordTablePath, true);
-    jobConf.setOutputFormat(BasicTableOutputFormat.class);
-    jobConf.setReducerClass(FreqWords.ReduceClass.class);
-    jobConf.setCombinerClass(FreqWords.CombinerClass.class);
-    BasicTableOutputFormat.setOutputPath(jobConf, freqWordTablePath);
-    BasicTableOutputFormat.setSchema(jobConf, "count:int");
-    jobConf.setNumReduceTasks(1);
-
-    JobClient.runJob(jobConf);
-    BasicTableOutputFormat.close(jobConf);
-  }
-
-  void printFreqWords() throws IOException, ParseException {
-    LOG.info("Printing the most frequent words");
-    BasicTable.Reader reader = new BasicTable.Reader(freqWordTablePath, conf);
-    TableScanner scanner = reader.getScanner(null, true);
-    BytesWritable key = new BytesWritable();
-    Schema schema = Projection.toSchema(scanner.getProjection());
-    int idxCount = schema.getColumnIndex("count");
-    Tuple value = TypesUtils.createTuple(schema);
-    for (; !scanner.atEnd(); scanner.advance()) {
-      scanner.getKey(key);
-      scanner.getValue(value);
-      try {
-        String word = new String(key.get(), 0, key.getSize());
-        int count = (Integer) value.get(idxCount);
-        LOG.info(String.format("%s\t%d", word, count));
-      } catch (ExecException e) {
-        e.printStackTrace();
-      }
-    }
-    scanner.close();
-  }
-
-  /**
-   * Testing BasicTableOutputFormat and TableInputFormat by running a sequence
-   * of MapReduce jobs.
-   * 
-   * @throws IOException
-   */
-  public void testBasicTable() throws IOException, ParseException {
-    LOG.info("testBasicTable");
-    LOG.info("testing BasicTableOutputFormat in Map-only job");
-    for (int i = 0; i < options.numBatches; ++i) {
-      String batchName = batchName(i);
-      createSourceFiles(batchName);
-      runForwardIndexGen(batchName);
-      LOG.info("Forward index conversion complete: " + batchName);
-      Assert.assertEquals(summary.get(batchName).wordCount, countRows(new Path(
-          fwdIndexRootPath, batchName)));
-    }
-    runInvertedIndexGen();
-    verifyWordCount();
-    runFreqWords();
-    printFreqWords();
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestCheckin.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestCheckin.java
deleted file mode 100644
index 5ff644d0c..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestCheckin.java
+++ /dev/null
@@ -1,38 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapred;
-
-import org.junit.runner.RunWith;
-import org.junit.runners.Suite;
- 
-@RunWith(Suite.class)
-@Suite.SuiteClasses({
-  TestBasicTableIOFormatLocalFS.class,
-  TestBasicTableIOFormatDFS.class,
-  TestTfileSplit.class
-})
-
-public class TestCheckin {
-  // the class remains completely empty, 
-  // being used only as a holder for the above annotations
-}
-
-
-  
-
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestMultipleOutputs.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestMultipleOutputs.java
deleted file mode 100644
index 4589892c8..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestMultipleOutputs.java
+++ /dev/null
@@ -1,626 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.BufferedWriter;
-import java.io.ByteArrayOutputStream;
-import java.io.FileWriter;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.Iterator;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapred.FileInputFormat;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.Mapper;
-import org.apache.hadoop.mapred.OutputCollector;
-import org.apache.hadoop.mapred.Reducer;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapred.TextInputFormat;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.mapred.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.types.ZebraTuple;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * This is a sample a complete MR sample code for Table. It doens't contain
- * 'read' part. But, it should be similar and easier to write. Refer to test
- * cases in the same directory.
- * 
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * us 2
- * japan 2
- * india 4
- * us 2
- * japan 1
- * india 3
- * nouse 5
- * nowhere 4
- * 
- * 
- */
-public class TestMultipleOutputs extends BaseTestCase implements Tool {
-  static String inputPath;
-  static String inputFileName = "multi-input.txt";
-  public static String sortKey = null;
-
-  private static String strTable1 = null;
-  private static String strTable2 = null;
-  private static String strTable3 = null;
-
-  @BeforeClass
-  public static void setUpOnce() throws Exception {
-	  init();
-    inputPath = getTableFullPath(inputFileName).toString();
-    writeToFile(inputPath);
-  }
-  
-  @AfterClass
-  public static void tearDown() throws Exception {
-    pigServer.shutdown();
-  }
-  
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    StringTokenizer st = new StringTokenizer(t, ".");
-    String methodName = null;
-    while (st.hasMoreTokens()) {
-      methodName = st.nextToken();
-    }
-    return methodName;
-  }
-
-  public static void writeToFile (String inputFile) throws IOException{
-    if (mode==TestMode.local){
-      FileWriter fstream = new FileWriter(inputFile);
-      BufferedWriter out = new BufferedWriter(fstream);
-      out.write("us 2\n");
-      out.write("japan 2\n");
-      out.write("india 4\n");
-      out.write("us 2\n");
-      out.write("japan 1\n");
-      out.write("india 3\n");
-      out.write("nouse 5\n");
-      out.write("nowhere 4\n");
-      out.close();
-    } else {
-      FSDataOutputStream fout = fs.create(new Path (inputFile));
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 2\n");
-      fout.writeBytes("india 4\n");
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 1\n");
-      fout.writeBytes("india 3\n");
-      fout.writeBytes("nouse 5\n");
-      fout.writeBytes("nowhere 4\n");
-      fout.close();
-    }
-  }
-  
-  public static void getTablePaths(String myMultiLocs) {
-    StringTokenizer st = new StringTokenizer(myMultiLocs, ",");
-
-    // get how many tokens inside st object
-    System.out.println("tokens count: " + st.countTokens());
-    int count = 0;
-
-    // iterate st object to get more tokens from it
-    while (st.hasMoreElements()) {
-      count++;
-      String token = st.nextElement().toString();
-      if (count == 1)
-        strTable1 = token;
-      if (count == 2)
-        strTable2 = token;
-      if (count == 3)
-        strTable3 = token;
-      System.out.println("token = " + token);
-    }
-  }
-
-  public static void checkTable(String myMultiLocs) throws IOException {
-    System.out.println("myMultiLocs:" + myMultiLocs);
-    System.out.println("sorgetTablePathst key:" + sortKey);
-
-    getTablePaths(myMultiLocs);
-    String query1 = null;
-    String query2 = null;
-    String query3 = null;
-
-    if (strTable1 != null) {
-
-      query1 = "records1 = LOAD '" + strTable1
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-    if (strTable2 != null) {
-      query2 = "records2 = LOAD '" + strTable2
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-    if (strTable3 != null) {
-      query3 = "records3 = LOAD '" + strTable3
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-
-    int count1 = 0;
-    int count2 = 0;
-    int count3 = 0;
-
-    if (query1 != null) {
-      System.out.println(query1);
-      pigServer.registerQuery(query1);
-      Iterator<Tuple> it = pigServer.openIterator("records1");
-      while (it.hasNext()) {
-        count1++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-        // test 1 us table
-        if (query1.contains("test1") || query1.contains("test2")) {
-          if (count1 == 1) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 2) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 3) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count1 == 4) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-        } // test1, test2
-        if (query1.contains("test3")) {
-          if (count1 == 1) {
-            System.out.println("test3: rowvalue: " + RowValue.toString());
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count1 == 2) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count1 == 3) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 4) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-        } // test3
-      }// while
-      if (query1.contains("test1") || query1.contains("test2")
-          || query1.contains("test3")) {
-        Assert.assertEquals(4, count1);
-      }
-    }// if query1 != null
-
-    if (query2 != null) {
-      pigServer.registerQuery(query2);
-      Iterator<Tuple> it = pigServer.openIterator("records2");
-
-      while (it.hasNext()) {
-        count2++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-
-        // if test1 india table
-        if (query2.contains("test1")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test1
-        if (query1.contains("test2")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-        }// if test2
-
-        if (query1.contains("test3")) {
-          if (count1 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count1 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        } // test3
-
-      }// while
-      if (query2.contains("test1") || query2.contains("test2")
-          || query2.contains("test3")) {
-        Assert.assertEquals(2, count2);
-      }
-    }// if query2 != null
-    // test 1 us table should have 4 lines
-
-    if (query3 != null) {
-      pigServer.registerQuery(query3);
-      Iterator<Tuple> it = pigServer.openIterator("records3");
-
-      while (it.hasNext()) {
-        count3++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-
-        // if test1 japan table
-        if (query3.contains("test1")) {
-          if (count3 == 1) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count3 == 2) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-        }// if test1 test2 japan table
-
-        if (query3.contains("test2")) {
-          if (count3 == 1) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count3 == 2) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-        }// test2
-        if (query1.contains("test3")) {
-          if (count1 == 1) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count1 == 2) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-        } // test3
-
-      }// while
-      if (query3.contains("test1") || query3.contains("test2")
-          || query3.contains("test3")) {
-        Assert.assertEquals(2, count3);
-      }
-    }// if query2 != null
-  }
-
-  @Test
-  public void test1() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test combine sort keys
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "word,count";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-
-    Path pathUs = getTableFullPath( "us" + methodName );
-    Path pathIndia = getTableFullPath( "india" + methodName );
-    Path pathJapan = getTableFullPath( "japan" + methodName );
-    myMultiLocs = pathUs.toString() + "," + 
-	    pathIndia.toString() + "," +
-	    pathJapan.toString();
-
-    getTablePaths(myMultiLocs);
-    System.out.println("strTable1: " + strTable1.toString());
-    System.out.println("strTable2: " + strTable2.toString());
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    removeDir(new Path(strTable3));
-    runMR(myMultiLocs, sortKey);
-    checkTable(myMultiLocs);
-    System.out.println("DONE test 1");
-
-  }
-
-  @Test
-  public void test2() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test 'word' sort key
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "word";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-
-    Path pathUs = getTableFullPath( "us" + methodName );
-    Path pathIndia = getTableFullPath( "india" + methodName );
-    Path pathJapan = getTableFullPath( "japan" + methodName );
-    myMultiLocs = pathUs.toString() + "," + 
-	    pathIndia.toString() + "," +
-	    pathJapan.toString();
-
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    removeDir(new Path(strTable3));
-    runMR(myMultiLocs, sortKey);
-    checkTable(myMultiLocs);
-    System.out.println("DONE test 2");
-  }
-
-  @Test
-  public void test3() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test 'count' sort key
-     */
-    // setUpOnce();
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "count";
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-
-    Path pathUs = getTableFullPath( "us" + methodName );
-    Path pathIndia = getTableFullPath( "india" + methodName );
-    Path pathJapan = getTableFullPath( "japan" + methodName );
-    myMultiLocs = pathUs.toString() + "," + 
-	    pathIndia.toString() + "," +
-	    pathJapan.toString();
-
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    removeDir(new Path(strTable3));
-    runMR(myMultiLocs, sortKey);
-    checkTable(myMultiLocs);
-    System.out.println("DONE test 3");
-  }
-
-  static class MapClass implements
-      Mapper<LongWritable, Text, BytesWritable, Tuple> {
-    private BytesWritable bytesKey;
-    private Tuple tupleRow;
-    private Object javaObj;
-
-    @Override
-    public void map(LongWritable key, Text value,
-        OutputCollector<BytesWritable, Tuple> output, Reporter reporter)
-        throws IOException {
-      // value should contain "word count"
-      String[] wdct = value.toString().split(" ");
-      if (wdct.length != 2) {
-        // LOG the error
-        return;
-      }
-
-      byte[] word = wdct[0].getBytes();
-      bytesKey.set(word, 0, word.length);
-      System.out.println("word: " + new String(word));
-      tupleRow.set(0, new String(word));
-      tupleRow.set(1, Integer.parseInt(wdct[1]));
-      System.out.println("count:  " + Integer.parseInt(wdct[1]));
-
-      // This key has to be created by user
-      /*
-       * Tuple userKey = new ZebraTuple(); userKey.append(new String(word));
-       * userKey.append(Integer.parseInt(wdct[1]));
-       */
-      System.out.println("in map, sortkey: " + sortKey);
-      Tuple userKey = new ZebraTuple();
-      if (sortKey.equalsIgnoreCase("word,count")) {
-        userKey.append(new String(word));
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("count")) {
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("word")) {
-        userKey.append(new String(word));
-      }
-
-      try {
-
-        /* New M/R Interface */
-        /* Converts user key to zebra BytesWritable key */
-        /* using sort key expr tree */
-        /* Returns a java base object */
-        /* Done for each user key */
-
-        bytesKey = BasicTableOutputFormat.getSortKey(javaObj, userKey);
-      } catch (Exception e) {
-
-      }
-
-      output.collect(bytesKey, tupleRow);
-    }
-
-    @Override
-    public void configure(JobConf job) {
-      bytesKey = new BytesWritable();
-      //conf = job;
-      sortKey = job.get("sortKey");
-      try {
-        Schema outSchema = BasicTableOutputFormat.getSchema(job);
-        tupleRow = TypesUtils.createTuple(outSchema);
-        javaObj = BasicTableOutputFormat.getSortKeyGenerator(job);
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      } catch (org.apache.hadoop.zebra.parser.ParseException e) {
-        throw new RuntimeException(e);
-      }
-    }
-
-    @Override
-    public void close() throws IOException {
-      // no-op
-    }
-  }
-
-  static class ReduceClass implements
-      Reducer<BytesWritable, Tuple, BytesWritable, Tuple> {
-    Tuple outRow;
-
-    @Override
-    public void configure(JobConf job) {
-    }
-
-    @Override
-    public void close() throws IOException {
-    }
-
-    public void reduce(BytesWritable key, Iterator<Tuple> values,
-        OutputCollector<BytesWritable, Tuple> output, Reporter reporter)
-        throws IOException {
-      try {
-        for (; values.hasNext();) {
-          output.collect(key, values.next());
-        }
-      } catch (ExecException e) {
-        e.printStackTrace();
-      }
-    }
-
-  }
-
-  static class OutputPartitionerClass extends ZebraOutputPartition {
-
-    @Override
-    public int getOutputPartition(BytesWritable key, Tuple value) {
-
-      // System.out.println(this.jobConf);
-
-      String reg = null;
-      try {
-        reg = (String) (value.get(0));
-      } catch (Exception e) {
-        //
-      }
-
-      if (reg.equals("us"))
-        return 0;
-      if (reg.equals("india"))
-        return 1;
-      if (reg.equals("japan"))
-        return 2;
-
-      return 0;
-    }
-  }
-
-  public void runMR(String myMultiLocs, String sortKey) throws ParseException,
-      IOException, Exception, org.apache.hadoop.zebra.parser.ParseException {
-    
-    JobConf jobConf = new JobConf(conf);
-    
-    jobConf.setJobName("TestMultipleOutputs");
-    jobConf.setJarByClass(TestMultipleOutputs.class);
-    jobConf.set("table.output.tfile.compression", "gz");
-    jobConf.set("sortKey", sortKey);
-    
-    // input settings
-    jobConf.setInputFormat(TextInputFormat.class);
-    jobConf.setMapperClass(TestMultipleOutputs.MapClass.class);
-    jobConf.setMapOutputKeyClass(BytesWritable.class);
-    jobConf.setMapOutputValueClass(ZebraTuple.class);
-    FileInputFormat.setInputPaths(jobConf, inputPath);
-
-    jobConf.setNumMapTasks(1);
-
-    // output settings
-    jobConf.setOutputFormat(BasicTableOutputFormat.class);
-    BasicTableOutputFormat.setMultipleOutputs(jobConf, myMultiLocs, TestMultipleOutputs.OutputPartitionerClass.class);
-
-    // set the logical schema with 2 columns
-    BasicTableOutputFormat.setSchema(jobConf, "word:string, count:int");
-    // for demo purposes, create 2 physical column groups
-    BasicTableOutputFormat.setStorageHint(jobConf, "[word];[count]");
-    BasicTableOutputFormat.setSortInfo(jobConf, sortKey);
-    System.out.println("in runMR, sortkey: " + sortKey);
-    
-    // set map-only job.
-    jobConf.setNumReduceTasks(1);
-    JobClient.runJob(jobConf);
-    BasicTableOutputFormat.close(jobConf);
-  }
-  
-  @Override
-  public int run(String[] args) throws Exception {
-    TestMultipleOutputs test = new TestMultipleOutputs();
-    TestMultipleOutputs.setUpOnce();
-    test.test1();
-    test.test2();
-    test.test3();
-    
-    return 0;
-  }
-  
-  public static void main(String[] args) throws Exception {
-    conf = new Configuration();
-    
-    int res = ToolRunner.run(conf, new TestMultipleOutputs(), args);
-    
-    System.out.println("PASS");
-    System.exit(res);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestMultipleOutputs2.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestMultipleOutputs2.java
deleted file mode 100644
index 05b72b740..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestMultipleOutputs2.java
+++ /dev/null
@@ -1,605 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.BufferedWriter;
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.FileWriter;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.Iterator;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapred.FileInputFormat;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.Mapper;
-import org.apache.hadoop.mapred.OutputCollector;
-import org.apache.hadoop.mapred.Reducer;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapred.TextInputFormat;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.mapred.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.types.ZebraTuple;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.Tuple;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * This is a sample a complete MR sample code for Table. It doens't contain
- * 'read' part. But, it should be similar and easier to write. Refer to test
- * cases in the same directory.
- * 
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * us 2
- * japan 2
- * india 4
- * us 2
- * japan 1
- * india 3
- * nouse 5
- * nowhere 4
- * 
- * 
- */
-public class TestMultipleOutputs2 extends BaseTestCase implements Tool {
-  static String inputPath;
-  static String inputFileName = "multi-input.txt";
-  public static String sortKey = null;
-
-  private static String strTable1 = null;
-  private static String strTable2 = null;
-  private static String strTable3 = null;
-
-  @BeforeClass
-  public static void setUpOnce() throws Exception {
-	  init();
-      inputPath = getTableFullPath(inputFileName).toString();
-      writeToFile(inputPath);
-  }
-
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    StringTokenizer st = new StringTokenizer(t, ".");
-    String methodName = null;
-    while (st.hasMoreTokens()) {
-      methodName = st.nextToken();
-    }
-    return methodName;
-  }
-  
-  public static void writeToFile (String inputFile) throws IOException{
-    if ( mode == TestMode.local ){
-    FileWriter fstream = new FileWriter(inputFile);
-    BufferedWriter out = new BufferedWriter(fstream);
-    out.write("us 2\n");
-    out.write("japan 2\n");
-    out.write("india 4\n");
-    out.write("us 2\n");
-    out.write("japan 1\n");
-    out.write("india 3\n");
-    out.write("nouse 5\n");
-    out.write("nowhere 4\n");
-    out.close();
-    } else {
-    FSDataOutputStream fout = fs.create(new Path (inputFile));
-    fout.writeBytes("us 2\n");
-    fout.writeBytes("japan 2\n");
-    fout.writeBytes("india 4\n");
-    fout.writeBytes("us 2\n");
-    fout.writeBytes("japan 1\n");
-    fout.writeBytes("india 3\n");
-    fout.writeBytes("nouse 5\n");
-    fout.writeBytes("nowhere 4\n");
-    fout.close();
-    }
-  }
-
-  public static void getTablePaths(String myMultiLocs) {
-    StringTokenizer st = new StringTokenizer(myMultiLocs, ",");
-
-    // get how many tokens inside st object
-    System.out.println("tokens count: " + st.countTokens());
-    int count = 0;
-
-    // iterate st object to get more tokens from it
-    while (st.hasMoreElements()) {
-      count++;
-      String token = st.nextElement().toString();
-      if ( mode == TestMode.local ) {
-        System.out.println("in mini, token: "+token); 
-         if (count == 1)
-          strTable1 = token;
-        if (count == 2)
-          strTable2 = token;
-        if (count == 3)
-          strTable3 = token;
-      } else {
-        System.out.println("in real, token: "+token);
-        //in real, token: /user/hadoopqa/ustest3
-        //note: no prefix file:  in real cluster
-        if (count == 1)
-          strTable1 = token;
-        if (count == 2)
-          strTable2 = token;
-        if (count == 3)
-          strTable3 = token;
-      }
-      
-    }
-  }
-
-  public static void checkTable(String myMultiLocs) throws IOException {
-    System.out.println("myMultiLocs:" + myMultiLocs);
-    System.out.println("sorgetTablePathst key:" + sortKey);
-
-    getTablePaths(myMultiLocs);
-    String query1 = null;
-    String query2 = null;
- 
-    if (strTable1 != null) {
-
-      query1 = "records1 = LOAD '" + strTable1
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-    if (strTable2 != null) {
-      query2 = "records2 = LOAD '" + strTable2
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-
-    int count1 = 0;
-    int count2 = 0;
-  
-    if (query1 != null) {
-      System.out.println(query1);
-      pigServer.registerQuery(query1);
-      Iterator<Tuple> it = pigServer.openIterator("records1");
-      while (it.hasNext()) {
-        count1++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-        // test 1 us table
-        if (query1.contains("test1") || query1.contains("test2")|| query1.contains("test3")) {
-          
-          if (count1 == 1) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count1 == 2) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-        } // test1, test2
-    
-      }// while
-      if (query1.contains("test1") || query1.contains("test2")
-          || query1.contains("test3")) {
-        Assert.assertEquals(2, count1);
-      }
-    }// if query1 != null
-
-    if (query2 != null) {
-      pigServer.registerQuery(query2);
-      Iterator<Tuple> it = pigServer.openIterator("records2");
-
-      while (it.hasNext()) {
-        count2++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-
-        // if test1 other table
-        if (query2.contains("test1")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          
-          if (count1 == 5) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test1
-     // if test2 other table
-        if (query2.contains("test2")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          
-          if (count1 == 5) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test2
-        // if test3 other table
-        if (query2.contains("test3")) {
-          if (count2 == 1) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 5) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          
-        }// if test3
-        
-      }// while
-      if (query2.contains("test1") || query2.contains("test2")
-          || query2.contains("test3")) {
-        Assert.assertEquals(6, count2);
-      }
-    }// if query2 != null
-
-  }
-
-  
-  @Test
-  public void test1() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test combine sort keys
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "word,count";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-
-    Path pathUs = getTableFullPath("us" + methodName);
-    Path pathOthers = getTableFullPath("others" + methodName);
-    myMultiLocs = pathUs.toString() + "," + pathOthers.toString();    	
-
-    getTablePaths(myMultiLocs);
-    System.out.println("strTable1: "+strTable1);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    runMR(myMultiLocs, sortKey);
-    checkTable(myMultiLocs);
-    System.out.println("DONE test" +getCurrentMethodName());
-  
-  }
-  @Test
-  public void test2() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test word sort keys
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "word";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-
-    Path pathUs = getTableFullPath("us" + methodName);
-    Path pathOthers = getTableFullPath("others" + methodName);
-    myMultiLocs = pathUs.toString() + "," + pathOthers.toString();    	
-
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    runMR(myMultiLocs, sortKey);
-    checkTable(myMultiLocs);
-    System.out.println("DONE test" +getCurrentMethodName());
-  
-  }
- @Test
-  public void test3() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test count sort keys
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "count";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-
-    Path pathUs = getTableFullPath("us" + methodName);
-    Path pathOthers = getTableFullPath("others" + methodName);
-    myMultiLocs = pathUs.toString() + "," + pathOthers.toString();
-
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    runMR(myMultiLocs, sortKey);
- 
-    checkTableExists(true, strTable1);
-    checkTableExists(true, strTable2);
-    checkTable(myMultiLocs);
-    System.out.println("DONE test" +getCurrentMethodName());
-  
-  }
-
- static class MapClass implements
-      Mapper<LongWritable, Text, BytesWritable, Tuple> {
-    private BytesWritable bytesKey;
-    private Tuple tupleRow;
-    private Object javaObj;
-    //private JobConf conf;
-
-    @Override
-    public void map(LongWritable key, Text value,
-        OutputCollector<BytesWritable, Tuple> output, Reporter reporter)
-        throws IOException {
-      // value should contain "word count"
-      String[] wdct = value.toString().split(" ");
-      if (wdct.length != 2) {
-        // LOG the error
-        return;
-      }
-
-      byte[] word = wdct[0].getBytes();
-      bytesKey.set(word, 0, word.length);
-      System.out.println("word: " + new String(word));
-      tupleRow.set(0, new String(word));
-      tupleRow.set(1, Integer.parseInt(wdct[1]));
-      System.out.println("count:  " + Integer.parseInt(wdct[1]));
-
-      // This key has to be created by user
-      /*
-       * Tuple userKey = new DefaultTuple(); userKey.append(new String(word));
-       * userKey.append(Integer.parseInt(wdct[1]));
-       */
-      System.out.println("in map, sortkey: " + sortKey);
-      Tuple userKey = new ZebraTuple();
-      if (sortKey.equalsIgnoreCase("word,count")) {
-        userKey.append(new String(word));
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("count")) {
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("word")) {
-        userKey.append(new String(word));
-      }
-
-      try {
-
-        /* New M/R Interface */
-        /* Converts user key to zebra BytesWritable key */
-        /* using sort key expr tree */
-        /* Returns a java base object */
-        /* Done for each user key */
-
-        bytesKey = BasicTableOutputFormat.getSortKey(javaObj, userKey);
-      } catch (Exception e) {
-
-      }
-
-      output.collect(bytesKey, tupleRow);
-    }
-
-    @Override
-    public void configure(JobConf job) {
-      bytesKey = new BytesWritable();
-      //conf = job;
-      sortKey = job.get("sortKey");
-      try {
-        Schema outSchema = BasicTableOutputFormat.getSchema(job);
-        tupleRow = TypesUtils.createTuple(outSchema);
-        javaObj = BasicTableOutputFormat.getSortKeyGenerator(job);
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      } catch (org.apache.hadoop.zebra.parser.ParseException e) {
-        throw new RuntimeException(e);
-      }
-    }
-
-    @Override
-    public void close() throws IOException {
-      // no-op
-    }
-  }
-
-  static class ReduceClass implements
-      Reducer<BytesWritable, Tuple, BytesWritable, Tuple> {
-    Tuple outRow;
-
-    @Override
-    public void configure(JobConf job) {
-    }
-
-    @Override
-    public void close() throws IOException {
-    }
-
-    public void reduce(BytesWritable key, Iterator<Tuple> values,
-        OutputCollector<BytesWritable, Tuple> output, Reporter reporter)
-        throws IOException {
-      try {
-        for (; values.hasNext();) {
-          output.collect(key, values.next());
-        }
-      } catch (ExecException e) {
-        e.printStackTrace();
-      }
-    }
-
-  }
-
-  static class OutputPartitionerClass extends ZebraOutputPartition {
-
-    @Override
-    public int getOutputPartition(BytesWritable key, Tuple value) throws ExecException {
-
-     //check table schema and table data are passed to this class
-/*
- * value0 : india
-value0 : india
-value0 : japan
-value0 : japan
-value0 : nouse
-value0 : nowhere
-value0 : us
-value0 : us
- */
-System.out.println("value0 : "+value.get(0));
-      String reg = null;
-      try {
-        reg = (String) (value.get(0));
-      } catch (Exception e) {
-        //
-      }
-
-      if (reg.equals("us"))
-        return 0;
-      else
-        return 1;
-
-    }
-
-  }
-
-  public void runMR(String myMultiLocs, String sortKey) throws ParseException,
-      IOException, Exception, org.apache.hadoop.zebra.parser.ParseException {
-
-    JobConf jobConf = new JobConf(conf);
-    jobConf.setJobName("TestMultipleOutput2");
-    jobConf.setJarByClass(TestMultipleOutputs2.class);
-    jobConf.set("table.output.tfile.compression", "gz");
-    jobConf.set("sortKey", sortKey);
-    // input settings
-    jobConf.setInputFormat(TextInputFormat.class);
-    jobConf.setMapperClass(TestMultipleOutputs2.MapClass.class);
-    jobConf.setMapOutputKeyClass(BytesWritable.class);
-    jobConf.setMapOutputValueClass(ZebraTuple.class);
-    FileInputFormat.setInputPaths(jobConf, inputPath);
-
-    jobConf.setNumMapTasks(1);
-
-    // output settings
-
-    jobConf.setOutputFormat(BasicTableOutputFormat.class);
-    BasicTableOutputFormat.setMultipleOutputs(jobConf, myMultiLocs,
-        TestMultipleOutputs2.OutputPartitionerClass.class);
-
-    // set the logical schema with 2 columns
-    BasicTableOutputFormat.setSchema(jobConf, "word:string, count:int");
-    // for demo purposes, create 2 physical column groups
-    BasicTableOutputFormat.setStorageHint(jobConf, "[word];[count]");
-    BasicTableOutputFormat.setSortInfo(jobConf, sortKey);
-    System.out.println("in runMR, sortkey: " + sortKey);
-    // set map-only job.
-    jobConf.setNumReduceTasks(1);
-    JobClient.runJob(jobConf);
-    BasicTableOutputFormat.close(jobConf);
-  }
-
-  @Override
-  public int run(String[] args) throws Exception {
-    TestMultipleOutputs2 test = new TestMultipleOutputs2();
-    TestMultipleOutputs2.setUpOnce();
-    test.test1();
-    test.test2();
-    test.test3();
-   
-    return 0;
-  }
-  
-  
-  public static void main(String[] args) throws Exception {
-    conf = new Configuration();
-    
-    int res = ToolRunner.run(conf, new TestMultipleOutputs2(), args);
-    System.out.println("PASS");
-    System.exit(res);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestMultipleOutputs2TypedApi.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestMultipleOutputs2TypedApi.java
deleted file mode 100644
index 7688d66c4..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestMultipleOutputs2TypedApi.java
+++ /dev/null
@@ -1,620 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.BufferedWriter;
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.FileWriter;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapred.FileInputFormat;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.Mapper;
-import org.apache.hadoop.mapred.OutputCollector;
-import org.apache.hadoop.mapred.Reducer;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapred.TextInputFormat;
-import org.apache.hadoop.mapred.TextOutputFormat;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.mapred.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.types.ZebraTuple;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DefaultTuple;
-import org.apache.pig.data.Tuple;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * This is a sample a complete MR sample code for Table. It doens't contain
- * 'read' part. But, it should be similar and easier to write. Refer to test
- * cases in the same directory.
- * 
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * us 2
- * japan 2
- * india 4
- * us 2
- * japan 1
- * india 3
- * nouse 5
- * nowhere 4
- * 
- * 
- */
-public class TestMultipleOutputs2TypedApi extends BaseTestCase implements Tool {
-  static String inputPath;
-  static String inputFileName = "multi-input.txt";
-  public static String sortKey = null;
-
-  private static String strTable1 = null;
-  private static String strTable2 = null;
-  private static String strTable3 = null;
-
-  @BeforeClass
-  public static void setUpOnce() throws Exception {
-	  init();
-      inputPath = getTableFullPath(inputFileName).toString();
-      writeToFile(inputPath);
-  }
-
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    StringTokenizer st = new StringTokenizer(t, ".");
-    String methodName = null;
-    while (st.hasMoreTokens()) {
-      methodName = st.nextToken();
-    }
-    return methodName;
-  }
-
-  public static void writeToFile(String inputFile) throws IOException {
-    if ( mode == TestMode.local ) {
-      FileWriter fstream = new FileWriter(inputFile);
-      BufferedWriter out = new BufferedWriter(fstream);
-      out.write("us 2\n");
-      out.write("japan 2\n");
-      out.write("india 4\n");
-      out.write("us 2\n");
-      out.write("japan 1\n");
-      out.write("india 3\n");
-      out.write("nouse 5\n");
-      out.write("nowhere 4\n");
-      out.close();
-    } else {
-      FSDataOutputStream fout = fs.create(new Path(inputFile));
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 2\n");
-      fout.writeBytes("india 4\n");
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 1\n");
-      fout.writeBytes("india 3\n");
-      fout.writeBytes("nouse 5\n");
-      fout.writeBytes("nowhere 4\n");
-      fout.close();
-    }
-  }
-
-  public static void getTablePaths(String myMultiLocs) {
-    StringTokenizer st = new StringTokenizer(myMultiLocs, ",");
-
-    // get how many tokens inside st object
-    System.out.println("tokens count: " + st.countTokens());
-    int count = 0;
-
-    // iterate st object to get more tokens from it
-    while (st.hasMoreElements()) {
-      count++;
-      String token = st.nextElement().toString();
-      if ( mode == TestMode.local ) {
-        System.out.println("in mini, token: " + token);
-        // in mini, token:
-        // file:/homes/<uid>/grid/multipleoutput/pig-table/contrib/zebra/ustest3
-        if (count == 1)
-          strTable1 = token;
-        if (count == 2)
-          strTable2 = token;
-        if (count == 3)
-          strTable3 = token;
-      } else {
-        System.out.println("in real, token: " + token);
-        // in real, token: /user/hadoopqa/ustest3
-        // note: no prefix file: in real cluster
-        if (count == 1)
-          strTable1 = token;
-        if (count == 2)
-          strTable2 = token;
-        if (count == 3)
-          strTable3 = token;
-      }
-
-    }
-  }
-
-  public static void checkTable(String myMultiLocs) throws IOException {
-    System.out.println("myMultiLocs:" + myMultiLocs);
-    System.out.println("sorgetTablePathst key:" + sortKey);
-
-    getTablePaths(myMultiLocs);
-    String query1 = null;
-    String query2 = null;
-
-    if (strTable1 != null) {
-
-      query1 = "records1 = LOAD '" + strTable1
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-    if (strTable2 != null) {
-      query2 = "records2 = LOAD '" + strTable2
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-
-    int count1 = 0;
-    int count2 = 0;
-
-    if (query1 != null) {
-      System.out.println(query1);
-      pigServer.registerQuery(query1);
-      Iterator<Tuple> it = pigServer.openIterator("records1");
-      while (it.hasNext()) {
-        count1++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-        // test 1 us table
-        if (query1.contains("test1") || query1.contains("test2")
-            || query1.contains("test3")) {
-
-          if (count1 == 1) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count1 == 2) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-        } // test1, test2
-
-      }// while
-      if (query1.contains("test1") || query1.contains("test2")
-          || query1.contains("test3")) {
-        Assert.assertEquals(2, count1);
-      }
-    }// if query1 != null
-
-    if (query2 != null) {
-      pigServer.registerQuery(query2);
-      Iterator<Tuple> it = pigServer.openIterator("records2");
-
-      while (it.hasNext()) {
-        count2++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-
-        // if test1 other table
-        if (query2.contains("test1")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-
-          if (count1 == 5) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test1
-        // if test2 other table
-        if (query2.contains("test2")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-
-          if (count1 == 5) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test2
-        // if test3 other table
-        if (query2.contains("test3")) {
-          if (count2 == 1) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 5) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-
-        }// if test3
-
-      }// while
-      if (query2.contains("test1") || query2.contains("test2")
-          || query2.contains("test3")) {
-        Assert.assertEquals(6, count2);
-      }
-    }// if query2 != null
-
-  }
-
-  @Test
-  public void test1() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test combine sort keys
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "word,count";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-    List<Path> paths = new ArrayList<Path>(2);
-    
-    Path pathUs = getTableFullPath("us" + methodName);
-    Path pathOthers = getTableFullPath("others" + methodName);
-    paths.add( pathUs );
-    paths.add( pathOthers );
-    myMultiLocs = pathUs.toString() + "," + pathOthers.toString();    	
-
-    getTablePaths(myMultiLocs);
-    System.out.println("strTable1: " + strTable1);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    runMR(sortKey, paths.toArray(new Path[2]));
-    checkTable(myMultiLocs);
-    System.out.println("DONE test" + getCurrentMethodName());
-
-  }
-
-  @Test
-  public void test2() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test word sort keys
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "word";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-    List<Path> paths = new ArrayList<Path>(2);
-
-    Path pathUs = getTableFullPath("us" + methodName);
-    Path pathOthers = getTableFullPath("others" + methodName);
-    paths.add( pathUs );
-    paths.add( pathOthers );
-    myMultiLocs = pathUs.toString() + "," + pathOthers.toString();    	
-
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    runMR(sortKey, paths.toArray(new Path[2]));
-    checkTable(myMultiLocs);
-    System.out.println("DONE test" + getCurrentMethodName());
-
-  }
-
-  @Test
-  public void test3() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test count sort keys
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "count";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-    List<Path> paths = new ArrayList<Path>(2);
-
-    Path pathUs = getTableFullPath("us" + methodName);
-    Path pathOthers = getTableFullPath("others" + methodName);
-    paths.add( pathUs );
-    paths.add( pathOthers );
-    myMultiLocs = pathUs.toString() + "," + pathOthers.toString();    	
-
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    runMR(sortKey, paths.toArray(new Path[2]));
-
-    checkTableExists(true, strTable1);
-    checkTableExists(true, strTable2);
-    checkTable(myMultiLocs);
-    System.out.println("DONE test" + getCurrentMethodName());
-
-  }
-
-  static class MapClass implements
-      Mapper<LongWritable, Text, BytesWritable, Tuple> {
-    private BytesWritable bytesKey;
-    private Tuple tupleRow;
-    private Object javaObj;
-    //private JobConf conf;
-
-    @Override
-    public void map(LongWritable key, Text value,
-        OutputCollector<BytesWritable, Tuple> output, Reporter reporter)
-        throws IOException {
-      // value should contain "word count"
-      String[] wdct = value.toString().split(" ");
-      if (wdct.length != 2) {
-        // LOG the error
-        return;
-      }
-
-      byte[] word = wdct[0].getBytes();
-      bytesKey.set(word, 0, word.length);
-      System.out.println("word: " + new String(word));
-      tupleRow.set(0, new String(word));
-      tupleRow.set(1, Integer.parseInt(wdct[1]));
-      System.out.println("count:  " + Integer.parseInt(wdct[1]));
-
-      // This key has to be created by user
-      /*
-       * Tuple userKey = new DefaultTuple(); userKey.append(new String(word));
-       * userKey.append(Integer.parseInt(wdct[1]));
-       */
-      System.out.println("in map, sortkey: " + sortKey);
-      Tuple userKey = new DefaultTuple();
-      if (sortKey.equalsIgnoreCase("word,count")) {
-        userKey.append(new String(word));
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("count")) {
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("word")) {
-        userKey.append(new String(word));
-      }
-
-      try {
-
-        /* New M/R Interface */
-        /* Converts user key to zebra BytesWritable key */
-        /* using sort key expr tree */
-        /* Returns a java base object */
-        /* Done for each user key */
-
-        bytesKey = BasicTableOutputFormat.getSortKey(javaObj, userKey);
-      } catch (Exception e) {
-
-      }
-
-      output.collect(bytesKey, tupleRow);
-    }
-
-    @Override
-    public void configure(JobConf job) {
-      bytesKey = new BytesWritable();
-      //conf = job;
-      sortKey = job.get("sortKey");
-      try {
-        Schema outSchema = BasicTableOutputFormat.getSchema(job);
-        tupleRow = TypesUtils.createTuple(outSchema);
-        javaObj = BasicTableOutputFormat.getSortKeyGenerator(job);
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      } catch (org.apache.hadoop.zebra.parser.ParseException e) {
-        throw new RuntimeException(e);
-      }
-    }
-
-    @Override
-    public void close() throws IOException {
-      // no-op
-    }
-  }
-
-  static class ReduceClass implements
-      Reducer<BytesWritable, Tuple, BytesWritable, Tuple> {
-    Tuple outRow;
-
-    @Override
-    public void configure(JobConf job) {
-    }
-
-    @Override
-    public void close() throws IOException {
-    }
-
-    public void reduce(BytesWritable key, Iterator<Tuple> values,
-        OutputCollector<BytesWritable, Tuple> output, Reporter reporter)
-        throws IOException {
-      try {
-        for (; values.hasNext();) {
-          output.collect(key, values.next());
-        }
-      } catch (ExecException e) {
-        e.printStackTrace();
-      }
-    }
-
-  }
-
-  static class OutputPartitionerClass extends ZebraOutputPartition {
-
-    @Override
-    public int getOutputPartition(BytesWritable key, Tuple value)
-        throws ExecException {
-
-      // check table schema and table data are passed to this class
-      /*
-       * value0 : india value0 : india value0 : japan value0 : japan value0 :
-       * nouse value0 : nowhere value0 : us value0 : us
-       */
-      System.out.println("value0 : " + value.get(0));
-      String reg = null;
-      try {
-        reg = (String) (value.get(0));
-      } catch (Exception e) {
-        //
-      }
-
-      if (reg.equals("us"))
-        return 0;
-      else
-        return 1;
-
-    }
-
-  }
-
-  public void runMR(String sortKey, Path... paths) throws ParseException,
-      IOException, Exception, org.apache.hadoop.zebra.parser.ParseException {
-
-    JobConf jobConf = new JobConf(conf);
-    jobConf.setJobName("TestMultipleOutputs2TypedApi");
-    jobConf.setJarByClass(TestMultipleOutputs2TypedApi.class);
-    jobConf.set("table.output.tfile.compression", "gz");
-    jobConf.set("sortKey", sortKey);
-    // input settings
-    jobConf.setInputFormat(TextInputFormat.class);
-    jobConf.setMapperClass(TestMultipleOutputs2TypedApi.MapClass.class);
-    jobConf.setMapOutputKeyClass(BytesWritable.class);
-    jobConf.setMapOutputValueClass(ZebraTuple.class);
-
-    FileInputFormat.setInputPaths(jobConf, inputPath);
-
-    jobConf.setNumMapTasks(1);
-
-    // output settings
-
-    jobConf.setOutputFormat(BasicTableOutputFormat.class);
-    
-    String schema = "word:string, count:int";
-    String storageHint = "[word];[count]";
-    BasicTableOutputFormat.setMultipleOutputs(jobConf,
-        TestMultipleOutputs2TypedApi.OutputPartitionerClass.class, paths);
-    ZebraSchema zSchema = ZebraSchema.createZebraSchema(schema);
-    ZebraStorageHint zStorageHint = ZebraStorageHint
-        .createZebraStorageHint(storageHint);
-    ZebraSortInfo zSortInfo = ZebraSortInfo.createZebraSortInfo(sortKey, null);
-    BasicTableOutputFormat.setStorageInfo(jobConf, zSchema, zStorageHint,
-        zSortInfo);
-
-    // set map-only job.
-    jobConf.setNumReduceTasks(1);
-    JobClient.runJob(jobConf);
-    BasicTableOutputFormat.close(jobConf);
-  }
-
-  @Override
-  public int run(String[] args) throws Exception {
-    TestMultipleOutputs2TypedApi test = new TestMultipleOutputs2TypedApi();
-    TestMultipleOutputs2TypedApi.setUpOnce();
-    test.test1();
-    test.test2();
-    test.test3();
-
-    return 0;
-  }
-  
-  public static void main(String[] args) throws Exception {
-    conf = new Configuration();
-    
-    int res = ToolRunner.run(conf, new TestMultipleOutputs2TypedApi(), args);
-    System.out.println("PASS");
-    System.exit(res);
-  }  
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestMultipleOutputs3.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestMultipleOutputs3.java
deleted file mode 100644
index e3a35f561..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestMultipleOutputs3.java
+++ /dev/null
@@ -1,522 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.BufferedWriter;
-import java.io.ByteArrayOutputStream;
-import java.io.FileWriter;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.Iterator;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapred.FileInputFormat;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.Mapper;
-import org.apache.hadoop.mapred.OutputCollector;
-import org.apache.hadoop.mapred.Reducer;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapred.TextInputFormat;
-import org.apache.hadoop.mapred.TextOutputFormat;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.mapred.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.types.ZebraTuple;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.Tuple;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * This is a sample a complete MR sample code for Table. It doens't contain
- * 'read' part. But, it should be similar and easier to write. Refer to test
- * cases in the same directory.
- * 
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * us 2
- * japan 2
- * india 4
- * us 2
- * japan 1
- * india 3
- * nouse 5
- * nowhere 4
- * 
- * 
- */
-public class TestMultipleOutputs3 extends BaseTestCase implements Tool {
-  static String inputPath;
-  static String inputFileName = "multi-input.txt";
-  public static String sortKey = null;
-
-  private static String strTable1 = null;
-  private static String strTable2 = null;
-  private static String strTable3 = null;
-
-  @BeforeClass
-  public static void setUpOnce() throws Exception {
-	  init();
-      inputPath = getTableFullPath(inputFileName).toString();
-      writeToFile(inputPath);
-  }
-
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    StringTokenizer st = new StringTokenizer(t, ".");
-    String methodName = null;
-    while (st.hasMoreTokens()) {
-      methodName = st.nextToken();
-    }
-    return methodName;
-  }
-  public static void writeToFile (String inputFile) throws IOException{
-    if ( mode == TestMode.local ){
-    FileWriter fstream = new FileWriter(inputFile);
-    BufferedWriter out = new BufferedWriter(fstream);
-    out.write("us 2\n");
-    out.write("japan 2\n");
-    out.write("india 4\n");
-    out.write("us 2\n");
-    out.write("japan 1\n");
-    out.write("india 3\n");
-    out.write("nouse 5\n");
-    out.write("nowhere 4\n");
-    out.close();
-    } else {
-    FSDataOutputStream fout = fs.create(new Path (inputFile));
-    fout.writeBytes("us 2\n");
-    fout.writeBytes("japan 2\n");
-    fout.writeBytes("india 4\n");
-    fout.writeBytes("us 2\n");
-    fout.writeBytes("japan 1\n");
-    fout.writeBytes("india 3\n");
-    fout.writeBytes("nouse 5\n");
-    fout.writeBytes("nowhere 4\n");
-    fout.close();
-    }
-  }
-  
-
-  public static void getTablePaths(String myMultiLocs) {
-    StringTokenizer st = new StringTokenizer(myMultiLocs, ",");
-
-    // get how many tokens inside st object
-    System.out.println("tokens count: " + st.countTokens());
-    int count = 0;
-
-    // iterate st object to get more tokens from it
-    while (st.hasMoreElements()) {
-      count++;
-      String token = st.nextElement().toString();
-      if (count == 1)
-        strTable1 = token;
-      if (count == 2)
-        strTable2 = token;
-      if (count == 3)
-        strTable3 = token;
-      System.out.println("token = " + token);
-    }
-  }
-
-  public static void checkTable(String myMultiLocs) throws IOException {
-    System.out.println("myMultiLocs:" + myMultiLocs);
-    System.out.println("sorgetTablePathst key:" + sortKey);
-
-    getTablePaths(myMultiLocs);
-    String query1 = null;
-    String query2 = null;
-
-    if (strTable1 != null) {
-
-      query1 = "records1 = LOAD '" + strTable1
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-    if (strTable2 != null) {
-      query2 = "records2 = LOAD '" + strTable2
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-
-    int count1 = 0;
-    int count2 = 0;
-
-    if (query1 != null) {
-      System.out.println(query1);
-      pigServer.registerQuery(query1);
-      Iterator<Tuple> it = pigServer.openIterator("records1");
-      while (it.hasNext()) {
-        count1++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-        // test 1 us table
-        if (query1.contains("test1") || query1.contains("test2")
-            || query1.contains("test3")) {
-
-          if (count1 == 1) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count1 == 2) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-        } // test1, test2
-
-      }// while
-      if (query1.contains("test1") || query1.contains("test2")
-          || query1.contains("test3")) {
-        Assert.assertEquals(2, count1);
-      }
-    }// if query1 != null
-
-    if (query2 != null) {
-      pigServer.registerQuery(query2);
-      Iterator<Tuple> it = pigServer.openIterator("records2");
-
-      while (it.hasNext()) {
-        count2++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-
-        // if test1 other table
-        if (query2.contains("test1")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-
-          if (count1 == 5) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test1
-        // if test2 other table
-        if (query2.contains("test2")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-
-          if (count1 == 5) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test2
-        // if test3 other table
-        if (query2.contains("test3")) {
-          if (count2 == 1) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 5) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-
-        }// if test3
-
-      }// while
-      if (query2.contains("test1") || query2.contains("test2")
-          || query2.contains("test3")) {
-        Assert.assertEquals(6, count2);
-      }
-    }// if query2 != null
-
-  }
- 
-  //@Test(expected = IOException.class)
-  @Test
-  public void test1() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test combine sort keys
-     */
-    System.out.println("******Starttt  testcase: " + getCurrentMethodName());
-    sortKey = "word,count";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-    
-    Path pathUs = getTableFullPath("us" + methodName);
-    Path pathOthers = getTableFullPath("others" + methodName);
-    myMultiLocs = pathUs.toString() + "," + pathOthers.toString();
-    
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    runMR(myMultiLocs, sortKey);
-  //  checkTable(myMultiLocs);
-  //  Assert.fail("test 1 ,should have thrown IOExcepiton");
-    System.out.println("DONE test " + getCurrentMethodName());
-  }
-  
-
-  static class MapClass implements
-      Mapper<LongWritable, Text, BytesWritable, Tuple> {
-    private BytesWritable bytesKey;
-    private Tuple tupleRow;
-    private Object javaObj;
-    //private JobConf conf;
-
-    @Override
-    public void map(LongWritable key, Text value,
-        OutputCollector<BytesWritable, Tuple> output, Reporter reporter)
-        throws IOException {
-      // value should contain "word count"
-      String[] wdct = value.toString().split(" ");
-      if (wdct.length != 2) {
-        // LOG the error
-        return;
-      }
-
-      byte[] word = wdct[0].getBytes();
-      bytesKey.set(word, 0, word.length);
-      System.out.println("word: " + new String(word));
-      tupleRow.set(0, new String(word));
-      tupleRow.set(1, Integer.parseInt(wdct[1]));
-      System.out.println("count:  " + Integer.parseInt(wdct[1]));
-
-      // This key has to be created by user
-      /*
-       * Tuple userKey = new DefaultTuple(); userKey.append(new String(word));
-       * userKey.append(Integer.parseInt(wdct[1]));
-       */
-      System.out.println("in map, sortkey: " + sortKey);
-      Tuple userKey = new ZebraTuple();
-      if (sortKey.equalsIgnoreCase("word,count")) {
-        userKey.append(new String(word));
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("count")) {
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("word")) {
-        userKey.append(new String(word));
-      }
-
-      try {
-
-        /* New M/R Interface */
-        /* Converts user key to zebra BytesWritable key */
-        /* using sort key expr tree */
-        /* Returns a java base object */
-        /* Done for each user key */
-
-        bytesKey = BasicTableOutputFormat.getSortKey(javaObj, userKey);
-      } catch (Exception e) {
-
-      }
-
-      output.collect(bytesKey, tupleRow);
-    }
-
-    @Override
-    public void configure(JobConf job) {
-      bytesKey = new BytesWritable();
-      //conf = job;
-      sortKey = job.get("sortKey");
-      try {
-        Schema outSchema = BasicTableOutputFormat.getSchema(job);
-        tupleRow = TypesUtils.createTuple(outSchema);
-        javaObj = BasicTableOutputFormat.getSortKeyGenerator(job);
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      } catch (org.apache.hadoop.zebra.parser.ParseException e) {
-        throw new RuntimeException(e);
-      }
-    }
-
-    @Override
-    public void close() throws IOException {
-      // no-op
-    }
-  }
-
-  static class ReduceClass implements
-      Reducer<BytesWritable, Tuple, BytesWritable, Tuple> {
-    Tuple outRow;
-
-    @Override
-    public void configure(JobConf job) {
-    }
-
-    @Override
-    public void close() throws IOException {
-    }
-
-    public void reduce(BytesWritable key, Iterator<Tuple> values,
-        OutputCollector<BytesWritable, Tuple> output, Reporter reporter)
-        throws IOException {
-      try {
-        for (; values.hasNext();) {
-          output.collect(key, values.next());
-        }
-      } catch (ExecException e) {
-        e.printStackTrace();
-      }
-    }
-
-  }
-
-  static class OutputPartitionerClass extends ZebraOutputPartition {
-    @Override
-    public int getOutputPartition(BytesWritable key, Tuple value) throws IndexOutOfBoundsException, ExecException {
-      // System.out.println(this.jobConf);
-      try {
-        value.get(2);
-      } catch (IndexOutOfBoundsException e) {
-        return 0;
-      }
-      
-      // should not reach here
-      Assert.fail("int try, should have thrown exception");
-      return 0;
-    }
-  }
-
-  public void runMR(String myMultiLocs, String sortKey) throws ParseException,
-      IOException, Exception, org.apache.hadoop.zebra.parser.ParseException {
-
-    JobConf jobConf = new JobConf(conf);
-    jobConf.setJobName("TestMultipleOutputs3");
-    jobConf.setJarByClass(TestMultipleOutputs3.class);
-    jobConf.set("table.output.tfile.compression", "gz");
-    jobConf.set("sortKey", sortKey);
-    // input settings
-    jobConf.setInputFormat(TextInputFormat.class);
-    jobConf.setMapperClass(TestMultipleOutputs3.MapClass.class);
-    jobConf.setMapOutputKeyClass(BytesWritable.class);
-    jobConf.setMapOutputValueClass(ZebraTuple.class);
-    FileInputFormat.setInputPaths(jobConf, inputPath);
-
-    jobConf.setNumMapTasks(1);
-
-    // output settings
-
-    jobConf.setOutputFormat(BasicTableOutputFormat.class);
-    BasicTableOutputFormat.setMultipleOutputs(jobConf, myMultiLocs,
-        TestMultipleOutputs3.OutputPartitionerClass.class);
-
-    // set the logical schema with 2 columns
-    BasicTableOutputFormat.setSchema(jobConf, "word:string, count:int");
-    // for demo purposes, create 2 physical column groups
-    BasicTableOutputFormat.setStorageHint(jobConf, "[word];[count]");
-    BasicTableOutputFormat.setSortInfo(jobConf, sortKey);
-    System.out.println("in runMR, sortkey: " + sortKey);
-    // set map-only job.
-    jobConf.setNumReduceTasks(1);
-    JobClient.runJob(jobConf);
-    BasicTableOutputFormat.close(jobConf);
-  }
-
-  @Override
-  public int run(String[] args) throws Exception {
-    TestMultipleOutputs3 test = new TestMultipleOutputs3();
-    TestMultipleOutputs3.setUpOnce();
-    test.test1();
-
-    return 0;
-  }
-  
-  public static void main(String[] args) throws Exception {
-    conf = new Configuration();
-    
-    int res = ToolRunner.run(conf, new TestMultipleOutputs3(), args);
-    System.out.println("PASS");
-    System.exit(res);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestMultipleOutputs3TypedApi.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestMultipleOutputs3TypedApi.java
deleted file mode 100644
index bfe95ec27..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestMultipleOutputs3TypedApi.java
+++ /dev/null
@@ -1,530 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.BufferedWriter;
-import java.io.ByteArrayOutputStream;
-import java.io.FileWriter;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapred.FileInputFormat;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.Mapper;
-import org.apache.hadoop.mapred.OutputCollector;
-import org.apache.hadoop.mapred.Reducer;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapred.TextInputFormat;
-import org.apache.hadoop.mapred.TextOutputFormat;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.mapred.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.types.ZebraTuple;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DefaultTuple;
-import org.apache.pig.data.Tuple;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * This is a sample a complete MR sample code for Table. It doens't contain
- * 'read' part. But, it should be similar and easier to write. Refer to test
- * cases in the same directory.
- * 
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * us 2
- * japan 2
- * india 4
- * us 2
- * japan 1
- * india 3
- * nouse 5
- * nowhere 4
- * 
- * 
- */
-public class TestMultipleOutputs3TypedApi extends BaseTestCase implements Tool {
-  static String inputPath;
-  static String inputFileName = "multi-input.txt";
-  public static String sortKey = null;
-
-  private static String strTable1 = null;
-  private static String strTable2 = null;
-  private static String strTable3 = null;
-
-  @BeforeClass
-  public static void setUpOnce() throws Exception {
-	  init();
-      inputPath = getTableFullPath(inputFileName).toString();
-      writeToFile(inputPath);
-  }
-
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    StringTokenizer st = new StringTokenizer(t, ".");
-    String methodName = null;
-    while (st.hasMoreTokens()) {
-      methodName = st.nextToken();
-    }
-    return methodName;
-  }
-  public static void writeToFile (String inputFile) throws IOException{
-    if ( mode == TestMode.local ){
-    FileWriter fstream = new FileWriter(inputFile);
-    BufferedWriter out = new BufferedWriter(fstream);
-    out.write("us 2\n");
-    out.write("japan 2\n");
-    out.write("india 4\n");
-    out.write("us 2\n");
-    out.write("japan 1\n");
-    out.write("india 3\n");
-    out.write("nouse 5\n");
-    out.write("nowhere 4\n");
-    out.close();
-    } else {
-    FSDataOutputStream fout = fs.create(new Path (inputFile));
-    fout.writeBytes("us 2\n");
-    fout.writeBytes("japan 2\n");
-    fout.writeBytes("india 4\n");
-    fout.writeBytes("us 2\n");
-    fout.writeBytes("japan 1\n");
-    fout.writeBytes("india 3\n");
-    fout.writeBytes("nouse 5\n");
-    fout.writeBytes("nowhere 4\n");
-    fout.close();
-    }
-  }
-  
-  public static void getTablePaths(String myMultiLocs) {
-    StringTokenizer st = new StringTokenizer(myMultiLocs, ",");
-
-    // get how many tokens inside st object
-    System.out.println("tokens count: " + st.countTokens());
-    int count = 0;
-
-    // iterate st object to get more tokens from it
-    while (st.hasMoreElements()) {
-      count++;
-      String token = st.nextElement().toString();
-      if (count == 1)
-        strTable1 = token;
-      if (count == 2)
-        strTable2 = token;
-      if (count == 3)
-        strTable3 = token;
-      System.out.println("token = " + token);
-    }
-  }
-
-  public static void checkTable(String myMultiLocs) throws IOException {
-    System.out.println("myMultiLocs:" + myMultiLocs);
-    System.out.println("sorgetTablePathst key:" + sortKey);
-
-    getTablePaths(myMultiLocs);
-    String query1 = null;
-    String query2 = null;
-
-    if (strTable1 != null) {
-
-      query1 = "records1 = LOAD '" + strTable1
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-    if (strTable2 != null) {
-      query2 = "records2 = LOAD '" + strTable2
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-
-    int count1 = 0;
-    int count2 = 0;
-
-    if (query1 != null) {
-      System.out.println(query1);
-      pigServer.registerQuery(query1);
-      Iterator<Tuple> it = pigServer.openIterator("records1");
-      while (it.hasNext()) {
-        count1++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-        // test 1 us table
-        if (query1.contains("test1") || query1.contains("test2")
-            || query1.contains("test3")) {
-
-          if (count1 == 1) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count1 == 2) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-        } // test1, test2
-
-      }// while
-      if (query1.contains("test1") || query1.contains("test2")
-          || query1.contains("test3")) {
-        Assert.assertEquals(2, count1);
-      }
-    }// if query1 != null
-
-    if (query2 != null) {
-      pigServer.registerQuery(query2);
-      Iterator<Tuple> it = pigServer.openIterator("records2");
-
-      while (it.hasNext()) {
-        count2++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-
-        // if test1 other table
-        if (query2.contains("test1")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-
-          if (count1 == 5) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test1
-        // if test2 other table
-        if (query2.contains("test2")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-
-          if (count1 == 5) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test2
-        // if test3 other table
-        if (query2.contains("test3")) {
-          if (count2 == 1) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 5) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-
-        }// if test3
-
-      }// while
-      if (query2.contains("test1") || query2.contains("test2")
-          || query2.contains("test3")) {
-        Assert.assertEquals(6, count2);
-      }
-    }// if query2 != null
-
-  }
- 
-  @Test
-  public void test1() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test combine sort keys
-     */
-    System.out.println("******Starttt  testcase: " + getCurrentMethodName());
-    sortKey = "word,count";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-    List<Path> paths = new ArrayList<Path>(2);
-
-    Path pathUs = getTableFullPath("us" + methodName);
-    Path pathOthers = getTableFullPath("others" + methodName);
-    paths.add( pathUs );
-    paths.add( pathOthers );
-    myMultiLocs = pathUs.toString() + "," + pathOthers.toString();    	
-
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    runMR(sortKey, paths.toArray(new Path[2]));
-
-  //  checkTable(myMultiLocs);
-  //  Assert.fail("test 1 ,should have thrown IOExcepiton");
-    System.out.println("DONE test " + getCurrentMethodName());
-
-  }
-
-
-  static class MapClass implements
-      Mapper<LongWritable, Text, BytesWritable, Tuple> {
-    private BytesWritable bytesKey;
-    private Tuple tupleRow;
-    private Object javaObj;
-    //private JobConf conf;
-
-    @Override
-    public void map(LongWritable key, Text value,
-        OutputCollector<BytesWritable, Tuple> output, Reporter reporter)
-        throws IOException {
-      // value should contain "word count"
-      String[] wdct = value.toString().split(" ");
-      if (wdct.length != 2) {
-        // LOG the error
-        return;
-      }
-
-      byte[] word = wdct[0].getBytes();
-      bytesKey.set(word, 0, word.length);
-      System.out.println("word: " + new String(word));
-      tupleRow.set(0, new String(word));
-      tupleRow.set(1, Integer.parseInt(wdct[1]));
-      System.out.println("count:  " + Integer.parseInt(wdct[1]));
-
-      // This key has to be created by user
-      /*
-       * Tuple userKey = new DefaultTuple(); userKey.append(new String(word));
-       * userKey.append(Integer.parseInt(wdct[1]));
-       */
-      System.out.println("in map, sortkey: " + sortKey);
-      Tuple userKey = new DefaultTuple();
-      if (sortKey.equalsIgnoreCase("word,count")) {
-        userKey.append(new String(word));
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("count")) {
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("word")) {
-        userKey.append(new String(word));
-      }
-
-      try {
-
-        /* New M/R Interface */
-        /* Converts user key to zebra BytesWritable key */
-        /* using sort key expr tree */
-        /* Returns a java base object */
-        /* Done for each user key */
-
-        bytesKey = BasicTableOutputFormat.getSortKey(javaObj, userKey);
-      } catch (Exception e) {
-
-      }
-
-      output.collect(bytesKey, tupleRow);
-    }
-
-    @Override
-    public void configure(JobConf job) {
-      bytesKey = new BytesWritable();
-      //conf = job;
-      sortKey = job.get("sortKey");
-      try {
-        Schema outSchema = BasicTableOutputFormat.getSchema(job);
-        tupleRow = TypesUtils.createTuple(outSchema);
-        javaObj = BasicTableOutputFormat.getSortKeyGenerator(job);
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      } catch (org.apache.hadoop.zebra.parser.ParseException e) {
-        throw new RuntimeException(e);
-      }
-    }
-
-    @Override
-    public void close() throws IOException {
-      // no-op
-    }
-  }
-
-  static class ReduceClass implements
-      Reducer<BytesWritable, Tuple, BytesWritable, Tuple> {
-    Tuple outRow;
-
-    @Override
-    public void configure(JobConf job) {
-    }
-
-    @Override
-    public void close() throws IOException {
-    }
-
-    public void reduce(BytesWritable key, Iterator<Tuple> values,
-        OutputCollector<BytesWritable, Tuple> output, Reporter reporter)
-        throws IOException {
-      try {
-        for (; values.hasNext();) {
-          output.collect(key, values.next());
-        }
-      } catch (ExecException e) {
-        e.printStackTrace();
-      }
-    }
-
-  }
-
-  static class OutputPartitionerClass extends ZebraOutputPartition {
-
-    @Override
-    public int getOutputPartition(BytesWritable key, Tuple value) throws IndexOutOfBoundsException, ExecException{
-      try {
-        value.get(2);
-      } catch (IndexOutOfBoundsException e) {
-        return 0;
-      }
-      
-      // should not reach here
-      Assert.fail("int try, should have thrown exception");
-      return 0;
-    }
-  }
-
-  public void runMR(String sortKey, Path...paths) throws ParseException,
-      IOException, Exception, org.apache.hadoop.zebra.parser.ParseException {
-
-    JobConf jobConf = new JobConf(conf);
-    jobConf.setJobName("TestMultipleOutputs3TypedApi");
-    jobConf.setJarByClass(TestMultipleOutputs3TypedApi.class);
-    jobConf.set("table.output.tfile.compression", "gz");
-    jobConf.set("sortKey", sortKey);
-    // input settings
-    jobConf.setInputFormat(TextInputFormat.class);
-    jobConf.setMapperClass(TestMultipleOutputs3TypedApi.MapClass.class);
-    jobConf.setMapOutputKeyClass(BytesWritable.class);
-    jobConf.setMapOutputValueClass(ZebraTuple.class);
-    FileInputFormat.setInputPaths(jobConf, inputPath);
-
-    jobConf.setNumMapTasks(1);
-
-    // output settings
-
-    jobConf.setOutputFormat(BasicTableOutputFormat.class);
-   
-    String schema = "word:string, count:int";
-    String storageHint = "[word];[count]";
-    BasicTableOutputFormat.setMultipleOutputs(jobConf,
-        TestMultipleOutputs3TypedApi.OutputPartitionerClass.class, paths);
-    ZebraSchema zSchema = ZebraSchema.createZebraSchema(schema);
-    ZebraStorageHint zStorageHint = ZebraStorageHint
-        .createZebraStorageHint(storageHint);
-    ZebraSortInfo zSortInfo = ZebraSortInfo.createZebraSortInfo(sortKey, null);
-    BasicTableOutputFormat.setStorageInfo(jobConf, zSchema, zStorageHint,
-        zSortInfo);
-    
-    jobConf.setNumReduceTasks(1);
-    JobClient.runJob(jobConf);
-    BasicTableOutputFormat.close(jobConf);
-  }
-
-  @Override
-  public int run(String[] args) throws Exception {
-    TestMultipleOutputs3TypedApi test = new TestMultipleOutputs3TypedApi();
-    TestMultipleOutputs3TypedApi.setUpOnce();
-    test.test1();
-
-    return 0;
-  }
-  
-  public static void main(String[] args) throws Exception {
-    conf = new Configuration();
-    
-    int res = ToolRunner.run(conf, new TestMultipleOutputs3TypedApi(), args);
-    System.out.println("PASS");
-    System.exit(res);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestMultipleOutputs4.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestMultipleOutputs4.java
deleted file mode 100644
index 8a7b58c63..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestMultipleOutputs4.java
+++ /dev/null
@@ -1,526 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.BufferedWriter;
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.FileWriter;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.Iterator;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapred.FileInputFormat;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.Mapper;
-import org.apache.hadoop.mapred.OutputCollector;
-import org.apache.hadoop.mapred.Reducer;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapred.TextInputFormat;
-import org.apache.hadoop.mapred.TextOutputFormat;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.mapred.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.types.ZebraTuple;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.Tuple;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * This is a sample a complete MR sample code for Table. It doens't contain
- * 'read' part. But, it should be similar and easier to write. Refer to test
- * cases in the same directory.
- * 
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * us 2
- * japan 2
- * india 4
- * us 2
- * japan 1
- * india 3
- * nouse 5
- * nowhere 4
- * 
- * 
- */
-public class TestMultipleOutputs4 extends BaseTestCase implements Tool {
-  static String inputPath;
-  static String inputFileName = "multi-input.txt";
-  public static String sortKey = null;
-
-  private static String strTable1 = null;
-  private static String strTable2 = null;
-  private static String strTable3 = null;
-
-  @BeforeClass
-  public static void setUpOnce() throws Exception {
-	  init();
-      inputPath = getTableFullPath(inputFileName).toString();
-      writeToFile(inputPath);
-  }
-
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    StringTokenizer st = new StringTokenizer(t, ".");
-    String methodName = null;
-    while (st.hasMoreTokens()) {
-      methodName = st.nextToken();
-    }
-    return methodName;
-  }
-
-  public static void writeToFile (String inputFile) throws IOException{
-    if ( mode == TestMode.local ){
-    FileWriter fstream = new FileWriter(inputFile);
-    BufferedWriter out = new BufferedWriter(fstream);
-    out.write("us 2\n");
-    out.write("japan 2\n");
-    out.write("india 4\n");
-    out.write("us 2\n");
-    out.write("japan 1\n");
-    out.write("india 3\n");
-    out.write("nouse 5\n");
-    out.write("nowhere 4\n");
-    out.close();
-    } else {
-    FSDataOutputStream fout = fs.create(new Path (inputFile));
-    fout.writeBytes("us 2\n");
-    fout.writeBytes("japan 2\n");
-    fout.writeBytes("india 4\n");
-    fout.writeBytes("us 2\n");
-    fout.writeBytes("japan 1\n");
-    fout.writeBytes("india 3\n");
-    fout.writeBytes("nouse 5\n");
-    fout.writeBytes("nowhere 4\n");
-    fout.close();
-    }
-  }
-  
-  public static void getTablePaths(String myMultiLocs) {
-    StringTokenizer st = new StringTokenizer(myMultiLocs, ",");
-
-    // get how many tokens inside st object
-    System.out.println("tokens count: " + st.countTokens());
-    int count = 0;
-
-    // iterate st object to get more tokens from it
-    while (st.hasMoreElements()) {
-      count++;
-      String token = st.nextElement().toString();
-      if ( mode == TestMode.local ) {
-        System.out.println("in mini, token: "+token); 
-        //in mini, token: file:/homes/<uid>/grid/multipleoutput/pig-table/contrib/zebra/ustest3
-        if (count == 1)
-          strTable1 = token;
-        if (count == 2)
-          strTable2 = token;
-        if (count == 3)
-          strTable3 = token;
-      } else {
-        System.out.println("in real, token: "+token);
-        //in real, token: /user/hadoopqa/ustest3
-        //note: no prefix file:  in real cluster
-        if (count == 1)
-          strTable1 = token;
-        if (count == 2)
-          strTable2 = token;
-        if (count == 3)
-          strTable3 = token;
-      }
-      
-    }
-  }
-
-  public static void checkTable(String myMultiLocs) throws IOException {
-    System.out.println("myMultiLocs:" + myMultiLocs);
-    System.out.println("sorgetTablePathst key:" + sortKey);
-
-    getTablePaths(myMultiLocs);
-    String query1 = null;
-    String query2 = null;
- 
-    if (strTable1 != null) {
-
-      query1 = "records1 = LOAD '" + strTable1
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-    if (strTable2 != null) {
-      query2 = "records2 = LOAD '" + strTable2
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-
-    int count1 = 0;
-    int count2 = 0;
-  
-    if (query1 != null) {
-      System.out.println(query1);
-      pigServer.registerQuery(query1);
-      Iterator<Tuple> it = pigServer.openIterator("records1");
-      while (it.hasNext()) {
-        count1++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-        // test 1 us table
-        if (query1.contains("test1") || query1.contains("test2")|| query1.contains("test3")) {
-          
-          if (count1 == 1) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count1 == 2) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-        } // test1, test2
-    
-      }// while
-      if (query1.contains("test1") || query1.contains("test2")
-          || query1.contains("test3")) {
-        Assert.assertEquals(2, count1);
-      }
-    }// if query1 != null
-
-    if (query2 != null) {
-      pigServer.registerQuery(query2);
-      Iterator<Tuple> it = pigServer.openIterator("records2");
-
-      while (it.hasNext()) {
-        count2++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-
-        // if test1 other table
-        if (query2.contains("test1")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          
-          if (count1 == 5) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test1
-     // if test2 other table
-        if (query2.contains("test2")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          
-          if (count1 == 5) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test2
-        // if test3 other table
-        if (query2.contains("test3")) {
-          if (count2 == 1) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 5) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          
-        }// if test3
-        
-      }// while
-      if (query2.contains("test1") || query2.contains("test2")
-          || query2.contains("test3")) {
-        Assert.assertEquals(6, count2);
-      }
-    }// if query2 != null
-
-  }
-
-  
-  @Test
-  public void test1() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test combine sort keys
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "word,count";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-   
-    myMultiLocs = getTableFullPath( "a" ).toString();
-    
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    runMR(myMultiLocs, sortKey);
-    checkTableExists(true, strTable1);
-    System.out.println("DONE test " + getCurrentMethodName());
-
-  }
-
-
-  static class MapClass implements
-      Mapper<LongWritable, Text, BytesWritable, Tuple> {
-    private BytesWritable bytesKey;
-    private Tuple tupleRow;
-    private Object javaObj;
-
-    @Override
-    public void map(LongWritable key, Text value,
-        OutputCollector<BytesWritable, Tuple> output, Reporter reporter)
-        throws IOException {
-      // value should contain "word count"
-      String[] wdct = value.toString().split(" ");
-      if (wdct.length != 2) {
-        // LOG the error
-        return;
-      }
-
-      byte[] word = wdct[0].getBytes();
-      bytesKey.set(word, 0, word.length);
-      System.out.println("word: " + new String(word));
-      tupleRow.set(0, new String(word));
-      tupleRow.set(1, Integer.parseInt(wdct[1]));
-      System.out.println("count:  " + Integer.parseInt(wdct[1]));
-
-      // This key has to be created by user
-      /*
-       * Tuple userKey = new DefaultTuple(); userKey.append(new String(word));
-       * userKey.append(Integer.parseInt(wdct[1]));
-       */
-      System.out.println("in map, sortkey: " + sortKey);
-      Tuple userKey = new ZebraTuple();
-      if (sortKey.equalsIgnoreCase("word,count")) {
-        userKey.append(new String(word));
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("count")) {
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("word")) {
-        userKey.append(new String(word));
-      }
-
-      try {
-
-        /* New M/R Interface */
-        /* Converts user key to zebra BytesWritable key */
-        /* using sort key expr tree */
-        /* Returns a java base object */
-        /* Done for each user key */
-
-        bytesKey = BasicTableOutputFormat.getSortKey(javaObj, userKey);
-      } catch (Exception e) {
-
-      }
-
-      output.collect(bytesKey, tupleRow);
-    }
-
-    @Override
-    public void configure(JobConf job) {
-      bytesKey = new BytesWritable();
-      sortKey = job.get("sortKey");
-      try {
-        Schema outSchema = BasicTableOutputFormat.getSchema(job);
-        tupleRow = TypesUtils.createTuple(outSchema);
-        javaObj = BasicTableOutputFormat.getSortKeyGenerator(job);
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      } catch (org.apache.hadoop.zebra.parser.ParseException e) {
-        throw new RuntimeException(e);
-      }
-    }
-
-    @Override
-    public void close() throws IOException {
-      // no-op
-    }
-  }
-
-  static class ReduceClass implements
-      Reducer<BytesWritable, Tuple, BytesWritable, Tuple> {
-    Tuple outRow;
-
-    @Override
-    public void configure(JobConf job) {
-    }
-
-    @Override
-    public void close() throws IOException {
-    }
-
-    public void reduce(BytesWritable key, Iterator<Tuple> values,
-        OutputCollector<BytesWritable, Tuple> output, Reporter reporter)
-        throws IOException {
-      try {
-        for (; values.hasNext();) {
-          output.collect(key, values.next());
-        }
-      } catch (ExecException e) {
-        e.printStackTrace();
-      }
-    }
-
-  }
-
-  static class OutputPartitionerClass extends ZebraOutputPartition {
-
-    @Override
-    public int getOutputPartition(BytesWritable key, Tuple value) throws ExecException {
-      
-      return 0;
-    }
-
-  }
-
-  public void runMR(String myMultiLocs, String sortKey) throws ParseException,
-      IOException, Exception, org.apache.hadoop.zebra.parser.ParseException {
-
-    JobConf jobConf = new JobConf(conf);
-    jobConf.setJobName("TestMultipleOutputs4");
-    jobConf.setJarByClass(TestMultipleOutputs4.class);
-    jobConf.set("table.output.tfile.compression", "gz");
-    jobConf.set("sortKey", sortKey);
-    // input settings
-    jobConf.setInputFormat(TextInputFormat.class);
-    jobConf.setMapperClass(TestMultipleOutputs4.MapClass.class);
-    jobConf.setMapOutputKeyClass(BytesWritable.class);
-    jobConf.setMapOutputValueClass(ZebraTuple.class);
-    FileInputFormat.setInputPaths(jobConf, inputPath);
-
-    jobConf.setNumMapTasks(1);
-
-    // output settings
-
-    jobConf.setOutputFormat(BasicTableOutputFormat.class);
-    BasicTableOutputFormat.setMultipleOutputs(jobConf, myMultiLocs,
-        TestMultipleOutputs4.OutputPartitionerClass.class);
-
-    // set the logical schema with 2 columns
-    BasicTableOutputFormat.setSchema(jobConf, "word:string, count:int");
-    // for demo purposes, create 2 physical column groups
-    BasicTableOutputFormat.setStorageHint(jobConf, "[word];[count]");
-    BasicTableOutputFormat.setSortInfo(jobConf, sortKey);
-    System.out.println("in runMR, sortkey: " + sortKey);
-    // set map-only job.
-    jobConf.setNumReduceTasks(1);
-    JobClient.runJob(jobConf);
-    BasicTableOutputFormat.close(jobConf);
-  }
-
-  @Override
-  public int run(String[] args) throws Exception {
-    TestMultipleOutputs4 test = new TestMultipleOutputs4();
-    TestMultipleOutputs4.setUpOnce();
-   
-    test.test1();
-   
-    return 0;
-  }
-  
-  public static void main(String[] args) throws Exception {
-    conf = new Configuration();
-    
-    int res = ToolRunner.run(conf, new TestMultipleOutputs4(), args);
-    System.out.println("PASS");
-    System.exit(res);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestMultipleOutputs4TypedApi.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestMultipleOutputs4TypedApi.java
deleted file mode 100644
index 8ca92f5d9..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestMultipleOutputs4TypedApi.java
+++ /dev/null
@@ -1,536 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.BufferedWriter;
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.FileWriter;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapred.FileInputFormat;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.Mapper;
-import org.apache.hadoop.mapred.OutputCollector;
-import org.apache.hadoop.mapred.Reducer;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapred.TextInputFormat;
-import org.apache.hadoop.mapred.TextOutputFormat;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.mapred.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.types.ZebraTuple;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DefaultTuple;
-import org.apache.pig.data.Tuple;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * This is a sample a complete MR sample code for Table. It doens't contain
- * 'read' part. But, it should be similar and easier to write. Refer to test
- * cases in the same directory.
- * 
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * us 2
- * japan 2
- * india 4
- * us 2
- * japan 1
- * india 3
- * nouse 5
- * nowhere 4
- * 
- * 
- */
-public class TestMultipleOutputs4TypedApi extends BaseTestCase implements Tool {
-  static String inputPath;
-  static String inputFileName = "multi-input.txt";
-  public static String sortKey = null;
-
-  private static String strTable1 = null;
-  private static String strTable2 = null;
-  private static String strTable3 = null;
-
-  @BeforeClass
-  public static void setUpOnce() throws Exception {
-	  init();
-      inputPath = getTableFullPath(inputFileName).toString();
-      writeToFile(inputPath);
-  }
-
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    StringTokenizer st = new StringTokenizer(t, ".");
-    String methodName = null;
-    while (st.hasMoreTokens()) {
-      methodName = st.nextToken();
-    }
-    return methodName;
-  }
-
-   public static void writeToFile(String inputFile) throws IOException {
-    if ( mode == TestMode.local ) {
-      FileWriter fstream = new FileWriter(inputFile);
-      BufferedWriter out = new BufferedWriter(fstream);
-      out.write("us 2\n");
-      out.write("japan 2\n");
-      out.write("india 4\n");
-      out.write("us 2\n");
-      out.write("japan 1\n");
-      out.write("india 3\n");
-      out.write("nouse 5\n");
-      out.write("nowhere 4\n");
-      out.close();
-    } else {
-      FSDataOutputStream fout = fs.create(new Path(inputFile));
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 2\n");
-      fout.writeBytes("india 4\n");
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 1\n");
-      fout.writeBytes("india 3\n");
-      fout.writeBytes("nouse 5\n");
-      fout.writeBytes("nowhere 4\n");
-      fout.close();
-    }
-  }
-
-  public static void getTablePaths(String myMultiLocs) {
-    StringTokenizer st = new StringTokenizer(myMultiLocs, ",");
-
-    // get how many tokens inside st object
-    System.out.println("tokens count: " + st.countTokens());
-    int count = 0;
-
-    // iterate st object to get more tokens from it
-    while (st.hasMoreElements()) {
-      count++;
-      String token = st.nextElement().toString();
-      if ( mode == TestMode.local ) {
-        System.out.println("in mini, token: " + token);
-        // in mini, token:
-        // file:/homes/<uid>/grid/multipleoutput/pig-table/contrib/zebra/ustest3
-        if (count == 1)
-          strTable1 = token;
-        if (count == 2)
-          strTable2 = token;
-        if (count == 3)
-          strTable3 = token;
-      } else {
-        System.out.println("in real, token: " + token);
-        // in real, token: /user/hadoopqa/ustest3
-        // note: no prefix file: in real cluster
-        if (count == 1)
-          strTable1 = token;
-        if (count == 2)
-          strTable2 = token;
-        if (count == 3)
-          strTable3 = token;
-      }
-
-    }
-  }
-
-  public static void checkTable(String myMultiLocs) throws IOException {
-    System.out.println("myMultiLocs:" + myMultiLocs);
-    System.out.println("sorgetTablePathst key:" + sortKey);
-
-    getTablePaths(myMultiLocs);
-    String query1 = null;
-    String query2 = null;
-
-    if (strTable1 != null) {
-
-      query1 = "records1 = LOAD '" + strTable1
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-    if (strTable2 != null) {
-      query2 = "records2 = LOAD '" + strTable2
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-
-    int count1 = 0;
-    int count2 = 0;
-
-    if (query1 != null) {
-      System.out.println(query1);
-      pigServer.registerQuery(query1);
-      Iterator<Tuple> it = pigServer.openIterator("records1");
-      while (it.hasNext()) {
-        count1++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-        // test 1 us table
-        if (query1.contains("test1") || query1.contains("test2")
-            || query1.contains("test3")) {
-
-          if (count1 == 1) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count1 == 2) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-        } // test1, test2
-
-      }// while
-      if (query1.contains("test1") || query1.contains("test2")
-          || query1.contains("test3")) {
-        Assert.assertEquals(2, count1);
-      }
-    }// if query1 != null
-
-    if (query2 != null) {
-      pigServer.registerQuery(query2);
-      Iterator<Tuple> it = pigServer.openIterator("records2");
-
-      while (it.hasNext()) {
-        count2++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-
-        // if test1 other table
-        if (query2.contains("test1")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-
-          if (count1 == 5) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test1
-        // if test2 other table
-        if (query2.contains("test2")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-
-          if (count1 == 5) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test2
-        // if test3 other table
-        if (query2.contains("test3")) {
-          if (count2 == 1) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 5) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-
-        }// if test3
-
-      }// while
-      if (query2.contains("test1") || query2.contains("test2")
-          || query2.contains("test3")) {
-        Assert.assertEquals(6, count2);
-      }
-    }// if query2 != null
-
-  }
-
-  @Test
-  public void test1() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test combine sort keys
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "word,count";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-    List<Path> paths = new ArrayList<Path>(1);
-    
-    Path pathA = getTableFullPath( "a" );
-    paths.add( pathA );
-    myMultiLocs = pathA.toString();
-    
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    runMR(sortKey, paths.toArray(new Path[1]));
-    checkTableExists(true, strTable1);
-    System.out.println("DONE test " + getCurrentMethodName());
-
-  }
-
-  static class MapClass implements
-      Mapper<LongWritable, Text, BytesWritable, Tuple> {
-    private BytesWritable bytesKey;
-    private Tuple tupleRow;
-    private Object javaObj;
-
-    @Override
-    public void map(LongWritable key, Text value,
-        OutputCollector<BytesWritable, Tuple> output, Reporter reporter)
-        throws IOException {
-      // value should contain "word count"
-      String[] wdct = value.toString().split(" ");
-      if (wdct.length != 2) {
-        // LOG the error
-        return;
-      }
-
-      byte[] word = wdct[0].getBytes();
-      bytesKey.set(word, 0, word.length);
-      System.out.println("word: " + new String(word));
-      tupleRow.set(0, new String(word));
-      tupleRow.set(1, Integer.parseInt(wdct[1]));
-      System.out.println("count:  " + Integer.parseInt(wdct[1]));
-
-      // This key has to be created by user
-      /*
-       * Tuple userKey = new DefaultTuple(); userKey.append(new String(word));
-       * userKey.append(Integer.parseInt(wdct[1]));
-       */
-      System.out.println("in map, sortkey: " + sortKey);
-      Tuple userKey = new DefaultTuple();
-      if (sortKey.equalsIgnoreCase("word,count")) {
-        userKey.append(new String(word));
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("count")) {
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("word")) {
-        userKey.append(new String(word));
-      }
-
-      try {
-
-        /* New M/R Interface */
-        /* Converts user key to zebra BytesWritable key */
-        /* using sort key expr tree */
-        /* Returns a java base object */
-        /* Done for each user key */
-
-        bytesKey = BasicTableOutputFormat.getSortKey(javaObj, userKey);
-      } catch (Exception e) {
-
-      }
-
-      output.collect(bytesKey, tupleRow);
-    }
-
-    @Override
-    public void configure(JobConf job) {
-      bytesKey = new BytesWritable();
-      sortKey = job.get("sortKey");
-      try {
-        Schema outSchema = BasicTableOutputFormat.getSchema(job);
-        tupleRow = TypesUtils.createTuple(outSchema);
-        javaObj = BasicTableOutputFormat.getSortKeyGenerator(job);
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      } catch (org.apache.hadoop.zebra.parser.ParseException e) {
-        throw new RuntimeException(e);
-      }
-    }
-
-    @Override
-    public void close() throws IOException {
-      // no-op
-    }
-  }
-
-  static class ReduceClass implements
-      Reducer<BytesWritable, Tuple, BytesWritable, Tuple> {
-    Tuple outRow;
-
-    @Override
-    public void configure(JobConf job) {
-    }
-
-    @Override
-    public void close() throws IOException {
-    }
-
-    public void reduce(BytesWritable key, Iterator<Tuple> values,
-        OutputCollector<BytesWritable, Tuple> output, Reporter reporter)
-        throws IOException {
-      try {
-        for (; values.hasNext();) {
-          output.collect(key, values.next());
-        }
-      } catch (ExecException e) {
-        e.printStackTrace();
-      }
-    }
-
-  }
-
-  static class OutputPartitionerClass extends ZebraOutputPartition {
-
-    @Override
-    public int getOutputPartition(BytesWritable key, Tuple value)
-        throws ExecException {
-
-      return 0;
-    }
-
-  }
-
-  public void runMR(String sortKey, Path... paths) throws ParseException,
-      IOException, Exception, org.apache.hadoop.zebra.parser.ParseException {
-
-    JobConf jobConf = new JobConf(conf);
-    jobConf.setJobName("TestMultipleOutputs4TypedApi");
-    jobConf.setJarByClass(TestMultipleOutputs4TypedApi.class);
-    jobConf.set("table.output.tfile.compression", "gz");
-    jobConf.set("sortKey", sortKey);
-    // input settings
-    jobConf.setInputFormat(TextInputFormat.class);
-    jobConf.setMapperClass(TestMultipleOutputs4TypedApi.MapClass.class);
-    jobConf.setMapOutputKeyClass(BytesWritable.class);
-    jobConf.setMapOutputValueClass(ZebraTuple.class);
-    FileInputFormat.setInputPaths(jobConf, inputPath);
-
-    jobConf.setNumMapTasks(1);
-
-    // output settings
-
-    jobConf.setOutputFormat(BasicTableOutputFormat.class);
-
-    String schema = "word:string, count:int";
-    String storageHint = "[word];[count]";
-    BasicTableOutputFormat.setMultipleOutputs(jobConf,
-        TestMultipleOutputs4TypedApi.OutputPartitionerClass.class, paths);
-    ZebraSchema zSchema = ZebraSchema.createZebraSchema(schema);
-    ZebraStorageHint zStorageHint = ZebraStorageHint
-        .createZebraStorageHint(storageHint);
-    ZebraSortInfo zSortInfo = ZebraSortInfo.createZebraSortInfo(sortKey, null);
-    BasicTableOutputFormat.setStorageInfo(jobConf, zSchema, zStorageHint,
-        zSortInfo);
-
-    // set map-only job.
-    jobConf.setNumReduceTasks(1);
-    JobClient.runJob(jobConf);
-    BasicTableOutputFormat.close(jobConf);
-  }
-
-  @Override
-  public int run(String[] args) throws Exception {
-    TestMultipleOutputs4TypedApi test = new TestMultipleOutputs4TypedApi();
-    TestMultipleOutputs4TypedApi.setUpOnce();
-
-    test.test1();
-    
-    return 0;
-  }
-  
-  public static void main(String[] args) throws Exception {
-    conf = new Configuration();
-    
-    int res = ToolRunner.run(conf, new TestMultipleOutputs4TypedApi(), args);
-    System.out.println("PASS");
-    System.exit(res);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestMultipleOutputsTypeApi.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestMultipleOutputsTypeApi.java
deleted file mode 100644
index 6a5dc0160..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestMultipleOutputsTypeApi.java
+++ /dev/null
@@ -1,646 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.BufferedWriter;
-import java.io.ByteArrayOutputStream;
-import java.io.FileWriter;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapred.FileInputFormat;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.Mapper;
-import org.apache.hadoop.mapred.OutputCollector;
-import org.apache.hadoop.mapred.Reducer;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapred.TextInputFormat;
-import org.apache.hadoop.mapred.TextOutputFormat;
-import org.apache.hadoop.mapred.lib.MultipleOutputs;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.mapred.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.types.ZebraTuple;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DefaultTuple;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * This is a sample a complete MR sample code for Table. It doens't contain
- * 'read' part. But, it should be similar and easier to write. Refer to test
- * cases in the same directory.
- * 
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * us 2
- * japan 2
- * india 4
- * us 2
- * japan 1
- * india 3
- * nouse 5
- * nowhere 4
- * 
- * 
- */
-public class TestMultipleOutputsTypeApi extends BaseTestCase implements Tool {
-  static String inputPath;
-  static String inputFileName = "multi-input.txt";
-  public static String sortKey = null;
-
-  private static String strTable1 = null;
-  private static String strTable2 = null;
-  private static String strTable3 = null;
-
-  @BeforeClass
-  public static void setUpOnce() throws Exception {
-	  init();
-      inputPath = getTableFullPath(inputFileName).toString();
-      writeToFile(inputPath);
-  }
-
-  @AfterClass
-  public static void tearDown() throws Exception {
-      pigServer.shutdown();
-  }
-
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    StringTokenizer st = new StringTokenizer(t, ".");
-    String methodName = null;
-    while (st.hasMoreTokens()) {
-      methodName = st.nextToken();
-    }
-    return methodName;
-  }
-
-  public static void writeToFile(String inputFile) throws IOException {
-    if ( mode == TestMode.local ) {
-      FileWriter fstream = new FileWriter(inputFile);
-      BufferedWriter out = new BufferedWriter(fstream);
-      out.write("us 2\n");
-      out.write("japan 2\n");
-      out.write("india 4\n");
-      out.write("us 2\n");
-      out.write("japan 1\n");
-      out.write("india 3\n");
-      out.write("nouse 5\n");
-      out.write("nowhere 4\n");
-      out.close();
-    } else {
-      FSDataOutputStream fout = fs.create(new Path(inputFile));
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 2\n");
-      fout.writeBytes("india 4\n");
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 1\n");
-      fout.writeBytes("india 3\n");
-      fout.writeBytes("nouse 5\n");
-      fout.writeBytes("nowhere 4\n");
-      fout.close();
-    }
-  }
-
-  public static void getTablePaths(String myMultiLocs) {
-    StringTokenizer st = new StringTokenizer(myMultiLocs, ",");
-
-    // get how many tokens inside st object
-    System.out.println("tokens count: " + st.countTokens());
-    int count = 0;
-
-    // iterate st object to get more tokens from it
-    while (st.hasMoreElements()) {
-      count++;
-      String token = st.nextElement().toString();
-      if (count == 1)
-        strTable1 = token;
-      if (count == 2)
-        strTable2 = token;
-      if (count == 3)
-        strTable3 = token;
-      System.out.println("token = " + token);
-    }
-  }
-
-  public static void checkTable(String myMultiLocs) throws IOException {
-    System.out.println("myMultiLocs:" + myMultiLocs);
-    System.out.println("sorgetTablePathst key:" + sortKey);
-
-    getTablePaths(myMultiLocs);
-    String query1 = null;
-    String query2 = null;
-    String query3 = null;
-
-    if (strTable1 != null) {
-
-      query1 = "records1 = LOAD '" + strTable1
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-    if (strTable2 != null) {
-      query2 = "records2 = LOAD '" + strTable2
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-    if (strTable3 != null) {
-      query3 = "records3 = LOAD '" + strTable3
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-
-    int count1 = 0;
-    int count2 = 0;
-    int count3 = 0;
-
-    if (query1 != null) {
-      System.out.println(query1);
-      pigServer.registerQuery(query1);
-      Iterator<Tuple> it = pigServer.openIterator("records1");
-      while (it.hasNext()) {
-        count1++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-        // test 1 us table
-        if (query1.contains("test1") || query1.contains("test2")) {
-          if (count1 == 1) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 2) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 3) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count1 == 4) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-        } // test1, test2
-        if (query1.contains("test3")) {
-          if (count1 == 1) {
-            System.out.println("test3: rowvalue: " + RowValue.toString());
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count1 == 2) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count1 == 3) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 4) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-        } // test3
-      }// while
-      if (query1.contains("test1") || query1.contains("test2")
-          || query1.contains("test3")) {
-        Assert.assertEquals(4, count1);
-      }
-    }// if query1 != null
-
-    if (query2 != null) {
-      pigServer.registerQuery(query2);
-      Iterator<Tuple> it = pigServer.openIterator("records2");
-
-      while (it.hasNext()) {
-        count2++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-
-        // if test1 india table
-        if (query2.contains("test1")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test1
-        if (query1.contains("test2")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-        }// if test2
-
-        if (query1.contains("test3")) {
-          if (count1 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count1 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        } // test3
-
-      }// while
-      if (query2.contains("test1") || query2.contains("test2")
-          || query2.contains("test3")) {
-        Assert.assertEquals(2, count2);
-      }
-    }// if query2 != null
-    // test 1 us table should have 4 lines
-
-    if (query3 != null) {
-      pigServer.registerQuery(query3);
-      Iterator<Tuple> it = pigServer.openIterator("records3");
-
-      while (it.hasNext()) {
-        count3++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-
-        // if test1 japan table
-        if (query3.contains("test1")) {
-          if (count3 == 1) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count3 == 2) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-        }// if test1 test2 japan table
-
-        if (query3.contains("test2")) {
-          if (count3 == 1) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count3 == 2) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-        }// test2
-        if (query1.contains("test3")) {
-          if (count1 == 1) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count1 == 2) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-        } // test3
-
-      }// while
-      if (query3.contains("test1") || query3.contains("test2")
-          || query3.contains("test3")) {
-        Assert.assertEquals(2, count3);
-      }
-    }// if query2 != null
-  }
-
-  @Test
-  public void test1() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test combine sort keys
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "word,count";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-    List<Path> paths = new ArrayList<Path>(3);
-    
-    Path pathUs = getTableFullPath( "us" + methodName );
-    Path pathIndia = getTableFullPath( "india" + methodName );
-    Path pathJapan = getTableFullPath( "japan" + methodName );
-    myMultiLocs = pathUs.toString() + "," + 
-	    pathIndia.toString() + "," +
-	    pathJapan.toString();
-    paths.add(pathUs);
-    paths.add(pathIndia);
-    paths.add(pathJapan);
-    
-    getTablePaths(myMultiLocs);
-    System.out.println("strTable1: " + strTable1.toString());
-    System.out.println("strTable2: " + strTable2.toString());
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    removeDir(new Path(strTable3));
-    runMR(sortKey, paths.toArray(new Path[3]));
-    checkTable(myMultiLocs);
-    System.out.println("DONE test 1");
-
-  }
-
-  @Test
-  public void test2() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test 'word' sort key
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "word";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-    List<Path> paths = new ArrayList<Path>(3);
-
-    Path pathUs = getTableFullPath( "us" + methodName );
-    Path pathIndia = getTableFullPath( "india" + methodName );
-    Path pathJapan = getTableFullPath( "japan" + methodName );
-    myMultiLocs = pathUs.toString() + "," + 
-	    pathIndia.toString() + "," +
-	    pathJapan.toString();
-    paths.add(pathUs);
-    paths.add(pathIndia);
-    paths.add(pathJapan);
-
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    removeDir(new Path(strTable3));
-    runMR(sortKey, paths.toArray(new Path[3]));
-    checkTable(myMultiLocs);
-    System.out.println("DONE test 2");
-  }
-
-  @Test
-  public void test3() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test 'count' sort key
-     */
-    // setUpOnce();
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "count";
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-    List<Path> paths = new ArrayList<Path>(3);
-
-    Path pathUs = getTableFullPath( "us" + methodName );
-    Path pathIndia = getTableFullPath( "india" + methodName );
-    Path pathJapan = getTableFullPath( "japan" + methodName );
-    myMultiLocs = pathUs.toString() + "," + 
-	    pathIndia.toString() + "," +
-	    pathJapan.toString();
-    paths.add(pathUs);
-    paths.add(pathIndia);
-    paths.add(pathJapan);
-
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    removeDir(new Path(strTable3));
-    runMR(sortKey, paths.toArray(new Path[3]));
-    checkTable(myMultiLocs);
-    System.out.println("DONE test 3");
-  }
-
-  static class MapClass implements
-      Mapper<LongWritable, Text, BytesWritable, Tuple> {
-    private BytesWritable bytesKey;
-    private Tuple tupleRow;
-    private Object javaObj;
-
-    @Override
-    public void map(LongWritable key, Text value,
-        OutputCollector<BytesWritable, Tuple> output, Reporter reporter)
-        throws IOException {
-      // value should contain "word count"
-      String[] wdct = value.toString().split(" ");
-      if (wdct.length != 2) {
-        // LOG the error
-        return;
-      }
-
-      byte[] word = wdct[0].getBytes();
-      bytesKey.set(word, 0, word.length);
-      System.out.println("word: " + new String(word));
-      tupleRow.set(0, new String(word));
-      tupleRow.set(1, Integer.parseInt(wdct[1]));
-      System.out.println("count:  " + Integer.parseInt(wdct[1]));
-
-      // This key has to be created by user
-      /*
-       * Tuple userKey = new DefaultTuple(); userKey.append(new String(word));
-       * userKey.append(Integer.parseInt(wdct[1]));
-       */
-      System.out.println("in map, sortkey: " + sortKey);
-      Tuple userKey = new DefaultTuple();
-      if (sortKey.equalsIgnoreCase("word,count")) {
-        userKey.append(new String(word));
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("count")) {
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("word")) {
-        userKey.append(new String(word));
-      }
-
-      try {
-
-        /* New M/R Interface */
-        /* Converts user key to zebra BytesWritable key */
-        /* using sort key expr tree */
-        /* Returns a java base object */
-        /* Done for each user key */
-
-        bytesKey = BasicTableOutputFormat.getSortKey(javaObj, userKey);
-      } catch (Exception e) {
-
-      }
-
-      output.collect(bytesKey, tupleRow);
-    }
-
-    @Override
-    public void configure(JobConf job) {
-      bytesKey = new BytesWritable();
-      sortKey = job.get("sortKey");
-      try {
-        Schema outSchema = BasicTableOutputFormat.getSchema(job);
-        tupleRow = TypesUtils.createTuple(outSchema);
-        javaObj = BasicTableOutputFormat.getSortKeyGenerator(job);
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      } catch (org.apache.hadoop.zebra.parser.ParseException e) {
-        throw new RuntimeException(e);
-      }
-    }
-
-    @Override
-    public void close() throws IOException {
-      // no-op
-    }
-  }
-
-  static class ReduceClass implements
-      Reducer<BytesWritable, Tuple, BytesWritable, Tuple> {
-    Tuple outRow;
-
-    @Override
-    public void configure(JobConf job) {
-    }
-
-    @Override
-    public void close() throws IOException {
-    }
-
-    public void reduce(BytesWritable key, Iterator<Tuple> values,
-        OutputCollector<BytesWritable, Tuple> output, Reporter reporter)
-        throws IOException {
-      try {
-        for (; values.hasNext();) {
-          output.collect(key, values.next());
-        }
-      } catch (ExecException e) {
-        e.printStackTrace();
-      }
-    }
-
-  }
-
-  static class OutputPartitionerClass extends ZebraOutputPartition {
-
-    @Override
-    public int getOutputPartition(BytesWritable key, Tuple value) {
-
-      // System.out.println(this.jobConf);
-
-      String reg = null;
-      try {
-        reg = (String) (value.get(0));
-      } catch (Exception e) {
-        //
-      }
-
-      if (reg.equals("us"))
-        return 0;
-      if (reg.equals("india"))
-        return 1;
-      if (reg.equals("japan"))
-        return 2;
-
-      return 0;
-    }
-
-  }
-
-  public void runMR(String sortKey, Path... paths) throws ParseException,
-      IOException, Exception, org.apache.hadoop.zebra.parser.ParseException {
-
-    JobConf jobConf = new JobConf(conf);
-    //XXX
-    jobConf.setJobName("TestMultipleOutputsTypeApi");
-    jobConf.setJarByClass(TestMultipleOutputsTypeApi.class);
-    jobConf.set("table.output.tfile.compression", "gz");
-    jobConf.set("sortKey", sortKey);
-    // input settings
-    jobConf.setInputFormat(TextInputFormat.class);
-    jobConf.setMapperClass(TestMultipleOutputsTypeApi.MapClass.class);
-    jobConf.setMapOutputKeyClass(BytesWritable.class);
-    jobConf.setMapOutputValueClass(ZebraTuple.class);
-    FileInputFormat.setInputPaths(jobConf, inputPath);
-
-    jobConf.setNumMapTasks(1);
-
-    // output settings
-
-    jobConf.setOutputFormat(BasicTableOutputFormat.class);
-
-    String schema = "word:string, count:int";
-    String storageHint = "[word];[count]";
-    BasicTableOutputFormat.setMultipleOutputs(jobConf,
-        TestMultipleOutputsTypeApi.OutputPartitionerClass.class, paths);
-    ZebraSchema zSchema = ZebraSchema.createZebraSchema(schema);
-    ZebraStorageHint zStorageHint = ZebraStorageHint
-        .createZebraStorageHint(storageHint);
-    ZebraSortInfo zSortInfo = ZebraSortInfo.createZebraSortInfo(sortKey, null);
-    BasicTableOutputFormat.setStorageInfo(jobConf, zSchema, zStorageHint,
-        zSortInfo);
-
-    jobConf.setNumReduceTasks(1);
-    JobClient.runJob(jobConf);
-    BasicTableOutputFormat.close(jobConf);
-  }
-
-  @Override
-  public int run(String[] args) throws Exception {
-    TestMultipleOutputsTypeApi test = new TestMultipleOutputsTypeApi();
-    TestMultipleOutputsTypeApi.setUpOnce();
-    System.out.println("after setup");
-    
-    test.test1();
-    test.test2();
-    test.test3();
-
-    return 0;
-  }
-  
-  public static void main(String[] args) throws Exception {
-    conf = new Configuration();
-    
-    int res = ToolRunner.run(conf, new TestMultipleOutputsTypeApi(), args);
-    System.out.println("PASS");
-    System.exit(res);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestMultipleOutputsTypedApiNeg.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestMultipleOutputsTypedApiNeg.java
deleted file mode 100644
index 983ace9f2..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestMultipleOutputsTypedApiNeg.java
+++ /dev/null
@@ -1,675 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.BufferedWriter;
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.FileWriter;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-import java.util.StringTokenizer;
-import java.util.TreeMap;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.conf.Configured;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapred.FileInputFormat;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.Mapper;
-import org.apache.hadoop.mapred.OutputCollector;
-import org.apache.hadoop.mapred.Reducer;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapred.TextInputFormat;
-import org.apache.hadoop.mapred.TextOutputFormat;
-import org.apache.hadoop.mapred.lib.MultipleOutputs;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.mapred.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.mapred.TestBasicTableIOFormatLocalFS.InvIndex;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.types.ZebraTuple;
-import org.apache.pig.ExecType;
-import org.apache.pig.PigServer;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DefaultTuple;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.test.MiniCluster;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * This is a sample a complete MR sample code for Table. It doens't contain
- * 'read' part. But, it should be similar and easier to write. Refer to test
- * cases in the same directory.
- * 
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * us 2
- * japan 2
- * india 4
- * us 2
- * japan 1
- * india 3
- * nouse 5
- * nowhere 4
- * 
- * 
- */
-public class TestMultipleOutputsTypedApiNeg extends BaseTestCase implements Tool {
-  static String inputPath;
-  static String inputFileName = "multi-input.txt";
-  public static String sortKey = null;
-
-  private static String strTable1 = null;
-  private static String strTable2 = null;
-  private static String strTable3 = null;
-
-  @BeforeClass
-  public static void setUpOnce() throws Exception {
-	  init();
-    // set inputPath and output path
-    String workingDir = null;
-    inputPath = getTableFullPath(inputFileName).toString();
-    writeToFile(inputPath);
-  }
-
-  public static void writeToFile(String inputFile) throws IOException {
-    if ( mode ==  TestMode.local ) {
-      FileWriter fstream = new FileWriter(inputFile);
-      BufferedWriter out = new BufferedWriter(fstream);
-      out.write("us 2\n");
-      out.write("japan 2\n");
-      out.write("india 4\n");
-      out.write("us 2\n");
-      out.write("japan 1\n");
-      out.write("india 3\n");
-      out.write("nouse 5\n");
-      out.write("nowhere 4\n");
-      out.close();
-    } else {
-      FSDataOutputStream fout = fs.create(new Path(inputFile));
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 2\n");
-      fout.writeBytes("india 4\n");
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 1\n");
-      fout.writeBytes("india 3\n");
-      fout.writeBytes("nouse 5\n");
-      fout.writeBytes("nowhere 4\n");
-      fout.close();
-    }
-  }
-
-  public String getCurrentMethodName() {
-	  ByteArrayOutputStream baos = new ByteArrayOutputStream();
-	  PrintWriter pw = new PrintWriter(baos);
-	  (new Throwable()).printStackTrace(pw);
-	  pw.flush();
-	  String stackTrace = baos.toString();
-	  pw.close();
-
-	  StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-	  tok.nextToken(); // 'java.lang.Throwable'
-	  tok.nextToken(); // 'at ...getCurrentMethodName'
-	  String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-	  // Parse line 3
-	  tok = new StringTokenizer(l.trim(), " <(");
-	  String t = tok.nextToken(); // 'at'
-	  t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-	  StringTokenizer st = new StringTokenizer(t, ".");
-	  String methodName = null;
-	  while (st.hasMoreTokens()) {
-		  methodName = st.nextToken();
-	  }
-	  return methodName;
-  }
-  
-  public static void getTablePaths(String myMultiLocs) {
-    StringTokenizer st = new StringTokenizer(myMultiLocs, ",");
-
-    // get how many tokens inside st object
-    System.out.println("tokens count: " + st.countTokens());
-    int count = 0;
-
-    // iterate st object to get more tokens from it
-    while (st.hasMoreElements()) {
-      count++;
-      String token = st.nextElement().toString();
-      if ( mode == TestMode.local ) {
-        System.out.println("in mini, token: " + token);
-        // in mini, token:
-        // file:/homes/<uid>/grid/multipleoutput/pig-table/contrib/zebra/ustest3
-        if (count == 1)
-          strTable1 = token;
-        if (count == 2)
-          strTable2 = token;
-        if (count == 3)
-          strTable3 = token;
-      } else {
-        System.out.println("in real, token: " + token);
-        // in real, token: /user/hadoopqa/ustest3
-        // note: no prefix file: in real cluster
-        if (count == 1)
-          strTable1 = token;
-        if (count == 2)
-          strTable2 = token;
-        if (count == 3)
-          strTable3 = token;
-      }
-
-    }
-  }
-
-  public static void checkTable(String myMultiLocs) throws IOException {
-    System.out.println("myMultiLocs:" + myMultiLocs);
-    System.out.println("sorgetTablePathst key:" + sortKey);
-
-    getTablePaths(myMultiLocs);
-    String query1 = null;
-    String query2 = null;
-
-    if (strTable1 != null) {
-
-      query1 = "records1 = LOAD '" + strTable1
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-    if (strTable2 != null) {
-      query2 = "records2 = LOAD '" + strTable2
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-
-    int count1 = 0;
-    int count2 = 0;
-
-    if (query1 != null) {
-      System.out.println(query1);
-      pigServer.registerQuery(query1);
-      Iterator<Tuple> it = pigServer.openIterator("records1");
-      while (it.hasNext()) {
-        count1++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-        // test 1 us table
-        if (query1.contains("test1") || query1.contains("test2")
-            || query1.contains("test3")) {
-
-          if (count1 == 1) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count1 == 2) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-        } // test1, test2
-
-      }// while
-      if (query1.contains("test1") || query1.contains("test2")
-          || query1.contains("test3")) {
-        Assert.assertEquals(2, count1);
-      }
-    }// if query1 != null
-
-    if (query2 != null) {
-      pigServer.registerQuery(query2);
-      Iterator<Tuple> it = pigServer.openIterator("records2");
-
-      while (it.hasNext()) {
-        count2++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-
-        // if test1 other table
-        if (query2.contains("test1")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-
-          if (count1 == 5) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test1
-        // if test2 other table
-        if (query2.contains("test2")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-
-          if (count1 == 5) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test2
-        // if test3 other table
-        if (query2.contains("test3")) {
-          if (count2 == 1) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 5) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-
-        }// if test3
-
-      }// while
-      if (query2.contains("test1") || query2.contains("test2")
-          || query2.contains("test3")) {
-        Assert.assertEquals(6, count2);
-      }
-    }// if query2 != null
-
-  }
-
-  @Test(expected = IllegalArgumentException.class)
-  public void test1() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * path list have an empty value in the middle should throw
-     * illegalArgumentExcepiton: Can not create a path from an empty string
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "word,count";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-    List<Path> paths = new ArrayList<Path>(3);
-
-    myMultiLocs = getTableFullPath( "a" + methodName  ).toString() + "," + 
-        getTableFullPath( "b" + methodName ).toString();
-
-
-    paths.add(getTableFullPath( "a" + methodName  ));
-    
-    if (mode == TestMode.cluster) {
-      try {
-        paths.add(new Path(""));
-      } catch (IllegalArgumentException e) {
-        System.out.println(e.getMessage());
-        return;
-      }
-    } else {
-      paths.add(new Path(""));
-    }
-
-    // should not reach here
-    Assert.fail("Should have seen exception already");
-
-    paths.add(getTableFullPath( "b" + methodName  ));
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    removeDir(new Path(strTable3));
-    runMR(sortKey, paths.toArray(new Path[3]));
-    System.out.println("DONE test 1");
-  }
-
-  @Test(expected = IOException.class)
-  public void test2() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * path list have only one element OutputPartitionClass return 0 and return
-     * 1, expecting more element in the path list
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "word,count";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-    List<Path> paths = new ArrayList<Path>(1);
-    
-    Path path = getTableFullPath( "a" + methodName  );
-    myMultiLocs = path.toString();
-    paths.add( path );
-
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    runMR(sortKey, paths.toArray(new Path[1]));
-    System.out.println("DONE test 2");
-  }
-
-  @Test(expected = NullPointerException.class)
-  public void test3() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * path list is null
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "word,count";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-    List<Path> paths = new ArrayList<Path>(1);
-
-    myMultiLocs = getTableFullPath( "a" + methodName ).toString();
-    paths.add(null);
-    
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    
-    if (mode == TestMode.cluster) {
-      try {
-        runMR(sortKey, paths.toArray(new Path[1]));
-      } catch (NullPointerException e) {
-        System.err.println(e.getMessage());
-        System.out.println("DONE test 3");
-        return;
-      }
-    } else {
-      runMR(sortKey, paths.toArray(new Path[1]));
-      System.out.println("DONE test 3");
-    }
-  }
-
-  @Test(expected = IOException.class)
-  public void test4() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * path list have repeat element, for example atest1 dir has been already
-     * created. should throw IOExcepiton. complaining atest4/CG0/.meta already
-     * exists
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "word,count";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-    List<Path> paths = new ArrayList<Path>(3);
-
-    Path pathA = getTableFullPath( "a" + methodName  );
-    Path pathB = getTableFullPath( "b" + methodName  );
-
-    myMultiLocs = pathA.toString() + "," + pathA.toString() + "," 
-    + pathB.toString();
-
-    paths.add( pathA );
-    paths.add( pathA );
-    paths.add( pathB );
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    removeDir(new Path(strTable3));
-    runMR(sortKey, paths.toArray(new Path[3]));
-    System.out.println("DONE test 4");
-  }
-
-  static class MapClass implements
-      Mapper<LongWritable, Text, BytesWritable, Tuple> {
-    private BytesWritable bytesKey;
-    private Tuple tupleRow;
-    private Object javaObj;
-
-    @Override
-    public void map(LongWritable key, Text value,
-        OutputCollector<BytesWritable, Tuple> output, Reporter reporter)
-        throws IOException {
-      // value should contain "word count"
-      String[] wdct = value.toString().split(" ");
-      if (wdct.length != 2) {
-        // LOG the error
-        return;
-      }
-
-      byte[] word = wdct[0].getBytes();
-      bytesKey.set(word, 0, word.length);
-      System.out.println("word: " + new String(word));
-      tupleRow.set(0, new String(word));
-      tupleRow.set(1, Integer.parseInt(wdct[1]));
-      System.out.println("count:  " + Integer.parseInt(wdct[1]));
-
-      // This key has to be created by user
-      /*
-       * Tuple userKey = new DefaultTuple(); userKey.append(new String(word));
-       * userKey.append(Integer.parseInt(wdct[1]));
-       */
-      System.out.println("in map, sortkey: " + sortKey);
-      Tuple userKey = new DefaultTuple();
-      if (sortKey.equalsIgnoreCase("word,count")) {
-        userKey.append(new String(word));
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("count")) {
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("word")) {
-        userKey.append(new String(word));
-      }
-
-      try {
-
-        /* New M/R Interface */
-        /* Converts user key to zebra BytesWritable key */
-        /* using sort key expr tree */
-        /* Returns a java base object */
-        /* Done for each user key */
-
-        bytesKey = BasicTableOutputFormat.getSortKey(javaObj, userKey);
-      } catch (Exception e) {
-
-      }
-
-      output.collect(bytesKey, tupleRow);
-    }
-
-    @Override
-    public void configure(JobConf job) {
-      bytesKey = new BytesWritable();
-      sortKey = job.get("sortKey");
-      try {
-        Schema outSchema = BasicTableOutputFormat.getSchema(job);
-        tupleRow = TypesUtils.createTuple(outSchema);
-        javaObj = BasicTableOutputFormat.getSortKeyGenerator(job);
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      } catch (org.apache.hadoop.zebra.parser.ParseException e) {
-        throw new RuntimeException(e);
-      }
-    }
-
-    @Override
-    public void close() throws IOException {
-      // no-op
-    }
-  }
-
-  static class ReduceClass implements
-      Reducer<BytesWritable, Tuple, BytesWritable, Tuple> {
-    Tuple outRow;
-
-    @Override
-    public void configure(JobConf job) {
-    }
-
-    @Override
-    public void close() throws IOException {
-    }
-
-    public void reduce(BytesWritable key, Iterator<Tuple> values,
-        OutputCollector<BytesWritable, Tuple> output, Reporter reporter)
-        throws IOException {
-      try {
-        for (; values.hasNext();) {
-          output.collect(key, values.next());
-        }
-      } catch (ExecException e) {
-        e.printStackTrace();
-      }
-    }
-
-  }
-
-  static class OutputPartitionerClass extends ZebraOutputPartition {
-
-    @Override
-    public int getOutputPartition(BytesWritable key, Tuple value)
-        throws ExecException {
-
-      String reg = null;
-      try {
-        reg = (String) (value.get(0));
-      } catch (Exception e) {
-        //
-      }
-
-      if (reg.equals("us"))
-        return 0;
-      else
-        return 1;
-
-    }
-  }
-
-  public void runMR(String sortKey, Path... paths) throws ParseException,
-      IOException, Exception, org.apache.hadoop.zebra.parser.ParseException {
-
-    JobConf jobConf = new JobConf(conf);
-    jobConf.setJobName("TestMultipleOutputsTypedApiNeg");
-    jobConf.setJarByClass(TestMultipleOutputsTypedApiNeg.class);
-    jobConf.set("table.output.tfile.compression", "gz");
-    jobConf.set("sortKey", sortKey);
-    // input settings
-    jobConf.setInputFormat(TextInputFormat.class);
-    jobConf.setMapperClass(TestMultipleOutputsTypedApiNeg.MapClass.class);
-    jobConf.setMapOutputKeyClass(BytesWritable.class);
-    jobConf.setMapOutputValueClass(ZebraTuple.class);
-    FileInputFormat.setInputPaths(jobConf, inputPath);
-
-    jobConf.setNumMapTasks(1);
-
-    // output settings
-
-    jobConf.setOutputFormat(BasicTableOutputFormat.class);
-    
-    String schema = "word:string, count:int";
-    String storageHint = "[word];[count]";
-    BasicTableOutputFormat.setMultipleOutputs(jobConf,
-        TestMultipleOutputsTypedApiNeg.OutputPartitionerClass.class, paths);
-    ZebraSchema zSchema = ZebraSchema.createZebraSchema(schema);
-    ZebraStorageHint zStorageHint = ZebraStorageHint
-        .createZebraStorageHint(storageHint);
-    ZebraSortInfo zSortInfo = ZebraSortInfo.createZebraSortInfo(sortKey, null);
-    BasicTableOutputFormat.setStorageInfo(jobConf, zSchema, zStorageHint,
-        zSortInfo);
-    jobConf.setNumReduceTasks(1);
-    JobClient.runJob(jobConf);
-    BasicTableOutputFormat.close(jobConf);
-  }
-
-  @Override
-  public int run(String[] args) throws Exception {
-    TestMultipleOutputsTypedApiNeg test = new TestMultipleOutputsTypedApiNeg();
-    TestMultipleOutputsTypedApiNeg.setUpOnce();
-
-    test.test1();
-    
-    //TODO: backend exception - will migrate to real cluster later
-    //test.test2();
-    
-    test.test3();
-    
-    //TODO: backend exception
-    //test.test4();
-    
-    return 0;
-  }
-
-  public static void main(String[] args) throws Exception {
-    conf = new Configuration();
-    
-    int res = ToolRunner.run(conf, new TestMultipleOutputsTypedApiNeg(), args);
-    
-    System.out.println("PASS");
-    System.exit(res);
-  } 
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestSmokeMR.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestSmokeMR.java
deleted file mode 100644
index 810ab6bc0..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestSmokeMR.java
+++ /dev/null
@@ -1,285 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.BufferedWriter;
-import java.io.File;
-import java.io.FileWriter;
-import java.io.IOException;
-import java.util.Iterator;
-import java.util.StringTokenizer;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapred.FileInputFormat;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.Mapper;
-import org.apache.hadoop.mapred.OutputCollector;
-import org.apache.hadoop.mapred.Reducer;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapred.TextInputFormat;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.mapred.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.types.ZebraTuple;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.Tuple;
-import org.junit.Assert;
-import org.junit.BeforeClass;
-
-/**
- * This is a sample a complete MR sample code for Table. It doens't contain
- * 'read' part. But, it should be similar and easier to write. Refer to test
- * cases in the same directory.
- * 
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * this 2
- * is 1
- * a 4 
- * test 2 
- * hello 1 
- * world 3
- * </pre>
- * 
- */
-public class TestSmokeMR extends BaseTestCase implements Tool{
-  static String inputPath;
-  static String outputPath;
-  static String inputFileName = "smoke.txt";
-  static String outputTableName ="smokeTable";
-  public static String sortKey = null;
- 
-	static class MapClass implements
-      Mapper<LongWritable, Text, BytesWritable, Tuple> {
-    private BytesWritable bytesKey;
-    private Tuple tupleRow;
-    private Object javaObj;
-
-    @Override
-    public void map(LongWritable key, Text value,
-        OutputCollector<BytesWritable, Tuple> output, Reporter reporter)
-        throws IOException {
-        // value should contain "word count"
-        String[] wdct = value.toString().split(" ");
-        if (wdct.length != 2) {
-          // LOG the error
-          return;
-        }
-
-        byte[] word = wdct[0].getBytes();
-        bytesKey.set(word, 0, word.length);
-        tupleRow.set(0, new String(word));
-        tupleRow.set(1, Integer.parseInt(wdct[1]));
-
-        // This key has to be created by user
-        Tuple userKey = new ZebraTuple();
-        userKey.append(new String(word));
-        userKey.append(Integer.parseInt(wdct[1]));
-        try {
-
-            /* New M/R Interface */
-            /* Converts user key to zebra BytesWritable key */
-            /* using sort key expr tree  */
-            /* Returns a java base object */
-            /* Done for each user key */
-        	
-          bytesKey = BasicTableOutputFormat.getSortKey(javaObj, userKey);
-        } catch(Exception e) {
-        	
-        }
-
-        output.collect(bytesKey, tupleRow);
-    }
-
-    @Override
-    public void configure(JobConf job) {
-      bytesKey = new BytesWritable();
-      try {
-        Schema outSchema = BasicTableOutputFormat.getSchema(job);
-        tupleRow = TypesUtils.createTuple(outSchema);
-        
-        /* New M/R Interface */
-        /* returns an expression tree for sort keys */
-        /* Returns a java base object */
-        /* Done once per table */
-        javaObj = BasicTableOutputFormat.getSortKeyGenerator(job);
-        
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      } catch (ParseException e) {
-        throw new RuntimeException(e);
-      }
-    }
-
-    @Override
-    public void close() throws IOException {
-      // no-op
-    }
-  }
-
-  static class ReduceClass implements
-    Reducer<BytesWritable, Tuple, BytesWritable, Tuple> {
-      Tuple outRow;
-     
-
-    @Override
-    public void configure(JobConf job) {
-    }
-
-    @Override
-    public void close() throws IOException {
-    }	
-    public void reduce(BytesWritable key, Iterator<Tuple> values,
-      OutputCollector<BytesWritable, Tuple> output, Reporter reporter)
-   	    throws IOException {
-    	  try {
-    		for(; values.hasNext();)  {
-    	      output.collect(key, values.next());
-    		}  
-    	  } catch (ExecException e) {
-    	    e.printStackTrace();
-    	  }
-    }
-  
-  }  
-  
-  @BeforeClass
-  public static void setUpOnce() throws IOException {
-      inputPath = getTableFullPath( inputFileName ).toString();
-      outputPath =  getTableFullPath(outputTableName).toString();
-      writeToFile(inputPath);
-  }
-
-  public static void writeToFile(String inputFile) throws IOException {
-    if ( mode == TestMode.cluster ) {
-      FileWriter fstream = new FileWriter(inputFile);
-      BufferedWriter out = new BufferedWriter(fstream);
-      out.write("us 2\n");
-      out.write("japan 2\n");
-      out.write("india 4\n");
-      out.write("us 2\n");
-      out.write("japan 1\n");
-      out.write("india 3\n");
-      out.write("nouse 5\n");
-      out.write("nowhere 4\n");
-      out.close();
-    } else {
-      FSDataOutputStream fout = fs.create(new Path(inputFile));
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 2\n");
-      fout.writeBytes("india 4\n");
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 1\n");
-      fout.writeBytes("india 3\n");
-      fout.writeBytes("nouse 5\n");
-      fout.writeBytes("nowhere 4\n");
-      fout.close();
-    }
-  }
-
-  public static void removeDir(Path outPath) throws IOException {
-    String command = null;
-    if ( mode == TestMode.cluster ) {
-      command = System.getenv("HADOOP_HOME") + "/bin/hadoop fs -rmr "
-          + outPath.toString();
-    } else {
-      StringTokenizer st = new StringTokenizer(outPath.toString(), ":");
-      int count = 0;
-      String file = null;
-      while (st.hasMoreElements()) {
-        count++;
-        String token = st.nextElement().toString();
-        if (count == 2)
-          file = token;
-      }
-      command = "rm -rf " + file;
-    }
-    Runtime runtime = Runtime.getRuntime();
-    Process proc = runtime.exec(command);
-    int exitVal = -1;
-    try {
-      exitVal = proc.waitFor();
-    } catch (InterruptedException e) {
-      System.err.println(e);
-    }
-
-  }
-  public static void main(String[] args) throws ParseException, IOException, Exception {
-    int res = ToolRunner.run(new Configuration(), new TestSmokeMR(), args);
-    System.out.println("res: "+res);
-    checkTableExists(true, outputPath);
-    if (res == 0) {
-      System.out.println("TEST PASSED!");
-    } else { 
-       System.out.println("TEST FAILED");
-       throw new IOException("Zebra MR Smoke Test Failed");
-    }   
-    
-  }
-
-  @Override
-  public int run(String[] arg0) throws Exception {
-    conf = getConf();
-    TestSmokeMR.setUpOnce();
-    removeDir(new Path(outputPath));
-    JobConf jobConf = new JobConf(conf,TestSmokeMR.class);
-
-    jobConf.setJobName("TableMRSortedTableZebraKeyGenerator");
-    jobConf.set("table.output.tfile.compression", "gz");
-    jobConf.setJarByClass(TestSmokeMR.class);
-    // input settings
-    jobConf.setInputFormat(TextInputFormat.class);
-    jobConf.setMapperClass(TestSmokeMR.MapClass.class);
-    jobConf.setReducerClass(TestSmokeMR.ReduceClass.class);
-    jobConf.setMapOutputKeyClass(BytesWritable.class);
-    jobConf.setMapOutputValueClass(ZebraTuple.class);
-    FileInputFormat.setInputPaths(jobConf, inputPath);
-    jobConf.setNumMapTasks(1);
-
-    // output settings
-   jobConf.setOutputFormat(BasicTableOutputFormat.class);
-    BasicTableOutputFormat.setOutputPath(jobConf, new Path(outputPath));
-    // set the logical schema with 2 columns
-    BasicTableOutputFormat.setSchema(jobConf, "word:string, count:int");
-    // for demo purposes, create 2 physical column groups
-    BasicTableOutputFormat.setStorageHint(jobConf, "[word];[count]");
-    
-    /* New M/R Interface */
-    /* Set sort columns in a comma separated string */
-    /* Each sort column should belong to schema columns */
-    BasicTableOutputFormat.setSortInfo(jobConf, "word, count");
-
-    // set map-only job.
-    jobConf.setNumReduceTasks(1);
-    JobClient.runJob(jobConf);
-    BasicTableOutputFormat.close(jobConf);
-    return 0;
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestTfileSplit.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestTfileSplit.java
deleted file mode 100644
index a9287aa33..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestTfileSplit.java
+++ /dev/null
@@ -1,128 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.IOException;
-import java.util.StringTokenizer;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.InputSplit;
-import org.apache.hadoop.zebra.mapred.TableInputFormat;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TestBasicTable;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.junit.AfterClass;
-import org.junit.Assert;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-public class TestTfileSplit {
-  private static Configuration conf;
-  private static Path path;
-  
-  @BeforeClass
-  public static void setUpOnce() throws IOException {
-    TestBasicTable.setUpOnce();
-    conf = TestBasicTable.conf;
-    path = new Path(TestBasicTable.rootPath, "TfileSplitTest");
-  }
-  
-  @AfterClass
-  public static void tearDown() throws IOException {
-    BasicTable.drop(path, conf);
-  }
-  
-  /* In this test, we test creating input splits for the projection on non-existing column case.
-   * The first non-deleted column group should be used for split. */ 
-  @Test
-  public void testTfileSplit1() 
-          throws IOException, ParseException {
-    BasicTable.drop(path, conf);
-    TestBasicTable.createBasicTable(1, 100, "a, b, c, d, e, f", "[a, b]; [c, d]", null, path, true);    
-
-    TableInputFormat inputFormat = new TableInputFormat();
-    JobConf jobConf = new JobConf(conf);
-    inputFormat.setInputPaths(jobConf, path);
-    inputFormat.setMinSplitSize(jobConf, 100);
-    inputFormat.setProjection(jobConf, "aa");
-    InputSplit[] splits = inputFormat.getSplits(jobConf, 40);
-    
-    RowTableSplit split = (RowTableSplit) splits[0];
-    String str = split.getSplit().toString();
-    StringTokenizer tokens = new StringTokenizer(str, "\n");
-    str = tokens.nextToken();
-    tokens = new StringTokenizer(str, " ");
-    tokens.nextToken();
-    tokens.nextToken();
-    String s = tokens.nextToken();
-    s = s.substring(0, s.length()-1);
-    int cgIndex = Integer.parseInt(s);
-    Assert.assertEquals(cgIndex, 0); 
-  }
-
-  /* In this test, we test creating input splits when dropped column groups are around.
-   * Here the projection involves all columns and only one valid column group is present.
-   * As such, that column group should be used for split.*/
-  @Test
-  public void testTfileSplit2() 
-          throws IOException, ParseException {    
-    BasicTable.drop(path, conf);
-    TestBasicTable.createBasicTable(1, 100, "a, b, c, d, e, f", "[a, b]; [c, d]", null, path, true);    
-    BasicTable.dropColumnGroup(path, conf, "CG0");
-    BasicTable.dropColumnGroup(path, conf, "CG2");
-    
-    TableInputFormat inputFormat = new TableInputFormat();
-    JobConf jobConf = new JobConf(conf);
-    inputFormat.setInputPaths(jobConf, path);
-    inputFormat.setMinSplitSize(jobConf, 100);
-    InputSplit[] splits = inputFormat.getSplits(jobConf, 40);
-    
-    RowTableSplit split = (RowTableSplit) splits[0];
-    String str = split.getSplit().toString(); 
-    StringTokenizer tokens = new StringTokenizer(str, "\n");
-    str = tokens.nextToken();
-    tokens = new StringTokenizer(str, " ");
-    tokens.nextToken();
-    tokens.nextToken();
-    String s = tokens.nextToken();
-    s = s.substring(0, s.length()-1);
-    int cgIndex = Integer.parseInt(s);
-    Assert.assertEquals(cgIndex, 1); 
-  }
-  
-  /* In this test, we test creating input splits when there is no valid column group present.
-   * Should return 0 splits. */
-  @Test
-  public void testTfileSplit3() 
-          throws IOException, ParseException {    
-    BasicTable.drop(path, conf);
-    TestBasicTable.createBasicTable(1, 100, "a, b, c, d, e, f", "[a, b]; [c, d]", null, path, true);    
-    BasicTable.dropColumnGroup(path, conf, "CG0");
-    BasicTable.dropColumnGroup(path, conf, "CG1");
-    BasicTable.dropColumnGroup(path, conf, "CG2");
-    
-    TableInputFormat inputFormat = new TableInputFormat();
-    JobConf jobConf = new JobConf(conf);
-    inputFormat.setInputPaths(jobConf, path);
-    inputFormat.setMinSplitSize(jobConf, 100);
-    InputSplit[] splits = inputFormat.getSplits(jobConf, 40);
-    
-    Assert.assertEquals(splits.length, 0);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestTypedApi.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestTypedApi.java
deleted file mode 100644
index 24f8cb446..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestTypedApi.java
+++ /dev/null
@@ -1,953 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.BufferedWriter;
-import java.io.ByteArrayOutputStream;
-import java.io.FileWriter;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.io.Serializable;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-import java.util.StringTokenizer;
-import junit.framework.Assert;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.RawComparator;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.WritableComparator;
-import org.apache.hadoop.mapred.FileInputFormat;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.Mapper;
-import org.apache.hadoop.mapred.OutputCollector;
-import org.apache.hadoop.mapred.Reducer;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapred.TextInputFormat;
-import org.apache.hadoop.mapred.lib.MultipleOutputs;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.mapred.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.types.ZebraTuple;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-import org.apache.hadoop.zebra.mapred.ZebraSchema;
-import org.apache.hadoop.zebra.mapred.ZebraSortInfo;
-import org.apache.hadoop.zebra.mapred.ZebraStorageHint;
-
-/**
- * This is a sample a complete MR sample code for Table. It doens't contain
- * 'read' part. But, it should be similar and easier to write. Refer to test
- * cases in the same directory.
- * 
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * us 2
- * japan 2
- * india 4
- * us 2
- * japan 1
- * india 3
- * nouse 5
- * nowhere 4
- * 
- * 
- */
-public class TestTypedApi extends BaseTestCase implements Tool {
-  static String inputPath;
-  static String inputFileName = "multi-input.txt";
-  public static String sortKey = null;
-
-  private static String strTable1 = null;
-  private static String strTable2 = null;
-  private static String strTable3 = null;
-
-  @BeforeClass
-  public static void setUpOnce() throws Exception {
-	  init();
-    inputPath = getTableFullPath(inputFileName).toString();
-    writeToFile(inputPath);
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws Exception {
-    pigServer.shutdown();
-    if (strTable1 != null) {
-      BasicTable.drop(new Path(strTable1), conf);
-    }
-    if (strTable1 != null) {
-      BasicTable.drop(new Path(strTable2), conf);
-    }
-    if (strTable1 != null) {
-      BasicTable.drop(new Path(strTable3 + ""), conf);
-    }
-  }
-
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    StringTokenizer st = new StringTokenizer(t, ".");
-    String methodName = null;
-    while (st.hasMoreTokens()) {
-      methodName = st.nextToken();
-    }
-    return methodName;
-  }
-
-  public static void writeToFile(String inputFile) throws IOException {
-    if( mode == TestMode.local) {
-      FileWriter fstream = new FileWriter(inputFile);
-      BufferedWriter out = new BufferedWriter(fstream);
-      out.write("us 2\n");
-      out.write("japan 2\n");
-      out.write("india 4\n");
-      out.write("us 2\n");
-      out.write("japan 1\n");
-      out.write("india 3\n");
-      out.write("nouse 5\n");
-      out.write("nowhere 4\n");
-      out.close();
-    } else {
-      FSDataOutputStream fout = fs.create(new Path(inputFile));
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 2\n");
-      fout.writeBytes("india 4\n");
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 1\n");
-      fout.writeBytes("india 3\n");
-      fout.writeBytes("nouse 5\n");
-      fout.writeBytes("nowhere 4\n");
-      fout.close();
-    }
-  }
-
-  public static void getTablePaths(String myMultiLocs) {
-    StringTokenizer st = new StringTokenizer(myMultiLocs, ",");
-
-    // get how many tokens inside st object
-    System.out.println("tokens count: " + st.countTokens());
-    int count = 0;
-
-    // iterate st object to get more tokens from it
-    while (st.hasMoreElements()) {
-      count++;
-      String token = st.nextElement().toString();
-      if (count == 1)
-        strTable1 = token;
-      if (count == 2)
-        strTable2 = token;
-      if (count == 3)
-        strTable3 = token;
-      System.out.println("token = " + token);
-    }
-  }
-
-  public static void checkTable(String myMultiLocs) throws IOException {
-    System.out.println("myMultiLocs:" + myMultiLocs);
-    System.out.println("sorgetTablePathst key:" + sortKey);
-
-    getTablePaths(myMultiLocs);
-    String query1 = null;
-    String query2 = null;
-
-    if (strTable1 != null) {
-
-      query1 = "records1 = LOAD '" + strTable1
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-    if (strTable2 != null) {
-      query2 = "records2 = LOAD '" + strTable2
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-
-    int count1 = 0;
-    int count2 = 0;
-
-    if (query1 != null) {
-      System.out.println(query1);
-      pigServer.registerQuery(query1);
-      Iterator<Tuple> it = pigServer.openIterator("records1");
-      while (it.hasNext()) {
-        count1++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-        // test 1 us table
-        if (query1.contains("test1") || query1.contains("test2")
-            || query1.contains("test3")) {
-
-          if (count1 == 1) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count1 == 2) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-        } // test1, test2
-
-      }// while
-      if (query1.contains("test1") || query1.contains("test2")
-          || query1.contains("test3")) {
-        Assert.assertEquals(2, count1);
-      }
-    }// if query1 != null
-
-    if (query2 != null) {
-      pigServer.registerQuery(query2);
-      Iterator<Tuple> it = pigServer.openIterator("records2");
-
-      while (it.hasNext()) {
-        count2++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-
-        // if test1 other table
-        if (query2.contains("test1")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-
-          if (count1 == 5) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test1
-        // if test2 other table
-        if (query2.contains("test2")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-
-          if (count1 == 5) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test2
-        // if test3 other table
-        if (query2.contains("test3")) {
-          if (count2 == 1) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 5) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-
-        }// if test3
-
-      }// while
-      if (query2.contains("test1") || query2.contains("test2")
-          || query2.contains("test3")) {
-        Assert.assertEquals(6, count2);
-      }
-    }// if query2 != null
-
-  }
-
-  @Test
-  public void test1() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test positive test case. schema, projection, sortInfo are all good ones.
-     */
-    System.out.println("******Start testcase: " + getCurrentMethodName());
-    List<Path> paths = new ArrayList<Path>(1);
-
-    sortKey = "word,count";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-    
-    paths.add( getTableFullPath( "us" + methodName ) );
-    paths.add( getTableFullPath( "others" + methodName ) );
-    myMultiLocs = paths.get(0).toString() + "," + paths.get(1).toString();
-
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    String schema = "word:string, count:int";
-    String storageHint = "[word];[count]";
-    runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-    checkTable(myMultiLocs);
-    System.out.println("DONE test " + getCurrentMethodName());
-  }
-
-  @Test(expected = ParseException.class)
-  public void test2() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test negative test case. wrong schema fomat: schema = "{, count:int";
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    List<Path> paths = new ArrayList<Path>(1);
-
-    sortKey = "word,count";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-
-    paths.add( getTableFullPath( "us" + methodName ) );
-    paths.add( getTableFullPath( "others" + methodName ) );
-    myMultiLocs = paths.get(0).toString() + "," + paths.get(1).toString();
-
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    String schema = "{, count:int";
-    String storageHint = "[word];[count]";
-
-    if( mode == TestMode.cluster) {
-      try { 
-        runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-      } catch (ParseException e) {
-        System.out.println(e.getMessage());
-        System.out.println("done test 2");
-        return;
-      }
-      // should not reach here
-      Assert.fail("in try, should have thrown exception");
-    } else {
-      runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-      System.out.println("done test 2");
-    }
-  }
-
-  @Test(expected = IOException.class)
-  public void test3() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test negative test case. non-exist sort key
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    List<Path> paths = new ArrayList<Path>(1);
-
-    sortKey = "not exist";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-
-    paths.add( getTableFullPath( "us" + methodName ) );
-    paths.add( getTableFullPath( "others" + methodName ) );
-    myMultiLocs = paths.get(0).toString() + "," + paths.get(1).toString();
-
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    String schema = "word:string, count:int";
-    String storageHint = "[word];[count]";
-    
-    if ( mode == TestMode.cluster ) {
-      try {
-        runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-      } catch (IOException e) {
-        System.out.println(e.getMessage());
-        System.out.println("done test 3");
-        return;
-      }
-      // should not reach here
-      Assert.fail("in try, should have thrown exception");
-    } else {
-      runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-      System.out.println("done test 3");
-    }
-  }
-
-  @Test(expected = IOException.class)
-  public void test4() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test negative test case. sort key is empty string
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    List<Path> paths = new ArrayList<Path>(1);
-
-    sortKey = "";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-
-    paths.add( getTableFullPath( "us" + methodName ) );
-    paths.add( getTableFullPath( "others" + methodName ) );
-    myMultiLocs = paths.get(0).toString() + "," + paths.get(1).toString();
-    
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    String schema = "word:string, count:int";
-    String storageHint = "[word];[count]";
-    
-    if ( mode == TestMode.cluster ) {
-      try {
-        runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-      } catch (IOException e) {
-        System.out.println(e.getMessage());
-        System.out.println("done test 4");
-        return;
-      }
-      // should not reach here
-      Assert.fail("in try, should have thrown exception");
-    } else {
-      runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-      System.out.println("done test 4");
-    }
-  }
-
-  @Test(expected = NullPointerException.class)
-  public void test5() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test negative test case. sort key null
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    List<Path> paths = new ArrayList<Path>(1);
-
-    sortKey = null;
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-
-    paths.add( getTableFullPath( "us" + methodName ) );
-    paths.add( getTableFullPath( "others" + methodName ) );
-    myMultiLocs = paths.get(0).toString() + "," + paths.get(1).toString();
-
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    String schema = "word:string, count:int";
-    String storageHint = "[word];[count]";
-    
-    //runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-    if ( mode == TestMode.cluster ) {
-      try {
-        runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-      } catch (NullPointerException e) {
-        System.out.println(e.getMessage());
-        System.out.println("done test 5");
-        return;
-      }
-      // should not reach here
-      Assert.fail("in try, should have thrown exception");
-    } else {
-      runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-      System.out.println("done test 5");
-    }
-
-  }
-
-  @Test(expected = ParseException.class)
-  public void test6() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test negative test case. storage hint: none exist column
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    List<Path> paths = new ArrayList<Path>(1);
-
-    sortKey = "word,count";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-
-    paths.add( getTableFullPath( "us" + methodName ) );
-    paths.add( getTableFullPath( "others" + methodName ) );
-    myMultiLocs = paths.get(0).toString() + "," + paths.get(1).toString();
-
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    String schema = "word:string, count:int";
-    String storageHint = "[none-exist-column]";
-
-    //runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-    if ( mode == TestMode.cluster ) {
-      try {
-        runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-      } catch (ParseException e) {
-        System.out.println(e.getMessage());
-        System.out.println("done test 6");
-        return;
-      }
-      // should not reach here
-      Assert.fail("in try, should have thrown exception");
-    } else {
-      runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-      System.out.println("done test 6");
-    }
-
-  }
-
-  @Test(expected = ParseException.class)
-  public void test7() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test negative test case. storage hint: wrong storage hint format, missing
-     * [
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    List<Path> paths = new ArrayList<Path>(1);
-
-    sortKey = "word,count";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-
-    paths.add( getTableFullPath( "us" + methodName ) );
-    paths.add( getTableFullPath( "others" + methodName ) );
-    myMultiLocs = paths.get(0).toString() + "," + paths.get(1).toString();
-
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    String schema = "word:string, count:int";
-    String storageHint = "none-exist-column]";
-    //runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-    if ( mode == TestMode.cluster ) {
-      try {
-        runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-      } catch (ParseException e) {
-        System.out.println(e.getMessage());
-        System.out.println("done test 7");
-        return;
-      }
-      // should not reach here
-      Assert.fail("in try, should have thrown exception");
-    } else {
-      runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-      System.out.println("done test 7");
-    }
-  }
-
-  @Test(expected = ParseException.class)
-  public void test8() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test negative test case. schema defines more columns then the input file.
-     * user input has only two fields
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    List<Path> paths = new ArrayList<Path>(1);
-
-    sortKey = "word,count";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-
-    paths.add( getTableFullPath( "us" + methodName ) );
-    paths.add( getTableFullPath( "others" + methodName ) );
-    myMultiLocs = paths.get(0).toString() + "," + paths.get(1).toString();
-
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    String schema = "word:string, count:int,word:string, count:int";
-    String storageHint = "[word];[count]";
-    //runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-    
-    if ( mode == TestMode.cluster ) {
-      try {
-        runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-      } catch (ParseException e) {
-        System.out.println(e.getMessage());
-        System.out.println("done test 8");
-        return;
-      }
-      // should not reach here
-      Assert.fail("in try, should have thrown exception");
-    } else {
-      runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-      System.out.println("done test 8");
-    }
-  }
-
-  @Test(expected = ParseException.class)
-  public void test9() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test negative test case. data type defined in schema is wrong. it is
-     * inttt instead of int
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    List<Path> paths = new ArrayList<Path>(1);
-
-    sortKey = "word,count";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-
-    paths.add( getTableFullPath( "us" + methodName ) );
-    paths.add( getTableFullPath( "others" + methodName ) );
-    myMultiLocs = paths.get(0).toString() + "," + paths.get(1).toString();
-
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    String schema = "word:string, count:inttt";
-    String storageHint = "[word];[count]";
-    //runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-    if ( mode == TestMode.cluster ) {
-      try {
-        runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-      } catch (ParseException e) {
-        System.out.println(e.getMessage());
-        System.out.println("done test 9");
-        return;
-      }
-      // should not reach here
-      Assert.fail("in try, should have thrown exception");
-    } else {
-      runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-      System.out.println("done test 9");
-    }
-  }
-
-  @Test(expected = ParseException.class)
-  public void test10() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test negative test case. schema format is wrong, schema is seperated by ;
-     * instead of ,
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    List<Path> paths = new ArrayList<Path>(1);
-
-    sortKey = "word,count";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-
-    paths.add( getTableFullPath( "us" + methodName ) );
-    paths.add( getTableFullPath( "others" + methodName ) );
-    myMultiLocs = paths.get(0).toString() + "," + paths.get(1).toString();
-
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    String schema = "word:string; count:int";
-    String storageHint = "[word];[count]";
-
-    //runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-    if ( mode == TestMode.cluster ) {
-      try {
-        runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-      } catch (ParseException e) {
-        System.out.println(e.getMessage());
-        System.out.println("done test 10");
-        return;
-      }
-      // should not reach here
-      Assert.fail("in try, should have thrown exception");
-    } else {
-      runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-      System.out.println("done test 10");
-    }
-    
-    System.out.println("done test 10");
-  }
-
-  static class MapClass implements
-      Mapper<LongWritable, Text, BytesWritable, ZebraTuple> {
-    private BytesWritable bytesKey;
-    private ZebraTuple tupleRow;
-    private Object javaObj;
-    //private JobConf conf;
-
-    @Override
-    public void map(LongWritable key, Text value,
-        OutputCollector<BytesWritable, ZebraTuple> output, Reporter reporter)
-        throws IOException {
-      // value should contain "word count"
-      String[] wdct = value.toString().split(" ");
-      if (wdct.length != 2) {
-        // LOG the error
-        return;
-      }
-
-      byte[] word = wdct[0].getBytes();
-      bytesKey.set(word, 0, word.length);
-      System.out.println("word: " + new String(word));
-      tupleRow.set(0, new String(word));
-      tupleRow.set(1, Integer.parseInt(wdct[1]));
-      System.out.println("count:  " + Integer.parseInt(wdct[1]));
-
-      // This key has to be created by user
-      /*
-       * Tuple userKey = new DefaultTuple(); userKey.append(new String(word));
-       * userKey.append(Integer.parseInt(wdct[1]));
-       */
-      System.out.println("in map, sortkey: " + sortKey);
-      ZebraTuple userKey = new ZebraTuple();
-      if (sortKey.equalsIgnoreCase("word,count")) {
-        userKey.append(new String(word));
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("count")) {
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("word")) {
-        userKey.append(new String(word));
-      }
-
-      try {
-
-        /* New M/R Interface */
-        /* Converts user key to zebra BytesWritable key */
-        /* using sort key expr tree */
-        /* Returns a java base object */
-        /* Done for each user key */
-
-        bytesKey = BasicTableOutputFormat.getSortKey(javaObj, userKey);
-      } catch (Exception e) {
-
-      }
-
-      output.collect(bytesKey, tupleRow);
-    }
-
-    @Override
-    public void configure(JobConf job) {
-      bytesKey = new BytesWritable();
-      //conf = job;
-      sortKey = job.get("sortKey");
-      try {
-        Schema outSchema = BasicTableOutputFormat.getSchema(job);
-        tupleRow = (ZebraTuple) TypesUtils.createTuple(outSchema);
-        javaObj = BasicTableOutputFormat.getSortKeyGenerator(job);
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      } catch (org.apache.hadoop.zebra.parser.ParseException e) {
-        throw new RuntimeException(e);
-      }
-    }
-
-    @Override
-    public void close() throws IOException {
-      // no-op
-    }
-  }
-
-  static class ReduceClass implements
-      Reducer<BytesWritable, Tuple, BytesWritable, Tuple> {
-    Tuple outRow;
-
-    @Override
-    public void configure(JobConf job) {
-    }
-
-    @Override
-    public void close() throws IOException {
-    }
-
-    public void reduce(BytesWritable key, Iterator<Tuple> values,
-        OutputCollector<BytesWritable, Tuple> output, Reporter reporter)
-        throws IOException {
-      try {
-        for (; values.hasNext();) {
-          output.collect(key, values.next());
-        }
-      } catch (ExecException e) {
-        e.printStackTrace();
-      }
-    }
-
-  }
-
-  static class OutputPartitionerClass extends ZebraOutputPartition {
-
-    @Override
-    public int getOutputPartition(BytesWritable key, Tuple value)
-        throws IndexOutOfBoundsException, ExecException {
-
-      System.out.println("value0 : " + value.get(0));
-      String reg = null;
-      try {
-        reg = (String) (value.get(0));
-      } catch (Exception e) {
-        //
-      }
-
-      if (reg.equals("us"))
-        return 0;
-      else
-        return 1;
-    }
-
-  }
-
-  public static final class MemcmpRawComparator implements
-      RawComparator<Object>, Serializable {
-    @Override
-    public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
-      return WritableComparator.compareBytes(b1, s1, l1, b2, s2, l2);
-    }
-
-    @Override
-    public int compare(Object o1, Object o2) {
-
-      throw new RuntimeException("Object comparison not supported");
-    }
-  }
-
-  public void runMR(String sortKey, String schema, String storageHint,
-      Path... paths) throws ParseException, IOException, Exception,
-      org.apache.hadoop.zebra.parser.ParseException {
-
-    JobConf jobConf = new JobConf(conf);
-    jobConf.setJobName("TestTypedAPI");
-    jobConf.setJarByClass(TestTypedApi.class);
-    jobConf.set("table.output.tfile.compression", "gz");
-    jobConf.set("sortKey", sortKey);
-    // input settings
-    jobConf.setInputFormat(TextInputFormat.class);
-    jobConf.setMapperClass(TestTypedApi.MapClass.class);
-    jobConf.setMapOutputKeyClass(BytesWritable.class);
-    jobConf.setMapOutputValueClass(ZebraTuple.class);
-    FileInputFormat.setInputPaths(jobConf, inputPath);
-
-    jobConf.setNumMapTasks(1);
-
-    // output settings
-    jobConf.setOutputFormat(BasicTableOutputFormat.class);
-
-    BasicTableOutputFormat.setMultipleOutputs(jobConf,
-        TestTypedApi.OutputPartitionerClass.class, paths);
-    ZebraSchema zSchema = ZebraSchema.createZebraSchema(schema);
-    ZebraStorageHint zStorageHint = ZebraStorageHint
-        .createZebraStorageHint(storageHint);
-    ZebraSortInfo zSortInfo = ZebraSortInfo.createZebraSortInfo(sortKey, null);
-    BasicTableOutputFormat.setStorageInfo(jobConf, zSchema, zStorageHint,
-        zSortInfo);
-    System.out.println("in runMR, sortkey: " + sortKey);
-
-    jobConf.setNumReduceTasks(1);
-    JobClient.runJob(jobConf);
-    BasicTableOutputFormat.close(jobConf);
-  }
-
-  @Override
-  public int run(String[] args) throws Exception {
-    TestTypedApi test = new TestTypedApi();
-    TestTypedApi.setUpOnce();
-    
-    test.test1();
-    test.test2();
-    test.test3();
-    
-    // TODO: backend exception - will migrate later
-    //test.test4();
-    
-    test.test5();
-    test.test6();
-    test.test7();
-    test.test8();
-    test.test9();
-    test.test10();
-    
-    return 0;
-  }
-
-  public static void main(String[] args) throws Exception {
-    conf = new Configuration();
-    
-    int res = ToolRunner.run(conf, new TestTypedApi(), args);
-    System.out.println("PASS");
-    System.exit(res);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestTypedApi2.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestTypedApi2.java
deleted file mode 100644
index d2b75456a..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestTypedApi2.java
+++ /dev/null
@@ -1,555 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.BufferedWriter;
-import java.io.ByteArrayOutputStream;
-import java.io.FileWriter;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.io.Serializable;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.RawComparator;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.WritableComparator;
-import org.apache.hadoop.mapred.FileInputFormat;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.Mapper;
-import org.apache.hadoop.mapred.OutputCollector;
-import org.apache.hadoop.mapred.Reducer;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapred.TextInputFormat;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.mapred.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.types.ZebraTuple;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.Tuple;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-import org.apache.hadoop.zebra.mapred.ZebraSchema;
-import org.apache.hadoop.zebra.mapred.ZebraSortInfo;
-import org.apache.hadoop.zebra.mapred.ZebraStorageHint;
-
-/**
- * This is a sample a complete MR sample code for Table. It doens't contain
- * 'read' part. But, it should be similar and easier to write. Refer to test
- * cases in the same directory.
- * 
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * us 2
- * japan 2
- * india 4
- * us 2
- * japan 1
- * india 3
- * nouse 5
- * nowhere 4
- * 
- * 
- */
-public class TestTypedApi2 extends BaseTestCase implements Tool {
-
-  static String inputPath;
-  static String inputFileName = "multi-input.txt";
-  public static String sortKey = null;
-
-  private static String strTable1 = null;
-  private static String strTable2 = null;
-  private static String strTable3 = null;
-
-  @BeforeClass
-  public static void setUpOnce() throws Exception {
-	  init();
-	  inputPath = getTableFullPath(inputFileName).toString();
-      writeToFile(inputPath);
-  }
-
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    StringTokenizer st = new StringTokenizer(t, ".");
-    String methodName = null;
-    while (st.hasMoreTokens()) {
-      methodName = st.nextToken();
-    }
-    return methodName;
-  }
-
-  public static void writeToFile(String inputFile) throws IOException {
-    if( BaseTestCase.mode == TestMode.local ) {
-      FileWriter fstream = new FileWriter(inputFile);
-      BufferedWriter out = new BufferedWriter(fstream);
-      out.write("us 2\n");
-      out.write("japan 2\n");
-      out.write("india 4\n");
-      out.write("us 2\n");
-      out.write("japan 1\n");
-      out.write("india 3\n");
-      out.write("nouse 5\n");
-      out.write("nowhere 4\n");
-      out.close();
-    } else {
-      FSDataOutputStream fout = fs.create(new Path(inputFile));
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 2\n");
-      fout.writeBytes("india 4\n");
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 1\n");
-      fout.writeBytes("india 3\n");
-      fout.writeBytes("nouse 5\n");
-      fout.writeBytes("nowhere 4\n");
-      fout.close();
-    }
-  }
-
-  public static void getTablePaths(String myMultiLocs) {
-    StringTokenizer st = new StringTokenizer(myMultiLocs, ",");
-
-    // get how many tokens inside st object
-    System.out.println("tokens count: " + st.countTokens());
-    int count = 0;
-
-    // iterate st object to get more tokens from it
-    while (st.hasMoreElements()) {
-      count++;
-      String token = st.nextElement().toString();
-      if (count == 1)
-        strTable1 = token;
-      if (count == 2)
-        strTable2 = token;
-      if (count == 3)
-        strTable3 = token;
-      System.out.println("token = " + token);
-    }
-  }
-
-  public static void checkTable(String myMultiLocs) throws IOException {
-    System.out.println("myMultiLocs:" + myMultiLocs);
-    System.out.println("sorgetTablePathst key:" + sortKey);
-
-    getTablePaths(myMultiLocs);
-    String query1 = null;
-    String query2 = null;
-
-    if (strTable1 != null) {
-
-      query1 = "records1 = LOAD '" + strTable1
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-    if (strTable2 != null) {
-      query2 = "records2 = LOAD '" + strTable2
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-
-    int count1 = 0;
-    int count2 = 0;
-
-    if (query1 != null) {
-      System.out.println(query1);
-      pigServer.registerQuery(query1);
-      Iterator<Tuple> it = pigServer.openIterator("records1");
-      while (it.hasNext()) {
-        count1++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-        // test 1 us table
-        if (query1.contains("test1") || query1.contains("test2")
-            || query1.contains("test3")) {
-
-          if (count1 == 1) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count1 == 2) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-        } // test1, test2
-
-      }// while
-      if (query1.contains("test1") || query1.contains("test2")
-          || query1.contains("test3")) {
-        Assert.assertEquals(2, count1);
-      }
-    }// if query1 != null
-
-    if (query2 != null) {
-      pigServer.registerQuery(query2);
-      Iterator<Tuple> it = pigServer.openIterator("records2");
-
-      while (it.hasNext()) {
-        count2++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-
-        // if test1 other table
-        if (query2.contains("test1")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-
-          if (count1 == 5) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test1
-        // if test2 other table
-        if (query2.contains("test2")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-
-          if (count1 == 5) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test2
-        // if test3 other table
-        if (query2.contains("test3")) {
-          if (count2 == 1) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 5) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-
-        }// if test3
-
-      }// while
-      if (query2.contains("test1") || query2.contains("test2")
-          || query2.contains("test3")) {
-        Assert.assertEquals(6, count2);
-      }
-    }// if query2 != null
-
-  }
-
-  @Test
-  public void test1() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test positive test case. User defined comparator class
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    List<Path> paths = new ArrayList<Path>(3);
-    sortKey = "word,count";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-    
-    paths.add( getTableFullPath( "us" + methodName ) );
-    paths.add( getTableFullPath( "others" + methodName ) );
-    myMultiLocs = paths.get(0).toString() + "," + paths.get(1).toString();
-
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    String schema = "word:string, count:int";
-    String storageHint = "[word];[count]";
-    //String sortInfo = null;
-    runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-    // runMR( sortKey, schema, storageHint, myMultiLocs);
-    checkTable(myMultiLocs);
-    System.out.println("DONE test " + getCurrentMethodName());
-  }
-
-  static class MapClass implements
-      Mapper<LongWritable, Text, BytesWritable, ZebraTuple> {
-    private BytesWritable bytesKey;
-    private ZebraTuple tupleRow;
-    private Object javaObj;
-    //private JobConf conf;
-    
-    @Override
-    public void map(LongWritable key, Text value,
-        OutputCollector<BytesWritable, ZebraTuple> output, Reporter reporter)
-        throws IOException {
-      // value should contain "word count"
-      String[] wdct = value.toString().split(" ");
-      if (wdct.length != 2) {
-        // LOG the error
-        return;
-      }
-
-      byte[] word = wdct[0].getBytes();
-      bytesKey.set(word, 0, word.length);
-      System.out.println("word: " + new String(word));
-      tupleRow.set(0, new String(word));
-      tupleRow.set(1, Integer.parseInt(wdct[1]));
-      System.out.println("count:  " + Integer.parseInt(wdct[1]));
-
-      // This key has to be created by user
-      /*
-       * Tuple userKey = new DefaultTuple(); userKey.append(new String(word));
-       * userKey.append(Integer.parseInt(wdct[1]));
-       */
-      System.out.println("in map, sortkey: " + sortKey);
-      ZebraTuple userKey = new ZebraTuple();
-      if (sortKey.equalsIgnoreCase("word,count")) {
-        userKey.append(new String(word));
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("count")) {
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("word")) {
-        userKey.append(new String(word));
-      }
-
-      try {
-
-        /* New M/R Interface */
-        /* Converts user key to zebra BytesWritable key */
-        /* using sort key expr tree */
-        /* Returns a java base object */
-        /* Done for each user key */
-
-        bytesKey = BasicTableOutputFormat.getSortKey(javaObj, userKey);
-      } catch (Exception e) {
-
-      }
-
-      output.collect(bytesKey, tupleRow);
-    }
-
-    @Override
-    public void configure(JobConf job) {
-      bytesKey = new BytesWritable();
-      //conf = job;
-      sortKey = job.get("sortKey");
-      try {
-        Schema outSchema = BasicTableOutputFormat.getSchema(job);
-        tupleRow = (ZebraTuple) TypesUtils.createTuple(outSchema);
-        javaObj = BasicTableOutputFormat.getSortKeyGenerator(job);
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      } catch (org.apache.hadoop.zebra.parser.ParseException e) {
-        throw new RuntimeException(e);
-      }
-    }
-
-    @Override
-    public void close() throws IOException {
-      // no-op
-    }
-  }
-
-  static class ReduceClass implements
-      Reducer<BytesWritable, Tuple, BytesWritable, Tuple> {
-    Tuple outRow;
-
-    @Override
-    public void configure(JobConf job) {
-    }
-
-    @Override
-    public void close() throws IOException {
-    }
-
-    public void reduce(BytesWritable key, Iterator<Tuple> values,
-        OutputCollector<BytesWritable, Tuple> output, Reporter reporter)
-        throws IOException {
-      try {
-        for (; values.hasNext();) {
-          output.collect(key, values.next());
-        }
-      } catch (ExecException e) {
-        e.printStackTrace();
-      }
-    }
-
-  }
-
-  static class OutputPartitionerClass extends ZebraOutputPartition {
-
-    @Override
-    public int getOutputPartition(BytesWritable key, Tuple value)
-        throws IndexOutOfBoundsException, ExecException {
-
-      System.out.println("value0 : " + value.get(0));
-      String reg = null;
-      try {
-        reg = (String) (value.get(0));
-      } catch (Exception e) {
-        //
-      }
-
-      if (reg.equals("us"))
-        return 0;
-      else
-        return 1;
-    }
-
-  }
-
-  public static final class MemcmpRawComparator implements
-      RawComparator<Object>, Serializable {
-    @Override
-    public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
-      return WritableComparator.compareBytes(b1, s1, l1, b2, s2, l2);
-    }
-
-    @Override
-    public int compare(Object o1, Object o2) {
-      throw new RuntimeException("Object comparison not supported");
-    }
-  }
-
-  public void runMR(String sortKey, String schema, String storageHint,
-      Path... paths) throws ParseException, IOException, Exception,
-      org.apache.hadoop.zebra.parser.ParseException {
-
-    JobConf jobConf = new JobConf(conf);
-    jobConf.setJobName("TestTypedApi2");
-    jobConf.setJarByClass(TestTypedApi2.class);
-    jobConf.set("table.output.tfile.compression", "gz");
-    jobConf.set("sortKey", sortKey);
-    // input settings
-    jobConf.setInputFormat(TextInputFormat.class);
-    jobConf.setMapperClass(TestTypedApi2.MapClass.class);
-    jobConf.setMapOutputKeyClass(BytesWritable.class);
-    jobConf.setMapOutputValueClass(ZebraTuple.class);
-    FileInputFormat.setInputPaths(jobConf, inputPath);
-
-    jobConf.setNumMapTasks(1);
-
-    // output settings
-    jobConf.setOutputFormat(BasicTableOutputFormat.class);
-    BasicTableOutputFormat.setMultipleOutputs(jobConf,
-        TestTypedApi2.OutputPartitionerClass.class, paths);
-
-    ZebraSchema zSchema = ZebraSchema.createZebraSchema(schema);
-    ZebraStorageHint zStorageHint = ZebraStorageHint.createZebraStorageHint(storageHint);
-    ZebraSortInfo zSortInfo = ZebraSortInfo.createZebraSortInfo(sortKey, TestTypedApi2.MemcmpRawComparator.class);
-    
-    BasicTableOutputFormat.setStorageInfo(jobConf, zSchema, zStorageHint, zSortInfo);
-    System.out.println("in runMR, sortkey: " + sortKey);
-
-    jobConf.setNumReduceTasks(1);
-    JobClient.runJob(jobConf);
-    BasicTableOutputFormat.close(jobConf);
-  }
-
-  @Override
-  public int run(String[] args) throws Exception {
-    TestTypedApi2 test = new TestTypedApi2();
-    TestTypedApi2.setUpOnce();
-    
-    //TODO: User defined comparator class has some problem when migrating to real cluster
-    //will migrate later;
-    //test.test1();
-
-    return 0;
-  }
-  
-  public static void main(String[] args) throws Exception {
-    conf = new Configuration();
-    
-    int res = ToolRunner.run(conf, new TestTypedApi2(), args);
-    System.out.println("PASS");
-    System.exit(res);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestUnsortedTableIndex.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestUnsortedTableIndex.java
deleted file mode 100644
index f92eeb331..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/TestUnsortedTableIndex.java
+++ /dev/null
@@ -1,94 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.StringTokenizer;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.mapred.InputSplit;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TestBasicTable;
-import org.apache.hadoop.zebra.mapred.RowTableSplit;
-import org.apache.hadoop.zebra.mapred.TableInputFormat;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.mapred.RecordReader;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.pig.data.Tuple;
-
-import org.junit.AfterClass;
-import org.junit.Assert;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-public class TestUnsortedTableIndex {
-	private static Configuration conf;
-	private static JobConf jobConf;
-	private static Path path1, path2;
-
-	@BeforeClass
-	public static void setUpOnce() throws IOException {
-		TestBasicTable.setUpOnce();
-		conf = TestBasicTable.conf;
-    jobConf = new JobConf(conf);
-		path1 = new Path(TestBasicTable.rootPath, "TestUnsortedTableIndex1");
-		path2 = new Path(TestBasicTable.rootPath, "TestUnsortedTableIndex2");
-	}
-
-	@AfterClass
-	public static void tearDown() throws IOException {
-		BasicTable.drop(path1, conf);
-		BasicTable.drop(path2, conf);
-	}
-
-	@Test
-	public void testUnsortedTableIndex() 
-	throws IOException, ParseException, InterruptedException {
-		BasicTable.drop(path1, conf);
-		BasicTable.drop(path2, conf);
-		int total1 = TestBasicTable.createBasicTable(1, 100, "a, b, c, d, e, f", "[a, b]; [c, d]", null, path1, true);    
-		int total2 = TestBasicTable.createBasicTable(1, 100, "a, b, c, d, e, f", "[a, b]; [c, d]", null, path2, true);    
-
-		TableInputFormat inputFormat = new TableInputFormat();
-		TableInputFormat.setInputPaths(jobConf, path1, path2);
-		TableInputFormat.setProjection(jobConf, "source_table");
-		InputSplit[] splits = inputFormat.getSplits(jobConf, -1);
-    Assert.assertEquals(splits.length, 2);
-    for (int i = 0; i < 2; i++)
-    {
-	    int count = 0;
-	  	RowTableSplit split = (RowTableSplit) splits[i];
-	    TableRecordReader rr = (TableRecordReader) inputFormat.getRecordReader(split, jobConf, null);
-	    Tuple t = TypesUtils.createTuple(1);
-      BytesWritable key = new BytesWritable();
-	    while (rr.next(key, t)) {
-        int idx= (Integer) t.get(0);
-        Assert.assertEquals(idx, i);
-	      count++;
-	    }
-	    rr.close();
-      if (i == 0)
-  	    Assert.assertEquals(count, total1);
-      else
-  	    Assert.assertEquals(count, total2);
-    }
-	}
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/ToolTestComparator.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/ToolTestComparator.java
deleted file mode 100644
index f78e7353d..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/ToolTestComparator.java
+++ /dev/null
@@ -1,965 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-package org.apache.hadoop.zebra.mapred;
-
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.io.PrintStream;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.ArrayList;
-import java.util.Map;
-import java.util.Random;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.pig.TableStorer;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.executionengine.ExecJob;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.Assert;
-
-/**
- * TestComparator
- * 
- * Utility for verifying tables created during Zebra Stress Testing
- * 
- */
-public class ToolTestComparator extends BaseTestCase {
-
-  final static String TABLE_SCHEMA = "count:int,seed:int,int1:int,int2:int,str1:string,str2:string,byte1:bytes,"
-      + "byte2:bytes,float1:float,long1:long,double1:double,m1:map(string),r1:record(f1:string, f2:string),"
-      + "c1:collection(record(a:string, b:string))";
-  final static String TABLE_STORAGE = "[count,seed,int1,int2,str1,str2,byte1,byte2,float1,long1,double1];[m1#{a}];[r1,c1]";
-
-  private static Random generator = new Random();
-
-  protected static ExecJob pigJob;
-
-  private static int totalNumbCols;
-  private static long totalNumbVerifiedRows;
-
-  /**
-   * Setup and initialize environment
-   */
-  public static void setUp() throws Exception {
-	  init();
-  }
-
-  /**
-   * Verify load/store
-   * 
-   */
-  public static void verifyLoad(String pathTable1, String pathTable2,
-      int numbCols) throws IOException {
-    System.out.println("verifyLoad()");
-
-    // Load table1
-    String query1 = "table1 = LOAD '" + pathTable1
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    System.out.println("verifyLoad() running query : " + query1);
-    pigServer.registerQuery(query1);
-
-    // Load table2
-    String query2 = "table2 = LOAD '" + pathTable2
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    System.out.println("verifyLoad() running query : " + query2);
-    pigServer.registerQuery(query2);
-
-    // Get metrics from first table
-    Iterator<Tuple> it1 = pigServer.openIterator("table1");
-
-    int numbCols1 = 0;
-    long numbRows1 = 0;
-
-    while (it1.hasNext()) {
-      ++numbRows1; // increment row count
-      Tuple rowValue = it1.next();
-      numbCols1 = rowValue.size();
-      if (numbCols != 0)
-        Assert.assertEquals(
-            "Verify failed - Table1 has wrong number of expected columns "
-                + "\n row number : " + numbRows1 + "\n expected column size : "
-                + numbCols + "\n actual columns size  : " + numbCols1,
-            numbCols, numbCols1);
-    }
-
-    // Get metrics from second table
-    Iterator<Tuple> it2 = pigServer.openIterator("table2");
-
-    int numbCols2 = 0;
-    long numbRows2 = 0;
-
-    while (it2.hasNext()) {
-      ++numbRows2; // increment row count
-      Tuple rowValue = it2.next();
-      numbCols2 = rowValue.size();
-      if (numbCols != 0)
-        Assert.assertEquals(
-            "Verify failed - Table2 has wrong number of expected columns "
-                + "\n row number : " + numbRows2 + "\n expected column size : "
-                + numbCols + "\n actual columns size  : " + numbCols2,
-            numbCols, numbCols2);
-    }
-
-    Assert
-        .assertEquals(
-            "Verify failed - Tables have different number row sizes "
-                + "\n table1 rows : " + numbRows1 + "\n table2 rows : "
-                + numbRows2, numbRows1, numbRows2);
-
-    Assert.assertEquals(
-        "Verify failed - Tables have different number column sizes "
-            + "\n table1 column size : " + numbCols1
-            + "\n table2 column size : " + numbCols2, numbCols1, numbCols2);
-
-    System.out.println();
-    System.out.println("Verify load - table1 columns : " + numbCols1);
-    System.out.println("Verify load - table2 columns : " + numbCols2);
-    System.out.println("Verify load - table1 rows : " + numbRows1);
-    System.out.println("Verify load - table2 rows : " + numbRows2);
-    System.out.println("Verify load - PASS");
-  }
-
-  /**
-   * Verify table
-   * 
-   */
-  public static void verifyTable(String pathTable1) throws IOException {
-    System.out.println("verifyTable()");
-
-    // Load table1
-    String query1 = "table1 = LOAD '" + pathTable1
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    System.out.println("verifyTable() running query : " + query1);
-    pigServer.registerQuery(query1);
-
-    // Get metrics from table
-    Iterator<Tuple> it1 = pigServer.openIterator("table1");
-
-    int numbCols1 = 0;
-    long numbRows1 = 0;
-
-    System.out.println("DEBUG starting to iterate table1");
-
-    while (it1.hasNext()) {
-      ++numbRows1; // increment row count
-      Tuple rowValue = it1.next();
-      numbCols1 = rowValue.size();
-    }
-
-    System.out.println();
-    System.out.println("Verify table columns : " + numbCols1);
-    System.out.println("Verify table rows : " + numbRows1);
-    System.out.println("Verify table complete");
-  }
-
-  /**
-   * Verify sorted
-   * 
-   */
-  public static void verifySorted(String pathTable1, String pathTable2,
-      int sortCol, String sortKey, int numbCols, int rowMod)
-      throws IOException, ParseException {
-    System.out.println("verifySorted()");
-
-    // Load table1
-    String query1 = "table1 = LOAD '" + pathTable1
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    System.out.println("verifySorted() running query : " + query1);
-    pigServer.registerQuery(query1);
-
-    //
-    // Get metrics from first table (unsorted)
-    //
-    Iterator<Tuple> it1 = pigServer.openIterator("table1");
-
-    int numbCols1 = 0;
-    long numbRows1 = 0;
-
-    System.out.println("DEBUG starting to iterate table1");
-
-    while (it1.hasNext()) {
-      ++numbRows1; // increment row count
-      Tuple rowValue = it1.next();
-      numbCols1 = rowValue.size();
-      if (numbCols != 0)
-        Assert.assertEquals(
-            "Verify failed - Table1 has wrong number of expected columns "
-                + "\n row number : " + numbRows1 + "\n expected column size : "
-                + numbCols + "\n actual columns size  : " + numbCols1,
-            numbCols, numbCols1);
-    }
-
-    System.out.println();
-    System.out.println("Verify unsorted table1 columns : " + numbCols1);
-    System.out.println("Verify unsorted table1 rows : " + numbRows1);
-
-    System.out.println("\nDEBUG starting to iterate table2");
-
-    //
-    // Get metrics from second table (sorted)
-    //
-    long numbRows2 = verifySortedTable(pathTable2, sortCol, sortKey, numbCols,
-        rowMod, null);
-
-    int numbCols2 = totalNumbCols;
-    long numbVerifiedRows = totalNumbVerifiedRows;
-
-    Assert
-        .assertEquals(
-            "Verify failed - Tables have different number row sizes "
-                + "\n table1 rows : " + numbRows1 + "\n table2 rows : "
-                + numbRows2, numbRows1, numbRows2);
-
-    Assert.assertEquals(
-        "Verify failed - Tables have different number column sizes "
-            + "\n table1 column size : " + numbCols1
-            + "\n table2 column size : " + numbCols2, numbCols1, numbCols2);
-
-    System.out.println();
-    System.out.println("Verify unsorted table1 columns : " + numbCols1);
-    System.out.println("Verify sorted   table2 columns : " + numbCols2);
-    System.out.println("Verify unsorted table1 rows : " + numbRows1);
-    System.out.println("Verify sorted   table2 rows : " + numbRows2);
-    System.out.println("Verify sorted - numb verified rows : "
-        + numbVerifiedRows);
-    System.out.println("Verify sorted - sortCol : " + sortCol);
-    System.out.println("Verify sorted - PASS");
-  }
-
-  /**
-   * Verify merge-join
-   * 
-   */
-  public static void verifyMergeJoin(String pathTable1, int sortCol,
-      String sortKey, int numbCols, int rowMod, String verifyDataColName) throws IOException,
-      ParseException {
-    System.out.println("verifyMergeJoin()");
-
-    //
-    // Verify sorted table
-    //
-    long numbRows = verifySortedTable(pathTable1, sortCol, sortKey, numbCols,
-        rowMod, verifyDataColName);
-
-    System.out.println();
-    System.out.println("Verify merge-join   table columns : " + totalNumbCols);
-    System.out.println("Verify merge-join   table rows : " + numbRows);
-    System.out.println("Verify merge-join - numb verified rows : "
-        + totalNumbVerifiedRows);
-    System.out.println("Verify merge-join - sortCol : " + sortCol);
-    System.out.println("Verify merge-join - PASS");
-  }
-
-  /**
-   * Verify sorted-union
-   * 
-   */
-  public static void verifySortedUnion(ArrayList<String> unionPaths,
-      String pathTable1, int sortCol, String sortKey, int numbCols, int rowMod,
-      String verifyDataColName) throws IOException, ParseException {
-    System.out.println("verifySortedUnion()");
-
-    long numbUnionRows = 0;
-    ArrayList<Long> numbRows = new ArrayList<Long>();
-
-    // Get number of rows from each of the input union tables
-   for (int i = 0; i < unionPaths.size(); ++i) {
-      // Load table1
-      String query1 = "table1 = LOAD '" + unionPaths.get(i)
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-      System.out.println("verifySortedUnion() running query : " + query1);
-      pigServer.registerQuery(query1);
-      String orderby1 = "sort1 = ORDER table1 BY " + sortKey + " ;";
-      System.out.println("orderby1 : " + orderby1);
-      pigServer.registerQuery(orderby1);
-
-      // Get metrics for each input sorted table
-      Iterator<Tuple> it1 = pigServer.openIterator("sort1");
-      long numbRows1 = 0;
-
-      while (it1.hasNext()) {
-        ++numbRows1; // increment row count
-        Tuple rowValue = it1.next();
-      }
-      numbRows.add(numbRows1);
-      numbUnionRows += numbRows1;
-    }
-
-    //
-    // Verify sorted union table
-    //
-    long numbRows1 = verifySortedTable(pathTable1, sortCol, sortKey, numbCols,
-        rowMod, verifyDataColName);
-
-   
-    //
-    // Print all union input tables and rows for each
-    //
-    System.out.println();
-    for (int i = 0; i < unionPaths.size(); ++i) {
-      System.out.println("Input union table" + i + " path  : "
-          + unionPaths.get(i));
-      System.out.println("Input union table" + i + " rows  : "
-          + numbRows.get(i));
-    }
-    System.out.println();
-    System.out.println("Input union total rows   : " + numbUnionRows);
-
-    System.out.println();
-    System.out.println("Verify union - table columns : " + totalNumbCols);
-    System.out.println("Verify union - table rows : " + numbRows1);
-    System.out.println("Verify union - numb verified rows : "
-        + totalNumbVerifiedRows);
-    System.out.println("Verify union - sortCol : " + sortCol);
-
-  /*  Assert.assertEquals(
-        "Verify failed - sorted union table row comparison error "
-            + "\n expected table rows : " + numbUnionRows
-            + "\n actual table rows : " + numbRows1, numbUnionRows, numbRows1);
-*/
-    System.out.println("Verify union - PASS");
-  }
-
-  /**
-   * Create unsorted table
-   * 
-   */
-  public static void createtable(String pathTable1, long numbRows, int seed,
-      boolean debug) throws ExecException, IOException, ParseException {
-    System.out.println("createtable()");
-
-    Path unsortedPath = new Path(pathTable1);
-
-    // Remove old table (if present)
-    removeDir(unsortedPath);
-
-    // Create table
-    BasicTable.Writer writer = new BasicTable.Writer(unsortedPath,
-        TABLE_SCHEMA, TABLE_STORAGE, conf);
-
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-    TableInserter inserter = writer.getInserter("ins", false);
-
-    Map<String, String> m1 = new HashMap<String, String>();
-
-    Tuple tupRecord1; // record
-    tupRecord1 = TypesUtils.createTuple(schema.getColumnSchema("r1")
-        .getSchema()); // r1 schema
-
-    DataBag bag1 = TypesUtils.createBag();
-    Schema schColl = schema.getColumnSchema("c1").getSchema(); // c1 schema
-    Tuple tupColl1 = TypesUtils.createTuple(schColl);
-    Tuple tupColl2 = TypesUtils.createTuple(schColl);
-
-    int randRange = new Long(numbRows / 10).intValue(); // random range to allow
-    // for duplicate values
-    for (int i = 0; i < numbRows; ++i) {
-      int random = generator.nextInt(randRange);
-
-      TypesUtils.resetTuple(tuple); // reset row tuple
-      m1.clear(); // reset map
-      TypesUtils.resetTuple(tupRecord1); // reset record
-      TypesUtils.resetTuple(tupColl1); // reset collection
-      TypesUtils.resetTuple(tupColl2);
-      bag1.clear();
-
-      tuple.set(0, i); // count
-      tuple.set(1, seed); // seed
-
-      tuple.set(2, i); // int1
-      tuple.set(3, random); // int2
-      tuple.set(4, "string " + i); // str1
-      tuple.set(5, "string random " + random); // str2
-      tuple.set(6, new DataByteArray("byte " + i)); // byte1
-      tuple.set(7, new DataByteArray("byte random " + random)); // byte2
-
-      tuple.set(8, new Float(i * -1)); // float1 negative
-      tuple.set(9, new Long(numbRows - i)); // long1 reverse
-      tuple.set(10, new Double(i * 100)); // double1
-
-      // insert map1
-      m1.put("a", "m1");
-      m1.put("b", "m1 " + i);
-      tuple.set(11, m1);
-
-      // insert record1
-      tupRecord1.set(0, "r1 " + seed);
-      tupRecord1.set(1, "r1 " + i);
-      tuple.set(12, tupRecord1);
-
-      // insert collection1
-      // tupColl1.set(0, "c1 a " + seed);
-      // tupColl1.set(1, "c1 a " + i);
-      // bag1.add(tupColl1); // first collection item
-      bag1.add(tupRecord1); // first collection item
-      bag1.add(tupRecord1); // second collection item
-
-      // tupColl2.set(0, "c1 b " + seed);
-      // tupColl2.set(1, "c1 b " + i);
-      // bag1.add(tupColl2); // second collection item
-
-      tuple.set(13, bag1);
-
-      inserter.insert(new BytesWritable(("key" + i).getBytes()), tuple);
-    }
-    inserter.close();
-    writer.close();
-
-    if (debug == true) {
-      // Load tables
-      String query1 = "table1 = LOAD '" + unsortedPath.toString()
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-      pigServer.registerQuery(query1);
-
-      // Print Table
-      printTable("table1");
-    }
-
-    System.out.println("Table Path : " + unsortedPath);
-  }
-
-  /**
-   * Create sorted table
-   * 
-   */
-  public static void createsortedtable(String pathTable1, String pathTable2,
-      String sortString, boolean debug) throws ExecException, IOException {
-    System.out.println("createsortedtable()");
-
-    Path unsortedPath = new Path(pathTable1);
-    Path sortedPath = new Path(pathTable2);
-
-    // Remove old table (if present)
-    removeDir(sortedPath);
-
-    // Load tables
-    String query1 = "table1 = LOAD '" + unsortedPath.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    pigServer.registerQuery(query1);
-
-    // Sort table
-    String orderby1 = "sort1 = ORDER table1 BY " + sortString + " ;";
-    System.out.println("orderby1 : " + orderby1);
-    pigServer.registerQuery(orderby1);
-
-    // Store sorted tables
-    pigJob = pigServer.store("sort1", sortedPath.toString(), TableStorer.class
-        .getCanonicalName()
-        + "('" + TABLE_STORAGE + "')");
-    Assert.assertNull(pigJob.getException());
-
-    // Print Table
-    if (debug == true)
-      printTable("sort1");
-
-    System.out.println("Sorted Path : " + sortedPath);
-  }
-
-  /**
-   * Delete table
-   * 
-   */
-  public static void deleteTable(String pathTable1) throws ExecException,
-      IOException {
-    System.out.println("deleteTable()");
-
-    Path tablePath = new Path(pathTable1);
-
-    // Remove table (if present)
-    removeDir(tablePath);
-
-    System.out.println("Deleted Table Path : " + tablePath);
-  }
-
-  /**
-   * Verify sorted table
-   * 
-   * Using BasicTable.Reader, read all table rows and verify that sortCol is in
-   * sorted order
-   * 
-   */
-  private static long verifySortedTable(String pathTable1, int sortCol,
-      String sortKey, int numbCols, int rowMod, String verifyDataColName)
-      throws IOException, ParseException {
-
-    long numbRows = 0;
-
-    Path tablePath = new Path(pathTable1);
-
-    BasicTable.Reader reader = new BasicTable.Reader(tablePath, conf);
-   
-    JobConf conf1 = new JobConf(conf);
-    System.out.println("sortKey: " + sortKey);
-    TableInputFormat.setInputPaths(conf1, new Path(pathTable1));
- 
-    TableInputFormat.requireSortedTable(conf1, null);
-    TableInputFormat tif = new TableInputFormat();
- 
-    SortedTableSplit split = (SortedTableSplit) tif.getSplits(conf1, 1)[0];
-    
-    TableScanner scanner = reader.getScanner(split.getBegin(), split.getEnd(), true);
-    BytesWritable key = new BytesWritable();
-    Tuple rowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    Object lastVal = null;
-    int numbCols1 = 0;
-    long numbVerifiedRows = 0;
-
-    while (!scanner.atEnd()) {
-      ++numbRows;
-      scanner.getKey(key);
-
-      scanner.getValue(rowValue);
-
-      // Verify every nth row
-      if ((numbRows % rowMod) == 0) {
-        ++numbVerifiedRows;
-        numbCols1 = rowValue.size();
-        if (numbCols != 0)
-          Assert.assertEquals(
-              "Verify failed - Table1 has wrong number of expected columns "
-                  + "\n row numberrr : " + numbRows
-                  + "\n expected column size : " + numbCols
-                  + "\n actual columns size  : " + numbCols1, numbCols,
-              numbCols1);
-
-        Object newVal = rowValue.get(sortCol);
-
-        // Verify sort key is in sorted order
-        Assert.assertTrue("Verify failed - Table1 sort comparison error "
-            + "\n row number : " + numbRows + "\n sort column : " + sortCol
-            + "\n sort column last value    : " + lastVal
-            + "\n sort column current value : " + newVal, compareTo(newVal,
-            lastVal) >= 0);
-
-        lastVal = newVal; // save last compare value
-
-        //
-        // Optionally verify data
-        //
-       
-        if (verifyDataColName != null && verifyDataColName.equals("long1")) {
-          Object newValLong1 = rowValue.get(sortCol);
-          if (numbRows < 2000){
-            System.out.println("Row : "+ (numbRows-1) +" long1 value : "+newValLong1.toString());
-          }
-          Assert.assertEquals(
-              "Verify failed - Union table data verification error for column name : "
-                  + verifyDataColName + "\n row number : " + (numbRows-1)
-                  + "\n expected value : " + (numbRows-1 + 4) / 4 + // long1 will start with value 1
-                  "\n actual value   : " + newValLong1, (numbRows-1 + 4) / 4,
-              newValLong1);
-
-        }
-
-        scanner.advance();
-      }
-      
-
-    }
-    
-    System.out.println("\nTable Pathh : " + pathTable1);
-    System.out.println("++++++++++Table Row number : " + numbRows);
-    
-   
-    reader.close();
-   
-    totalNumbCols = numbCols1;
-    totalNumbVerifiedRows = numbVerifiedRows;
-
-    return numbRows;
-  }
-
-  /**
-   * Print table rows
-   * 
-   * Print the first number of specified table rows
-   * 
-   */
-  public static void printRows(String pathTable1, long numbRows)
-      throws IOException {
-    System.out.println("printRows()");
-
-    // Load table1
-    String query1 = "table1 = LOAD '" + pathTable1
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    pigServer.registerQuery(query1);
-    
-   //
-    // Get metrics from first table (unsorted)
-    //
-    long count = 0;
-    Iterator<Tuple> it1 = pigServer.openIterator("table1");
-
-    while (it1.hasNext()) {
-      ++count;
-      if (count > numbRows)
-        break;
-      Tuple RowValue1 = it1.next();
-      System.out.println();
-      for (int i = 0; i < RowValue1.size(); ++i)
-        System.out.println("DEBUG: " + "table" + " RowValue.get(" + i + ") = "
-            + RowValue1.get(i));
-    }
-    System.out.println("\nTable Path : " + pathTable1);
-    System.out.println("Table Rows Printed : " + numbRows);
-  }
-  
-  /* 
-  * Print the first number of specified table rows
-  * 
-  */
- public static void printRowNumber(String pathTable1, String sortKey)
-      throws IOException, ParseException {
-    long numbRows = 0;
-
-    Path tablePath = new Path(pathTable1);
-
-    BasicTable.Reader reader = new BasicTable.Reader(tablePath, conf);
-   
-    JobConf conf1 = new JobConf(conf);
-    System.out.println("sortKey: " + sortKey);
-    TableInputFormat.setInputPaths(conf1, new Path(pathTable1));
-
-    TableInputFormat.requireSortedTable(conf1, null);
-    TableInputFormat tif = new TableInputFormat();
-
-  
-    TableScanner scanner = reader.getScanner(null, null, true);
-    BytesWritable key = new BytesWritable();
-    Tuple rowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    while (!scanner.atEnd()) {
-      ++numbRows;
-      scanner.getKey(key);
-      scanner.advance();
-    }
-    System.out.println("\nTable Path : " + pathTable1);
-    System.out.println("Table Row number : " + numbRows);
-  }
-  /**
-   * Compare table rows
-   * 
-   */
-  private static boolean compareRow(Tuple rowValues1, Tuple rowValues2)
-      throws IOException {
-    boolean result = true;
-    Assert.assertEquals(rowValues1.size(), rowValues2.size());
-    for (int i = 0; i < rowValues1.size(); ++i) {
-      if (!compareObj(rowValues1.get(i), rowValues2.get(i))) {
-        System.out.println("DEBUG: " + " RowValue.get(" + i
-            + ") value compare error : " + rowValues1.get(i) + " : "
-            + rowValues2.get(i));
-        result = false;
-        break;
-      }
-    }
-    return result;
-  }
-
-  /**
-   * Compare table values
-   * 
-   */
-  private static boolean compareObj(Object object1, Object object2) {
-    if (object1 == null) {
-      if (object2 == null)
-        return true;
-      else
-        return false;
-    } else if (object1.equals(object2))
-      return true;
-    else
-      return false;
-  }
-
-  /**
-   * Compares two objects that implement the Comparable interface
-   * 
-   * Zebra supported "sort" types of String, DataByteArray, Integer, Float,
-   * Long, Double, and Boolean all implement the Comparable interface.
-   * 
-   * Returns a negative integer, zero, or a positive integer if object1 is less
-   * than, equal to, or greater than object2.
-   * 
-   */
-  private static int compareTo(Object object1, Object object2) {
-    if (object1 == null) {
-      if (object2 == null)
-        return 0;
-      else
-        return -1;
-    } else if (object2 == null) {
-      return 1;
-    } else
-      return ((Comparable) object1).compareTo((Comparable) object2);
-  }
-
-  /**
-   * Print Table Metadata Info (for debugging)
-   * 
-   */
-  private static void printTableInfo(String pathString) throws IOException {
-    ByteArrayOutputStream bos = new ByteArrayOutputStream();
-    PrintStream ps = new PrintStream(bos);
-    System.out.println("start dumpinfo ===========");
-    BasicTable.dumpInfo(pathString, ps, conf);
-
-    System.out.println("bos.toString() : " + bos.toString());
-  }
-
-  /**
-   * Print Pig Table (for debugging)
-   * 
-   */
-  private static int printTable(String tablename) throws IOException {
-    Iterator<Tuple> it1 = pigServer.openIterator(tablename);
-    int numbRows = 0;
-    while (it1.hasNext()) {
-      Tuple RowValue1 = it1.next();
-      ++numbRows;
-      System.out.println();
-      for (int i = 0; i < RowValue1.size(); ++i)
-        System.out.println("DEBUG: " + tablename + " RowValue.get(" + i
-            + ") = " + RowValue1.get(i));
-    }
-    System.out.println("\nRow count : " + numbRows);
-    return numbRows;
-  }
-
-  /**
-   * Calculate elapsed time
-   * 
-   */
-  private static String printTime(long start, long stop) {
-    long timeMillis = stop - start;
-    long time = timeMillis / 1000;
-    String seconds = Integer.toString((int) (time % 60));
-    String minutes = Integer.toString((int) ((time % 3600) / 60));
-    String hours = Integer.toString((int) (time / 3600));
-
-    for (int i = 0; i < 2; i++) {
-      if (seconds.length() < 2) {
-        seconds = "0" + seconds;
-      }
-      if (minutes.length() < 2) {
-        minutes = "0" + minutes;
-      }
-      if (hours.length() < 2) {
-        hours = "0" + hours;
-      }
-    }
-    String formatTime = hours + ":" + minutes + ":" + seconds;
-    return formatTime;
-  }
-
-  /**
-   * Main
-   * 
-   * Command line options:
-   * 
-   * -verifyOption : <load, sort, merge-join, sorted-union, dump, tableinfo,
-   * createtable, createsorttable, deletetable, printrows>
-   * 
-   * -pathTable1 : <hdfs path> -pathTable2 : <hdfs path>
-   * 
-   * -pathUnionTables : <hdfs path> <hdfs path> ...
-   * 
-   * -rowMod : verify every nth row (optional)
-   * 
-   * -numbCols : number of columns table should have (optional)
-   * 
-   * -sortCol : for sort option (default is column 0)
-   * 
-   * -sortString : sort string for sort option
-   * 
-   * -numbRows : number of rows for new table to create
-   * 
-   * -seed : unique column number used for creating new tables
-   * 
-   * -debug : print out debug info with results (use caution, for example do not
-   * used when creating large tables)
-   * 
-   * examples:
-   * 
-   * java -DwhichCluster="realCluster" -DHADOOP_HOME=$HADOOP_HOME -DUSER=$USER
-   * TestComparator -verifyOption load -pathTable1 /user/hadoopqa/table1
-   * -pathTable2 /user/hadoopqa/table2
-   * 
-   * java -DwhichCluster="realCluster" -DHADOOP_HOME=$HADOOP_HOME -DUSER=$USER
-   * TestComparator -verifyOption sort -pathTable1 /user/hadoopqa/table1
-   * -pathTable2 /user/hadoopqa/table2 -sortCol 0
-   * 
-   * java -DwhichCluster="realCluster" -DHADOOP_HOME=$HADOOP_HOME -DUSER=$USER
-   * TestComparator -verifyOption merge-join -pathTable1 /user/hadoopqa/table1
-   * -sortCol 0
-   * 
-   * java -DwhichCluster="realCluster" -DHADOOP_HOME=$HADOOP_HOME -DUSER=$USER
-   * TestComparator -verifyOption sorted-union -pathTable1
-   * /user/hadoopqa/unionTable1 -pathUnionTables /user/hadoopqa/inputTable1
-   * /user/hadoopqa/inputTable2 /user/hadoopqa/inputTable3 -sortCol 0 -rowMod 5
-   * 
-   * java -DwhichCluster="realCluster" -DHADOOP_HOME=$HADOOP_HOME -DUSER=$USER
-   * TestComparator -verifyOption dump -pathTable1 /user/hadoopqa/table1
-   * 
-   * @param args
-   */
-
-  public static void main(String[] args) {
-    long startTime = System.currentTimeMillis();
-
-    System.out.println("Running Zebra TestComparator");
-    try {
-      ArrayList<String> unionPaths = new ArrayList<String>();
-      String verifyOption = null;
-      String pathTable1 = null;
-      String pathTable2 = null;
-      String sortString = null;
-      String verifyDataColName = null;
-      int rowMod = 1; // default to verify every table row
-      int numbCols = 0; // if provided, verify that table has these number of
-      // columns
-      int sortCol = 0; // default to first column as sort index
-      long numbRows = 0; // number of rows to create for new table
-      int seed = 0; // used for creating new tabletable1
-      boolean debug = false;
-
-      // Read arguments
-      if (args.length >= 2) {
-        for (int i = 0; i < args.length; ++i) {
-
-          if (args[i].equals("-verifyOption")) {
-            verifyOption = args[++i];
-          } else if (args[i].equals("-pathTable1")) {
-            pathTable1 = args[++i];
-          } else if (args[i].equals("-pathTable2")) {
-            pathTable2 = args[++i];
-          } else if (args[i].equals("-pathUnionTables")) {
-            while (++i < args.length && !args[i].startsWith("-")) {
-              System.out.println("args[i] : " + args[i]);
-              unionPaths.add(args[i]);
-            }
-            if (i < args.length)
-              --i;
-          } else if (args[i].equals("-rowMod")) {
-            rowMod = new Integer(args[++i]).intValue();
-          } else if (args[i].equals("-sortString")) {
-            sortString = args[++i];
-          } else if (args[i].equals("-sortCol")) {
-            sortCol = new Integer(args[++i]).intValue();
-          } else if (args[i].equals("-numbCols")) {
-            numbCols = new Integer(args[++i]).intValue();
-          } else if (args[i].equals("-numbRows")) {
-            numbRows = new Long(args[++i]).intValue();
-          } else if (args[i].equals("-seed")) {
-            seed = new Integer(args[++i]).intValue();
-          } else if (args[i].equals("-verifyDataColName")) {
-            verifyDataColName = args[++i];
-          } else if (args[i].equals("-debug")) {
-            debug = true;
-          } else {
-            System.out.println("Exiting - unknown argument : " + args[i]);
-            System.exit(0);
-          }
-        }
-      } else {
-        System.out
-            .println("Error - need to provide required comparator arguments");
-        System.exit(0);
-      }
-
-      // Setup environment
-      setUp();
-
-      //
-      // Run appropriate verify option
-      //
-      if (verifyOption == null) {
-        System.out.println("Exiting -verifyOption not set");
-        System.exit(0);
-      }
-
-      if (verifyOption.equals("load")) {
-        // Verify both tables are equal
-        verifyLoad(pathTable1, pathTable2, numbCols);
-      } else if (verifyOption.equals("sort")) {
-        // Verify table is in sorted order
-        verifySorted(pathTable1, pathTable2, sortCol, sortString, numbCols,
-            rowMod);
-      } else if (verifyOption.equals("merge-join")) {
-        // Verify merge-join table is in sorted order
-        verifyMergeJoin(pathTable1, sortCol, sortString, numbCols, rowMod,verifyDataColName);
-      } else if (verifyOption.equals("sorted-union")) {
-        Object lastVal = null;
-        
-        // Verify sorted-union table is in sorted order
-        verifySortedUnion(unionPaths, pathTable1, sortCol, sortString,
-            numbCols, rowMod, verifyDataColName);
-      } else if (verifyOption.equals("dump")) {
-        // Dump table info
-        printTableInfo(pathTable1);
-      } else if (verifyOption.equals("tableinfo")) {
-        // Verify table to get row and column info
-        verifyTable(pathTable1);
-      } else if (verifyOption.equals("deletetable")) {
-        // Delete table directory
-        deleteTable(pathTable1);
-      } else if (verifyOption.equals("printrows")) {
-        // Print some table rows
-        printRows(pathTable1, numbRows);
-      } else if (verifyOption.equals("createtable")) {
-        // Create unsorted table
-        createtable(pathTable1, numbRows, seed, debug);
-      } else if (verifyOption.equals("createsorttable")) {
-        // Create sorted table
-        createsortedtable(pathTable1, pathTable2, sortString, debug);
-      }else if (verifyOption.equals("printrownumber")) {
-        Object lastVal = null;
-        //print total number of rows of the table
-        printRowNumber(pathTable1,sortString);
-      }
-      //
-      else {
-        System.out.println("Exiting - unknown -verifyOption value : "
-            + verifyOption);
-        System.exit(0);
-      }
-
-    } catch (Exception e) {
-      e.printStackTrace();
-    }
-
-    long stopTime = System.currentTimeMillis();
-    System.out.println("\nElapsed time : " + printTime(startTime, stopTime)
-        + "\n");
-  }
-
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/ArticleGenerator.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/ArticleGenerator.java
deleted file mode 100644
index 1ad2e8dee..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/ArticleGenerator.java
+++ /dev/null
@@ -1,152 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-import java.util.Random;
-
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.zebra.tfile.RandomDistribution.DiscreteRNG;
-import org.apache.hadoop.zebra.tfile.RandomDistribution.Flat;
-
-/**
- * Generate some input text files.
- */
-class ArticleGenerator {
-	Random random;
-	Dictionary dict;
-	int pageWidth;
-	DiscreteRNG lastLineLenGen;
-	DiscreteRNG paragraphLineLenGen;
-	DiscreteRNG paragraphLenGen;
-	long wordCount;
-	long lineCount;
-
-	/**
-	 * Create an article generator.
-	 * 
-	 * @param dictWordCnt
-	 *          Number of words in the dictionary.
-	 * @param minWordLen
-	 *          Minimum word length
-	 * @param maxWordLen
-	 *          Maximum word length
-	 * @param lineWidth
-	 *          Line width.
-	 */
-	ArticleGenerator(int dictWordCnt, int minWordLen, int maxWordLen,
-			int pageWidth) {
-		random = new Random(System.nanoTime());
-		dict = new Dictionary(random, dictWordCnt, minWordLen, maxWordLen, 100);
-		this.pageWidth = pageWidth;
-		lastLineLenGen = new Flat(random, 1, pageWidth);
-		paragraphLineLenGen = new Flat(random, pageWidth * 3 / 4, pageWidth);
-		paragraphLenGen = new Flat(random, 1, 40);
-	}
-
-	/**
-	 * Create an article
-	 * 
-	 * @param fs
-	 *          File system.
-	 * @param path
-	 *          path of the file
-	 * @param length
-	 *          Expected size of the file.
-	 * @throws IOException
-	 */
-	void createArticle(FileSystem fs, Path path, long length) throws IOException {
-		FSDataOutputStream fsdos = fs.create(path, false);
-		StringBuilder sb = new StringBuilder();
-		int remainLinesInParagraph = paragraphLenGen.nextInt();
-		while (fsdos.getPos() < length) {
-			if (remainLinesInParagraph == 0) {
-				remainLinesInParagraph = paragraphLenGen.nextInt();
-				fsdos.write('\n');
-			}
-			int lineLen = paragraphLineLenGen.nextInt();
-			if (--remainLinesInParagraph == 0) {
-				lineLen = lastLineLenGen.nextInt();
-			}
-			sb.setLength(0);
-			while (sb.length() < lineLen) {
-				if (sb.length() > 0) {
-					sb.append(' ');
-				}
-				sb.append(dict.nextWord());
-				++wordCount;
-			}
-			sb.append('\n');
-			fsdos.write(sb.toString().getBytes());
-			++lineCount;
-		}
-		fsdos.close();
-	}
-
-	/**
-	 * Create a bunch of files under the same directory.
-	 * 
-	 * @param fs
-	 *          File system
-	 * @param parent
-	 *          directory where files should be created
-	 * @param prefix
-	 *          prefix name of the files
-	 * @param n
-	 *          total number of files
-	 * @param length
-	 *          length of each file.
-	 * @throws IOException
-	 */
-	void batchArticalCreation(FileSystem fs, Path parent, String prefix, int n,
-			long length) throws IOException {
-		for (int i = 0; i < n; ++i) {
-			createArticle(fs, new Path(parent, String.format("%s%06d", prefix, i)),
-					length);
-		}
-	}
-
-	static class Summary {
-		long wordCount;
-		long lineCount;
-		Map<String, Long> wordCntDist;
-
-		Summary() {
-			wordCntDist = new HashMap<String, Long>();
-		}
-	}
-
-	void resetSummary() {
-		wordCount = 0;
-		lineCount = 0;
-		dict.resetWordCnts();
-	}
-
-	Summary getSummary() {
-		Summary ret = new Summary();
-		ret.wordCount = wordCount;
-		ret.lineCount = lineCount;
-		ret.wordCntDist = dict.getWordCounts();
-		return ret;
-	}
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/Dictionary.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/Dictionary.java
deleted file mode 100644
index e14180454..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/Dictionary.java
+++ /dev/null
@@ -1,113 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Map;
-import java.util.Random;
-import java.util.Set;
-
-import org.apache.hadoop.zebra.tfile.RandomDistribution.Binomial;
-import org.apache.hadoop.zebra.tfile.RandomDistribution.DiscreteRNG;
-import org.apache.hadoop.zebra.tfile.RandomDistribution.Zipf;
-
-/**
- * A dictionary that generates English words, whose frequency follows Zipf
- * distributions, and length follows Binomial distribution.
- */
-class Dictionary {
-	private static final double BINOMIAL_P = 0.3;
-	private static final double SIGMA = 1.1;
-	private final int lead;
-	private final Zipf zipf;
-	private final String[] dict;
-	private final long[] wordCnts;
-
-	private static String makeWord(DiscreteRNG rng, Random random) {
-		int len = rng.nextInt();
-		StringBuilder sb = new StringBuilder(len);
-		for (int i = 0; i < len; ++i) {
-			sb.append((char) ('a' + random.nextInt(26)));
-		}
-		return sb.toString();
-	}
-
-	/**
-	 * Constructor
-	 * 
-	 * @param entries
-	 *          How many words exist in the dictionary.
-	 * @param minWordLen
-	 *          Minimum word length.
-	 * @param maxWordLen
-	 *          Maximum word length.
-	 * @param freqRatio
-	 *          Expected ratio between the most frequent words and the least
-	 *          frequent words. (e.g. 100)
-	 */
-	public Dictionary(Random random, int entries, int minWordLen, int maxWordLen,
-			int freqRatio) {
-		Binomial binomial = new Binomial(random, minWordLen, maxWordLen, BINOMIAL_P);
-		lead = Math.max(0,
-				(int) (entries / (Math.exp(Math.log(freqRatio) / SIGMA) - 1)) - 1);
-		zipf = new Zipf(random, lead, entries + lead, 1.1);
-		dict = new String[entries];
-		// Use a set to ensure no dup words in dictionary
-		Set<String> dictTmp = new HashSet<String>();
-		for (int i = 0; i < entries; ++i) {
-			while (true) {
-				String word = makeWord(binomial, random);
-				if (!dictTmp.contains(word)) {
-					dictTmp.add(word);
-					dict[i] = word;
-					break;
-				}
-			}
-		}
-		wordCnts = new long[dict.length];
-	}
-
-	/**
-	 * Get the next word from the dictionary.
-	 * 
-	 * @return The next word from the dictionary.
-	 */
-	public String nextWord() {
-		int index = zipf.nextInt() - lead;
-		++wordCnts[index];
-		return dict[index];
-	}
-
-	public void resetWordCnts() {
-		for (int i = 0; i < wordCnts.length; ++i) {
-			wordCnts[i] = 0;
-		}
-	}
-
-	public Map<String, Long> getWordCounts() {
-		Map<String, Long> ret = new HashMap<String, Long>();
-		for (int i = 0; i < dict.length; ++i) {
-			if (wordCnts[i] > 0) {
-				ret.put(dict[i], wordCnts[i]);
-			}
-		}
-		return ret;
-	}
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TableMRSample.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TableMRSample.java
deleted file mode 100644
index 6bbb6ad4e..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TableMRSample.java
+++ /dev/null
@@ -1,121 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
-import org.apache.hadoop.zebra.mapreduce.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.Tuple;
-
-/**
- * This is a sample a complete MR sample code for Table. It doens't contain
- * 'read' part. But, it should be similar and easier to write. Refer to test
- * cases in the same directory.
- * 
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * this 2
- * is 1
- * a 4 
- * test 2 
- * hello 1 
- * world 3
- * </pre>
- * 
- */
-public class TableMRSample {
-	static class MapClass extends
-	Mapper<LongWritable, Text, BytesWritable, Tuple> {
-		private BytesWritable bytesKey;
-		private Tuple tupleRow;
-
-		@Override
-		public void map(LongWritable key, Text value, Context context)
-		throws IOException, InterruptedException {
-
-			// value should contain "word count"
-			String[] wdct = value.toString().split(" ");
-			if (wdct.length != 2) {
-				// LOG the error
-				return;
-			}
-
-			byte[] word = wdct[0].getBytes();
-			bytesKey.set(word, 0, word.length);
-			tupleRow.set(0, new String(word));
-			tupleRow.set(1, Integer.parseInt(wdct[1]));
-
-			context.write(bytesKey, tupleRow);
-		}
-
-		@Override
-		public void setup(Context context) {
-			bytesKey = new BytesWritable();
-			try {
-				Schema outSchema = BasicTableOutputFormat.getSchema(context);
-				tupleRow = TypesUtils.createTuple(outSchema);
-			} catch (IOException e) {
-				throw new RuntimeException(e);
-			} catch (ParseException e) {
-				throw new RuntimeException(e);
-			}
-		}
-
-	}
-
-	public static void main(String[] args) throws ParseException, IOException, 
-	InterruptedException, ClassNotFoundException {
-		Job job = new Job();
-		job.setJobName("tableMRSample");
-		Configuration conf = job.getConfiguration();
-		conf.set("table.output.tfile.compression", "gz");
-
-		// input settings
-		job.setInputFormatClass(TextInputFormat.class);
-		job.setMapperClass(TableMRSample.MapClass.class);
-		FileInputFormat.setInputPaths(job, new Path(
-				"/user/joe/inputdata/input.txt"));
-
-		// output settings
-		Path outPath = new Path("/user/joe/outputdata/");
-		job.setOutputFormatClass(BasicTableOutputFormat.class);
-		BasicTableOutputFormat.setOutputPath(job, outPath);
-		// set the logical schema with 2 columns
-		BasicTableOutputFormat.setSchema(job, "word:string, count:int");
-		// for demo purposes, create 2 physical column groups
-		BasicTableOutputFormat.setStorageHint(job, "[word];[count]");
-
-		// set map-only job.
-		job.setNumReduceTasks(0);
-		job.submit();
-	}
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TableMRSample1.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TableMRSample1.java
deleted file mode 100644
index f01904c2d..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TableMRSample1.java
+++ /dev/null
@@ -1,122 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
-import org.apache.hadoop.zebra.mapreduce.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.Tuple;
-
-/**
- * This is a sample a complete MR sample code for Table. It doens't contain
- * 'read' part. But, it should be similar and easier to write. Refer to test
- * cases in the same directory.
- * 
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * this 2
- * is 1
- * a 4 
- * test 2 
- * hello 1 
- * world 3
- * </pre>
- * 
- */
-public class TableMRSample1 {
-	static class MapClass extends Mapper<LongWritable, Text, BytesWritable, Tuple> {
-		private BytesWritable bytesKey;
-		private Tuple tupleRow;
-
-		@Override
-		public void map(LongWritable key, Text value, Context context)
-		throws IOException, InterruptedException {
-
-			// value should contain "word count"
-			String[] wdct = value.toString().split(" ");
-			if (wdct.length != 2) {
-				// LOG the error
-				throw new IOException("Does not contain two fields");
-			}
-
-			byte[] word = wdct[0].getBytes();
-			bytesKey.set(word, 0, word.length);
-			tupleRow.set(0, new String(word));
-			tupleRow.set(1, Integer.parseInt(wdct[1]));
-
-			context.write(bytesKey, tupleRow);
-		}
-
-		@Override
-		public void setup(Context context) {
-			bytesKey = new BytesWritable();
-			try {
-				Schema outSchema = BasicTableOutputFormat.getSchema(context);
-				tupleRow = TypesUtils.createTuple(outSchema);
-			} catch (IOException e) {
-				throw new RuntimeException(e);
-			} catch (ParseException e) {
-				throw new RuntimeException(e);
-			}
-		}
-
-	}
-
-	public static void main(String[] args) throws ParseException, IOException, 
-	InterruptedException, ClassNotFoundException {
-		Job job = new Job();
-		job.setJobName("tableMRSample");
-		Configuration conf = job.getConfiguration();
-		conf.set("table.output.tfile.compression", "gz");
-
-		// input settings
-		job.setInputFormatClass(TextInputFormat.class);
-		job.setMapperClass(TableMRSample1.MapClass.class);
-		FileInputFormat.setInputPaths(job, new Path("/user/joe/input.txt"));
-
-		// TODO: need to find a replacement
-		//jobConf.setNumMapTasks(2);
-
-		// output settings
-		Path outPath = new Path("/user/joe/tableout");
-		job.setOutputFormatClass(BasicTableOutputFormat.class);
-		BasicTableOutputFormat.setOutputPath(job, outPath);
-		// set the logical schema with 2 columns
-		BasicTableOutputFormat.setSchema(job, "word:string, count:int");
-		// for demo purposes, create 2 physical column groups
-		BasicTableOutputFormat.setStorageHint(job, "[word];[count]");
-
-		// set map-only job.
-		job.setNumReduceTasks(0);
-		job.submit();
-	}
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TableMRSample2.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TableMRSample2.java
deleted file mode 100644
index 5e02a1c3d..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TableMRSample2.java
+++ /dev/null
@@ -1,105 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.ArrayList;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.zebra.mapreduce.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.mapreduce.TableInputFormat;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.Tuple;
-
-/**
- * This is a sample to show using zebra table to do a simple basic union in
- * map/reduce * To run this, we need have two basic tables ready. They contain
- * the data as in Sample 1, i.e., (word, count). In this example, they are at:
- * /homes/chaow/mapredu/t1 /homes/chaow/mapredu/t2 The resulting table is put
- * at: /homes/chaow/mapredu2/t1
- * 
- */
-public class TableMRSample2 {
-	static class MapClass extends
-	Mapper<BytesWritable, Tuple, BytesWritable, Tuple> {
-		private BytesWritable bytesKey;
-		private Tuple tupleRow;
-
-		@Override
-		public void map(BytesWritable key, Tuple value, Context context)
-		throws IOException, InterruptedException {
-			System.out.println(key.toString() + value.toString());
-			context.write(key, value);
-		}
-
-		@Override
-		public void setup(Context context) {
-			bytesKey = new BytesWritable();
-			try {
-				Schema outSchema = BasicTableOutputFormat.getSchema(context);
-				tupleRow = TypesUtils.createTuple(outSchema);
-			} catch (IOException e) {
-				throw new RuntimeException(e);
-			} catch (ParseException e) {
-				throw new RuntimeException(e);
-			}
-		}
-
-		public static void main(String[] args) throws ParseException, IOException, 
-		InterruptedException, ClassNotFoundException {
-			Job job = new Job();
-			job.setJobName("tableMRSample");
-			Configuration conf = job.getConfiguration();
-			conf.set("table.output.tfile.compression", "gz");
-
-			// input settings
-			job.setInputFormatClass(TableInputFormat.class);
-			job.setOutputFormatClass(BasicTableOutputFormat.class);
-			job.setMapperClass(TableMRSample2.MapClass.class);
-
-			List<Path> paths = new ArrayList<Path>(2);
-			Path p = new Path("/homes/chaow/mapredu/t1");
-			System.out.println("path = " + p);
-			paths.add(p);
-			p = new Path("/homes/chaow/mapredu/t2");
-			paths.add(p);
-
-			TableInputFormat.setInputPaths(job, paths.toArray(new Path[2]));
-			TableInputFormat.setProjection(job, "word");
-			BasicTableOutputFormat.setOutputPath(job, new Path(
-			"/homes/chaow/mapredu2/t1"));
-
-			BasicTableOutputFormat.setSchema(job, "word:string");
-			BasicTableOutputFormat.setStorageHint(job, "[word]");
-
-			// set map-only job.
-			job.setNumReduceTasks(0);
-			// TODO: need to find a replacement
-			//job.setNumMapTasks(2);
-			job.submit();
-		}
-	}
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TableMRSampleSortedTable.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TableMRSampleSortedTable.java
deleted file mode 100644
index d7789a006..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TableMRSampleSortedTable.java
+++ /dev/null
@@ -1,179 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.TreeMap;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
-import org.apache.hadoop.zebra.mapreduce.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.mapreduce.TestBasicTableIOFormatLocalFS.InvIndex;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.SortInfo;
-import org.apache.hadoop.zebra.schema.*;
-import org.apache.hadoop.zebra.types.*;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DefaultTuple;
-import org.apache.pig.data.Tuple;
-
-/**
- * This is a sample a complete MR sample code for Table. It doens't contain
- * 'read' part. But, it should be similar and easier to write. Refer to test
- * cases in the same directory.
- * 
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * this 2
- * is 1
- * a 4 
- * test 2 
- * hello 1 
- * world 3
- * </pre>
- * 
- */
-public class TableMRSampleSortedTable {
-	static class MapClass extends
-	Mapper<LongWritable, Text, BytesWritable, Tuple> {
-		private BytesWritable bytesKey;
-		private Tuple tupleRow;
-		private Object javaObj;
-		private byte[] types;
-		private ColumnType[] sortColTypes;
-
-		@Override
-		public void map(LongWritable key, Text value, Context context)
-		throws IOException, InterruptedException {
-			// value should contain "word count"
-			String[] wdct = value.toString().split(" ");
-			if (wdct.length != 2) {
-				// LOG the error
-				return;
-			}
-
-			byte[] word = wdct[0].getBytes();
-
-			// This key has to be created by user
-			bytesKey.set(word, 0, word.length);
-
-			// This tuple has to be created by user
-			tupleRow.set(0, new String(word));
-			tupleRow.set(1, Integer.parseInt(wdct[1]));
-
-			context.write(bytesKey, tupleRow);
-		}
-
-		@Override
-		public void setup(Context context) {
-			bytesKey = new BytesWritable();
-			try {
-
-				/* New M/R Interface to get sort information */  
-				SortInfo sortInfo = BasicTableOutputFormat.getSortInfo(context);
-
-				/* New M/R Interface SortInfo is exposed to user 
-				 * To get types of sort columns.
-				 * Similar interface to get names and indices
-				 */
-				sortColTypes  = sortInfo.getSortColumnTypes();
-				types = new byte[sortColTypes.length];
-				for(int i =0 ; i < sortColTypes.length; ++i){
-					types[i] = sortColTypes[i].pigDataType();
-				}
-				Schema outSchema = BasicTableOutputFormat.getSchema(context);
-				tupleRow = TypesUtils.createTuple(outSchema);
-
-			} catch (IOException e) {
-				throw new RuntimeException(e);
-			} catch (ParseException e) {
-				throw new RuntimeException(e);
-			}
-		}
-
-	}
-
-	static class ReduceClass extends
-	Reducer<BytesWritable, Tuple, BytesWritable, Tuple> {
-		Tuple outRow;
-
-
-		public void reduce(BytesWritable key, Iterator<Tuple> values, Context context)
-		throws IOException, InterruptedException {
-			try {
-				for(; values.hasNext();)  {
-					context.write(key, values.next());
-				}  
-			} catch (ExecException e) {
-				e.printStackTrace();
-			}
-		}
-
-	}  
-
-	public static void main(String[] args) throws ParseException, IOException, 
-	InterruptedException, ClassNotFoundException {
-		Job job = new Job();
-		job.setJobName("tableMRSample");
-		Configuration conf = job.getConfiguration();
-		conf.set("table.output.tfile.compression", "gz");
-
-		// input settings
-		job.setInputFormatClass(TextInputFormat.class);
-		job.setMapperClass(TableMRSampleSortedTable.MapClass.class);
-		job.setReducerClass(TableMRSampleSortedTable.ReduceClass.class);
-		job.setMapOutputKeyClass(BytesWritable.class);
-		job.setMapOutputValueClass(DefaultTuple.class);
-		FileInputFormat.setInputPaths(job, new Path(
-				"/home/gauravj/work/grid/myTesting/input.txt"));
-		// TODO: need to find a replacement
-		//job.setNumMapTasks(1);
-
-		// output settings
-		Path outPath = new Path("/home/gauravj/work/grid/myTesting/tableOuts");
-		job.setOutputFormatClass(BasicTableOutputFormat.class);
-		BasicTableOutputFormat.setOutputPath(job, outPath);
-		// set the logical schema with 2 columns
-		BasicTableOutputFormat.setSchema(job, "word:string, count:int");
-		// for demo purposes, create 2 physical column groups
-		BasicTableOutputFormat.setStorageHint(job, "[word];[count]");
-
-		/* New M/R Interface */
-		/* Set sort columns in a comma separated string */
-		/* Each sort column should belong to schema columns */
-		BasicTableOutputFormat.setSortInfo(job, "word, count");
-
-		// set map-only job.
-		job.setNumReduceTasks(1);
-		job.submit();
-	}
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TableMRSortedTableZebraKeyGenerator.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TableMRSortedTableZebraKeyGenerator.java
deleted file mode 100644
index d8155c099..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TableMRSortedTableZebraKeyGenerator.java
+++ /dev/null
@@ -1,177 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.IOException;
-import java.util.Iterator;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
-import org.apache.hadoop.zebra.mapreduce.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DefaultTuple;
-import org.apache.pig.data.Tuple;
-
-/**
- * This is a sample a complete MR sample code for Table. It doens't contain
- * 'read' part. But, it should be similar and easier to write. Refer to test
- * cases in the same directory.
- * 
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * this 2
- * is 1
- * a 4 
- * test 2 
- * hello 1 
- * world 3
- * </pre>
- * 
- */
-public class TableMRSortedTableZebraKeyGenerator {
-	static class MapClass extends
-	Mapper<LongWritable, Text, BytesWritable, Tuple> {
-		private BytesWritable bytesKey;
-		private Tuple tupleRow;
-		private Object javaObj;
-
-		@Override
-		public void map(LongWritable key, Text value, Context context)
-		throws IOException, InterruptedException {
-			// value should contain "word count"
-			String[] wdct = value.toString().split(" ");
-			if (wdct.length != 2) {
-				// LOG the error
-				return;
-			}
-
-			byte[] word = wdct[0].getBytes();
-			bytesKey.set(word, 0, word.length);
-			tupleRow.set(0, new String(word));
-			tupleRow.set(1, Integer.parseInt(wdct[1]));
-
-			// This key has to be created by user
-			Tuple userKey = new DefaultTuple();
-			userKey.append(new String(word));
-			userKey.append(Integer.parseInt(wdct[1]));
-			try {
-
-				/* New M/R Interface */
-				/* Converts user key to zebra BytesWritable key */
-				/* using sort key expr tree  */
-				/* Returns a java base object */
-				/* Done for each user key */
-
-				bytesKey = BasicTableOutputFormat.getSortKey(javaObj, userKey);
-			} catch(Exception e) {
-
-			}
-
-			context.write(bytesKey, tupleRow);
-		}
-
-		@Override
-		public void setup(Context context) {
-			bytesKey = new BytesWritable();
-			try {
-				Schema outSchema = BasicTableOutputFormat.getSchema(context);
-				tupleRow = TypesUtils.createTuple(outSchema);
-
-				/* New M/R Interface */
-				/* returns an expression tree for sort keys */
-				/* Returns a java base object */
-				/* Done once per table */
-				javaObj = BasicTableOutputFormat.getSortKeyGenerator(context);
-
-			} catch (IOException e) {
-				throw new RuntimeException(e);
-			} catch (ParseException e) {
-				throw new RuntimeException(e);
-			}
-		}
-
-	}
-
-	static class ReduceClass extends
-	Reducer<BytesWritable, Tuple, BytesWritable, Tuple> {
-		Tuple outRow;
-
-		public void reduce(BytesWritable key, Iterator<Tuple> values, Context context)
-		throws IOException, InterruptedException {
-			try {
-				for(; values.hasNext();)  {
-					context.write(key, values.next());
-				}  
-			} catch (ExecException e) {
-				e.printStackTrace();
-			}
-		}
-
-	}  
-
-	public static void main(String[] args) throws ParseException, IOException, 
-	InterruptedException, ClassNotFoundException {
-		Job job = new Job();
-		job.setJobName("tableMRSample");
-		Configuration conf = job.getConfiguration();
-		conf.set("table.output.tfile.compression", "gz");
-
-		// input settings
-		job.setInputFormatClass(TextInputFormat.class);
-		job.setMapperClass(TableMRSortedTableZebraKeyGenerator.MapClass.class);
-		job.setReducerClass(TableMRSortedTableZebraKeyGenerator.ReduceClass.class);
-		job.setMapOutputKeyClass(BytesWritable.class);
-		job.setMapOutputValueClass(DefaultTuple.class);
-		FileInputFormat.setInputPaths(job, new Path(
-				"/home/gauravj/work/grid/myTesting/input.txt"));
-
-		// TODO: need to find a replacement.
-		//job.setNumMapTasks(1);
-
-		// output settings
-		Path outPath = new Path("/home/gauravj/work/grid/myTesting/tableOuts");
-		job.setOutputFormatClass(BasicTableOutputFormat.class);
-		BasicTableOutputFormat.setOutputPath(job, outPath);
-		// set the logical schema with 2 columns
-		BasicTableOutputFormat.setSchema(job, "word:string, count:int");
-		// for demo purposes, create 2 physical column groups
-		BasicTableOutputFormat.setStorageHint(job, "[word];[count]");
-
-		/* New M/R Interface */
-		/* Set sort columns in a comma separated string */
-		/* Each sort column should belong to schema columns */
-		BasicTableOutputFormat.setSortInfo(job, "word, count");
-
-		// set map-only job.
-		job.setNumReduceTasks(1);
-		job.submit();
-	}
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TableMapReduceExample.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TableMapReduceExample.java
deleted file mode 100644
index d8abe7a17..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TableMapReduceExample.java
+++ /dev/null
@@ -1,233 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapreduce;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.conf.Configured;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
-import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.hadoop.zebra.mapreduce.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.mapreduce.TableInputFormat;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.Tuple;
-
-import java.io.IOException;
-import java.util.Iterator;
-
-/**
- * <code>TableMapReduceExample<code> is a map-reduce example for Table Input/Output Format.
- * <p/>
- * Schema for Table is set to two columns containing Word of type <i>string</i> and Count of type <i>int</i> using <code> BasicTableOutputFormat.setSchema(jobConf, "word:string, count:int"); </code>
- * <p/>
- * Hint for creation of Column Groups is specified using
- * <code>  BasicTableOutputFormat.setStorageHint(jobConf, "[word];[count]"); </code>
- * . Here we have two column groups.
- * <p/>
- * Input file should contain rows of word and count, separated by a space. For
- * example:
- * 
- * <pre>
- * this 2
- * is 1
- * a 5
- * test 2
- * hello 1
- * world 3
- * </pre>
- * <p/>
- * <p>
- * Second job reads output from the first job which is in Table Format. Here we
- * specify <i>count</i> as projection column. Table Input Format projects in put
- * row which has both word and count into a row containing only the count column
- * and hands it to map.
- * <p/>
- * Reducer sums the counts and produces a sum of counts which should match total
- * number of words in original text.
- */
-
-public class TableMapReduceExample extends Configured implements Tool {
-
-	static class Map extends Mapper<LongWritable, Text, BytesWritable, Tuple> {
-		private BytesWritable bytesKey;
-		private Tuple tupleRow;
-
-		/**
-		 * Map method for reading input.
-		 */
-		@Override
-		public void map(LongWritable key, Text value, Context context)
-		throws IOException, InterruptedException {
-
-			// value should contain "word count"
-			String[] wordCount = value.toString().split(" ");
-			if (wordCount.length != 2) {
-				// LOG the error
-				throw new IOException("Value does not contain two fields:" + value);
-			}
-
-			byte[] word = wordCount[0].getBytes();
-			bytesKey.set(word, 0, word.length);
-			tupleRow.set(0, new String(word));
-			tupleRow.set(1, Integer.parseInt(wordCount[1]));
-
-			context.write( bytesKey, tupleRow );
-		}
-
-		/**
-		 * Configuration of the job. Here we create an empty Tuple Row.
-		 */
-		@Override
-		public void setup(Context context) {
-			bytesKey = new BytesWritable();
-			try {
-				Schema outSchema = BasicTableOutputFormat.getSchema( context );
-				tupleRow = TypesUtils.createTuple(outSchema);
-			} catch (IOException e) {
-				throw new RuntimeException(e);
-			} catch (ParseException e) {
-				throw new RuntimeException(e);
-			}
-		}
-
-	}
-
-	static class ProjectionMap extends
-	Mapper<BytesWritable, Tuple, Text, IntWritable> {
-		private final static Text all = new Text("All");
-
-		/**
-		 * Map method which gets count column after projection.
-		 * 
-		 * @throws IOException
-		 */
-		@Override
-		public void map(BytesWritable key, Tuple value, Context context)
-		throws IOException, InterruptedException {
-			context.write( all, new IntWritable((Integer) value.get(0)) );
-		}
-	}
-
-	public static class ProjectionReduce extends Reducer<Text, IntWritable, Text, IntWritable> {
-		/**
-		 * Reduce method which implements summation. Acts as both reducer and
-		 * combiner.
-		 * 
-		 * @throws IOException
-		 */
-		@Override
-		public void reduce(Text key, Iterable<IntWritable> values, Context context)
-		throws IOException, InterruptedException {
-			int sum = 0;
-			Iterator<IntWritable> iterator = values.iterator();
-			while (iterator.hasNext()) {
-				sum += iterator.next().get();
-			}
-			context.write(key, new IntWritable(sum));
-		}
-	}
-
-	/**
-	 * Where jobs and their settings and sequence is set.
-	 * 
-	 * @param args
-	 *          arguments with exception of Tools understandable ones.
-	 */
-	public int run(String[] args) throws Exception {
-		if (args == null || args.length != 3) {
-			System.out
-			.println("usage: TableMapReduceExample input_path_for_text_file output_path_for_table output_path_for_text_file");
-			System.exit(-1);
-		}
-
-		/*
-		 * First MR Job creating a Table with two columns
-		 */
-		Job job = new Job();
-		job.setJobName("TableMapReduceExample");
-		Configuration conf = job.getConfiguration();
-		conf.set("table.output.tfile.compression", "none");
-
-		// Input settings
-		job.setInputFormatClass(TextInputFormat.class);
-		job.setMapperClass(Map.class);
-		FileInputFormat.setInputPaths(job, new Path(args[0]));
-
-		// Output settings
-		job.setOutputFormatClass(BasicTableOutputFormat.class);
-		BasicTableOutputFormat.setOutputPath( job, new Path(args[1]) );
-
-		// set the logical schema with 2 columns
-		BasicTableOutputFormat.setSchema( job, "word:string, count:int" );
-
-		// for demo purposes, create 2 physical column groups
-		BasicTableOutputFormat.setStorageHint( job, "[word];[count]" );
-
-		// set map-only job.
-		job.setNumReduceTasks(0);
-
-		// Run Job
-		job.submit();
-
-		/*
-		 * Second MR Job for Table Projection of count column
-		 */
-		Job projectionJob = new Job();
-		projectionJob.setJobName("TableProjectionMapReduceExample");
-		conf = projectionJob.getConfiguration();
-
-		// Input settings
-		projectionJob.setMapperClass(ProjectionMap.class);
-		projectionJob.setInputFormatClass(TableInputFormat.class);
-		TableInputFormat.setProjection(job, "count");
-		TableInputFormat.setInputPaths(job, new Path(args[1]));
-		projectionJob.setMapOutputKeyClass(Text.class);
-		projectionJob.setMapOutputValueClass(IntWritable.class);
-
-		// Output settings
-		projectionJob.setOutputFormatClass(TextOutputFormat.class);
-		FileOutputFormat.setOutputPath(projectionJob, new Path(args[2]));
-		projectionJob.setReducerClass(ProjectionReduce.class);
-		projectionJob.setCombinerClass(ProjectionReduce.class);
-
-		// Run Job
-		projectionJob.submit();
-
-		return 0;
-	}
-
-	public static void main(String[] args) throws Exception {
-		int res = ToolRunner.run(new Configuration(), new TableMapReduceExample(),
-				args);
-		System.exit(res);
-	}
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestBasicTableIOFMoreLocal.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestBasicTableIOFMoreLocal.java
deleted file mode 100644
index 0e85b451d..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestBasicTableIOFMoreLocal.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.IOException;
-
-import org.apache.commons.logging.LogFactory;
-
-/**
- * Testing BasicTableOutputFormat and TableInputFormat using LocalFS with larger
- * settings.
- */
-public class TestBasicTableIOFMoreLocal extends TestBasicTableIOFormatLocalFS {
-	@Override
-	protected void setUp() throws IOException {
-		LOG = LogFactory.getLog(TestBasicTableIOFMoreLocal.class.getName());
-
-		if (options == null) {
-			options = new Options();
-			options.srcFiles = 20;
-			options.numBatches = 1;
-			options.numMapper = 5;
-			options.numReducer = 4;
-		}
-
-		super.setUp();
-	}
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestBasicTableIOFormatDFS.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestBasicTableIOFormatDFS.java
deleted file mode 100644
index 951e4e110..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestBasicTableIOFormatDFS.java
+++ /dev/null
@@ -1,40 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.IOException;
-
-import org.apache.commons.logging.LogFactory;
-
-/**
- * Testing BasicTableOutputFormat and TableInputFormat using DFS
- */
-public class TestBasicTableIOFormatDFS extends TestBasicTableIOFormatLocalFS {
-	@Override
-	protected void setUp() throws IOException {
-		LOG = LogFactory.getLog(TestBasicTableIOFormatDFS.class.getName());
-
-		if (options == null) {
-			options = new Options();
-			options.localFS = false;
-		}
-
-		super.setUp();
-	}
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestBasicTableIOFormatLocalFS.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestBasicTableIOFormatLocalFS.java
deleted file mode 100644
index 849f605fb..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestBasicTableIOFormatLocalFS.java
+++ /dev/null
@@ -1,966 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.File;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Comparator;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.PriorityQueue;
-import java.util.StringTokenizer;
-import java.util.TreeMap;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hdfs.MiniDFSCluster;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.RawComparator;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.zebra.tfile.Utils;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.input.FileSplit;
-import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.mapreduce.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.mapreduce.TableInputFormat;
-import org.apache.hadoop.zebra.mapreduce.ArticleGenerator.Summary;
-import org.apache.hadoop.zebra.mapreduce.TestBasicTableIOFormatLocalFS.FreqWordCache.Item;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Projection;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.Tuple;
-
-/**
- * Testing BasicTableOutputFormat and TableInputFormat using Local FS
- */
-public class TestBasicTableIOFormatLocalFS extends TestCase {
-	static class Options {
-		int taskTrackers = 4;
-		int dataNodes = 4;
-		int srcFiles = 7;
-		int numBatches = 1;
-		int srcFileLen = 2 * 1024; // 20KB
-		int numMapper = 3;
-		int numReducer = 2;
-		int numFreqWords = 10;
-		long minTableSplitSize = 16 * 1024L;
-		boolean localFS = true;
-		String compression = "none";
-		String rootPath = "TestBasicTableIOFormat";
-		String srcPath = "docs";
-		String fwdIndexRootPath = "fwdIndex";
-		String invIndexTablePath = "invIndex";
-		String freqWordTablePath = "freqWord";
-	}
-
-	static Log LOG = LogFactory.getLog(TestBasicTableIOFormatLocalFS.class
-			.getName());
-
-	Options options;
-	Configuration conf;
-	MiniDFSCluster dfs;
-	FileSystem fileSys;
-	Path rootPath;
-	Path srcPath;
-	Path fwdIndexRootPath;
-	Path invIndexTablePath;
-	Path freqWordTablePath;
-	ArticleGenerator articalGen;
-	Map<String, Summary> summary;
-
-	@Override
-	protected void setUp() throws IOException {
-		if (System.getProperty("hadoop.log.dir") == null) {
-			String base = new File(".").getPath(); // getAbsolutePath();
-			System
-			.setProperty("hadoop.log.dir", new Path(base).toString() + "./logs");
-		}
-
-		if (options == null) {
-			options = new Options();
-		}
-
-		if (conf == null) {
-			conf = new Configuration();
-		}
-
-		articalGen = new ArticleGenerator(1000, 1, 20, 100);
-		summary = new HashMap<String, Summary>();
-
-		if (options.localFS) {
-			Path localFSRootPath = new Path(System.getProperty("test.build.data",
-			"build/test/data/work-dir"));
-			fileSys = localFSRootPath.getFileSystem(conf);
-			rootPath = new Path(localFSRootPath, options.rootPath);
-		} else {
-			dfs = new MiniDFSCluster(conf, options.dataNodes, true, null);
-			fileSys = dfs.getFileSystem();
-			rootPath = new Path(fileSys.getWorkingDirectory(), options.rootPath);
-		}
-		conf = getJobConf();
-		srcPath = new Path(rootPath, options.srcPath);
-		fwdIndexRootPath = new Path(rootPath, options.fwdIndexRootPath);
-		invIndexTablePath = new Path(rootPath, options.invIndexTablePath);
-		freqWordTablePath = new Path(rootPath, options.freqWordTablePath);
-	}
-
-	@Override
-	protected void tearDown() throws IOException {
-		if (dfs != null) {
-			dfs.shutdown();
-		}
-	}
-
-	Configuration getJobConf() {
-		Configuration conf = new Configuration();
-		conf.setInt("table.input.split.minSize", 1);
-		options.minTableSplitSize = 1; // force more splits
-		conf.setInt("dfs.block.size", 1024); // force multiple blocks
-		conf.set("table.output.tfile.compression", options.compression);
-		conf.setInt("mapred.app.freqWords.count", options.numFreqWords);
-		return conf;
-	}
-
-	/**
-	 * Create a bunch of text files under a sub-directory of the srcPath. The name
-	 * of the sub-directory is named after the batch name.
-	 * 
-	 * @param batchName
-	 *          The batch name.
-	 * @throws IOException
-	 */
-	void createSourceFiles(String batchName) throws IOException {
-		LOG.info("Creating source data folder: " + batchName);
-		Path batchDir = new Path(srcPath, batchName);
-		LOG.info("Cleaning directory: " + batchName);
-		fileSys.delete(batchDir, true);
-		LOG.info("Generating input files: " + batchName);
-		articalGen.batchArticalCreation(fileSys, new Path(srcPath, batchName),
-				"doc-", options.srcFiles, options.srcFileLen);
-		Summary s = articalGen.getSummary();
-		// dumpSummary(s);
-		long tmp = 0;
-		for (Iterator<Long> it = s.wordCntDist.values().iterator(); it.hasNext(); tmp += it
-		.next())
-			;
-		Assert.assertEquals(tmp, s.wordCount);
-		summary.put(batchName, s);
-		articalGen.resetSummary();
-	}
-
-	/**
-	 * Generate forward index. Map-only task.
-	 * 
-	 * <pre>
-	 * Map Input:
-	 *    K = LongWritable (byte offset)
-	 *    V = Text (text line)
-	 * Map Output:
-	 *    K = word: String.
-	 *    V = Tuple of {fileName:String, wordPos:Integer, lineNo:Integer }
-	 * </pre>
-	 */
-	static class ForwardIndexGen {
-		static class MapClass extends
-		Mapper<LongWritable, Text, BytesWritable, Tuple> {
-			private BytesWritable outKey;
-			private Tuple outRow;
-			// index into the output tuple for fileName, wordPos, lineNo.
-			private int idxFileName, idxWordPos, idxLineNo;
-			private String filePath;
-			// maintain line number and word position.
-			private int lineNo = 0;
-			private int wordPos = 0;
-
-			@Override
-			public void map(LongWritable key, Text value, Context context)
-			throws IOException, InterruptedException {
-				if (filePath == null) {
-					FileSplit split = (FileSplit) context.getInputSplit();
-					filePath = split.getPath().toString();
-				}
-				String line = value.toString();
-				StringTokenizer st = new StringTokenizer(line, " ");
-				while (st.hasMoreElements()) {
-					byte[] word = st.nextToken().getBytes();
-					outKey.set(word, 0, word.length);
-					TypesUtils.resetTuple(outRow);
-					try {
-						outRow.set(idxFileName, filePath);
-						outRow.set(idxWordPos, new Integer(wordPos));
-						outRow.set(idxLineNo, new Integer(lineNo));
-						context.write(outKey, outRow);
-					} catch (ExecException e) {
-						e.printStackTrace();
-					}
-
-					++wordPos;
-				}
-				++lineNo;
-
-			}
-
-			@Override
-			public void setup(Context context) {
-				LOG.info("ForwardIndexGen.MapClass.configure");
-				outKey = new BytesWritable();
-				try {
-					Schema outSchema = BasicTableOutputFormat.getSchema(context);
-					outRow = TypesUtils.createTuple(outSchema);
-					idxFileName = outSchema.getColumnIndex("fileName");
-					idxWordPos = outSchema.getColumnIndex("wordPos");
-					idxLineNo = outSchema.getColumnIndex("lineNo");
-				} catch (IOException e) {
-					throw new RuntimeException("Schema parsing failed : "
-							+ e.getMessage());
-				} catch (ParseException e) {
-					throw new RuntimeException("Schema parsing failed : "
-							+ e.getMessage());
-				}
-			}
-
-		}
-	}
-
-	/**
-	 * Run forward index generation.
-	 * 
-	 * @param batchName
-	 * @throws IOException
-	 * @throws ClassNotFoundException 
-	 * @throws InterruptedException 
-	 */
-	void runForwardIndexGen(String batchName) throws IOException, ParseException, 
-	InterruptedException, ClassNotFoundException {
-		LOG.info("Run Map-only job to convert source data to forward index: "
-				+ batchName);
-
-		//JobConf jobConf = getJobConf("fwdIndexGen-" + batchName);
-		Job job = new Job();
-		job.setJobName( "fwdIndexGen-" + batchName );
-		// input-related settings
-		job.setInputFormatClass(TextInputFormat.class);
-		job.setMapperClass(ForwardIndexGen.MapClass.class);
-
-		//Path wd = fileSys.getWorkingDirectory();
-		FileInputFormat.setInputPaths(job, new Path(srcPath, batchName));
-
-		// output related settings
-		Path outPath = new Path(fwdIndexRootPath, batchName);
-		fileSys.delete(outPath, true);
-		job.setOutputFormatClass(BasicTableOutputFormat.class);
-		BasicTableOutputFormat.setOutputPath(job, outPath);
-		BasicTableOutputFormat.setSchema(job, "fileName:string, wordPos:int, lineNo:int");
-
-		// set map-only job.
-		job.setNumReduceTasks(0);
-		job.submit();
-		job.waitForCompletion(true);
-		BasicTableOutputFormat.close( job );
-	}
-
-	/**
-	 * Count the # of rows of a BasicTable
-	 * 
-	 * @param tablePath
-	 *          The path to the BasicTable
-	 * @return Number of rows.
-	 * @throws IOException
-	 */
-	long countRows(Path tablePath) throws IOException, ParseException {
-		BasicTable.Reader reader = new BasicTable.Reader(tablePath, conf);
-		//reader.setProjection("");
-		long totalRows = 0;
-		TableScanner scanner = reader.getScanner(null, true);
-		for (; !scanner.atEnd(); scanner.advance()) {
-			++totalRows;
-		}
-		scanner.close();
-		return totalRows;
-	}
-
-	/**
-	 * Given a batch ID, return the batch name.
-	 * 
-	 * @param i
-	 *          batch ID
-	 * @return Batch name.
-	 */
-	static String batchName(int i) {
-		return String.format("batch-%03d", i);
-	}
-
-	/**
-	 * Inverted index for one word.
-	 */
-	static class InvIndex implements Writable {
-		int count = 0;
-		// a map from filePath to all occurrences of the word positions.
-		Map<String, ArrayList<Integer>> index;
-
-		InvIndex() {
-			index = new TreeMap<String, ArrayList<Integer>>();
-		}
-
-		InvIndex(String fileName, int pos) {
-			this();
-			add(fileName, pos);
-		}
-
-		void add(String fileName, int pos) {
-			++count;
-			ArrayList<Integer> list = index.get(fileName);
-			if (list == null) {
-				list = new ArrayList<Integer>(1);
-				index.put(fileName, list);
-			}
-			list.add(pos);
-		}
-
-		void add(String fileName, ArrayList<Integer> positions) {
-			ArrayList<Integer> list = index.get(fileName);
-			if (list == null) {
-				list = new ArrayList<Integer>();
-				index.put(fileName, list);
-			}
-			count += positions.size();
-			list.ensureCapacity(list.size() + positions.size());
-			list.addAll(positions);
-		}
-
-		void reduce(InvIndex other) {
-			for (Iterator<Map.Entry<String, ArrayList<Integer>>> it = other.index
-					.entrySet().iterator(); it.hasNext();) {
-				Map.Entry<String, ArrayList<Integer>> e = it.next();
-				add(e.getKey(), e.getValue());
-			}
-		}
-
-		@Override
-		public void readFields(DataInput in) throws IOException {
-			count = 0;
-			index.clear();
-			count = Utils.readVInt(in);
-			if (count > 0) {
-				int n = Utils.readVInt(in);
-				for (int i = 0; i < n; ++i) {
-					String fileName = Utils.readString(in);
-					int m = Utils.readVInt(in);
-					ArrayList<Integer> list = new ArrayList<Integer>(m);
-					index.put(fileName, list);
-					for (int j = 0; j < m; ++j) {
-						list.add(Utils.readVInt(in));
-					}
-				}
-			}
-		}
-
-		@Override
-		public void write(DataOutput out) throws IOException {
-			Utils.writeVInt(out, count);
-			if (count > 0) {
-				Utils.writeVInt(out, index.size());
-				for (Iterator<Map.Entry<String, ArrayList<Integer>>> it = index
-						.entrySet().iterator(); it.hasNext();) {
-					Map.Entry<String, ArrayList<Integer>> e = it.next();
-					Utils.writeString(out, e.getKey());
-					ArrayList<Integer> list = e.getValue();
-					Utils.writeVInt(out, list.size());
-					for (Iterator<Integer> it2 = list.iterator(); it2.hasNext();) {
-						Utils.writeVInt(out, it2.next());
-					}
-				}
-			}
-		}
-	}
-
-	/**
-	 * Generate inverted Index.
-	 * 
-	 * <pre>
-	 * Mapper Input = 
-	 *    K: BytesWritable word; 
-	 *    V: Tuple { fileName:String, wordPos:Integer };
-	 *    
-	 * Mapper Output =
-	 *    K: BytesWritable word;
-	 *    V: InvIndex;
-	 *   
-	 * Reducer Output =
-	 *    K: BytesWritable word; 
-	 *    V: Tuple {count:Integer, index: Map of {fileName:String, Bag of {wordPos:Integer}}};
-	 * </pre>
-	 */
-	static class InvertedIndexGen {
-		static class MapClass extends
-		Mapper<BytesWritable, Tuple, BytesWritable, InvIndex> {
-			// index of fileName and wordPos fileds of the input tuple
-			int idxFileName, idxWordPos;
-
-			@Override
-			public void map(BytesWritable key, Tuple value, Context context)
-			throws IOException, InterruptedException {
-				try {
-					String fileName = (String) value.get(idxFileName);
-					int wordPos = (Integer) value.get(idxWordPos);
-					context.write(key, new InvIndex(fileName, wordPos));
-				} catch (ExecException e) {
-					e.printStackTrace();
-				}
-			}
-
-			@Override
-			public void setup(Context context) {
-				LOG.info("InvertedIndexGen.MapClass.configure");
-				String projection;
-				try {
-					projection = TableInputFormat.getProjection(context);
-				} catch (ParseException e) {
-					throw new RuntimeException("Schema parsing failed : "
-							+ e.getMessage());
-				} catch (IOException e) {
-					throw new RuntimeException("TableInputFormat.getProjection", e);
-				}
-				idxFileName = Projection.getColumnIndex(projection, "fileName");
-				idxWordPos = Projection.getColumnIndex(projection, "wordPos");
-			}
-
-		}
-
-		static class CombinerClass extends
-		Reducer<BytesWritable, InvIndex, BytesWritable, InvIndex> {
-
-			@Override
-			public void reduce(BytesWritable key, Iterable<InvIndex> values, Context context)
-			throws IOException, InterruptedException {
-				InvIndex sum = new InvIndex();
-				Iterator<InvIndex> it = values.iterator();
-				for (; it.hasNext();) {
-					sum.reduce(it.next());
-				}
-				context.write(key, sum);
-			}
-
-		}
-
-		static class ReduceClass extends
-		Reducer<BytesWritable, InvIndex, BytesWritable, Tuple> {
-			Tuple outRow;
-			int idxCount, idxIndex;
-			Schema wordPosSchema;
-			int idxWordPos;
-
-			@Override
-			public void setup(Context context) {
-				LOG.info("InvertedIndexGen.ReduceClass.configure");
-				try {
-					Schema outSchema = BasicTableOutputFormat.getSchema(context);
-					outRow = TypesUtils.createTuple(outSchema);
-					idxCount = outSchema.getColumnIndex("count");
-					idxIndex = outSchema.getColumnIndex("index");
-					wordPosSchema = new Schema("wordPos");
-					idxWordPos = wordPosSchema.getColumnIndex("wordPos");
-				} catch (IOException e) {
-					throw new RuntimeException("Schema parsing failed :" + e.getMessage());
-				} catch (ParseException e) {
-					throw new RuntimeException("Schema parsing failed :" + e.getMessage());
-				}
-			}
-
-			Map<String, DataBag> convertInvIndex(Map<String, ArrayList<Integer>> index)
-			throws IOException {
-				Map<String, DataBag> ret = new TreeMap<String, DataBag>();
-				for (Iterator<Map.Entry<String, ArrayList<Integer>>> it = index
-						.entrySet().iterator(); it.hasNext();) {
-					Map.Entry<String, ArrayList<Integer>> e = it.next();
-					DataBag bag = TypesUtils.createBag();
-					for (Iterator<Integer> it2 = e.getValue().iterator(); it2.hasNext();) {
-						Tuple tuple = TypesUtils.createTuple(wordPosSchema);
-						tuple.set(idxWordPos, it2.next());
-						bag.add(tuple);
-					}
-					ret.put(e.getKey(), bag);
-				}
-
-				return ret;
-			}
-
-			@Override
-			public void reduce(BytesWritable key, Iterable<InvIndex> values, Context context)
-			throws IOException, InterruptedException {
-				InvIndex sum = new InvIndex();
-				Iterator<InvIndex> it = values.iterator();
-				for (; it.hasNext();) {
-					sum.reduce(it.next());
-				}
-				try {
-					outRow.set(idxCount, sum.count);
-					outRow.set(idxIndex, convertInvIndex(sum.index));
-					context.write(key, outRow);
-				} catch (ExecException e) {
-					e.printStackTrace();
-				}
-			}
-		}
-	}
-
-	void runInvertedIndexGen() throws IOException, ParseException,
-	InterruptedException, ClassNotFoundException {
-		LOG.info("Converting forward index to inverted index");
-		//JobConf jobConf = getJobConf("runInvertedIndexGen");
-
-		Job job = new Job();
-		job.setJobName( "runInvertedIndexGen" );
-		// input-related settings
-		job.setInputFormatClass(TableInputFormat.class);
-		job.setMapperClass(InvertedIndexGen.MapClass.class);
-		Path[] paths = new Path[options.numBatches];
-		for (int i = 0; i < options.numBatches; ++i) {
-			paths[i] = new Path(fwdIndexRootPath, batchName(i));
-		}
-		TableInputFormat.setInputPaths(job, paths);
-		// TableInputFormat.setProjection(jobConf, "fileName, wordPos");
-		TableInputFormat.setMinSplitSize(job, options.minTableSplitSize);
-		job.setMapOutputKeyClass(BytesWritable.class);
-		job.setMapOutputValueClass(InvIndex.class);
-
-		// output related settings
-		fileSys.delete(invIndexTablePath, true);
-		job.setOutputFormatClass(BasicTableOutputFormat.class);
-		job.setReducerClass(InvertedIndexGen.ReduceClass.class);
-		job.setCombinerClass(InvertedIndexGen.CombinerClass.class);
-		BasicTableOutputFormat.setOutputPath(job, invIndexTablePath);
-		BasicTableOutputFormat.setSchema(job, "count:int, index:map()");
-		job.setNumReduceTasks(options.numReducer);
-
-		job.submit();
-		job.waitForCompletion(true);
-		BasicTableOutputFormat.close( job );
-	}
-
-	void reduce(Summary sum, Summary delta) {
-		sum.lineCount += delta.lineCount;
-		sum.wordCount += delta.wordCount;
-		reduce(sum.wordCntDist, delta.wordCntDist);
-	}
-
-	void reduce(Map<String, Long> sum, Map<String, Long> delta) {
-		for (Iterator<Map.Entry<String, Long>> it = delta.entrySet().iterator(); it
-		.hasNext();) {
-			Map.Entry<String, Long> e = it.next();
-			String key = e.getKey();
-			Long base = sum.get(key);
-			sum.put(key, (base == null) ? e.getValue() : base + e.getValue());
-		}
-	}
-
-	void dumpSummary(Summary s) {
-		LOG.info("Dumping Summary");
-		LOG.info("Word Count: " + s.wordCount);
-		for (Iterator<Map.Entry<String, Long>> it = s.wordCntDist.entrySet()
-				.iterator(); it.hasNext();) {
-			Map.Entry<String, Long> e = it.next();
-			LOG.info(e.getKey() + "->" + e.getValue());
-		}
-	}
-
-	/**
-	 * Verify the word counts from the invIndexTable is the same as collected from
-	 * the ArticleGenerator.
-	 * 
-	 * @throws IOException
-	 */
-	void verifyWordCount() throws IOException, ParseException {
-		Summary expected = new Summary();
-		for (Iterator<Summary> it = summary.values().iterator(); it.hasNext();) {
-			Summary e = it.next();
-			// dumpSummary(e);
-			reduce(expected, e);
-		}
-		// LOG.info("Dumping aggregated Summary");
-		// dumpSummary(expected);
-
-		Summary actual = new Summary();
-		BasicTable.Reader reader = new BasicTable.Reader(invIndexTablePath, conf);
-		reader.setProjection("count");
-		TableScanner scanner = reader.getScanner(null, true);
-		Tuple tuple = TypesUtils.createTuple(Projection.toSchema(scanner
-				.getProjection()));
-		BytesWritable key = new BytesWritable();
-		for (; !scanner.atEnd(); scanner.advance()) {
-			scanner.getKey(key);
-			scanner.getValue(tuple);
-			int count = 0;
-			try {
-				count = (Integer) tuple.get(0);
-			} catch (ExecException e) {
-				e.printStackTrace();
-			}
-			actual.wordCount += count;
-			String word = new String(key.getBytes(), 0, key.getLength());
-			actual.wordCntDist.put(word, (long) count);
-		}
-		scanner.close();
-		// LOG.info("Dumping MR calculated Summary");
-		// dumpSummary(actual);
-		Assert.assertEquals(expected.wordCount, actual.wordCount);
-		Assert.assertEquals(expected.wordCntDist.size(), actual.wordCntDist.size());
-		for (Iterator<Map.Entry<String, Long>> it = expected.wordCntDist.entrySet()
-				.iterator(); it.hasNext();) {
-			Map.Entry<String, Long> e = it.next();
-			String word = e.getKey();
-			Long myCount = actual.wordCntDist.get(word);
-			Assert.assertFalse(word, myCount == null);
-			Assert.assertEquals(word, e.getValue(), myCount);
-		}
-	}
-
-	/**
-	 * Caching the top K frequent words.
-	 */
-	static class FreqWordCache {
-		static class Item {
-			BytesWritable word;
-			int count;
-
-			Item(BytesWritable w, int c) {
-				word = new BytesWritable();
-				word.set(w.getBytes(), 0, w.getLength());
-				count = c;
-			}
-		}
-
-		int k;
-		PriorityQueue<Item> words;
-
-		FreqWordCache(int k) {
-			if (k <= 0) {
-				throw new IllegalArgumentException("Expecting positive int");
-			}
-			this.k = k;
-			words = new PriorityQueue<Item>(k, new Comparator<Item>() {
-				@Override
-				public int compare(Item o1, Item o2) {
-					if (o1.count != o2.count) {
-						return o1.count - o2.count;
-					}
-					return -o1.word.compareTo(o2.word);
-				}
-			});
-		}
-
-		void add(BytesWritable word, int cnt) {
-			while ((words.size() >= k) && words.peek().count < cnt) {
-				words.poll();
-			}
-			if ((words.size() < k) || words.peek().count == cnt) {
-				words.add(new Item(word, cnt));
-			}
-		}
-
-		void add(Iterator<BytesWritable> itWords, int cnt) {
-			while ((words.size() >= k) && words.peek().count < cnt) {
-				words.poll();
-			}
-			if ((words.size() < k) || words.peek().count == cnt) {
-				for (; itWords.hasNext();) {
-					words.add(new Item(itWords.next(), cnt));
-				}
-			}
-		}
-
-		Item[] toArray() {
-			Item[] ret = new Item[words.size()];
-			for (int i = 0; i < ret.length; ++i) {
-				ret[i] = words.poll();
-			}
-
-			for (int i = 0; i < ret.length / 2; ++i) {
-				Item tmp = ret[i];
-				ret[i] = ret[ret.length - i - 1];
-				ret[ret.length - i - 1] = tmp;
-			}
-
-			return ret;
-		}
-	}
-
-	/**
-	 * Get the most frequent words from inverted index. The mapper uses a priority
-	 * queue to keep the top frequent words in memory and output the results in
-	 * close().
-	 * 
-	 * <pre>
-	 * Mapper Input = 
-	 *    K: BytesWritable word; 
-	 *    V: Tuple { count:Integer }.
-	 *    
-	 * Mapper Output =
-	 *    K: IntWritabl: count
-	 *    V: BytesWritable: word
-	 *   
-	 * Reducer Output =
-	 *    K: BytesWritable word;
-	 *    V: Tuple { count:Integer }.
-	 * </pre>
-	 */
-	static class FreqWords {
-		static int getFreqWordsCount(Configuration conf) {
-			return conf.getInt("mapred.app.freqWords.count", 100);
-		}
-
-		static class MapClass extends
-		Mapper<BytesWritable, Tuple, IntWritable, BytesWritable> {
-			int idxCount;
-			FreqWordCache freqWords;
-			IntWritable intWritable;
-
-			@Override
-			public void map(BytesWritable key, Tuple value, Context context)
-			throws IOException {
-				try {
-					int count = (Integer) value.get(idxCount);
-					freqWords.add(key, count);
-					context.progress();
-				} catch (ExecException e) {
-					e.printStackTrace();
-				}
-			}
-
-			@Override
-			public void setup(Context context) {
-				LOG.info("FreqWords.MapClass.configure");
-				String inSchema;
-				try {
-					inSchema = TableInputFormat.getProjection(context);
-				} catch (ParseException e) {
-					throw new RuntimeException("Projection parsing failed : "
-							+ e.getMessage());
-				} catch (IOException e) {
-					throw new RuntimeException("TableInputFormat.getprojection", e);
-				}
-				idxCount = Projection.getColumnIndex(inSchema, "count");
-				intWritable = new IntWritable();
-				freqWords = new FreqWordCache(getFreqWordsCount(context.getConfiguration()));
-			}
-
-			@Override
-			public void cleanup(Context context) throws IOException, InterruptedException {
-				Item[] items = freqWords.toArray();
-				for (Item i : items) {
-					intWritable.set(i.count);
-					context.write(intWritable, i.word);
-				}
-			}
-		}
-
-		static class CombinerClass extends
-		Reducer<IntWritable, BytesWritable, IntWritable, BytesWritable> {
-			FreqWordCache freqWords;
-			IntWritable intWritable;
-
-			@Override
-			public void setup(Context context) {
-				LOG.info("FreqWords.CombinerClass.configure");
-				freqWords = new FreqWordCache(getFreqWordsCount(context.getConfiguration()));
-				intWritable = new IntWritable();
-			}
-
-			@Override
-			public void cleanup(Context context) throws IOException, InterruptedException {
-				Item[] items = freqWords.toArray();
-				for (Item i : items) {
-					intWritable.set(i.count);
-					context.write(intWritable, i.word);
-				}
-			}
-
-			@Override
-			public void reduce(IntWritable key, Iterable<BytesWritable> values, Context context)
-			throws IOException, InterruptedException {
-				freqWords.add(values.iterator(), key.get());
-				context.progress();
-			}
-		}
-
-		static class ReduceClass extends
-		Reducer<IntWritable, BytesWritable, BytesWritable, Tuple> {
-			FreqWordCache freqWords;
-			Tuple outRow;
-			int idxCount;
-
-			@Override
-			public void setup(Context context) {
-				LOG.info("FreqWords.ReduceClass.configure");
-				freqWords = new FreqWordCache(getFreqWordsCount(context.getConfiguration()));
-				try {
-					Schema outSchema = BasicTableOutputFormat.getSchema(context);
-					outRow = TypesUtils.createTuple(outSchema);
-					idxCount = outSchema.getColumnIndex("count");
-				} catch (IOException e) {
-					throw new RuntimeException("Schema parsing failed : "
-							+ e.getMessage());
-				} catch (ParseException e) {
-					throw new RuntimeException("Schema parsing failed : "
-							+ e.getMessage());
-				}
-			}
-
-			@Override
-			public void cleanup(Context context) throws IOException, InterruptedException {
-				Item[] items = freqWords.toArray();
-				for (Item i : items) {
-					try {
-						outRow.set(idxCount, new Integer(i.count));
-						context.write(i.word, outRow);
-					} catch (ExecException e) {
-						e.printStackTrace();
-					}
-				}
-			}
-
-			@Override
-			public void reduce(IntWritable key, Iterable<BytesWritable> values, Context context)
-			throws IOException {
-				freqWords.add(values.iterator(), key.get());
-				context.progress();
-			}
-		}
-	}
-
-	static class InverseIntRawComparator implements RawComparator<IntWritable> {
-		IntWritable.Comparator comparator;
-
-		InverseIntRawComparator() {
-			comparator = new IntWritable.Comparator();
-		}
-
-		@Override
-		public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
-			return -comparator.compare(b1, s1, l1, b2, s2, l2);
-		}
-
-		@Override
-		public int compare(IntWritable o1, IntWritable o2) {
-			return -comparator.compare(o1, o2);
-		}
-	}
-
-	void runFreqWords() throws IOException, ParseException, 
-	InterruptedException, ClassNotFoundException {
-		LOG.info("Find the most frequent words");
-		//JobConf jobConf = getJobConf("runFreqWords");
-		Job job = new Job();
-		job.setJobName( "runFreqWords" );
-		// input-related settings
-		job.setInputFormatClass(TableInputFormat.class);
-		job.setMapperClass(FreqWords.MapClass.class);
-		TableInputFormat.setInputPaths(job, invIndexTablePath);
-		TableInputFormat.setProjection(job, "count");
-		TableInputFormat.setMinSplitSize(job, options.minTableSplitSize);
-		job.setMapOutputKeyClass(IntWritable.class);
-		job.setMapOutputValueClass(BytesWritable.class);
-		// Set customized output comparator.
-		job.setSortComparatorClass(InverseIntRawComparator.class);
-
-		// output related settings
-		fileSys.delete(freqWordTablePath, true);
-		job.setOutputFormatClass(BasicTableOutputFormat.class);
-		job.setReducerClass(FreqWords.ReduceClass.class);
-		job.setCombinerClass(FreqWords.CombinerClass.class);
-		BasicTableOutputFormat.setOutputPath(job, freqWordTablePath);
-		BasicTableOutputFormat.setSchema(job, "count:int");
-		job.setNumReduceTasks(1);
-
-		job.submit();
-		job.waitForCompletion(true);
-		BasicTableOutputFormat.close( job );
-	}
-
-	void printFreqWords() throws IOException, ParseException {
-		LOG.info("Printing the most frequent words");
-		BasicTable.Reader reader = new BasicTable.Reader(freqWordTablePath, conf);
-		TableScanner scanner = reader.getScanner(null, true);
-		BytesWritable key = new BytesWritable();
-		Schema schema = Projection.toSchema(scanner.getProjection());
-		int idxCount = schema.getColumnIndex("count");
-		Tuple value = TypesUtils.createTuple(schema);
-		for (; !scanner.atEnd(); scanner.advance()) {
-			scanner.getKey(key);
-			scanner.getValue(value);
-			try {
-				String word = new String(key.getBytes(), 0, key.getLength());
-				int count = (Integer) value.get(idxCount);
-				LOG.info(String.format("%s\t%d", word, count));
-			} catch (ExecException e) {
-				e.printStackTrace();
-			}
-		}
-		scanner.close();
-	}
-
-	/**
-	 * Testing BasicTableOutputFormat and TableInputFormat by running a sequence
-	 * of MapReduce jobs.
-	 * 
-	 * @throws IOException
-	 * @throws ClassNotFoundException 
-	 * @throws InterruptedException 
-	 */
-	public void testBasicTable() throws IOException, ParseException, 
-	InterruptedException, ClassNotFoundException {
-		LOG.info("testBasicTable");
-		LOG.info("testing BasicTableOutputFormat in Map-only job");
-		for (int i = 0; i < options.numBatches; ++i) {
-			String batchName = batchName(i);
-			createSourceFiles(batchName);
-			runForwardIndexGen(batchName);
-			LOG.info("Forward index conversion complete: " + batchName);
-			//      System.out.println( "expected number = " + summary.get( batchName).wordCount );
-			//      System.out.println( "actual number = "  + countRows( new Path( fwdIndexRootPath, batchName)) );
-			Assert.assertEquals(summary.get(batchName).wordCount, countRows(new Path(
-					fwdIndexRootPath, batchName)));
-		}
-		runInvertedIndexGen();
-		verifyWordCount();
-		runFreqWords();
-		printFreqWords();
-	}
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestBasicTableUnion.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestBasicTableUnion.java
deleted file mode 100644
index 2e39c38f2..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestBasicTableUnion.java
+++ /dev/null
@@ -1,269 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.BufferedReader;
-import java.io.BufferedWriter;
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.FileReader;
-import java.io.FileWriter;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.Iterator;
-import java.util.StringTokenizer;
-import java.net.URI;
-
-import junit.framework.Assert;
-
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.hadoop.mapreduce.Mapper.Context;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
-import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.mapreduce.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.mapreduce.ZebraOutputPartition;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.types.ZebraTuple;
-import org.apache.pig.ExecType;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * This is a sample a complete MR sample code for Table. It doens't contain
- * 'read' part. But, it should be similar and easier to write. Refer to test
- * cases in the same directory.
- * 
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * us 2
- * japan 2
- * india 4
- * us 2
- * japan 1
- * india 3
- * nouse 5
- * nowhere 4
- * 
- */
-public class TestBasicTableUnion extends BaseTestCase implements Tool {
-
-  static String inputPath;
-  
-  final static String STR_SCHEMA1 = "a:string,b:string";
-  final static String STR_STORAGE1 = "[a];[b]";
-  final static String STR_SCHEMA2 = "a:string,c:string";
-  final static String STR_STORAGE2 = "[a];[c]";
-  
-  static Path path1, path2, path3;
-
-  @BeforeClass
-  public static void setUpOnce() throws Exception {
-    init();
-
-    path1 = getTableFullPath("t1");
-    path2 = getTableFullPath("t2");
-    path3 = getTableFullPath("t3");
-    removeDir(path1);
-    removeDir(path2);
-    removeDir(path3);
-
-    /*
-     * create 1st basic table;
-     */
-    BasicTable.Writer writer = new BasicTable.Writer(path1, STR_SCHEMA1, STR_STORAGE1, conf);
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    final int numsBatch = 10;
-    final int numsInserters = 1;
-    TableInserter[] inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tuple);
-        for (int k = 0; k < tuple.size(); ++k) {
-          try {
-            tuple.set(k, b + "_" + i + "" + k);
-          } catch (ExecException e) {
-
-          }
-        }// k
-        inserters[i].insert(new BytesWritable(("key1" + i).getBytes()), tuple);
-      }// i
-    }// b
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-    writer.close();
-
-    /*
-     * create 2nd basic table;
-     */
-    writer = new BasicTable.Writer(path2, STR_SCHEMA2, STR_STORAGE2, conf);
-    schema = writer.getSchema();
-    tuple = TypesUtils.createTuple(schema);
-
-    inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tuple);
-        for (int k = 0; k < tuple.size(); ++k) {
-          try {
-            tuple.set(k, b + "_" + i + "" + k);
-          } catch (ExecException e) {
-
-          }
-        }
-        inserters[i].insert(new BytesWritable(("key2" + i).getBytes()), tuple);
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-    writer.close();  
-  }
-  
-  @AfterClass
-  public static void tearDown() throws Exception {
-    removeDir(path1);
-    removeDir(path2);
-    removeDir(path3);
-  }
- 
-  static class MapClass extends Mapper<BytesWritable, Tuple, BytesWritable, Tuple> {    
-    @Override
-    public void map(BytesWritable key, Tuple value, Context context) throws IOException, InterruptedException {      
-      System.out.println("key = " + key);
-      System.out.println("value = " + value);
-      
-      context.write(key, value);
-    }
-
-    @Override
-    public void setup(Context context) throws IOException {
-    }
-  }
-
-  @Test
-  public void test1() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-
-    Job job = new Job(conf);
-    job.setJobName("Test1");
-    job.setJarByClass(TestBasicTableUnion.class);
-    
-   
-    // input settings
-    job.setInputFormatClass(TableInputFormat.class);
-    job.setMapperClass(MapClass.class);
-    job.setMapOutputKeyClass(BytesWritable.class);
-    job.setMapOutputValueClass(ZebraTuple.class);
-    job.setOutputFormatClass(BasicTableOutputFormat.class);
-    
-    TableInputFormat.setInputPaths(job, path1, path2);
-    TableInputFormat.setProjection(job, "a,b,c");
-    BasicTableOutputFormat.setOutputPath(job, path3);
-
-    BasicTableOutputFormat.setSchema(job, "a:string, b:string, c:string");
-    BasicTableOutputFormat.setStorageHint(job, "[a,b,c]");
-    
-    job.submit();
-    job.waitForCompletion( true );
-    
-    BasicTableOutputFormat.close(job);    
-  }
-  
-  @Test(expected = IOException.class)
-  public void testNegative1() throws ParseException, IOException, InterruptedException, ClassNotFoundException {
-    Job job = new Job(conf);
-    job.setJobName("Test1");
-    job.setJarByClass(TestBasicTableUnion.class);
-   
-    // input settings
-    job.setInputFormatClass(TableInputFormat.class);
-    job.setMapperClass(MapClass.class);
-    job.setMapOutputKeyClass(BytesWritable.class);
-    job.setMapOutputValueClass(ZebraTuple.class);
-    job.setOutputFormatClass(BasicTableOutputFormat.class);
-    
-    TableInputFormat.setInputPaths(job, path1, path2);
-    TableInputFormat.setProjection(job, "a,b,c");
-    BasicTableOutputFormat.setOutputPath(job, path3);
-
-    BasicTableOutputFormat.setSchema(job, "a:string, b:string, c:string");
-    BasicTableOutputFormat.setStorageHint(job, "[a,b,c]");
-    
-    job.submit();
-    job.waitForCompletion( true );
-    
-    BasicTableOutputFormat.close(job);        
-  }
-
-  @Override
-  public int run(String[] args) throws Exception {
-    TestBasicTableUnion test = new TestBasicTableUnion();
-    TestBasicTableUnion.setUpOnce();
-    System.out.println("after setup");
-
-    test.test1();
-
-    return 0;
-  }
-
-  
-  public static void main(String[] args) throws Exception {
-    conf = new Configuration();
-    
-    int res = ToolRunner.run(conf, new TestBasicTableUnion(), args);
-    System.out.println("PASS");
-    System.exit(res);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestCheckin.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestCheckin.java
deleted file mode 100644
index 732a94887..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestCheckin.java
+++ /dev/null
@@ -1,34 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapreduce;
-
-import org.junit.runner.RunWith;
-import org.junit.runners.Suite;
-
-@RunWith(Suite.class)
-@Suite.SuiteClasses({
-	TestBasicTableIOFormatLocalFS.class,
-	TestBasicTableIOFormatDFS.class,
-	TestTfileSplit.class
-})
-
-public class TestCheckin {
-	// the class remains completely empty, 
-	// being used only as a holder for the above annotations
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestMultipleOutputs.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestMultipleOutputs.java
deleted file mode 100644
index 51957da9f..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestMultipleOutputs.java
+++ /dev/null
@@ -1,617 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.BufferedWriter;
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.FileWriter;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.Iterator;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.mapreduce.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.mapreduce.ZebraOutputPartition;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.types.ZebraTuple;
-import org.apache.pig.ExecType;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * This is a sample a complete MR sample code for Table. It doens't contain
- * 'read' part. But, it should be similar and easier to write. Refer to test
- * cases in the same directory.
- * 
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * us 2
- * japan 2
- * india 4
- * us 2
- * japan 1
- * india 3
- * nouse 5
- * nowhere 4
- * 
- * 
- */
-public class TestMultipleOutputs extends BaseTestCase implements Tool {
-
-  static String inputPath;
-  static String inputFileName = "multi-input.txt";
-  public static String sortKey = null;
-
-  private static String strTable1 = null;
-  private static String strTable2 = null;
-  private static String strTable3 = null;
-
-  @BeforeClass
-  public static void setUpOnce() throws Exception {
-    init();
-    
-    inputPath = getTableFullPath(inputFileName).toString();
-    
-    writeToFile(inputPath);
-  }
-  
-  @AfterClass
-  public static void tearDown() throws Exception {
-    if (mode == TestMode.local) {
-      pigServer.shutdown();
-    }
-  }
-  
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    StringTokenizer st = new StringTokenizer(t, ".");
-    String methodName = null;
-    while (st.hasMoreTokens()) {
-      methodName = st.nextToken();
-    }
-    return methodName;
-  }
-
-  public static void writeToFile (String inputFile) throws IOException{
-    if (mode == TestMode.local) {
-      FileWriter fstream = new FileWriter(inputFile);
-      BufferedWriter out = new BufferedWriter(fstream);
-      out.write("us 2\n");
-      out.write("japan 2\n");
-      out.write("india 4\n");
-      out.write("us 2\n");
-      out.write("japan 1\n");
-      out.write("india 3\n");
-      out.write("nouse 5\n");
-      out.write("nowhere 4\n");
-      out.close();
-    }
-
-    if (mode == TestMode.cluster) {
-      FSDataOutputStream fout = fs.create(new Path (inputFile));
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 2\n");
-      fout.writeBytes("india 4\n");
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 1\n");
-      fout.writeBytes("india 3\n");
-      fout.writeBytes("nouse 5\n");
-      fout.writeBytes("nowhere 4\n");
-      fout.close();
-    }
-  }
-
-  public static void getTablePaths(String myMultiLocs) {
-    StringTokenizer st = new StringTokenizer(myMultiLocs, ",");
-
-    // get how many tokens inside st object
-    System.out.println("tokens count: " + st.countTokens());
-    int count = 0;
-
-    // iterate st object to get more tokens from it
-    while (st.hasMoreElements()) {
-      count++;
-      String token = st.nextElement().toString();
-      if (count == 1)
-        strTable1 = token;
-      if (count == 2)
-        strTable2 = token;
-      if (count == 3)
-        strTable3 = token;
-      System.out.println("token = " + token);
-    }
-  }
-
-  public static void checkTable(String myMultiLocs) throws IOException {
-    System.out.println("myMultiLocs:" + myMultiLocs);
-    System.out.println("sorgetTablePathst key:" + sortKey);
-
-    getTablePaths(myMultiLocs);
-    String query1 = null;
-    String query2 = null;
-    String query3 = null;
-
-    if (strTable1 != null) {
-
-      query1 = "records1 = LOAD '" + strTable1
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-    if (strTable2 != null) {
-      query2 = "records2 = LOAD '" + strTable2
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-    if (strTable3 != null) {
-      query3 = "records3 = LOAD '" + strTable3
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-
-    int count1 = 0;
-    int count2 = 0;
-    int count3 = 0;
-
-    if (query1 != null) {
-      System.out.println(query1);
-      pigServer.registerQuery(query1);
-      Iterator<Tuple> it = pigServer.openIterator("records1");
-      while (it.hasNext()) {
-        count1++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-        // test 1 us table
-        if (query1.contains("test1") || query1.contains("test2")) {
-          if (count1 == 1) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 2) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 3) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count1 == 4) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-        } // test1, test2
-        if (query1.contains("test3")) {
-          if (count1 == 1) {
-            System.out.println("test3: rowvalue: " + RowValue.toString());
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count1 == 2) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count1 == 3) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 4) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-        } // test3
-      }// while
-      if (query1.contains("test1") || query1.contains("test2")
-          || query1.contains("test3")) {
-        Assert.assertEquals(4, count1);
-      }
-    }// if query1 != null
-
-    if (query2 != null) {
-      pigServer.registerQuery(query2);
-      Iterator<Tuple> it = pigServer.openIterator("records2");
-
-      while (it.hasNext()) {
-        count2++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-
-        // if test1 india table
-        if (query2.contains("test1")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test1
-        if (query1.contains("test2")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-        }// if test2
-
-        if (query1.contains("test3")) {
-          if (count1 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count1 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        } // test3
-
-      }// while
-      if (query2.contains("test1") || query2.contains("test2")
-          || query2.contains("test3")) {
-        Assert.assertEquals(2, count2);
-      }
-    }// if query2 != null
-    // test 1 us table should have 4 lines
-
-    if (query3 != null) {
-      pigServer.registerQuery(query3);
-      Iterator<Tuple> it = pigServer.openIterator("records3");
-
-      while (it.hasNext()) {
-        count3++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-
-        // if test1 japan table
-        if (query3.contains("test1")) {
-          if (count3 == 1) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count3 == 2) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-        }// if test1 test2 japan table
-
-        if (query3.contains("test2")) {
-          if (count3 == 1) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count3 == 2) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-        }// test2
-        if (query1.contains("test3")) {
-          if (count1 == 1) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count1 == 2) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-        } // test3
-
-      }// while
-      if (query3.contains("test1") || query3.contains("test2")
-          || query3.contains("test3")) {
-        Assert.assertEquals(2, count3);
-      }
-    }// if query2 != null
-  }
-
-  @Test
-  public void test1() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test combine sort keys
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "word,count";
-    System.out.println("hello sort on word and count");
-    
-    String myMultiLocs = null;
-    String methodName = getCurrentMethodName();
-    
-    String str1 = getTableFullPath("us" + methodName).toString();
-    String str2 = getTableFullPath("india" + methodName).toString();
-    String str3 = getTableFullPath("japan" + methodName).toString();
-    myMultiLocs = str1 + "," + str2 + "," + str3;
-    
-    getTablePaths(myMultiLocs);
-    System.out.println("strTable1: " + strTable1.toString());
-    System.out.println("strTable2: " + strTable2.toString());
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    removeDir(new Path(strTable3));
-    runMR(myMultiLocs, sortKey);
-    checkTable(myMultiLocs);
-    System.out.println("DONE test 1");
-  }
-
-  @Test
-  public void test2() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test 'word' sort key
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "word";
-    System.out.println("hello sort on word and count");
-    String methodName = "test2";
-    String myMultiLocs = null;
-
-    String str1 = getTableFullPath("us" + methodName).toString();
-    String str2 = getTableFullPath("india" + methodName).toString();
-    String str3 = getTableFullPath("japan" + methodName).toString();
-    myMultiLocs = str1 + "," + str2 + "," + str3;
-
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    removeDir(new Path(strTable3));
-    runMR(myMultiLocs, sortKey);
-    checkTable(myMultiLocs);
-    System.out.println("DONE test 2");
-  }
-
-  @Test
-  public void test3() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test 'count' sort key
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "count";
-    
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-    String str1 = getTableFullPath("us" + methodName).toString();
-    String str2 = getTableFullPath("india" + methodName).toString();
-    String str3 = getTableFullPath("japan" + methodName).toString();
-    myMultiLocs = str1 + "," + str2 + "," + str3;
-
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    removeDir(new Path(strTable3));
-    runMR(myMultiLocs, sortKey);
-    checkTable(myMultiLocs);
-    System.out.println("DONE test 3");
-  }
-
-  static class MapClass extends
-      Mapper<LongWritable, Text, BytesWritable, Tuple> {
-    private BytesWritable bytesKey;
-    private Tuple tupleRow;
-    private Object javaObj;
-    private Configuration conf;
-
-    @Override
-    public void map(LongWritable key, Text value, Context context)
-        throws IOException, InterruptedException {
-      // value should contain "word count"
-      String[] wdct = value.toString().split(" ");
-      if (wdct.length != 2) {
-        // LOG the error
-        return;
-      }
-
-      byte[] word = wdct[0].getBytes();
-      bytesKey.set(word, 0, word.length);
-      System.out.println("word: " + new String(word));
-      tupleRow.set(0, new String(word));
-      tupleRow.set(1, Integer.parseInt(wdct[1]));
-      System.out.println("count:  " + Integer.parseInt(wdct[1]));
-
-      // This key has to be created by user
-      /*
-       * Tuple userKey = new ZebraTuple(); userKey.append(new String(word));
-       * userKey.append(Integer.parseInt(wdct[1]));
-       */
-      System.out.println("in map, sortkey: " + sortKey);
-      Tuple userKey = new ZebraTuple();
-      if (sortKey.equalsIgnoreCase("word,count")) {
-        userKey.append(new String(word));
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("count")) {
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("word")) {
-        userKey.append(new String(word));
-      }
-
-      try {
-
-        /* New M/R Interface */
-        /* Converts user key to zebra BytesWritable key */
-        /* using sort key expr tree */
-        /* Returns a java base object */
-        /* Done for each user key */
-
-        bytesKey = BasicTableOutputFormat.getSortKey(javaObj, userKey);
-      } catch (Exception e) {
-
-      }
-
-      context.write(bytesKey, tupleRow);
-    }
-
-    @Override
-    public void setup(Context context) {
-      bytesKey = new BytesWritable();
-      conf = context.getConfiguration();
-      sortKey = conf.get("sortKey");
-      try {
-        Schema outSchema = BasicTableOutputFormat.getSchema(context);
-        tupleRow = TypesUtils.createTuple(outSchema);
-        javaObj = BasicTableOutputFormat.getSortKeyGenerator(context);
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      } catch (org.apache.hadoop.zebra.parser.ParseException e) {
-        throw new RuntimeException(e);
-      }
-    }
-
-  }
-
-  static class ReduceClass extends
-      Reducer<BytesWritable, Tuple, BytesWritable, Tuple> {
-    Tuple outRow;
-
-
-    public void reduce(BytesWritable key, Iterator<Tuple> values, Context context)
-        throws IOException, InterruptedException {
-      try {
-        for (; values.hasNext();) {
-          context.write(key, values.next());
-        }
-      } catch (ExecException e) {
-        e.printStackTrace();
-      }
-    }
-
-  }
-
-  static class OutputPartitionerClass extends ZebraOutputPartition {
-
-    @Override
-    public int getOutputPartition(BytesWritable key, Tuple value) {
-
-      // System.out.println(this.jobConf);
-
-      String reg = null;
-      try {
-        reg = (String) (value.get(0));
-      } catch (Exception e) {
-        //
-      }
-
-      if (reg.equals("us"))
-        return 0;
-      if (reg.equals("india"))
-        return 1;
-      if (reg.equals("japan"))
-        return 2;
-
-      return 0;
-    }
-  }
-
-  public void runMR(String myMultiLocs, String sortKey) throws ParseException,
-      IOException, Exception, org.apache.hadoop.zebra.parser.ParseException {
-    Job job = new Job(conf);
-    job.setJobName("TestMultipleOutputs");
-    job.setJarByClass(TestMultipleOutputs.class);
-    Configuration config = job.getConfiguration();
-    config.set("table.output.tfile.compression", "gz");
-    config.set("sortKey", sortKey);
-    // input settings
-    job.setInputFormatClass(TextInputFormat.class);
-    job.setMapperClass(TestMultipleOutputs.MapClass.class);
-    job.setMapOutputKeyClass(BytesWritable.class);
-    job.setMapOutputValueClass(ZebraTuple.class);
-    FileInputFormat.setInputPaths(job, inputPath);
-
-    // TODO: Need to find a replacement
-    //job.setNumMapTasks(1);
-
-    // output settings
-    job.setOutputFormatClass(BasicTableOutputFormat.class);
-    BasicTableOutputFormat.setMultipleOutputs(job, myMultiLocs,
-        TestMultipleOutputs.OutputPartitionerClass.class);
-
-    // set the logical schema with 2 columns
-    BasicTableOutputFormat.setSchema(job, "word:string, count:int");
-    // for demo purposes, create 2 physical column groups
-    BasicTableOutputFormat.setStorageHint(job, "[word];[count]");
-    BasicTableOutputFormat.setSortInfo(job, sortKey);
-    System.out.println("in runMR, sortkey: " + sortKey);
-    // set map-only job.
-    job.setNumReduceTasks(1);
-    job.submit();
-    job.waitForCompletion( true );
-    BasicTableOutputFormat.close( job );
-  }
-
-  @Override
-  public int run(String[] args) throws Exception {
-    TestMultipleOutputs test = new TestMultipleOutputs();
-    TestMultipleOutputs.setUpOnce();
-    System.out.println("after setup");
-
-    test.test1();
-    test.test2();
-    test.test3();
-
-    return 0;
-  }
-  
-  public static void main(String[] args) throws Exception {
-    conf = new Configuration();
-    
-    int res = ToolRunner.run(conf, new TestMultipleOutputs(), args);
-    System.out.println("PASS");
-    System.exit(res);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestMultipleOutputs2.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestMultipleOutputs2.java
deleted file mode 100644
index bab7c6db6..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestMultipleOutputs2.java
+++ /dev/null
@@ -1,607 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.BufferedWriter;
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.FileWriter;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.Iterator;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.hadoop.zebra.mapreduce.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.mapreduce.ZebraOutputPartition;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.types.ZebraTuple;
-import org.apache.pig.ExecType;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.Tuple;
-import org.junit.BeforeClass;
-import org.junit.Test;
-import org.apache.hadoop.zebra.BaseTestCase;
-
-/**
- * This is a sample a complete MR sample code for Table. It doens't contain
- * 'read' part. But, it should be similar and easier to write. Refer to test
- * cases in the same directory.
- * 
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * us 2
- * japan 2
- * india 4
- * us 2
- * japan 1
- * india 3
- * nouse 5
- * nowhere 4
- * 
- * 
- */
-public class TestMultipleOutputs2 extends BaseTestCase implements Tool {
-  static String inputPath;
-  static String inputFileName = "multi-input.txt";
-  public static String sortKey = null;
-
-  private static String strTable1 = null;
-  private static String strTable2 = null;
-
-  @BeforeClass
-  public static void setUpOnce() throws Exception {
-    init();
-    
-    inputPath = getTableFullPath(inputFileName).toString();
-
-    writeToFile(inputPath);
-  }
-
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    StringTokenizer st = new StringTokenizer(t, ".");
-    String methodName = null;
-    while (st.hasMoreTokens()) {
-      methodName = st.nextToken();
-    }
-    return methodName;
-  }
-  public static void writeToFile (String inputFile) throws IOException{
-    if (mode == TestMode.local) {
-      FileWriter fstream = new FileWriter(inputFile);
-      BufferedWriter out = new BufferedWriter(fstream);
-      out.write("us 2\n");
-      out.write("japan 2\n");
-      out.write("india 4\n");
-      out.write("us 2\n");
-      out.write("japan 1\n");
-      out.write("india 3\n");
-      out.write("nouse 5\n");
-      out.write("nowhere 4\n");
-      out.close();
-    }
-    
-    if (mode == TestMode.cluster) {
-      FSDataOutputStream fout = fs.create(new Path (inputFile));
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 2\n");
-      fout.writeBytes("india 4\n");
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 1\n");
-      fout.writeBytes("india 3\n");
-      fout.writeBytes("nouse 5\n");
-      fout.writeBytes("nowhere 4\n");
-      fout.close();
-    }
-  }
-
-  public static void getTablePaths(String myMultiLocs) {
-    StringTokenizer st = new StringTokenizer(myMultiLocs, ",");
-
-    // get how many tokens inside st object
-    System.out.println("tokens count: " + st.countTokens());
-    int count = 0;
-
-    // iterate st object to get more tokens from it
-    while (st.hasMoreElements()) {
-      count++;
-      String token = st.nextElement().toString();
-      if (mode == TestMode.local) {
-        System.out.println("in mini, token: "+token); 
-         if (count == 1)
-          strTable1 = token;
-        if (count == 2)
-          strTable2 = token;
-      }
-      
-      //if (whichCluster.equalsIgnoreCase("realCluster")) {
-      if (mode == TestMode.cluster) {
-        System.out.println("in real, token: "+token);
-        //in real, token: /user/hadoopqa/ustest3
-        //note: no prefix file:  in real cluster
-        if (count == 1)
-          strTable1 = token;
-        if (count == 2)
-          strTable2 = token;
-      }
-    }
-  }
-  
-  public static void checkTableExists(boolean expected, String strDir) throws IOException{  
-     File theDir = null; 
-     boolean actual = false;
-
-     theDir = new File(strDir.split(":")[0]);
-     actual = fs.exists(new Path (theDir.toString()));
-     
-     System.out.println("the dir : "+ theDir.toString());
-    
-     if (actual != expected){
-       Assert.fail("dir exists or not is different from what expected.");
-     }
-  }
-  
-  public static void checkTable(String myMultiLocs) throws IOException {
-    System.out.println("myMultiLocs:" + myMultiLocs);
-    System.out.println("sorgetTablePathst key:" + sortKey);
-
-    getTablePaths(myMultiLocs);
-    String query1 = null;
-    String query2 = null;
- 
-    if (strTable1 != null) {
-      query1 = "records1 = LOAD '" + strTable1
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-    
-    if (strTable2 != null) {
-      query2 = "records2 = LOAD '" + strTable2
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-
-    int count1 = 0;
-    int count2 = 0;
-  
-    if (query1 != null) {
-      System.out.println(query1);
-      pigServer.registerQuery(query1);
-      Iterator<Tuple> it = pigServer.openIterator("records1");
-      while (it.hasNext()) {
-        count1++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-        // test 1 us table
-        if (query1.contains("test1") || query1.contains("test2")|| query1.contains("test3")) {
-          
-          if (count1 == 1) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count1 == 2) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-        } // test1, test2
-    
-      }// while
-      if (query1.contains("test1") || query1.contains("test2")
-          || query1.contains("test3")) {
-        Assert.assertEquals(2, count1);
-      }
-    }// if query1 != null
-
-    if (query2 != null) {
-      pigServer.registerQuery(query2);
-      Iterator<Tuple> it = pigServer.openIterator("records2");
-
-      while (it.hasNext()) {
-        count2++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-
-        // if test1 other table
-        if (query2.contains("test1")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          
-          if (count1 == 5) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test1
-     // if test2 other table
-        if (query2.contains("test2")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          
-          if (count1 == 5) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test2
-        // if test3 other table
-        if (query2.contains("test3")) {
-          if (count2 == 1) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 5) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          
-        }// if test3
-        
-      }// while
-      if (query2.contains("test1") || query2.contains("test2")
-          || query2.contains("test3")) {
-        Assert.assertEquals(6, count2);
-      }
-    }// if query2 != null
-
-  }
-  
-  @Test
-  public void test1() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test combine sort keys
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "word,count";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-  
-    String str1 = getTableFullPath("us" + methodName).toString();
-    String str2 = getTableFullPath("others" + methodName).toString();
-    myMultiLocs = str1 + "," + str2;
-    
-    getTablePaths(myMultiLocs);
-    System.out.println("strTable1: "+strTable1);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    runMR(myMultiLocs, sortKey);
-    checkTable(myMultiLocs);
-    System.out.println("DONE test" +getCurrentMethodName());
-  
-  }
-  
-  @Test
-  public void test2() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test word sort keys
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "word";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-
-    String str1 = getTableFullPath("us" + methodName).toString();
-    String str2 = getTableFullPath("others" + methodName).toString();
-    myMultiLocs = str1 + "," + str2;
-    
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    runMR(myMultiLocs, sortKey);
-    checkTable(myMultiLocs);
-    System.out.println("DONE test" +getCurrentMethodName());
-  
-  }
-  
-  @Test
-  public void test3() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test count sort keys
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "count";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-    
-    String str1 = getTableFullPath("us" + methodName).toString();
-    String str2 = getTableFullPath("others" + methodName).toString();
-    myMultiLocs = str1 + "," + str2;
-    
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    runMR(myMultiLocs, sortKey);
- 
-    System.out.println("strTable1 = " + strTable1);
-    System.out.println("strTable2 = " + strTable2);
-    checkTableExists(true, strTable1);
-    checkTableExists(true, strTable2);
-    checkTable(myMultiLocs);
-    System.out.println("DONE test" +getCurrentMethodName());
-  }
-
-  static class MapClass extends
-      Mapper<LongWritable, Text, BytesWritable, Tuple> {
-    private BytesWritable bytesKey;
-    private Tuple tupleRow;
-    private Object javaObj;
-
-    @Override
-    public void map(LongWritable key, Text value, Context context)
-        throws IOException, InterruptedException {
-      // value should contain "word count"
-      String[] wdct = value.toString().split(" ");
-      if (wdct.length != 2) {
-        // LOG the error
-        return;
-      }
-
-      byte[] word = wdct[0].getBytes();
-      bytesKey.set(word, 0, word.length);
-      System.out.println("word: " + new String(word));
-      tupleRow.set(0, new String(word));
-      tupleRow.set(1, Integer.parseInt(wdct[1]));
-      System.out.println("count:  " + Integer.parseInt(wdct[1]));
-
-      // This key has to be created by user
-      /*
-       * Tuple userKey = new DefaultTuple(); userKey.append(new String(word));
-       * userKey.append(Integer.parseInt(wdct[1]));
-       */
-      System.out.println("in map, sortkey: " + sortKey);
-      Tuple userKey = new ZebraTuple();
-      if (sortKey.equalsIgnoreCase("word,count")) {
-        userKey.append(new String(word));
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("count")) {
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("word")) {
-        userKey.append(new String(word));
-      }
-
-      try {
-
-        /* New M/R Interface */
-        /* Converts user key to zebra BytesWritable key */
-        /* using sort key expr tree */
-        /* Returns a java base object */
-        /* Done for each user key */
-
-        bytesKey = BasicTableOutputFormat.getSortKey(javaObj, userKey);
-      } catch (Exception e) {
-
-      }
-
-      context.write(bytesKey, tupleRow);
-    }
-
-    @Override
-    public void setup(Context context) {
-      bytesKey = new BytesWritable();
-      conf = context.getConfiguration();
-      sortKey = conf.get("sortKey");
-      try {
-        Schema outSchema = BasicTableOutputFormat.getSchema(context);
-        tupleRow = TypesUtils.createTuple(outSchema);
-        javaObj = BasicTableOutputFormat.getSortKeyGenerator(context);
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      } catch (org.apache.hadoop.zebra.parser.ParseException e) {
-        throw new RuntimeException(e);
-      }
-    }
-
-  }
-
-  static class ReduceClass extends
-      Reducer<BytesWritable, Tuple, BytesWritable, Tuple> {
-    Tuple outRow;
-
-    public void reduce(BytesWritable key, Iterator<Tuple> values, Context context)
-        throws IOException, InterruptedException {
-      try {
-        for (; values.hasNext();) {
-          context.write(key, values.next());
-        }
-      } catch (ExecException e) {
-        e.printStackTrace();
-      }
-    }
-
-  }
-
-  static class OutputPartitionerClass extends ZebraOutputPartition {
-
-    @Override
-    public int getOutputPartition(BytesWritable key, Tuple value) throws ExecException {
-
-     //check table schema and table data are passed to this class
-/*
- * value0 : india
-value0 : india
-value0 : japan
-value0 : japan
-value0 : nouse
-value0 : nowhere
-value0 : us
-value0 : us
- */
-System.out.println("value0 : "+value.get(0));
-      String reg = null;
-      try {
-        reg = (String) (value.get(0));
-      } catch (Exception e) {
-        //
-      }
-
-      if (reg.equals("us"))
-        return 0;
-      else
-        return 1;
-
-    }
-
-  }
-
-  public void runMR(String myMultiLocs, String sortKey) throws ParseException,
-      IOException, Exception, org.apache.hadoop.zebra.parser.ParseException {
-    Job job = new Job(conf);
-    job.setJobName("tTestMultipleOutputs2");
-    job.setJarByClass(TestMultipleOutputs2.class);
-    Configuration conf = job.getConfiguration();
-    conf.set("table.output.tfile.compression", "gz");
-    conf.set("sortKey", sortKey);
-    // input settings
-    job.setInputFormatClass(TextInputFormat.class);
-    job.setMapperClass(TestMultipleOutputs2.MapClass.class);
-    job.setMapOutputKeyClass(BytesWritable.class);
-    job.setMapOutputValueClass(ZebraTuple.class);
-    FileInputFormat.setInputPaths(job, inputPath);
-
-    // TODO: need to find a replacement
-    //job.setNumMapTasks(1);
-
-    // output settings
-
-    job.setOutputFormatClass(BasicTableOutputFormat.class);
-    BasicTableOutputFormat.setMultipleOutputs(job, myMultiLocs,
-        TestMultipleOutputs2.OutputPartitionerClass.class);
-
-    // set the logical schema with 2 columns
-    BasicTableOutputFormat.setSchema(job, "word:string, count:int");
-    // for demo purposes, create 2 physical column groups
-    BasicTableOutputFormat.setStorageHint(job, "[word];[count]");
-    BasicTableOutputFormat.setSortInfo(job, sortKey);
-    System.out.println("in runMR, sortkey: " + sortKey);
-    // set map-only job.
-    job.setNumReduceTasks(1);
-    job.submit();
-    job.waitForCompletion( true );
-    BasicTableOutputFormat.close( job );
-  }
-
-  @Override
-  public int run(String[] args) throws Exception {
-    TestMultipleOutputs2 test = new TestMultipleOutputs2();
-    TestMultipleOutputs2.setUpOnce();
-    
-    test.test1();
-    test.test2();
-    test.test3();
-   
-    return 0;
-  }
-  
-  public static void main(String[] args) throws Exception {
-    conf = new Configuration();
-    
-    int res = ToolRunner.run(conf, new TestMultipleOutputs2(), args);
-    System.out.println("PASS");
-    System.exit(res);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestMultipleOutputs2TypedApi.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestMultipleOutputs2TypedApi.java
deleted file mode 100644
index 0819c4e30..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestMultipleOutputs2TypedApi.java
+++ /dev/null
@@ -1,621 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.BufferedWriter;
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.FileWriter;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.hadoop.zebra.mapreduce.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.mapreduce.ZebraOutputPartition;
-import org.apache.hadoop.zebra.mapreduce.ZebraSchema;
-import org.apache.hadoop.zebra.mapreduce.ZebraSortInfo;
-import org.apache.hadoop.zebra.mapreduce.ZebraStorageHint;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.types.ZebraTuple;
-import org.apache.pig.ExecType;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DefaultTuple;
-import org.apache.pig.data.Tuple;
-import org.junit.BeforeClass;
-import org.junit.Test;
-import org.apache.hadoop.zebra.BaseTestCase;
-
-/**
- * This is a sample a complete MR sample code for Table. It doens't contain
- * 'read' part. But, it should be similar and easier to write. Refer to test
- * cases in the same directory.
- * 
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * us 2
- * japan 2
- * india 4
- * us 2
- * japan 1
- * india 3
- * nouse 5
- * nowhere 4
- * 
- * 
- */
-public class TestMultipleOutputs2TypedApi extends BaseTestCase implements Tool {
-  static String inputPath;
-  static String inputFileName = "multi-input.txt";
-  public static String sortKey = null;
-
-  private static String strTable1 = null;
-  private static String strTable2 = null;
-
-  @BeforeClass
-  public static void setUpOnce() throws Exception {
-    init();
-    
-    inputPath = getTableFullPath(inputFileName).toString();
-
-    writeToFile(inputPath);
-  }
-
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    StringTokenizer st = new StringTokenizer(t, ".");
-    String methodName = null;
-    while (st.hasMoreTokens()) {
-      methodName = st.nextToken();
-    }
-    return methodName;
-  }
-
-  public static void writeToFile(String inputFile) throws IOException {
-    if (mode == TestMode.local) {
-      FileWriter fstream = new FileWriter(inputFile);
-      BufferedWriter out = new BufferedWriter(fstream);
-      out.write("us 2\n");
-      out.write("japan 2\n");
-      out.write("india 4\n");
-      out.write("us 2\n");
-      out.write("japan 1\n");
-      out.write("india 3\n");
-      out.write("nouse 5\n");
-      out.write("nowhere 4\n");
-      out.close();
-    }
-    
-    if (mode == TestMode.cluster) {
-      FSDataOutputStream fout = fs.create(new Path(inputFile));
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 2\n");
-      fout.writeBytes("india 4\n");
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 1\n");
-      fout.writeBytes("india 3\n");
-      fout.writeBytes("nouse 5\n");
-      fout.writeBytes("nowhere 4\n");
-      fout.close();
-    }
-  }
-
-  public static void getTablePaths(String myMultiLocs) {
-    StringTokenizer st = new StringTokenizer(myMultiLocs, ",");
-
-    // get how many tokens inside st object
-    System.out.println("tokens count: " + st.countTokens());
-    int count = 0;
-
-    // iterate st object to get more tokens from it
-    while (st.hasMoreElements()) {
-      count++;
-      String token = st.nextElement().toString();
-      if (mode == TestMode.local) {
-        System.out.println("in mini, token: " + token);
-        // in mini, token:
-        // file:/homes/<uid>/grid/multipleoutput/pig-table/contrib/zebra/ustest3
-        if (count == 1)
-          strTable1 = token;
-        if (count == 2)
-          strTable2 = token;
-      }
-      
-      if (mode == TestMode.cluster) {
-        System.out.println("in real, token: " + token);
-        // in real, token: /user/hadoopqa/ustest3
-        // note: no prefix file: in real cluster
-        if (count == 1)
-          strTable1 = token;
-        if (count == 2)
-          strTable2 = token;
-      }
-
-    }
-  }
-
-  public static void checkTableExists(boolean expected, String strDir)
-      throws IOException {
-    File theDir = null;
-    boolean actual = false;
-
-    theDir = new File(strDir);
-    actual = fs.exists(new Path(theDir.toString()));
-
-    System.out.println("the dir : " + theDir.toString());
-  
-    if (actual != expected) {
-      Assert.fail("dir exists or not is different from what expected.");
-    }
-  }
-
-  public static void checkTable(String myMultiLocs) throws IOException {
-    System.out.println("myMultiLocs:" + myMultiLocs);
-    System.out.println("sorgetTablePathst key:" + sortKey);
-
-    getTablePaths(myMultiLocs);
-    String query1 = null;
-    String query2 = null;
-
-    if (strTable1 != null) {
-
-      query1 = "records1 = LOAD '" + strTable1
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-    if (strTable2 != null) {
-      query2 = "records2 = LOAD '" + strTable2
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-
-    int count1 = 0;
-    int count2 = 0;
-
-    if (query1 != null) {
-      System.out.println(query1);
-      pigServer.registerQuery(query1);
-      Iterator<Tuple> it = pigServer.openIterator("records1");
-      while (it.hasNext()) {
-        count1++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-        // test 1 us table
-        if (query1.contains("test1") || query1.contains("test2")
-            || query1.contains("test3")) {
-
-          if (count1 == 1) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count1 == 2) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-        } // test1, test2
-
-      }// while
-      if (query1.contains("test1") || query1.contains("test2")
-          || query1.contains("test3")) {
-        Assert.assertEquals(2, count1);
-      }
-    }// if query1 != null
-
-    if (query2 != null) {
-      pigServer.registerQuery(query2);
-      Iterator<Tuple> it = pigServer.openIterator("records2");
-
-      while (it.hasNext()) {
-        count2++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-
-        // if test1 other table
-        if (query2.contains("test1")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-
-          if (count1 == 5) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test1
-        // if test2 other table
-        if (query2.contains("test2")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-
-          if (count1 == 5) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test2
-        // if test3 other table
-        if (query2.contains("test3")) {
-          if (count2 == 1) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 5) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-
-        }// if test3
-
-      }// while
-      if (query2.contains("test1") || query2.contains("test2")
-          || query2.contains("test3")) {
-        Assert.assertEquals(6, count2);
-      }
-    }// if query2 != null
-
-  }
-
-  @Test
-  public void test1() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test combine sort keys
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "word,count";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-    List<Path> paths = new ArrayList<Path>(2);    
-    
-    Path path1 = getTableFullPath("us" + methodName);
-    Path path2 = getTableFullPath("others" + methodName);
-    myMultiLocs = path1.toString() + "," + path2.toString();
-    paths.add(path1);
-    paths.add(path2);
-    
-    getTablePaths(myMultiLocs);
-    System.out.println("strTable1: " + strTable1);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    runMR(sortKey, paths.toArray(new Path[2]));
-    checkTable(myMultiLocs);
-    System.out.println("DONE test" + getCurrentMethodName());
-  }
-
-  @Test
-  public void test2() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test word sort keys
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "word";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-    List<Path> paths = new ArrayList<Path>(2);
-
-    Path path1 = getTableFullPath("us" + methodName);
-    Path path2 = getTableFullPath("others" + methodName);
-    myMultiLocs = path1.toString() + "," + path2.toString();
-    paths.add(path1);
-    paths.add(path2);
-    
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    runMR(sortKey, paths.toArray(new Path[2]));
-    checkTable(myMultiLocs);
-    System.out.println("DONE test" + getCurrentMethodName());
-  }
-
-  @Test
-  public void test3() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test count sort keys
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "count";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-    List<Path> paths = new ArrayList<Path>(2);
-
-    Path path1 = getTableFullPath("us" + methodName);
-    Path path2 = getTableFullPath("others" + methodName);
-    myMultiLocs = path1.toString() + "," + path2.toString();
-    paths.add(path1);
-    paths.add(path2);
-    
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    runMR(sortKey, paths.toArray(new Path[2]));
-
-    checkTableExists(true, strTable1);
-    checkTableExists(true, strTable2);
-    checkTable(myMultiLocs);
-    System.out.println("DONE test" + getCurrentMethodName());
-  }
-
-  static class MapClass extends
-      Mapper<LongWritable, Text, BytesWritable, Tuple> {
-    private BytesWritable bytesKey;
-    private Tuple tupleRow;
-    private Object javaObj;
-
-    @Override
-    public void map(LongWritable key, Text value, Context context)
-        throws IOException, InterruptedException {
-      // value should contain "word count"
-      String[] wdct = value.toString().split(" ");
-      if (wdct.length != 2) {
-        // LOG the error
-        return;
-      }
-
-      byte[] word = wdct[0].getBytes();
-      bytesKey.set(word, 0, word.length);
-      System.out.println("word: " + new String(word));
-      tupleRow.set(0, new String(word));
-      tupleRow.set(1, Integer.parseInt(wdct[1]));
-      System.out.println("count:  " + Integer.parseInt(wdct[1]));
-
-      // This key has to be created by user
-      /*
-       * Tuple userKey = new DefaultTuple(); userKey.append(new String(word));
-       * userKey.append(Integer.parseInt(wdct[1]));
-       */
-      System.out.println("in map, sortkey: " + sortKey);
-      Tuple userKey = new DefaultTuple();
-      if (sortKey.equalsIgnoreCase("word,count")) {
-        userKey.append(new String(word));
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("count")) {
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("word")) {
-        userKey.append(new String(word));
-      }
-
-      try {
-
-        /* New M/R Interface */
-        /* Converts user key to zebra BytesWritable key */
-        /* using sort key expr tree */
-        /* Returns a java base object */
-        /* Done for each user key */
-
-        bytesKey = BasicTableOutputFormat.getSortKey(javaObj, userKey);
-      } catch (Exception e) {
-
-      }
-
-      context.write(bytesKey, tupleRow);
-    }
-
-    @Override
-    public void setup(Context context) {
-      bytesKey = new BytesWritable();
-      sortKey = context.getConfiguration().get("sortKey");
-      try {
-        Schema outSchema = BasicTableOutputFormat.getSchema(context);
-        tupleRow = TypesUtils.createTuple(outSchema);
-        javaObj = BasicTableOutputFormat.getSortKeyGenerator(context);
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      } catch (org.apache.hadoop.zebra.parser.ParseException e) {
-        throw new RuntimeException(e);
-      }
-    }
-
-  }
-
-  static class ReduceClass extends
-      Reducer<BytesWritable, Tuple, BytesWritable, Tuple> {
-    Tuple outRow;
-
-    public void reduce(BytesWritable key, Iterator<Tuple> values, Context context)
-        throws IOException, InterruptedException {
-      try {
-        for (; values.hasNext();) {
-          context.write(key, values.next());
-        }
-      } catch (ExecException e) {
-        e.printStackTrace();
-      }
-    }
-
-  }
-
-  static class OutputPartitionerClass extends ZebraOutputPartition {
-
-    @Override
-    public int getOutputPartition(BytesWritable key, Tuple value)
-        throws ExecException {
-
-      // check table schema and table data are passed to this class
-      /*
-       * value0 : india value0 : india value0 : japan value0 : japan value0 :
-       * nouse value0 : nowhere value0 : us value0 : us
-       */
-      System.out.println("value0 : " + value.get(0));
-      String reg = null;
-      try {
-        reg = (String) (value.get(0));
-      } catch (Exception e) {
-        //
-      }
-
-      if (reg.equals("us"))
-        return 0;
-      else
-        return 1;
-
-    }
-
-  }
-
-  public void runMR(String sortKey, Path... paths) throws ParseException,
-      IOException, Exception, org.apache.hadoop.zebra.parser.ParseException {
-
-    Job job = new Job(conf);
-    job.setJobName("TestMultipleOutputs2TypedApi");
-    job.setJarByClass(TestMultipleOutputs2TypedApi.class);
-    Configuration conf = job.getConfiguration();
-    conf.set("table.output.tfile.compression", "gz");
-    conf.set("sortKey", sortKey);
-    // input settings
-    job.setInputFormatClass(TextInputFormat.class);
-    job.setMapperClass(TestMultipleOutputs2TypedApi.MapClass.class);
-    job.setMapOutputKeyClass(BytesWritable.class);
-    job.setMapOutputValueClass(ZebraTuple.class);
-
-    FileInputFormat.setInputPaths(job, inputPath);
-
-    // TODO: find a replacement
-    //job.setNumMapTasks(1);
-
-    // output settings
-
-    job.setOutputFormatClass(BasicTableOutputFormat.class);
-    
-    String schema = "word:string, count:int";
-    String storageHint = "[word];[count]";
-    BasicTableOutputFormat.setMultipleOutputs(job,
-        TestMultipleOutputs2TypedApi.OutputPartitionerClass.class, paths);
-    ZebraSchema zSchema = ZebraSchema.createZebraSchema(schema);
-    ZebraStorageHint zStorageHint = ZebraStorageHint
-        .createZebraStorageHint(storageHint);
-    ZebraSortInfo zSortInfo = ZebraSortInfo.createZebraSortInfo(sortKey, null);
-    BasicTableOutputFormat.setStorageInfo(job, zSchema, zStorageHint,
-        zSortInfo);
-
-    // set map-only job.
-    job.setNumReduceTasks(1);
-    job.submit();
-    job.waitForCompletion( true );
-    BasicTableOutputFormat.close( job );
-  }
-
-  @Override
-  public int run(String[] args) throws Exception {
-    TestMultipleOutputs2TypedApi test = new TestMultipleOutputs2TypedApi();
-    TestMultipleOutputs2TypedApi.setUpOnce();
-    test.test1();
-    test.test2();
-    test.test3();
-
-    return 0;
-  }
-  
-  public static void main(String[] args) throws Exception {
-    conf = new Configuration();
-    
-    int res = ToolRunner.run(conf, new TestMultipleOutputs2TypedApi(), args);
-    System.out.println("PASS");
-    System.exit(res);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestMultipleOutputs3.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestMultipleOutputs3.java
deleted file mode 100644
index 9251ff350..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestMultipleOutputs3.java
+++ /dev/null
@@ -1,510 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.BufferedWriter;
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.FileWriter;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.Iterator;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.hadoop.zebra.mapreduce.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.mapreduce.ZebraOutputPartition;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.types.ZebraTuple;
-import org.apache.pig.ExecType;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.Tuple;
-import org.junit.BeforeClass;
-import org.junit.Test;
-import org.apache.hadoop.zebra.BaseTestCase;
-
-/**
- * This is a sample a complete MR sample code for Table. It doens't contain
- * 'read' part. But, it should be similar and easier to write. Refer to test
- * cases in the same directory.
- * 
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * us 2
- * japan 2
- * india 4
- * us 2
- * japan 1
- * india 3
- * nouse 5
- * nowhere 4
- * 
- * 
- */
-public class TestMultipleOutputs3 extends BaseTestCase implements Tool {
-
-  static String inputPath;
-  static String inputFileName = "multi-input.txt";
-  public static String sortKey = null;
-
-  private static String strTable1 = null;
-  private static String strTable2 = null;
-
-  @BeforeClass
-  public static void setUpOnce() throws Exception {
-    init();
-    
-    inputPath = getTableFullPath(inputFileName).toString();
-
-    writeToFile(inputPath);
-  }
-
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    StringTokenizer st = new StringTokenizer(t, ".");
-    String methodName = null;
-    while (st.hasMoreTokens()) {
-      methodName = st.nextToken();
-    }
-    return methodName;
-  }
-  
-  public static void writeToFile (String inputFile) throws IOException{
-    if (mode == TestMode.local) {
-      FileWriter fstream = new FileWriter(inputFile);
-      BufferedWriter out = new BufferedWriter(fstream);
-      out.write("us 2\n");
-      out.write("japan 2\n");
-      out.write("india 4\n");
-      out.write("us 2\n");
-      out.write("japan 1\n");
-      out.write("india 3\n");
-      out.write("nouse 5\n");
-      out.write("nowhere 4\n");
-      out.close();
-    }
-    
-    if (mode == TestMode.cluster) {
-      FSDataOutputStream fout = fs.create(new Path (inputFile));
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 2\n");
-      fout.writeBytes("india 4\n");
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 1\n");
-      fout.writeBytes("india 3\n");
-      fout.writeBytes("nouse 5\n");
-      fout.writeBytes("nowhere 4\n");
-      fout.close();
-    }
-  }
-
-  public static void getTablePaths(String myMultiLocs) {
-    StringTokenizer st = new StringTokenizer(myMultiLocs, ",");
-
-    // get how many tokens inside st object
-    System.out.println("tokens count: " + st.countTokens());
-    int count = 0;
-
-    // iterate st object to get more tokens from it
-    while (st.hasMoreElements()) {
-      count++;
-      String token = st.nextElement().toString();
-      if (count == 1)
-        strTable1 = token;
-      if (count == 2)
-        strTable2 = token;
-
-      System.out.println("token = " + token);
-    }
-  }
-
-  public static void checkTable(String myMultiLocs) throws IOException {
-    System.out.println("myMultiLocs:" + myMultiLocs);
-    System.out.println("sorgetTablePathst key:" + sortKey);
-
-    getTablePaths(myMultiLocs);
-    String query1 = null;
-    String query2 = null;
-
-    if (strTable1 != null) {
-
-      query1 = "records1 = LOAD '" + strTable1
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-    if (strTable2 != null) {
-      query2 = "records2 = LOAD '" + strTable2
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-
-    int count1 = 0;
-    int count2 = 0;
-
-    if (query1 != null) {
-      System.out.println(query1);
-      pigServer.registerQuery(query1);
-      Iterator<Tuple> it = pigServer.openIterator("records1");
-      while (it.hasNext()) {
-        count1++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-        // test 1 us table
-        if (query1.contains("test1") || query1.contains("test2")
-            || query1.contains("test3")) {
-
-          if (count1 == 1) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count1 == 2) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-        } // test1, test2
-
-      }// while
-      if (query1.contains("test1") || query1.contains("test2")
-          || query1.contains("test3")) {
-        Assert.assertEquals(2, count1);
-      }
-    }// if query1 != null
-
-    if (query2 != null) {
-      pigServer.registerQuery(query2);
-      Iterator<Tuple> it = pigServer.openIterator("records2");
-
-      while (it.hasNext()) {
-        count2++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-
-        // if test1 other table
-        if (query2.contains("test1")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-
-          if (count1 == 5) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test1
-        // if test2 other table
-        if (query2.contains("test2")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-
-          if (count1 == 5) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test2
-        // if test3 other table
-        if (query2.contains("test3")) {
-          if (count2 == 1) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 5) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-
-        }// if test3
-
-      }// while
-      if (query2.contains("test1") || query2.contains("test2")
-          || query2.contains("test3")) {
-        Assert.assertEquals(6, count2);
-      }
-    }// if query2 != null
-
-  }
- 
-  // TODO: No exception is thrown any more due to Hadoop 20 API. This test case
-  // should be re-designed in the future.
-  @Test//(expected = IOException.class)
-  public void test1() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test combine sort keys
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "word,count";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-    
-    String str1 = getTableFullPath("us" + methodName).toString();
-    String str2 = getTableFullPath("others" + methodName).toString();
-    myMultiLocs = str1 + "," + str2;
-    
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    runMR(myMultiLocs, sortKey);
-  //  checkTable(myMultiLocs);
-  //  Assert.fail("test 1 ,should have thrown IOExcepiton");
-    System.out.println("DONE test " + getCurrentMethodName());
-
-  }
-
-
-  static class MapClass extends
-      Mapper<LongWritable, Text, BytesWritable, Tuple> {
-    private BytesWritable bytesKey;
-    private Tuple tupleRow;
-    private Object javaObj;
-
-    @Override
-    public void map(LongWritable key, Text value, Context context)
-        throws IOException, InterruptedException {
-      // value should contain "word count"
-      String[] wdct = value.toString().split(" ");
-      if (wdct.length != 2) {
-        // LOG the error
-        return;
-      }
-
-      byte[] word = wdct[0].getBytes();
-      bytesKey.set(word, 0, word.length);
-      System.out.println("word: " + new String(word));
-      tupleRow.set(0, new String(word));
-      tupleRow.set(1, Integer.parseInt(wdct[1]));
-      System.out.println("count:  " + Integer.parseInt(wdct[1]));
-
-      // This key has to be created by user
-      /*
-       * Tuple userKey = new DefaultTuple(); userKey.append(new String(word));
-       * userKey.append(Integer.parseInt(wdct[1]));
-       */
-      System.out.println("in map, sortkey: " + sortKey);
-      Tuple userKey = new ZebraTuple();
-      if (sortKey.equalsIgnoreCase("word,count")) {
-        userKey.append(new String(word));
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("count")) {
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("word")) {
-        userKey.append(new String(word));
-      }
-
-      try {
-
-        /* New M/R Interface */
-        /* Converts user key to zebra BytesWritable key */
-        /* using sort key expr tree */
-        /* Returns a java base object */
-        /* Done for each user key */
-
-        bytesKey = BasicTableOutputFormat.getSortKey(javaObj, userKey);
-      } catch (Exception e) {
-
-      }
-
-      context.write(bytesKey, tupleRow);
-    }
-
-    @Override
-    public void setup(Context context) {
-      bytesKey = new BytesWritable();
-      sortKey = context.getConfiguration().get("sortKey");
-      try {
-        Schema outSchema = BasicTableOutputFormat.getSchema(context);
-        tupleRow = TypesUtils.createTuple(outSchema);
-        javaObj = BasicTableOutputFormat.getSortKeyGenerator(context);
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      } catch (org.apache.hadoop.zebra.parser.ParseException e) {
-        throw new RuntimeException(e);
-      }
-    }
-
-  }
-
-  static class ReduceClass extends
-      Reducer<BytesWritable, Tuple, BytesWritable, Tuple> {
-    Tuple outRow;
-
-    public void reduce(BytesWritable key, Iterator<Tuple> values, Context context)
-        throws IOException, InterruptedException {
-      try {
-        for (; values.hasNext();) {
-          context.write(key, values.next());
-        }
-      } catch (ExecException e) {
-        e.printStackTrace();
-      }
-    }
-
-  }
-
-  static class OutputPartitionerClass extends ZebraOutputPartition {
-
-    @Override
-    public int getOutputPartition(BytesWritable key, Tuple value) throws IndexOutOfBoundsException, ExecException{
-      try {
-        value.get(2);
-      } catch (IndexOutOfBoundsException e) {
-        return 0;
-      }
-      
-      // should not reach here
-      Assert.fail("in try, should have thrown exception");
-      return 0;
-    }
-
-  }
-
-  public void runMR(String myMultiLocs, String sortKey) throws ParseException,
-      IOException, Exception, org.apache.hadoop.zebra.parser.ParseException {
-
-    Job job = new Job(conf);
-    job.setJobName("TestMultipleOutputs3");
-    job.setJarByClass(TestMultipleOutputs3.class);
-    Configuration conf = job.getConfiguration();
-    conf.set("table.output.tfile.compression", "gz");
-    conf.set("sortKey", sortKey);
-    // input settings
-    job.setInputFormatClass(TextInputFormat.class);
-    job.setMapperClass(TestMultipleOutputs3.MapClass.class);
-    job.setMapOutputKeyClass(BytesWritable.class);
-    job.setMapOutputValueClass(ZebraTuple.class);
-    FileInputFormat.setInputPaths(job, inputPath);
-
-    // output settings
-    job.setOutputFormatClass(BasicTableOutputFormat.class);
-    BasicTableOutputFormat.setMultipleOutputs(job, myMultiLocs,
-        TestMultipleOutputs3.OutputPartitionerClass.class);
-
-    // set the logical schema with 2 columns
-    BasicTableOutputFormat.setSchema(job, "word:string, count:int");
-    // for demo purposes, create 2 physical column groups
-    BasicTableOutputFormat.setStorageHint(job, "[word];[count]");
-    BasicTableOutputFormat.setSortInfo(job, sortKey);
-    System.out.println("in runMR, sortkey: " + sortKey);
-    // set map-only job.
-    job.setNumReduceTasks(1);
-    job.submit();
-    job.waitForCompletion( true );
-    BasicTableOutputFormat.close( job );
-  }
-
-  @Override
-  public int run(String[] args) throws Exception {
-    TestMultipleOutputs3 test = new TestMultipleOutputs3();
-    TestMultipleOutputs3.setUpOnce();
-    test.test1();
-
-    return 0;
-  }
-  
-  public static void main(String[] args) throws Exception {
-    conf = new Configuration();
-    
-    int res = ToolRunner.run(conf, new TestMultipleOutputs3(), args);
-    System.out.println("PASS");    
-    System.exit(res);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestMultipleOutputs3TypedApi.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestMultipleOutputs3TypedApi.java
deleted file mode 100644
index 586e0d2cf..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestMultipleOutputs3TypedApi.java
+++ /dev/null
@@ -1,523 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.BufferedWriter;
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.FileWriter;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.hadoop.zebra.mapreduce.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.mapreduce.ZebraOutputPartition;
-import org.apache.hadoop.zebra.mapreduce.ZebraSchema;
-import org.apache.hadoop.zebra.mapreduce.ZebraSortInfo;
-import org.apache.hadoop.zebra.mapreduce.ZebraStorageHint;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.types.ZebraTuple;
-import org.apache.pig.ExecType;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DefaultTuple;
-import org.apache.pig.data.Tuple;
-import org.junit.BeforeClass;
-import org.junit.Test;
-import org.apache.hadoop.zebra.BaseTestCase;
-
-/**
- * This is a sample a complete MR sample code for Table. It doens't contain
- * 'read' part. But, it should be similar and easier to write. Refer to test
- * cases in the same directory.
- * 
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * us 2
- * japan 2
- * india 4
- * us 2
- * japan 1
- * india 3
- * nouse 5
- * nowhere 4
- * 
- * 
- */
-public class TestMultipleOutputs3TypedApi extends BaseTestCase implements Tool{
-
-  static String inputPath;
-  static String inputFileName = "multi-input.txt";
-  public static String sortKey = null;
-
-  private static String strTable1 = null;
-  private static String strTable2 = null;
-
-  @BeforeClass
-  public static void setUpOnce() throws Exception {
-    init();
-    
-    inputPath = getTableFullPath(inputFileName).toString();
-
-    writeToFile(inputPath);
-  }
-
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    StringTokenizer st = new StringTokenizer(t, ".");
-    String methodName = null;
-    while (st.hasMoreTokens()) {
-      methodName = st.nextToken();
-    }
-    return methodName;
-  }
-  
-  public static void writeToFile (String inputFile) throws IOException{
-    if (mode == TestMode.local) {
-      FileWriter fstream = new FileWriter(inputFile);
-      BufferedWriter out = new BufferedWriter(fstream);
-      out.write("us 2\n");
-      out.write("japan 2\n");
-      out.write("india 4\n");
-      out.write("us 2\n");
-      out.write("japan 1\n");
-      out.write("india 3\n");
-      out.write("nouse 5\n");
-      out.write("nowhere 4\n");
-      out.close();
-    }
-    
-    if (mode == TestMode.cluster) {
-      FSDataOutputStream fout = fs.create(new Path (inputFile));
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 2\n");
-      fout.writeBytes("india 4\n");
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 1\n");
-      fout.writeBytes("india 3\n");
-      fout.writeBytes("nouse 5\n");
-      fout.writeBytes("nowhere 4\n");
-      fout.close();
-    }
-  }
-
-  public static void getTablePaths(String myMultiLocs) {
-    StringTokenizer st = new StringTokenizer(myMultiLocs, ",");
-
-    // get how many tokens inside st object
-    System.out.println("tokens count: " + st.countTokens());
-    int count = 0;
-
-    // iterate st object to get more tokens from it
-    while (st.hasMoreElements()) {
-      count++;
-      String token = st.nextElement().toString();
-      if (count == 1)
-        strTable1 = token;
-      if (count == 2)
-        strTable2 = token;
-      System.out.println("token = " + token);
-    }
-  }
-
-  public static void checkTable(String myMultiLocs) throws IOException {
-    System.out.println("myMultiLocs:" + myMultiLocs);
-    System.out.println("sorgetTablePathst key:" + sortKey);
-
-    getTablePaths(myMultiLocs);
-    String query1 = null;
-    String query2 = null;
-
-    if (strTable1 != null) {
-
-      query1 = "records1 = LOAD '" + strTable1
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-    if (strTable2 != null) {
-      query2 = "records2 = LOAD '" + strTable2
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-
-    int count1 = 0;
-    int count2 = 0;
-
-    if (query1 != null) {
-      System.out.println(query1);
-      pigServer.registerQuery(query1);
-      Iterator<Tuple> it = pigServer.openIterator("records1");
-      while (it.hasNext()) {
-        count1++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-        // test 1 us table
-        if (query1.contains("test1") || query1.contains("test2")
-            || query1.contains("test3")) {
-
-          if (count1 == 1) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count1 == 2) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-        } // test1, test2
-
-      }// while
-      if (query1.contains("test1") || query1.contains("test2")
-          || query1.contains("test3")) {
-        Assert.assertEquals(2, count1);
-      }
-    }// if query1 != null
-
-    if (query2 != null) {
-      pigServer.registerQuery(query2);
-      Iterator<Tuple> it = pigServer.openIterator("records2");
-
-      while (it.hasNext()) {
-        count2++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-
-        // if test1 other table
-        if (query2.contains("test1")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-
-          if (count1 == 5) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test1
-        // if test2 other table
-        if (query2.contains("test2")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-
-          if (count1 == 5) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test2
-        // if test3 other table
-        if (query2.contains("test3")) {
-          if (count2 == 1) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 5) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-
-        }// if test3
-
-      }// while
-      if (query2.contains("test1") || query2.contains("test2")
-          || query2.contains("test3")) {
-        Assert.assertEquals(6, count2);
-      }
-    }// if query2 != null
-
-  }
- 
-  // TODO: No exception is thrown any more due to Hadoop 20 API. This test case
-  // should be re-designed in the future.
-  @Test//(expected = IOException.class)
-  public void test1() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test combine sort keys
-     */
-    System.out.println("******Starttt  testcase: " + getCurrentMethodName());
-    sortKey = "word,count";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-    List<Path> paths = new ArrayList<Path>(2);
-
-    Path path1 = getTableFullPath("us" + methodName);
-    Path path2 = getTableFullPath("others" + methodName);
-    myMultiLocs = path1.toString() + "," + path2.toString();
-    paths.add(path1);
-    paths.add(path2);
-    
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    runMR(sortKey, paths.toArray(new Path[2]));
-
-  //  checkTable(myMultiLocs);
-  //  Assert.fail("test 1 ,should have thrown IOExcepiton");
-    System.out.println("DONE test " + getCurrentMethodName());
-
-  }
-
-
-  static class MapClass extends
-      Mapper<LongWritable, Text, BytesWritable, Tuple> {
-    private BytesWritable bytesKey;
-    private Tuple tupleRow;
-    private Object javaObj;
-
-    @Override
-    public void map(LongWritable key, Text value, Context context)
-        throws IOException, InterruptedException {
-      // value should contain "word count"
-      String[] wdct = value.toString().split(" ");
-      if (wdct.length != 2) {
-        // LOG the error
-        return;
-      }
-
-      byte[] word = wdct[0].getBytes();
-      bytesKey.set(word, 0, word.length);
-      System.out.println("word: " + new String(word));
-      tupleRow.set(0, new String(word));
-      tupleRow.set(1, Integer.parseInt(wdct[1]));
-      System.out.println("count:  " + Integer.parseInt(wdct[1]));
-
-      // This key has to be created by user
-      /*
-       * Tuple userKey = new DefaultTuple(); userKey.append(new String(word));
-       * userKey.append(Integer.parseInt(wdct[1]));
-       */
-      System.out.println("in map, sortkey: " + sortKey);
-      Tuple userKey = new DefaultTuple();
-      if (sortKey.equalsIgnoreCase("word,count")) {
-        userKey.append(new String(word));
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("count")) {
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("word")) {
-        userKey.append(new String(word));
-      }
-
-      try {
-
-        /* New M/R Interface */
-        /* Converts user key to zebra BytesWritable key */
-        /* using sort key expr tree */
-        /* Returns a java base object */
-        /* Done for each user key */
-
-        bytesKey = BasicTableOutputFormat.getSortKey(javaObj, userKey);
-      } catch (Exception e) {
-
-      }
-
-      context.write(bytesKey, tupleRow);
-    }
-
-    @Override
-    public void setup(Context context) {
-      bytesKey = new BytesWritable();
-      sortKey = context.getConfiguration().get("sortKey");
-      try {
-        Schema outSchema = BasicTableOutputFormat.getSchema(context);
-        tupleRow = TypesUtils.createTuple(outSchema);
-        javaObj = BasicTableOutputFormat.getSortKeyGenerator(context);
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      } catch (org.apache.hadoop.zebra.parser.ParseException e) {
-        throw new RuntimeException(e);
-      }
-    }
-
-  }
-
-  static class ReduceClass extends
-      Reducer<BytesWritable, Tuple, BytesWritable, Tuple> {
-    Tuple outRow;
-
-    public void reduce(BytesWritable key, Iterator<Tuple> values, Context context)
-        throws IOException, InterruptedException {
-      try {
-        for (; values.hasNext();) {
-          context.write(key, values.next());
-        }
-      } catch (ExecException e) {
-        e.printStackTrace();
-      }
-    }
-
-  }
-
-  static class OutputPartitionerClass extends ZebraOutputPartition {
-
-    @Override
-    public int getOutputPartition(BytesWritable key, Tuple value) throws IndexOutOfBoundsException, ExecException{
-      try {
-        value.get(2);
-      } catch (IndexOutOfBoundsException e) {
-        return 0;
-      }
-      
-      // should not reach here
-      Assert.fail("int try, should have thrown exception");
-      return 0;
-    }
-  }
-
-  public void runMR(String sortKey, Path...paths) throws ParseException,
-      IOException, Exception, org.apache.hadoop.zebra.parser.ParseException {
-
-    Job job = new Job(conf);
-    job.setJobName("TestMultipleOutputs3TypedApi");
-    job.setJarByClass(TestMultipleOutputs3TypedApi.class);
-    Configuration conf = job.getConfiguration();
-    conf.set("table.output.tfile.compression", "gz");
-    conf.set("sortKey", sortKey);
-    // input settings
-    job.setInputFormatClass(TextInputFormat.class);
-    job.setMapperClass(TestMultipleOutputs3TypedApi.MapClass.class);
-    job.setMapOutputKeyClass(BytesWritable.class);
-    job.setMapOutputValueClass(ZebraTuple.class);
-    FileInputFormat.setInputPaths(job, inputPath);
-
-    //TODO: need to find a replacement
-    //job.setNumMapTasks(1);
-
-    // output settings
-
-    job.setOutputFormatClass(BasicTableOutputFormat.class);
-   
-    String schema = "word:string, count:int";
-    String storageHint = "[word];[count]";
-    BasicTableOutputFormat.setMultipleOutputs(job,
-        TestMultipleOutputs3TypedApi.OutputPartitionerClass.class, paths);
-    ZebraSchema zSchema = ZebraSchema.createZebraSchema(schema);
-    ZebraStorageHint zStorageHint = ZebraStorageHint
-        .createZebraStorageHint(storageHint);
-    ZebraSortInfo zSortInfo = ZebraSortInfo.createZebraSortInfo(sortKey, null);
-    BasicTableOutputFormat.setStorageInfo(job, zSchema, zStorageHint,
-        zSortInfo);
-    
-    job.setNumReduceTasks(1);
-    job.submit();
-    job.waitForCompletion( true );
-    BasicTableOutputFormat.close( job );
-  }
-
-  public int run(String[] args) throws Exception {
-    TestMultipleOutputs3TypedApi test = new TestMultipleOutputs3TypedApi();
-    TestMultipleOutputs3TypedApi.setUpOnce();
-    test.test1();
-
-    return 0;
-  }
-  
-  public static void main(String[] args) throws Exception {
-    conf = new Configuration();
-    
-    int res = ToolRunner.run(conf, new TestMultipleOutputs3TypedApi(), args);
-    System.out.println("PASS");    
-    System.exit(res);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestMultipleOutputs4.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestMultipleOutputs4.java
deleted file mode 100644
index 04f4af53c..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestMultipleOutputs4.java
+++ /dev/null
@@ -1,524 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.BufferedWriter;
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.FileWriter;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.Iterator;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.hadoop.zebra.mapreduce.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.mapreduce.ZebraOutputPartition;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.types.ZebraTuple;
-import org.apache.pig.ExecType;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.Tuple;
-import org.junit.BeforeClass;
-import org.junit.Test;
-import org.apache.hadoop.zebra.BaseTestCase;
-
-/**
- * This is a sample a complete MR sample code for Table. It doens't contain
- * 'read' part. But, it should be similar and easier to write. Refer to test
- * cases in the same directory.
- * 
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * us 2
- * japan 2
- * india 4
- * us 2
- * japan 1
- * india 3
- * nouse 5
- * nowhere 4
- * 
- * 
- */
-public class TestMultipleOutputs4 extends BaseTestCase implements Tool{
-
-  static String inputPath;
-  static String inputFileName = "multi-input.txt";
-  public static String sortKey = null;
-
-  private static String strTable1 = null;
-  private static String strTable2 = null;
-
-  @BeforeClass
-  public static void setUpOnce() throws Exception {
-    init();
-    
-    inputPath = getTableFullPath(inputFileName).toString();
-
-    writeToFile(inputPath);
-  }
-
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    StringTokenizer st = new StringTokenizer(t, ".");
-    String methodName = null;
-    while (st.hasMoreTokens()) {
-      methodName = st.nextToken();
-    }
-    return methodName;
-  }
-
-  public static void writeToFile (String inputFile) throws IOException{
-    if (mode == TestMode.local) {
-      FileWriter fstream = new FileWriter(inputFile);
-      BufferedWriter out = new BufferedWriter(fstream);
-      out.write("us 2\n");
-      out.write("japan 2\n");
-      out.write("india 4\n");
-      out.write("us 2\n");
-      out.write("japan 1\n");
-      out.write("india 3\n");
-      out.write("nouse 5\n");
-      out.write("nowhere 4\n");
-      out.close();
-    }
-
-    if (mode == TestMode.cluster) {
-      FSDataOutputStream fout = fs.create(new Path (inputFile));
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 2\n");
-      fout.writeBytes("india 4\n");
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 1\n");
-      fout.writeBytes("india 3\n");
-      fout.writeBytes("nouse 5\n");
-      fout.writeBytes("nowhere 4\n");
-      fout.close();
-    }
-  }
-  
-  public static void getTablePaths(String myMultiLocs) {
-    StringTokenizer st = new StringTokenizer(myMultiLocs, ",");
-
-    // get how many tokens inside st object
-    System.out.println("tokens count: " + st.countTokens());
-    int count = 0;
-
-    // iterate st object to get more tokens from it
-    while (st.hasMoreElements()) {
-      count++;
-      String token = st.nextElement().toString();
-      if (mode == TestMode.local) {
-        System.out.println("in mini, token: "+token); 
-        //in mini, token: file:/homes/<uid>/grid/multipleoutput/pig-table/contrib/zebra/ustest3
-        if (count == 1)
-          strTable1 = token;
-        if (count == 2)
-          strTable2 = token;
-      }
-      
-      if (mode == TestMode.cluster) {
-        System.out.println("in real, token: "+token);
-        //in real, token: /user/hadoopqa/ustest3
-        //note: no prefix file:  in real cluster
-        if (count == 1)
-          strTable1 = token;
-        if (count == 2)
-          strTable2 = token;
-      }
-      
-    }
-  }
-  
-  public static void checkTableExists(boolean expected, String strDir) throws IOException{
-    File theDir = null; 
-    boolean actual = false;
-     
-    theDir = new File(strDir);
-    actual = fs.exists(new Path (theDir.toString()));
-     
-    System.out.println("the dir : "+ theDir.toString());
-    if (actual != expected){
-      Assert.fail("dir exists or not is different from what expected.");
-    }
-  }
-  
-  public static void checkTable(String myMultiLocs) throws IOException {
-    System.out.println("myMultiLocs:" + myMultiLocs);
-    System.out.println("sorgetTablePathst key:" + sortKey);
-
-    getTablePaths(myMultiLocs);
-    String query1 = null;
-    String query2 = null;
- 
-    if (strTable1 != null) {
-
-      query1 = "records1 = LOAD '" + strTable1
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-    if (strTable2 != null) {
-      query2 = "records2 = LOAD '" + strTable2
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-
-    int count1 = 0;
-    int count2 = 0;
-  
-    if (query1 != null) {
-      System.out.println(query1);
-      pigServer.registerQuery(query1);
-      Iterator<Tuple> it = pigServer.openIterator("records1");
-      while (it.hasNext()) {
-        count1++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-        // test 1 us table
-        if (query1.contains("test1") || query1.contains("test2")|| query1.contains("test3")) {
-          
-          if (count1 == 1) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count1 == 2) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-        } // test1, test2
-    
-      }// while
-      if (query1.contains("test1") || query1.contains("test2")
-          || query1.contains("test3")) {
-        Assert.assertEquals(2, count1);
-      }
-    }// if query1 != null
-
-    if (query2 != null) {
-      pigServer.registerQuery(query2);
-      Iterator<Tuple> it = pigServer.openIterator("records2");
-
-      while (it.hasNext()) {
-        count2++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-
-        // if test1 other table
-        if (query2.contains("test1")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          
-          if (count1 == 5) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test1
-     // if test2 other table
-        if (query2.contains("test2")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          
-          if (count1 == 5) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test2
-        // if test3 other table
-        if (query2.contains("test3")) {
-          if (count2 == 1) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 5) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          
-        }// if test3
-        
-      }// while
-      if (query2.contains("test1") || query2.contains("test2")
-          || query2.contains("test3")) {
-        Assert.assertEquals(6, count2);
-      }
-    }// if query2 != null
-
-  }
-
-  
-  @Test
-  public void test1() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test combine sort keys
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "word,count";
-    System.out.println("hello sort on word and count");
-    String myMultiLocs = null;
-    
-    myMultiLocs = getTableFullPath("a").toString();
-    
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    runMR(myMultiLocs, sortKey);
-    checkTableExists(true, strTable1);
-    System.out.println("DONE test " + getCurrentMethodName());
-  }
-
-
-  static class MapClass extends
-      Mapper<LongWritable, Text, BytesWritable, Tuple> {
-    private BytesWritable bytesKey;
-    private Tuple tupleRow;
-    private Object javaObj;
-
-    @Override
-    public void map(LongWritable key, Text value, Context context)
-        throws IOException, InterruptedException {
-      // value should contain "word count"
-      String[] wdct = value.toString().split(" ");
-      if (wdct.length != 2) {
-        // LOG the error
-        return;
-      }
-
-      byte[] word = wdct[0].getBytes();
-      bytesKey.set(word, 0, word.length);
-      System.out.println("word: " + new String(word));
-      tupleRow.set(0, new String(word));
-      tupleRow.set(1, Integer.parseInt(wdct[1]));
-      System.out.println("count:  " + Integer.parseInt(wdct[1]));
-
-      // This key has to be created by user
-      /*
-       * Tuple userKey = new DefaultTuple(); userKey.append(new String(word));
-       * userKey.append(Integer.parseInt(wdct[1]));
-       */
-      System.out.println("in map, sortkey: " + sortKey);
-      Tuple userKey = new ZebraTuple();
-      if (sortKey.equalsIgnoreCase("word,count")) {
-        userKey.append(new String(word));
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("count")) {
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("word")) {
-        userKey.append(new String(word));
-      }
-
-      try {
-
-        /* New M/R Interface */
-        /* Converts user key to zebra BytesWritable key */
-        /* using sort key expr tree */
-        /* Returns a java base object */
-        /* Done for each user key */
-
-        bytesKey = BasicTableOutputFormat.getSortKey(javaObj, userKey);
-      } catch (Exception e) {
-
-      }
-
-      context.write(bytesKey, tupleRow);
-    }
-
-    @Override
-    public void setup(Context context) {
-      bytesKey = new BytesWritable();
-      sortKey = context.getConfiguration().get("sortKey");
-      try {
-        Schema outSchema = BasicTableOutputFormat.getSchema(context);
-        tupleRow = TypesUtils.createTuple(outSchema);
-        javaObj = BasicTableOutputFormat.getSortKeyGenerator(context);
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      } catch (org.apache.hadoop.zebra.parser.ParseException e) {
-        throw new RuntimeException(e);
-      }
-    }
-
-  }
-
-  static class ReduceClass extends
-      Reducer<BytesWritable, Tuple, BytesWritable, Tuple> {
-    Tuple outRow;
-
-    public void reduce(BytesWritable key, Iterator<Tuple> values, Context context)
-        throws IOException, InterruptedException {
-      try {
-        for (; values.hasNext();) {
-          context.write(key, values.next());
-        }
-      } catch (ExecException e) {
-        e.printStackTrace();
-      }
-    }
-
-  }
-
-  static class OutputPartitionerClass extends ZebraOutputPartition {
-
-    @Override
-    public int getOutputPartition(BytesWritable key, Tuple value) throws ExecException {
-      
-      return 0;
-    }
-
-  }
-
-  public void runMR(String myMultiLocs, String sortKey) throws ParseException,
-      IOException, Exception, org.apache.hadoop.zebra.parser.ParseException {
-
-    Job job = new Job(conf);
-    job.setJobName("TestMultipleOutputs4");
-    job.setJarByClass(TestMultipleOutputs4.class);
-    Configuration conf = job.getConfiguration();
-    conf.set("table.output.tfile.compression", "gz");
-    conf.set("sortKey", sortKey);
-    // input settings
-    job.setInputFormatClass(TextInputFormat.class);
-    job.setMapperClass(TestMultipleOutputs4.MapClass.class);
-    job.setMapOutputKeyClass(BytesWritable.class);
-    job.setMapOutputValueClass(ZebraTuple.class);
-    FileInputFormat.setInputPaths(job, inputPath);
-
-    // TODO: find a replacement
-    //job.setNumMapTasks(1);
-
-    // output settings
-
-    job.setOutputFormatClass(BasicTableOutputFormat.class);
-    BasicTableOutputFormat.setMultipleOutputs(job, myMultiLocs,
-        TestMultipleOutputs4.OutputPartitionerClass.class);
-
-    // set the logical schema with 2 columns
-    BasicTableOutputFormat.setSchema(job, "word:string, count:int");
-    // for demo purposes, create 2 physical column groups
-    BasicTableOutputFormat.setStorageHint(job, "[word];[count]");
-    BasicTableOutputFormat.setSortInfo(job, sortKey);
-    System.out.println("in runMR, sortkey: " + sortKey);
-    // set map-only job.
-    job.setNumReduceTasks(1);
-    job.submit();
-    job.waitForCompletion( true );
-    BasicTableOutputFormat.close( job );
-  }
-
-  public int run (String[] args) throws Exception {
-    TestMultipleOutputs4 test = new TestMultipleOutputs4();
-    TestMultipleOutputs4.setUpOnce();
-   
-   test.test1();
-   return 0;
-  }
-  
-  public static void main(String[] args) throws Exception {
-    conf = new Configuration();
-    
-    int res = ToolRunner.run(conf, new TestMultipleOutputs4(), args);
-    System.out.println("PASS");    
-    System.exit(res);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestMultipleOutputs4TypedApi.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestMultipleOutputs4TypedApi.java
deleted file mode 100644
index 76885e746..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestMultipleOutputs4TypedApi.java
+++ /dev/null
@@ -1,541 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.BufferedWriter;
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.FileWriter;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.hadoop.zebra.mapreduce.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.mapreduce.ZebraOutputPartition;
-import org.apache.hadoop.zebra.mapreduce.ZebraSchema;
-import org.apache.hadoop.zebra.mapreduce.ZebraSortInfo;
-import org.apache.hadoop.zebra.mapreduce.ZebraStorageHint;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.types.ZebraTuple;
-import org.apache.pig.ExecType;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DefaultTuple;
-import org.apache.pig.data.Tuple;
-import org.junit.BeforeClass;
-import org.junit.Test;
-import org.apache.hadoop.zebra.BaseTestCase;
-
-/**
- * This is a sample a complete MR sample code for Table. It doens't contain
- * 'read' part. But, it should be similar and easier to write. Refer to test
- * cases in the same directory.
- * 
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * us 2
- * japan 2
- * india 4
- * us 2
- * japan 1
- * india 3
- * nouse 5
- * nowhere 4
- * 
- * 
- */
-public class TestMultipleOutputs4TypedApi extends BaseTestCase implements Tool {
-
-  static String inputPath;
-  static String inputFileName = "multi-input.txt";
-
-  public static String sortKey = null;
-
-  private static String strTable1 = null;
-  private static String strTable2 = null;
-
-  @BeforeClass
-  public static void setUpOnce() throws Exception {
-    init();
-    
-    inputPath = getTableFullPath(inputFileName).toString();
-
-    writeToFile(inputPath);
-  }
-
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    StringTokenizer st = new StringTokenizer(t, ".");
-    String methodName = null;
-    while (st.hasMoreTokens()) {
-      methodName = st.nextToken();
-    }
-    return methodName;
-  }
-
-  public static void writeToFile(String inputFile) throws IOException {
-    if (mode == TestMode.local) {
-      FileWriter fstream = new FileWriter(inputFile);
-      BufferedWriter out = new BufferedWriter(fstream);
-      out.write("us 2\n");
-      out.write("japan 2\n");
-      out.write("india 4\n");
-      out.write("us 2\n");
-      out.write("japan 1\n");
-      out.write("india 3\n");
-      out.write("nouse 5\n");
-      out.write("nowhere 4\n");
-      out.close();
-    }
-    
-    if (mode == TestMode.cluster) {
-      FSDataOutputStream fout = fs.create(new Path(inputFile));
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 2\n");
-      fout.writeBytes("india 4\n");
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 1\n");
-      fout.writeBytes("india 3\n");
-      fout.writeBytes("nouse 5\n");
-      fout.writeBytes("nowhere 4\n");
-      fout.close();
-    }
-  }
-
-  public static void getTablePaths(String myMultiLocs) {
-    StringTokenizer st = new StringTokenizer(myMultiLocs, ",");
-
-    // get how many tokens inside st object
-    System.out.println("tokens count: " + st.countTokens());
-    int count = 0;
-
-    // iterate st object to get more tokens from it
-    while (st.hasMoreElements()) {
-      count++;
-      String token = st.nextElement().toString();
-      if (mode == TestMode.local) {
-        System.out.println("in mini, token: " + token);
-        // in mini, token:
-        // file:/homes/<uid>/grid/multipleoutput/pig-table/contrib/zebra/ustest3
-        if (count == 1)
-          strTable1 = token;
-        if (count == 2)
-          strTable2 = token;
-      }
-      
-      if (mode == TestMode.cluster) {
-        System.out.println("in real, token: " + token);
-        // in real, token: /user/hadoopqa/ustest3
-        // note: no prefix file: in real cluster
-        if (count == 1)
-          strTable1 = token;
-        if (count == 2)
-          strTable2 = token;
-      }
-    }
-  }
-
-  public static void checkTableExists(boolean expected, String strDir)
-      throws IOException {
-
-    File theDir = null;
-    boolean actual = false;
-
-    theDir = new File(strDir);
-    actual = fs.exists(new Path(theDir.toString()));
-
-    System.out.println("the dir : " + theDir.toString());
-
-    if (actual != expected) {
-      Assert.fail("dir exists or not is different from what expected.");
-    }
-  }
-
-  public static void checkTable(String myMultiLocs) throws IOException {
-    System.out.println("myMultiLocs:" + myMultiLocs);
-    System.out.println("sorgetTablePathst key:" + sortKey);
-
-    getTablePaths(myMultiLocs);
-    String query1 = null;
-    String query2 = null;
-
-    if (strTable1 != null) {
-
-      query1 = "records1 = LOAD '" + strTable1
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-    if (strTable2 != null) {
-      query2 = "records2 = LOAD '" + strTable2
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-
-    int count1 = 0;
-    int count2 = 0;
-
-    if (query1 != null) {
-      System.out.println(query1);
-      pigServer.registerQuery(query1);
-      Iterator<Tuple> it = pigServer.openIterator("records1");
-      while (it.hasNext()) {
-        count1++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-        // test 1 us table
-        if (query1.contains("test1") || query1.contains("test2")
-            || query1.contains("test3")) {
-
-          if (count1 == 1) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count1 == 2) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-        } // test1, test2
-
-      }// while
-      if (query1.contains("test1") || query1.contains("test2")
-          || query1.contains("test3")) {
-        Assert.assertEquals(2, count1);
-      }
-    }// if query1 != null
-
-    if (query2 != null) {
-      pigServer.registerQuery(query2);
-      Iterator<Tuple> it = pigServer.openIterator("records2");
-
-      while (it.hasNext()) {
-        count2++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-
-        // if test1 other table
-        if (query2.contains("test1")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-
-          if (count1 == 5) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test1
-        // if test2 other table
-        if (query2.contains("test2")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-
-          if (count1 == 5) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test2
-        // if test3 other table
-        if (query2.contains("test3")) {
-          if (count2 == 1) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 5) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-
-        }// if test3
-
-      }// while
-      if (query2.contains("test1") || query2.contains("test2")
-          || query2.contains("test3")) {
-        Assert.assertEquals(6, count2);
-      }
-    }// if query2 != null
-
-  }
-
-  @Test
-  public void test1() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test combine sort keys
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "word,count";
-    System.out.println("hello sort on word and count");
-    String myMultiLocs = null;
-    List<Path> paths = new ArrayList<Path>(1);
-    
-    Path path1 = getTableFullPath("a");
-    paths.add(path1);
-    myMultiLocs = path1.toString();
-
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    runMR(sortKey, paths.toArray(new Path[1]));
-    checkTableExists(true, strTable1);
-    System.out.println("DONE test " + getCurrentMethodName());
-  }
-
-  static class MapClass extends
-      Mapper<LongWritable, Text, BytesWritable, Tuple> {
-    private BytesWritable bytesKey;
-    private Tuple tupleRow;
-    private Object javaObj;
-
-    @Override
-    public void map(LongWritable key, Text value, Context context)
-        throws IOException, InterruptedException {
-      // value should contain "word count"
-      String[] wdct = value.toString().split(" ");
-      if (wdct.length != 2) {
-        // LOG the error
-        return;
-      }
-
-      byte[] word = wdct[0].getBytes();
-      bytesKey.set(word, 0, word.length);
-      System.out.println("word: " + new String(word));
-      tupleRow.set(0, new String(word));
-      tupleRow.set(1, Integer.parseInt(wdct[1]));
-      System.out.println("count:  " + Integer.parseInt(wdct[1]));
-
-      // This key has to be created by user
-      /*
-       * Tuple userKey = new DefaultTuple(); userKey.append(new String(word));
-       * userKey.append(Integer.parseInt(wdct[1]));
-       */
-      System.out.println("in map, sortkey: " + sortKey);
-      Tuple userKey = new DefaultTuple();
-      if (sortKey.equalsIgnoreCase("word,count")) {
-        userKey.append(new String(word));
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("count")) {
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("word")) {
-        userKey.append(new String(word));
-      }
-
-      try {
-
-        /* New M/R Interface */
-        /* Converts user key to zebra BytesWritable key */
-        /* using sort key expr tree */
-        /* Returns a java base object */
-        /* Done for each user key */
-
-        bytesKey = BasicTableOutputFormat.getSortKey(javaObj, userKey);
-      } catch (Exception e) {
-
-      }
-
-      context.write(bytesKey, tupleRow);
-    }
-
-    @Override
-    public void setup(Context context) {
-      bytesKey = new BytesWritable();
-      sortKey = context.getConfiguration().get("sortKey");
-      try {
-        Schema outSchema = BasicTableOutputFormat.getSchema(context);
-        tupleRow = TypesUtils.createTuple(outSchema);
-        javaObj = BasicTableOutputFormat.getSortKeyGenerator(context);
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      } catch (org.apache.hadoop.zebra.parser.ParseException e) {
-        throw new RuntimeException(e);
-      }
-    }
-
-  }
-
-  static class ReduceClass extends
-      Reducer<BytesWritable, Tuple, BytesWritable, Tuple> {
-    Tuple outRow;
-
-    public void reduce(BytesWritable key, Iterator<Tuple> values, Context context)
-        throws IOException, InterruptedException {
-      try {
-        for (; values.hasNext();) {
-          context.write(key, values.next());
-        }
-      } catch (ExecException e) {
-        e.printStackTrace();
-      }
-    }
-
-  }
-
-  static class OutputPartitionerClass extends ZebraOutputPartition {
-
-    @Override
-    public int getOutputPartition(BytesWritable key, Tuple value)
-        throws ExecException {
-
-      return 0;
-    }
-
-  }
-
-  public void runMR(String sortKey, Path... paths) throws ParseException,
-      IOException, Exception, org.apache.hadoop.zebra.parser.ParseException {
-
-    Job job = new Job(conf);
-    job.setJobName("TestMultipleOutputs4TypedApi");
-    job.setJarByClass(TestMultipleOutputs4TypedApi.class);
-    Configuration conf = job.getConfiguration();
-    conf.set("table.output.tfile.compression", "gz");
-    conf.set("sortKey", sortKey);
-    // input settings
-    job.setInputFormatClass(TextInputFormat.class);
-    job.setMapperClass(TestMultipleOutputs4TypedApi.MapClass.class);
-    job.setMapOutputKeyClass(BytesWritable.class);
-    job.setMapOutputValueClass(ZebraTuple.class);
-    FileInputFormat.setInputPaths(job, inputPath);
-
-    // TODO:
-    //job.setNumMapTasks(1);
-
-    // output settings
-
-    job.setOutputFormatClass(BasicTableOutputFormat.class);
-
-    String schema = "word:string, count:int";
-    String storageHint = "[word];[count]";
-    BasicTableOutputFormat.setMultipleOutputs(job,
-        TestMultipleOutputs4TypedApi.OutputPartitionerClass.class, paths);
-    ZebraSchema zSchema = ZebraSchema.createZebraSchema(schema);
-    ZebraStorageHint zStorageHint = ZebraStorageHint
-        .createZebraStorageHint(storageHint);
-    ZebraSortInfo zSortInfo = ZebraSortInfo.createZebraSortInfo(sortKey, null);
-    BasicTableOutputFormat.setStorageInfo(job, zSchema, zStorageHint,
-        zSortInfo);
-
-    // set map-only job.
-    job.setNumReduceTasks(1);
-    job.submit();
-    job.waitForCompletion( true );
-    BasicTableOutputFormat.close( job );
-  }
-
-  public int run(String[] args) throws Exception {
-    TestMultipleOutputs4TypedApi test = new TestMultipleOutputs4TypedApi();
-    TestMultipleOutputs4TypedApi.setUpOnce();
-
-    test.test1();
-    
-    return 0;
-  }
-  
-  public static void main(String[] args) throws Exception {
-    conf = new Configuration();
-    
-    int res = ToolRunner.run(conf, new TestMultipleOutputs4TypedApi(), args);
-    System.out.println("PASS");    
-    System.exit(res);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestMultipleOutputsTypeApi.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestMultipleOutputsTypeApi.java
deleted file mode 100644
index ba1761903..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestMultipleOutputsTypeApi.java
+++ /dev/null
@@ -1,634 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.BufferedWriter;
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.FileWriter;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.mapreduce.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.mapreduce.ZebraOutputPartition;
-import org.apache.hadoop.zebra.mapreduce.ZebraSchema;
-import org.apache.hadoop.zebra.mapreduce.ZebraSortInfo;
-import org.apache.hadoop.zebra.mapreduce.ZebraStorageHint;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.types.ZebraTuple;
-import org.apache.pig.ExecType;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DefaultTuple;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * This is a sample a complete MR sample code for Table. It doens't contain
- * 'read' part. But, it should be similar and easier to write. Refer to test
- * cases in the same directory.
- * 
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * us 2
- * japan 2
- * india 4
- * us 2
- * japan 1
- * india 3
- * nouse 5
- * nowhere 4
- * 
- * 
- */
-public class TestMultipleOutputsTypeApi extends BaseTestCase implements Tool{
-
-  static String inputPath;
-  static String inputFileName = "multi-input.txt";
-  public static String sortKey = null;
-
-  private static String strTable1 = null;
-  private static String strTable2 = null;
-  private static String strTable3 = null;
-
-  @BeforeClass
-  public static void setUpOnce() throws Exception {
-    init();
-    
-    inputPath = getTableFullPath(inputFileName).toString();
-
-    writeToFile(inputPath);
-  }
-
-  @AfterClass
-  public static void tearDown() throws Exception {
-    if (mode == TestMode.local) {
-      pigServer.shutdown();
-    }
-  }
-
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    StringTokenizer st = new StringTokenizer(t, ".");
-    String methodName = null;
-    while (st.hasMoreTokens()) {
-      methodName = st.nextToken();
-    }
-    return methodName;
-  }
-
-  public static void writeToFile(String inputFile) throws IOException {
-    if (mode == TestMode.local) {
-      FileWriter fstream = new FileWriter(inputFile);
-      BufferedWriter out = new BufferedWriter(fstream);
-      out.write("us 2\n");
-      out.write("japan 2\n");
-      out.write("india 4\n");
-      out.write("us 2\n");
-      out.write("japan 1\n");
-      out.write("india 3\n");
-      out.write("nouse 5\n");
-      out.write("nowhere 4\n");
-      out.close();
-    }
-
-    if (mode == TestMode.cluster) {
-      FSDataOutputStream fout = fs.create(new Path(inputFile));
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 2\n");
-      fout.writeBytes("india 4\n");
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 1\n");
-      fout.writeBytes("india 3\n");
-      fout.writeBytes("nouse 5\n");
-      fout.writeBytes("nowhere 4\n");
-      fout.close();
-    }
-  }
-
-  public static void getTablePaths(String myMultiLocs) {
-    StringTokenizer st = new StringTokenizer(myMultiLocs, ",");
-
-    // get how many tokens inside st object
-    System.out.println("tokens count: " + st.countTokens());
-    int count = 0;
-
-    // iterate st object to get more tokens from it
-    while (st.hasMoreElements()) {
-      count++;
-      String token = st.nextElement().toString();
-      if (count == 1)
-        strTable1 = token;
-      if (count == 2)
-        strTable2 = token;
-      if (count == 3)
-        strTable3 = token;
-      System.out.println("token = " + token);
-    }
-  }
-
-  public static void checkTable(String myMultiLocs) throws IOException {
-    System.out.println("myMultiLocs:" + myMultiLocs);
-    System.out.println("sorgetTablePathst key:" + sortKey);
-
-    getTablePaths(myMultiLocs);
-    String query1 = null;
-    String query2 = null;
-    String query3 = null;
-
-    if (strTable1 != null) {
-
-      query1 = "records1 = LOAD '" + strTable1
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-    if (strTable2 != null) {
-      query2 = "records2 = LOAD '" + strTable2
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-    if (strTable3 != null) {
-      query3 = "records3 = LOAD '" + strTable3
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-
-    int count1 = 0;
-    int count2 = 0;
-    int count3 = 0;
-
-    if (query1 != null) {
-      System.out.println(query1);
-      pigServer.registerQuery(query1);
-      Iterator<Tuple> it = pigServer.openIterator("records1");
-      while (it.hasNext()) {
-        count1++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-        // test 1 us table
-        if (query1.contains("test1") || query1.contains("test2")) {
-          if (count1 == 1) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 2) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 3) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count1 == 4) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-        } // test1, test2
-        if (query1.contains("test3")) {
-          if (count1 == 1) {
-            System.out.println("test3: rowvalue: " + RowValue.toString());
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count1 == 2) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count1 == 3) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 4) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-        } // test3
-      }// while
-      if (query1.contains("test1") || query1.contains("test2")
-          || query1.contains("test3")) {
-        Assert.assertEquals(4, count1);
-      }
-    }// if query1 != null
-
-    if (query2 != null) {
-      pigServer.registerQuery(query2);
-      Iterator<Tuple> it = pigServer.openIterator("records2");
-
-      while (it.hasNext()) {
-        count2++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-
-        // if test1 india table
-        if (query2.contains("test1")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test1
-        if (query1.contains("test2")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-        }// if test2
-
-        if (query1.contains("test3")) {
-          if (count1 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count1 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        } // test3
-
-      }// while
-      if (query2.contains("test1") || query2.contains("test2")
-          || query2.contains("test3")) {
-        Assert.assertEquals(2, count2);
-      }
-    }// if query2 != null
-    // test 1 us table should have 4 lines
-
-    if (query3 != null) {
-      pigServer.registerQuery(query3);
-      Iterator<Tuple> it = pigServer.openIterator("records3");
-
-      while (it.hasNext()) {
-        count3++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-
-        // if test1 japan table
-        if (query3.contains("test1")) {
-          if (count3 == 1) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count3 == 2) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-        }// if test1 test2 japan table
-
-        if (query3.contains("test2")) {
-          if (count3 == 1) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count3 == 2) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-        }// test2
-        if (query1.contains("test3")) {
-          if (count1 == 1) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count1 == 2) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-        } // test3
-
-      }// while
-      if (query3.contains("test1") || query3.contains("test2")
-          || query3.contains("test3")) {
-        Assert.assertEquals(2, count3);
-      }
-    }// if query2 != null
-  }
-
-  @Test
-  public void test1() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test combine sort keys
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "word,count";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-    List<Path> paths = new ArrayList<Path>(3);
-
-    Path path1 = getTableFullPath("us" + methodName);
-    Path path2 = getTableFullPath("india" + methodName);
-    Path path3 = getTableFullPath("japan" + methodName);
-    myMultiLocs = path1.toString() + "," + path2.toString() + "," + path3.toString();
-    paths.add(path1);
-    paths.add(path2);
-    paths.add(path3);
-    
-    getTablePaths(myMultiLocs);
-    System.out.println("strTable1: " + strTable1.toString());
-    System.out.println("strTable2: " + strTable2.toString());
-    System.out.println("strTable3: " + strTable3.toString());
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    removeDir(new Path(strTable3));
-    runMR(sortKey, paths.toArray(new Path[3]));
-    checkTable(myMultiLocs);
-    System.out.println("DONE test 1");
-
-  }
-
-  @Test
-  public void test2() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test 'word' sort key
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "word";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-    List<Path> paths = new ArrayList<Path>(3);
-   
-    Path path1 = getTableFullPath("us" + methodName);
-    Path path2 = getTableFullPath("india" + methodName);
-    Path path3 = getTableFullPath("japan" + methodName);
-    myMultiLocs = path1.toString() + "," + path2.toString() + "," + path3.toString();
-    paths.add(path1);
-    paths.add(path2);
-    paths.add(path3);
-    
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    removeDir(new Path(strTable3));
-    runMR(sortKey, paths.toArray(new Path[3]));
-    checkTable(myMultiLocs);
-    System.out.println("DONE test 2");
-  }
-
-  @Test
-  public void test3() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * test 'count' sort key
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "count";
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-    List<Path> paths = new ArrayList<Path>(3);
-
-    Path path1 = getTableFullPath("us" + methodName);
-    Path path2 = getTableFullPath("india" + methodName);
-    Path path3 = getTableFullPath("japan" + methodName);
-    myMultiLocs = path1.toString() + "," + path2.toString() + "," + path3.toString();
-    paths.add(path1);
-    paths.add(path2);
-    paths.add(path3);
-    
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    removeDir(new Path(strTable3));
-    runMR(sortKey, paths.toArray(new Path[3]));
-    checkTable(myMultiLocs);
-    System.out.println("DONE test 3");
-  }
-
-  static class MapClass extends
-      Mapper<LongWritable, Text, BytesWritable, Tuple> {
-    private BytesWritable bytesKey;
-    private Tuple tupleRow;
-    private Object javaObj;
-
-    @Override
-    public void map(LongWritable key, Text value, Context context)
-        throws IOException, InterruptedException {
-      // value should contain "word count"
-      String[] wdct = value.toString().split(" ");
-      if (wdct.length != 2) {
-        // LOG the error
-        return;
-      }
-
-      byte[] word = wdct[0].getBytes();
-      bytesKey.set(word, 0, word.length);
-      System.out.println("word: " + new String(word));
-      tupleRow.set(0, new String(word));
-      tupleRow.set(1, Integer.parseInt(wdct[1]));
-      System.out.println("count:  " + Integer.parseInt(wdct[1]));
-
-      // This key has to be created by user
-      /*
-       * Tuple userKey = new DefaultTuple(); userKey.append(new String(word));
-       * userKey.append(Integer.parseInt(wdct[1]));
-       */
-      System.out.println("in map, sortkey: " + sortKey);
-      Tuple userKey = new DefaultTuple();
-      if (sortKey.equalsIgnoreCase("word,count")) {
-        userKey.append(new String(word));
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("count")) {
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("word")) {
-        userKey.append(new String(word));
-      }
-
-      try {
-
-        /* New M/R Interface */
-        /* Converts user key to zebra BytesWritable key */
-        /* using sort key expr tree */
-        /* Returns a java base object */
-        /* Done for each user key */
-
-        bytesKey = BasicTableOutputFormat.getSortKey(javaObj, userKey);
-      } catch (Exception e) {
-
-      }
-
-      context.write(bytesKey, tupleRow);
-    }
-
-    @Override
-    public void setup(Context context) {
-      bytesKey = new BytesWritable();
-      sortKey = context.getConfiguration().get("sortKey");
-      try {
-        Schema outSchema = BasicTableOutputFormat.getSchema(context);
-        tupleRow = TypesUtils.createTuple(outSchema);
-        javaObj = BasicTableOutputFormat.getSortKeyGenerator(context);
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      } catch (org.apache.hadoop.zebra.parser.ParseException e) {
-        throw new RuntimeException(e);
-      }
-    }
-  }
-
-  static class ReduceClass extends
-      Reducer<BytesWritable, Tuple, BytesWritable, Tuple> {
-    Tuple outRow;
-
-    public void reduce(BytesWritable key, Iterator<Tuple> values, Context context)
-        throws IOException, InterruptedException {
-      try {
-        for (; values.hasNext();) {
-          context.write(key, values.next());
-        }
-      } catch (ExecException e) {
-        e.printStackTrace();
-      }
-    }
-
-  }
-
-  static class OutputPartitionerClass extends ZebraOutputPartition {
-
-    @Override
-    public int getOutputPartition(BytesWritable key, Tuple value) {
-
-      // System.out.println(this.jobConf);
-
-      String reg = null;
-      try {
-        reg = (String) (value.get(0));
-      } catch (Exception e) {
-        //
-      }
-
-      if (reg.equals("us"))
-        return 0;
-      if (reg.equals("india"))
-        return 1;
-      if (reg.equals("japan"))
-        return 2;
-
-      return 0;
-    }
-
-  }
-
-  public void runMR(String sortKey, Path... paths) throws ParseException,
-      IOException, Exception, org.apache.hadoop.zebra.parser.ParseException {
-
-    Job job = new Job(conf);
-    job.setJobName("TestMultipleOutputsTypeApi");
-    job.setJarByClass(TestMultipleOutputsTypeApi.class);
-    Configuration conf = job.getConfiguration();
-    conf.set("table.output.tfile.compression", "gz");
-    conf.set("sortKey", sortKey);
-    // input settings
-    job.setInputFormatClass(TextInputFormat.class);
-    job.setMapperClass(TestMultipleOutputsTypeApi.MapClass.class);
-    job.setMapOutputKeyClass(BytesWritable.class);
-    job.setMapOutputValueClass(ZebraTuple.class);
-    FileInputFormat.setInputPaths(job, inputPath);
-
-    // TODO:
-    //job.setNumMapTasks(1);
-
-    // output settings
-
-    job.setOutputFormatClass(BasicTableOutputFormat.class);
-
-    String schema = "word:string, count:int";
-    String storageHint = "[word];[count]";
-    BasicTableOutputFormat.setMultipleOutputs(job,
-        TestMultipleOutputsTypeApi.OutputPartitionerClass.class, paths);
-    ZebraSchema zSchema = ZebraSchema.createZebraSchema(schema);
-    ZebraStorageHint zStorageHint = ZebraStorageHint
-        .createZebraStorageHint(storageHint);
-    ZebraSortInfo zSortInfo = ZebraSortInfo.createZebraSortInfo(sortKey, null);
-    BasicTableOutputFormat.setStorageInfo(job, zSchema, zStorageHint,
-        zSortInfo);
-
-    job.setNumReduceTasks(1);
-    job.submit();
-    job.waitForCompletion( true );
-    BasicTableOutputFormat.close( job );
-  }
-
-  @Override
-  public int run(String[] args) throws Exception {
-    TestMultipleOutputsTypeApi test = new TestMultipleOutputsTypeApi();
-    TestMultipleOutputsTypeApi.setUpOnce();
-    System.out.println("after setup");
-    test.test1();
-    test.test2();
-    test.test3();
-
-    return 0;
-  }
-  
-  public static void main(String[] args) throws Exception {
-    conf = new Configuration();
-    
-    int res = ToolRunner.run(conf, new TestMultipleOutputsTypeApi(), args);
-    System.out.println("PASS");    
-    System.exit(res);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestMultipleOutputsTypedApiNeg.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestMultipleOutputsTypedApiNeg.java
deleted file mode 100644
index c6faaab1d..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestMultipleOutputsTypedApiNeg.java
+++ /dev/null
@@ -1,681 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.BufferedWriter;
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.FileWriter;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.mapreduce.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.mapreduce.ZebraOutputPartition;
-import org.apache.hadoop.zebra.mapreduce.ZebraSchema;
-import org.apache.hadoop.zebra.mapreduce.ZebraSortInfo;
-import org.apache.hadoop.zebra.mapreduce.ZebraStorageHint;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.types.ZebraTuple;
-import org.apache.pig.ExecType;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DefaultTuple;
-import org.apache.pig.data.Tuple;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * This is a sample a complete MR sample code for Table. It doens't contain
- * 'read' part. But, it should be similar and easier to write. Refer to test
- * cases in the same directory.
- * 
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * us 2
- * japan 2
- * india 4
- * us 2
- * japan 1
- * india 3
- * nouse 5
- * nowhere 4
- * 
- * 
- */
-public class TestMultipleOutputsTypedApiNeg extends BaseTestCase implements Tool{
-
-  static String inputPath;
-  static String inputFileName = "multi-input.txt";
-  public static String sortKey = null;
-  
-  private static String strTable1 = null;
-  private static String strTable2 = null;
-  private static String strTable3 = null;
-
-  @BeforeClass
-  public static void setUpOnce() throws Exception {
-    init();
-    
-    inputPath = getTableFullPath(inputFileName).toString();
-    
-    writeToFile(inputPath);
-  }
-
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    StringTokenizer st = new StringTokenizer(t, ".");
-    String methodName = null;
-    while (st.hasMoreTokens()) {
-      methodName = st.nextToken();
-    }
-    return methodName;
-  }
-
-  public static void writeToFile(String inputFile) throws IOException {
-    if (mode == TestMode.local) {
-      FileWriter fstream = new FileWriter(inputFile);
-      BufferedWriter out = new BufferedWriter(fstream);
-      out.write("us 2\n");
-      out.write("japan 2\n");
-      out.write("india 4\n");
-      out.write("us 2\n");
-      out.write("japan 1\n");
-      out.write("india 3\n");
-      out.write("nouse 5\n");
-      out.write("nowhere 4\n");
-      out.close();
-    }
-
-    if (mode == TestMode.cluster) {
-      FSDataOutputStream fout = fs.create(new Path(inputFile));
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 2\n");
-      fout.writeBytes("india 4\n");
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 1\n");
-      fout.writeBytes("india 3\n");
-      fout.writeBytes("nouse 5\n");
-      fout.writeBytes("nowhere 4\n");
-      fout.close();
-    }
-  }
-
-  public static void getTablePaths(String myMultiLocs) {
-    StringTokenizer st = new StringTokenizer(myMultiLocs, ",");
-
-    // get how many tokens inside st object
-    System.out.println("tokens count: " + st.countTokens());
-    int count = 0;
-
-    // iterate st object to get more tokens from it
-    while (st.hasMoreElements()) {
-      count++;
-      String token = st.nextElement().toString();
-      if (mode == TestMode.local) {
-        System.out.println("in mini, token: " + token);
-        // in mini, token:
-        // file:/homes/<uid>/grid/multipleoutput/pig-table/contrib/zebra/ustest3
-        if (count == 1)
-          strTable1 = token;
-        if (count == 2)
-          strTable2 = token;
-        if (count == 3)
-          strTable3 = token;
-      }
-      
-      if (mode == TestMode.cluster) {
-        System.out.println("in real, token: " + token);
-        // in real, token: /user/hadoopqa/ustest3
-        // note: no prefix file: in real cluster
-        if (count == 1)
-          strTable1 = token;
-        if (count == 2)
-          strTable2 = token;
-        if (count == 3)
-          strTable3 = token;
-      }
-
-    }
-  }
-
-  public static void checkTableExists(boolean expected, String strDir)
-      throws IOException {
-
-    File theDir = null;
-    boolean actual = false;
-
-    theDir = new File(strDir);
-    actual = fs.exists(new Path(theDir.toString()));
-    
-    System.out.println("the dir : " + theDir.toString());
-   
-    if (actual != expected) {
-      Assert.fail("dir exists or not is different from what expected.");
-    }
-  }
-
-  public static void checkTable(String myMultiLocs) throws IOException {
-    System.out.println("myMultiLocs:" + myMultiLocs);
-    System.out.println("sorgetTablePathst key:" + sortKey);
-
-    getTablePaths(myMultiLocs);
-    String query1 = null;
-    String query2 = null;
-
-    if (strTable1 != null) {
-
-      query1 = "records1 = LOAD '" + strTable1
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-    if (strTable2 != null) {
-      query2 = "records2 = LOAD '" + strTable2
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-
-    int count1 = 0;
-    int count2 = 0;
-
-    if (query1 != null) {
-      System.out.println(query1);
-      pigServer.registerQuery(query1);
-      Iterator<Tuple> it = pigServer.openIterator("records1");
-      while (it.hasNext()) {
-        count1++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-        // test 1 us table
-        if (query1.contains("test1") || query1.contains("test2")
-            || query1.contains("test3")) {
-
-          if (count1 == 1) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count1 == 2) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-        } // test1, test2
-
-      }// while
-      if (query1.contains("test1") || query1.contains("test2")
-          || query1.contains("test3")) {
-        Assert.assertEquals(2, count1);
-      }
-    }// if query1 != null
-
-    if (query2 != null) {
-      pigServer.registerQuery(query2);
-      Iterator<Tuple> it = pigServer.openIterator("records2");
-
-      while (it.hasNext()) {
-        count2++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-
-        // if test1 other table
-        if (query2.contains("test1")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-
-          if (count1 == 5) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test1
-        // if test2 other table
-        if (query2.contains("test2")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-
-          if (count1 == 5) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test2
-        // if test3 other table
-        if (query2.contains("test3")) {
-          if (count2 == 1) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 5) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-
-        }// if test3
-
-      }// while
-      if (query2.contains("test1") || query2.contains("test2")
-          || query2.contains("test3")) {
-        Assert.assertEquals(6, count2);
-      }
-    }// if query2 != null
-
-  }
-
-  @Test(expected = IllegalArgumentException.class)
-  public void test1() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * path list have an empty value in the middle should throw
-     * illegalArgumentExcepiton: Can not create a path from an empty string
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "word,count";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-    List<Path> paths = new ArrayList<Path>(3);
- 
-    
-    Path path1 = getTableFullPath("a" + methodName);
-    paths.add(path1);
-    
-    if (mode == TestMode.cluster) {
-      try {
-        paths.add(new Path(""));
-      } catch (IllegalArgumentException e) {
-        System.out.println(e.getMessage());
-        System.out.println("DONE test 1");
-        return;
-      }
-    } else {
-      paths.add(new Path(""));
-    }
-    
-    // should not reach here
-    Assert.fail("Should have seen exception already");
-
-    Path path3 = getTableFullPath("b" + methodName);
-    paths.add(path3);
-        
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    removeDir(new Path(strTable3));
-    runMR(sortKey, paths.toArray(new Path[3]));
-    System.out.println("DONE test 1");
-  }
-
-  // TODO: No exception is thrown any more due to Hadoop 20 API. This test case
-  // should be re-designed in the future.
-  @Test//(expected = IOException.class)
-  public void test2() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * path list have only one element OutputPartitionClass return 0 and return
-     * 1, expecting more element in the path list
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "word,count";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-    List<Path> paths = new ArrayList<Path>(1);
-
-    Path path1 = getTableFullPath("a" + methodName);
-    paths.add(path1);
-    myMultiLocs = path1.toString();
-    
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    runMR(sortKey, paths.toArray(new Path[1]));
-  }
-
-  @Test(expected = NullPointerException.class)
-  public void test3() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * path list is null
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "word,count";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-    List<Path> paths = new ArrayList<Path>(1);
-
-    Path path1 = getTableFullPath("a" + methodName);
-    paths.add(null);
-    myMultiLocs = path1.toString();
-    
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    
-    if (mode == TestMode.cluster) {
-      try {
-        runMR(sortKey, paths.toArray(new Path[1]));
-      } catch (NullPointerException e) {
-        System.err.println(e.getMessage());
-        System.out.println("DONE test 3");
-        return;
-      }
-    } else {
-      runMR(sortKey, paths.toArray(new Path[1]));
-    }
-  }
-
-  @Test(expected = IOException.class)
-  public void test4() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * path list have repeat element, for example atest1 dir has been already
-     * created. should throw IOExcepiton. complaining atest4/CG0/.meta already
-     * exists
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "word,count";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-    List<Path> paths = new ArrayList<Path>(3);
-
-    Path path1 = getTableFullPath("a" + methodName);
-    Path path2 = getTableFullPath("a" + methodName);
-    Path path3 = getTableFullPath("b" + methodName);
-    paths.add(path1);
-    paths.add(path2);
-    paths.add(path3);
-    myMultiLocs = path1.toString() + "," +path2.toString() + "," + path3.toString(); 
-    
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    removeDir(new Path(strTable3));
-    
-    if (mode == TestMode.cluster) {
-      try {
-        runMR(sortKey, paths.toArray(new Path[3]));
-      } catch (IOException e) {
-        System.err.println(e.getMessage());
-        return;
-      }
-    } else {
-      runMR(sortKey, paths.toArray(new Path[3]));
-    }
-  }
-
-  static class MapClass extends
-      Mapper<LongWritable, Text, BytesWritable, Tuple> {
-    private BytesWritable bytesKey;
-    private Tuple tupleRow;
-    private Object javaObj;
-
-    @Override
-    public void map(LongWritable key, Text value, Context context)
-        throws IOException, InterruptedException {
-      // value should contain "word count"
-      String[] wdct = value.toString().split(" ");
-      if (wdct.length != 2) {
-        // LOG the error
-        return;
-      }
-
-      byte[] word = wdct[0].getBytes();
-      bytesKey.set(word, 0, word.length);
-      System.out.println("word: " + new String(word));
-      tupleRow.set(0, new String(word));
-      tupleRow.set(1, Integer.parseInt(wdct[1]));
-      System.out.println("count:  " + Integer.parseInt(wdct[1]));
-
-      // This key has to be created by user
-      /*
-       * Tuple userKey = new DefaultTuple(); userKey.append(new String(word));
-       * userKey.append(Integer.parseInt(wdct[1]));
-       */
-      System.out.println("in map, sortkey: " + sortKey);
-      Tuple userKey = new DefaultTuple();
-      if (sortKey.equalsIgnoreCase("word,count")) {
-        userKey.append(new String(word));
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("count")) {
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("word")) {
-        userKey.append(new String(word));
-      }
-
-      try {
-
-        /* New M/R Interface */
-        /* Converts user key to zebra BytesWritable key */
-        /* using sort key expr tree */
-        /* Returns a java base object */
-        /* Done for each user key */
-
-        bytesKey = BasicTableOutputFormat.getSortKey(javaObj, userKey);
-      } catch (Exception e) {
-
-      }
-
-      context.write(bytesKey, tupleRow);
-    }
-
-    @Override
-    public void setup(Context context) {
-      bytesKey = new BytesWritable();
-      sortKey = context.getConfiguration().get("sortKey");
-      try {
-        Schema outSchema = BasicTableOutputFormat.getSchema(context);
-        tupleRow = TypesUtils.createTuple(outSchema);
-        javaObj = BasicTableOutputFormat.getSortKeyGenerator(context);
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      } catch (org.apache.hadoop.zebra.parser.ParseException e) {
-        throw new RuntimeException(e);
-      }
-    }
-
-  }
-
-  static class ReduceClass extends
-      Reducer<BytesWritable, Tuple, BytesWritable, Tuple> {
-    Tuple outRow;
-
-    public void reduce(BytesWritable key, Iterator<Tuple> values, Context context)
-        throws IOException, InterruptedException {
-      try {
-        for (; values.hasNext();) {
-          context.write(key, values.next());
-        }
-      } catch (ExecException e) {
-        e.printStackTrace();
-      }
-    }
-
-  }
-
-  static class OutputPartitionerClass extends ZebraOutputPartition {
-
-    @Override
-    public int getOutputPartition(BytesWritable key, Tuple value)
-        throws ExecException {
-
-      String reg = null;
-      try {
-        reg = (String) (value.get(0));
-      } catch (Exception e) {
-        //
-      }
-
-      if (reg.equals("us"))
-        return 0;
-      else
-        return 1;
-    }
-  }
-
-  public void runMR(String sortKey, Path... paths) throws ParseException,
-      IOException, Exception, org.apache.hadoop.zebra.parser.ParseException {
-
-    Job job = new Job(conf);
-    job.setJobName("TestMultipleOutputsTypedApiNeg");
-    job.setJarByClass(TestMultipleOutputsTypedApiNeg.class);
-    Configuration conf = job.getConfiguration();
-    conf.set("table.output.tfile.compression", "gz");
-    conf.set("sortKey", sortKey);
-    // input settings
-    job.setInputFormatClass(TextInputFormat.class);
-    job.setMapperClass(TestMultipleOutputsTypedApiNeg.MapClass.class);
-    job.setMapOutputKeyClass(BytesWritable.class);
-    job.setMapOutputValueClass(ZebraTuple.class);
-    FileInputFormat.setInputPaths(job, inputPath);
-
-    // TODO: 
-    //job.setNumMapTasks(1);
-
-    // output settings
-
-    job.setOutputFormatClass(BasicTableOutputFormat.class);
-    
-    String schema = "word:string, count:int";
-    String storageHint = "[word];[count]";
-    BasicTableOutputFormat.setMultipleOutputs(job,
-        TestMultipleOutputsTypedApiNeg.OutputPartitionerClass.class, paths);
-    ZebraSchema zSchema = ZebraSchema.createZebraSchema(schema);
-    ZebraStorageHint zStorageHint = ZebraStorageHint
-        .createZebraStorageHint(storageHint);
-    ZebraSortInfo zSortInfo = ZebraSortInfo.createZebraSortInfo(sortKey, null);
-    BasicTableOutputFormat.setStorageInfo(job, zSchema, zStorageHint,
-        zSortInfo);
-    job.setNumReduceTasks(1);
-    job.submit();
-    job.waitForCompletion( true );
-    BasicTableOutputFormat.close( job );
-  }
-
-  @Override
-  public int run(String[] args) throws Exception {
-    TestMultipleOutputsTypedApiNeg test = new TestMultipleOutputsTypedApiNeg();
-    TestMultipleOutputsTypedApiNeg.setUpOnce();
-
-    test.test1();
-    
-    //TODO: backend exception - will migrate to real cluster later
-    //test.test2();
-    
-    test.test3();
-    
-    //TODO: backend exception
-    //test.test4();
-    
-    return 0;
-  }
-  
-  public static void main(String[] args) throws Exception {
-    conf = new Configuration();
-    
-    int res = ToolRunner.run(conf, new TestMultipleOutputsTypedApiNeg(), args);
-    System.out.println("PASS");    
-    System.exit(res);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestTempDirRemoval.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestTempDirRemoval.java
deleted file mode 100644
index d623d40f8..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestTempDirRemoval.java
+++ /dev/null
@@ -1,572 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.BufferedWriter;
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.FileWriter;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.mapreduce.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.mapreduce.ZebraOutputPartition;
-import org.apache.hadoop.zebra.mapreduce.ZebraSchema;
-import org.apache.hadoop.zebra.mapreduce.ZebraSortInfo;
-import org.apache.hadoop.zebra.mapreduce.ZebraStorageHint;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.types.ZebraTuple;
-import org.apache.pig.ExecType;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DefaultTuple;
-import org.apache.pig.data.Tuple;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * This is a sample a complete MR sample code for Table. It doens't contain
- * 'read' part. But, it should be similar and easier to write. Refer to test
- * cases in the same directory.
- * 
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * us 2
- * japan 2
- * india 4
- * us 2
- * japan 1
- * india 3
- * nouse 5
- * nowhere 4
- * 
- * 
- */
-public class TestTempDirRemoval extends BaseTestCase implements Tool {
-  static String inputPath;
-  static String inputFileName = "multi-input.txt";
-  public static String sortKey = null;
-
-  private static String strTable1 = null;
-  private static String strTable2 = null;
-
-  @BeforeClass
-  public static void setUpOnce() throws Exception {
-    init();
-    
-    inputPath = getTableFullPath(inputFileName).toString();
-
-    writeToFile(inputPath);
-  }
-
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    StringTokenizer st = new StringTokenizer(t, ".");
-    String methodName = null;
-    while (st.hasMoreTokens()) {
-      methodName = st.nextToken();
-    }
-    return methodName;
-  }
-
-  public static void writeToFile(String inputFile) throws IOException {
-    if (mode == TestMode.local) {
-      FileWriter fstream = new FileWriter(inputFile);
-      BufferedWriter out = new BufferedWriter(fstream);
-      out.write("us 2\n");
-      out.write("japan 2\n");
-      out.write("india 4\n");
-      out.write("us 2\n");
-      out.write("japan 1\n");
-      out.write("india 3\n");
-      out.write("nouse 5\n");
-      out.write("nowhere 4\n");
-      out.close();
-    }
-    
-    if (mode == TestMode.cluster) {
-      FSDataOutputStream fout = fs.create(new Path(inputFile));
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 2\n");
-      fout.writeBytes("india 4\n");
-      fout.writeBytes("us 2\n");
-      fout.writeBytes("japan 1\n");
-      fout.writeBytes("india 3\n");
-      fout.writeBytes("nouse 5\n");
-      fout.writeBytes("nowhere 4\n");
-      fout.close();
-    }
-  }
-
-  public static void getTablePaths(String myMultiLocs) {
-    StringTokenizer st = new StringTokenizer(myMultiLocs, ",");
-
-    // get how many tokens inside st object
-    System.out.println("tokens count: " + st.countTokens());
-    int count = 0;
-
-    // iterate st object to get more tokens from it
-    while (st.hasMoreElements()) {
-      count++;
-      String token = st.nextElement().toString();
-      if (mode == TestMode.local) {
-        System.out.println("in mini, token: " + token);
-        // in mini, token:
-        // file:/homes/<uid>/grid/multipleoutput/pig-table/contrib/zebra/ustest3
-        if (count == 1)
-          strTable1 = token;
-        if (count == 2)
-          strTable2 = token;
-      }
-      
-      if (mode == TestMode.cluster) {
-        System.out.println("in real, token: " + token);
-        // in real, token: /user/hadoopqa/ustest3
-        // note: no prefix file: in real cluster
-        if (count == 1)
-          strTable1 = token;
-        if (count == 2)
-          strTable2 = token;
-
-      }
-
-    }
-  }
-
-  public static void checkTableExists(boolean expected, String strDir)
-      throws IOException {
-
-    File theDir = null;
-    boolean actual = false;
-
-    theDir = new File(strDir);
-    actual = fs.exists(new Path(theDir.toString()));
-    
-    System.out.println("the dir : " + theDir.toString());
-   
-    if (actual != expected) {
-      Assert.fail("dir exists or not is different from what expected.");
-    }
-  }
-
-  public static void checkTable(String myMultiLocs) throws IOException {
-    System.out.println("myMultiLocs:" + myMultiLocs);
-    System.out.println("sorgetTablePathst key:" + sortKey);
-
-    getTablePaths(myMultiLocs);
-    String query1 = null;
-    String query2 = null;
-
-    if (strTable1 != null) {
-
-      query1 = "records1 = LOAD '" + strTable1
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-    if (strTable2 != null) {
-      query2 = "records2 = LOAD '" + strTable2
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    }
-
-    int count1 = 0;
-    int count2 = 0;
-
-    if (query1 != null) {
-      System.out.println(query1);
-      pigServer.registerQuery(query1);
-      Iterator<Tuple> it = pigServer.openIterator("records1");
-      while (it.hasNext()) {
-        count1++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-        // test 1 us table
-        if (query1.contains("test1") || query1.contains("test2")
-            || query1.contains("test3")) {
-
-          if (count1 == 1) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count1 == 2) {
-            Assert.assertEquals("us", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-        } // test1, test2
-
-      }// while
-      if (query1.contains("test1") || query1.contains("test2")
-          || query1.contains("test3")) {
-        Assert.assertEquals(2, count1);
-      }
-    }// if query1 != null
-
-    if (query2 != null) {
-      pigServer.registerQuery(query2);
-      Iterator<Tuple> it = pigServer.openIterator("records2");
-
-      while (it.hasNext()) {
-        count2++;
-        Tuple RowValue = it.next();
-        System.out.println(RowValue);
-
-        // if test1 other table
-        if (query2.contains("test1")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-
-          if (count1 == 5) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test1
-        // if test2 other table
-        if (query2.contains("test2")) {
-          if (count2 == 1) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-
-          if (count1 == 5) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-        }// if test2
-        // if test3 other table
-        if (query2.contains("test3")) {
-          if (count2 == 1) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(1, RowValue.get(1));
-          }
-          if (count2 == 2) {
-            Assert.assertEquals("japan", RowValue.get(0));
-            Assert.assertEquals(2, RowValue.get(1));
-          }
-          if (count2 == 3) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(3, RowValue.get(1));
-          }
-          if (count2 == 4) {
-            Assert.assertEquals("india", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 5) {
-            Assert.assertEquals("nowhere", RowValue.get(0));
-            Assert.assertEquals(4, RowValue.get(1));
-          }
-          if (count1 == 6) {
-            Assert.assertEquals("nouse", RowValue.get(0));
-            Assert.assertEquals(5, RowValue.get(1));
-          }
-
-        }// if test3
-
-      }// while
-      if (query2.contains("test1") || query2.contains("test2")
-          || query2.contains("test3")) {
-        Assert.assertEquals(6, count2);
-      }
-    }// if query2 != null
-
-  }
-
-
-  @Test
-  public void test4() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    /*
-     * path list have repeat element, for example atest1 dir has been already
-     * created. should throw IOExcepiton. complaining atest4/CG0/.meta already
-     * exists
-     */
-    System.out.println("******Start  testcase: " + getCurrentMethodName());
-    sortKey = "word,count";
-    System.out.println("hello sort on word and count");
-    String methodName = getCurrentMethodName();
-    String myMultiLocs = null;
-    List<Path> paths = new ArrayList<Path>(2);
-
-    Path path1 = getTableFullPath("a" + methodName);
-    Path path2 = getTableFullPath("b" + methodName);
-    paths.add(path1);
-    paths.add(path2);
-    myMultiLocs = path1.toString() + "," + path2.toString();
-        
-    getTablePaths(myMultiLocs);
-    removeDir(new Path(strTable1));
-    removeDir(new Path(strTable2));
-    runMR(sortKey, paths.toArray(new Path[2]));
-    System.out.println("DONE test 4");
-  }
-
-  static class MapClass extends
-      Mapper<LongWritable, Text, BytesWritable, Tuple> {
-    private BytesWritable bytesKey;
-    private Tuple tupleRow;
-    private Object javaObj;
-
-    @Override
-    public void map(LongWritable key, Text value, Context context)
-        throws IOException, InterruptedException {
-      // value should contain "word count"
-      String[] wdct = value.toString().split(" ");
-      if (wdct.length != 2) {
-        // LOG the error
-        return;
-      }
-
-      byte[] word = wdct[0].getBytes();
-      bytesKey.set(word, 0, word.length);
-      System.out.println("word: " + new String(word));
-      tupleRow.set(0, new String(word));
-      tupleRow.set(1, Integer.parseInt(wdct[1]));
-      System.out.println("count:  " + Integer.parseInt(wdct[1]));
-
-      // This key has to be created by user
-      /*
-       * Tuple userKey = new DefaultTuple(); userKey.append(new String(word));
-       * userKey.append(Integer.parseInt(wdct[1]));
-       */
-      System.out.println("in map, sortkey: " + sortKey);
-      Tuple userKey = new DefaultTuple();
-      if (sortKey.equalsIgnoreCase("word,count")) {
-        userKey.append(new String(word));
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("count")) {
-        userKey.append(Integer.parseInt(wdct[1]));
-      }
-
-      if (sortKey.equalsIgnoreCase("word")) {
-        userKey.append(new String(word));
-      }
-
-      try {
-
-        /* New M/R Interface */
-        /* Converts user key to zebra BytesWritable key */
-        /* using sort key expr tree */
-        /* Returns a java base object */
-        /* Done for each user key */
-
-        bytesKey = BasicTableOutputFormat.getSortKey(javaObj, userKey);
-      } catch (Exception e) {
-
-      }
-
-      context.write(bytesKey, tupleRow);
-    }
-
-    @Override
-    public void setup(Context context) {
-      bytesKey = new BytesWritable();
-      sortKey = context.getConfiguration().get("sortKey");
-      try {
-        Schema outSchema = BasicTableOutputFormat.getSchema(context);
-        tupleRow = TypesUtils.createTuple(outSchema);
-        javaObj = BasicTableOutputFormat.getSortKeyGenerator(context);
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      } catch (org.apache.hadoop.zebra.parser.ParseException e) {
-        throw new RuntimeException(e);
-      }
-    }
-
-  }
-
-  static class ReduceClass extends
-      Reducer<BytesWritable, Tuple, BytesWritable, Tuple> {
-    Tuple outRow;
-
-    public void reduce(BytesWritable key, Iterator<Tuple> values, Context context)
-        throws IOException, InterruptedException {
-      try {
-        for (; values.hasNext();) {
-          context.write(key, values.next());
-        }
-      } catch (ExecException e) {
-        e.printStackTrace();
-      }
-    }
-
-  }
-
-  static class OutputPartitionerClass extends ZebraOutputPartition {
-
-    @Override
-    public int getOutputPartition(BytesWritable key, Tuple value)
-        throws ExecException {
-
-      String reg = null;
-      try {
-        reg = (String) (value.get(0));
-      } catch (Exception e) {
-        //
-      }
-
-      if (reg.equals("us"))
-        return 0;
-      else
-        return 1;
-
-    }
-  }
-
-  public void runMR(String sortKey, Path... paths) throws ParseException,
-      IOException, Exception, org.apache.hadoop.zebra.parser.ParseException {
-
-    Job job = new Job(conf);
-    job.setJobName("tableMRSample");
-    job.setJarByClass(TestTempDirRemoval.class);
-    Configuration conf = job.getConfiguration();
-    conf.set("table.output.tfile.compression", "gz");
-    conf.set("sortKey", sortKey);
-    // input settings
-    job.setInputFormatClass(TextInputFormat.class);
-    job.setMapperClass(TestTempDirRemoval.MapClass.class);
-    job.setMapOutputKeyClass(BytesWritable.class);
-    job.setMapOutputValueClass(ZebraTuple.class);
-    FileInputFormat.setInputPaths(job, inputPath);
-
-    // TODO: 
-    //job.setNumMapTasks(1);
-
-    // output settings
-
-    job.setOutputFormatClass(BasicTableOutputFormat.class);
-    
-    String schema = "word:string, count:int";
-    String storageHint = "[word];[count]";
-    BasicTableOutputFormat.setMultipleOutputs(job,
-        TestTempDirRemoval.OutputPartitionerClass.class, paths);
-    ZebraSchema zSchema = ZebraSchema.createZebraSchema(schema);
-    ZebraStorageHint zStorageHint = ZebraStorageHint
-        .createZebraStorageHint(storageHint);
-    ZebraSortInfo zSortInfo = ZebraSortInfo.createZebraSortInfo(sortKey, null);
-    BasicTableOutputFormat.setStorageInfo(job, zSchema, zStorageHint,
-        zSortInfo);
-    job.setNumReduceTasks(1);
-    job.submit();
-    job.waitForCompletion( true );
-    for ( int i =0; i < paths.length; ++i) {
-    	Path tmpPath = new Path(paths[i], "_temporary");
-    	FileSystem fileSys = tmpPath.getFileSystem(conf);
-    	if(!fileSys.exists(tmpPath)) {
-    		throw new RuntimeException("Temp Dir sld exist before BTOF.close() " + tmpPath.toString());
-    	}
-    }
-    BasicTableOutputFormat.close( job );
-    for ( int i =0; i < paths.length; ++i) {
-    	Path tmpPath = new Path(paths[i], "_temporary");
-    	FileSystem fileSys = tmpPath.getFileSystem(conf);
-    	if(fileSys.exists(tmpPath)) {
-    		throw new RuntimeException("Temp Dir sld not exist after BTOF.close()" + tmpPath.toString());
-    	}
-    }
-  }
-  
-  @Override
-  public int run(String[] args) throws Exception {
-    TestTempDirRemoval test = new TestTempDirRemoval();
-    TestTempDirRemoval.setUpOnce();
-    
-    test.test4();
-
-    return 0;
-  }
-  
-  public static void main(String[] args) throws Exception {
-    conf = new Configuration();
-    
-    int res = ToolRunner.run(conf, new TestTempDirRemoval(), args);
-    System.out.println("PASS");
-    System.exit(res);
-  }
-  
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestTfileSplit.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestTfileSplit.java
deleted file mode 100644
index e12014b68..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestTfileSplit.java
+++ /dev/null
@@ -1,152 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.StringTokenizer;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.mapreduce.InputSplit;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TestBasicTable;
-import org.apache.hadoop.zebra.mapreduce.RowTableSplit;
-import org.apache.hadoop.zebra.mapreduce.TableInputFormat;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.junit.AfterClass;
-import org.junit.Assert;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-public class TestTfileSplit {
-	private static Configuration conf;
-	private static Path path;
-
-	@BeforeClass
-	public static void setUpOnce() throws IOException {
-		TestBasicTable.setUpOnce();
-		conf = TestBasicTable.conf;
-		path = new Path(TestBasicTable.rootPath, "TfileSplitTest");
-	}
-
-	@AfterClass
-	public static void tearDown() throws IOException {
-		BasicTable.drop(path, conf);
-	}
-
-	/* In this test, we test creating input splits for the projection on non-existing column case.
-	 * The first non-deleted column group should be used for split. */ 
-	@Test
-	public void testTfileSplit1() 
-	throws IOException, ParseException {
-		BasicTable.drop(path, conf);
-		TestBasicTable.createBasicTable(1, 100, "a, b, c, d, e, f", "[a, b]; [c, d]", null, path, true);    
-
-		TableInputFormat inputFormat = new TableInputFormat();
-		Job job = new Job(conf);
-		inputFormat.setInputPaths(job, path);
-		inputFormat.setMinSplitSize(job, 100);
-		inputFormat.setProjection(job, "aa");
-		List<InputSplit> splits = inputFormat.getSplits(job);
-
-		RowTableSplit split = (RowTableSplit) splits.get(0);
-		String str = split.getSplit().toString();
-		StringTokenizer tokens = new StringTokenizer(str, "\n");
-		str = tokens.nextToken();
-		tokens = new StringTokenizer(str, " ");
-		tokens.nextToken();
-		tokens.nextToken();
-		String s = tokens.nextToken();
-		s = s.substring(0, s.length()-1);
-		int cgIndex = Integer.parseInt(s);
-		Assert.assertEquals(cgIndex, 0); 
-	}
-
-	/* In this test, we test creating input splits when dropped column groups are around.
-	 * Here the projection involves all columns and only one valid column group is present.
-	 * As such, that column group should be used for split.*/
-	@Test
-	public void testTfileSplit2() 
-	throws IOException, ParseException {    
-		BasicTable.drop(path, conf);
-		TestBasicTable.createBasicTable(1, 100, "a, b, c, d, e, f", "[a, b]; [c, d]", null, path, true);    
-		BasicTable.dropColumnGroup(path, conf, "CG0");
-		BasicTable.dropColumnGroup(path, conf, "CG2");
-
-		TableInputFormat inputFormat = new TableInputFormat();
-		Job job = new Job(conf);
-		inputFormat.setInputPaths(job, path);
-		inputFormat.setMinSplitSize(job, 100);
-		List<InputSplit> splits = inputFormat.getSplits(job);
-
-		RowTableSplit split = (RowTableSplit) splits.get( 0 );
-		String str = split.getSplit().toString(); 
-		StringTokenizer tokens = new StringTokenizer(str, "\n");
-		str = tokens.nextToken();
-		tokens = new StringTokenizer(str, " ");
-		tokens.nextToken();
-		tokens.nextToken();
-		String s = tokens.nextToken();
-		s = s.substring(0, s.length()-1);
-		int cgIndex = Integer.parseInt(s);
-		Assert.assertEquals(cgIndex, 1); 
-	}
-
-	/* In this test, we test creating input splits when there is no valid column group present.
-	 * Should return 0 splits. */
-	@Test
-	public void testTfileSplit3() 
-	throws IOException, ParseException {    
-		BasicTable.drop(path, conf);
-		TestBasicTable.createBasicTable(1, 100, "a, b, c, d, e, f", "[a, b]; [c, d]", null, path, true);    
-		BasicTable.dropColumnGroup(path, conf, "CG0");
-		BasicTable.dropColumnGroup(path, conf, "CG1");
-		BasicTable.dropColumnGroup(path, conf, "CG2");
-
-		TableInputFormat inputFormat = new TableInputFormat();
-		Job job = new Job(conf);
-		inputFormat.setInputPaths(job, path);
-		inputFormat.setMinSplitSize(job, 100);
-		List<InputSplit> splits = inputFormat.getSplits(job);
-
-		Assert.assertEquals(splits.size(), 0);
-	}
-	
-	@Test
-	public void testSortedSplitOrdering() throws IOException, ParseException {
-		BasicTable.drop(path, conf);
-		TestBasicTable.createBasicTable(1, 1000000, "a, b, c, d, e, f", "[a, e, d]", "a", path, true);    
-
-		TableInputFormat inputFormat = new TableInputFormat();
-		Job job = new Job(conf);
-		inputFormat.setInputPaths(job, path);
-		inputFormat.setMinSplitSize(job, 100);
-		inputFormat.setProjection(job, "d");
-		inputFormat.requireSortedTable( job, null );
-		List<InputSplit> splits = inputFormat.getSplits(job);
-		
-		int index = 0;
-		for( InputSplit is : splits ) {
-			Assert.assertTrue( is instanceof SortedTableSplit );
-			SortedTableSplit split = (SortedTableSplit)is;
-			Assert.assertEquals( index++, split.getIndex() );
-		}
-	}
-	
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestTypeCheck.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestTypeCheck.java
deleted file mode 100644
index 2ab8f526b..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestTypeCheck.java
+++ /dev/null
@@ -1,232 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.IOException;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.mapreduce.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.types.ZebraTuple;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * This is to test type checking at M/R level;
- * 
- */
-public class TestTypeCheck extends BaseTestCase implements Tool {
-  static String inputPath;
-  
-  final static String STR_SCHEMA1 = "a:string,b:string";
-  final static String STR_STORAGE1 = "[a];[b]";
-  final static String STR_SCHEMA2 = "a:string,c:string";
-  final static String STR_STORAGE2 = "[a];[c]";
-  
-  static Path path1, path2, path3;
-
-  @BeforeClass
-  public static void setUpOnce() throws Exception {
-    init();
-
-    path1 = getTableFullPath("t1");
-    path2 = getTableFullPath("t2");
-    path3 = getTableFullPath("t3");
-    removeDir(path1);
-    removeDir(path2);
-    removeDir(path3);
-
-    /*
-     * create 1st basic table;
-     */
-    BasicTable.Writer writer = new BasicTable.Writer(path1, STR_SCHEMA1, STR_STORAGE1, conf);
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    final int numsBatch = 10;
-    final int numsInserters = 1;
-    TableInserter[] inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tuple);
-        for (int k = 0; k < tuple.size(); ++k) {
-          try {
-            tuple.set(k, b + "_" + i + "" + k);
-          } catch (ExecException e) {
-
-          }
-        }// k
-        inserters[i].insert(new BytesWritable(("key1" + i).getBytes()), tuple);
-      }// i
-    }// b
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-    writer.close();
-
-    /*
-     * create 2nd basic table;
-     */
-    writer = new BasicTable.Writer(path2, STR_SCHEMA2, STR_STORAGE2, conf);
-    schema = writer.getSchema();
-    tuple = TypesUtils.createTuple(schema);
-
-    inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tuple);
-        for (int k = 0; k < tuple.size(); ++k) {
-          try {
-            tuple.set(k, b + "_" + i + "" + k);
-          } catch (ExecException e) {
-
-          }
-        }
-        inserters[i].insert(new BytesWritable(("key2" + i).getBytes()), tuple);
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-    writer.close();  
-  }
-  
-  @AfterClass
-  public static void tearDown() throws Exception {
-    removeDir(path1);
-    removeDir(path2);
-    removeDir(path3);
-  }
- 
-  static class MapClass extends Mapper<BytesWritable, Tuple, BytesWritable, Tuple> {    
-    @Override
-    public void map(BytesWritable key, Tuple value, Context context) throws IOException, InterruptedException {      
-      System.out.println("key = " + key);
-      System.out.println("value = " + value);
-      
-      context.write(key, value);
-    }
-
-    @Override
-    public void setup(Context context) throws IOException {
-    }
-  }
-  
-  @Test
-  public void test1() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    Job job = new Job(conf);
-    job.setJobName("Test1");
-    job.setJarByClass(TestTypeCheck.class);    
-   
-    // input settings
-    job.setInputFormatClass(TableInputFormat.class);
-    job.setMapperClass(MapClass.class);
-    job.setMapOutputKeyClass(BytesWritable.class);
-    job.setMapOutputValueClass(ZebraTuple.class);
-    job.setOutputFormatClass(BasicTableOutputFormat.class);
-    
-    TableInputFormat.setInputPaths(job, path1, path2);
-    TableInputFormat.setProjection(job, "a,b,c");
-    BasicTableOutputFormat.setOutputPath(job, path3);
-    removeDir(path3);
-
-    BasicTableOutputFormat.setSchema(job, "a:string, b:string, c:string");
-    BasicTableOutputFormat.setStorageHint(job, "[a,b,c]");
-    
-    job.submit();
-    job.waitForCompletion( true );
-    Assert.assertTrue(job.isSuccessful());
-    
-    BasicTableOutputFormat.close(job);    
-  }
-
-  @Test
-  public void testNegative1() throws ParseException, IOException, InterruptedException, ClassNotFoundException {
-    Job job = new Job(conf);
-    job.setJobName("Test1");
-    job.setJarByClass(TestTypeCheck.class);
-   
-    // input settings
-    job.setInputFormatClass(TableInputFormat.class);
-    job.setMapperClass(MapClass.class);
-    job.setMapOutputKeyClass(BytesWritable.class);
-    job.setMapOutputValueClass(ZebraTuple.class);
-    job.setOutputFormatClass(BasicTableOutputFormat.class);
-    
-    TableInputFormat.setInputPaths(job, path1, path2);
-    TableInputFormat.setProjection(job, "a,b,c");
-    BasicTableOutputFormat.setOutputPath(job, path3);
-    removeDir(path3);
-
-    BasicTableOutputFormat.setSchema(job, "a:int, b:string, c:string");
-    BasicTableOutputFormat.setStorageHint(job, "[a,b,c]");
-    
-    job.submit();
-    job.waitForCompletion( true );
-    Assert.assertFalse(job.isSuccessful());
-    
-    BasicTableOutputFormat.close(job);        
-  }
-
-  @Override
-  public int run(String[] args) throws Exception {
-    TestTypeCheck test = new TestTypeCheck();
-    TestTypeCheck.setUpOnce();
-    System.out.println("after setup");
-
-    test.test1();
-    test.testNegative1();
-
-    return 0;
-  }
-
-  
-  public static void main(String[] args) throws Exception {
-    conf = new Configuration();
-    
-    int res = ToolRunner.run(conf, new TestTypeCheck(), args);
-    System.out.println("PASS");
-    System.exit(res);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestTypedApi.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestTypedApi.java
deleted file mode 100644
index c53a6d1ee..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestTypedApi.java
+++ /dev/null
@@ -1,926 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.BufferedWriter;
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.FileWriter;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.io.Serializable;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.RawComparator;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.WritableComparator;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.types.ZebraTuple;
-import org.apache.pig.ExecType;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-import org.apache.hadoop.zebra.mapreduce.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.mapreduce.ZebraOutputPartition;
-import org.apache.hadoop.zebra.mapreduce.ZebraSchema;
-import org.apache.hadoop.zebra.mapreduce.ZebraSortInfo;
-import org.apache.hadoop.zebra.mapreduce.ZebraStorageHint;
-
-/**
- * This is a sample a complete MR sample code for Table. It doens't contain
- * 'read' part. But, it should be similar and easier to write. Refer to test
- * cases in the same directory.
- * 
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * us 2
- * japan 2
- * india 4
- * us 2
- * japan 1
- * india 3
- * nouse 5
- * nowhere 4
- * 
- * 
- */
-public class TestTypedApi extends BaseTestCase implements Tool {
-	static String inputPath;
-	static String inputFileName = "multi-input.txt";
-	public static String sortKey = null;
-
-	private static String strTable1 = null;
-	private static String strTable2 = null;
-	private static String strTable3 = null;
-
-	@BeforeClass
-	public static void setUpOnce() throws Exception {
-    init();
-    
-    inputPath = getTableFullPath(inputFileName).toString();
-    
-    writeToFile(inputPath);
-	}
-
-	@AfterClass
-	public static void tearDownOnce() throws Exception {
-		pigServer.shutdown();
-		
-		if (strTable1 != null) {
-			BasicTable.drop(new Path(strTable1), conf);
-		}
-		if (strTable1 != null) {
-			BasicTable.drop(new Path(strTable2), conf);
-		}
-		if (strTable1 != null) {
-			BasicTable.drop(new Path(strTable3 + ""), conf);
-		}
-	}
-
-	public String getCurrentMethodName() {
-		ByteArrayOutputStream baos = new ByteArrayOutputStream();
-		PrintWriter pw = new PrintWriter(baos);
-		(new Throwable()).printStackTrace(pw);
-		pw.flush();
-		String stackTrace = baos.toString();
-		pw.close();
-
-		StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-		tok.nextToken(); // 'java.lang.Throwable'
-		tok.nextToken(); // 'at ...getCurrentMethodName'
-		String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-		// Parse line 3
-		tok = new StringTokenizer(l.trim(), " <(");
-		String t = tok.nextToken(); // 'at'
-		t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-		StringTokenizer st = new StringTokenizer(t, ".");
-		String methodName = null;
-		while (st.hasMoreTokens()) {
-			methodName = st.nextToken();
-		}
-		return methodName;
-	}
-
-	public static void writeToFile(String inputFile) throws IOException {
-		if (mode == TestMode.local) {
-			FileWriter fstream = new FileWriter(inputFile);
-			BufferedWriter out = new BufferedWriter(fstream);
-			out.write("us 2\n");
-			out.write("japan 2\n");
-			out.write("india 4\n");
-			out.write("us 2\n");
-			out.write("japan 1\n");
-			out.write("india 3\n");
-			out.write("nouse 5\n");
-			out.write("nowhere 4\n");
-			out.close();
-		}
-		
-		if (mode == TestMode.cluster) {
-			FSDataOutputStream fout = fs.create(new Path(inputFile));
-			fout.writeBytes("us 2\n");
-			fout.writeBytes("japan 2\n");
-			fout.writeBytes("india 4\n");
-			fout.writeBytes("us 2\n");
-			fout.writeBytes("japan 1\n");
-			fout.writeBytes("india 3\n");
-			fout.writeBytes("nouse 5\n");
-			fout.writeBytes("nowhere 4\n");
-			fout.close();
-		}
-	}
-
-	public static void getTablePaths(String myMultiLocs) {
-		StringTokenizer st = new StringTokenizer(myMultiLocs, ",");
-
-		// get how many tokens inside st object
-		System.out.println("tokens count: " + st.countTokens());
-		int count = 0;
-
-		// iterate st object to get more tokens from it
-		while (st.hasMoreElements()) {
-			count++;
-			String token = st.nextElement().toString();
-			if (count == 1)
-				strTable1 = token;
-			if (count == 2)
-				strTable2 = token;
-			if (count == 3)
-				strTable3 = token;
-			System.out.println("token = " + token);
-		}
-	}
-
-	public static void checkTable(String myMultiLocs) throws IOException {
-		System.out.println("myMultiLocs:" + myMultiLocs);
-		System.out.println("sorgetTablePathst key:" + sortKey);
-
-		getTablePaths(myMultiLocs);
-		String query1 = null;
-		String query2 = null;
-
-		if (strTable1 != null) {
-
-			query1 = "records1 = LOAD '" + strTable1
-			+ "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		}
-		if (strTable2 != null) {
-			query2 = "records2 = LOAD '" + strTable2
-			+ "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		}
-
-		int count1 = 0;
-		int count2 = 0;
-
-		if (query1 != null) {
-			System.out.println(query1);
-			pigServer.registerQuery(query1);
-			Iterator<Tuple> it = pigServer.openIterator("records1");
-			while (it.hasNext()) {
-				count1++;
-				Tuple RowValue = it.next();
-				System.out.println(RowValue);
-				// test 1 us table
-				if (query1.contains("test1") || query1.contains("test2")
-						|| query1.contains("test3")) {
-
-					if (count1 == 1) {
-						Assert.assertEquals("us", RowValue.get(0));
-						Assert.assertEquals(2, RowValue.get(1));
-					}
-					if (count1 == 2) {
-						Assert.assertEquals("us", RowValue.get(0));
-						Assert.assertEquals(2, RowValue.get(1));
-					}
-				} // test1, test2
-
-			}// while
-			if (query1.contains("test1") || query1.contains("test2")
-					|| query1.contains("test3")) {
-				Assert.assertEquals(2, count1);
-			}
-		}// if query1 != null
-
-		if (query2 != null) {
-			pigServer.registerQuery(query2);
-			Iterator<Tuple> it = pigServer.openIterator("records2");
-
-			while (it.hasNext()) {
-				count2++;
-				Tuple RowValue = it.next();
-				System.out.println(RowValue);
-
-				// if test1 other table
-				if (query2.contains("test1")) {
-					if (count2 == 1) {
-						Assert.assertEquals("india", RowValue.get(0));
-						Assert.assertEquals(3, RowValue.get(1));
-					}
-					if (count2 == 2) {
-						Assert.assertEquals("india", RowValue.get(0));
-						Assert.assertEquals(4, RowValue.get(1));
-					}
-					if (count2 == 3) {
-						Assert.assertEquals("japan", RowValue.get(0));
-						Assert.assertEquals(1, RowValue.get(1));
-					}
-					if (count2 == 4) {
-						Assert.assertEquals("japan", RowValue.get(0));
-						Assert.assertEquals(2, RowValue.get(1));
-					}
-
-					if (count1 == 5) {
-						Assert.assertEquals("nouse", RowValue.get(0));
-						Assert.assertEquals(5, RowValue.get(1));
-					}
-					if (count1 == 6) {
-						Assert.assertEquals("nowhere", RowValue.get(0));
-						Assert.assertEquals(4, RowValue.get(1));
-					}
-				}// if test1
-				// if test2 other table
-				if (query2.contains("test2")) {
-					if (count2 == 1) {
-						Assert.assertEquals("india", RowValue.get(0));
-						Assert.assertEquals(4, RowValue.get(1));
-					}
-					if (count2 == 2) {
-						Assert.assertEquals("india", RowValue.get(0));
-						Assert.assertEquals(3, RowValue.get(1));
-					}
-					if (count2 == 3) {
-						Assert.assertEquals("japan", RowValue.get(0));
-						Assert.assertEquals(2, RowValue.get(1));
-					}
-					if (count2 == 4) {
-						Assert.assertEquals("japan", RowValue.get(0));
-						Assert.assertEquals(1, RowValue.get(1));
-					}
-
-					if (count1 == 5) {
-						Assert.assertEquals("nouse", RowValue.get(0));
-						Assert.assertEquals(5, RowValue.get(1));
-					}
-					if (count1 == 6) {
-						Assert.assertEquals("nowhere", RowValue.get(0));
-						Assert.assertEquals(4, RowValue.get(1));
-					}
-				}// if test2
-				// if test3 other table
-				if (query2.contains("test3")) {
-					if (count2 == 1) {
-						Assert.assertEquals("japan", RowValue.get(0));
-						Assert.assertEquals(1, RowValue.get(1));
-					}
-					if (count2 == 2) {
-						Assert.assertEquals("japan", RowValue.get(0));
-						Assert.assertEquals(2, RowValue.get(1));
-					}
-					if (count2 == 3) {
-						Assert.assertEquals("india", RowValue.get(0));
-						Assert.assertEquals(3, RowValue.get(1));
-					}
-					if (count2 == 4) {
-						Assert.assertEquals("india", RowValue.get(0));
-						Assert.assertEquals(4, RowValue.get(1));
-					}
-					if (count1 == 5) {
-						Assert.assertEquals("nowhere", RowValue.get(0));
-						Assert.assertEquals(4, RowValue.get(1));
-					}
-					if (count1 == 6) {
-						Assert.assertEquals("nouse", RowValue.get(0));
-						Assert.assertEquals(5, RowValue.get(1));
-					}
-
-				}// if test3
-
-			}// while
-			if (query2.contains("test1") || query2.contains("test2")
-					|| query2.contains("test3")) {
-				Assert.assertEquals(6, count2);
-			}
-		}// if query2 != null
-
-	}
-
-	@Test
-	public void test1() throws ParseException, IOException,
-	org.apache.hadoop.zebra.parser.ParseException, Exception {
-		/*
-		 * test positive test case. schema, projection, sortInfo are all good ones.
-		 */
-		System.out.println("******Starttt  testcase: " + getCurrentMethodName());
-		List<Path> paths = new ArrayList<Path>(2);
-
-		sortKey = "word,count";
-		System.out.println("hello sort on word and count");
-		String methodName = getCurrentMethodName();
-		String myMultiLocs = null;
-		
-    Path path1 = getTableFullPath("us" + methodName);
-    Path path2 = getTableFullPath("others" + methodName);
-    myMultiLocs = path1.toString() + "," + path2.toString();
-    paths.add(path1);
-    paths.add(path2);
-		
-		getTablePaths(myMultiLocs);
-		removeDir(new Path(strTable1));
-		removeDir(new Path(strTable2));
-		String schema = "word:string, count:int";
-		String storageHint = "[word];[count]";
-		runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-		checkTable(myMultiLocs);
-		System.out.println("DONE test " + getCurrentMethodName());
-	}
-
-	@Test(expected = ParseException.class)
-	public void test2() throws ParseException, IOException,
-	org.apache.hadoop.zebra.parser.ParseException, Exception {
-		/*
-		 * test negative test case. wrong schema fomat: schema = "{, count:int";
-		 */
-		System.out.println("******Starttt  testcase: " + getCurrentMethodName());
-		List<Path> paths = new ArrayList<Path>(2);
-
-		sortKey = "word,count";
-		System.out.println("hello sort on word and count");
-		String methodName = getCurrentMethodName();
-		String myMultiLocs = null;
-
-    Path path1 = getTableFullPath("us" + methodName);
-    Path path2 = getTableFullPath("others" + methodName);
-    myMultiLocs = path1.toString() + "," + path2.toString();
-    paths.add(path1);
-    paths.add(path2);
-
-		
-		getTablePaths(myMultiLocs);
-		removeDir(new Path(strTable1));
-		removeDir(new Path(strTable2));
-		String schema = "{, count:int";
-		String storageHint = "[word];[count]";
-		
-		if (mode == TestMode.cluster) {
-      try { 
-        runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-      } catch (ParseException e) {
-        System.out.println(e.getMessage());
-        System.out.println("done test 2");
-        return;
-      }
-      // should not reach here
-      Assert.fail("in try, should have thrown exception");
-      
-    } else {
-      runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-      System.out.println("done test 2");
-    }
-	}
-
-	@Test(expected = IOException.class)
-	public void test3() throws ParseException, IOException,
-	org.apache.hadoop.zebra.parser.ParseException, Exception {
-		/*
-		 * test negative test case. non-exist sort key
-		 */
-		System.out.println("******Starttt  testcase: " + getCurrentMethodName());
-		List<Path> paths = new ArrayList<Path>(2);
-
-		sortKey = "not exist";
-		System.out.println("hello sort on word and count");
-		String methodName = getCurrentMethodName();
-		String myMultiLocs = null;
-
-    Path path1 = getTableFullPath("us" + methodName);
-    Path path2 = getTableFullPath("others" + methodName);
-    myMultiLocs = path1.toString() + "," + path2.toString();
-    paths.add(path1);
-    paths.add(path2);
-		
-		getTablePaths(myMultiLocs);
-		removeDir(new Path(strTable1));
-		removeDir(new Path(strTable2));
-		String schema = "word:string, count:int";
-		String storageHint = "[word];[count]";
-		
-		if (mode == TestMode.cluster) {
-      try {
-        runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-      } catch (IOException e) {
-        System.out.println(e.getMessage());
-        System.out.println("done test 3");
-        return;
-      }
-      // should not reach here
-      Assert.fail("in try, should have thrown exception");
-      
-    } else {
-      runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-      System.out.println("done test 3");
-    }
-	}
-
-	@Test(expected = NullPointerException.class)
-	public void test5() throws ParseException, IOException,
-	org.apache.hadoop.zebra.parser.ParseException, Exception {
-		/*
-		 * test negative test case. sort key null
-		 */
-		System.out.println("******Starttt  testcase: " + getCurrentMethodName());
-		List<Path> paths = new ArrayList<Path>(2);
-
-		sortKey = null;
-		System.out.println("hello sort on word and count");
-		String methodName = getCurrentMethodName();
-		String myMultiLocs = null;
-
-    Path path1 = getTableFullPath("us" + methodName);
-    Path path2 = getTableFullPath("others" + methodName);
-    myMultiLocs = path1.toString() + "," + path2.toString();
-    paths.add(path1);
-    paths.add(path2);
-		
-		getTablePaths(myMultiLocs);
-		removeDir(new Path(strTable1));
-		removeDir(new Path(strTable2));
-		String schema = "word:string, count:int";
-		String storageHint = "[word];[count]";
-
-    if (mode == TestMode.cluster) {
-      try {
-        runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-      } catch (NullPointerException e) {
-        System.out.println(e.getMessage());
-        System.out.println("done test 5");
-        return;
-      }
-      // should not reach here
-      Assert.fail("in try, should have thrown exception");
-      
-    } else {
-      runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-      System.out.println("done test 5");
-    }
-	}
-
-	@Test(expected = ParseException.class)
-	public void test6() throws ParseException, IOException,
-	org.apache.hadoop.zebra.parser.ParseException, Exception {
-		/*
-		 * test negative test case. storage hint: none exist column
-		 */
-		System.out.println("******Starttt  testcase: " + getCurrentMethodName());
-		List<Path> paths = new ArrayList<Path>(2);
-
-		sortKey = "word,count";
-		System.out.println("hello sort on word and count");
-		String methodName = getCurrentMethodName();
-		String myMultiLocs = null;
-		
-    Path path1 = getTableFullPath("us" + methodName);
-    Path path2 = getTableFullPath("others" + methodName);
-    myMultiLocs = path1.toString() + "," + path2.toString();  
-    paths.add(path1);
-    paths.add(path2);
-
-		getTablePaths(myMultiLocs);
-		removeDir(new Path(strTable1));
-		removeDir(new Path(strTable2));
-		String schema = "word:string, count:int";
-		String storageHint = "[none-exist-column]";
-		//runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-
-		if (mode == TestMode.cluster) {
-      try {
-        runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-      } catch (ParseException e) {
-        System.out.println(e.getMessage());
-        System.out.println("done test 6");
-        return;
-      }
-      // should not reach here
-      Assert.fail("in try, should have thrown exception");
-    } else {
-      runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-      System.out.println("done test 6");
-    }
-	}
-
-	@Test(expected = ParseException.class)
-	public void test7() throws ParseException, IOException,
-	org.apache.hadoop.zebra.parser.ParseException, Exception {
-		/*
-		 * test negative test case. storage hint: wrong storage hint format, missing
-		 * [
-		 */
-		System.out.println("******Starttt  testcase: " + getCurrentMethodName());
-		List<Path> paths = new ArrayList<Path>(2);
-
-		sortKey = "word,count";
-		System.out.println("hello sort on word and count");
-		String methodName = getCurrentMethodName();
-		String myMultiLocs = null;
-		
-    Path path1 = getTableFullPath("us" + methodName);
-    Path path2 = getTableFullPath("others" + methodName);
-    myMultiLocs = path1.toString() + "," + path2.toString();
-    paths.add(path1);
-    paths.add(path2);
-		
-		getTablePaths(myMultiLocs);
-		removeDir(new Path(strTable1));
-		removeDir(new Path(strTable2));
-		String schema = "word:string, count:int";
-		String storageHint = "none-exist-column]";
-		//runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-
-		if (mode == TestMode.cluster) {
-      try {
-        runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-      } catch (ParseException e) {
-        System.out.println(e.getMessage());
-        System.out.println("done test 7");  
-        return;
-      }
-      // should not reach here
-      Assert.fail("in try, should have thrown exception");
-    } else {
-      runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-      System.out.println("done test 7");
-    }
-	}
-
-	@Test(expected = ParseException.class)
-	public void test8() throws ParseException, IOException,
-	org.apache.hadoop.zebra.parser.ParseException, Exception {
-		/*
-		 * test negative test case. schema defines more columns then the input file.
-		 * user input has only two fields
-		 */
-		System.out.println("******Starttt  testcase: " + getCurrentMethodName());
-		List<Path> paths = new ArrayList<Path>(2);
-
-		sortKey = "word,count";
-		System.out.println("hello sort on word and count");
-		String methodName = getCurrentMethodName();
-		String myMultiLocs = null;
-
-    Path path1 = getTableFullPath("us" + methodName);
-    Path path2 = getTableFullPath("others" + methodName);
-    myMultiLocs = path1.toString() + "," + path2.toString(); 
-    paths.add(path1);
-    paths.add(path2);
-		
-		getTablePaths(myMultiLocs);
-		removeDir(new Path(strTable1));
-		removeDir(new Path(strTable2));
-		String schema = "word:string, count:int,word:string, count:int";
-		String storageHint = "[word];[count]";
-		//runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-		
-		if (mode == TestMode.cluster) {
-      try {
-        runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-      } catch (ParseException e) {
-        System.out.println(e.getMessage());
-        System.out.println("done test 8");
-        return;
-      }
-
-      // should not reach here
-      Assert.fail("in try, should have thrown exception");      
-    } else {
-      runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-      System.out.println("done test 8");
-    }
-	}
-
-	@Test(expected = ParseException.class)
-	public void test9() throws ParseException, IOException,
-	org.apache.hadoop.zebra.parser.ParseException, Exception {
-		/*
-		 * test negative test case. data type defined in schema is wrong. it is
-		 * inttt instead of int
-		 */
-		System.out.println("******Starttt  testcase: " + getCurrentMethodName());
-		List<Path> paths = new ArrayList<Path>(2);
-
-		sortKey = "word,count";
-		System.out.println("hello sort on word and count");
-		String methodName = getCurrentMethodName();
-		String myMultiLocs = null;
-		
-    Path path1 = getTableFullPath("us" + methodName);
-    Path path2 = getTableFullPath("others" + methodName);
-    myMultiLocs = path1.toString() + "," + path2.toString(); 
-    paths.add(path1);
-    paths.add(path2);
-		
-		getTablePaths(myMultiLocs);
-		removeDir(new Path(strTable1));
-		removeDir(new Path(strTable2));
-		String schema = "word:string, count:inttt";
-		String storageHint = "[word];[count]";
-		//runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-		
-		if (mode == TestMode.cluster) {
-      try {
-        runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-      } catch (ParseException e) {
-        System.out.println(e.getMessage());
-        System.out.println("done test 9");
-        return;
-      }
-
-      // should not reach here
-      Assert.fail("in try, should have thrown exception");
-
-    } else {
-      runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-      System.out.println("done test 9");
-    }
-
-	}
-
-	@Test(expected = ParseException.class)
-	public void test10() throws ParseException, IOException,
-	org.apache.hadoop.zebra.parser.ParseException, Exception {
-		/*
-		 * test negative test case. schema format is wrong, schema is seperated by ;
-		 * instead of ,
-		 */
-		System.out.println("******Starttt  testcase: " + getCurrentMethodName());
-		List<Path> paths = new ArrayList<Path>(2);
-
-		sortKey = "word,count";
-		System.out.println("hello sort on word and count");
-		String methodName = getCurrentMethodName();
-		String myMultiLocs = null;
-		
-    Path path1 = getTableFullPath("us" + methodName);
-    Path path2 = getTableFullPath("others" + methodName);
-    myMultiLocs = path1.toString() + "," + path2.toString(); 
-    paths.add(path1);
-    paths.add(path2);
-		
-		getTablePaths(myMultiLocs);
-		removeDir(new Path(strTable1));
-		removeDir(new Path(strTable2));
-		String schema = "word:string; count:int";
-		String storageHint = "[word];[count]";
-		//runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-		
-		if (mode == TestMode.cluster) {
-      try {
-        runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-      } catch (ParseException e) {
-        System.out.println(e.getMessage());
-        System.out.println("done test 10");
-        return;
-      }
-     
-      // should not reach here
-      Assert.fail("in try, should have thrown exception");
-    } else {
-      runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-      System.out.println("done test 10");
-    }    
-	}
-
-	static class MapClass extends
-	Mapper<LongWritable, Text, BytesWritable, ZebraTuple> {
-		private BytesWritable bytesKey;
-		private ZebraTuple tupleRow;
-		private Object javaObj;
-
-		@Override
-		public void map(LongWritable key, Text value, Context context)
-		throws IOException, InterruptedException {
-			// value should contain "word count"
-			String[] wdct = value.toString().split(" ");
-			if (wdct.length != 2) {
-				// LOG the error
-				return;
-			}
-
-			byte[] word = wdct[0].getBytes();
-			bytesKey.set(word, 0, word.length);
-			System.out.println("word: " + new String(word));
-			tupleRow.set(0, new String(word));
-			tupleRow.set(1, Integer.parseInt(wdct[1]));
-			System.out.println("count:  " + Integer.parseInt(wdct[1]));
-
-			// This key has to be created by user
-			/*
-			 * Tuple userKey = new DefaultTuple(); userKey.append(new String(word));
-			 * userKey.append(Integer.parseInt(wdct[1]));
-			 */
-			System.out.println("in map, sortkey: " + sortKey);
-			ZebraTuple userKey = new ZebraTuple();
-			if (sortKey.equalsIgnoreCase("word,count")) {
-				userKey.append(new String(word));
-				userKey.append(Integer.parseInt(wdct[1]));
-			}
-
-			if (sortKey.equalsIgnoreCase("count")) {
-				userKey.append(Integer.parseInt(wdct[1]));
-			}
-
-			if (sortKey.equalsIgnoreCase("word")) {
-				userKey.append(new String(word));
-			}
-
-			try {
-
-				/* New M/R Interface */
-				/* Converts user key to zebra BytesWritable key */
-				/* using sort key expr tree */
-				/* Returns a java base object */
-				/* Done for each user key */
-
-				bytesKey = BasicTableOutputFormat.getSortKey(javaObj, userKey);
-			} catch (Exception e) {
-
-			}
-
-			context.write(bytesKey, tupleRow);
-		}
-
-		@Override
-		public void setup(Context context) {
-			bytesKey = new BytesWritable();
-			sortKey = context.getConfiguration().get("sortKey");
-			try {
-				Schema outSchema = BasicTableOutputFormat.getSchema(context);
-				tupleRow = (ZebraTuple) TypesUtils.createTuple(outSchema);
-				javaObj = BasicTableOutputFormat.getSortKeyGenerator(context);
-			} catch (IOException e) {
-				throw new RuntimeException(e);
-			} catch (org.apache.hadoop.zebra.parser.ParseException e) {
-				throw new RuntimeException(e);
-			}
-		}
-
-	}
-
-	static class ReduceClass extends
-	Reducer<BytesWritable, Tuple, BytesWritable, Tuple> {
-		Tuple outRow;
-
-		public void reduce(BytesWritable key, Iterator<Tuple> values, Context context)
-		throws IOException, InterruptedException {
-			try {
-				for (; values.hasNext();) {
-					context.write(key, values.next());
-				}
-			} catch (ExecException e) {
-				e.printStackTrace();
-			}
-		}
-
-	}
-
-	static class OutputPartitionerClass extends ZebraOutputPartition {
-
-		@Override
-		public int getOutputPartition(BytesWritable key, Tuple value)
-		throws IndexOutOfBoundsException, ExecException {
-
-			System.out.println("value0 : " + value.get(0));
-			String reg = null;
-			try {
-				reg = (String) (value.get(0));
-			} catch (Exception e) {
-				//
-			}
-
-			if (reg.equals("us"))
-				return 0;
-			else
-				return 1;
-		}
-
-	}
-
-	public static final class MemcmpRawComparator implements
-	RawComparator<Object>, Serializable {
-		@Override
-		public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
-			return WritableComparator.compareBytes(b1, s1, l1, b2, s2, l2);
-		}
-
-		@Override
-		public int compare(Object o1, Object o2) {
-
-			throw new RuntimeException("Object comparison not supported");
-		}
-	}
-
-	public void runMR(String sortKey, String schema, String storageHint,
-			Path... paths) throws ParseException, IOException, Exception,
-			org.apache.hadoop.zebra.parser.ParseException {
-
-		Job job = new Job(conf);
-		job.setJobName("TestTypedAPI");
-		job.setJarByClass(TestTypedApi.class);
-		Configuration conf = job.getConfiguration();
-		conf.set("table.output.tfile.compression", "gz");
-		conf.set("sortKey", sortKey);
-		// input settings
-		job.setInputFormatClass(TextInputFormat.class);
-		job.setMapperClass(TestTypedApi.MapClass.class);
-		job.setMapOutputKeyClass(BytesWritable.class);
-		job.setMapOutputValueClass(ZebraTuple.class);
-		FileInputFormat.setInputPaths(job, inputPath);
-
-		// output settings
-
-		job.setOutputFormatClass(BasicTableOutputFormat.class);
-
-		BasicTableOutputFormat.setMultipleOutputs(job,
-				TestTypedApi.OutputPartitionerClass.class, paths);
-		ZebraSchema zSchema = ZebraSchema.createZebraSchema(schema);
-		ZebraStorageHint zStorageHint = ZebraStorageHint
-		.createZebraStorageHint(storageHint);
-		ZebraSortInfo zSortInfo = ZebraSortInfo.createZebraSortInfo(sortKey, null);
-		BasicTableOutputFormat.setStorageInfo(job, zSchema, zStorageHint,
-				zSortInfo);
-		System.out.println("in runMR, sortkey: " + sortKey);
-
-		job.setNumReduceTasks(1);
-		job.submit();
-		job.waitForCompletion( true );
-		BasicTableOutputFormat.close( job );
-	}
-
-	@Override
-	public int run(String[] args) throws Exception {
-		TestTypedApi test = new TestTypedApi();
-		TestTypedApi.setUpOnce();
-		test.test1();
-		test.test2();
-		test.test3();
-		
-    //TODO: backend exception - will migrate later
-		//test.test4();
-		
-		test.test5();
-		test.test6();
-		test.test7();
-		test.test8();
-		test.test9();
-		test.test10();
-		
-		return 0;
-	}
-  
-  public static void main(String[] args) throws Exception {
-    conf = new Configuration();
-    
-    int res = ToolRunner.run(conf, new TestTypedApi(), args);
-    System.out.println("PASS");    
-    System.exit(res);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestTypedApi2.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestTypedApi2.java
deleted file mode 100644
index 05ecd05d3..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/TestTypedApi2.java
+++ /dev/null
@@ -1,544 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.BufferedWriter;
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.FileWriter;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.io.Serializable;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-import java.util.StringTokenizer;
-import junit.framework.Assert;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.RawComparator;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.WritableComparator;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.mapreduce.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.mapreduce.ZebraOutputPartition;
-import org.apache.hadoop.zebra.mapreduce.ZebraSchema;
-import org.apache.hadoop.zebra.mapreduce.ZebraSortInfo;
-import org.apache.hadoop.zebra.mapreduce.ZebraStorageHint;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.types.ZebraTuple;
-import org.apache.pig.ExecType;
-import org.apache.pig.backend.executionengine.ExecException;
-
-import org.apache.pig.data.Tuple;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * This is a sample a complete MR sample code for Table. It doens't contain
- * 'read' part. But, it should be similar and easier to write. Refer to test
- * cases in the same directory.
- * 
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * us 2
- * japan 2
- * india 4
- * us 2
- * japan 1
- * india 3
- * nouse 5
- * nowhere 4
- * 
- * 
- */
-public class TestTypedApi2 extends BaseTestCase implements Tool{
-	static String inputPath;
-	static String inputFileName = "multi-input.txt";
-	public static String sortKey = null;
-
-	private static String strTable1 = null;
-	private static String strTable2 = null;
-
-	@BeforeClass
-	public static void setUpOnce() throws Exception {
-    init();
-    
-    inputPath = getTableFullPath(inputFileName).toString();
-    
-    writeToFile(inputPath);
-	}
-
-	public String getCurrentMethodName() {
-		ByteArrayOutputStream baos = new ByteArrayOutputStream();
-		PrintWriter pw = new PrintWriter(baos);
-		(new Throwable()).printStackTrace(pw);
-		pw.flush();
-		String stackTrace = baos.toString();
-		pw.close();
-
-		StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-		tok.nextToken(); // 'java.lang.Throwable'
-		tok.nextToken(); // 'at ...getCurrentMethodName'
-		String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-		// Parse line 3
-		tok = new StringTokenizer(l.trim(), " <(");
-		String t = tok.nextToken(); // 'at'
-		t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-		StringTokenizer st = new StringTokenizer(t, ".");
-		String methodName = null;
-		while (st.hasMoreTokens()) {
-			methodName = st.nextToken();
-		}
-		return methodName;
-	}
-
-	public static void writeToFile(String inputFile) throws IOException {
-		if (mode == TestMode.local) {
-			FileWriter fstream = new FileWriter(inputFile);
-			BufferedWriter out = new BufferedWriter(fstream);
-			out.write("us 2\n");
-			out.write("japan 2\n");
-			out.write("india 4\n");
-			out.write("us 2\n");
-			out.write("japan 1\n");
-			out.write("india 3\n");
-			out.write("nouse 5\n");
-			out.write("nowhere 4\n");
-			out.close();
-		}
-		
-		if (mode == TestMode.cluster) {
-			FSDataOutputStream fout = fs.create(new Path(inputFile));
-			fout.writeBytes("us 2\n");
-			fout.writeBytes("japan 2\n");
-			fout.writeBytes("india 4\n");
-			fout.writeBytes("us 2\n");
-			fout.writeBytes("japan 1\n");
-			fout.writeBytes("india 3\n");
-			fout.writeBytes("nouse 5\n");
-			fout.writeBytes("nowhere 4\n");
-			fout.close();
-		}
-	}
-
-	public static void getTablePaths(String myMultiLocs) {
-		StringTokenizer st = new StringTokenizer(myMultiLocs, ",");
-
-		// get how many tokens inside st object
-		System.out.println("tokens count: " + st.countTokens());
-		int count = 0;
-
-		// iterate st object to get more tokens from it
-		while (st.hasMoreElements()) {
-			count++;
-			String token = st.nextElement().toString();
-			if (count == 1)
-				strTable1 = token;
-			if (count == 2)
-				strTable2 = token;
-
-			System.out.println("token = " + token);
-		}
-	}
-
-	public static void checkTable(String myMultiLocs) throws IOException {
-		System.out.println("myMultiLocs:" + myMultiLocs);
-		System.out.println("sorgetTablePathst key:" + sortKey);
-
-		getTablePaths(myMultiLocs);
-		String query1 = null;
-		String query2 = null;
-
-		if (strTable1 != null) {
-
-			query1 = "records1 = LOAD '" + strTable1
-			+ "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		}
-		if (strTable2 != null) {
-			query2 = "records2 = LOAD '" + strTable2
-			+ "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		}
-
-		int count1 = 0;
-		int count2 = 0;
-
-		if (query1 != null) {
-			System.out.println(query1);
-			pigServer.registerQuery(query1);
-			Iterator<Tuple> it = pigServer.openIterator("records1");
-			while (it.hasNext()) {
-				count1++;
-				Tuple RowValue = it.next();
-				System.out.println(RowValue);
-				// test 1 us table
-				if (query1.contains("test1") || query1.contains("test2")
-						|| query1.contains("test3")) {
-
-					if (count1 == 1) {
-						Assert.assertEquals("us", RowValue.get(0));
-						Assert.assertEquals(2, RowValue.get(1));
-					}
-					if (count1 == 2) {
-						Assert.assertEquals("us", RowValue.get(0));
-						Assert.assertEquals(2, RowValue.get(1));
-					}
-				} // test1, test2
-
-			}// while
-			if (query1.contains("test1") || query1.contains("test2")
-					|| query1.contains("test3")) {
-				Assert.assertEquals(2, count1);
-			}
-		}// if query1 != null
-
-		if (query2 != null) {
-			pigServer.registerQuery(query2);
-			Iterator<Tuple> it = pigServer.openIterator("records2");
-
-			while (it.hasNext()) {
-				count2++;
-				Tuple RowValue = it.next();
-				System.out.println(RowValue);
-
-				// if test1 other table
-				if (query2.contains("test1")) {
-					if (count2 == 1) {
-						Assert.assertEquals("india", RowValue.get(0));
-						Assert.assertEquals(3, RowValue.get(1));
-					}
-					if (count2 == 2) {
-						Assert.assertEquals("india", RowValue.get(0));
-						Assert.assertEquals(4, RowValue.get(1));
-					}
-					if (count2 == 3) {
-						Assert.assertEquals("japan", RowValue.get(0));
-						Assert.assertEquals(1, RowValue.get(1));
-					}
-					if (count2 == 4) {
-						Assert.assertEquals("japan", RowValue.get(0));
-						Assert.assertEquals(2, RowValue.get(1));
-					}
-
-					if (count1 == 5) {
-						Assert.assertEquals("nouse", RowValue.get(0));
-						Assert.assertEquals(5, RowValue.get(1));
-					}
-					if (count1 == 6) {
-						Assert.assertEquals("nowhere", RowValue.get(0));
-						Assert.assertEquals(4, RowValue.get(1));
-					}
-				}// if test1
-				// if test2 other table
-				if (query2.contains("test2")) {
-					if (count2 == 1) {
-						Assert.assertEquals("india", RowValue.get(0));
-						Assert.assertEquals(4, RowValue.get(1));
-					}
-					if (count2 == 2) {
-						Assert.assertEquals("india", RowValue.get(0));
-						Assert.assertEquals(3, RowValue.get(1));
-					}
-					if (count2 == 3) {
-						Assert.assertEquals("japan", RowValue.get(0));
-						Assert.assertEquals(2, RowValue.get(1));
-					}
-					if (count2 == 4) {
-						Assert.assertEquals("japan", RowValue.get(0));
-						Assert.assertEquals(1, RowValue.get(1));
-					}
-
-					if (count1 == 5) {
-						Assert.assertEquals("nouse", RowValue.get(0));
-						Assert.assertEquals(5, RowValue.get(1));
-					}
-					if (count1 == 6) {
-						Assert.assertEquals("nowhere", RowValue.get(0));
-						Assert.assertEquals(4, RowValue.get(1));
-					}
-				}// if test2
-				// if test3 other table
-				if (query2.contains("test3")) {
-					if (count2 == 1) {
-						Assert.assertEquals("japan", RowValue.get(0));
-						Assert.assertEquals(1, RowValue.get(1));
-					}
-					if (count2 == 2) {
-						Assert.assertEquals("japan", RowValue.get(0));
-						Assert.assertEquals(2, RowValue.get(1));
-					}
-					if (count2 == 3) {
-						Assert.assertEquals("india", RowValue.get(0));
-						Assert.assertEquals(3, RowValue.get(1));
-					}
-					if (count2 == 4) {
-						Assert.assertEquals("india", RowValue.get(0));
-						Assert.assertEquals(4, RowValue.get(1));
-					}
-					if (count1 == 5) {
-						Assert.assertEquals("nowhere", RowValue.get(0));
-						Assert.assertEquals(4, RowValue.get(1));
-					}
-					if (count1 == 6) {
-						Assert.assertEquals("nouse", RowValue.get(0));
-						Assert.assertEquals(5, RowValue.get(1));
-					}
-
-				}// if test3
-
-			}// while
-			if (query2.contains("test1") || query2.contains("test2")
-					|| query2.contains("test3")) {
-				Assert.assertEquals(6, count2);
-			}
-		}// if query2 != null
-
-	}
-
-	@Test
-	public void test1() throws ParseException, IOException,
-	org.apache.hadoop.zebra.parser.ParseException, Exception {
-		/*
-		 * test positive test case. User defined comparator class
-		 */
-		System.out.println("******Start testcase: " + getCurrentMethodName());
-		List<Path> paths = new ArrayList<Path>(3);
-		sortKey = "word,count";
-		System.out.println("hello sort on word and count");
-		String methodName = getCurrentMethodName();
-		String myMultiLocs = null;
-		
-		Path path1 = getTableFullPath("us" + methodName);
-		Path path2 = getTableFullPath("others" + methodName);
-		paths.add(path1);
-		paths.add(path2);
-		myMultiLocs = path1.toString() + "," + path2.toString();		
-		
-		getTablePaths(myMultiLocs);
-		removeDir(new Path(strTable1));
-		removeDir(new Path(strTable2));
-		String schema = "word:string, count:int";
-		String storageHint = "[word];[count]";
-		
-		runMR(sortKey, schema, storageHint, paths.toArray(new Path[2]));
-		// runMR( sortKey, schema, storageHint, myMultiLocs);
-		checkTable(myMultiLocs);
-		System.out.println("DONE test " + getCurrentMethodName());
-
-	}
-
-	static class MapClass extends
-	Mapper<LongWritable, Text, BytesWritable, ZebraTuple> {
-		private BytesWritable bytesKey;
-		private ZebraTuple tupleRow;
-		private Object javaObj;
-
-		@Override
-		public void map(LongWritable key, Text value, Context context)
-		throws IOException, InterruptedException {
-			// value should contain "word count"
-			String[] wdct = value.toString().split(" ");
-			if (wdct.length != 2) {
-				// LOG the error
-				return;
-			}
-
-			byte[] word = wdct[0].getBytes();
-			bytesKey.set(word, 0, word.length);
-			System.out.println("word: " + new String(word));
-			tupleRow.set(0, new String(word));
-			tupleRow.set(1, Integer.parseInt(wdct[1]));
-			System.out.println("count:  " + Integer.parseInt(wdct[1]));
-
-			// This key has to be created by user
-			/*
-			 * Tuple userKey = new DefaultTuple(); userKey.append(new String(word));
-			 * userKey.append(Integer.parseInt(wdct[1]));
-			 */
-			System.out.println("in map, sortkey: " + sortKey);
-			ZebraTuple userKey = new ZebraTuple();
-			if (sortKey.equalsIgnoreCase("word,count")) {
-				userKey.append(new String(word));
-				userKey.append(Integer.parseInt(wdct[1]));
-			}
-
-			if (sortKey.equalsIgnoreCase("count")) {
-				userKey.append(Integer.parseInt(wdct[1]));
-			}
-
-			if (sortKey.equalsIgnoreCase("word")) {
-				userKey.append(new String(word));
-			}
-
-			try {
-
-				/* New M/R Interface */
-				/* Converts user key to zebra BytesWritable key */
-				/* using sort key expr tree */
-				/* Returns a java base object */
-				/* Done for each user key */
-
-				bytesKey = BasicTableOutputFormat.getSortKey(javaObj, userKey);
-			} catch (Exception e) {
-
-			}
-
-			context.write(bytesKey, tupleRow);
-		}
-
-		@Override
-		public void setup(Context context) {
-			bytesKey = new BytesWritable();
-			sortKey = context.getConfiguration().get("sortKey");
-			try {
-				Schema outSchema = BasicTableOutputFormat.getSchema(context);
-				tupleRow = (ZebraTuple) TypesUtils.createTuple(outSchema);
-				javaObj = BasicTableOutputFormat.getSortKeyGenerator(context);
-			} catch (IOException e) {
-				throw new RuntimeException(e);
-			} catch (org.apache.hadoop.zebra.parser.ParseException e) {
-				throw new RuntimeException(e);
-			}
-		}
-
-	}
-
-	static class ReduceClass extends
-	Reducer<BytesWritable, Tuple, BytesWritable, Tuple> {
-		Tuple outRow;
-
-		public void reduce(BytesWritable key, Iterator<Tuple> values, Context context)
-		throws IOException, InterruptedException {
-			try {
-				for (; values.hasNext();) {
-					context.write(key, values.next());
-				}
-			} catch (ExecException e) {
-				e.printStackTrace();
-			}
-		}
-
-	}
-
-	static class OutputPartitionerClass extends ZebraOutputPartition {
-		@Override
-		public int getOutputPartition(BytesWritable key, Tuple value)
-		throws IndexOutOfBoundsException, ExecException {
-
-			System.out.println("value0 : " + value.get(0));
-			String reg = null;
-			try {
-				reg = (String) (value.get(0));
-			} catch (Exception e) {
-				//
-			}
-
-			if (reg.equals("us"))
-				return 0;
-			else
-				return 1;
-		}
-	}
-
-	public static final class MemcmpRawComparator implements
-	RawComparator<Object>, Serializable {
-		@Override
-		public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
-			return WritableComparator.compareBytes(b1, s1, l1, b2, s2, l2);
-		}
-
-		@Override
-		public int compare(Object o1, Object o2) {
-
-			throw new RuntimeException("Object comparison not supported");
-		}
-	}
-
-	public void runMR(String sortKey, String schema, String storageHint,
-			Path... paths) throws ParseException, IOException, Exception,
-			org.apache.hadoop.zebra.parser.ParseException {
-
-		Job job = new Job(conf);
-		job.setJobName("TestTypedApi2");
-		job.setJarByClass(TestTypedApi2.class);
-		Configuration conf = job.getConfiguration();
-		conf.set("table.output.tfile.compression", "gz");
-		conf.set("sortKey", sortKey);
-		// input settings
-		job.setInputFormatClass(TextInputFormat.class);
-		job.setMapperClass(TestTypedApi2.MapClass.class);
-		job.setMapOutputKeyClass(BytesWritable.class);
-		job.setMapOutputValueClass(ZebraTuple.class);
-		FileInputFormat.setInputPaths(job, inputPath);
-
-		// TODO:
-		//job.setNumMapTasks(1);
-
-		// output settings
-
-		job.setOutputFormatClass(BasicTableOutputFormat.class);
-		BasicTableOutputFormat.setMultipleOutputs(job,
-				TestTypedApi2.OutputPartitionerClass.class, paths);
-
-		ZebraSchema zSchema = ZebraSchema.createZebraSchema(schema);
-		ZebraStorageHint zStorageHint = ZebraStorageHint
-		.createZebraStorageHint(storageHint);
-		ZebraSortInfo zSortInfo = ZebraSortInfo.createZebraSortInfo(sortKey,
-				TestTypedApi2.MemcmpRawComparator.class);
-		BasicTableOutputFormat.setStorageInfo(job, zSchema, zStorageHint,
-				zSortInfo);
-		System.out.println("in runMR, sortkey: " + sortKey);
-
-		job.setNumReduceTasks(1);
-		job.submit();
-		job.waitForCompletion( true );
-		BasicTableOutputFormat.close( job );
-	}
-
-	@Override
-	public int run(String[] args) throws Exception {
-		TestTypedApi2 test = new TestTypedApi2();
-		TestTypedApi2.setUpOnce();
-		test.test1();
-
-		return 0;
-	}
-	
-  public static void main(String[] args) throws Exception {
-    conf = new Configuration();
-    
-    int res = ToolRunner.run(conf, new TestTypedApi2(), args);
-    System.out.println("PASS");
-    System.exit(res);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/ToolTestComparator.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/ToolTestComparator.java
deleted file mode 100644
index 68b4ba8c9..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/mapreduce/ToolTestComparator.java
+++ /dev/null
@@ -1,927 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.mapreduce;
-
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.io.PrintStream;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.ArrayList;
-import java.util.Map;
-import java.util.Random;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.mapreduce.SortedTableSplit;
-import org.apache.hadoop.zebra.mapreduce.TableInputFormat;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.pig.TableStorer;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.executionengine.ExecJob;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.Assert;
-
-/**
- * TestComparator
- * 
- * Utility for verifying tables created during Zebra Stress Testing
- * 
- */
-public class ToolTestComparator extends BaseTestCase {
-	final static String TABLE_SCHEMA = "count:int,seed:int,int1:int,int2:int,str1:string,str2:string,byte1:bytes,"
-		+ "byte2:bytes,float1:float,long1:long,double1:double,m1:map(string),r1:record(f1:string, f2:string),"
-		+ "c1:collection(record(a:string, b:string))";
-	final static String TABLE_STORAGE = "[count,seed,int1,int2,str1,str2,byte1,byte2,float1,long1,double1];[m1#{a}];[r1,c1]";
-
-	private static Random generator = new Random();
-
-	protected static ExecJob pigJob;
-
-	private static int totalNumbCols;
-	private static long totalNumbVerifiedRows;
-
-	/**
-	 * Setup and initialize environment
-	 */
-	public static void setUp() throws Exception {
-		init();
-	}
-
-	/**
-	 * Verify load/store
-	 * 
-	 */
-	public static void verifyLoad(String pathTable1, String pathTable2,
-			int numbCols) throws IOException {
-		System.out.println("verifyLoad()");
-
-		// Load table1
-		String query1 = "table1 = LOAD '" + pathTable1
-		+ "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		System.out.println("verifyLoad() running query : " + query1);
-		pigServer.registerQuery(query1);
-
-		// Load table2
-		String query2 = "table2 = LOAD '" + pathTable2
-		+ "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		System.out.println("verifyLoad() running query : " + query2);
-		pigServer.registerQuery(query2);
-
-		// Get metrics from first table
-		Iterator<Tuple> it1 = pigServer.openIterator("table1");
-
-		int numbCols1 = 0;
-		long numbRows1 = 0;
-
-		while (it1.hasNext()) {
-			++numbRows1; // increment row count
-			Tuple rowValue = it1.next();
-			numbCols1 = rowValue.size();
-			if (numbCols != 0)
-				Assert.assertEquals(
-						"Verify failed - Table1 has wrong number of expected columns "
-						+ "\n row number : " + numbRows1 + "\n expected column size : "
-						+ numbCols + "\n actual columns size  : " + numbCols1,
-						numbCols, numbCols1);
-		}
-
-		// Get metrics from second table
-		Iterator<Tuple> it2 = pigServer.openIterator("table2");
-
-		int numbCols2 = 0;
-		long numbRows2 = 0;
-
-		while (it2.hasNext()) {
-			++numbRows2; // increment row count
-			Tuple rowValue = it2.next();
-			numbCols2 = rowValue.size();
-			if (numbCols != 0)
-				Assert.assertEquals(
-						"Verify failed - Table2 has wrong number of expected columns "
-						+ "\n row number : " + numbRows2 + "\n expected column size : "
-						+ numbCols + "\n actual columns size  : " + numbCols2,
-						numbCols, numbCols2);
-		}
-
-		Assert
-		.assertEquals(
-				"Verify failed - Tables have different number row sizes "
-				+ "\n table1 rows : " + numbRows1 + "\n table2 rows : "
-				+ numbRows2, numbRows1, numbRows2);
-
-		Assert.assertEquals(
-				"Verify failed - Tables have different number column sizes "
-				+ "\n table1 column size : " + numbCols1
-				+ "\n table2 column size : " + numbCols2, numbCols1, numbCols2);
-
-		System.out.println();
-		System.out.println("Verify load - table1 columns : " + numbCols1);
-		System.out.println("Verify load - table2 columns : " + numbCols2);
-		System.out.println("Verify load - table1 rows : " + numbRows1);
-		System.out.println("Verify load - table2 rows : " + numbRows2);
-		System.out.println("Verify load - PASS");
-	}
-
-	/**
-	 * Verify table
-	 * 
-	 */
-	public static void verifyTable(String pathTable1) throws IOException {
-		System.out.println("verifyTable()");
-
-		// Load table1
-		String query1 = "table1 = LOAD '" + pathTable1
-		+ "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		System.out.println("verifyTable() running query : " + query1);
-		pigServer.registerQuery(query1);
-
-		// Get metrics from table
-		Iterator<Tuple> it1 = pigServer.openIterator("table1");
-
-		int numbCols1 = 0;
-		long numbRows1 = 0;
-
-		System.out.println("DEBUG starting to iterate table1");
-
-		while (it1.hasNext()) {
-			++numbRows1; // increment row count
-			Tuple rowValue = it1.next();
-			numbCols1 = rowValue.size();
-		}
-
-		System.out.println();
-		System.out.println("Verify table columns : " + numbCols1);
-		System.out.println("Verify table rows : " + numbRows1);
-		System.out.println("Verify table complete");
-	}
-
-	/**
-	 * Verify sorted
-	 * 
-	 */
-	public static void verifySorted(String pathTable1, String pathTable2,
-			int sortCol, String sortKey, int numbCols, int rowMod)
-	throws IOException, ParseException {
-		System.out.println("verifySorted()");
-
-		// Load table1
-		String query1 = "table1 = LOAD '" + pathTable1
-		+ "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		System.out.println("verifySorted() running query : " + query1);
-		pigServer.registerQuery(query1);
-
-		//
-		// Get metrics from first table (unsorted)
-		//
-		Iterator<Tuple> it1 = pigServer.openIterator("table1");
-
-		int numbCols1 = 0;
-		long numbRows1 = 0;
-
-		System.out.println("DEBUG starting to iterate table1");
-
-		while (it1.hasNext()) {
-			++numbRows1; // increment row count
-			Tuple rowValue = it1.next();
-			numbCols1 = rowValue.size();
-			if (numbCols != 0)
-				Assert.assertEquals(
-						"Verify failed - Table1 has wrong number of expected columns "
-						+ "\n row number : " + numbRows1 + "\n expected column size : "
-						+ numbCols + "\n actual columns size  : " + numbCols1,
-						numbCols, numbCols1);
-		}
-
-		System.out.println();
-		System.out.println("Verify unsorted table1 columns : " + numbCols1);
-		System.out.println("Verify unsorted table1 rows : " + numbRows1);
-
-		System.out.println("\nDEBUG starting to iterate table2");
-
-		//
-		// Get metrics from second table (sorted)
-		//
-		long numbRows2 = verifySortedTable(pathTable2, sortCol, sortKey, numbCols,
-				rowMod, null);
-
-		int numbCols2 = totalNumbCols;
-		long numbVerifiedRows = totalNumbVerifiedRows;
-
-		Assert
-		.assertEquals(
-				"Verify failed - Tables have different number row sizes "
-				+ "\n table1 rows : " + numbRows1 + "\n table2 rows : "
-				+ numbRows2, numbRows1, numbRows2);
-
-		Assert.assertEquals(
-				"Verify failed - Tables have different number column sizes "
-				+ "\n table1 column size : " + numbCols1
-				+ "\n table2 column size : " + numbCols2, numbCols1, numbCols2);
-
-		System.out.println();
-		System.out.println("Verify unsorted table1 columns : " + numbCols1);
-		System.out.println("Verify sorted   table2 columns : " + numbCols2);
-		System.out.println("Verify unsorted table1 rows : " + numbRows1);
-		System.out.println("Verify sorted   table2 rows : " + numbRows2);
-		System.out.println("Verify sorted - numb verified rows : "
-				+ numbVerifiedRows);
-		System.out.println("Verify sorted - sortCol : " + sortCol);
-		System.out.println("Verify sorted - PASS");
-	}
-
-	/**
-	 * Verify merge-join
-	 * 
-	 */
-	public static void verifyMergeJoin(String pathTable1, int sortCol,
-			String sortKey, int numbCols, int rowMod, String verifyDataColName) throws IOException,
-			ParseException {
-		System.out.println("verifyMergeJoin()");
-
-		//
-		// Verify sorted table
-		//
-		long numbRows = verifySortedTable(pathTable1, sortCol, sortKey, numbCols,
-				rowMod, verifyDataColName);
-
-		System.out.println();
-		System.out.println("Verify merge-join   table columns : " + totalNumbCols);
-		System.out.println("Verify merge-join   table rows : " + numbRows);
-		System.out.println("Verify merge-join - numb verified rows : "
-				+ totalNumbVerifiedRows);
-		System.out.println("Verify merge-join - sortCol : " + sortCol);
-		System.out.println("Verify merge-join - PASS");
-	}
-
-	/**
-	 * Verify sorted-union
-	 * 
-	 */
-	public static void verifySortedUnion(ArrayList<String> unionPaths,
-			String pathTable1, int sortCol, String sortKey, int numbCols, int rowMod,
-			String verifyDataColName) throws IOException, ParseException {
-		System.out.println("verifySortedUnion()");
-
-		long numbUnionRows = 0;
-		ArrayList<Long> numbRows = new ArrayList<Long>();
-
-		// Get number of rows from each of the input union tables
-		for (int i = 0; i < unionPaths.size(); ++i) {
-			// Load table1
-			String query1 = "table1 = LOAD '" + unionPaths.get(i)
-			+ "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-			System.out.println("verifySortedUnion() running query : " + query1);
-			pigServer.registerQuery(query1);
-			String orderby1 = "sort1 = ORDER table1 BY " + sortKey + " ;";
-			System.out.println("orderby1 : " + orderby1);
-			pigServer.registerQuery(orderby1);
-
-			// Get metrics for each input sorted table
-			Iterator<Tuple> it1 = pigServer.openIterator("sort1");
-			long numbRows1 = 0;
-
-			while (it1.hasNext()) {
-				++numbRows1; // increment row count
-				it1.next();
-			}
-			numbRows.add(numbRows1);
-			numbUnionRows += numbRows1;
-		}
-
-		//
-		// Verify sorted union table
-		//
-		long numbRows1 = verifySortedTable(pathTable1, sortCol, sortKey, numbCols,
-				rowMod, verifyDataColName);
-
-
-		//
-		// Print all union input tables and rows for each
-		//
-		System.out.println();
-		for (int i = 0; i < unionPaths.size(); ++i) {
-			System.out.println("Input union table" + i + " path  : "
-					+ unionPaths.get(i));
-			System.out.println("Input union table" + i + " rows  : "
-					+ numbRows.get(i));
-		}
-		System.out.println();
-		System.out.println("Input union total rows   : " + numbUnionRows);
-
-		System.out.println();
-		System.out.println("Verify union - table columns : " + totalNumbCols);
-		System.out.println("Verify union - table rows : " + numbRows1);
-		System.out.println("Verify union - numb verified rows : "
-				+ totalNumbVerifiedRows);
-		System.out.println("Verify union - sortCol : " + sortCol);
-
-		/*  Assert.assertEquals(
-        "Verify failed - sorted union table row comparison error "
-            + "\n expected table rows : " + numbUnionRows
-            + "\n actual table rows : " + numbRows1, numbUnionRows, numbRows1);
-		 */
-		System.out.println("Verify union - PASS");
-	}
-
-	/**
-	 * Create unsorted table
-	 * 
-	 */
-	public static void createtable(String pathTable1, long numbRows, int seed,
-			boolean debug) throws ExecException, IOException, ParseException {
-		System.out.println("createtable()");
-
-		Path unsortedPath = new Path(pathTable1);
-
-		// Remove old table (if present)
-		removeDir(unsortedPath);
-
-		// Create table
-		BasicTable.Writer writer = new BasicTable.Writer(unsortedPath,
-				TABLE_SCHEMA, TABLE_STORAGE, conf);
-
-		Schema schema = writer.getSchema();
-		Tuple tuple = TypesUtils.createTuple(schema);
-		TableInserter inserter = writer.getInserter("ins", false);
-
-		Map<String, String> m1 = new HashMap<String, String>();
-
-		Tuple tupRecord1; // record
-		tupRecord1 = TypesUtils.createTuple(schema.getColumnSchema("r1")
-				.getSchema()); // r1 schema
-
-		DataBag bag1 = TypesUtils.createBag();
-		Schema schColl = schema.getColumnSchema("c1").getSchema(); // c1 schema
-		Tuple tupColl1 = TypesUtils.createTuple(schColl);
-		Tuple tupColl2 = TypesUtils.createTuple(schColl);
-
-		int randRange = new Long(numbRows / 10).intValue(); // random range to allow
-		// for duplicate values
-		for (int i = 0; i < numbRows; ++i) {
-			int random = generator.nextInt(randRange);
-
-			TypesUtils.resetTuple(tuple); // reset row tuple
-			m1.clear(); // reset map
-			TypesUtils.resetTuple(tupRecord1); // reset record
-			TypesUtils.resetTuple(tupColl1); // reset collection
-			TypesUtils.resetTuple(tupColl2);
-			bag1.clear();
-
-			tuple.set(0, i); // count
-			tuple.set(1, seed); // seed
-
-			tuple.set(2, i); // int1
-			tuple.set(3, random); // int2
-			tuple.set(4, "string " + i); // str1
-			tuple.set(5, "string random " + random); // str2
-			tuple.set(6, new DataByteArray("byte " + i)); // byte1
-			tuple.set(7, new DataByteArray("byte random " + random)); // byte2
-
-			tuple.set(8, new Float(i * -1)); // float1 negative
-			tuple.set(9, new Long(numbRows - i)); // long1 reverse
-			tuple.set(10, new Double(i * 100)); // double1
-
-			// insert map1
-			m1.put("a", "m1");
-			m1.put("b", "m1 " + i);
-			tuple.set(11, m1);
-
-			// insert record1
-			tupRecord1.set(0, "r1 " + seed);
-			tupRecord1.set(1, "r1 " + i);
-			tuple.set(12, tupRecord1);
-
-			// insert collection1
-			// tupColl1.set(0, "c1 a " + seed);
-			// tupColl1.set(1, "c1 a " + i);
-			// bag1.add(tupColl1); // first collection item
-      bag1.add(tupRecord1); // first collection item
-      bag1.add(tupRecord1); // second collection item
-
-			// tupColl2.set(0, "c1 b " + seed);
-			// tupColl2.set(1, "c1 b " + i);
-			// bag1.add(tupColl2); // second collection item
-
-			tuple.set(13, bag1);
-
-			inserter.insert(new BytesWritable(("key" + i).getBytes()), tuple);
-		}
-		inserter.close();
-		writer.close();
-
-		if (debug == true) {
-			// Load tables
-			String query1 = "table1 = LOAD '" + unsortedPath.toString()
-			+ "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-			pigServer.registerQuery(query1);
-
-			// Print Table
-			printTable("table1");
-		}
-
-		System.out.println("Table Path : " + unsortedPath);
-	}
-
-	/**
-	 * Create sorted table
-	 * 
-	 */
-	public static void createsortedtable(String pathTable1, String pathTable2,
-			String sortString, boolean debug) throws ExecException, IOException {
-		System.out.println("createsortedtable()");
-
-		Path unsortedPath = new Path(pathTable1);
-		Path sortedPath = new Path(pathTable2);
-
-		// Remove old table (if present)
-		removeDir(sortedPath);
-
-		// Load tables
-		String query1 = "table1 = LOAD '" + unsortedPath.toString()
-		+ "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		pigServer.registerQuery(query1);
-
-		// Sort table
-		String orderby1 = "sort1 = ORDER table1 BY " + sortString + " ;";
-		System.out.println("orderby1 : " + orderby1);
-		pigServer.registerQuery(orderby1);
-
-		// Store sorted tables
-		pigJob = pigServer.store("sort1", sortedPath.toString(), TableStorer.class
-				.getCanonicalName()
-				+ "('" + TABLE_STORAGE + "')");
-		Assert.assertNull(pigJob.getException());
-
-		// Print Table
-		if (debug == true)
-			printTable("sort1");
-
-		System.out.println("Sorted Path : " + sortedPath);
-	}
-
-	/**
-	 * Delete table
-	 * 
-	 */
-	public static void deleteTable(String pathTable1) throws ExecException,
-	IOException {
-		System.out.println("deleteTable()");
-
-		Path tablePath = new Path(pathTable1);
-
-		// Remove table (if present)
-		removeDir(tablePath);
-
-		System.out.println("Deleted Table Path : " + tablePath);
-	}
-
-	/**
-	 * Verify sorted table
-	 * 
-	 * Using BasicTable.Reader, read all table rows and verify that sortCol is in
-	 * sorted order
-	 * 
-	 */
-	private static long verifySortedTable(String pathTable1, int sortCol,
-			String sortKey, int numbCols, int rowMod, String verifyDataColName)
-	throws IOException, ParseException {
-
-		long numbRows = 0;
-
-		Path tablePath = new Path(pathTable1);
-
-		BasicTable.Reader reader = new BasicTable.Reader(tablePath, conf);
-
-		Job job = new Job(conf);
-		System.out.println("sortKey: " + sortKey);
-		TableInputFormat.setInputPaths(job, new Path(pathTable1));
-
-		TableInputFormat.requireSortedTable(job, null);
-		TableInputFormat tif = new TableInputFormat();
-
-		SortedTableSplit split = (SortedTableSplit) tif.getSplits(job, true).get(0);
-
-		TableScanner scanner = reader.getScanner(split.getBegin(), split.getEnd(), true);
-		BytesWritable key = new BytesWritable();
-		Tuple rowValue = TypesUtils.createTuple(scanner.getSchema());
-
-		Object lastVal = null;
-		int numbCols1 = 0;
-		long numbVerifiedRows = 0;
-
-		while (!scanner.atEnd()) {
-			++numbRows;
-			scanner.getKey(key);
-
-			scanner.getValue(rowValue);
-
-			// Verify every nth row
-			if ((numbRows % rowMod) == 0) {
-				++numbVerifiedRows;
-				numbCols1 = rowValue.size();
-				if (numbCols != 0)
-					Assert.assertEquals(
-							"Verify failed - Table1 has wrong number of expected columns "
-							+ "\n row numberrr : " + numbRows
-							+ "\n expected column size : " + numbCols
-							+ "\n actual columns size  : " + numbCols1, numbCols,
-							numbCols1);
-
-				Object newVal = rowValue.get(sortCol);
-
-				// Verify sort key is in sorted order
-				Assert.assertTrue("Verify failed - Table1 sort comparison error "
-						+ "\n row number : " + numbRows + "\n sort column : " + sortCol
-						+ "\n sort column last value    : " + lastVal
-						+ "\n sort column current value : " + newVal, compareTo(newVal,
-								lastVal) >= 0);
-
-				lastVal = newVal; // save last compare value
-
-				//
-				// Optionally verify data
-				//
-
-				if (verifyDataColName != null && verifyDataColName.equals("long1")) {
-					Object newValLong1 = rowValue.get(sortCol);
-					if (numbRows < 2000){
-						System.out.println("Row : "+ (numbRows-1) +" long1 value : "+newValLong1.toString());
-					}
-					Assert.assertEquals(
-							"Verify failed - Union table data verification error for column name : "
-							+ verifyDataColName + "\n row number : " + (numbRows-1)
-							+ "\n expected value : " + (numbRows-1 + 4) / 4 + // long1 will start with value 1
-							"\n actual value   : " + newValLong1, (numbRows-1 + 4) / 4,
-							newValLong1);
-
-				}
-
-				scanner.advance();
-			}
-
-
-		}
-
-		System.out.println("\nTable Pathh : " + pathTable1);
-		System.out.println("++++++++++Table Row number : " + numbRows);
-
-
-		reader.close();
-
-		totalNumbCols = numbCols1;
-		totalNumbVerifiedRows = numbVerifiedRows;
-
-		return numbRows;
-	}
-
-	/**
-	 * Print table rows
-	 * 
-	 * Print the first number of specified table rows
-	 * 
-	 */
-	public static void printRows(String pathTable1, long numbRows)
-	throws IOException {
-		System.out.println("printRows()");
-
-		// Load table1
-		String query1 = "table1 = LOAD '" + pathTable1
-		+ "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		pigServer.registerQuery(query1);
-
-		//
-		// Get metrics from first table (unsorted)
-		//
-		long count = 0;
-		Iterator<Tuple> it1 = pigServer.openIterator("table1");
-
-		while (it1.hasNext()) {
-			++count;
-			if (count > numbRows)
-				break;
-			Tuple RowValue1 = it1.next();
-			System.out.println();
-			for (int i = 0; i < RowValue1.size(); ++i)
-				System.out.println("DEBUG: " + "table" + " RowValue.get(" + i + ") = "
-						+ RowValue1.get(i));
-		}
-		System.out.println("\nTable Path : " + pathTable1);
-		System.out.println("Table Rows Printed : " + numbRows);
-	}
-
-	/* 
-	 * Print the first number of specified table rows
-	 * 
-	 */
-	public static void printRowNumber(String pathTable1, String sortKey)
-	throws IOException, ParseException {
-		long numbRows = 0;
-
-		Path tablePath = new Path(pathTable1);
-
-		BasicTable.Reader reader = new BasicTable.Reader(tablePath, conf);
-
-		Job job = new Job(conf);
-		System.out.println("sortKey: " + sortKey);
-		TableInputFormat.setInputPaths(job, new Path(pathTable1));
-
-		TableInputFormat.requireSortedTable(job, null);
-
-		TableScanner scanner = reader.getScanner(null, null, true);
-		BytesWritable key = new BytesWritable();
-
-		while (!scanner.atEnd()) {
-			++numbRows;
-			scanner.getKey(key);
-			scanner.advance();
-		}
-		System.out.println("\nTable Path : " + pathTable1);
-		System.out.println("Table Row number : " + numbRows);
-	}
-
-	/**
-	 * Compares two objects that implement the Comparable interface
-	 * 
-	 * Zebra supported "sort" types of String, DataByteArray, Integer, Float,
-	 * Long, Double, and Boolean all implement the Comparable interface.
-	 * 
-	 * Returns a negative integer, zero, or a positive integer if object1 is less
-	 * than, equal to, or greater than object2.
-	 * 
-	 */
-	private static int compareTo(Object object1, Object object2) {
-		if (object1 == null) {
-			if (object2 == null)
-				return 0;
-			else
-				return -1;
-		} else if (object2 == null) {
-			return 1;
-		} else
-			return ((Comparable) object1).compareTo((Comparable) object2);
-	}
-
-	/**
-	 * Print Table Metadata Info (for debugging)
-	 * 
-	 */
-	private static void printTableInfo(String pathString) throws IOException {
-		ByteArrayOutputStream bos = new ByteArrayOutputStream();
-		PrintStream ps = new PrintStream(bos);
-		System.out.println("start dumpinfo ===========");
-		BasicTable.dumpInfo(pathString, ps, conf);
-
-		System.out.println("bos.toString() : " + bos.toString());
-	}
-
-	/**
-	 * Print Pig Table (for debugging)
-	 * 
-	 */
-	private static int printTable(String tablename) throws IOException {
-		Iterator<Tuple> it1 = pigServer.openIterator(tablename);
-		int numbRows = 0;
-		while (it1.hasNext()) {
-			Tuple RowValue1 = it1.next();
-			++numbRows;
-			System.out.println();
-			for (int i = 0; i < RowValue1.size(); ++i)
-				System.out.println("DEBUG: " + tablename + " RowValue.get(" + i
-						+ ") = " + RowValue1.get(i));
-		}
-		System.out.println("\nRow count : " + numbRows);
-		return numbRows;
-	}
-
-	/**
-	 * Calculate elapsed time
-	 * 
-	 */
-	private static String printTime(long start, long stop) {
-		long timeMillis = stop - start;
-		long time = timeMillis / 1000;
-		String seconds = Integer.toString((int) (time % 60));
-		String minutes = Integer.toString((int) ((time % 3600) / 60));
-		String hours = Integer.toString((int) (time / 3600));
-
-		for (int i = 0; i < 2; i++) {
-			if (seconds.length() < 2) {
-				seconds = "0" + seconds;
-			}
-			if (minutes.length() < 2) {
-				minutes = "0" + minutes;
-			}
-			if (hours.length() < 2) {
-				hours = "0" + hours;
-			}
-		}
-		String formatTime = hours + ":" + minutes + ":" + seconds;
-		return formatTime;
-	}
-
-	/**
-	 * Main
-	 * 
-	 * Command line options:
-	 * 
-	 * -verifyOption : <load, sort, merge-join, sorted-union, dump, tableinfo,
-	 * createtable, createsorttable, deletetable, printrows>
-	 * 
-	 * -pathTable1 : <hdfs path> -pathTable2 : <hdfs path>
-	 * 
-	 * -pathUnionTables : <hdfs path> <hdfs path> ...
-	 * 
-	 * -rowMod : verify every nth row (optional)
-	 * 
-	 * -numbCols : number of columns table should have (optional)
-	 * 
-	 * -sortCol : for sort option (default is column 0)
-	 * 
-	 * -sortString : sort string for sort option
-	 * 
-	 * -numbRows : number of rows for new table to create
-	 * 
-	 * -seed : unique column number used for creating new tables
-	 * 
-	 * -debug : print out debug info with results (use caution, for example do not
-	 * used when creating large tables)
-	 * 
-	 * examples:
-	 * 
-	 * java -DwhichCluster="realCluster" -DHADOOP_HOME=$HADOOP_HOME -DUSER=$USER
-	 * TestComparator -verifyOption load -pathTable1 /user/hadoopqa/table1
-	 * -pathTable2 /user/hadoopqa/table2
-	 * 
-	 * java -DwhichCluster="realCluster" -DHADOOP_HOME=$HADOOP_HOME -DUSER=$USER
-	 * TestComparator -verifyOption sort -pathTable1 /user/hadoopqa/table1
-	 * -pathTable2 /user/hadoopqa/table2 -sortCol 0
-	 * 
-	 * java -DwhichCluster="realCluster" -DHADOOP_HOME=$HADOOP_HOME -DUSER=$USER
-	 * TestComparator -verifyOption merge-join -pathTable1 /user/hadoopqa/table1
-	 * -sortCol 0
-	 * 
-	 * java -DwhichCluster="realCluster" -DHADOOP_HOME=$HADOOP_HOME -DUSER=$USER
-	 * TestComparator -verifyOption sorted-union -pathTable1
-	 * /user/hadoopqa/unionTable1 -pathUnionTables /user/hadoopqa/inputTable1
-	 * /user/hadoopqa/inputTable2 /user/hadoopqa/inputTable3 -sortCol 0 -rowMod 5
-	 * 
-	 * java -DwhichCluster="realCluster" -DHADOOP_HOME=$HADOOP_HOME -DUSER=$USER
-	 * TestComparator -verifyOption dump -pathTable1 /user/hadoopqa/table1
-	 * 
-	 * @param args
-	 */
-
-	public static void main(String[] args) {
-		long startTime = System.currentTimeMillis();
-
-		System.out.println("Running Zebra TestComparator");
-		try {
-			ArrayList<String> unionPaths = new ArrayList<String>();
-			String verifyOption = null;
-			String pathTable1 = null;
-			String pathTable2 = null;
-			String sortString = null;
-			String verifyDataColName = null;
-			int rowMod = 1; // default to verify every table row
-			int numbCols = 0; // if provided, verify that table has these number of
-			// columns
-			int sortCol = 0; // default to first column as sort index
-			long numbRows = 0; // number of rows to create for new table
-			int seed = 0; // used for creating new tabletable1
-			boolean debug = false;
-
-			// Read arguments
-			if (args.length >= 2) {
-				for (int i = 0; i < args.length; ++i) {
-
-					if (args[i].equals("-verifyOption")) {
-						verifyOption = args[++i];
-					} else if (args[i].equals("-pathTable1")) {
-						pathTable1 = args[++i];
-					} else if (args[i].equals("-pathTable2")) {
-						pathTable2 = args[++i];
-					} else if (args[i].equals("-pathUnionTables")) {
-						while (++i < args.length && !args[i].startsWith("-")) {
-							System.out.println("args[i] : " + args[i]);
-							unionPaths.add(args[i]);
-						}
-						if (i < args.length)
-							--i;
-					} else if (args[i].equals("-rowMod")) {
-						rowMod = new Integer(args[++i]).intValue();
-					} else if (args[i].equals("-sortString")) {
-						sortString = args[++i];
-					} else if (args[i].equals("-sortCol")) {
-						sortCol = new Integer(args[++i]).intValue();
-					} else if (args[i].equals("-numbCols")) {
-						numbCols = new Integer(args[++i]).intValue();
-					} else if (args[i].equals("-numbRows")) {
-						numbRows = new Long(args[++i]).intValue();
-					} else if (args[i].equals("-seed")) {
-						seed = new Integer(args[++i]).intValue();
-					} else if (args[i].equals("-verifyDataColName")) {
-						verifyDataColName = args[++i];
-					} else if (args[i].equals("-debug")) {
-						debug = true;
-					} else {
-						System.out.println("Exiting - unknown argument : " + args[i]);
-						System.exit(0);
-					}
-				}
-			} else {
-				System.out
-				.println("Error - need to provide required comparator arguments");
-				System.exit(0);
-			}
-
-			// Setup environment
-			setUp();
-
-			//
-			// Run appropriate verify option
-			//
-			if (verifyOption == null) {
-				System.out.println("Exiting -verifyOption not set");
-				System.exit(0);
-			}
-
-			if (verifyOption.equals("load")) {
-				// Verify both tables are equal
-				verifyLoad(pathTable1, pathTable2, numbCols);
-			} else if (verifyOption.equals("sort")) {
-				// Verify table is in sorted order
-				verifySorted(pathTable1, pathTable2, sortCol, sortString, numbCols,
-						rowMod);
-			} else if (verifyOption.equals("merge-join")) {
-				// Verify merge-join table is in sorted order
-				verifyMergeJoin(pathTable1, sortCol, sortString, numbCols, rowMod,verifyDataColName);
-			} else if (verifyOption.equals("sorted-union")) {
-
-				// Verify sorted-union table is in sorted order
-				verifySortedUnion(unionPaths, pathTable1, sortCol, sortString,
-						numbCols, rowMod, verifyDataColName);
-			} else if (verifyOption.equals("dump")) {
-				// Dump table info
-				printTableInfo(pathTable1);
-			} else if (verifyOption.equals("tableinfo")) {
-				// Verify table to get row and column info
-				verifyTable(pathTable1);
-			} else if (verifyOption.equals("deletetable")) {
-				// Delete table directory
-				deleteTable(pathTable1);
-			} else if (verifyOption.equals("printrows")) {
-				// Print some table rows
-				printRows(pathTable1, numbRows);
-			} else if (verifyOption.equals("createtable")) {
-				// Create unsorted table
-				createtable(pathTable1, numbRows, seed, debug);
-			} else if (verifyOption.equals("createsorttable")) {
-				// Create sorted table
-				createsortedtable(pathTable1, pathTable2, sortString, debug);
-			}else if (verifyOption.equals("printrownumber")) {
-				//print total number of rows of the table
-				printRowNumber(pathTable1,sortString);
-			}
-			//
-			else {
-				System.out.println("Exiting - unknown -verifyOption value : "
-						+ verifyOption);
-				System.exit(0);
-			}
-
-		} catch (Exception e) {
-			e.printStackTrace();
-		}
-
-		long stopTime = System.currentTimeMillis();
-		System.out.println("\nElapsed time : " + printTime(startTime, stopTime)
-				+ "\n");
-	}
-
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestBasicTableSourceTableIndex.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestBasicTableSourceTableIndex.java
deleted file mode 100644
index 613007d0e..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestBasicTableSourceTableIndex.java
+++ /dev/null
@@ -1,205 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.IOException;
-import java.util.Iterator;
-import java.util.ArrayList;
-import java.util.HashMap;
-import junit.framework.Assert;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-
-public class TestBasicTableSourceTableIndex extends BaseTestCase {
-  private static Path pathTable1, pathTable2;
-  
-  @BeforeClass
-  public static void setUp() throws Exception {
-    init();
-    
-    pathTable1 = getTableFullPath("TestBasicTableSourceTableIndex1");
-    pathTable2 = getTableFullPath("TestBasicTableSourceTableIndex2");    
-    removeDir(pathTable1);
-    removeDir(pathTable2);
-
-    /*
-     * create 1st basic table;
-     */
-
-    BasicTable.Writer writer = new BasicTable.Writer(pathTable1,
-        "a:string,b:string,c:string", "[a,b];[c]", conf);
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    final int numsBatch = 10;
-    final int numsInserters = 2;
-    TableInserter[] inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tuple);
-        for (int k = 0; k < tuple.size(); ++k) {
-          try {
-            tuple.set(k, b + "_" + i + "" + k);
-          } catch (ExecException e) {
-            e.printStackTrace();
-          }
-        }
-        inserters[i].insert(new BytesWritable(("key1" + i).getBytes()), tuple);
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-    writer.close();
-    
-    /*
-     * create 2nd basic table;
-     */
-
-    writer = new BasicTable.Writer(pathTable2, "a:string,b:string,d:string",
-        "[a,b];[d]", conf);
-    schema = writer.getSchema();
-    tuple = TypesUtils.createTuple(schema);
-
-    inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-
-    int j;
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tuple);
-        j = i + 2;
-        for (int k = 0; k < tuple.size(); ++k) {
-          try {
-            tuple.set(k, b + "_" + j + "" + k);
-          } catch (ExecException e) {
-            e.printStackTrace();
-          }
-        }
-        inserters[i].insert(new BytesWritable(("key2" + i).getBytes()), tuple);
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-	  writer.close();
-  }
-
-  @AfterClass
-  public static void tearDown() throws Exception {
-    pigServer.shutdown();
-  }
-
-  @Test
-  public void testReader() throws ExecException, IOException {
-    String str1 = pathTable1.toString();
-    String str2 = pathTable2.toString();    
-
-    String query = "records = LOAD '" + str1 + "," + str2
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('source_table, a, b, c, d');";
-    System.out.println(query);
-    pigServer.registerQuery(query);
-    
- // Verify union table
-    HashMap<Integer, ArrayList<ArrayList<Object>>> resultTable
-        = new HashMap<Integer, ArrayList<ArrayList<Object>>>();
-    
-    ArrayList<ArrayList<Object>> rows = new ArrayList<ArrayList<Object>>();
-    for (int i = 0; i < 10; i++)
-      addResultRow(rows, 0,  i+"_00",  i+"_01",  i+"_02");
-    for (int i = 0; i < 10; i++)
-      addResultRow(rows, 0,  i+"_10",  i+"_11",  i+"_12");
-    resultTable.put(0, rows);
-    
-    rows = new ArrayList<ArrayList<Object>>();
-    for (int i = 0; i < 10; i++)
-      addResultRow(rows, 0,  i+"_20",  i+"_21",  i+"_22");
-    for (int i = 0; i < 10; i++)
-      addResultRow(rows, 0,  i+"_30",  i+"_31",  i+"_32");
-    resultTable.put(1, rows);
-    
-    // Verify union table
-    Iterator<Tuple> it = pigServer.openIterator("records");
-    int numbRows = verifyTable(resultTable, 1, 0, it);
-    
-    Assert.assertEquals(numbRows, 40);
-  }
-  
-  /**
-   * Test source table index from a single table
-   * @throws ExecException
-   * @throws IOException
-   */
-  @Test
-  public void testSingleReader() throws ExecException, IOException {
-    String str1 = pathTable1.toString();    
-
-    String query = "records = LOAD '" + str1
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('source_table, a, b, c, d');";
-    System.out.println(query);
-    pigServer.registerQuery(query);
-    
- // Verify union table
-    HashMap<Integer, ArrayList<ArrayList<Object>>> resultTable
-        = new HashMap<Integer, ArrayList<ArrayList<Object>>>();
-    
-    ArrayList<ArrayList<Object>> rows = new ArrayList<ArrayList<Object>>();
-    for (int i = 0; i < 10; i++)
-      addResultRow(rows, 0,  i+"_00",  i+"_01",  i+"_02");
-    for (int i = 0; i < 10; i++)
-      addResultRow(rows, 0,  i+"_10",  i+"_11",  i+"_12");
-    resultTable.put(0, rows);
-    
-    // Verify union table
-    Iterator<Tuple> it = pigServer.openIterator("records");
-    int numbRows = verifyTable(resultTable, 1, 0, it);
-    
-    Assert.assertEquals(numbRows, 20);
-  }
-  
-  /**
-   *Add a row to expected results table
-   * 
-   */
-  private void addResultRow(ArrayList<ArrayList<Object>> resultTable, Object ... values) {
-    ArrayList<Object> resultRow = new ArrayList<Object>();
-    
-    for (int i = 0; i < values.length; i++) {
-      resultRow.add(values[i]);
-    }
-    resultTable.add(resultRow);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestBasicTableUnionLoader.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestBasicTableUnionLoader.java
deleted file mode 100644
index 229488f95..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestBasicTableUnionLoader.java
+++ /dev/null
@@ -1,170 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.IOException;
-import java.util.Iterator;
-import junit.framework.Assert;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-
-public class TestBasicTableUnionLoader extends BaseTestCase {
-  private static Path pathTable1, pathTable2;
-  
-  @BeforeClass
-  public static void setUp() throws Exception {
-    init();
-    
-    pathTable1 = getTableFullPath("TestBasicTableUnionLoader1");
-    pathTable2 = getTableFullPath("TestBasicTableUnionLoader2");    
-    removeDir(pathTable1);
-    removeDir(pathTable2);
-
-    /*
-     * create 1st basic table;
-     */
-
-    BasicTable.Writer writer = new BasicTable.Writer(pathTable1,
-        "a:string,b:string,c:string", "[a,b];[c]", conf);
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    final int numsBatch = 10;
-    final int numsInserters = 2;
-    TableInserter[] inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tuple);
-        for (int k = 0; k < tuple.size(); ++k) {
-          try {
-            tuple.set(k, b + "_" + i + "" + k);
-          } catch (ExecException e) {
-            e.printStackTrace();
-          }
-        }
-        inserters[i].insert(new BytesWritable(("key1" + i).getBytes()), tuple);
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-    writer.close();
-    
-    /*
-     * create 2nd basic table;
-     */
-
-    writer = new BasicTable.Writer(pathTable2, "a:string,b:string,d:string",
-        "[a,b];[d]", conf);
-    schema = writer.getSchema();
-    tuple = TypesUtils.createTuple(schema);
-
-    inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tuple);
-        for (int k = 0; k < tuple.size(); ++k) {
-          try {
-            tuple.set(k, b + "_" + i + "" + k);
-          } catch (ExecException e) {
-            e.printStackTrace();
-          }
-        }
-        inserters[i].insert(new BytesWritable(("key2" + i).getBytes()), tuple);
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-	  writer.close();
-  }
-
-  @AfterClass
-  public static void tearDown() throws Exception {
-    pigServer.shutdown();
-  }
-
-  @Test
-  public void testReader() throws ExecException, IOException {
-    String str1 = pathTable1.toString();
-    String str2 = pathTable2.toString();    
-
-    String query = "records = LOAD '" + str1 + "," + str2
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('a, b, c, d');";
-    System.out.println(query);
-    pigServer.registerQuery(query);
-    Iterator<Tuple> it = pigServer.openIterator("records");
-
-    int cnt = 0;
-    Tuple cur;
-    while (it.hasNext()) {
-      cur = it.next();
-      cnt++;
-      
-      if (cnt == 1) {
-        String col0 = (String)cur.get(0);
-        Assert.assertTrue(col0.equals("0_00") || col0.equals("0_10"));
-        
-        String col1 = (String)cur.get(1);
-        Assert.assertTrue(col1.equals("0_01") || col1.equals("0_11"));
-        
-        String col2 = (String)cur.get(2);
-        Assert.assertTrue(col2 == null || col2.equals("0_02") || col2.equals("0_12"));
-        
-        String col3 = (String)cur.get(3);
-        Assert.assertTrue(col3 == null || col3.equals("0_02") || col3.equals("0_12"));
-      }
-      
-      if (cnt == 21) {
-        String col0 = (String)cur.get(0);
-        Assert.assertTrue(col0.equals("0_00") || col0.equals("0_10"));
-        
-        String col1 = (String)cur.get(1);
-        Assert.assertTrue(col1.equals("0_01") || col1.equals("0_11"));
-        
-        String col2 = (String)cur.get(2);
-        Assert.assertTrue(col2 == null || col2.equals("0_02") || col2.equals("0_12"));
-        
-        String col3 = (String)cur.get(3);
-        Assert.assertTrue(col3 == null || col3.equals("0_02") || col3.equals("0_12"));        
-      }
-    }
-    
-    Assert.assertEquals(cnt, 40);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestBasicUnion.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestBasicUnion.java
deleted file mode 100644
index cffa0f22f..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestBasicUnion.java
+++ /dev/null
@@ -1,838 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.IOException;
-import java.util.Iterator;
-import java.util.List;
-
-import junit.framework.Assert;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.ExecType;
-import org.apache.pig.PigServer;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.test.MiniCluster;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * Note:
- * 
- * Make sure you add the build/pig-0.1.0-dev-core.jar to the Classpath of the
- * app/debug configuration, when run this from inside the Eclipse.
- * 
- */
-public class TestBasicUnion extends BaseTestCase {
-
-  final static String STR_SCHEMA1 = "a:string,b,c:string,e,f";
-  final static String STR_STORAGE1 = "[a];[c]";
-  final static String STR_SCHEMA2 = "a:string,b,d:string,f,e";
-  final static String STR_STORAGE2 = "[a,b];[d]";
-  final static String STR_SCHEMA3 = "b,a";
-  final static String STR_STORAGE3 = "[a];[b]";
-  final static String STR_SCHEMA4 = "b:string,a,c:string";
-  final static String STR_STORAGE4 = "[a,b];[c]";
-  final static String STR_SCHEMA5 = "b,a:string";
-  final static String STR_STORAGE5 = "[a,b]";
-
-  private static Path path1, path2, path3, path4, path5;
-
-  @BeforeClass
-  public static void setUp() throws Exception {
-
-    init();
-
-    path1 = getTableFullPath("/TestBasicUnion1");
-    path2 = getTableFullPath("/TestBasicUnion2");
-    path3 = getTableFullPath("/TestBasicUnion3");
-    path4 = getTableFullPath("/TestBasicUnion4");
-    path5 = getTableFullPath("/TestBasicUnion5");
-    removeDir(path1);
-    removeDir(path2);
-    removeDir(path3);
-    removeDir(path4);
-    removeDir(path5);
-
-    /*
-     * create 1st basic table;
-     */
-
-    BasicTable.Writer writer = new BasicTable.Writer(path1, STR_SCHEMA1,
-        STR_STORAGE1, conf);
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    final int numsBatch = 10;
-    final int numsInserters = 2;
-    TableInserter[] inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tuple);
-        for (int k = 0; k < tuple.size(); ++k) {
-          try {
-            if (k == 1 || k == 3 || k == 4) { // bytes type
-              tuple.set(k, new DataByteArray(new String(b + "_" + i + "" + k).toString()));
-            } else {
-              tuple.set(k, b + "_" + i + "" + k);
-            }
-          } catch (ExecException e) {
-
-          }
-        }// k
-        inserters[i].insert(new BytesWritable(("key1" + i).getBytes()), tuple);
-      }// i
-    }// b
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-    writer.close();
-    
-    /*
-     * create 2nd basic table;
-     */
-    writer = new BasicTable.Writer(path2, STR_SCHEMA2, STR_STORAGE2, conf);
-    schema = writer.getSchema();
-    tuple = TypesUtils.createTuple(schema);
-
-    inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tuple);
-        for (int k = 0; k < tuple.size(); ++k) {
-          try {
-            if (k == 1 || k == 3 || k == 4) {
-              tuple.set(k, new DataByteArray(new String(b + "_" + i + "" + k).toString()));
-            } else {
-              tuple.set(k, b + "_" + i + "" + k);
-            }
-          } catch (ExecException e) {
-
-          }
-        }
-        inserters[i].insert(new BytesWritable(("key2" + i).getBytes()), tuple);
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-    writer.close();
-    
-    /*
-     * create 3rd basic table;
-     */
-    
-    writer = new BasicTable.Writer(path3, STR_SCHEMA3, STR_STORAGE3, conf);
-    schema = writer.getSchema();
-    tuple = TypesUtils.createTuple(schema);
-
-    inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tuple);
-        for (int k = 0; k < tuple.size(); ++k) {
-          try {
-            tuple.set(k, new DataByteArray(new String(b + "_" + i + "" + k).toString()));
-          } catch (ExecException e) {
-
-          }
-        }
-        inserters[i].insert(new BytesWritable(("key3" + i).getBytes()), tuple);
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-	  writer.close();
-	
-    /*
-     * create 4th basic table;
-     */
-
-    writer = new BasicTable.Writer(path4, STR_SCHEMA4, STR_STORAGE4, conf);
-    schema = writer.getSchema();
-    tuple = TypesUtils.createTuple(schema);
-
-    inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tuple);
-        for (int k = 0; k < tuple.size(); ++k) {
-          try {
-            if (k == 1) {
-              tuple.set(k, new DataByteArray(new String(b + "_" + i + "" + k).toString()));
-            } else {
-              tuple.set(k, b + "_" + i + "" + k);
-            }
-          } catch (ExecException e) {
-
-          }
-        }
-        inserters[i].insert(new BytesWritable(("key4" + i).getBytes()), tuple);
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-	  writer.close();
-	  
-    /*
-     * create 5th basic table;
-     */
-
-    writer = new BasicTable.Writer(path5, STR_SCHEMA5, STR_STORAGE5, conf);
-    schema = writer.getSchema();
-    tuple = TypesUtils.createTuple(schema);
-
-    inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tuple);
-        for (int k = 0; k < tuple.size(); ++k) {
-          try {
-            if (k == 0) {
-              tuple.set(k, new DataByteArray(new String(b + "_" + i + "" + k).toString()));
-            } else {
-              tuple.set(k, b + "_" + i + "" + k);
-            }
-          } catch (ExecException e) {
-
-          }
-        }
-        inserters[i].insert(new BytesWritable(("key5" + i).getBytes()), tuple);
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-	  writer.close();
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws Exception {
-    pigServer.shutdown();
-    BasicTable.drop(path1, conf);
-    BasicTable.drop(path2, conf);
-    BasicTable.drop(path3, conf);
-    BasicTable.drop(path4, conf);
-    BasicTable.drop(path5, conf);
-  }
-
-  @Test
-  public void testReader1() throws ExecException, IOException {
-
-    pigServer.registerQuery(constructQuery(path1, path2, "'a, b, c, d'"));
-    Iterator<Tuple> it = pigServer.openIterator("records");
-
-    int i = 0;
-    int k = -1;
-    Tuple cur = null;
-    int t = -1;
-    int j = -1;
-
-    while (it.hasNext()) {
-      cur = it.next();
-
-      System.out.println("cur: " + cur);
-      // first table
-      if (i <= 9) {
-        System.out.println("first table first part: " + cur.toString());
-        Assert.assertEquals(i + "_00", cur.get(0));
-        Assert.assertEquals(i + "_01", cur.get(1).toString());
-        Assert.assertTrue(((cur.get(2) == null) || (cur.get(2)
-            .equals(i + "_02"))));
-        Assert.assertTrue(((cur.get(3) == null) || (cur.get(3)
-            .equals(i + "_02"))));
-      }
-      if (i >= 10) {
-        k++;
-      }
-      if (k <= 9 && k >= 0) {
-        System.out.println("first table second part:  : " + cur.toString());
-        Assert.assertEquals(k + "_10", cur.get(0));
-        Assert.assertEquals(k + "_11", cur.get(1).toString());
-        Assert.assertTrue(((cur.get(2) == null) || (cur.get(2)
-            .equals(k + "_12"))));
-        Assert.assertTrue(((cur.get(3) == null) || (cur.get(3)
-            .equals(k + "_12"))));
-
-      }
-
-      // second table
-      if (k >= 10) {
-        t++;
-      }
-      if (t <= 9 && t >= 0) {
-        System.out.println("second table first part: " + cur.toString());
-        Assert.assertEquals(t + "_00", cur.get(0));
-        Assert.assertEquals(t + "_01", cur.get(1).toString());
-        Assert.assertTrue(((cur.get(2) == null) || (cur.get(2)
-            .equals(t + "_02"))));
-        Assert.assertTrue(((cur.get(3) == null) || (cur.get(3)
-            .equals(t + "_02"))));
-      }
-      if (t >= 10) {
-        j++;
-      }
-      if (j <= 9 && j >= 0) {
-        System.out.println("second table first part: " + cur.toString());
-        Assert.assertEquals(j + "_10", cur.get(0));
-        Assert.assertEquals(j + "_11", cur.get(1).toString());
-        Assert.assertTrue(((cur.get(2) == null) || (cur.get(2)
-            .equals(j + "_12"))));
-        Assert.assertTrue(((cur.get(3) == null) || (cur.get(3)
-            .equals(j + "_12"))));
-      }
-      i++;
-    }// while
-    Assert.assertEquals(40, i);
-  }
-
-  @Test
-  public void testReaderThroughIO() throws ExecException, IOException,
-    ParseException {
-
-    String projection1 = new String("a,b,c");
-    BasicTable.Reader reader = new BasicTable.Reader(path1, conf);
-    reader.setProjection(projection1);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-
-    System.out.println("read record or record:" + RowValue.toString());
-
-    for (int i = 0; i <= 9; i++) {
-      scanner.getValue(RowValue);
-      System.out.println("read record or record:" + RowValue.toString());
-      Assert.assertEquals(i + "_00", RowValue.get(0));
-      Assert.assertEquals(i + "_01", RowValue.get(1).toString());
-      Assert.assertEquals(i + "_02", RowValue.get(2));
-      scanner.advance();
-    }
-    for (int i = 0; i <= 9; i++) {
-      scanner.getValue(RowValue);
-      System.out.println("read record or record:" + RowValue.toString());
-      Assert.assertEquals(i + "_10", RowValue.get(0));
-      Assert.assertEquals(i + "_11", RowValue.get(1).toString());
-      Assert.assertEquals(i + "_12", RowValue.get(2));
-      scanner.advance();
-    }
-
-    reader.close();
-  }
-
-  // field c
-  @Test
-  public void testReader2() throws ExecException, IOException {
-
-    pigServer.registerQuery(constructQuery(path1, path2, "'c'"));
-    Iterator<Tuple> it = pigServer.openIterator("records");
-
-    int i = 0;
-    int k = -1;
-    Tuple cur = null;
-    int t = -1;
-    int j = -1;
-
-    while (it.hasNext()) {
-      cur = it.next();
-
-      System.out.println("cur: " + cur);
-      if (i <= 9) {
-        System.out.println("first table first part: " + cur.toString());
-
-        Assert.assertTrue(((cur.get(0) == null) || (cur.get(0)
-            .equals(i + "_02"))));
-
-        try {
-          cur.get(1);
-          Assert.fail("should throw index out of bound exception ");
-        } catch (Exception e) {
-
-        }
-      }
-      if (i >= 10) {
-        k++;
-      }
-      if (k <= 9 && k >= 0) {
-        System.out.println("first table second part:  : " + cur.toString());
-        Assert.assertTrue(((cur.get(0) == null) || (cur.get(0)
-            .equals(k + "_12"))));
-
-        try {
-          cur.get(1);
-          Assert.fail("should throw index out of bound exception ");
-        } catch (Exception e) {
-
-        }
-      }
-      if (k >= 10) {
-        t++;
-      }
-      if (t <= 9 && t >= 0) {
-        System.out.println("second table first part: " + cur.toString());
-        Assert.assertTrue(((cur.get(0) == null) || (cur.get(0)
-            .equals(t + "_02"))));
-
-        try {
-          cur.get(1);
-          Assert.fail("should throw index out of bound exception ");
-        } catch (Exception e) {
-
-        }
-      }
-      if (t >= 10) {
-        j++;
-      }
-      if (j <= 9 && j >= 0) {
-        System.out.println("second table first part: " + cur.toString());
-        Assert.assertTrue(((cur.get(0) == null) || (cur.get(0)
-            .equals(j + "_12"))));
-
-        try {
-          cur.get(1);
-          Assert.fail("should throw index out of bound exception ");
-        } catch (Exception e) {
-
-        }
-      }
-      i++;
-    }// while
-    Assert.assertEquals(40, i);
-  }
-
-  // projection for common exist colum a
-  @Test
-  public void testReader3() throws ExecException, IOException {
-
-    pigServer.registerQuery(constructQuery(path1, path2, "'a'"));
-    Iterator<Tuple> it = pigServer.openIterator("records");
-
-    int i = 0;
-    int k = -1;
-    Tuple cur = null;
-    int t = -1;
-    int j = -1;
-
-    while (it.hasNext()) {
-      cur = it.next();
-
-      System.out.println("cur: " + cur);
-      // first table
-      if (i <= 9) {
-        System.out.println("first table first part: " + cur.toString());
-        Assert.assertTrue(((cur.get(0) == null) || (cur.get(0)
-            .equals(i + "_00"))));
-
-        try {
-          cur.get(1);
-          Assert.fail("should throw index out of bound exception ");
-        } catch (Exception e) {
-
-        }
-      }
-      if (i >= 10) {
-        k++;
-      }
-      if (k <= 9 && k >= 0) {
-        System.out.println("first table second part:  : " + cur.toString());
-        Assert.assertTrue(((cur.get(0) == null) || (cur.get(0)
-            .equals(k + "_10"))));
-
-        try {
-          cur.get(1);
-          Assert.fail("should throw index out of bound exception ");
-        } catch (Exception e) {
-
-        }
-      }
-
-      // second table
-      if (k >= 10) {
-        t++;
-      }
-      if (t <= 9 && t >= 0) {
-        System.out.println("second table first part: " + cur.toString());
-        Assert.assertTrue(((cur.get(0) == null) || (cur.get(0)
-            .equals(t + "_00"))));
-
-        try {
-          cur.get(1);
-          Assert.fail("should throw index out of bound exception ");
-        } catch (Exception e) {
-
-        }
-      }
-      if (t >= 10) {
-        j++;
-      }
-      if (j <= 9 && j >= 0) {
-        System.out.println("second table first part: " + cur.toString());
-        Assert.assertTrue(((cur.get(0) == null) || (cur.get(0)
-            .equals(j + "_10"))));
-
-        try {
-          cur.get(1);
-          Assert.fail("should throw index out of bound exception ");
-        } catch (Exception e) {
-
-        }
-      }
-      i++;
-    }// while
-    Assert.assertEquals(40, i);
-  }
-
-  // some common fields
-  @Test
-  public void testReader4() throws ExecException, IOException {
-
-    pigServer.registerQuery(constructQuery(path1, path2, "'a, b'"));
-    Iterator<Tuple> it = pigServer.openIterator("records");
-
-    int i = 0;
-    int k = -1;
-    Tuple cur = null;
-    int t = -1;
-    int j = -1;
-
-    while (it.hasNext()) {
-      cur = it.next();
-
-      System.out.println("cur: " + cur);
-      // first table
-      if (i <= 9) {
-        System.out.println("first table first part: " + cur.toString());
-        Assert.assertEquals(i + "_00", cur.get(0));
-        Assert.assertEquals(i + "_01", cur.get(1).toString());
-        try {
-          cur.get(2);
-          Assert.fail("should throw index out of bound exception");
-        } catch (Exception e) {
-
-        }
-
-      }
-      if (i >= 10) {
-        k++;
-      }
-      if (k <= 9 && k >= 0) {
-        System.out.println("first table second part:  : " + cur.toString());
-        Assert.assertEquals(k + "_10", cur.get(0));
-        Assert.assertEquals(k + "_11", cur.get(1).toString());
-        try {
-          cur.get(2);
-          Assert.fail("should throw index out of bound exception");
-        } catch (Exception e) {
-
-        }
-      }
-
-      // second table
-      if (k >= 10) {
-        t++;
-      }
-      if (t <= 9 && t >= 0) {
-        System.out.println("second table first part: " + cur.toString());
-        Assert.assertEquals(t + "_00", cur.get(0));
-        Assert.assertEquals(t + "_01", cur.get(1).toString());
-        try {
-          cur.get(2);
-          Assert.fail("should throw index out of bound exception");
-        } catch (Exception e) {
-
-        }
-      }
-      if (t >= 10) {
-        j++;
-      }
-      if (j <= 9 && j >= 0) {
-        System.out.println("second table first part: " + cur.toString());
-        Assert.assertEquals(j + "_10", cur.get(0));
-        Assert.assertEquals(j + "_11", cur.get(1).toString());
-        try {
-          cur.get(2);
-          Assert.fail("should throw index out of bound exception");
-        } catch (Exception e) {
-
-        }
-      }
-      i++;
-    }// while
-    Assert.assertEquals(40, i);
-  }
-
-  // common column, but different posion
-  @Test
-  public void testReader5() throws ExecException, IOException {
-
-    pigServer.registerQuery(constructQuery(path1, path2, "'e,f'"));
-    Iterator<Tuple> it = pigServer.openIterator("records");
-
-    int i = 0;
-    int k = -1;
-    Tuple cur = null;
-    int t = -1;
-    int j = -1;
-
-    while (it.hasNext()) {
-      cur = it.next();
-
-      System.out.println("cur: " + cur);
-      // first table
-      if (i <= 9) {
-        System.out.println("first table first part: " + cur.toString());
-        Assert.assertTrue(((cur.get(0)).toString().equals(i + "_03") || (cur.get(0).toString()
-            .equals(i + "_04"))));
-        System.out.println("get1: " + cur.get(1));
-        Assert.assertTrue(((cur.get(1)).toString().equals(i + "_03") || (cur.get(1).toString()
-            .equals(i + "_04"))));
-        try {
-          cur.get(2);
-          Assert.fail("should throw out of index bound exception");
-        } catch (Exception e) {
-
-        }
-      }
-      if (i >= 10) {
-        k++;
-      }
-      if (k <= 9 && k >= 0) {
-        System.out.println("first table second part:  : " + cur.toString());
-        Assert.assertTrue(((cur.get(0).toString().equals(k + "_13")) || (cur.get(0).toString()
-            .equals(k + "_14"))));
-        Assert.assertTrue(((cur.get(1).toString().equals(k + "_13")) || (cur.get(1).toString()
-            .equals(k + "_14"))));
-
-      }
-
-      // second table
-      if (k >= 10) {
-        t++;
-      }
-      if (t <= 9 && t >= 0) {
-        System.out.println("second table first part: " + cur.toString());
-        Assert.assertTrue(((cur.get(0).toString().equals(t + "_03")) || (cur.get(0).toString()
-            .equals(t + "_04"))));
-        Assert.assertTrue(((cur.get(1).toString().equals(t + "_03")) || (cur.get(1).toString()
-            .equals(t + "_04"))));
-
-      }
-      if (t >= 10) {
-        j++;
-      }
-      if (j <= 9 && j >= 0) {
-        System.out.println("second table first part: " + cur.toString());
-        Assert.assertTrue(((cur.get(0).toString().equals(j + "_13")) || (cur.get(0).toString()
-            .equals(j + "_14"))));
-        Assert.assertTrue(((cur.get(1).toString().equals(j + "_13")) || (cur.get(1).toString()
-            .equals(j + "_14"))));
-
-      }
-      i++;
-    }// while
-    Assert.assertEquals(40, i);
-  }
-
-  @Test
-  // union two tables with different column numbers and column positions
-  public void testReader6() throws ExecException, IOException {
-    pigServer.registerQuery(constructQuery(path1, path5, "'b,a'"));
-    Iterator<Tuple> it = pigServer.openIterator("records");
-
-    int i = 0;
-    int count = 0;
-    Tuple cur = null;
-
-    String[] exp1 = new String[] { "0_01", "1_01", "2_01", "3_01", "4_01", "5_01", "6_01", "7_01", "8_01", "9_01", 
-    		                       "0_11", "1_11", "2_11", "3_11", "4_11", "5_11", "6_11", "7_11", "8_11", "9_11"};
-    String[] exp2 = new String[] { "0_00", "1_00", "2_00", "3_00", "4_00", "5_00", "6_00", "7_00", "8_00", "9_00", 
-                                   "0_10", "1_10", "2_10", "3_10", "4_10", "5_10", "6_10", "7_10", "8_10", "9_10"};
-    while (it.hasNext()) {
-      count++;
-      cur = it.next();
-      System.out.println("cur #" + i + ": " + cur);
-      try {
-          cur.get(2);
-          Assert.fail("should throw index out of bound exception");
-      } catch (Exception e) {
-      }
-
-      //String b = (String)cur.get(0);
-      String b = cur.get(0).toString();
-      String a = (String)cur.get(1);
-      Assert.assertTrue( b.equals(exp1[i]) || b.equals(exp2[i]) );
-      Assert.assertTrue( a.equals(exp2[i]) || a.equals(exp1[i]) );
-      if( ++i == 20 )
-    	  i = 0;
-    }// while
-    Assert.assertEquals(40, count);
-  }
-
-  // both paths is hdfs:///../jars. mini cluster need to substr, real cluster
-  // don't need to
-  @Test
-  public void testNeg1() throws ExecException, IOException {
-
-      pigServer.registerQuery(constructQuery(path1, path2, "'a,b,c,d'"));
-      Iterator<Tuple> it = pigServer.openIterator("records");
-
-      int cnt = 0;
-      Tuple cur = it.next();
-      cnt++;
-      while (it.hasNext()) {
-          cur = it.next();
-          System.out.println(cur);
-          cnt++;
-          if (cnt == 1) {
-              Assert.assertEquals("0_00", cur.get(0));
-              Assert.assertEquals("0_01", cur.get(1));
-              Assert
-                      .assertTrue(((cur.get(2) == null) || (cur.get(2).equals("0_02"))));
-              Assert
-                      .assertTrue(((cur.get(3) == null) || (cur.get(3).equals("0_02"))));
-          }
-          if (cnt == 22) {
-              Assert.assertEquals("1_00", cur.get(0));
-              Assert.assertEquals("1_01", cur.get(1).toString());
-              Assert
-                      .assertTrue(((cur.get(2) == null) || (cur.get(2).equals("1_02"))));
-              Assert
-                      .assertTrue(((cur.get(3) == null) || (cur.get(3).equals("1_02"))));
-
-          }
-      }
-      Assert.assertEquals(cnt, 40);
-
-  }
-
-  // non-existing column
-  @Test
-  public void testNeg2() throws ExecException, IOException {
-
-    pigServer.registerQuery(constructQuery(path1, path2, "'a,f,x'"));
-
-    Iterator<Tuple> it = pigServer.openIterator("records");
-
-    int cnt = 0;
-    Tuple cur = it.next();
-  //  cnt++;
-    while (it.hasNext()) {
-      cur = it.next();
-      System.out.println(cur);
-      cnt++;
-      if (cnt == 1) {
-        Assert.assertEquals("1_00", cur.get(0));
-        System.out.println("neg2, cnt ==1: " +cur.get(1));
-        Assert
-            .assertTrue(((cur.get(1) == null) || (cur.get(1).toString().equals("1_03"))||(cur.get(1).toString().equals("1_04"))));
-        Assert.assertEquals(null, cur.get(2));
-
-        try {
-          cur.get(3);
-          Assert.fail("should throw index out of bound exception");
-        } catch (Exception e) {
-
-        }
-      }
-      if (cnt == 21) {
-        Assert.assertEquals("1_00", cur.get(0));
-        System.out.println("neg2, cnt ==22: " +cur.get(1));
-        
-        Assert
-            .assertTrue(((cur.get(1) == null) || (cur.get(1).toString().equals("1_04"))||(cur.get(1).toString().equals("1_03"))));
-        Assert.assertEquals(null, cur.get(2));
-        try {
-          cur.get(3);
-          Assert.fail("should throw index out of bound exception");
-        } catch (Exception e) {
-
-        }
-      }
-    }
-    Assert.assertEquals(39, cnt);
-  }
-
-  @Test
-  // 2 table with same column name but different type and different position.
-  // should throw exception
-  public void testNeg3() throws ExecException, IOException {
-    try {
-      pigServer.setValidateEachStatement(true);
-      pigServer.registerQuery(constructQuery(path1, path3, "'a, b'"));
-      Assert.fail("should throw exception");
-    } catch (Exception e) {
-    }
-  }
-
-    @Test
-  // union table1 and table4. they have same culumn name with differnt types ,
-  // should throw excepiton in union
-  public void testNeg4() throws ExecException, IOException {
-      try {
-      pigServer.setValidateEachStatement(true);
-      pigServer.registerQuery(constructQuery(path1, path4, "'a, b, c'"));
-      Assert.fail("should throw exception");
-    } catch (Exception e) {
-    }
-  }
-
-  protected String constructQuery(Path path1, Path path2, String schema)
-  {
-    return "records = LOAD '" + path1 + "," + path2
-            + "' USING org.apache.hadoop.zebra.pig.TableLoader(" + schema + ");";
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestCheckin1.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestCheckin1.java
deleted file mode 100644
index 04da0f617..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestCheckin1.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import org.junit.runner.RunWith;
-import org.junit.runners.Suite;
- 
-@RunWith(Suite.class)
-@Suite.SuiteClasses({
-  TestBasicUnion.class
-  //TestMapTableLoader.class
-  //TestMapTableStorer.class,
-  //TestTableLoader.class,
-  //TestTableStorer.class
-})
-
-public class TestCheckin1 {
-  // the class remains completely empty,
-  // being used only as a holder for the above annotations
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestCheckin2.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestCheckin2.java
deleted file mode 100644
index cae8ce3c4..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestCheckin2.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import org.junit.runner.RunWith;
-import org.junit.runners.Suite;
- 
-@RunWith(Suite.class)
-@Suite.SuiteClasses({
-  //TestBasicUnion.class,
-  TestMapTableLoader.class
-  //TestMapTableStorer.class,
-  //TestTableLoader.class,
-  //TestTableStorer.class
-})
-
-public class TestCheckin2 {
-  // the class remains completely empty,
-  // being used only as a holder for the above annotations
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestCheckin3.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestCheckin3.java
deleted file mode 100644
index 76f106aa7..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestCheckin3.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import org.junit.runner.RunWith;
-import org.junit.runners.Suite;
- 
-@RunWith(Suite.class)
-@Suite.SuiteClasses({
-  //TestBasicUnion.class,
-  //TestMapTableLoader.class
-  TestMapTableStorer.class
-  //TestTableLoader.class,
-  //TestTableStorer.class
-})
-
-public class TestCheckin3 {
-  // the class remains completely empty,
-  // being used only as a holder for the above annotations
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestCheckin4.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestCheckin4.java
deleted file mode 100644
index 27c83718e..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestCheckin4.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import org.junit.runner.RunWith;
-import org.junit.runners.Suite;
- 
-@RunWith(Suite.class)
-@Suite.SuiteClasses({
-  //TestBasicUnion.class,
-  //TestMapTableLoader.class
-  //TestMapTableStorer.class,
-  TestTableLoader.class
-  //TestTableStorer.class
-})
-
-public class TestCheckin4 {
-  // the class remains completely empty,
-  // being used only as a holder for the above annotations
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestCheckin5.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestCheckin5.java
deleted file mode 100644
index a9996fd90..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestCheckin5.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import org.junit.runner.RunWith;
-import org.junit.runners.Suite;
- 
-@RunWith(Suite.class)
-@Suite.SuiteClasses({
-  //TestBasicUnion.class,
-  //TestMapTableLoader.class
-  //TestMapTableStorer.class,
-  //TestTableLoader.class,
-  TestTableStorer.class
-})
-
-public class TestCheckin5 {
-  // the class remains completely empty,
-  // being used only as a holder for the above annotations
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestCogroup.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestCogroup.java
deleted file mode 100644
index d72ce5cd9..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestCogroup.java
+++ /dev/null
@@ -1,188 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.Iterator;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.pig.TableStorer;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.ExecType;
-import org.apache.pig.PigServer;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
-import org.apache.pig.backend.executionengine.ExecJob;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.test.MiniCluster;
-import org.junit.After;
-import org.junit.AfterClass;
-import org.junit.Before;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * Note:
- * 
- * Make sure you add the build/pig-0.1.0-dev-core.jar to the Classpath of the
- * app/debug configuration, when run this from inside the Eclipse.
- * 
- */
-public class TestCogroup extends BaseTestCase
-{
-  private static Path pathTable;
-
-
-  @BeforeClass
-  public static void setUp() throws Exception {
-
-    init();
-    pathTable = getTableFullPath("TestCogroup");
-    removeDir(pathTable);
-
-    BasicTable.Writer writer = new BasicTable.Writer(pathTable,
-        "SF_a:string,SF_b:string,SF_c:string,SF_d:string,SF_e:string,SF_f:string,SF_g:string",
-        "[SF_a, SF_b, SF_c]; [SF_e, SF_f, SF_g]", conf);
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    final int numsBatch = 10;
-    final int numsInserters = 1;
-    TableInserter[] inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tuple);
-        for (int k = 0; k < tuple.size(); ++k) {
-          try {
-            tuple.set(k, (9-b) + "_" + i + "" + k);
-          } catch (ExecException e) {
-            e.printStackTrace();
-          }
-        }
-        inserters[i].insert(new BytesWritable(("key" + i).getBytes()), tuple);
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDown() throws Exception {
-    pigServer.shutdown();
-    removeDir(pathTable);
-  }
-
-  /**
-   * Return the name of the routine that called getCurrentMethodName
-   * 
-   */
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    return t;
-  }
-
-  @Test
-  public void testStorer() throws ExecException, IOException {
-    /*
-     * Use pig LOAD to load testing data for store
-     */
-    String query = "records = LOAD '" + pathTable.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    pigServer.registerQuery(query);
-
-    /*
-    Iterator<Tuple> it2 = pigServer.openIterator("records");
-    int row0 = 0;
-    Tuple RowValue2 = null;
-    while (it2.hasNext()) {
-      // Last row value
-      RowValue2 = it2.next();
-      row0++;
-      if (row0 == 10) {
-        Assert.assertEquals("0_01", RowValue2.get(1));
-        Assert.assertEquals("0_00", RowValue2.get(0));
-      }
-    }
-    Assert.assertEquals(10, row0);
-    */
-
-    String filter1 = "records3 = FILTER records BY SF_a > '4';";
-    pigServer.registerQuery(filter1);
-
-    String filter2 = "records4 = FILTER records BY SF_a > '4';";
-    pigServer.registerQuery(filter2);
-
-    String cog = "records5 = cogroup records3 by SF_a, records4 by SF_a;";
-    pigServer.registerQuery(cog);
-
-    String foreach = "records6 = foreach records5 generate flatten(records3), flatten(records4);";
-    pigServer.registerQuery(foreach);
-
-    Path newPath = new Path("TestCogroup.testStorer");
-    System.out.println("newPath = " + newPath);
-    removeDir(getTableFullPath(newPath.toString()+"1"));
-
-    /*
-     * Table1 creation
-     */
-    ExecJob pigJob = pigServer
-        .store(
-            "records6",
-            newPath.toString()+"1",
-            TableStorer.class.getCanonicalName()
-                + "('[records3::SF_a]; [records4::SF_a]')");
-    Assert.assertNull(pigJob.getException());
-    
-    removeDir(getTableFullPath(newPath.toString()+"1"));
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestCollection.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestCollection.java
deleted file mode 100644
index 4d3bd127e..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestCollection.java
+++ /dev/null
@@ -1,221 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.File;
-import java.io.IOException;
-import java.util.Iterator;
-import java.util.List;
-import junit.framework.Assert;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.ExecType;
-import org.apache.pig.PigServer;
-import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.test.MiniCluster;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * 
- * Test projections on complicated column types.
- * 
- */
-public class TestCollection extends BaseTestCase
-{
-  final static String STR_SCHEMA = "c:collection(record(a:double, b:double, c:bytes))";
-  final static String STR_STORAGE = "[c]";
-
-  private static Path path;
-
-  @BeforeClass
-  public static void setUp() throws Exception {
-
-    init();
-
-    path = getTableFullPath("TestCollection");
-    removeDir(path);
-
-    BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-        STR_STORAGE, conf);
-   
-
-    writer.finish();
-
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-    TypesUtils.resetTuple(tuple);
-    DataBag bagColl = TypesUtils.createBag();
-    Schema schColl = schema.getColumn(0).getSchema().getColumn(0).getSchema();
-    Tuple tupColl1 = TypesUtils.createTuple(schColl);
-    Tuple tupColl2 = TypesUtils.createTuple(schColl);
-    byte[] abs1 = new byte[3];
-    byte[] abs2 = new byte[4];
-    tupColl1.set(0, 3.1415926);
-    tupColl1.set(1, 1.6);
-    abs1[0] = 11;
-    abs1[1] = 12;
-    abs1[2] = 13;
-    tupColl1.set(2, new DataByteArray(abs1));
-    bagColl.add(tupColl1);
-    tupColl2.set(0, 123.456789);
-    tupColl2.set(1, 100);
-    abs2[0] = 21;
-    abs2[1] = 22;
-    abs2[2] = 23;
-    abs2[3] = 24;
-    tupColl2.set(2, new DataByteArray(abs2));
-    bagColl.add(tupColl2);
-    tuple.set(0, bagColl);
-
-    int row = 0;
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    bagColl.clear();
-    TypesUtils.resetTuple(tupColl1);
-    TypesUtils.resetTuple(tupColl2);
-    tupColl1.set(0, 7654.321);
-    tupColl1.set(1, 0.0001);
-    abs1[0] = 31;
-    abs1[1] = 32;
-    abs1[2] = 33;
-    tupColl1.set(2, new DataByteArray(abs1));
-    bagColl.add(tupColl1);
-    tupColl2.set(0, 0.123456789);
-    tupColl2.set(1, 0.3333);
-    abs2[0] = 41;
-    abs2[1] = 42;
-    abs2[2] = 43;
-    abs2[3] = 44;
-    tupColl2.set(2, new DataByteArray(abs2));
-    bagColl.add(tupColl2);
-    tuple.set(0, bagColl);
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    inserter.close();
-    writer1.finish();
-
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-    BasicTable.drop(path, conf);
-  }
-
-  @Test
-  public void testRead1() throws IOException, ParseException {
-    String query = "records = LOAD '" + path.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('c');";
-    System.out.println(query);
-    pigServer.registerQuery(query);
-    Iterator<Tuple> it = pigServer.openIterator("records");
-    int row = 0;
-    int inner = 0;
-    while (it.hasNext()) {
-      Tuple cur = it.next();
-      row++;
-      if (row == 1) {
-        Iterator<Tuple> bag = ((DataBag) cur.get(0)).iterator();
-        while (bag.hasNext()) {
-          Tuple cur2 = bag.next();
-          inner++;
-          if (inner == 1) {
-            Assert.assertEquals(3.1415926, cur2.get(0));
-            Assert.assertEquals(1.6, cur2.get(1));
-          }
-          if (inner == 2) {
-            Assert.assertEquals(123.456789, cur2.get(0));
-            Assert.assertEquals(100, cur2.get(1));
-          }
-        }// inner while
-      } // if count ==1
-      if (row == 2) {
-        Iterator<Tuple> bag = ((DataBag) cur.get(0)).iterator();
-        while (bag.hasNext()) {
-          Tuple cur2 = bag.next();
-          inner++;
-          if (inner == 1) {
-            Assert.assertEquals(7654.321, cur2.get(0));
-            Assert.assertEquals(0.0001, cur2.get(1));
-            System.out.println("cur : " + cur2.toString());
-            System.out.println("byte : " + cur2.get(2).toString());
-          }
-          if (inner == 2) {
-            Assert.assertEquals(0.123456789, cur2.get(0));
-            Assert.assertEquals(0.3333, cur2.get(1));
-            System.out.println("byte : " + cur2.get(2).toString());
-          }
-        }// inner while
-      }// if count ==2
-      TypesUtils.resetTuple(cur);
-    }
-  }
-
-  @Test
-  // Negative none exist column, using IO layer impl
-  public void testRead2() throws IOException, ParseException {
-    String projection = new String("d");
-    BasicTable.Reader reader = new BasicTable.Reader(path, conf);
-    reader.setProjection(projection);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    // Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    // RowValue is an emplty record
-    Assert.assertEquals(null, RowValue.get(0));
-    Assert.assertEquals(1, RowValue.size());
-    reader.close();
-  }
-
-  // Negative none exist column, TODO: failed, throw null pointer
-  @Test
-  public void testReadNeg2() throws IOException, ParseException {
-    String query = "records = LOAD '" + path.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('d');";
-    System.out.println(query);
-    pigServer.registerQuery(query);
-    // TODO: verify it returns a tuple with null value
-  }
-
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestCollectionTableLoader.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestCollectionTableLoader.java
deleted file mode 100644
index c25d1f323..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestCollectionTableLoader.java
+++ /dev/null
@@ -1,117 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.Iterator;
-
-public class TestCollectionTableLoader extends BaseTestCase
-{
-
-  private static Path path;
-
-  @BeforeClass
-  public static void setUp() throws Exception {
-    init();
-
-    path = getTableFullPath("TestCollectionTableLoader");
-    removeDir(path);
-
-    BasicTable.Writer writer = new BasicTable.Writer(path,
-        "c:collection(record(a:double, b:double, c:bytes))", "[c]", conf);
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    final int numsBatch = 10;
-    final int numsInserters = 2;
-    TableInserter[] inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tuple);
-
-        DataBag bagColl = TypesUtils.createBag();
-        Schema schColl = schema.getColumn(0).getSchema().getColumn(0).getSchema();
-        Tuple tupColl1 = TypesUtils.createTuple(schColl);
-        Tuple tupColl2 = TypesUtils.createTuple(schColl);
-        byte[] abs1 = new byte[3];
-        byte[] abs2 = new byte[4];
-        tupColl1.set(0, 3.1415926);
-        tupColl1.set(1, 1.6);
-        abs1[0] = 11;
-        abs1[1] = 12;
-        abs1[2] = 13;
-        tupColl1.set(2, new DataByteArray(abs1));
-        bagColl.add(tupColl1);
-        tupColl2.set(0, 123.456789);
-        tupColl2.set(1, 100);
-        abs2[0] = 21;
-        abs2[1] = 22;
-        abs2[2] = 23;
-        abs2[3] = 24;
-        tupColl2.set(2, new DataByteArray(abs2));
-        bagColl.add(tupColl2);
-        tuple.set(0, bagColl);
-
-        inserters[i].insert(new BytesWritable(("key" + i).getBytes()), tuple);
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-	writer.close();
-  }
-
-  @AfterClass
-  public static void tearDown() throws Exception {
-    pigServer.shutdown();
-  }
-
-  @Test
-  public void testReader() throws ExecException, IOException {
-    String query = "records = LOAD '" + path.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('c');";
-    System.out.println(query);
-    pigServer.registerQuery(query);
-    Iterator<Tuple> it = pigServer.openIterator("records");
-    while (it.hasNext()) {
-      Tuple cur = it.next();
-      System.out.println(cur);
-    }
-  }
-
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestCollectionTableStorer.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestCollectionTableStorer.java
deleted file mode 100644
index 4717fa7e1..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestCollectionTableStorer.java
+++ /dev/null
@@ -1,122 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import junit.framework.Assert;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.executionengine.ExecJob;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.Iterator;
-
-public class TestCollectionTableStorer extends BaseTestCase
-{
-  private static Path pathTable;
-
-  @BeforeClass
-  public static void setUp() throws Exception {
-    init();
-    pathTable = getTableFullPath("TestCollectionTableStorer");
-    removeDir(pathTable);
-
-    BasicTable.Writer writer = new BasicTable.Writer(pathTable,
-        "c:collection(record(a:double, b:double, c:bytes))", "[c]", conf);
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    final int numsBatch = 10;
-    final int numsInserters = 2;
-    TableInserter[] inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tuple);
-
-        DataBag bagColl = TypesUtils.createBag();
-        Schema schColl = schema.getColumn(0).getSchema().getColumn(0).getSchema();
-        Tuple tupColl1 = TypesUtils.createTuple(schColl);
-        Tuple tupColl2 = TypesUtils.createTuple(schColl);
-        byte[] abs1 = new byte[3];
-        byte[] abs2 = new byte[4];
-        tupColl1.set(0, 3.1415926);
-        tupColl1.set(1, 1.6);
-        abs1[0] = 11;
-        abs1[1] = 12;
-        abs1[2] = 13;
-        tupColl1.set(2, new DataByteArray(abs1));
-        bagColl.add(tupColl1);
-        tupColl2.set(0, 123.456789);
-        tupColl2.set(1, 100);
-        abs2[0] = 21;
-        abs2[1] = 22;
-        abs2[2] = 23;
-        abs2[3] = 24;
-        tupColl2.set(2, new DataByteArray(abs2));
-        bagColl.add(tupColl2);
-        tuple.set(0, bagColl);
-
-        inserters[i].insert(new BytesWritable(("key" + i).getBytes()), tuple);
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDown() throws Exception {
-    pigServer.shutdown();
-  }
-
-  @Test
-  public void testStorer() throws ExecException, IOException {
-    System.out.println("testStorer");
-    String query = "records = LOAD '" + pathTable.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    pigServer.registerQuery(query);
-
-    Iterator<Tuple> it = pigServer.openIterator("records");
-    while (it.hasNext()) {
-      Tuple cur = it.next();
-      System.out.println(cur);
-    }
-
-    ExecJob pigJob = pigServer.store("records", new Path(pathTable, "store")
-        .toString(), TableStorer.class.getCanonicalName() + "('[c]')");
-
-    Assert.assertNull(pigJob.getException());
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestGlobTableLoader.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestGlobTableLoader.java
deleted file mode 100644
index b3877aef9..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestGlobTableLoader.java
+++ /dev/null
@@ -1,152 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.File;
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.ExecType;
-import org.apache.pig.PigServer;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.test.MiniCluster;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * Note:
- * 
- * Make sure you add the build/pig-0.1.0-dev-core.jar to the Classpath of the
- * app/debug configuration, when run this from inside the Eclipse.
- * 
- */
-public class TestGlobTableLoader extends BaseTestCase {
-
-  private static Path pathTable;
-
-  @BeforeClass
-  public static void setUp() throws Exception {
-    init();
-    pathTable = getTableFullPath("TestGlobTableLoader");
-    removeDir(pathTable);
-
-    BasicTable.Writer writer = new BasicTable.Writer(pathTable,
-        "m1:map(string)", "[m1#{a}]", conf);
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    final int numsBatch = 10;
-    final int numsInserters = 2;
-    TableInserter[] inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tuple);
-        Map<String, String> map = new HashMap<String, String>();
-        map.put("a", "x");
-        map.put("b", "y");
-        map.put("c", "z");
-        tuple.set(0, map);
-
-        try {
-          inserters[i].insert(new BytesWritable(("key" + i).getBytes()), tuple);
-        } catch (Exception e) {
-          System.out.println(e.getMessage());
-        }
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-  	writer.close();
-  }
-
-  @AfterClass
-  public static void tearDown() throws Exception {
-    pigServer.shutdown();
-  }
-
-  // @Test
-  public void test1() throws IOException, ParseException {
-    String projection = new String("m1#{b}");
-    BasicTable.Reader reader = new BasicTable.Reader(pathTable, conf);
-    reader.setProjection(projection);
-    // long totalBytes = reader.getStatus().getSize();
-
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    reader.close();
-    reader = new BasicTable.Reader(pathTable, conf);
-    reader.setProjection(projection);
-
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple value = TypesUtils.createTuple(scanner.getSchema());
-    // HashMap<String, Object> mapval;
-    while (!scanner.atEnd()) {
-      scanner.getKey(key);
-      // Assert.assertEquals(key, new BytesWritable("key0".getBytes()));
-      scanner.getValue(value);
-      System.out.println("key = " + key + " value = " + value);
-
-      // mapval = (HashMap<String, Object>) value.get(0);
-      // Assert.assertEquals("x", mapval.get("a"));
-      // Assert.assertEquals(null, mapval.get("b"));
-      // Assert.assertEquals(null, mapval.get("c"));
-      scanner.advance();
-    }
-    reader.close();
-  }
-
-  @Test
-  public void testReader() throws ExecException, IOException {
-    //pathTable = new Path("/user/" + System.getenv("USER") + "/{TestGlobTableLoader}");
-    pathTable = getTableFullPath("{TestGlobTableLoader}");
-    
-    String query = "records = LOAD '" + pathTable.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('m1#{a}');";
-    System.out.println(query);
-    pigServer.registerQuery(query);
-    Iterator<Tuple> it = pigServer.openIterator("records");
-    while (it.hasNext()) {
-      Tuple cur = it.next();
-      System.out.println(cur);
-    }
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestLoaderWithCollection.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestLoaderWithCollection.java
deleted file mode 100644
index ddad6663d..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestLoaderWithCollection.java
+++ /dev/null
@@ -1,100 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.Iterator;
-
-public class TestLoaderWithCollection extends BaseTestCase
-{
-    private static Path pathTable;
-
-    @BeforeClass
-    public static void setUp() throws Exception {
-      init();
-      pathTable = getTableFullPath("TestMapTableLoader");
-      removeDir(pathTable);
-
-        BasicTable.Writer writer = new BasicTable.Writer(pathTable,
-                "c:collection(record(a:double))", "[c]", conf);
-        Schema schema = writer.getSchema();
-        Tuple tuple = TypesUtils.createTuple(schema);
-
-        final int numsBatch = 10;
-        final int numsInserters = 2;
-        TableInserter[] inserters = new TableInserter[numsInserters];
-        for (int i = 0; i < numsInserters; i++) {
-            inserters[i] = writer.getInserter("ins" + i, false);
-        }
-
-        for (int b = 0; b < numsBatch; b++) {
-            for (int i = 0; i < numsInserters; i++) {
-                TypesUtils.resetTuple(tuple);
-
-                DataBag bagColl = TypesUtils.createBag();
-                Schema schColl = schema.getColumn(0).getSchema().getColumn(0).getSchema();
-                Tuple tupColl1 = TypesUtils.createTuple(schColl);
-                Tuple tupColl2 = TypesUtils.createTuple(schColl);
-                tupColl1.set(0, 3.1415926);
-                bagColl.add(tupColl1);
-                tupColl2.set(0, 123.456789);
-                bagColl.add(tupColl2);
-                tuple.set(0, bagColl);
-
-                inserters[i].insert(new BytesWritable(("key" + i).getBytes()), tuple);
-            }
-        }
-        for (int i = 0; i < numsInserters; i++) {
-            inserters[i].close();
-        }
-        writer.close();
-    }
-
-    @AfterClass
-    public static void tearDown() throws Exception {
-        pigServer.shutdown();
-    }
-
-    @Test
-    public void test() throws ExecException, IOException {
-        String query = "records = LOAD '" + pathTable.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('c');";
-        System.out.println(query);
-        pigServer.registerQuery(query);
-        Iterator<Tuple> it = pigServer.openIterator("records");
-        while (it.hasNext()) {
-            Tuple cur = it.next();
-            System.out.println(cur);
-        }
-    }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMapSideCoGroup.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMapSideCoGroup.java
deleted file mode 100644
index e04a21faa..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMapSideCoGroup.java
+++ /dev/null
@@ -1,136 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.IOException;
-import java.util.Iterator;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TestBasicTable;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.data.DataBag;
-import org.junit.AfterClass;
-import org.junit.Assert;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-public class TestMapSideCoGroup extends BaseTestCase {
-    private static Path table1, table2;
-    private static Configuration conf;
-
-    @BeforeClass
-    public static void setUp() throws Exception {
-        init();
-        TestBasicTable.setUpOnce();
-        conf = TestBasicTable.conf;
-        table1 = getTableFullPath( "TestMapSideCoGroup1" );
-        removeDir( table1 );
-        table2 = getTableFullPath( "TestMapSideCoGroup2" );
-        removeDir( table2 );
-    }
-
-    @AfterClass
-    public static void tearDown() throws Exception {
-        pigServer.shutdown();
-    }
-
-    @Test
-    public void test() throws IOException {
-        int table1RowCount = 100000;
-        int table2RowCount = 200000;
-        int table1DupFactor = 15;
-        int table2DupFactor = 125;
-        createTable( table1RowCount, table1DupFactor, "a:int, b:string, c:string", "[a, b, c]", "a", table1 );    
-        createTable( table2RowCount, table2DupFactor, "a:int, d:string", "[a, d]", "a", table2 );
-
-        String qs1 = "T1 = load '" + table1.toString() + "' USING org.apache.hadoop.zebra.pig.TableLoader('a, b, c', 'sorted');";
-        System.out.println( "qs1: " + qs1 );
-        String qs2 = "T2 = load '" + table2.toString() + "' USING org.apache.hadoop.zebra.pig.TableLoader('a, d', 'sorted');";
-        System.out.println( "qs2: " + qs2 );
-
-        pigServer.registerQuery( qs1 );
-        pigServer.registerQuery( qs2 );
-
-        String qs3 = "T3 = cogroup T1 by a, T2 by a USING 'merge';";
-        pigServer.registerQuery( qs3 );
-
-        org.apache.pig.impl.logicalLayer.schema.Schema schema = pigServer.dumpSchema( "T3" );
-        Assert.assertEquals( "{group: int,T1: {(a: int,b: chararray,c: chararray)},T2: {(a: int,d: chararray)}}",
-                schema.toString() );
-        Iterator<Tuple> it = pigServer.openIterator( "T3" );
-        int count = 0;
-        int expectedCount = Math.max( table1RowCount/table1DupFactor, table2RowCount/table2DupFactor) + 1;
-        int totalRowsInBag1 = 0;
-        int totalRowsInBag2 = 0;
-        while( it.hasNext() ) {
-            Tuple result = it.next();
-            totalRowsInBag1 += ( (DataBag)result.get( 1 ) ).size();
-            totalRowsInBag2 += ( (DataBag)result.get( 2 ) ).size();
-//            System.out.println( "tuple = " + result.toDelimitedString( "," ) );
-            count++;
-        }
-
-        Assert.assertEquals( expectedCount, count );
-        Assert.assertEquals(table1RowCount, totalRowsInBag1 );
-        Assert.assertEquals(table2RowCount, totalRowsInBag2 );
-    }
-
-    public static void createTable(int rows, int step, String strSchema, String storage, String sortColumns, Path path)
-    throws IOException {
-        if( fs.exists(path) ) {
-            BasicTable.drop(path, conf);
-        }
-
-        BasicTable.Writer writer = new BasicTable.Writer(path, strSchema, storage, sortColumns, null, conf);
-        writer.finish();
-
-        Schema schema = writer.getSchema();
-        String colNames[] = schema.getColumns();
-        Tuple tuple = TypesUtils.createTuple(schema);
-
-        writer = new BasicTable.Writer(path, conf);
-        TableInserter inserter = writer.getInserter( String.format("part-%06d", 1), true );
-        for( int i = 1; i <= rows; ++i ) {
-            BytesWritable key = new BytesWritable( String.format( "key%09d", i/step ).getBytes() );
-            TypesUtils.resetTuple(tuple);
-            tuple.set( 0,  i / step );
-            for( int k = 1; k < tuple.size(); ++k ) {
-                try {
-                    tuple.set( k, new String( "col-" + colNames[k] + i * 10 ) );
-                } catch (ExecException e) {
-                    e.printStackTrace();
-                }
-            }
-            inserter.insert(key, tuple);
-        }
-        inserter.close();
-
-        writer = new BasicTable.Writer(path, conf);
-        writer.close();
-    }
-
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMapSideGroupBy.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMapSideGroupBy.java
deleted file mode 100644
index 7744f15d0..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMapSideGroupBy.java
+++ /dev/null
@@ -1,251 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-import junit.framework.Assert;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.pig.TableStorer;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.executionengine.ExecJob;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.data.DataBag;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-public class TestMapSideGroupBy extends BaseTestCase {
-  final static int numsBatch = 4;
-  final static int numsInserters = 1;
-  static Path pathTable1;
-  final static String STR_SCHEMA1 = "a:int,b:float,c:long,d:double,e:string,f:bytes,r1:record(f1:string, f2:string),m1:map(string)";
-  final static String STR_STORAGE1 = "[a, b, c]; [e, f]; [r1.f1]; [m1#{a}]";
-  static int t1 =0;
-
-  @BeforeClass
-  public static void setUp() throws Exception {
-    init();
-    
-    pathTable1 = getTableFullPath("TestGroupBy");
-    removeDir(pathTable1);
-
-    createFirstTable();
-  }
-  
-  public static void createFirstTable() throws IOException, ParseException {
-    BasicTable.Writer writer = new BasicTable.Writer(pathTable1, STR_SCHEMA1,
-        STR_STORAGE1, conf);
-    Schema schema = writer.getSchema();
-    //System.out.println("typeName" + schema.getColumn("a").type.pigDataType());
-    Tuple tuple = TypesUtils.createTuple(schema);
-  
-    TableInserter[] inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-    Tuple tupRecord1;
-    try {
-      tupRecord1 = TypesUtils.createTuple(schema.getColumnSchema("r1")
-          .getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-    Map<String, String> m1 = new HashMap<String, String>();
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tupRecord1);
-        TypesUtils.resetTuple(tuple);
-        m1.clear();
-  
-        try {
-          // first row of the table , the biggest row
-          if (i == 0 && b == 0) {
-            tuple.set(0, 100);
-            tuple.set(1, 100.1f);
-            tuple.set(2, 100L);
-            tuple.set(3, 50e+2);
-            tuple.set(4, "something");
-            tuple.set(5, new DataByteArray("something"));
-  
-          }
-          // the middle + 1 row of the table, the smallest row
-          else if (i == 0 && b == (numsBatch / 2)) {
-            tuple.set(0, -100);
-            tuple.set(1, -100.1f);
-            tuple.set(2, -100L);
-            tuple.set(3, -50e+2);
-            tuple.set(4, "so");
-            tuple.set(5, new DataByteArray("so"));
-  
-          }
-  
-          else {
-            Float f = 1.1f;
-            long l = 11;
-            double d = 1.1;
-            tuple.set(0, b);
-            tuple.set(1, f);
-            tuple.set(2, l);
-            tuple.set(3, d);
-            tuple.set(4, "some");
-            tuple.set(5, new DataByteArray("some"));
-          }
-  
-          // insert record
-          tupRecord1.set(0, "" + b);
-          tupRecord1.set(1, "" + b);
-          tuple.set(6, tupRecord1);
-  
-          // insert map
-          m1.put("a", "" + b);
-          m1.put("b", "" + b);
-          m1.put("c", "" + b);
-          tuple.set(7, m1);
-  
-        } catch (ExecException e) {
-          e.printStackTrace();
-        }
-  
-       inserters[i].insert(new BytesWritable(("key_" + b).getBytes()), tuple);
-       
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-    writer.close();
-    
-  
-    //check table is setup correctly
-    String projection = new String("a,b,c,d,e,f,r1,m1");
-    
-    BasicTable.Reader reader = new BasicTable.Reader(pathTable1, conf);
-    reader.setProjection(projection);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-  
-    scanner.getValue(RowValue);
-    System.out.println("rowvalue size:"+RowValue.size());
-    System.out.println("read a : " + RowValue.get(0).toString());
-    System.out.println("read string: " + RowValue.get(1).toString());
-  
-    scanner.advance();
-    scanner.getValue(RowValue);
-    System.out.println("read float in 2nd row: "+ RowValue.get(1).toString());
-    System.out.println("done insert table");
-  
-    reader.close();
-    
-  }
-
-  @AfterClass
-  public static void tearDown() throws Exception {
-    pigServer.shutdown();
-    BasicTable.drop(pathTable1, conf);
-  }
-  
-  public void verify(Iterator<Tuple> it3) throws ExecException {
-    int row = 0;
-    Tuple RowValue3 = null;
-    while (it3.hasNext()) {
-      RowValue3 = it3.next();
-      Assert.assertEquals(2, RowValue3.size());
-      row++;
-      String key = (String) RowValue3.get(0);
-      DataBag bag = (DataBag) RowValue3.get(1);
-      Iterator<Tuple> it = bag.iterator();
-      if (key.equals("so")) {
-        Assert.assertEquals(1, bag.size());
-        Tuple t = it.next();
-        Assert.assertEquals(-100, t.get(0));
-      } else if (key.equals("something")) {
-        Assert.assertEquals(1, bag.size());
-        Tuple t = it.next();
-        Assert.assertEquals(100, t.get(0));
-      } else if (key.equals("some")) {
-        Assert.assertEquals(2, bag.size());
-        Tuple t1 = it.next(), t2 = it.next();
-        int i1 = (Integer) t1.get(0), i2 = (Integer) t2.get(0);
-        Assert.assertTrue((i1 == 1 && i2 == 3) || (i1 == 3 || i1 ==1));
-      } else {
-        Assert.fail("Unexprected key: " + key);
-      }
-    }
-    Assert.assertEquals(3, row);
-  }
-
-  public Iterator<Tuple> groupby(String table1, String sortkey1) throws IOException {
-    String query1 = "records1 = LOAD '" + pathTable1.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    System.out.println("query1:" + query1);
-    pigServer.registerQuery(query1);
-
-    String orderby1 = "sort1 = ORDER records1 BY " + sortkey1 + " ;";
-    pigServer.registerQuery(orderby1);
-
-    t1++;
-    
-    String table1path = pathTable1.toString() + Integer.toString(t1);
-    removeDir(new Path(table1path));
-    ExecJob pigJob = pigServer.store("sort1", table1path, TableStorer.class.getCanonicalName()
-        + "('[a, b, c]; [d, e, f, r1, m1]')");
-
-    Assert.assertNull(pigJob.getException());
-
-    String query3 = "records1 = LOAD '"
-        + table1path
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('a, b, c, d, e, f, r1, m1', 'sorted');";
-
-    System.out.println("query3:" + query3);
-    pigServer.registerQuery(query3);   
-    
-    String foreach = "records11 = foreach records1 generate a as a, b as b, c as c, d as d, e as e, f as f, r1 as r1, m1 as m1;";
-    pigServer.registerQuery(foreach);
-
-    String join = "joinRecords = GROUP records11 BY " +  sortkey1 
-        + " USING \"collected\";";
-      pigServer.registerQuery(join);
-   
-    // check JOIN content
-    Iterator<Tuple> it3 = pigServer.openIterator("joinRecords");
-    return it3;
-  }
-  
-  @Test
-  public void test3() throws ExecException, IOException {
-    Iterator<Tuple> it3 = groupby(pathTable1.toString(), "e" );
-    verify(it3);
-  }  
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMapTableLoader.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMapTableLoader.java
deleted file mode 100644
index e32c7192d..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMapTableLoader.java
+++ /dev/null
@@ -1,142 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-
-/**
- * Note:
- * 
- * Make sure you add the build/pig-0.1.0-dev-core.jar to the Classpath of the
- * app/debug configuration, when run this from inside the Eclipse.
- * 
- */
-public class TestMapTableLoader extends BaseTestCase
-{
-  private static Path pathTable;
-
-  @BeforeClass
-  public static void setUp() throws Exception {
-    init();
-    pathTable = getTableFullPath("TestMapTableLoader");
-    removeDir(pathTable);
-
-    BasicTable.Writer writer = new BasicTable.Writer(pathTable,
-        "m1:map(string)", "[m1#{a}]", conf);
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    final int numsBatch = 10;
-    final int numsInserters = 2;
-    TableInserter[] inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tuple);
-        Map<String, String> map = new HashMap<String, String>();
-        map.put("a", "x");
-        map.put("b", "y");
-        map.put("c", "z");
-        tuple.set(0, map);
-
-        try {
-          inserters[i].insert(new BytesWritable(("key" + i).getBytes()), tuple);
-        } catch (Exception e) {
-          System.out.println(e.getMessage());
-        }
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-	writer.close();
-  }
-
-  @AfterClass
-  public static void tearDown() throws Exception {
-    pigServer.shutdown();
-  }
-
-  @Test
-  public void test1() throws IOException, ParseException {
-    String projection = new String("m1#{b}");
-    BasicTable.Reader reader = new BasicTable.Reader(pathTable, conf);
-    reader.setProjection(projection);
-    // long totalBytes = reader.getStatus().getSize();
-
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    reader.close();
-    reader = new BasicTable.Reader(pathTable, conf);
-    reader.setProjection(projection);
-
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple value = TypesUtils.createTuple(scanner.getSchema());
-    // HashMap<String, Object> mapval;
-    while (!scanner.atEnd()) {
-      scanner.getKey(key);
-      // Assert.assertEquals(key, new BytesWritable("key0".getBytes()));
-      scanner.getValue(value);
-      System.out.println("key = " + key + " value = " + value);
-
-      // mapval = (HashMap<String, Object>) value.get(0);
-      // Assert.assertEquals("x", mapval.get("a"));
-      // Assert.assertEquals(null, mapval.get("b"));
-      // Assert.assertEquals(null, mapval.get("c"));
-      scanner.advance();
-    }
-    reader.close();
-  }
-
-  @Test
-  public void testReader() throws ExecException, IOException {
-    String query = "records = LOAD '" + pathTable.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('m1#{a}');";
-    System.out.println(query);
-    pigServer.registerQuery(query);
-    Iterator<Tuple> it = pigServer.openIterator("records");
-    while (it.hasNext()) {
-      Tuple cur = it.next();
-      System.out.println(cur);
-    }
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMapTableStorer.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMapTableStorer.java
deleted file mode 100644
index 4ba1e8352..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMapTableStorer.java
+++ /dev/null
@@ -1,115 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import junit.framework.Assert;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.executionengine.ExecJob;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.Map;
-
-/**
- * Note:
- * 
- * Make sure you add the build/pig-0.1.0-dev-core.jar to the Classpath of the
- * app/debug configuration, when run this from inside the Eclipse.
- * 
- */
-public class TestMapTableStorer extends BaseTestCase
-{
-
-  private static Path pathTable ;
-
-
-  @BeforeClass
-  public static void setUp() throws Exception {
-    init();
-    pathTable = getTableFullPath(TestMapTableStorer.class.getSimpleName()) ;
-    removeDir(pathTable);
-
-    BasicTable.Writer writer = new BasicTable.Writer(pathTable,
-        "m:map(string)", "[m#{a}]", conf);
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    final int numsBatch = 10;
-    final int numsInserters = 2;
-    TableInserter[] inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tuple);
-
-        Map<String, String> map = new HashMap<String, String>();
-        map.put("a", "x");
-        map.put("b", "y");
-        map.put("c", "z");
-        tuple.set(0, map);
-
-        inserters[i].insert(new BytesWritable(("key" + i).getBytes()), tuple);
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDown() throws Exception {
-    pigServer.shutdown();
-  }
-
-  @Test
-  public void testStorer() throws ExecException, IOException {
-    System.out.println("testStorer");
-    String query = "records = LOAD '" + pathTable.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    pigServer.registerQuery(query);
-
-    Iterator<Tuple> it = pigServer.openIterator("records");
-    while (it.hasNext()) {
-      Tuple cur = it.next();
-      System.out.println(cur);
-    }
-
-    ExecJob pigJob = pigServer.store("records", new Path(pathTable, "store")
-        .toString(), TableStorer.class.getCanonicalName() + "('[m#{a|b}]')");
-
-    Assert.assertNull(pigJob.getException());
-
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMapType.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMapType.java
deleted file mode 100644
index 5b814671c..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMapType.java
+++ /dev/null
@@ -1,430 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.pig;
-
-import junit.framework.Assert;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-
-/**
- * 
- * Test projections on complicated column types.
- * 
- */
-public class TestMapType extends BaseTestCase
-{
-  final static String STR_SCHEMA = "m1:map(string),m2:map(map(int)), m4:map(map(record(f1:int,f2:string)))";
-  final static String STR_STORAGE = "[m1#{a}];[m2#{x|y}]; [m1#{b}, m2#{z}]; [m4#{a4}]; [m4#{b4|c4}]";
-
-  private static Path pathTable;
-
-
-  @BeforeClass
-  public static void setUp() throws Exception {
-    init();
-    pathTable = getTableFullPath(TestMapType.class.getSimpleName()) ;
-    removeDir(pathTable);
-
-    BasicTable.Writer writer = new BasicTable.Writer(pathTable, STR_SCHEMA,
-        STR_STORAGE, conf);
-    writer.finish();
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-    BasicTable.Writer writer1 = new BasicTable.Writer(pathTable, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-    TypesUtils.resetTuple(tuple);
-
-    Tuple record1;
-    try {
-      record1 = TypesUtils.createTuple(new Schema("f1:int, f2:string"));
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    Tuple record2;
-    try {
-      record2 = TypesUtils.createTuple(new Schema("f1:int, f2:string"));
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    Tuple record3;
-    try {
-      record3 = TypesUtils.createTuple(new Schema("f1:int, f2:string"));
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    // add data to row 1
-    // m1:map(string)
-    Map<String, String> m1 = new HashMap<String, String>();
-    m1.put("a", "A");
-    m1.put("b", "B");
-    m1.put("c", "C");
-    tuple.set(0, m1);
-
-    // m2:map(map(int))
-    HashMap<String, Map> m2 = new HashMap<String, Map>();
-    Map<String, Integer> m31 = new HashMap<String, Integer>();
-    m31.put("m311", 311);
-    m31.put("m321", 321);
-    m31.put("m331", 331);
-    Map<String, Integer> m32 = new HashMap<String, Integer>();
-    m32.put("m411", 411);
-    m32.put("m421", 421);
-    m32.put("m431", 431);
-    m2.put("x", m31);
-    m2.put("y", m32);
-    tuple.set(1, m2);
-
-    // m4:map(map(record(f1:int,f2:string)))
-    record1.set(0, 11);
-    record1.set(1, "record row 1.1");
-    Map<String, Tuple> m51 = new HashMap<String, Tuple>();
-    Map<String, Tuple> m52 = new HashMap<String, Tuple>();
-    Map<String, Tuple> m53 = new HashMap<String, Tuple>();
-    m51.put("ma4", (Tuple) record1);
-    m52.put("ma41", (Tuple) record1);
-    m53.put("ma43", (Tuple) record1);
-
-    record2.set(0, 12);
-    record2.set(1, "record row 1.2");
-    m51.put("mb4", (Tuple) record2);
-    m52.put("mb42", (Tuple) record2);
-    m53.put("ma43", (Tuple) record2);
-    System.out.println("record1-1: " + record1.toString());
-
-    record3.set(0, 13);
-    record3.set(1, "record row 1.3");
-    System.out.println("record1-3: " + record1.toString());
-
-    m51.put("mc4", (Tuple) record3);
-    m52.put("mc42", (Tuple) record3);
-    m53.put("ma43", (Tuple) record3);
-
-    Map<String, Map> m4 = new HashMap<String, Map>();
-    m4.put("a4", m51);
-    m4.put("b4", m52);
-    m4.put("c4", m53);
-    m4.put("d4", m53);
-    m4.put("ma43", m53);
-
-    tuple.set(2, m4);
-
-    int row = 0;
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // row 2
-    row++;
-    TypesUtils.resetTuple(tuple);
-    TypesUtils.resetTuple(record1);
-    TypesUtils.resetTuple(record2);
-    TypesUtils.resetTuple(record3);
-    m1.clear();
-    m2.clear();
-    m31.clear();
-    m32.clear();
-    m4.clear();
-    m51.clear();
-    m52.clear();
-    m53.clear();
-    // m1:map(string)
-    m1.put("a", "A2");
-    m1.put("b2", "B2");
-    m1.put("c2", "C2");
-    tuple.set(0, m1);
-
-    // m2:map(map(int))
-    m31.put("m321", 321);
-    m31.put("m322", 322);
-    m31.put("m323", 323);
-    m2.put("z", m31);
-    tuple.set(1, m2);
-
-    // m4:map(map(record(f1:int,f2:string)))
-    record1.set(0, 21);
-    record1.set(1, "record row 2.1");
-    m51.put("ma4", (Tuple) record1);
-    m52.put("ma41", (Tuple) record1);
-    m53.put("ma43", (Tuple) record1);
-
-    record2.set(0, 22);
-    record2.set(1, "record row 2.2");
-    m51.put("mb4", (Tuple) record2);
-    m52.put("mb42", (Tuple) record2);
-    m53.put("ma43", (Tuple) record2);
-
-    record3.set(0, 33);
-    record3.set(1, "record row 3.3");
-    m51.put("mc4", (Tuple) record3);
-    m52.put("mc42", (Tuple) record3);
-    m53.put("ma43", (Tuple) record3);
-
-    m4.put("a4", m51);
-    m4.put("b4", m52);
-
-    m4.put("ma43", m53);
-
-    tuple.set(2, m4);
-
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // finish building table, closing out the inserter, writer, writer1
-    inserter.close();
-    writer1.finish();
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-    BasicTable.drop(pathTable, conf);
-  }
-
-  // read one map
-  @Test
-  public void testReadSimpleMap() throws IOException, ParseException {
-
-    String query = "records = LOAD '" + pathTable.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('m1#{a}');";
-    System.out.println(query);
-    pigServer.registerQuery(query);
-    Iterator<Tuple> it = pigServer.openIterator("records");
-    BytesWritable key = new BytesWritable();
-    int row = 0;
-    while (it.hasNext()) {
-      Tuple RowValue = it.next();
-      System.out.println(RowValue);
-      row++;
-      if (row == 1) {
-        Assert.assertEquals("{a=A}", RowValue.get(0).toString());
-      }
-      if (row == 2) {
-        Assert.assertEquals("{a=A2}", RowValue.get(0).toString());
-      }
-    }
-  }
-
-  @Test
-  public void testReadMapOfMap() throws IOException, ParseException {
-    String query = "records = LOAD '"
-        + pathTable.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('m1#{b}, m2#{x|z}');";
-    System.out.println(query);
-    pigServer.registerQuery(query);
-    Iterator<Tuple> it = pigServer.openIterator("records");
-    BytesWritable key = new BytesWritable();
-    int row = 0;
-    while (it.hasNext()) {
-      Tuple RowValue = it.next();
-      System.out.println(RowValue);
-      row++;
-      if (row == 1) {
-        Assert.assertEquals("B", ((Map) RowValue.get(0)).get("b"));
-        Assert.assertEquals(321, ((Map) ((Map) RowValue.get(1)).get("x"))
-            .get("m321"));
-        Assert.assertEquals(311, ((Map) ((Map) RowValue.get(1)).get("x"))
-            .get("m311"));
-        Assert.assertEquals(331, ((Map) ((Map) RowValue.get(1)).get("x"))
-            .get("m331"));
-        Assert.assertEquals(null, ((Map) ((Map) RowValue.get(1)).get("x"))
-            .get("m341"));
-        Assert.assertEquals(null, ((Map) ((Map) RowValue.get(1)).get("z")));
-        Assert.assertEquals(null, ((Map) ((Map) RowValue.get(0)).get("a")));
-        Assert.assertEquals(null, ((Map) ((Map) RowValue.get(0)).get("c")));
-      }
-      if (row == 2) {
-        Assert.assertEquals(null, ((Map) RowValue.get(0)).get("b"));
-        Assert.assertEquals(null, ((Map) ((Map) RowValue.get(1)).get("x")));
-        Assert.assertEquals(323, ((Map) ((Map) RowValue.get(1)).get("z"))
-            .get("m323"));
-        Assert.assertEquals(322, ((Map) ((Map) RowValue.get(1)).get("z"))
-            .get("m322"));
-        Assert.assertEquals(321, ((Map) ((Map) RowValue.get(1)).get("z"))
-            .get("m321"));
-        Assert.assertEquals(null, ((Map) ((Map) RowValue.get(0)).get("a")));
-        Assert.assertEquals(null, ((Map) ((Map) RowValue.get(0)).get("b")));
-        Assert.assertEquals(null, ((Map) ((Map) RowValue.get(1)).get("a")));
-
-      }
-    }
-
-  }
-
-  @Test
-  public void testReadMapOfRecord1() throws IOException, ParseException {
-    String query = "records = LOAD '"
-        + pathTable.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('m1#{b}, m4#{a4|c4}');";
-    System.out.println(query);
-    pigServer.registerQuery(query);
-    Iterator<Tuple> it = pigServer.openIterator("records");
-    BytesWritable key = new BytesWritable();
-    int row = 0;
-    while (it.hasNext()) {
-      Tuple RowValue = it.next();
-      System.out.println(RowValue);
-      row++;
-      if (row == 1) {
-        Assert.assertEquals("B", ((Map) RowValue.get(0)).get("b"));
-        Assert.assertEquals(11, ((Tuple) ((Map) ((Map) RowValue.get(1))
-            .get("a4")).get("ma4")).get(0));
-        Assert.assertEquals("record row 1.1", ((Tuple) ((Map) ((Map) RowValue
-            .get(1)).get("a4")).get("ma4")).get(1));
-        Assert.assertEquals(12, ((Tuple) ((Map) ((Map) RowValue.get(1))
-            .get("a4")).get("mb4")).get(0));
-        Assert.assertEquals("record row 1.2", ((Tuple) ((Map) ((Map) RowValue
-            .get(1)).get("a4")).get("mb4")).get(1));
-        Assert.assertEquals(13, ((Tuple) ((Map) ((Map) RowValue.get(1))
-            .get("a4")).get("mc4")).get(0));
-        Assert.assertEquals("record row 1.3", ((Tuple) ((Map) ((Map) RowValue
-            .get(1)).get("a4")).get("mc4")).get(1));
-
-        Assert.assertEquals(13, ((Tuple) ((Map) ((Map) RowValue.get(1))
-            .get("c4")).get("ma43")).get(0));
-        Assert.assertEquals("record row 1.3", ((Tuple) ((Map) ((Map) RowValue
-            .get(1)).get("c4")).get("ma43")).get(1));
-
-        Assert.assertEquals(null, (((Map) ((Map) RowValue.get(1)).get("c4"))
-            .get("mc4")));
-        Assert.assertEquals(null, (((Map) ((Map) RowValue.get(1)).get("c4"))
-            .get("mb4")));
-
-      }
-      if (row == 2) {
-        Assert.assertEquals(21, ((Tuple) ((Map) ((Map) RowValue.get(1))
-            .get("a4")).get("ma4")).get(0));
-        Assert.assertEquals("record row 2.1", ((Tuple) ((Map) ((Map) RowValue
-            .get(1)).get("a4")).get("ma4")).get(1));
-        Assert.assertEquals(22, ((Tuple) ((Map) ((Map) RowValue.get(1))
-            .get("a4")).get("mb4")).get(0));
-        Assert.assertEquals("record row 2.2", ((Tuple) ((Map) ((Map) RowValue
-            .get(1)).get("a4")).get("mb4")).get(1));
-        Assert.assertEquals(33, ((Tuple) ((Map) ((Map) RowValue.get(1))
-            .get("a4")).get("mc4")).get(0));
-        Assert.assertEquals("record row 3.3", ((Tuple) ((Map) ((Map) RowValue
-            .get(1)).get("a4")).get("mc4")).get(1));
-
-      }
-    }
-
-  }
-
-  @Test
-  // Positive? map object column through pig loader
-  public void testRead4() throws IOException, ParseException {
-    String query = "records = LOAD '" + pathTable.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('m1');";
-
-    pigServer.registerQuery(query);
-    Iterator<Tuple> it = pigServer.openIterator("records");
-    int row = 0;
-    while (it.hasNext()) {
-      Tuple RowValue = it.next();
-      System.out.println(RowValue);
-      row++;
-      if (row == 1) {
-        Assert.assertEquals("A", ((Map) RowValue.get(0)).get("a"));
-        Assert.assertEquals("B", ((Map) RowValue.get(0)).get("b"));
-        Assert.assertEquals("C", ((Map) RowValue.get(0)).get("c"));
-        Assert.assertEquals(null, ((Map) RowValue.get(0)).get("non-existent"));
-        Assert.assertEquals(null, ((Map) RowValue.get(0)).get("b2"));
-        Assert.assertEquals(null, ((Map) RowValue.get(0)).get("c2"));
-      }
-      if (row == 2) {
-        Assert.assertEquals("A2", ((Map) RowValue.get(0)).get("a"));
-        Assert.assertEquals("B2", ((Map) RowValue.get(0)).get("b2"));
-        Assert.assertEquals("C2", ((Map) RowValue.get(0)).get("c2"));
-        Assert.assertEquals(null, ((Map) RowValue.get(0)).get("non-existent"));
-        Assert.assertEquals(null, ((Map) RowValue.get(0)).get("b"));
-        Assert.assertEquals(null, ((Map) RowValue.get(0)).get("c"));
-      }
-    }
-    Assert.assertEquals(2, row);
-  }
-
-  @Test
-  // Negative non-exist column through pig loader
-  public void testReadNeg1() throws IOException, ParseException {
-    String query = "records = LOAD '" + pathTable.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('m5');";
-
-    pigServer.registerQuery(query);
-    Iterator<Tuple> it = pigServer.openIterator("records");
-    int row = 0;
-    while (it.hasNext()) {
-      Tuple RowValue = it.next();
-      System.out.println(RowValue);
-      row++;
-      if (row == 1) {
-        Assert.assertEquals(null, RowValue.get(0));
-        Assert.assertEquals(1, RowValue.size());
-      }
-    }
-    Assert.assertEquals(2, row);
-  }
-
-  @Test
-  // non-existent column name through I/O
-  public void testNeg2() throws IOException, ParseException {
-    String projection = new String("m5");
-    BasicTable.Reader reader = new BasicTable.Reader(pathTable, conf);
-    reader.setProjection(projection);
-
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(null, RowValue.get(0));
-    Assert.assertEquals(1, RowValue.size());
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(null, RowValue.get(0));
-    Assert.assertEquals(1, RowValue.size());
-    reader.close();
-  }
-}
\ No newline at end of file
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMapTypePrune.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMapTypePrune.java
deleted file mode 100644
index 1e06f2f81..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMapTypePrune.java
+++ /dev/null
@@ -1,420 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.pig;
-
-import junit.framework.Assert;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-
-/**
- * 
- * Test projections on complicated column types.
- * 
- */
-public class TestMapTypePrune extends BaseTestCase
-{
-  final static String STR_SCHEMA = "m1:map(string),m2:map(map(int)), m4:map(map(record(f1:int,f2:string)))";
-  final static String STR_STORAGE = "[m1#{a}];[m2#{x|y}]; [m1#{b}, m2#{z}]; [m4#{a4}]; [m4#{b4|c4}]";
-
-  private static Path pathTable;
-
-
-  @BeforeClass
-  public static void setUp() throws Exception {
-    init();
-    pathTable = getTableFullPath(TestMapTypePrune.class.getSimpleName()) ;
-    removeDir(pathTable);
-
-    BasicTable.Writer writer = new BasicTable.Writer(pathTable, STR_SCHEMA,
-        STR_STORAGE, conf);
-    writer.finish();
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-    BasicTable.Writer writer1 = new BasicTable.Writer(pathTable, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-    TypesUtils.resetTuple(tuple);
-
-    Tuple record1;
-    try {
-      record1 = TypesUtils.createTuple(new Schema("f1:int, f2:string"));
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    Tuple record2;
-    try {
-      record2 = TypesUtils.createTuple(new Schema("f1:int, f2:string"));
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    Tuple record3;
-    try {
-      record3 = TypesUtils.createTuple(new Schema("f1:int, f2:string"));
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    // add data to row 1
-    // m1:map(string)
-    Map<String, String> m1 = new HashMap<String, String>();
-    m1.put("a", "A");
-    m1.put("b", "B");
-    m1.put("c", "C");
-    tuple.set(0, m1);
-
-    // m2:map(map(int))
-    HashMap<String, Map> m2 = new HashMap<String, Map>();
-    Map<String, Integer> m31 = new HashMap<String, Integer>();
-    m31.put("m311", 311);
-    m31.put("m321", 321);
-    m31.put("m331", 331);
-    Map<String, Integer> m32 = new HashMap<String, Integer>();
-    m32.put("m411", 411);
-    m32.put("m421", 421);
-    m32.put("m431", 431);
-    m2.put("x", m31);
-    m2.put("y", m32);
-    tuple.set(1, m2);
-
-    // m4:map(map(record(f1:int,f2:string)))
-    record1.set(0, 11);
-    record1.set(1, "record row 1.1");
-    Map<String, Tuple> m51 = new HashMap<String, Tuple>();
-    Map<String, Tuple> m52 = new HashMap<String, Tuple>();
-    Map<String, Tuple> m53 = new HashMap<String, Tuple>();
-    m51.put("ma4", (Tuple) record1);
-    m52.put("ma41", (Tuple) record1);
-    m53.put("ma43", (Tuple) record1);
-
-    record2.set(0, 12);
-    record2.set(1, "record row 1.2");
-    m51.put("mb4", (Tuple) record2);
-    m52.put("mb42", (Tuple) record2);
-    m53.put("ma43", (Tuple) record2);
-    System.out.println("record1-1: " + record1.toString());
-
-    record3.set(0, 13);
-    record3.set(1, "record row 1.3");
-    System.out.println("record1-3: " + record1.toString());
-
-    m51.put("mc4", (Tuple) record3);
-    m52.put("mc42", (Tuple) record3);
-    m53.put("ma43", (Tuple) record3);
-
-    Map<String, Map> m4 = new HashMap<String, Map>();
-    m4.put("a4", m51);
-    m4.put("b4", m52);
-    m4.put("c4", m53);
-    m4.put("d4", m53);
-    m4.put("ma43", m53);
-
-    tuple.set(2, m4);
-
-    int row = 0;
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // row 2
-    row++;
-    TypesUtils.resetTuple(tuple);
-    TypesUtils.resetTuple(record1);
-    TypesUtils.resetTuple(record2);
-    TypesUtils.resetTuple(record3);
-    m1.clear();
-    m2.clear();
-    m31.clear();
-    m32.clear();
-    m4.clear();
-    m51.clear();
-    m52.clear();
-    m53.clear();
-    // m1:map(string)
-    m1.put("a", "A2");
-    m1.put("b2", "B2");
-    m1.put("c2", "C2");
-    tuple.set(0, m1);
-
-    // m2:map(map(int))
-    m31.put("m321", 321);
-    m31.put("m322", 322);
-    m31.put("m323", 323);
-    m2.put("z", m31);
-    tuple.set(1, m2);
-
-    // m4:map(map(record(f1:int,f2:string)))
-    record1.set(0, 21);
-    record1.set(1, "record row 2.1");
-    m51.put("ma4", (Tuple) record1);
-    m52.put("ma41", (Tuple) record1);
-    m53.put("ma43", (Tuple) record1);
-
-    record2.set(0, 22);
-    record2.set(1, "record row 2.2");
-    m51.put("mb4", (Tuple) record2);
-    m52.put("mb42", (Tuple) record2);
-    m53.put("ma43", (Tuple) record2);
-
-    record3.set(0, 33);
-    record3.set(1, "record row 3.3");
-    m51.put("mc4", (Tuple) record3);
-    m52.put("mc42", (Tuple) record3);
-    m53.put("ma43", (Tuple) record3);
-
-    m4.put("a4", m51);
-    m4.put("b4", m52);
-
-    m4.put("ma43", m53);
-
-    tuple.set(2, m4);
-
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // finish building table, closing out the inserter, writer, writer1
-    inserter.close();
-    writer1.finish();
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-    BasicTable.drop(pathTable, conf);
-  }
-
-  // read one map
-//  @Test
-  public void testReadSimpleMap() throws IOException, ParseException {
-
-    String query = "records = LOAD '" + pathTable.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('m1');";
-    System.out.println(query);
-    String prune = "pruneR = foreach records generate m1#'a';";    
-    pigServer.registerQuery(query);
-    pigServer.registerQuery(prune);
-    Iterator<Tuple> it = pigServer.openIterator("pruneR");
-    int row = 0;
-    while (it.hasNext()) {
-      Tuple RowValue = it.next();
-      System.out.println(RowValue);
-      row++;
-      if (row == 1) {
-        Assert.assertEquals("A", RowValue.get(0).toString());
-      }
-      if (row == 2) {
-        Assert.assertEquals("A2", RowValue.get(0).toString());
-      }
-    }
-  }
-
-  @Test
-  public void testReadMapOfMap() throws IOException, ParseException {
-    String query = "records = LOAD '"
-        + pathTable.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('m1, m2');";
-    System.out.println(query);
-    String prune = "pruneR = foreach records generate m1#'b', m2#'x', m2#'z';";    
-    pigServer.registerQuery(query);
-    pigServer.registerQuery(prune);
-    Iterator<Tuple> it = pigServer.openIterator("pruneR");
-    BytesWritable key = new BytesWritable();
-    int row = 0;
-    while (it.hasNext()) {
-      Tuple RowValue = it.next();
-      System.out.println(RowValue);
-      row++;
-      if (row == 1) {
-        Assert.assertEquals("B", RowValue.get(0));
-        Assert.assertEquals(321, ((Map) RowValue.get(1)).get("m321"));
-        Assert.assertEquals(311, ((Map) RowValue.get(1)).get("m311"));
-        Assert.assertEquals(331, ((Map) RowValue.get(1)).get("m331"));
-        Assert.assertEquals(null, ((Map) RowValue.get(1)).get("m341"));
-        Assert.assertEquals(null, ((Map) ((Map) RowValue.get(1)).get("z")));
-      }
-      if (row == 2) {
-        Assert.assertEquals(null, RowValue.get(0));
-        Assert.assertEquals(323, ((Map) RowValue.get(2)).get("m323"));
-        Assert.assertEquals(322, ((Map) RowValue.get(2)).get("m322"));
-        Assert.assertEquals(321, ((Map) RowValue.get(2)).get("m321"));
-
-      }
-    }
-
-  }
-
-//  @Test
-  public void testReadMapOfRecord1() throws IOException, ParseException {
-    String query = "records = LOAD '"
-        + pathTable.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('m1#{b}, m4#{a4|c4}');";
-    System.out.println(query);
-    pigServer.registerQuery(query);
-    Iterator<Tuple> it = pigServer.openIterator("records");
-    BytesWritable key = new BytesWritable();
-    int row = 0;
-    while (it.hasNext()) {
-      Tuple RowValue = it.next();
-      System.out.println(RowValue);
-      row++;
-      if (row == 1) {
-        Assert.assertEquals("B", ((Map) RowValue.get(0)).get("b"));
-        Assert.assertEquals(11, ((Tuple) ((Map) ((Map) RowValue.get(1))
-            .get("a4")).get("ma4")).get(0));
-        Assert.assertEquals("record row 1.1", ((Tuple) ((Map) ((Map) RowValue
-            .get(1)).get("a4")).get("ma4")).get(1));
-        Assert.assertEquals(12, ((Tuple) ((Map) ((Map) RowValue.get(1))
-            .get("a4")).get("mb4")).get(0));
-        Assert.assertEquals("record row 1.2", ((Tuple) ((Map) ((Map) RowValue
-            .get(1)).get("a4")).get("mb4")).get(1));
-        Assert.assertEquals(13, ((Tuple) ((Map) ((Map) RowValue.get(1))
-            .get("a4")).get("mc4")).get(0));
-        Assert.assertEquals("record row 1.3", ((Tuple) ((Map) ((Map) RowValue
-            .get(1)).get("a4")).get("mc4")).get(1));
-
-        Assert.assertEquals(13, ((Tuple) ((Map) ((Map) RowValue.get(1))
-            .get("c4")).get("ma43")).get(0));
-        Assert.assertEquals("record row 1.3", ((Tuple) ((Map) ((Map) RowValue
-            .get(1)).get("c4")).get("ma43")).get(1));
-
-        Assert.assertEquals(null, (((Map) ((Map) RowValue.get(1)).get("c4"))
-            .get("mc4")));
-        Assert.assertEquals(null, (((Map) ((Map) RowValue.get(1)).get("c4"))
-            .get("mb4")));
-
-      }
-      if (row == 2) {
-        Assert.assertEquals(21, ((Tuple) ((Map) ((Map) RowValue.get(1))
-            .get("a4")).get("ma4")).get(0));
-        Assert.assertEquals("record row 2.1", ((Tuple) ((Map) ((Map) RowValue
-            .get(1)).get("a4")).get("ma4")).get(1));
-        Assert.assertEquals(22, ((Tuple) ((Map) ((Map) RowValue.get(1))
-            .get("a4")).get("mb4")).get(0));
-        Assert.assertEquals("record row 2.2", ((Tuple) ((Map) ((Map) RowValue
-            .get(1)).get("a4")).get("mb4")).get(1));
-        Assert.assertEquals(33, ((Tuple) ((Map) ((Map) RowValue.get(1))
-            .get("a4")).get("mc4")).get(0));
-        Assert.assertEquals("record row 3.3", ((Tuple) ((Map) ((Map) RowValue
-            .get(1)).get("a4")).get("mc4")).get(1));
-
-      }
-    }
-
-  }
-
-//  @Test
-  // Positive? map object column through pig loader
-  public void testRead4() throws IOException, ParseException {
-    String query = "records = LOAD '" + pathTable.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('m1');";
-
-    pigServer.registerQuery(query);
-    Iterator<Tuple> it = pigServer.openIterator("records");
-    int row = 0;
-    while (it.hasNext()) {
-      Tuple RowValue = it.next();
-      System.out.println(RowValue);
-      row++;
-      if (row == 1) {
-        Assert.assertEquals("A", ((Map) RowValue.get(0)).get("a"));
-        Assert.assertEquals("B", ((Map) RowValue.get(0)).get("b"));
-        Assert.assertEquals("C", ((Map) RowValue.get(0)).get("c"));
-        Assert.assertEquals(null, ((Map) RowValue.get(0)).get("non-existent"));
-        Assert.assertEquals(null, ((Map) RowValue.get(0)).get("b2"));
-        Assert.assertEquals(null, ((Map) RowValue.get(0)).get("c2"));
-      }
-      if (row == 2) {
-        Assert.assertEquals("A2", ((Map) RowValue.get(0)).get("a"));
-        Assert.assertEquals("B2", ((Map) RowValue.get(0)).get("b2"));
-        Assert.assertEquals("C2", ((Map) RowValue.get(0)).get("c2"));
-        Assert.assertEquals(null, ((Map) RowValue.get(0)).get("non-existent"));
-        Assert.assertEquals(null, ((Map) RowValue.get(0)).get("b"));
-        Assert.assertEquals(null, ((Map) RowValue.get(0)).get("c"));
-      }
-    }
-    Assert.assertEquals(2, row);
-  }
-
-//  @Test
-  // Negative non-exist column through pig loader
-  public void testReadNeg1() throws IOException, ParseException {
-    String query = "records = LOAD '" + pathTable.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('m5');";
-
-    pigServer.registerQuery(query);
-    Iterator<Tuple> it = pigServer.openIterator("records");
-    int row = 0;
-    while (it.hasNext()) {
-      Tuple RowValue = it.next();
-      System.out.println(RowValue);
-      row++;
-      if (row == 1) {
-        Assert.assertEquals(null, RowValue.get(0));
-        Assert.assertEquals(1, RowValue.size());
-      }
-    }
-    Assert.assertEquals(2, row);
-  }
-
-//  @Test
-  // non-existent column name through I/O
-  public void testNeg2() throws IOException, ParseException {
-    String projection = new String("m5");
-    BasicTable.Reader reader = new BasicTable.Reader(pathTable, conf);
-    reader.setProjection(projection);
-
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k11".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(null, RowValue.get(0));
-    Assert.assertEquals(1, RowValue.size());
-
-    scanner.advance();
-    scanner.getKey(key);
-    Assert.assertEquals(key, new BytesWritable("k12".getBytes()));
-    scanner.getValue(RowValue);
-    Assert.assertEquals(null, RowValue.get(0));
-    Assert.assertEquals(1, RowValue.size());
-    reader.close();
-  }
-}
\ No newline at end of file
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMergeJoin.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMergeJoin.java
deleted file mode 100644
index 33b4820dc..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMergeJoin.java
+++ /dev/null
@@ -1,589 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-import junit.framework.Assert;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.pig.TableStorer;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.executionengine.ExecJob;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-public class TestMergeJoin extends BaseTestCase {
-  final static int numsBatch = 4;
-  final static int numsInserters = 1;
-  static Path pathTable1;
-  static Path pathTable2;
-  final static String STR_SCHEMA1 = "a:int,b:float,c:long,d:double,e:string,f:bytes,r1:record(f1:string, f2:string),m1:map(string)";
-  final static String STR_SCHEMA2 = "m1:map(string),r1:record(f1:string, f2:string),f:bytes,e:string,d:double,c:long,b:float,a:int";
-
-  final static String STR_STORAGE1 = "[a, b, c]; [e, f]; [r1.f1]; [m1#{a}]";
-  final static String STR_STORAGE2 = "[a];[b]; [c]; [e]; [f]; [r1.f1]; [m1#{a}]";
-  static int t1 =0;
-
-  @BeforeClass
-  public static void setUp() throws Exception {
-    init();
-    
-    pathTable1 = getTableFullPath("TestMergeJoin1");
-    pathTable2 = getTableFullPath("TestMergeJoin2");    
-    removeDir(pathTable1);
-    removeDir(pathTable2);
-
-    createFirstTable();
-    createSecondTable();
-  }
-  
-  public static void createFirstTable() throws IOException, ParseException {
-    BasicTable.Writer writer = new BasicTable.Writer(pathTable1, STR_SCHEMA1,
-        STR_STORAGE1, conf);
-    Schema schema = writer.getSchema();
-    //System.out.println("typeName" + schema.getColumn("a").type.pigDataType());
-    Tuple tuple = TypesUtils.createTuple(schema);
-  
-    TableInserter[] inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-    Tuple tupRecord1;
-    try {
-      tupRecord1 = TypesUtils.createTuple(schema.getColumnSchema("r1")
-          .getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-    Map<String, String> m1 = new HashMap<String, String>();
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tupRecord1);
-        TypesUtils.resetTuple(tuple);
-        m1.clear();
-  
-        try {
-          // first row of the table , the biggest row
-          if (i == 0 && b == 0) {
-            tuple.set(0, 100);
-            tuple.set(1, 100.1f);
-            tuple.set(2, 100L);
-            tuple.set(3, 50e+2);
-            tuple.set(4, "something");
-            tuple.set(5, new DataByteArray("something"));
-  
-          }
-          // the middle + 1 row of the table, the smallest row
-          else if (i == 0 && b == (numsBatch / 2)) {
-            tuple.set(0, -100);
-            tuple.set(1, -100.1f);
-            tuple.set(2, -100L);
-            tuple.set(3, -50e+2);
-            tuple.set(4, "so");
-            tuple.set(5, new DataByteArray("so"));
-  
-          }
-  
-          else {
-            Float f = 1.1f;
-            long l = 11;
-            double d = 1.1;
-            tuple.set(0, b);
-            tuple.set(1, f);
-            tuple.set(2, l);
-            tuple.set(3, d);
-            tuple.set(4, "some");
-            tuple.set(5, new DataByteArray("some"));
-          }
-  
-          // insert record
-          tupRecord1.set(0, "" + b);
-          tupRecord1.set(1, "" + b);
-          tuple.set(6, tupRecord1);
-  
-          // insert map
-          m1.put("a", "" + b);
-          m1.put("b", "" + b);
-          m1.put("c", "" + b);
-          tuple.set(7, m1);
-  
-        } catch (ExecException e) {
-          e.printStackTrace();
-        }
-  
-       inserters[i].insert(new BytesWritable(("key_" + b).getBytes()), tuple);
-       
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-    writer.close();
-    
-  
-    //check table is setup correctly
-    String projection = new String("a,b,c,d,e,f,r1,m1");
-    
-    BasicTable.Reader reader = new BasicTable.Reader(pathTable1, conf);
-    reader.setProjection(projection);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-  
-    scanner.getValue(RowValue);
-    System.out.println("rowvalue size:"+RowValue.size());
-    System.out.println("read a : " + RowValue.get(0).toString());
-    System.out.println("read string: " + RowValue.get(1).toString());
-  
-    scanner.advance();
-    scanner.getValue(RowValue);
-    System.out.println("read float in 2nd row: "+ RowValue.get(1).toString());
-    System.out.println("done insert table");
-  
-    reader.close();
-    
-  }
-  public static void createSecondTable() throws IOException, ParseException {
-    BasicTable.Writer writer = new BasicTable.Writer(pathTable2, STR_SCHEMA2,
-        STR_STORAGE2, conf);
-    Schema schema = writer.getSchema();
-    //System.out.println("typeName" + schema.getColumn("a").type.pigDataType());
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    TableInserter[] inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-    Tuple tupRecord1;
-    try {
-      tupRecord1 = TypesUtils.createTuple(schema.getColumnSchema("r1")
-          .getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-    Map<String, String> m1 = new HashMap<String, String>();
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tupRecord1);
-        TypesUtils.resetTuple(tuple);
-        m1.clear();
-
-        try {
-           // first row of the table , the biggest row
-          if (i == 0 && b == 0) {
-            tuple.set(7, 100);
-            tuple.set(6, 100.1f);
-            tuple.set(5, 100L);
-            tuple.set(4, 50e+2);
-            tuple.set(3, "something");
-            tuple.set(2, new DataByteArray("something"));
-
-          }
-          // the middle +1 row of the table, the smallest row
-          else if (i == 0 && b == (numsBatch / 2)) {
-            tuple.set(7, -100);
-            tuple.set(6, -100.1f);
-            tuple.set(5, -100L);
-            tuple.set(4, -50e+2);
-            tuple.set(3, "so");
-            tuple.set(2, new DataByteArray("so"));
-
-          }
-
-          else {
-            Float f = 2.1f;
-            long l = 12;
-            double d = 2.1;
-            tuple.set(7, b*2);
-            tuple.set(6, f);
-            tuple.set(5, l);
-            tuple.set(4, d);
-            tuple.set(3, "somee");
-            tuple.set(2, new DataByteArray("somee"));
-          }
-
-          // insert record
-          tupRecord1.set(0, "" + b);
-          tupRecord1.set(1, "" + b);
-          tuple.set(1, tupRecord1);
-
-          // insert map
-
-          m1.put("a", "" + b);
-          m1.put("b", "" + b);
-          m1.put("c", "" + b);
-          tuple.set(0, m1);
-
-        } catch (ExecException e) {
-          e.printStackTrace();
-        }
-
-        inserters[i].insert(new BytesWritable(("key" + b).getBytes()), tuple);
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-    writer.close();
-    
-    //check table is setup correctly
-    String projection = new String("a,b,c,d,e,f,r1,m1");
-    
-    BasicTable.Reader reader = new BasicTable.Reader(pathTable2, conf);
-    reader.setProjection(projection);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getValue(RowValue);
-    System.out.println("rowvalue size:"+RowValue.size());
-    System.out.println("read a : " + RowValue.get(7).toString());
-    System.out.println("read string: " + RowValue.get(6).toString());
-   
-    scanner.advance();
-    scanner.getValue(RowValue);
-    System.out.println("read float in 2nd row: "+ RowValue.get(6).toString());
-    System.out.println("done insert table");
-
-
-    reader.close();
-    
-  }
-  
-  public static void sortTable(Path tablePath, String sortkey){
-    
-  }
-  @AfterClass
-  public static void tearDown() throws Exception {
-    pigServer.shutdown();
-    BasicTable.drop(pathTable1, conf);
-    BasicTable.drop(pathTable2, conf);
-    
-  }
-  
-  public void verify(Iterator<Tuple> it3) throws ExecException {
-    int row = 0;
-    Tuple RowValue3 = null;
-    while (it3.hasNext()) {
-      RowValue3 = it3.next();
-      Assert.assertEquals(16, RowValue3.size());
-      row++;
-
-      if (row == 1) {
-        // smallest row,   the middle row of original table
-        Assert.assertEquals(-100, RowValue3.get(0));// a
-        Assert.assertEquals(-100.1f, RowValue3.get(1)); // b
-        Assert.assertEquals(-100L, RowValue3.get(2)); // c
-        Assert.assertEquals(-5000.0, RowValue3.get(3)); // d
-        Assert.assertEquals("so", RowValue3.get(4)); // e
-        Assert.assertEquals("so", RowValue3.get(5).toString());// f
-        Assert.assertEquals("" + numsBatch / 2, ((Tuple) RowValue3.get(6))
-            .get(0));// r
-        Assert.assertEquals("" + numsBatch / 2, ((Tuple) RowValue3.get(6))
-            .get(1));// r
-        Assert.assertEquals("" + numsBatch / 2, ((Map) RowValue3.get(7))
-            .get("a"));// m
-        Assert.assertEquals("" + numsBatch / 2, ((Map) RowValue3.get(7))
-            .get("b"));// m
-        Assert.assertEquals("" + numsBatch / 2, ((Map) RowValue3.get(7))
-            .get("c"));// m
-        Assert.assertEquals(-100, RowValue3.get(15)); // a
-        Assert.assertEquals(-100.1f, RowValue3.get(14)); // b
-        Assert.assertEquals(-100L, RowValue3.get(13)); // c
-        Assert.assertEquals(-5000.0, RowValue3.get(12)); // d
-        Assert.assertEquals("so", RowValue3.get(11)); // e
-        Assert.assertEquals("so", RowValue3.get(10).toString());// f
-        Assert.assertEquals("" + numsBatch / 2, ((Tuple) RowValue3.get(9))
-            .get(0));// r
-        Assert.assertEquals("" + numsBatch / 2, ((Tuple) RowValue3.get(9))
-            .get(1));// r
-        Assert.assertEquals("" + numsBatch / 2, ((Map) RowValue3.get(8))
-            .get("a"));// m
-        Assert.assertEquals("" + numsBatch / 2, ((Map) RowValue3.get(8))
-            .get("b"));// m
-        Assert.assertEquals("" + numsBatch / 2, ((Map) RowValue3.get(8))
-            .get("c"));// m
-      }
-      
-      // largest row, the first row of the original table
-      if (row == 2) {
-        Assert.assertEquals(100, RowValue3.get(0));// a
-        Assert.assertEquals(100.1f, RowValue3.get(1)); // b
-        Assert.assertEquals(100L, RowValue3.get(2)); // c
-        Assert.assertEquals(5000.0, RowValue3.get(3)); // d
-        Assert.assertEquals("something", RowValue3.get(4)); // e
-        Assert.assertEquals("something", RowValue3.get(5).toString());// f
-        Assert.assertEquals("" + 0, ((Tuple) RowValue3.get(6))
-            .get(0));// r
-        Assert.assertEquals("" + 0, ((Tuple) RowValue3.get(6))
-            .get(1));// r
-        Assert.assertEquals("" + 0, ((Map) RowValue3.get(7))
-            .get("a"));// m
-        Assert.assertEquals("" + 0, ((Map) RowValue3.get(7))
-            .get("b"));// m
-        Assert.assertEquals("" + 0, ((Map) RowValue3.get(7))
-            .get("c"));// m
-        Assert.assertEquals(100, RowValue3.get(15)); // a
-        Assert.assertEquals(100.1f, RowValue3.get(14)); // b
-        Assert.assertEquals(100L, RowValue3.get(13)); // c
-        Assert.assertEquals(5000.0, RowValue3.get(12)); // d
-        Assert.assertEquals("something", RowValue3.get(11)); // e
-        Assert.assertEquals("something", RowValue3.get(10).toString());// f
-        Assert.assertEquals("" + 0, ((Tuple) RowValue3.get(9)).get(0));// r
-        Assert.assertEquals("" + 0, ((Tuple) RowValue3.get(9)).get(1));// r
-        Assert.assertEquals("" + 0, ((Map) RowValue3.get(8)).get("a"));// m
-        Assert.assertEquals("" + 0, ((Map) RowValue3.get(8)).get("b"));// m
-        Assert.assertEquals("" + 0, ((Map) RowValue3.get(8)).get("c"));// m
-      }
-    }
-    Assert.assertEquals(2, row);
-  }
-
-  public Iterator<Tuple> joinTable(String table1, String table2, String sortkey1, String sortkey2) throws IOException {
-    String query1 = "records1 = LOAD '" + this.pathTable1.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    System.out.println("query1:" + query1);
-    pigServer.registerQuery(query1);
-
-    String query2 = "records2 = LOAD '" + this.pathTable2.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    System.out.println("query2:" + query2);
-    pigServer.registerQuery(query2);
-
-   /* Iterator<Tuple> it_before_order = pigServer.openIterator("records");
-    int row_before_order = 0;
-    Tuple RowValue_before_order = null;
-    while (it_before_order.hasNext()) {
-      RowValue_before_order = it_before_order.next();
-      row_before_order++;
-      System.out.println("row : " + row_before_order + " field f value: "
-          + RowValue_before_order.get(5));
-    }
-    System.out.println("total row for orig table before ordered:"
-        + row_before_order);*/
-    String orderby1 = "sort1 = ORDER records1 BY " + sortkey1 + " ;";
-    String orderby2 = "sort2 = ORDER records2 BY " + sortkey2 + " ;";
-    pigServer.registerQuery(orderby1);
-    pigServer.registerQuery(orderby2);
-
-    /*Iterator<Tuple> it_after_order = pigServer.openIterator("srecs");
-    int row_after_order = 0;
-    Tuple RowValue_after_order = null;
-    while (it_after_order.hasNext()) {
-      RowValue_after_order = it_after_order.next();
-      row_after_order++;
-      System.out.println("row : " + row_after_order + " field b value: "
-          + RowValue_after_order.get(1));
-    }
-    System.out.println("total row for orig table after ordered:"
-        + row_after_order);*/
-    // Path newPath = new Path(getCurrentMethodName());
-
-    /*
-     * Table1 creation
-     */
-    this.t1++;
-    
-    String table1path = this.pathTable1.toString() + Integer.toString(this.t1);
-    removeDir(new Path(table1path));
-    ExecJob pigJob = pigServer.store("sort1", table1path, TableStorer.class.getCanonicalName()
-        + "('[a, b, c]; [d, e, f, r1, m1]')");
-
-    Assert.assertNull(pigJob.getException());
-
-    String query3 = "records1 = LOAD '"
-        + table1path
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('a, b, c, d, e, f, r1, m1', 'sorted');";
-
-    System.out.println("query3:" + query3);
-    pigServer.registerQuery(query3);   
-    
-    String foreach = "records11 = foreach records1 generate a as a, b as b, c as c, d as d, e as e, f as f, r1 as r1, m1 as m1;";
-    pigServer.registerQuery(foreach);
-//    System.out.println( "Left table: >>>>>.");
-//    printTuples(pigServer.openIterator("records11"));
-   /* Iterator<Tuple> it_ordered = pigServer.openIterator("records1");
-    int row_ordered = 0;
-    Tuple RowValue_ordered = null;
-    while (it_ordered.hasNext()) {
-      RowValue_ordered = it_ordered.next();
-      row_ordered++;
-      System.out.println("row : " + row_ordered + " field a value: "
-          + RowValue_ordered.get(0));
-    }
-    System.out.println("total row for table 1 after ordered:" + row_ordered);*/
-
-    /*
-     * Table2 creation
-     */
-    this.t1++;
-    String table2path = this.pathTable2.toString() + Integer.toString(this.t1);
-    removeDir(new Path(table2path));
-    pigJob = pigServer.store("sort2", table2path, TableStorer.class.getCanonicalName()
-        + "('[a, b, c]; [d,e,f,r1,m1]')");
-    if (pigJob.getException() != null){
-      System.out.println("******pig job exception"+ pigJob.getException().getMessage());
-    }
-    Assert.assertNull(pigJob.getException());
-    String query4 = "records2 = LOAD '" + table2path
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    pigServer.registerQuery(query4);
-
-//    System.out.println( "Right table: >>>>>.");
-//    printTuples(pigServer.openIterator("records2"));
-    
-    String filter = "records22 = FILTER records2 BY a == '1.9';";
-    pigServer.registerQuery(filter);
-    /*Iterator<Tuple> it_ordered2 = pigServer.openIterator("records2");
-    int row_ordered2 = 0;
-    Tuple RowValue_ordered2 = null;
-    while (it_ordered2.hasNext()) {
-      RowValue_ordered2 = it_ordered2.next();
-      row_ordered2++;
-      System.out.println("row for table 2 after ordereed: " + row_ordered2
-          + " field a value: " + RowValue_ordered2.get(0));
-    }
-
-    System.out.println("total row for table 2:" + row_ordered2);
-    */
-    String join = "joinRecords = JOIN records11 BY " + "(" + sortkey1 + ")" 
-        + " , records2 BY " + "("+ sortkey2 + ")"+" USING \"merge\";";
-   //TODO: can not use records22
-      pigServer.registerQuery(join);
-   
-    // check JOIN content
-    Iterator<Tuple> it3 = pigServer.openIterator("joinRecords");
-    return it3;
-  }
-  
-  //@Test
-  public void test1() throws ExecException, IOException {
-    /*
-     * join key: single integer column
-     */
-    Iterator<Tuple> it3 =  joinTable(this.pathTable1.toString(), this.pathTable2.toString(), "a","a" );
-    verify (it3);
-  }
-
-  //@Test
-  public void test2() throws ExecException, IOException {
-    /*
-     * join key: single float column
-     */
-    Iterator<Tuple> it3 = joinTable(this.pathTable1.toString(), this.pathTable2.toString(), "b","b" ); 
-    verify(it3);
-  }
-
-  @Test
-  public void test3() throws ExecException, IOException {
-    /*
-     * join key: single string column
-     */
-    Iterator<Tuple> it3 = joinTable(this.pathTable1.toString(), this.pathTable2.toString(), "e","e" );
-    verify(it3);
-    //printTuple( it3 );
-  }
-
-  @Test
-  public void test4() throws ExecException, IOException {
-    /*
-     * join key: single byte column
-     */
-    Iterator<Tuple> it3 = joinTable(this.pathTable1.toString(), this.pathTable2.toString(), "f","f" );
-    verify(it3);
-  }
-  
-  //@Test
-  public void test5() throws ExecException, IOException {
-    /*
-     * join key: single double column
-     */
-    Iterator<Tuple> it3 = joinTable(this.pathTable1.toString(), this.pathTable2.toString(), "d","d" );
-    verify(it3);
-  }
-
-  //@Test
-  public void test6() throws ExecException, IOException {
-    /*
-     * join key: single long column
-     */
-    Iterator<Tuple> it3 = joinTable(this.pathTable1.toString(), this.pathTable2.toString(), "c","c" );
-    verify(it3);
-  }
-  
-  //@Test
-  public void test7() throws ExecException, IOException {
-    /*
-     *  2 join keys: integer and float
-     */
-    System.out.println ("helloo");
-    Iterator<Tuple> it3 = joinTable(this.pathTable1.toString(), this.pathTable2.toString(), "a,b","a,b" );
-    verify(it3);
-  }
-  
-  //@Test
-  public void test8() throws ExecException, IOException {
-    /*
-     * multiple join keys: integer, float, long, double, string, bytes
-     */
-	  
-	// Failing with bytes (known bug)
-    Iterator<Tuple> it3 = joinTable(this.pathTable1.toString(), this.pathTable2.toString(), "a,b,c,d,e,f","a,b,c,d,e,f" );
-    verify(it3);
-  }
-  
-  //@Test(expected = IOException.class)
-  public void test9a() throws ExecException, IOException {
-    /*
-     * Negative test case, one join key is not primitive type which is a record
-     * 2 join keys: integer and record
-     */
-    Iterator<Tuple> it3 = joinTable(this.pathTable1.toString(), this.pathTable2.toString(), "a,r1,e","a,r1,e");
-  }
-  
-  //@Test(expected = IOException.class)
-  public void test9b() throws ExecException, IOException {
-    /*
-     * Negative test case, one join key is not primitive type which is a record
-     * 2 join keys: integer and map
-     */
-    Iterator<Tuple> it3 = joinTable(this.pathTable1.toString(), this.pathTable2.toString(), "a,m1,e","a,m1,e");
-  }
-  
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMergeJoin2.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMergeJoin2.java
deleted file mode 100644
index 30e82fcb8..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMergeJoin2.java
+++ /dev/null
@@ -1,574 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import junit.framework.Assert;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-
-public class TestMergeJoin2 extends BaseTestCase
-{
-  final static int numsBatch = 4;
-  final static int numsInserters = 1;
-
-  static Path pathTable1;
-  static Path pathTable2;
-  final static String STR_SCHEMA1 = "a:int,b:float,c:long,d:double,e:string,f:bytes,r1:record(f1:string, f2:string),m1:map(string)";
-  final static String STR_SCHEMA2 = "m1:map(string),r1:record(f1:string, f2:string),f:bytes,e:string,d:double,c:long,b:float,a:int";
-
-  final static String STR_STORAGE1 = "[a, b, c]; [e, f]; [r1.f1]; [m1#{a}]";
-  final static String STR_STORAGE2 = "[a];[b]; [c]; [e]; [f]; [r1.f1]; [m1#{a}]";
-  static int t1 =0;
-
-
-  @BeforeClass
-  public static void setUp() throws Exception {
-    init();
-    pathTable1 = getTableFullPath("TestMergeJoin1") ;
-    pathTable2 = getTableFullPath("TestMergeJoin2") ;
-    removeDir(pathTable1);
-    removeDir(pathTable2);
-    createFirstTable();
-    createSecondTable();
-  }
-  
-  public static void createFirstTable() throws IOException, ParseException {
-    BasicTable.Writer writer = new BasicTable.Writer(pathTable1, STR_SCHEMA1,
-        STR_STORAGE1, conf);
-    Schema schema = writer.getSchema();
-    //System.out.println("typeName" + schema.getColumn("a").type.pigDataType());
-    Tuple tuple = TypesUtils.createTuple(schema);
-    TableInserter[] inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-    Tuple tupRecord1;
-    try {
-      tupRecord1 = TypesUtils.createTuple(schema.getColumnSchema("r1")
-          .getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-    Map<String, String> m1 = new HashMap<String, String>();
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tupRecord1);
-        TypesUtils.resetTuple(tuple);
-        m1.clear();
-        try {
-          // first row of the table , the biggest row
-          if (i == 0 && b == 0) {
-            tuple.set(0, 100);
-            tuple.set(1, 100.1f);
-            tuple.set(2, 100L);
-            tuple.set(3, 50e+2);
-            tuple.set(4, "something2");
-            tuple.set(5, new DataByteArray("something2"));
-          }
-          // the middle + 1 row of the table, the smallest row
-          else if (i == 0 && b == (numsBatch / 2)) {
-            tuple.set(0, -100);
-            tuple.set(1, -100.1f);
-            tuple.set(2, -100L);
-            tuple.set(3, -50e+2);
-            tuple.set(4, "something1");
-            tuple.set(5, new DataByteArray("something1"));
-          }
-          else {
-            Float f = 1.1f;
-            long l = 11;
-            double d = 1.1;
-            tuple.set(0, b);
-            tuple.set(1, f);
-            tuple.set(2, l);
-            tuple.set(3, d);
-            tuple.set(4, "something1");
-            tuple.set(5, new DataByteArray("something1"));
-          }
-          // insert record
-          tupRecord1.set(0, "" + b);
-          tupRecord1.set(1, "" + b);
-          tuple.set(6, tupRecord1);
-
-          // insert map
-          m1.put("a", "" + b);
-          m1.put("b", "" + b);
-          m1.put("c", "" + b);
-          tuple.set(7, m1);
-
-        } catch (ExecException e) {
-          e.printStackTrace();
-        }
-       inserters[i].insert(new BytesWritable(("key_" + b).getBytes()), tuple);
-
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-    writer.close();
-
-
-    //check table is setup correctly
-    String projection = new String("a,b,c,d,e,f,r1,m1");
-
-    BasicTable.Reader reader = new BasicTable.Reader(pathTable1, conf);
-    reader.setProjection(projection);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getValue(RowValue);
-    System.out.println("rowvalue size:"+RowValue.size());
-    System.out.println("read a : " + RowValue.get(0).toString());
-    System.out.println("read string: " + RowValue.get(1).toString());
-
-    scanner.advance();
-    scanner.getValue(RowValue);
-    System.out.println("read float in 2nd row: "+ RowValue.get(1).toString());
-    System.out.println("done insert table");
-
-    reader.close();
-
-  }
-  public static void createSecondTable() throws IOException, ParseException {
-    BasicTable.Writer writer = new BasicTable.Writer(pathTable2, STR_SCHEMA2,
-        STR_STORAGE2, conf);
-    Schema schema = writer.getSchema();
-    //System.out.println("typeName" + schema.getColumn("a").type.pigDataType());
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    TableInserter[] inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-    Tuple tupRecord1;
-    try {
-      tupRecord1 = TypesUtils.createTuple(schema.getColumnSchema("r1")
-          .getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-    Map<String, String> m1 = new HashMap<String, String>();
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tupRecord1);
-        TypesUtils.resetTuple(tuple);
-        m1.clear();
-
-        try {
-           // first row of the table , the biggest row
-          if (i == 0 && b == 0) {
-            tuple.set(7, 100);
-            tuple.set(6, 100.1f);
-            tuple.set(5, 100L);
-            tuple.set(4, 50e+2);
-            tuple.set(3, "something");
-            tuple.set(2, new DataByteArray("something"));
-
-          }
-          // the middle +1 row of the table, the smallest row
-          else if (i == 0 && b == (numsBatch / 2)) {
-            tuple.set(7, -100);
-            tuple.set(6, -100.1f);
-            tuple.set(5, -100L);
-            tuple.set(4, -50e+2);
-            tuple.set(3, "so");
-            tuple.set(2, new DataByteArray("so"));
-
-          }
-
-          else {
-            Float f = 2.1f;
-            long l = 12;
-            double d = 2.1;
-            tuple.set(7, b*2);
-            tuple.set(6, f);
-            tuple.set(5, l);
-            tuple.set(4, d);
-            tuple.set(3, "somee");
-            tuple.set(2, new DataByteArray("somee"));
-          }
-
-          // insert record
-          tupRecord1.set(0, "" + b);
-          tupRecord1.set(1, "" + b);
-          tuple.set(1, tupRecord1);
-
-          // insert map
-
-          m1.put("a", "" + b);
-          m1.put("b", "" + b);
-          m1.put("c", "" + b);
-          tuple.set(0, m1);
-
-        } catch (ExecException e) {
-          e.printStackTrace();
-        }
-
-        inserters[i].insert(new BytesWritable(("key" + b).getBytes()), tuple);
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-    writer.close();
-
-
-
-
-    //check table is setup correctly
-    String projection = new String("a,b,c,d,e,f,r1,m1");
-
-    BasicTable.Reader reader = new BasicTable.Reader(pathTable2, conf);
-    reader.setProjection(projection);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getValue(RowValue);
-    System.out.println("rowvalue size:"+RowValue.size());
-    System.out.println("read a : " + RowValue.get(7).toString());
-    System.out.println("read string: " + RowValue.get(6).toString());
-
-    scanner.advance();
-    scanner.getValue(RowValue);
-    System.out.println("read float in 2nd row: "+ RowValue.get(6).toString());
-    System.out.println("done insert table");
-
-
-    reader.close();
-
-  }
-
-  public static void sortTable(Path tablePath, String sortkey){
-
-  }
-  @AfterClass
-  public static void tearDown() throws Exception {
-    pigServer.shutdown();
-  }
-
-
-  public void verify(Iterator<Tuple> it3) throws ExecException {
-    int row = 0;
-    Tuple RowValue3 = null;
-    while (it3.hasNext()) {
-      RowValue3 = it3.next();
-      Assert.assertEquals(16, RowValue3.size());
-      row++;
-
-      if (row == 1) {
-        // smallest row,   the middle row of original table
-        Assert.assertEquals(-100, RowValue3.get(0));// a
-        Assert.assertEquals(-100.1f, RowValue3.get(1)); // b
-        Assert.assertEquals(-100L, RowValue3.get(2)); // c
-        Assert.assertEquals(-5000.0, RowValue3.get(3)); // d
-        Assert.assertEquals("so", RowValue3.get(4)); // e
-        Assert.assertEquals("so", RowValue3.get(5).toString());// f
-        Assert.assertEquals("" + numsBatch / 2, ((Tuple) RowValue3.get(6))
-            .get(0));// r
-        Assert.assertEquals("" + numsBatch / 2, ((Tuple) RowValue3.get(6))
-            .get(1));// r
-        Assert.assertEquals("" + numsBatch / 2, ((Map) RowValue3.get(7))
-            .get("a"));// m
-        Assert.assertEquals("" + numsBatch / 2, ((Map) RowValue3.get(7))
-            .get("b"));// m
-        Assert.assertEquals("" + numsBatch / 2, ((Map) RowValue3.get(7))
-            .get("c"));// m
-        Assert.assertEquals(-100, RowValue3.get(15)); // a
-        Assert.assertEquals(-100.1f, RowValue3.get(14)); // b
-        Assert.assertEquals(-100L, RowValue3.get(13)); // c
-        Assert.assertEquals(-5000.0, RowValue3.get(12)); // d
-        Assert.assertEquals("so", RowValue3.get(11)); // e
-        Assert.assertEquals("so", RowValue3.get(10).toString());// f
-        Assert.assertEquals("" + numsBatch / 2, ((Tuple) RowValue3.get(9))
-            .get(0));// r
-        Assert.assertEquals("" + numsBatch / 2, ((Tuple) RowValue3.get(9))
-            .get(1));// r
-        Assert.assertEquals("" + numsBatch / 2, ((Map) RowValue3.get(8))
-            .get("a"));// m
-        Assert.assertEquals("" + numsBatch / 2, ((Map) RowValue3.get(8))
-            .get("b"));// m
-        Assert.assertEquals("" + numsBatch / 2, ((Map) RowValue3.get(8))
-            .get("c"));// m
-      }
-
-      // largest row, the first row of the original table
-      if (row == 2) {
-        /*
-        Assert.assertEquals(100, RowValue3.get(0));// a
-        Assert.assertEquals(100.1f, RowValue3.get(1)); // b
-        Assert.assertEquals(100L, RowValue3.get(2)); // c
-        Assert.assertEquals(5000.0, RowValue3.get(3)); // d
-        Assert.assertEquals("something", RowValue3.get(4)); // e
-        Assert.assertEquals("something", RowValue3.get(5).toString());// f
-        Assert.assertEquals("" + 0, ((Tuple) RowValue3.get(6))
-            .get(0));// r
-        Assert.assertEquals("" + 0, ((Tuple) RowValue3.get(6))
-            .get(1));// r
-        Assert.assertEquals("" + 0, ((Map) RowValue3.get(7))
-            .get("a"));// m
-        Assert.assertEquals("" + 0, ((Map) RowValue3.get(7))
-            .get("b"));// m
-        Assert.assertEquals("" + 0, ((Map) RowValue3.get(7))
-            .get("c"));// m
-        Assert.assertEquals(100, RowValue3.get(15)); // a
-        Assert.assertEquals(100.1f, RowValue3.get(14)); // b
-        Assert.assertEquals(100L, RowValue3.get(13)); // c
-        Assert.assertEquals(5000.0, RowValue3.get(12)); // d
-        Assert.assertEquals("something", RowValue3.get(11)); // e
-        Assert.assertEquals("something", RowValue3.get(10).toString());// f
-        Assert.assertEquals("" + 0, ((Tuple) RowValue3.get(9)).get(0));// r
-        Assert.assertEquals("" + 0, ((Tuple) RowValue3.get(9)).get(1));// r
-        Assert.assertEquals("" + 0, ((Map) RowValue3.get(8)).get("a"));// m
-        Assert.assertEquals("" + 0, ((Map) RowValue3.get(8)).get("b"));// m
-        Assert.assertEquals("" + 0, ((Map) RowValue3.get(8)).get("c"));// m
-        */
-      }
-    }
-    Assert.assertEquals(0, row);
-  }
-
-  public Iterator<Tuple> joinTable(String table1, String table2, String sortkey1, String sortkey2) throws IOException {
-    String query1 = "records1 = LOAD '" + this.pathTable1.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    System.out.println("query1:" + query1);
-    pigServer.registerQuery(query1);
-
-    String query2 = "records2 = LOAD '" + this.pathTable2.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    System.out.println("query2:" + query2);
-    pigServer.registerQuery(query2);
-
-   /* Iterator<Tuple> it_before_order = pigServer.openIterator("records");
-    int row_before_order = 0;
-    Tuple RowValue_before_order = null;
-    while (it_before_order.hasNext()) {
-      RowValue_before_order = it_before_order.next();
-      row_before_order++;
-      System.out.println("row : " + row_before_order + " field f value: "
-          + RowValue_before_order.get(5));
-    }
-    System.out.println("total row for orig table before ordered:"
-        + row_before_order);*/
-    String orderby1 = "sort1 = ORDER records1 BY " + sortkey1 + " ;";
-    String orderby2 = "sort2 = ORDER records2 BY " + sortkey2 + " ;";
-    pigServer.registerQuery(orderby1);
-    pigServer.registerQuery(orderby2);
-
-    /*Iterator<Tuple> it_after_order = pigServer.openIterator("srecs");
-    int row_after_order = 0;
-    Tuple RowValue_after_order = null;
-    while (it_after_order.hasNext()) {
-      RowValue_after_order = it_after_order.next();
-      row_after_order++;
-      System.out.println("row : " + row_after_order + " field b value: "
-          + RowValue_after_order.get(1));
-    }
-    System.out.println("total row for orig table after ordered:"
-        + row_after_order);*/
-    // Path newPath = new Path(getCurrentMethodName());
-
-    /*
-     * Table1 creation
-     */
-    this.t1++;
-
-    String table1path = this.pathTable1.toString() + Integer.toString(this.t1);
-    removeDir(new Path(table1path));
-    pigServer.store("sort1", table1path, TableStorer.class.getCanonicalName()
-        + "('[a, b, c]; [d, e, f, r1, m1]')");
-
-    String query3 = "records1 = LOAD '"
-        + table1path
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('a, b, c, d, e, f, r1, m1', 'sorted');";
-
-    System.out.println("query3:" + query3);
-    pigServer.registerQuery(query3);
-
-    String foreach = "records11 = foreach records1 generate a as a, b as b, c as c, d as d, e as e, f as f, r1 as r1, m1 as m1;";
-    pigServer.registerQuery(foreach);
-   /* Iterator<Tuple> it_ordered = pigServer.openIterator("records1");
-    int row_ordered = 0;
-    Tuple RowValue_ordered = null;
-    while (it_ordered.hasNext()) {
-      RowValue_ordered = it_ordered.next();
-      row_ordered++;
-      System.out.println("row : " + row_ordered + " field a value: "
-          + RowValue_ordered.get(0));
-    }
-    System.out.println("total row for table 1 after ordered:" + row_ordered);*/
-
-    /*
-     * Table2 creation
-     */
-    this.t1++;
-    String table2path = this.pathTable2.toString() + Integer.toString(this.t1);
-    removeDir(new Path(table2path));
-    pigServer.store("sort2", table2path, TableStorer.class.getCanonicalName()
-        + "('[a, b, c]; [d,e,f,r1,m1]')");
-
-    String query4 = "records2 = LOAD '" + table2path
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    pigServer.registerQuery(query4);
-
-
-    String filter = "records22 = FILTER records2 BY a == '1.9';";
-    pigServer.registerQuery(filter);
-    /*Iterator<Tuple> it_ordered2 = pigServer.openIterator("records2");
-    int row_ordered2 = 0;
-    Tuple RowValue_ordered2 = null;
-    while (it_ordered2.hasNext()) {
-      RowValue_ordered2 = it_ordered2.next();
-      row_ordered2++;
-      System.out.println("row for table 2 after ordereed: " + row_ordered2
-          + " field a value: " + RowValue_ordered2.get(0));
-    }
-
-    System.out.println("total row for table 2:" + row_ordered2);
-    */
-    String join = "joinRecords = JOIN records11 BY " + "(" + sortkey1 + ")"
-        + " , records2 BY " + "("+ sortkey2 + ")"+" USING \"merge\";";
-   //TODO: can not use records22
-      pigServer.registerQuery(join);
-
-    // check JOIN content
-    Iterator<Tuple> it3 = pigServer.openIterator("joinRecords");
-    return it3;
-  }
-
-  //@Test
-  public void test1() throws ExecException, IOException {
-    /*
-     * join key: single integer column
-     */
-    Iterator<Tuple> it3 =  joinTable(this.pathTable1.toString(), this.pathTable2.toString(), "a","a" );
-    verify (it3);
-  }
-
-  //@Test
-  public void test2() throws ExecException, IOException {
-    /*
-     * join key: single float column
-     */
-    Iterator<Tuple> it3 = joinTable(this.pathTable1.toString(), this.pathTable2.toString(), "b","b" );
-    verify(it3);
-  }
-
-  @Test
-  public void test3() throws ExecException, IOException {
-    /*
-     * join key: single string column
-     */
-    Iterator<Tuple> it3 = joinTable(this.pathTable1.toString(), this.pathTable2.toString(), "e","e" );
-    verify(it3);
-  }
-
-  //@Test
-  public void test4() throws ExecException, IOException {
-    /*
-     * join key: single byte column
-     */
-    Iterator<Tuple> it3 = joinTable(this.pathTable1.toString(), this.pathTable2.toString(), "f","f" );
-    verify(it3);
-  }
-
-  //@Test
-  public void test5() throws ExecException, IOException {
-    /*
-     * join key: single double column
-     */
-    Iterator<Tuple> it3 = joinTable(this.pathTable1.toString(), this.pathTable2.toString(), "d","d" );
-    verify(it3);
-  }
-
-  //@Test
-  public void test6() throws ExecException, IOException {
-    /*
-     * join key: single long column
-     */
-    Iterator<Tuple> it3 = joinTable(this.pathTable1.toString(), this.pathTable2.toString(), "c","c" );
-    verify(it3);
-  }
-
-  //@Test
-  public void test7() throws ExecException, IOException {
-    /*
-     *  2 join keys: integer and float
-     */
-    System.out.println ("helloo");
-    Iterator<Tuple> it3 = joinTable(this.pathTable1.toString(), this.pathTable2.toString(), "a,b","a,b" );
-    verify(it3);
-  }
-
-  //@Test
-  public void test8() throws ExecException, IOException {
-    /*
-     * multiple join keys: integer, float, long, double, string, bytes
-     */
-
-	// Failing with bytes (known bug)
-    Iterator<Tuple> it3 = joinTable(this.pathTable1.toString(), this.pathTable2.toString(), "a,b,c,d,e,f","a,b,c,d,e,f" );
-    verify(it3);
-  }
-
-  //@Test(expected = IOException.class)
-  public void test9a() throws ExecException, IOException {
-    /*
-     * Negative test case, one join key is not primitive type which is a record
-     * 2 join keys: integer and record
-     */
-    Iterator<Tuple> it3 = joinTable(this.pathTable1.toString(), this.pathTable2.toString(), "a,r1,e","a,r1,e");
-  }
-
-  //@Test(expected = IOException.class)
-  public void test9b() throws ExecException, IOException {
-    /*
-     * Negative test case, one join key is not primitive type which is a record
-     * 2 join keys: integer and map
-     */
-    Iterator<Tuple> it3 = joinTable(this.pathTable1.toString(), this.pathTable2.toString(), "a,m1,e","a,m1,e");
-  }
-
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMergeJoinEmpty.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMergeJoinEmpty.java
deleted file mode 100644
index ae26d22e7..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMergeJoinEmpty.java
+++ /dev/null
@@ -1,437 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import junit.framework.Assert;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-
-public class TestMergeJoinEmpty extends BaseTestCase
-{
-
-  final static int numsBatch = 4;
-  final static int numsInserters = 1;
-
-  static Path pathTable1;
-  static Path pathTable2;
-  final static String STR_SCHEMA1 = "a:int,b:float,c:long,d:double,e:string,f:bytes,r1:record(f1:string, f2:string),m1:map(string)";
-  final static String STR_SCHEMA2 = "m1:map(string),r1:record(f1:string, f2:string),f:bytes,e:string,d:double,c:long,b:float,a:int";
-
-  final static String STR_STORAGE1 = "[a, b, c]; [e, f]; [r1.f1]; [m1#{a}]";
-  final static String STR_STORAGE2 = "[a];[b]; [c]; [e]; [f]; [r1.f1]; [m1#{a}]";
-  static int t1 =0;
-  
-
-  @BeforeClass
-  public static void setUp() throws Exception {
-    init();
-    pathTable1 = getTableFullPath("TestMergeJoinEmpty1") ;
-    pathTable2 = getTableFullPath("TestMergeJoinEmpty2") ;
-    removeDir(pathTable1);
-    removeDir(pathTable2);
-    createFirstTable();
-    createSecondTable();    
-    
-  }
-  
-  public static void createFirstTable() throws IOException, ParseException {
-    BasicTable.Writer writer = new BasicTable.Writer(pathTable1, STR_SCHEMA1,
-        STR_STORAGE1, conf);
-    Schema schema = writer.getSchema();
-    //System.out.println("typeName" + schema.getColumn("a").type.pigDataType());
-    Tuple tuple = TypesUtils.createTuple(schema);
-  
-    TableInserter[] inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-    Tuple tupRecord1;
-    try {
-      tupRecord1 = TypesUtils.createTuple(schema.getColumnSchema("r1")
-          .getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-    Map<String, String> m1 = new HashMap<String, String>();
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tupRecord1);
-        TypesUtils.resetTuple(tuple);
-        m1.clear();
-  
-        try {
-          // first row of the table , the biggest row
-          if (i == 0 && b == 0) {
-            tuple.set(0, 100);
-            tuple.set(1, 100.1f);
-            tuple.set(2, 100L);
-            tuple.set(3, 50e+2);
-            tuple.set(4, "something");
-            tuple.set(5, new DataByteArray("something"));
-  
-          }
-          // the middle + 1 row of the table, the smallest row
-          else if (i == 0 && b == (numsBatch / 2)) {
-            tuple.set(0, -100);
-            tuple.set(1, -100.1f);
-            tuple.set(2, -100L);
-            tuple.set(3, -50e+2);
-            tuple.set(4, "so");
-            tuple.set(5, new DataByteArray("so"));
-  
-          }
-  
-          else {
-            Float f = 1.1f;
-            long l = 11;
-            double d = 1.1;
-            tuple.set(0, b);
-            tuple.set(1, f);
-            tuple.set(2, l);
-            tuple.set(3, d);
-            tuple.set(4, "some");
-            tuple.set(5, new DataByteArray("some"));
-          }
-  
-          // insert record
-          tupRecord1.set(0, "" + b);
-          tupRecord1.set(1, "" + b);
-          tuple.set(6, tupRecord1);
-  
-          // insert map
-          m1.put("a", "" + b);
-          m1.put("b", "" + b);
-          m1.put("c", "" + b);
-          tuple.set(7, m1);
-  
-        } catch (ExecException e) {
-          e.printStackTrace();
-        }
-  
-       inserters[i].insert(new BytesWritable(("key_" + b).getBytes()), tuple);
-       
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-    writer.close();
-  
-    //check table is setup correctly
-    String projection = new String("a,b,c,d,e,f,r1,m1");
-    
-    BasicTable.Reader reader = new BasicTable.Reader(pathTable1, conf);
-    reader.setProjection(projection);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-  
-    scanner.getValue(RowValue);
-    System.out.println("rowvalue size:"+RowValue.size());
-    System.out.println("read a : " + RowValue.get(0).toString());
-    System.out.println("read string: " + RowValue.get(1).toString());
-  
-    scanner.advance();
-    scanner.getValue(RowValue);
-    System.out.println("read float in 2nd row: "+ RowValue.get(1).toString());
-    System.out.println("done insert table");
-  
-    reader.close();
-    
-  }
-  
-  public static void createSecondTable() throws IOException, ParseException {
-    BasicTable.Writer writer = new BasicTable.Writer(pathTable2, STR_SCHEMA2,
-        STR_STORAGE2, conf);
-    Schema schema = writer.getSchema();
-    //System.out.println("typeName" + schema.getColumn("a").type.pigDataType());
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    TableInserter[] inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-    Tuple tupRecord1;
-    try {
-      tupRecord1 = TypesUtils.createTuple(schema.getColumnSchema("r1")
-          .getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-    Map<String, String> m1 = new HashMap<String, String>();
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tupRecord1);
-        TypesUtils.resetTuple(tuple);
-        m1.clear();
-
-        try {
-           // first row of the table , the biggest row
-          if (i == 0 && b == 0) {
-            tuple.set(7, 100);
-            tuple.set(6, 100.1f);
-            tuple.set(5, 100L);
-            tuple.set(4, 50e+2);
-            tuple.set(3, "something");
-            tuple.set(2, new DataByteArray("something"));
-
-          }
-          // the middle +1 row of the table, the smallest row
-          else if (i == 0 && b == (numsBatch / 2)) {
-            tuple.set(7, -100);
-            tuple.set(6, -100.1f);
-            tuple.set(5, -100L);
-            tuple.set(4, -50e+2);
-            tuple.set(3, "so");
-            tuple.set(2, new DataByteArray("so"));
-
-          }
-
-          else {
-            Float f = 2.1f;
-            long l = 12;
-            double d = 2.1;
-            tuple.set(7, b*2);
-            tuple.set(6, f);
-            tuple.set(5, l);
-            tuple.set(4, d);
-            tuple.set(3, "somee");
-            tuple.set(2, new DataByteArray("somee"));
-          }
-
-          // insert record
-          tupRecord1.set(0, "" + b);
-          tupRecord1.set(1, "" + b);
-          tuple.set(1, tupRecord1);
-
-          // insert map
-
-          m1.put("a", "" + b);
-          m1.put("b", "" + b);
-          m1.put("c", "" + b);
-          tuple.set(0, m1);
-
-        } catch (ExecException e) {
-          e.printStackTrace();
-        }
-//      empty table
-//        inserters[i].insert(new BytesWritable(("key" + b).getBytes()), tuple);
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-    writer.close();
- 
-    //check table is setup correctly
-    String projection = new String("a,b,c,d,e,f,r1,m1");
-    
-    BasicTable.Reader reader = new BasicTable.Reader(pathTable2, conf);
-    reader.setProjection(projection);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    BytesWritable key = new BytesWritable();
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    /*
-    scanner.getValue(RowValue);
-    System.out.println("rowvalue size:"+RowValue.size());
-    System.out.println("read a : " + RowValue.get(7).toString());
-    System.out.println("read string: " + RowValue.get(6).toString());
-   
-    scanner.advance();
-    scanner.getValue(RowValue);
-    System.out.println("read float in 2nd row: "+ RowValue.get(6).toString());
-    System.out.println("done insert table");
-    */
-
-    reader.close();
-    
-  }
-  
-  public static void sortTable(Path tablePath, String sortkey){
-    
-  }
-  
-  @AfterClass
-  public static void tearDown() throws Exception {
-    pigServer.shutdown();
-  }
-
-
-  public void verify(Iterator<Tuple> it3) throws ExecException {
-    int row = 0;
-    Tuple RowValue3 = null;
-    Assert.assertEquals(0, row);
-  }
-
-  public Iterator<Tuple> joinTable(String table1, String table2, String sortkey1, String sortkey2) throws IOException {
-    String query1 = "records1 = LOAD '" + this.pathTable1.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    System.out.println("query1:" + query1);
-    pigServer.registerQuery(query1);
-
-    String query2 = "records2 = LOAD '" + this.pathTable2.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    System.out.println("query2:" + query2);
-    pigServer.registerQuery(query2);
-
-   /* Iterator<Tuple> it_before_order = pigServer.openIterator("records");
-    int row_before_order = 0;
-    Tuple RowValue_before_order = null;
-    while (it_before_order.hasNext()) {
-      RowValue_before_order = it_before_order.next();
-      row_before_order++;
-      System.out.println("row : " + row_before_order + " field f value: "
-          + RowValue_before_order.get(5));
-    }
-    System.out.println("total row for orig table before ordered:"
-        + row_before_order);*/
-    String orderby1 = "sort1 = ORDER records1 BY " + sortkey1 + " ;";
-    String orderby2 = "sort2 = ORDER records2 BY " + sortkey2 + " ;";
-    pigServer.registerQuery(orderby1);
-    pigServer.registerQuery(orderby2);
-
-    /*Iterator<Tuple> it_after_order = pigServer.openIterator("srecs");
-    int row_after_order = 0;
-    Tuple RowValue_after_order = null;
-    while (it_after_order.hasNext()) {
-      RowValue_after_order = it_after_order.next();
-      row_after_order++;
-      System.out.println("row : " + row_after_order + " field b value: "
-          + RowValue_after_order.get(1));
-    }
-    System.out.println("total row for orig table after ordered:"
-        + row_after_order);*/
-    // Path newPath = new Path(getCurrentMethodName());
-
-    /*
-     * Table1 creation
-     */
-    this.t1++;
-    
-    String table1path = this.pathTable1.toString() + Integer.toString(this.t1);
-    pigServer.store("sort1", table1path, TableStorer.class.getCanonicalName()
-        + "('[a, b, c]; [d, e, f, r1, m1]')");
-
-    String query3 = "records1 = LOAD '"
-        + table1path
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('a, b, c, d, e, f, r1, m1', 'sorted');";
-
-    System.out.println("query3:" + query3);
-    pigServer.registerQuery(query3);   
-    
-    String foreach = "records11 = foreach records1 generate a as a, b as b, c as c, d as d, e as e, f as f, r1 as r1, m1 as m1;";
-    pigServer.registerQuery(foreach);
-   /* Iterator<Tuple> it_ordered = pigServer.openIterator("records1");
-    int row_ordered = 0;
-    Tuple RowValue_ordered = null;
-    while (it_ordered.hasNext()) {
-      RowValue_ordered = it_ordered.next();
-      row_ordered++;
-      System.out.println("row : " + row_ordered + " field a value: "
-          + RowValue_ordered.get(0));
-    }
-    System.out.println("total row for table 1 after ordered:" + row_ordered);*/
-
-    /*
-     * Table2 creation
-     */
-    this.t1++;
-    String table2path = this.pathTable2.toString() + Integer.toString(this.t1);
-    pigServer.store("sort2", table2path, TableStorer.class.getCanonicalName()
-        + "('[a, b, c]; [d,e,f,r1,m1]')");
-
-    String query4 = "records2 = LOAD '" + table2path
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    pigServer.registerQuery(query4);
-
-    
-    String filter = "records22 = FILTER records2 BY a == '1.9';";
-    pigServer.registerQuery(filter);
-    /*Iterator<Tuple> it_ordered2 = pigServer.openIterator("records2");
-    int row_ordered2 = 0;
-    Tuple RowValue_ordered2 = null;
-    while (it_ordered2.hasNext()) {
-      RowValue_ordered2 = it_ordered2.next();
-      row_ordered2++;
-      System.out.println("row for table 2 after ordereed: " + row_ordered2
-          + " field a value: " + RowValue_ordered2.get(0));
-    }
-
-    System.out.println("total row for table 2:" + row_ordered2);
-    */
-    String join = "joinRecords = JOIN records11 BY " + "(" + sortkey1 + ")" 
-        + " , records2 BY " + "("+ sortkey2 + ")"+" USING \"merge\";";
-   //TODO: can not use records22
-      pigServer.registerQuery(join);
-   
-    // check JOIN content
-    Iterator<Tuple> it3 = pigServer.openIterator("joinRecords");
-    return it3;
-  }
-  
-  @Test
-  public void test1() throws ExecException, IOException {
-    /*
-     * join key: single byte column
-     */
-    /*
-     * right empty table
-     */
-    Iterator<Tuple> it3 = joinTable(this.pathTable1.toString(), this.pathTable2.toString(), "f","f" );
-    verify(it3);
-  }
-  
-  @Test
-  public void test2() throws ExecException, IOException {
-    /*
-     * join key: single byte column
-     */
-    /*
-     * left empty table
-     */
-    Iterator<Tuple> it3 = joinTable(this.pathTable2.toString(), this.pathTable1.toString(), "f","f" );
-    verify(it3);
-  }
-  
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMergeJoinNegative.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMergeJoinNegative.java
deleted file mode 100644
index 216d46e78..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMergeJoinNegative.java
+++ /dev/null
@@ -1,533 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.pig.TableStorer;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-
-public class TestMergeJoinNegative extends BaseTestCase {
-	
-	final static String STR_SCHEMA1 = "a:int,b:float,c:long,d:double,e:string,f:bytes,m1:map(string)";
-	final static String STR_STORAGE1 = "[a, b, c]; [e, f]; [m1#{a}]";
-	final static String STR_SCHEMA2 = "aa:int,bb:float,ee:string";
-	final static String STR_STORAGE2 = "[aa, bb]; [ee]";
-	final static String STR_SCHEMA3 = "a:string,b:int,c:float";
-	final static String STR_STORAGE3 = "[a, b]; [c]";
-	
-	static int fileId = 0;
-	
-	private static Path pathTable1;
-	private static Path pathTable2;
-	private static Path pathTable3;
-	private static Path pathTable4;
-	
-	@BeforeClass
-	public static void setUp() throws Exception {
-    init();
-    
-    pathTable1 = getTableFullPath("TestMergeJoinNegative1");
-    pathTable2 = getTableFullPath("TestMergeJoinNegative2"); 
-    pathTable3 = getTableFullPath("TestMergeJoinNegative3");
-    pathTable4 = getTableFullPath("TestMergeJoinNegative4");
-    removeDir(pathTable1);
-    removeDir(pathTable2);
-    removeDir(pathTable3);
-    removeDir(pathTable4);
-    
-		// Create table1 data
-		Map<String, String> m1 = new HashMap<String, String>();
-		m1.put("a","m1-a");
-		m1.put("b","m1-b");
-		
-		Object[][] table1 = {
-			{5,		-3.25f,	1001L,	51e+2,	"Zebra",	new DataByteArray("Zebra"),		m1},
-			{-1,	3.25f,	1000L,	50e+2,	"zebra",	new DataByteArray("zebra"),		m1},
-			{1001,	100.0f,	1000L,	50e+2,	"apple",	new DataByteArray("apple"),		m1},
-			{1002,	28.0f,	1000L,	50e+2,	"hadoop",	new DataByteArray("hadoop"),	m1},
-			{1000,	0.0f,	1002L,	52e+2,	"apple",	new DataByteArray("apple"),		m1} };
-		
-		// Create table1
-		createTable(pathTable1, STR_SCHEMA1, STR_STORAGE1, table1);
-		
-		// Create table2 data
-		Map<String, String> m2 = new HashMap<String, String>();
-		m2.put("a","m2-a");
-		m2.put("b","m2-b");
-		
-		Object[][] table2 = {
-			{15,	56.0f,	1004L,	50e+2,	"green",	new DataByteArray("green"),		m2},
-			{-1,	-99.0f,	1008L,	51e+2,	"orange",	new DataByteArray("orange"),	m2},
-			{1001,	0.0f,	1000L,	55e+2,	"white",	new DataByteArray("white"),		m2},
-			{1001,	-88.0f,	1001L,	52e+2,	"brown",	new DataByteArray("brown"),		m2},
-			{2000,	33.0f,	1002L,	52e+2,	"beige",	new DataByteArray("beige"),		m2} };
-		
-		// Create table2
-		createTable(pathTable2, STR_SCHEMA1, STR_STORAGE1, table2);
-		
-		// Create table3 data
-		Object[][] table3 = {
-			{0,		7.0f,	"grape"},
-			{1001,	8.0f,	"orange"},
-			{-200,	9.0f,	"banana"},
-			{8,		-88.0f,	"peach"} };
-		
-		// Create table3
-		createTable(pathTable3, STR_SCHEMA2, STR_STORAGE2, table3);
-		
-		// Create table4 data
-		Object[][] table4 = {
-			{"grape",	0,		7.0f},
-			{"orange",	1001,	8.0f},
-			{"banana",	-200,	9.0f},
-			{"peach",	8,		-88.0f} };
-		
-		// Create table4
-		createTable(pathTable4, STR_SCHEMA3, STR_STORAGE3, table4);
-		
-		// Load table1
-		String query1 = "table1 = LOAD '" + pathTable1.toString() + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		pigServer.registerQuery(query1);
-		
-		// Load table2
-		String query2 = "table2 = LOAD '" + pathTable2.toString() + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		pigServer.registerQuery(query2);
-		
-		// Load table3
-		String query3 = "table3 = LOAD '" + pathTable3.toString() + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		pigServer.registerQuery(query3);
-		
-		// Load table4
-		String query4 = "table4 = LOAD '" + pathTable4.toString() + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		pigServer.registerQuery(query4);
-	}
-	
-	private static void createTable(Path path, String schemaString, String storageString, Object[][] tableData) throws IOException {
-		// Create table from tableData array
-		BasicTable.Writer writer = new BasicTable.Writer(path, schemaString, storageString, conf);
-		
-		Schema schema = writer.getSchema();
-		Tuple tuple = TypesUtils.createTuple(schema);
-		TableInserter inserter = writer.getInserter("ins", false);
-		
-		for (int i = 0; i < tableData.length; ++i) {
-			TypesUtils.resetTuple(tuple);
-			for (int k = 0; k < tableData[i].length; ++k) {
-				tuple.set(k, tableData[i][k]);
-				System.out.println("DEBUG: setting tuple k=" + k + "value= " + tableData[i][k]);
-			}
-			inserter.insert(new BytesWritable(("key" + i).getBytes()), tuple);
-		}
-		inserter.close();
-		writer.close();
-	}
-	
-	@AfterClass
-	public static void tearDown() throws Exception {
-		pigServer.shutdown();
-	}
-	
-	@Test(expected = IOException.class)
-	public void test_merge_joint_12() throws ExecException, IOException {
-		//
-		// Pig script changes position of join key (negative test)
-		//
-		
-		// Sort tables
-		String orderby1 = "sort1 = ORDER table1 BY " + "a" + " ;";
-		pigServer.registerQuery(orderby1);
-		
-		String orderby2 = "sort2 = ORDER table2 BY " + "a" + " ;";
-		pigServer.registerQuery(orderby2);
-		
-		// Store sorted tables
-		++fileId;  // increment filename suffix
-		String pathSort1 = pathTable1.toString() + Integer.toString(fileId);
-		pigServer.store("sort1", pathSort1, TableStorer.class.getCanonicalName() +
-			"('[a, b, c]; [d, e, f, m1]')");
-		
-		String pathSort2 = pathTable2.toString() + Integer.toString(fileId);
-		pigServer.store("sort2", pathSort2, TableStorer.class.getCanonicalName() +
-			"('[a, b, c]; [d, e, f, m1]')");
-		
-		// Load sorted tables
-		String load1 = "records1 = LOAD '" + pathSort1 +
-			"' USING org.apache.hadoop.zebra.pig.TableLoader('a, b, c, d, e, f, m1', 'sorted');";
-		pigServer.registerQuery(load1);
-		
-		String load2 = "records2 = LOAD '" + pathSort2 +
-			"' USING org.apache.hadoop.zebra.pig.TableLoader('a, b, c, d, e, f, m1', 'sorted');";
-		pigServer.registerQuery(load2);
-		
-		// Change position and name of join key
-		String reorder1 = "reorder1 = FOREACH records1 GENERATE b as n2, a as n1, c as c, d as d, e as e, f as f, m1 as m1;";
-		pigServer.registerQuery(reorder1);
-		
-		// Merge tables
-		String join = "joinRecords = JOIN reorder1 BY " + "(" + "n2" + ")" + " , records2 BY " + "("+ "a" + ")" +
-			" USING \"merge\";";    // n2 is wrong data type
-		pigServer.registerQuery(join);
-		
-		pigServer.openIterator("joinRecords");  // get iterator to trigger error
-	}
-	
-	@Test(expected = IOException.class)
-	public void test_merge_joint_13() throws ExecException, IOException {
-		//
-		// Pig script changes sort order (negative test)
-		//
-		
-		// Sort tables
-		String orderby1 = "sort1 = ORDER table1 BY " + "a" + " ;";
-		pigServer.registerQuery(orderby1);
-		
-		String orderby2 = "sort2 = ORDER table2 BY " + "a" + " ;";
-		pigServer.registerQuery(orderby2);
-		
-		// Store sorted tables
-		++fileId;  // increment filename suffix
-		String pathSort1 = pathTable1.toString() + Integer.toString(fileId);
-		pigServer.store("sort1", pathSort1, TableStorer.class.getCanonicalName() +
-			"('[a, b, c]; [d, e, f, m1]')");
-		
-		String pathSort2 = pathTable2.toString() + Integer.toString(fileId);
-		pigServer.store("sort2", pathSort2, TableStorer.class.getCanonicalName() +
-			"('[a, b, c]; [d, e, f, m1]')");
-		
-		// Load sorted tables
-		String load1 = "records1 = LOAD '" + pathSort1 +
-			"' USING org.apache.hadoop.zebra.pig.TableLoader('a, b, c, d, e, f, m1', 'sorted');";
-		pigServer.registerQuery(load1);
-		
-		String load2 = "records2 = LOAD '" + pathSort2 +
-			"' USING org.apache.hadoop.zebra.pig.TableLoader('a, b, c, d, e, f, m1', 'sorted');";
-		pigServer.registerQuery(load2);
-		
-		// Change sort order for key
-		String reorder1 = "reorder1 = FOREACH records1 GENERATE a*(-1) as a, b as b, c as c, d as d, e as e, f as f, m1 as m1;";
-		pigServer.registerQuery(reorder1);
-		
-		// Merge tables
-		String join = "joinRecords = JOIN reorder1 BY " + "(" + "a" + ")" + " , records2 BY " + "("+ "a" + ")" +
-			" USING \"merge\";";
-		pigServer.registerQuery(join);
-		
-		pigServer.openIterator("joinRecords");  // get iterator to trigger error
-	}
-	
-	@Test(expected = IOException.class)
-	public void test_merge_joint_14() throws ExecException, IOException {
-		//
-		// Left hand table is not in ascending order (negative test)
-		//
-		
-		// Sort tables
-		String orderby1 = "sort1 = ORDER table1 BY " + "b" + " ;";  // sort left hand table by wrong key
-		pigServer.registerQuery(orderby1);
-		
-		String orderby2 = "sort2 = ORDER table2 BY " + "a" + " ;";
-		pigServer.registerQuery(orderby2);
-		
-		// Store sorted tables
-		++fileId;  // increment filename suffix
-		String pathSort1 = pathTable1.toString() + Integer.toString(fileId);
-		pigServer.store("sort1", pathSort1, TableStorer.class.getCanonicalName() +
-			"('[a, b, c]; [d, e, f, m1]')");
-		
-		String pathSort2 = pathTable2.toString() + Integer.toString(fileId);
-		pigServer.store("sort2", pathSort2, TableStorer.class.getCanonicalName() +
-			"('[a, b, c]; [d, e, f, m1]')");
-		
-		// Load sorted tables
-		String load1 = "records1 = LOAD '" + pathSort1 +
-			"' USING org.apache.hadoop.zebra.pig.TableLoader('a, b, c, d, e, f, m1', 'sorted');";
-		pigServer.registerQuery(load1);
-		
-		String load2 = "records2 = LOAD '" + pathSort2 +
-			"' USING org.apache.hadoop.zebra.pig.TableLoader('a, b, c, d, e, f, m1', 'sorted');";
-		pigServer.registerQuery(load2);
-		
-		// Merge tables
-		String join = "joinRecords = JOIN records1 BY " + "(" + "a" + ")" + " , records2 BY " + "("+ "a" + ")" +
-			" USING \"merge\";";
-		pigServer.registerQuery(join);
-		
-		pigServer.openIterator("joinRecords");  // get iterator to trigger error
-	}
-	
-	@Test(expected = IOException.class)
-	public void test_merge_joint_15() throws ExecException, IOException {
-		//
-		// Right hand table is not in ascending order (negative test)
-		//
-		
-		// Sort tables
-		String orderby1 = "sort1 = ORDER table1 BY " + "a" + " ;";
-		pigServer.registerQuery(orderby1);
-		
-		String orderby2 = "sort2 = ORDER table2 BY " + "b" + " ;";  // sort right hand table by wrong key
-		pigServer.registerQuery(orderby2);
-		
-		// Store sorted tables
-		++fileId;  // increment filename suffix
-		String pathSort1 = pathTable1.toString() + Integer.toString(fileId);
-		pigServer.store("sort1", pathSort1, TableStorer.class.getCanonicalName() +
-			"('[a, b, c]; [d, e, f, m1]')");
-		
-		String pathSort2 = pathTable2.toString() + Integer.toString(fileId);
-		pigServer.store("sort2", pathSort2, TableStorer.class.getCanonicalName() +
-			"('[a, b, c]; [d, e, f, m1]')");
-		
-		// Load sorted tables
-		String load1 = "records1 = LOAD '" + pathSort1 +
-			"' USING org.apache.hadoop.zebra.pig.TableLoader('a, b, c, d, e, f, m1', 'sorted');";
-		pigServer.registerQuery(load1);
-		
-		String load2 = "records2 = LOAD '" + pathSort2 +
-			"' USING org.apache.hadoop.zebra.pig.TableLoader('a, b, c, d, e, f, m1', 'sorted');";
-		pigServer.registerQuery(load2);
-		
-		// Merge tables
-		String join = "joinRecords = JOIN records1 BY " + "(" + "a" + ")" + " , records2 BY " + "("+ "a" + ")" +
-			" USING \"merge\";";
-		pigServer.registerQuery(join);
-		
-		pigServer.openIterator("joinRecords");  // get iterator to trigger error
-	}
-	
-	@Test(expected = IOException.class)
-	public void test_merge_joint_16() throws ExecException, IOException {
-		//
-		// More than two input tables (negative test)
-		//
-		
-		// Sort tables
-		String orderby1 = "sort1 = ORDER table1 BY " + "a" + " ;";
-		pigServer.registerQuery(orderby1);
-		
-		String orderby2 = "sort2 = ORDER table2 BY " + "a" + " ;";
-		pigServer.registerQuery(orderby2);
-		
-		String orderby3 = "sort3 = ORDER table3 BY " + "aa" + " ;";
-		pigServer.registerQuery(orderby3);
-		
-		// Store sorted tables
-		++fileId;  // increment filename suffix
-		String pathSort1 = pathTable1.toString() + Integer.toString(fileId);
-		pigServer.store("sort1", pathSort1, TableStorer.class.getCanonicalName() +
-			"('[a, b, c]; [d, e, f, m1]')");
-		
-		String pathSort2 = pathTable2.toString() + Integer.toString(fileId);
-		pigServer.store("sort2", pathSort2, TableStorer.class.getCanonicalName() +
-			"('[a, b, c]; [d, e, f, m1]')");
-		
-		String pathSort3 = pathTable3.toString() + Integer.toString(fileId);
-		pigServer.store("sort3", pathSort3, TableStorer.class.getCanonicalName() +
-			"('[aa, bb]; [ee]')");
-		
-		// Load sorted tables
-		String load1 = "records1 = LOAD '" + pathSort1 +
-			"' USING org.apache.hadoop.zebra.pig.TableLoader('a, b, c, d, e, f, m1', 'sorted');";
-		pigServer.registerQuery(load1);
-		
-		String load2 = "records2 = LOAD '" + pathSort2 +
-			"' USING org.apache.hadoop.zebra.pig.TableLoader('a, b, c, d, e, f, m1', 'sorted');";
-		pigServer.registerQuery(load2);
-		
-		String load3 = "records3 = LOAD '" + pathSort3 +
-			"' USING org.apache.hadoop.zebra.pig.TableLoader('aa, bb, ee', 'sorted');";
-		pigServer.registerQuery(load3);
-		
-		// Merge tables
-		String join = "joinRecords = JOIN records1 BY " + "(" + "a" + ")" + " , records2 BY " + "("+ "a" + ")" +
-			" , records3 BY " + "("+ "aa" + ")" + " USING \"merge\";";  // merge three tables
-		pigServer.registerQuery(join);
-	}
-	
-	@Test(expected = IOException.class)
-	public void test_merge_joint_25() throws ExecException, IOException {
-		//
-		// Two tables do not have common join key (negative test)
-		//
-		
-		// Sort tables
-		String orderby1 = "sort1 = ORDER table1 BY " + "a" + " ;";
-		pigServer.registerQuery(orderby1);
-		
-		String orderby3 = "sort3 = ORDER table3 BY " + "aa" + " ;";
-		pigServer.registerQuery(orderby3);
-		
-		// Store sorted tables
-		++fileId;  // increment filename suffix
-		String pathSort1 = pathTable1.toString() + Integer.toString(fileId);
-		pigServer.store("sort1", pathSort1, TableStorer.class.getCanonicalName() +
-			"('[a, b, c]; [d, e, f, m1]')");
-		
-		String pathSort3 = pathTable3.toString() + Integer.toString(fileId);
-		pigServer.store("sort3", pathSort3, TableStorer.class.getCanonicalName() +
-			"('[aa, bb]; [ee]')");
-		
-		// Load sorted tables
-		String load1 = "records1 = LOAD '" + pathSort1 +
-			"' USING org.apache.hadoop.zebra.pig.TableLoader('a, b, c, d, e, f, m1', 'sorted');";
-		pigServer.registerQuery(load1);
-		
-		String load3 = "records3 = LOAD '" + pathSort3 +
-			"' USING org.apache.hadoop.zebra.pig.TableLoader('aa, bb, ee', 'sorted');";
-		pigServer.registerQuery(load3);
-		
-		// Merge tables
-		String join = "joinRecords = JOIN records1 BY " + "(" + "a" + ")" + " , records3 BY " + "("+ "a" + ")" +
-			" USING \"merge\";";					// sort key a does not exist for records3
-		pigServer.registerQuery(join);
-	}
-	
-	@Test(expected = IOException.class)
-	public void test_merge_joint_26() throws ExecException, IOException {
-		//
-		// Two tables do not have common join key (negative test)
-		//
-		
-		// Sort tables
-		String orderby1 = "sort1 = ORDER table1 BY " + "a" + " ;";
-		pigServer.registerQuery(orderby1);
-		
-		String orderby3 = "sort3 = ORDER table3 BY " + "aa" + " ;";
-		pigServer.registerQuery(orderby3);
-		
-		// Store sorted tables
-		++fileId;  // increment filename suffix
-		String pathSort1 = pathTable1.toString() + Integer.toString(fileId);
-		pigServer.store("sort1", pathSort1, TableStorer.class.getCanonicalName() +
-			"('[a, b, c]; [d, e, f, m1]')");
-		
-		String pathSort3 = pathTable3.toString() + Integer.toString(fileId);
-		pigServer.store("sort3", pathSort3, TableStorer.class.getCanonicalName() +
-			"('[aa, bb]; [ee]')");
-		
-		// Load sorted tables
-		String load1 = "records1 = LOAD '" + pathSort1 +
-			"' USING org.apache.hadoop.zebra.pig.TableLoader('a, b, c, d, e, f, m1', 'sorted');";
-		pigServer.registerQuery(load1);
-		
-		String load3 = "records3 = LOAD '" + pathSort3 +
-			"' USING org.apache.hadoop.zebra.pig.TableLoader('aa, bb, ee', 'sorted');";
-		pigServer.registerQuery(load3);
-		
-		// Merge tables
-		String join = "joinRecords = JOIN records1 BY " + "(" + "aa" + ")" + " , records3 BY " + "("+ "aa" + ")" +
-			" USING \"merge\";";					// sort key aa does not exist for records1
-		pigServer.registerQuery(join);
-	}
-	
-	@Test(expected = IOException.class)
-	public void test_merge_joint_27() throws ExecException, IOException {
-		//
-		// Two tables do not have common join key (negative test)
-		//
-		
-		// Sort tables
-		String orderby1 = "sort1 = ORDER table1 BY " + "a" + " ;";
-		pigServer.registerQuery(orderby1);
-		
-		String orderby3 = "sort3 = ORDER table3 BY " + "aa" + " ;";
-		pigServer.registerQuery(orderby3);
-		
-		// Store sorted tables
-		++fileId;  // increment filename suffix
-		String pathSort1 = pathTable1.toString() + Integer.toString(fileId);
-		pigServer.store("sort1", pathSort1, TableStorer.class.getCanonicalName() +
-			"('[a, b, c]; [d, e, f, m1]')");
-		
-		String pathSort3 = pathTable3.toString() + Integer.toString(fileId);
-		pigServer.store("sort3", pathSort3, TableStorer.class.getCanonicalName() +
-			"('[aa, bb]; [ee]')");
-		
-		// Load sorted tables
-		String load1 = "records1 = LOAD '" + pathSort1 +
-			"' USING org.apache.hadoop.zebra.pig.TableLoader('a, b, c, d, e, f, m1', 'sorted');";
-		pigServer.registerQuery(load1);
-		
-		String load3 = "records3 = LOAD '" + pathSort3 +
-			"' USING org.apache.hadoop.zebra.pig.TableLoader('aa, bb, ee', 'sorted');";
-		pigServer.registerQuery(load3);
-		
-		// Merge tables
-		String join = "joinRecords = JOIN records1 BY " + "(" + "aaa" + ")" + " , records3 BY " + "("+ "aaa" + ")" +
-			" USING \"merge\";";					// sort key aaa does not exist for records1 and records3
-		pigServer.registerQuery(join);
-	}
-	
-	@Test(expected = IOException.class)
-	public void test_merge_joint_28() throws ExecException, IOException {
-		//
-		// Two table key names are the same but the data types are different (negative test)
-		//
-		
-		// Sort tables
-		String orderby1 = "sort1 = ORDER table1 BY " + "a" + " ;";  // a is float
-		pigServer.registerQuery(orderby1);
-		
-		String orderby4 = "sort4 = ORDER table4 BY " + "a" + " ;";  // a is string
-		pigServer.registerQuery(orderby4);
-		
-		// Store sorted tables
-		++fileId;  // increment filename suffix
-		String pathSort1 = pathTable1.toString() + Integer.toString(fileId);
-		pigServer.store("sort1", pathSort1, TableStorer.class.getCanonicalName() +
-			"('[a, b, c]; [d, e, f, m1]')");
-		
-		String pathSort4 = pathTable4.toString() + Integer.toString(fileId);
-		pigServer.store("sort4", pathSort4, TableStorer.class.getCanonicalName() +
-			"('[a, b]; [c]')");
-		
-		// Load sorted tables
-		String load1 = "records1 = LOAD '" + pathSort1 +
-			"' USING org.apache.hadoop.zebra.pig.TableLoader('a, b, c, d, e, f, m1', 'sorted');";
-		pigServer.registerQuery(load1);
-		
-		String load4 = "records4 = LOAD '" + pathSort4 +
-			"' USING org.apache.hadoop.zebra.pig.TableLoader('a, b, c', 'sorted');";
-		pigServer.registerQuery(load4);
-		
-		// Merge tables
-		String join = "joinRecords = JOIN records1 BY " + "(" + "a" + ")" + " , records4 BY " + "("+ "a" + ")" +
-			" USING \"merge\";";					// sort key a is different data type for records1 and records4
-		pigServer.registerQuery(join);
-		
-	  pigServer.openIterator("joinRecords");  // get iterator to trigger error
-	}
-	
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMergeJoinPartial.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMergeJoinPartial.java
deleted file mode 100644
index 35690292e..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMergeJoinPartial.java
+++ /dev/null
@@ -1,365 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.ArrayList;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.pig.TableStorer;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-
-public class TestMergeJoinPartial extends BaseTestCase {
-	
-	final static String STR_SCHEMA1 = "a:int,b:float,c:long,d:double,e:string,f:bytes,m1:map(string)";
-	final static String STR_STORAGE1 = "[a, b, c]; [e, f]; [m1#{a}]";
-	
-	static int fileId = 0;
-	
-	private static Path pathTable1;
-	private static Path pathTable2;
-	
-	private static Object[][] table1;
-	private static Object[][] table2;
-	
-	@BeforeClass
-	public static void setUp() throws Exception {
-    init();
-    
-    pathTable1 = getTableFullPath("TestMergeJoinPartial1");
-    pathTable2 = getTableFullPath("TestMergeJoinPartial2");    
-    removeDir(pathTable1);
-    removeDir(pathTable2);
-		
-		// Create table1 data
-		Map<String, String> m1 = new HashMap<String, String>();
-		m1.put("a","m1-a");
-		m1.put("b","m1-b");
-		
-		table1 = new Object[][]{
-			{5,		-3.25f,	1001L,	51e+2,	"Zebra",	new DataByteArray("Zebra"),		m1},
-			{-1,	3.25f,	1000L,	50e+2,	"zebra",	new DataByteArray("zebra"),		m1},
-			{1001,	100.0f,	1003L,	50e+2,	"Apple",	new DataByteArray("Apple"),		m1},
-			{1001,	101.0f,	1001L,	50e+2,	"apple",	new DataByteArray("apple"),		m1},
-			{1001,	50.0f,	1000L,	50e+2,	"Pig",		new DataByteArray("Pig"),		m1},
-			{1001,	52.0f,	1001L,	50e+2,	"pig",		new DataByteArray("pig"),		m1},
-			{1002,	28.0f,	1000L,	50e+2,	"Hadoop",	new DataByteArray("Hadoop"),	m1},
-			{1000,	0.0f,	1002L,	52e+2,	"hadoop",	new DataByteArray("hadoop"),	m1} };
-		
-		// Create table1
-		createTable(pathTable1, STR_SCHEMA1, STR_STORAGE1, table1);
-		
-		// Create table2 data
-		Map<String, String> m2 = new HashMap<String, String>();
-		m2.put("a","m2-a");
-		m2.put("b","m2-b");
-		
-		table2 = new Object[][] {
-			{15,	56.0f,	1004L,	50e+2,	"green",	new DataByteArray("green"),		m2},
-			{-1,	-99.0f,	1002L,	51e+2,	"orange",	new DataByteArray("orange"),	m2},
-			{1001,	100.0f,	1003L,	55e+2,	"white",	new DataByteArray("white"),		m2},
-			{1001,	102.0f,	1001L,	52e+2,	"purple",	new DataByteArray("purple"),	m2},
-			{1001,	50.0f,	1008L,	52e+2,	"gray",		new DataByteArray("gray"),		m2},
-			{1001,	53.0f,	1001L,	52e+2,	"brown",	new DataByteArray("brown"),		m2},
-			{2000,	33.0f,	1006L,	52e+2,	"beige",	new DataByteArray("beige"),		m2} };
-		
-		// Create table2
-		createTable(pathTable2, STR_SCHEMA1, STR_STORAGE1, table2);
-		
-		// Load table1
-		String query1 = "table1 = LOAD '" + pathTable1.toString() + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		pigServer.registerQuery(query1);
-		
-		// Load table2
-		String query2 = "table2 = LOAD '" + pathTable2.toString() + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		pigServer.registerQuery(query2);
-		
-	}
-	
-	private static void createTable(Path path, String schemaString, String storageString, Object[][] tableData)
-			throws IOException {
-		//
-		// Create table from tableData array
-		//
-		BasicTable.Writer writer = new BasicTable.Writer(path, schemaString, storageString, conf);
-		
-		Schema schema = writer.getSchema();
-		Tuple tuple = TypesUtils.createTuple(schema);
-		TableInserter inserter = writer.getInserter("ins", false);
-		
-		for (int i = 0; i < tableData.length; ++i) {
-			TypesUtils.resetTuple(tuple);
-			for (int k = 0; k < tableData[i].length; ++k) {
-				tuple.set(k, tableData[i][k]);
-				System.out.println("DEBUG: setting tuple k=" + k + "value= " + tableData[i][k]);
-			}
-			inserter.insert(new BytesWritable(("key" + i).getBytes()), tuple);
-		}
-		inserter.close();
-		writer.close();
-	}
-	
-	@AfterClass
-	public static void tearDown() throws Exception {
-		pigServer.shutdown();
-	}
-	
-	
-	@Test
-	public void test_merge_joint_17() throws ExecException, IOException {
-		//
-		// Multiple join where join keys are partial, and the order of keys is honored
-		//
-		
-		// Sort tables
-		String orderby1 = "sort1 = ORDER table1 BY " + "a,b,c" + " ;";
-		pigServer.registerQuery(orderby1);
-		
-		String orderby2 = "sort2 = ORDER table2 BY " + "a,b,c" + " ;";
-		pigServer.registerQuery(orderby2);
-		
-		// Store sorted tables
-		++fileId;  // increment filename suffix
-		String pathSort1 = pathTable1.toString() + Integer.toString(fileId);
-		pigServer.store("sort1", pathSort1, TableStorer.class.getCanonicalName() +
-			"('[a, b, c]; [d, e, f, m1]')");
-		
-		String pathSort2 = pathTable2.toString() + Integer.toString(fileId);
-		pigServer.store("sort2", pathSort2, TableStorer.class.getCanonicalName() +
-			"('[a, b, c]; [d, e, f, m1]')");
-		
-		// Load sorted tables
-		String load1 = "records1 = LOAD '" + pathSort1 +
-			"' USING org.apache.hadoop.zebra.pig.TableLoader('a, b, c, d, e, f, m1', 'sorted');";
-		pigServer.registerQuery(load1);
-		
-		String load2 = "records2 = LOAD '" + pathSort2 +
-			"' USING org.apache.hadoop.zebra.pig.TableLoader('a, b, c, d, e, f, m1', 'sorted');";
-		pigServer.registerQuery(load2);
-		
-		// Merge tables
-		String join = "joinRecords = JOIN records1 BY " + "(" + "a,b,c" + ")" + " , records2 BY " + "("+ "a,b,c" + ")" +
-			" USING \"merge\";";
-		pigServer.registerQuery(join);
-		
-		// Verify merged tables
-		ArrayList<ArrayList<Object>> resultTable = new ArrayList<ArrayList<Object>>();
-		
-		addResultRow(resultTable, table1[2], table2[2]);  // set expected values for row1
-		
-		Iterator<Tuple> it = pigServer.openIterator("joinRecords");
-		verifyTable(resultTable, it);
-	}
-	
-	@Test
-	public void test_merge_joint_22() throws ExecException, IOException {
-		//
-		// Multiple join where join keys are partial, and the order of keys is honored
-		//
-		// Known bug with partial key join
-		// - Need to add verification to this test once bug is fixed
-		
-		// Sort tables
-		String orderby1 = "sort1 = ORDER table1 BY " + "a,b,c" + " ;";
-		pigServer.registerQuery(orderby1);
-		
-		String orderby2 = "sort2 = ORDER table2 BY " + "a,b,c" + " ;";
-		pigServer.registerQuery(orderby2);
-		
-		// Store sorted tables
-		++fileId;  // increment filename suffix
-		String pathSort1 = pathTable1.toString() + Integer.toString(fileId);
-		pigServer.store("sort1", pathSort1, TableStorer.class.getCanonicalName() +
-			"('[a, b, c]; [d, e, f, m1]')");
-		
-		String pathSort2 = pathTable2.toString() + Integer.toString(fileId);
-		pigServer.store("sort2", pathSort2, TableStorer.class.getCanonicalName() +
-			"('[a, b, c]; [d, e, f, m1]')");
-		
-		// Load sorted tables
-		String load1 = "records1 = LOAD '" + pathSort1 +
-			"' USING org.apache.hadoop.zebra.pig.TableLoader('a, b, c, d, e, f, m1', 'sorted');";
-		pigServer.registerQuery(load1);
-		
-		String load2 = "records2 = LOAD '" + pathSort2 +
-			"' USING org.apache.hadoop.zebra.pig.TableLoader('a, b, c, d, e, f, m1', 'sorted');";
-		pigServer.registerQuery(load2);
-		
-		// Merge tables
-		String join = "joinRecords = JOIN records1 BY " + "(" + "a,b" + ")" + " , records2 BY " + "("+ "a,b" + ")" +
-			" USING \"merge\";";
-		pigServer.registerQuery(join);
-		
-		printTable("joinRecords");
-	}
-	
-	@Test
-	public void test_merge_joint_23() throws ExecException, IOException {
-		//
-		// Multiple join where join keys are partial, and the order of keys is honored
-		//
-		// Known bug with partial key join
-		// - Need to add verification to this test once bug is fixed
-		
-		// Sort tables
-		String orderby1 = "sort1 = ORDER table1 BY " + "a,b,c" + " ;";
-		pigServer.registerQuery(orderby1);
-		
-		String orderby2 = "sort2 = ORDER table2 BY " + "a,b,c" + " ;";
-		pigServer.registerQuery(orderby2);
-		
-		// Store sorted tables
-		++fileId;  // increment filename suffix
-		String pathSort1 = pathTable1.toString() + Integer.toString(fileId);
-		pigServer.store("sort1", pathSort1, TableStorer.class.getCanonicalName() +
-			"('[a, b, c]; [d, e, f, m1]')");
-		
-		String pathSort2 = pathTable2.toString() + Integer.toString(fileId);
-		pigServer.store("sort2", pathSort2, TableStorer.class.getCanonicalName() +
-			"('[a, b, c]; [d, e, f, m1]')");
-		
-		// Load sorted tables
-		String load1 = "records1 = LOAD '" + pathSort1 +
-			"' USING org.apache.hadoop.zebra.pig.TableLoader('a, b, c, d, e, f, m1', 'sorted');";
-		pigServer.registerQuery(load1);
-		
-		String load2 = "records2 = LOAD '" + pathSort2 +
-			"' USING org.apache.hadoop.zebra.pig.TableLoader('a, b, c, d, e, f, m1', 'sorted');";
-		pigServer.registerQuery(load2);
-		
-		// Merge tables
-		String join = "joinRecords = JOIN records1 BY " + "(" + "a" + ")" + " , records2 BY " + "("+ "a" + ")" +
-			" USING \"merge\";";
-		pigServer.registerQuery(join);
-		
-		printTable("joinRecords");
-	}
-	
-	@Test(expected = IOException.class)
-	public void test_merge_joint_24() throws ExecException, IOException {
-		//
-		// Multiple join where join keys are partial, and the order of keys is honored
-		// (negative test)
-		
-		// Sort tables
-		String orderby1 = "sort1 = ORDER table1 BY " + "a,b,c" + " ;";
-		pigServer.registerQuery(orderby1);
-		
-		String orderby2 = "sort2 = ORDER table2 BY " + "a,b,c" + " ;";
-		pigServer.registerQuery(orderby2);
-		
-		// Store sorted tables
-		++fileId;  // increment filename suffix
-		String pathSort1 = pathTable1.toString() + Integer.toString(fileId);
-		pigServer.store("sort1", pathSort1, TableStorer.class.getCanonicalName() +
-			"('[a, b, c]; [d, e, f, m1]')");
-		
-		String pathSort2 = pathTable2.toString() + Integer.toString(fileId);
-		pigServer.store("sort2", pathSort2, TableStorer.class.getCanonicalName() +
-			"('[a, b, c]; [d, e, f, m1]')");
-		
-		// Load sorted tables
-		String load1 = "records1 = LOAD '" + pathSort1 +
-			"' USING org.apache.hadoop.zebra.pig.TableLoader('a, b, c, d, e, f, m1', 'sorted');";
-		pigServer.registerQuery(load1);
-		
-		String load2 = "records2 = LOAD '" + pathSort2 +
-			"' USING org.apache.hadoop.zebra.pig.TableLoader('a, b, c, d, e, f, m1', 'sorted');";
-		pigServer.registerQuery(load2);
-		
-		// Merge tables
-		String join = "joinRecords = JOIN records1 BY " + "(" + "a,c" + ")" + " , records2 BY " + "("+ "a,c" + ")" +
-			" USING \"merge\";";
-		pigServer.registerQuery(join);
-		
-		pigServer.openIterator("joinRecords");  // get iterator to trigger error
-	}
-	
-	public void printTable(String tablename) throws IOException {
-		//
-		// Print Pig Table (for debugging)
-		//
-		Iterator<Tuple> it1 = pigServer.openIterator(tablename);
-		Tuple RowValue1 = null;
-		while (it1.hasNext()) {
-			RowValue1 = it1.next();
-			System.out.println();
-			
-			for (int i = 0; i < RowValue1.size(); ++i) {
-				System.out.println("DEBUG: " + tablename + " RowValue.get(" + i + ") = " + RowValue1.get(i));
-				
-			}
-		}
-	}
-	
-	public void addResultRow(ArrayList<ArrayList<Object>> resultTable, Object[] leftRow, Object[] rightRow) {
-		//
-		// Add a row to expected results table
-		//
-		ArrayList<Object> resultRow = new ArrayList<Object>();
-		
-		for (int i=0; i<leftRow.length; ++i)
-			resultRow.add(leftRow[i]);
-		for (int i=0; i<rightRow.length; ++i)
-			resultRow.add(rightRow[i]);
-		
-		resultTable.add(resultRow);
-	}
-	
-	public void verifyTable(ArrayList<ArrayList<Object>> resultTable, Iterator<Tuple> it) throws IOException {
-		//
-		// Verify expected results table to returned test case table
-		//
-		Tuple RowValues;
-		int rowIndex = 0;
-		
-		while (it.hasNext()) {
-			RowValues = it.next();
-			ArrayList<Object> resultRow = resultTable.get(rowIndex);
-			Assert.assertEquals(resultRow.size(), RowValues.size());  // verify expected tuple count
-			System.out.println();
-			
-			for (int i = 0; i < RowValues.size(); ++i) {
-				System.out.println("DEBUG: resultTable " + " RowValue.get(" + i + ") = " + RowValues.get(i) +
-						" " + resultRow.get(i));
-				Assert.assertEquals(resultRow.get(i), RowValues.get(i));  // verify each row value
-			}
-			++rowIndex;
-		}
-		Assert.assertEquals(resultTable.size(), rowIndex);  // verify expected row count
-	}
-	
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMergeJoinPrune.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMergeJoinPrune.java
deleted file mode 100644
index 31dbf8f1d..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMergeJoinPrune.java
+++ /dev/null
@@ -1,537 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-import junit.framework.Assert;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.pig.TableStorer;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.executionengine.ExecJob;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-public class TestMergeJoinPrune extends BaseTestCase {
-  final static int numsBatch = 1;
-  final static int numsInserters = 1;
-  static Path pathTable1;
-  static Path pathTable2;
-  final static String STR_SCHEMA1 = "a:int,b:float,c:long,d:double,e:string,f:bytes,r1:record(f1:string, f2:string),m1:map(string)";
-  final static String STR_SCHEMA2 = "m1:map(string),r1:record(f1:string, f2:string),f:bytes,e:string,d:double,c:long,b:float,a:int";
-
-  final static String STR_STORAGE1 = "[a, b, c]; [e, f]; [r1.f1]; [m1#{a}]";
-  final static String STR_STORAGE2 = "[a];[b]; [c]; [e]; [f]; [r1.f1]; [m1#{a}]";
- 
-  private static int t1;
-
-  @BeforeClass
-  public static void setUp() throws Exception {
-    init();
-    pathTable1 = getTableFullPath("TestMergeJoinPrune1");
-    pathTable2 = getTableFullPath("TestMergeJoinPrune2");    
-    removeDir(pathTable1);
-    removeDir(pathTable2);
-    
-    createFirstTable();
-    createSecondTable();
-  }
-  public static void createFirstTable() throws IOException, ParseException {
-    BasicTable.Writer writer = new BasicTable.Writer(pathTable1, STR_SCHEMA1,
-        STR_STORAGE1, conf);
-    Schema schema = writer.getSchema();
-    //System.out.println("typeName" + schema.getColumn("a").type.pigDataType());
-    Tuple tuple = TypesUtils.createTuple(schema);
-  
-    TableInserter[] inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-    Tuple tupRecord1;
-    try {
-      tupRecord1 = TypesUtils.createTuple(schema.getColumnSchema("r1")
-          .getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-    Map<String, String> m1 = new HashMap<String, String>();
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tupRecord1);
-        TypesUtils.resetTuple(tuple);
-        m1.clear();
-  
-        try {
-          // first row of the table , the biggest row
-          if (i == 0 && b == 0) {
-            tuple.set(0, 100);
-            tuple.set(1, 100.1f);
-            tuple.set(2, 100L);
-            tuple.set(3, 50e+2);
-            tuple.set(4, "something");
-            tuple.set(5, new DataByteArray("something"));
-  
-          }
-          // the middle + 1 row of the table, the smallest row
-          else if (i == 0 && b == (numsBatch / 2)) {
-            tuple.set(0, -100);
-            tuple.set(1, -100.1f);
-            tuple.set(2, -100L);
-            tuple.set(3, -50e+2);
-            tuple.set(4, "so");
-            tuple.set(5, new DataByteArray("so"));
-  
-          }
-  
-          else {
-            Float f = 1.1f;
-            long l = 11;
-            double d = 1.1;
-            tuple.set(0, b);
-            tuple.set(1, f);
-            tuple.set(2, l);
-            tuple.set(3, d);
-            tuple.set(4, "some");
-            tuple.set(5, new DataByteArray("some"));
-          }
-  
-          // insert record
-          tupRecord1.set(0, "" + b);
-          tupRecord1.set(1, "" + b);
-          tuple.set(6, tupRecord1);
-  
-          // insert map
-          m1.put("a", "" + b);
-          m1.put("b", "" + b);
-          m1.put("c", "" + b);
-          tuple.set(7, m1);
-  
-        } catch (ExecException e) {
-          e.printStackTrace();
-        }
-  
-       inserters[i].insert(new BytesWritable(("key_" + b).getBytes()), tuple);
-       
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-    writer.close();
-    
-  
-    //check table is setup correctly
-    String projection = new String("a,b,c,d,e,f,r1,m1");
-    
-    BasicTable.Reader reader = new BasicTable.Reader(pathTable1, conf);
-    reader.setProjection(projection);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-  
-    scanner.getValue(RowValue);
-    System.out.println("rowvalue size:"+RowValue.size());
-    System.out.println("read a : " + RowValue.get(0).toString());
-    System.out.println("read string: " + RowValue.get(1).toString());
-  
-    scanner.advance();
-    if(!scanner.atEnd()) {
-    	scanner.getValue(RowValue);
-    	System.out.println("read float in 2nd row: "+ RowValue.get(1).toString());
-    	System.out.println("done insert table");
-    }
-  
-    reader.close();
-    
-  }
-  public static void createSecondTable() throws IOException, ParseException {
-    BasicTable.Writer writer = new BasicTable.Writer(pathTable2, STR_SCHEMA2,
-        STR_STORAGE2, conf);
-    Schema schema = writer.getSchema();
-    //System.out.println("typeName" + schema.getColumn("a").type.pigDataType());
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    TableInserter[] inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-    Tuple tupRecord1;
-    try {
-      tupRecord1 = TypesUtils.createTuple(schema.getColumnSchema("r1")
-          .getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-    Map<String, String> m1 = new HashMap<String, String>();
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tupRecord1);
-        TypesUtils.resetTuple(tuple);
-        m1.clear();
-
-        try {
-           // first row of the table , the biggest row
-          if (i == 0 && b == 0) {
-            tuple.set(7, 100);
-            tuple.set(6, 100.1f);
-            tuple.set(5, 100L);
-            tuple.set(4, 50e+2);
-            tuple.set(3, "something");
-            tuple.set(2, new DataByteArray("something"));
-
-          }
-          // the middle +1 row of the table, the smallest row
-          else if (i == 0 && b == (numsBatch / 2)) {
-            tuple.set(7, -100);
-            tuple.set(6, -100.1f);
-            tuple.set(5, -100L);
-            tuple.set(4, -50e+2);
-            tuple.set(3, "so");
-            tuple.set(2, new DataByteArray("so"));
-
-          }
-
-          else {
-            Float f = 2.1f;
-            long l = 12;
-            double d = 2.1;
-            tuple.set(7, b*2);
-            tuple.set(6, f);
-            tuple.set(5, l);
-            tuple.set(4, d);
-            tuple.set(3, "somee");
-            tuple.set(2, new DataByteArray("somee"));
-          }
-
-          // insert record
-          tupRecord1.set(0, "" + b);
-          tupRecord1.set(1, "" + b);
-          tuple.set(1, tupRecord1);
-
-          // insert map
-
-          m1.put("a", "" + b);
-          m1.put("b", "" + b);
-          m1.put("c", "" + b);
-          tuple.set(0, m1);
-
-        } catch (ExecException e) {
-          e.printStackTrace();
-        }
-
-        inserters[i].insert(new BytesWritable(("key" + b).getBytes()), tuple);
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-    writer.close();
-    
-    //check table is setup correctly
-    String projection = new String("a,b,c,d,e,f,r1,m1");
-    
-    BasicTable.Reader reader = new BasicTable.Reader(pathTable2, conf);
-    reader.setProjection(projection);
-    List<RangeSplit> splits = reader.rangeSplit(1);
-    TableScanner scanner = reader.getScanner(splits.get(0), true);
-    Tuple RowValue = TypesUtils.createTuple(scanner.getSchema());
-
-    scanner.getValue(RowValue);
-    System.out.println("rowvalue size:"+RowValue.size());
-    System.out.println("read a : " + RowValue.get(7).toString());
-    System.out.println("read string: " + RowValue.get(6).toString());
-   
-    scanner.advance();
-    if(!scanner.atEnd()) {
-    	scanner.getValue(RowValue);
-    	System.out.println("read float in 2nd row: "+ RowValue.get(6).toString());
-    	System.out.println("done insert table");
-    }	
-
-
-    reader.close();
-    
-  }
- 
-  @AfterClass
-  public static void tearDown() throws Exception {
-    pigServer.shutdown();
-    BasicTable.drop(pathTable1, conf);
-    BasicTable.drop(pathTable2, conf);
-    
-  }
-
-  public void verify(Iterator<Tuple> it3) throws ExecException {
-    int row = 0;
-    Tuple RowValue3 = null;
-    while (it3.hasNext()) {
-      RowValue3 = it3.next();
-      Assert.assertEquals(9, RowValue3.size());
-      row++;
-
-      if (row == 100) {
-        // smallest row,   the middle row of original table
-        Assert.assertEquals(-100, RowValue3.get(0));// a
-        Assert.assertEquals(-100.1f, RowValue3.get(1)); // b
-        Assert.assertEquals(-100L, RowValue3.get(2)); // c
-        Assert.assertEquals(-5000.0, RowValue3.get(3)); // d
-        Assert.assertEquals("so", RowValue3.get(4)); // e
-        Assert.assertEquals("so", RowValue3.get(5).toString());// f
-        Assert.assertEquals("" + numsBatch / 2, ((Tuple) RowValue3.get(6))
-            .get(0));// r
-        Assert.assertEquals("" + numsBatch / 2, ((Tuple) RowValue3.get(6))
-            .get(1));// r
-        Assert.assertEquals("" + numsBatch / 2, ((Map) RowValue3.get(7))
-            .get("a"));// m
-        Assert.assertEquals("" + numsBatch / 2, ((Map) RowValue3.get(7))
-            .get("b"));// m
-        Assert.assertEquals("" + numsBatch / 2, ((Map) RowValue3.get(7))
-            .get("c"));// m
-        Assert.assertEquals(-100, RowValue3.get(15)); // a
-        Assert.assertEquals(-100.1f, RowValue3.get(14)); // b
-        Assert.assertEquals(-100L, RowValue3.get(13)); // c
-        Assert.assertEquals(-5000.0, RowValue3.get(12)); // d
-        Assert.assertEquals("so", RowValue3.get(11)); // e
-        Assert.assertEquals("so", RowValue3.get(10).toString());// f
-        Assert.assertEquals("" + numsBatch / 2, ((Tuple) RowValue3.get(9))
-            .get(0));// r
-        Assert.assertEquals("" + numsBatch / 2, ((Tuple) RowValue3.get(9))
-            .get(1));// r
-        Assert.assertEquals("" + numsBatch / 2, ((Map) RowValue3.get(8))
-            .get("a"));// m
-        Assert.assertEquals("" + numsBatch / 2, ((Map) RowValue3.get(8))
-            .get("b"));// m
-        Assert.assertEquals("" + numsBatch / 2, ((Map) RowValue3.get(8))
-            .get("c"));// m
-      }
-      
-      // largest row, the first row of the original table
-      if (row == 200) {
-        Assert.assertEquals(100, RowValue3.get(0));// a
-        Assert.assertEquals(100.1f, RowValue3.get(1)); // b
-        Assert.assertEquals(100L, RowValue3.get(2)); // c
-        Assert.assertEquals(5000.0, RowValue3.get(3)); // d
-        Assert.assertEquals("something", RowValue3.get(4)); // e
-        Assert.assertEquals("something", RowValue3.get(5).toString());// f
-        Assert.assertEquals("" + 0, ((Tuple) RowValue3.get(6))
-            .get(0));// r
-        Assert.assertEquals("" + 0, ((Tuple) RowValue3.get(6))
-            .get(1));// r
-        Assert.assertEquals("" + 0, ((Map) RowValue3.get(7))
-            .get("a"));// m
-        Assert.assertEquals("" + 0, ((Map) RowValue3.get(7))
-            .get("b"));// m
-        Assert.assertEquals("" + 0, ((Map) RowValue3.get(7))
-            .get("c"));// m
-        Assert.assertEquals(100, RowValue3.get(15)); // a
-        Assert.assertEquals(100.1f, RowValue3.get(14)); // b
-        Assert.assertEquals(100L, RowValue3.get(13)); // c
-        Assert.assertEquals(5000.0, RowValue3.get(12)); // d
-        Assert.assertEquals("something", RowValue3.get(11)); // e
-        Assert.assertEquals("something", RowValue3.get(10).toString());// f
-        Assert.assertEquals("" + 0, ((Tuple) RowValue3.get(9)).get(0));// r
-        Assert.assertEquals("" + 0, ((Tuple) RowValue3.get(9)).get(1));// r
-        Assert.assertEquals("" + 0, ((Map) RowValue3.get(8)).get("a"));// m
-        Assert.assertEquals("" + 0, ((Map) RowValue3.get(8)).get("b"));// m
-        Assert.assertEquals("" + 0, ((Map) RowValue3.get(8)).get("c"));// m
-      }
-    }
-//    Assert.assertEquals(2, row);
-  }
-
-  public Iterator<Tuple> joinTable(String table1, String table2, String sortkey1, String sortkey2) throws IOException {
-    String query1 = "records1 = LOAD '" + pathTable1.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    System.out.println("query1:" + query1);
-    pigServer.registerQuery(query1);
-
-    String query2 = "records2 = LOAD '" + pathTable2.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    System.out.println("query2:" + query2);
-    pigServer.registerQuery(query2);
-
-    String orderby1 = "sort1 = ORDER records1 BY " + sortkey1 + " ;";
-    String orderby2 = "sort2 = ORDER records2 BY " + sortkey2 + " ;";
-    pigServer.registerQuery(orderby1);
-    pigServer.registerQuery(orderby2);
-
-   
-    t1++;
-    
-    String table1path = pathTable1.toString() + Integer.toString(t1);
-    removeDir(new Path(table1path));
-
-ExecJob pigJob =pigServer.store("sort1", table1path, TableStorer.class.getCanonicalName()
-        + "('[a, b, c]; [d, e, f, r1, m1]')");
-if (pigJob.getException() != null){
-  System.out.println("******pig job exception"+ pigJob.getException().getMessage());
-}
-Assert.assertNull(pigJob.getException());
-    String query3 = "records1 = LOAD '"
-        + table1path
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('a, b, c, d, e, f, r1, m1', 'sorted');";
-
-    System.out.println("query3:" + query3);
-    pigServer.registerQuery(query3);   
-    
-    String foreach = "records11 = foreach records1 generate a as a, b as b, c as c, d as d, e as e, f as f, r1 as r1, m1#'a' as ma1;";
-    pigServer.registerQuery(foreach);
-  
-    /*
-     * Table2 creation
-     */
-    this.t1++;
-    String table2path = this.pathTable2.toString() + Integer.toString(this.t1);
-    removeDir(new Path(table2path));
-    pigJob = pigServer.store("sort2", table2path, TableStorer.class.getCanonicalName()
-        + "('[a, b, c]; [d,e,f,r1,m1]')");
-    if (pigJob.getException() != null){
-      System.out.println("******pig job exception"+ pigJob.getException().getMessage());
-    }
-    Assert.assertNull(pigJob.getException());
-    String query4 = "records2 = LOAD '" + table2path
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    pigServer.registerQuery(query4);
-
-    
-    String filter = "records22 = foreach records2 generate " + sortkey2 + ";" ;
-    pigServer.registerQuery(filter);
-   
-    String join = "joinRecords = JOIN records11 BY " + "(" + sortkey1 + ")" 
-        + " , records22 BY " + "("+ sortkey2 + ")"+" USING \"merge\";";
-   //TODO: can not use records22
-      pigServer.registerQuery(join);
-   
-    // check JOIN content
-    Iterator<Tuple> it3 = pigServer.openIterator("joinRecords");
-    return it3;
-  }
-  
-
-  
-  @Test
-  public void test1() throws ExecException, IOException {
-    /*
-     * join key: single integer column
-     */
-    Iterator<Tuple> it3 =  joinTable(this.pathTable1.toString(), this.pathTable2.toString(), "a","a" );
-    verify (it3);
-  }
-
-//  @Test
-  public void test2() throws ExecException, IOException {
-    /*
-     * join key: single float column
-     */
-    Iterator<Tuple> it3 = joinTable(this.pathTable1.toString(), this.pathTable2.toString(), "b","b" ); 
-    verify(it3);
-  }
-
-//  @Test
-  public void test3() throws ExecException, IOException {
-    /*
-     * join key: single string column
-     */
-    Iterator<Tuple> it3 = joinTable(this.pathTable1.toString(), this.pathTable2.toString(), "e","e" );
-    verify(it3);
-  }
-
-//  @Test
-  public void test4() throws ExecException, IOException {
-    /*
-     * join key: single byte column
-     */
-    Iterator<Tuple> it3 = joinTable(this.pathTable1.toString(), this.pathTable2.toString(), "f","f" );
-    verify(it3);
-  }
-  
-//  @Test
-  public void test5() throws ExecException, IOException {
-    /*
-     * join key: single double column
-     */
-    Iterator<Tuple> it3 = joinTable(this.pathTable1.toString(), this.pathTable2.toString(), "d","d" );
-    verify(it3);
-  }
-
-//  @Test
-  public void test6() throws ExecException, IOException {
-    /*
-     * join key: single long column
-     */
-    Iterator<Tuple> it3 = joinTable(this.pathTable1.toString(), this.pathTable2.toString(), "c","c" );
-    verify(it3);
-  }
-  
-//  @Test
-  public void test7() throws ExecException, IOException {
-    /*
-     *  2 join keys: integer and float
-     */
-    System.out.println ("helloo");
-    Iterator<Tuple> it3 = joinTable(this.pathTable1.toString(), this.pathTable2.toString(), "a,b","a,b" );
-    verify(it3);
-  }
-  
-//  @Test
-  public void test8() throws ExecException, IOException {
-    /*
-     * multiple join keys: integer, float, long, double, string, bytes
-     */
-	  
-    Iterator<Tuple> it3 = joinTable(this.pathTable1.toString(), this.pathTable2.toString(), "a,b,c,d,e,f","a,b,c,d,e,f" );
-    verify(it3);
-  }
-  
-  //@Test(expected = IOException.class)
-  public void test9a() throws ExecException, IOException {
-    /*
-     * Negative test case, one join key is not primitive type which is a record
-     * 2 join keys: integer and record
-     */
-    Iterator<Tuple> it3 = joinTable(this.pathTable1.toString(), this.pathTable2.toString(), "a,r1,e","a,r1,e");
-  }
-  
-  //@Test(expected = IOException.class)
-  public void test9b() throws ExecException, IOException {
-    /*
-     * Negative test case, one join key is not primitive type which is a record
-     * 2 join keys: integer and map
-     */
-    Iterator<Tuple> it3 = joinTable(this.pathTable1.toString(), this.pathTable2.toString(), "a,m1,e","a,m1,e");
-  }
-  
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMixedType1.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMixedType1.java
deleted file mode 100644
index 56e7280de..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMixedType1.java
+++ /dev/null
@@ -1,288 +0,0 @@
-package org.apache.hadoop.zebra.pig;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.Map;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * 
- * Test projections on complicated column types.
- * 
- */
-public class TestMixedType1 extends BaseTestCase {
-  final static String STR_SCHEMA = "s1:bool, s2:int, s3:long, s4:double, s5:string, s6:bytes, r1:record(f1:int, f2:long), r2:record(r3:record(f3:double, f4)), m1:map(string),m2:map(map(int)), c:collection(record(f13:double, f14:double, f15:bytes))";
-  final static String STR_STORAGE = "[s1, s2]; [m1#{a}]; [r1.f1]; [s3, s4, r2.r3.f3]; [s5, s6, m2#{x|y}]; [r1.f2, m1#{b}]; [r2.r3.f4, m2#{z}]";
-
-  private static Path path;
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException, Exception {
-    System.out.println("ONCE SETUP !! ---------");
-    init();
-    
-    path = getTableFullPath("TestMixedType1");  
-    removeDir(path);
-    
-    BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-        STR_STORAGE, conf);
-    writer.finish();
-
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-    BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-    TypesUtils.resetTuple(tuple);
-
-    Tuple tupRecord1;
-    try {
-      tupRecord1 = TypesUtils.createTuple(schema.getColumnSchema("r1")
-          .getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    Tuple tupRecord2;
-    try {
-      tupRecord2 = TypesUtils.createTuple(schema.getColumnSchema("r2")
-          .getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    Tuple tupRecord3;
-    try {
-      tupRecord3 = TypesUtils.createTuple(new Schema("f3:float, f4"));
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    // row 1
-    tuple.set(0, true); // bool
-    tuple.set(1, 1); // int
-    tuple.set(2, 1001L); // long
-    tuple.set(3, 1.1); // float
-    tuple.set(4, "hello world 1"); // string
-    tuple.set(5, new DataByteArray("hello byte 1")); // byte
-
-    // r1:record(f1:int, f2:long
-    tupRecord1.set(0, 1);
-    tupRecord1.set(1, 1001L);
-    tuple.set(6, tupRecord1);
-
-    // r2:record(r3:record(f3:float, f4))
-    tupRecord2.set(0, tupRecord3);
-    tupRecord3.set(0, 1.3);
-    tupRecord3.set(1, new DataByteArray("r3 row 1 byte array "));
-    tuple.set(7, tupRecord2);
-
-    // m1:map(string)
-    Map<String, String> m1 = new HashMap<String, String>();
-    m1.put("a", "A");
-    m1.put("b", "B");
-    m1.put("c", "C");
-    tuple.set(8, m1);
-
-    // m2:map(map(int))
-    HashMap<String, Map> m2 = new HashMap<String, Map>();
-    Map<String, Integer> m3 = new HashMap<String, Integer>();
-    m3.put("m311", 311);
-    m3.put("m321", 321);
-    m3.put("m331", 331);
-    Map<String, Integer> m4 = new HashMap<String, Integer>();
-    m4.put("m411", 411);
-    m4.put("m421", 421);
-    m4.put("m431", 431);
-    m2.put("x", m3);
-    m2.put("y", m4);
-    tuple.set(9, m2);
-
-    // c:collection(f13:double, f14:float, f15:bytes)
-    DataBag bagColl = TypesUtils.createBag();
-    Schema schColl = schema.getColumn(10).getSchema().getColumn(0).getSchema();
-    Tuple tupColl1 = TypesUtils.createTuple(schColl);
-    Tuple tupColl2 = TypesUtils.createTuple(schColl);
-    byte[] abs1 = new byte[3];
-    byte[] abs2 = new byte[4];
-    tupColl1.set(0, 3.1415926);
-    tupColl1.set(1, 1.6);
-    abs1[0] = 11;
-    abs1[1] = 12;
-    abs1[2] = 13;
-    tupColl1.set(2, new DataByteArray(abs1));
-    bagColl.add(tupColl1);
-    tupColl2.set(0, 123.456789);
-    tupColl2.set(1, 100);
-    abs2[0] = 21;
-    abs2[1] = 22;
-    abs2[2] = 23;
-    abs2[3] = 24;
-    tupColl2.set(2, new DataByteArray(abs2));
-    bagColl.add(tupColl2);
-    tuple.set(10, bagColl);
-
-    int row = 0;
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // row 2
-    row++;
-    TypesUtils.resetTuple(tuple);
-    TypesUtils.resetTuple(tupRecord1);
-    TypesUtils.resetTuple(tupRecord2);
-    TypesUtils.resetTuple(tupRecord3);
-    m1.clear();
-    m2.clear();
-    m3.clear();
-    m4.clear();
-    tuple.set(0, false);
-    tuple.set(1, 2); // int
-    tuple.set(2, 1002L); // long
-    tuple.set(3, 3.1); // float
-    tuple.set(4, "hello world 2"); // string
-    tuple.set(5, new DataByteArray("hello byte 2")); // byte
-
-    // r1:record(f1:int, f2:long
-    tupRecord1.set(0, 2);
-    tupRecord1.set(1, 1002L);
-    tuple.set(6, tupRecord1);
-
-    // r2:record(r3:record(f3:float, f4))
-    tupRecord2.set(0, tupRecord3);
-    tupRecord3.set(0, 2.3);
-    tupRecord3.set(1, new DataByteArray("r3 row2  byte array"));
-    tuple.set(7, tupRecord2);
-
-    // m1:map(string)
-    m1.put("a2", "A2");
-    m1.put("b2", "B2");
-    m1.put("c2", "C2");
-    tuple.set(8, m1);
-
-    // m2:map(map(int))
-    m3.put("m321", 321);
-    m3.put("m322", 322);
-    m3.put("m323", 323);
-    m2.put("z", m3);
-    tuple.set(9, m2);
-
-    // c:collection(f13:double, f14:float, f15:bytes)
-    bagColl.clear();
-    TypesUtils.resetTuple(tupColl1);
-    TypesUtils.resetTuple(tupColl2);
-    tupColl1.set(0, 7654.321);
-    tupColl1.set(1, 0.0001);
-    abs1[0] = 31;
-    abs1[1] = 32;
-    abs1[2] = 33;
-    tupColl1.set(2, new DataByteArray(abs1));
-    bagColl.add(tupColl1);
-    tupColl2.set(0, 0.123456789);
-    tupColl2.set(1, 0.3333);
-    abs2[0] = 41;
-    abs2[1] = 42;
-    abs2[2] = 43;
-    abs2[3] = 44;
-    tupColl2.set(2, new DataByteArray(abs2));
-    bagColl.add(tupColl2);
-    tuple.set(10, bagColl);
-
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    inserter.close();
-    writer1.finish();
-
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-    BasicTable.drop(path, conf);
-  }
-
-  @Test
-  public void test1() throws IOException, ParseException {
-    String query = "records = LOAD '" + path.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('r1.f2, s1');";
-    System.out.println(query);
-    pigServer.registerQuery(query);
-    Iterator<Tuple> it = pigServer.openIterator("records");
-    BytesWritable key = new BytesWritable();
-    int row = 0;
-    while (it.hasNext()) {
-      Tuple RowValue = it.next();
-      System.out.println(RowValue);
-      row++;
-      if (row == 1) {
-        Assert.assertEquals(1001L, RowValue.get(0));
-        Assert.assertEquals(true, RowValue.get(1));
-      }
-      if (row == 2) {
-        Assert.assertEquals(1002L, RowValue.get(0));
-        Assert.assertEquals(false, RowValue.get(1));
-      }
-    }
-  }
-
-  @Test
-  public void testStitch() throws IOException, ParseException {
-    String query = "records = LOAD '" + path.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('s1, r1');";
-    System.out.println(query);
-    pigServer.registerQuery(query);
-    Iterator<Tuple> it = pigServer.openIterator("records");
-    BytesWritable key = new BytesWritable();
-    int row = 0;
-    while (it.hasNext()) {
-      Tuple RowValue = it.next();
-      System.out.println(RowValue);
-      row++;
-      if (row == 1) {
-        Tuple recordTuple = (Tuple) RowValue.get(1);
-        Assert.assertEquals(1, recordTuple.get(0));
-        Assert.assertEquals(1001L, recordTuple.get(1));
-        Assert.assertEquals(true, RowValue.get(0));
-      }
-
-    }
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMultipleOutputs1.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMultipleOutputs1.java
deleted file mode 100644
index 07debf36b..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMultipleOutputs1.java
+++ /dev/null
@@ -1,494 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.BufferedWriter;
-import java.io.FileWriter;
-import java.io.IOException;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.mapreduce.ZebraOutputPartition;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.pig.backend.executionengine.ExecJob;
-import org.apache.pig.data.Tuple;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Test;
-import java.util.Iterator;
-import junit.framework.Assert;
-
-/**
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * us 2
- * japan 2
- * india 4
- * us 2
- * japan 1
- * india 3
- * nouse 5
- * nowhere 4
- * 
- */
-public class TestMultipleOutputs1 extends BaseTestCase implements Tool {
-  static String inputPath;
-  static String inputFileName = "multi-input.txt";
-  public static String sortKey = null;
-
-  @Before
-  public void setUp() throws Exception {
-    init();
-    
-    inputPath = getTableFullPath(inputFileName).toString();
-    
-    writeToFile(inputPath);
-  }
-  
-  @After
-  public void tearDown() throws Exception {
-    if (mode == TestMode.local) {
-      pigServer.shutdown();
-    }
-  }
-  
-  public static void writeToFile (String inputFile) throws IOException{
-    if (mode == TestMode.local) {
-      FileWriter fstream = new FileWriter(inputFile);
-      BufferedWriter out = new BufferedWriter(fstream);
-      out.write("us\t2\n");
-      out.write("japan\t2\n");
-      out.write("india\t4\n");
-      out.write("us\t2\n");
-      out.write("japan\t1\n");
-      out.write("india\t3\n");
-      out.write("nouse\t5\n");
-      out.write("nowhere\t4\n");
-      out.close();
-    }
-
-    if (mode == TestMode.cluster) {
-      FSDataOutputStream fout = fs.create(new Path (inputFile));
-      fout.writeBytes("us\t2\n");
-      fout.writeBytes("japan\t2\n");
-      fout.writeBytes("india\t4\n");
-      fout.writeBytes("us\t2\n");
-      fout.writeBytes("japan\t1\n");
-      fout.writeBytes("india\t3\n");
-      fout.writeBytes("nouse\t5\n");
-      fout.writeBytes("nowhere\t4\n");
-      fout.close();
-    }
-  }
-  
-  // test no sort key;
-  @Test
-  public void test1() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    // Load data;
-    String query = "records = LOAD '" + inputPath + "' as (word:chararray, count:int);";
-    System.out.println("query = " + query);
-    pigServer.registerQuery(query);
-
-    Iterator<Tuple> it = pigServer.openIterator("records");
-    while (it.hasNext()) {
-      Tuple cur = it.next();
-      System.out.println(cur);
-    }
-    
-    // Store using multiple outputs;
-    String outputPaths = "us,india,japan";
-    removeDir(getTableFullPath("us"));
-    removeDir(getTableFullPath("india"));
-    removeDir(getTableFullPath("japan"));
-
-    query = "store records into '" + outputPaths + "' using org.apache.hadoop.zebra.pig.TableStorer('[word,count]'," +
-        "'org.apache.hadoop.zebra.pig.TestMultipleOutputs1$OutputPartitionerClass');";
-    System.out.println("query = " + query);
-    pigServer.registerQuery(query);    
-    
-    // Validate results;
-    query = "records = LOAD '" + "us"
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-
-    int count = 0;
-    System.out.println(query);
-    pigServer.registerQuery(query);
-    it = pigServer.openIterator("records");
-    while (it.hasNext()) {
-      count ++;
-      Tuple RowValue = it.next();
-      System.out.println(RowValue);
-      if (count == 1) {
-        Assert.assertEquals("us", RowValue.get(0));
-        Assert.assertEquals(2, RowValue.get(1));       
-      } else if (count == 2) {
-        Assert.assertEquals("us", RowValue.get(0));
-        Assert.assertEquals(2, RowValue.get(1));                 
-      } else if (count == 3) {
-        Assert.assertEquals("nouse", RowValue.get(0));
-        Assert.assertEquals(5, RowValue.get(1));       
-      } else if (count == 4) {
-        Assert.assertEquals("nowhere", RowValue.get(0));
-        Assert.assertEquals(4, RowValue.get(1));       
-      }
-    }
-    Assert.assertEquals(count, 4);
-
-    query = "records = LOAD '" + "india"
-      + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-
-    count = 0;
-    System.out.println(query);
-    pigServer.registerQuery(query);
-    it = pigServer.openIterator("records");
-    while (it.hasNext()) {
-      count ++;
-      Tuple RowValue = it.next();
-      System.out.println(RowValue);
-      if (count == 1) {
-        Assert.assertEquals("india", RowValue.get(0));
-        Assert.assertEquals(4, RowValue.get(1));       
-      } else if (count == 2) {
-        Assert.assertEquals("india", RowValue.get(0));
-        Assert.assertEquals(3, RowValue.get(1));                 
-      } 
-    }
-    Assert.assertEquals(count, 2);
-
-    query = "records = LOAD '" + "japan"
-    + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-
-    count = 0;
-    System.out.println(query);
-    pigServer.registerQuery(query);
-    it = pigServer.openIterator("records");
-    while (it.hasNext()) {
-      count ++;
-      Tuple RowValue = it.next();
-      System.out.println(RowValue);
-      if (count == 1) {
-        Assert.assertEquals("japan", RowValue.get(0));
-        Assert.assertEquals(2, RowValue.get(1));       
-      } else if (count == 2) {
-        Assert.assertEquals("japan", RowValue.get(0));
-        Assert.assertEquals(1, RowValue.get(1));                 
-      } 
-    }
-    Assert.assertEquals(count, 2);
-  }    
-
-  //Test sort key on word;
-  @Test
-  public void test2() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    // Load data;
-    String query = "a = LOAD '" + inputPath + "' as (word:chararray, count:int);";
-    System.out.println("query = " + query);
-    pigServer.registerQuery(query);
-    
-    query = "records = order a by word;";
-    System.out.println("query = " + query);
-    pigServer.registerQuery(query);
-
-    Iterator<Tuple> it = pigServer.openIterator("records");
-    while (it.hasNext()) {
-      Tuple cur = it.next();
-      System.out.println(cur);
-    }
-    
-    // Store using multiple outputs;
-    String outputPaths = "us,india,japan";
-    removeDir(getTableFullPath("us"));
-    removeDir(getTableFullPath("india"));
-    removeDir(getTableFullPath("japan"));
-    ExecJob pigJob = pigServer
-      .store(
-        "records",
-        outputPaths,
-        TableStorer.class.getCanonicalName() +
-             "('[word,count]', 'org.apache.hadoop.zebra.pig.TestMultipleOutputs1$OutputPartitionerClass')");    
-    
-    Assert.assertNull(pigJob.getException());
-    
-    // Validate results;
-    query = "records = LOAD '" + "us"
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-
-    int count = 0;
-    System.out.println(query);
-    pigServer.registerQuery(query);
-    it = pigServer.openIterator("records");
-    while (it.hasNext()) {
-      count ++;
-      Tuple RowValue = it.next();
-      System.out.println(RowValue);
-      if (count == 1) {
-        Assert.assertEquals("nouse", RowValue.get(0));
-        Assert.assertEquals(5, RowValue.get(1));       
-      } else if (count == 2) {
-        Assert.assertEquals("nowhere", RowValue.get(0));
-        Assert.assertEquals(4, RowValue.get(1));                 
-      } else if (count == 3) {
-        Assert.assertEquals("us", RowValue.get(0));
-        Assert.assertEquals(2, RowValue.get(1));       
-      } else if (count == 4) {
-        Assert.assertEquals("us", RowValue.get(0));
-        Assert.assertEquals(2, RowValue.get(1));       
-      }
-    }
-    Assert.assertEquals(count, 4);
-
-    query = "records = LOAD '" + "india"
-      + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-
-    count = 0;
-    System.out.println(query);
-    pigServer.registerQuery(query);
-    it = pigServer.openIterator("records");
-    while (it.hasNext()) {
-      count ++;
-      Tuple RowValue = it.next();
-      System.out.println(RowValue);
-      if (count == 1) {
-        Assert.assertEquals("india", RowValue.get(0));
-        Assert.assertEquals(4, RowValue.get(1));       
-      } else if (count == 2) {
-        Assert.assertEquals("india", RowValue.get(0));
-        Assert.assertEquals(3, RowValue.get(1));                 
-      } 
-    }
-    Assert.assertEquals(count, 2);
-
-    query = "records = LOAD '" + "japan"
-    + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-
-    count = 0;
-    System.out.println(query);
-    pigServer.registerQuery(query);
-    it = pigServer.openIterator("records");
-    while (it.hasNext()) {
-      count ++;
-      Tuple RowValue = it.next();
-      System.out.println(RowValue);
-      if (count == 1) {
-        Assert.assertEquals("japan", RowValue.get(0));
-        Assert.assertEquals(2, RowValue.get(1));       
-      } else if (count == 2) {
-        Assert.assertEquals("japan", RowValue.get(0));
-        Assert.assertEquals(1, RowValue.get(1));                 
-      } 
-    }
-    Assert.assertEquals(count, 2);
-  }
-  
-  //Test sort key on word and count;
-  @Test
-  public void test3() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    // Load data;
-    String query = "a = LOAD '" + inputPath + "' as (word:chararray, count:int);";
-    System.out.println("query = " + query);
-    pigServer.registerQuery(query);
-    
-    query = "records = order a by word, count;";
-    System.out.println("query = " + query);
-    pigServer.registerQuery(query);
-
-    Iterator<Tuple> it = pigServer.openIterator("records");
-    while (it.hasNext()) {
-      Tuple cur = it.next();
-      System.out.println(cur);
-    }
-    
-    // Store using multiple outputs;
-    String outputPaths = "us,india,japan";
-    removeDir(getTableFullPath("us"));
-    removeDir(getTableFullPath("india"));
-    removeDir(getTableFullPath("japan"));
-    ExecJob pigJob = pigServer
-      .store(
-        "records",
-        outputPaths,
-        TableStorer.class.getCanonicalName() +
-             "('[word,count]', 'org.apache.hadoop.zebra.pig.TestMultipleOutputs1$OutputPartitionerClass')");    
-    
-    Assert.assertNull(pigJob.getException());
-    
-    // Validate results;
-    query = "records = LOAD '" + "us"
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-
-    int count = 0;
-    System.out.println(query);
-    pigServer.registerQuery(query);
-    it = pigServer.openIterator("records");
-    while (it.hasNext()) {
-      count ++;
-      Tuple RowValue = it.next();
-      System.out.println(RowValue);
-      if (count == 1) {
-        Assert.assertEquals("nouse", RowValue.get(0));
-        Assert.assertEquals(5, RowValue.get(1));       
-      } else if (count == 2) {
-        Assert.assertEquals("nowhere", RowValue.get(0));
-        Assert.assertEquals(4, RowValue.get(1));                 
-      } else if (count == 3) {
-        Assert.assertEquals("us", RowValue.get(0));
-        Assert.assertEquals(2, RowValue.get(1));       
-      } else if (count == 4) {
-        Assert.assertEquals("us", RowValue.get(0));
-        Assert.assertEquals(2, RowValue.get(1));       
-      }
-    }
-    Assert.assertEquals(count, 4);
-
-    query = "records = LOAD '" + "india"
-      + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-
-    count = 0;
-    System.out.println(query);
-    pigServer.registerQuery(query);
-    it = pigServer.openIterator("records");
-    while (it.hasNext()) {
-      count ++;
-      Tuple RowValue = it.next();
-      System.out.println(RowValue);
-      if (count == 1) {
-        Assert.assertEquals("india", RowValue.get(0));
-        Assert.assertEquals(3, RowValue.get(1));       
-      } else if (count == 2) {
-        Assert.assertEquals("india", RowValue.get(0));
-        Assert.assertEquals(4, RowValue.get(1));                 
-      } 
-    }
-    Assert.assertEquals(count, 2);
-
-    query = "records = LOAD '" + "japan"
-    + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-
-    count = 0;
-    System.out.println(query);
-    pigServer.registerQuery(query);
-    it = pigServer.openIterator("records");
-    while (it.hasNext()) {
-      count ++;
-      Tuple RowValue = it.next();
-      System.out.println(RowValue);
-      if (count == 1) {
-        Assert.assertEquals("japan", RowValue.get(0));
-        Assert.assertEquals(1, RowValue.get(1));       
-      } else if (count == 2) {
-        Assert.assertEquals("japan", RowValue.get(0));
-        Assert.assertEquals(2, RowValue.get(1));                 
-      } 
-    }
-    Assert.assertEquals(count, 2);
-  }
-  
-  //Negative test case: invalid partition class;
-  @Test (expected = IOException.class)
-  public void testNegative1() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    // Load data;
-    String query = "a = LOAD '" + inputPath + "' as (word:chararray, count:int);";
-    System.out.println("query = " + query);
-    pigServer.registerQuery(query);
-    
-    query = "records = order a by word, count;";
-    System.out.println("query = " + query);
-    pigServer.registerQuery(query);
-
-    Iterator<Tuple> it = pigServer.openIterator("records");
-    while (it.hasNext()) {
-      Tuple cur = it.next();
-      System.out.println(cur);
-    }
-    
-    // Store using multiple outputs;
-    String outputPaths = "us,india,japan";
-    removeDir(getTableFullPath("us"));
-    removeDir(getTableFullPath("india"));
-    removeDir(getTableFullPath("japan"));
-    pigServer
-      .store(
-        "records",
-        outputPaths,
-        TableStorer.class.getCanonicalName() +
-             "('[word,count]', 'org.apache.hadoop.zebra.pig.notexistingclass')");    
-  }
-  
-  public static class OutputPartitionerClass extends ZebraOutputPartition {
-
-    @Override
-    public int getOutputPartition(BytesWritable key, Tuple value) {
-      String reg = null;
-      try {
-        reg = (String) (value.get(0));
-      } catch (Exception e) {
-        //
-      }
-
-      if (reg.equals("us"))
-        return 0;
-      if (reg.equals("india"))
-        return 1;
-      if (reg.equals("japan"))
-        return 2;
-
-      return 0;
-    }
-  }
-
-  @Override
-  public int run(String[] args) throws Exception {
-    TestMultipleOutputs1 test = new TestMultipleOutputs1();
-    
-    test.setUp();
-    test.test1();
-    test.tearDown();
-    
-    test.setUp();
-    test.test2();
-    test.tearDown();    
-
-    test.setUp();
-    test.test3();
-    test.tearDown();    
-
-    test.setUp();
-    test.testNegative1();
-    test.tearDown();
-    
-    return 0;
-  }
-  
-  public static void main(String[] args) throws Exception {
-    conf = new Configuration();
-    
-    int res = ToolRunner.run(conf, new TestMultipleOutputs1(), args);
-    System.out.println("PASS");
-    System.exit(res);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMultipleOutputs2.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMultipleOutputs2.java
deleted file mode 100644
index b3b31e59b..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestMultipleOutputs2.java
+++ /dev/null
@@ -1,250 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.BufferedWriter;
-import java.io.FileWriter;
-import java.io.IOException;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.mapreduce.BasicTableOutputFormat;
-import org.apache.hadoop.zebra.mapreduce.ZebraOutputPartition;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.pig.backend.executionengine.ExecJob;
-import org.apache.pig.data.Tuple;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Test;
-import java.util.Iterator;
-import junit.framework.Assert;
-
-/**
- * Assume the input files contain rows of word and count, separated by a space:
- * 
- * <pre>
- * us 2
- * japan 2
- * india 4
- * us 2
- * japan 1
- * india 3
- * nouse 5
- * nowhere 4
- * 
- */
-public class TestMultipleOutputs2 extends BaseTestCase implements Tool {
-  static String inputPath;
-  static String inputFileName = "multi-input.txt";
-  public static String sortKey = null;
-
-  @Before
-  public void setUp() throws Exception {
-    init();
-    
-    inputPath = getTableFullPath(inputFileName).toString();
-    
-    writeToFile(inputPath);
-  }
-  
-  @After
-  public void tearDown() throws Exception {
-    if (mode == TestMode.local) {
-      pigServer.shutdown();
-    }
-  }
-  
-  public static void writeToFile (String inputFile) throws IOException{
-    if (mode == TestMode.local) {
-      FileWriter fstream = new FileWriter(inputFile);
-      BufferedWriter out = new BufferedWriter(fstream);
-      out.write("us\t2\n");
-      out.write("japan\t2\n");
-      out.write("india\t4\n");
-      out.write("us\t2\n");
-      out.write("japan\t1\n");
-      out.write("india\t3\n");
-      out.write("nouse\t5\n");
-      out.write("nowhere\t4\n");
-      out.close();
-    }
-
-    if (mode == TestMode.cluster) {
-      FSDataOutputStream fout = fs.create(new Path (inputFile));
-      fout.writeBytes("us\t2\n");
-      fout.writeBytes("japan\t2\n");
-      fout.writeBytes("india\t4\n");
-      fout.writeBytes("us\t2\n");
-      fout.writeBytes("japan\t1\n");
-      fout.writeBytes("india\t3\n");
-      fout.writeBytes("nouse\t5\n");
-      fout.writeBytes("nowhere\t4\n");
-      fout.close();
-    }
-  }
-
-  @Test
-  public void test1() throws ParseException, IOException,
-      org.apache.hadoop.zebra.parser.ParseException, Exception {
-    // Load data;
-    String query = "records = LOAD '" + inputPath + "' as (word:chararray, count:int);";
-    System.out.println("query = " + query);
-    pigServer.registerQuery(query);    
-
-    Iterator<Tuple> it = pigServer.openIterator("records");
-    while (it.hasNext()) {
-      Tuple cur = it.next();
-      System.out.println(cur);
-    }
-    
-    // Store using multiple outputs;
-    String outputPaths = "us_0,india_1,japan_2";
-    removeDir(getTableFullPath("us_0"));
-    removeDir(getTableFullPath("india_1"));
-    removeDir(getTableFullPath("japan_2"));
-    ExecJob pigJob = pigServer
-      .store(
-        "records",
-        outputPaths,
-        TableStorer.class.getCanonicalName() +
-             "('[word,count]', 'org.apache.hadoop.zebra.pig.TestMultipleOutputs2$OutputPartitionerClass', 'us,india,japan')");    
-    
-    Assert.assertNull(pigJob.getException());
-    
-    // Validate results;
-    query = "records = LOAD '" + "us_0"
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-
-    int count = 0;
-    System.out.println(query);
-    pigServer.registerQuery(query);
-    it = pigServer.openIterator("records");
-    while (it.hasNext()) {
-      count ++;
-      Tuple RowValue = it.next();
-      System.out.println(RowValue);
-      if (count == 1) {
-        Assert.assertEquals("us", RowValue.get(0));
-        Assert.assertEquals(2, RowValue.get(1));       
-      } else if (count == 2) {
-        Assert.assertEquals("us", RowValue.get(0));
-        Assert.assertEquals(2, RowValue.get(1));                 
-      } else if (count == 3) {
-        Assert.assertEquals("nouse", RowValue.get(0));
-        Assert.assertEquals(5, RowValue.get(1));       
-      } else if (count == 4) {
-        Assert.assertEquals("nowhere", RowValue.get(0));
-        Assert.assertEquals(4, RowValue.get(1));       
-      }
-    }
-    Assert.assertEquals(count, 4);
-
-    query = "records = LOAD '" + "india_1"
-      + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-
-    count = 0;
-    System.out.println(query);
-    pigServer.registerQuery(query);
-    it = pigServer.openIterator("records");
-    while (it.hasNext()) {
-      count ++;
-      Tuple RowValue = it.next();
-      System.out.println(RowValue);
-      if (count == 1) {
-        Assert.assertEquals("india", RowValue.get(0));
-        Assert.assertEquals(4, RowValue.get(1));       
-      } else if (count == 2) {
-        Assert.assertEquals("india", RowValue.get(0));
-        Assert.assertEquals(3, RowValue.get(1));                 
-      } 
-    }
-    Assert.assertEquals(count, 2);
-
-    query = "records = LOAD '" + "japan_2"
-    + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-
-    count = 0;
-    System.out.println(query);
-    pigServer.registerQuery(query);
-    it = pigServer.openIterator("records");
-    while (it.hasNext()) {
-      count ++;
-      Tuple RowValue = it.next();
-      System.out.println(RowValue);
-      if (count == 1) {
-        Assert.assertEquals("japan", RowValue.get(0));
-        Assert.assertEquals(2, RowValue.get(1));       
-      } else if (count == 2) {
-        Assert.assertEquals("japan", RowValue.get(0));
-        Assert.assertEquals(1, RowValue.get(1));                 
-      } 
-    }
-    Assert.assertEquals(count, 2);
-  }
-
-  public static class OutputPartitionerClass extends ZebraOutputPartition {
-
-    @Override
-    public int getOutputPartition(BytesWritable key, Tuple value) {
-      String reg = null;
-      
-      try {
-        reg = (String) (value.get(0));
-      } catch (Exception e) {
-        //
-      }
-      
-      String argumentsString = BasicTableOutputFormat.getOutputPartitionClassArguments(conf);
-      
-      String[] arguments = argumentsString.split(",");
-
-      if (reg.equals(arguments[0]))
-        return 0;
-      if (reg.equals(arguments[1]))
-        return 1;
-      if (reg.equals(arguments[2]))
-        return 2;
-
-      return 0;
-    }
-  }
-
-  @Override
-  public int run(String[] args) throws Exception {
-    TestMultipleOutputs2 test = new TestMultipleOutputs2();
-
-    test.setUp();
-    test.test1();
-    test.tearDown();
-
-    return 0;
-  }
-  
-  public static void main(String[] args) throws Exception {
-    conf = new Configuration();
-    
-    int res = ToolRunner.run(conf, new TestMultipleOutputs2(), args);
-    System.out.println("PASS");
-    System.exit(res);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestOrderPreserveMultiTable.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestOrderPreserveMultiTable.java
deleted file mode 100644
index 7481a7cca..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestOrderPreserveMultiTable.java
+++ /dev/null
@@ -1,234 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.ArrayList;
-import java.util.StringTokenizer;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.pig.TableStorer;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.executionengine.ExecJob;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-
-import junit.framework.Assert;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-
-public class TestOrderPreserveMultiTable extends BaseTestCase {
-	
-	final static int NUMB_TABLE = 10;  		// number of tables for stress test
-	final static int NUMB_TABLE_ROWS = 5;	// number of rows for each table
-	
-	final static String TABLE_SCHEMA = "int1:int,str1:string,byte1:bytes";
-	final static String TABLE_STORAGE = "[int1,str1,byte1]";
-	
-	static int fileId = 0;
-	static int sortId = 0;
-	
-	protected static ExecJob pigJob;
-	
-	private static ArrayList<Path> pathTables;
-	private static int totalTableRows =0;
-	
-	@BeforeClass
-	public static void setUp() throws Exception {
-    init();
-			
-		pathTables = new ArrayList<Path>();
-		for (int i=0; i<NUMB_TABLE; ++i) {
-		  Path pathTable = getTableFullPath("TestOderPerserveMultiTable" + i);
-			pathTables.add(pathTable);
-			removeDir(pathTable);
-		}
-		
-		// Create tables
-		for (int i=0; i<NUMB_TABLE; ++i) {
-			// Create table data
-			Object[][] table = new Object[NUMB_TABLE_ROWS][3];  // three columns
-			
-			for (int j=0; j<NUMB_TABLE_ROWS; ++j) {
-				table[j][0] = i;
-				table[j][1] = new String("string" + j);
-				table[j][2] = new DataByteArray("byte" + (NUMB_TABLE_ROWS - j));
-				++totalTableRows;
-			}
-			// Create table
-			createTable(pathTables.get(i), TABLE_SCHEMA, TABLE_STORAGE, table);
-			
-			// Load Table
-			String query = "table" + i + " = LOAD '" + pathTables.get(i).toString() + 
-					"' USING org.apache.hadoop.zebra.pig.TableLoader();";
-			pigServer.registerQuery(query);
-		}
-	}
-	
-	private static void createTable(Path path, String schemaString, String storageString, Object[][] tableData)
-			throws IOException {
-		//
-		// Create table from tableData array
-		//
-		BasicTable.Writer writer = new BasicTable.Writer(path, schemaString, storageString, conf);
-		
-		Schema schema = writer.getSchema();
-		Tuple tuple = TypesUtils.createTuple(schema);
-		TableInserter inserter = writer.getInserter("ins", false);
-		
-		for (int i = 0; i < tableData.length; ++i) {
-			TypesUtils.resetTuple(tuple);
-			for (int k = 0; k < tableData[i].length; ++k) {
-				tuple.set(k, tableData[i][k]);
-				System.out.println("DEBUG: setting tuple k=" + k + "value= " + tableData[i][k]);
-			}
-			inserter.insert(new BytesWritable(("key" + i).getBytes()), tuple);
-		}
-		inserter.close();
-		writer.close();
-	}
-
-	@AfterClass
-	public static void tearDown() throws Exception {
-		pigServer.shutdown();
-	}
-	
-	private Iterator<Tuple> testOrderPreserveUnion(ArrayList<String> inputTables, String sortkey, String columns)
-				throws IOException {
-		//
-		// Test order preserve union from input tables and provided output columns
-		//
-		Assert.assertTrue("Table union requires two or more input tables", inputTables.size() >= 2);
-		
-		Path newPath = new Path(getCurrentMethodName());
-		ArrayList<String> pathList = new ArrayList<String>();
-		
-		// Load and store each of the input tables
-		for (int i=0; i<inputTables.size(); ++i) {
-			String tablename = inputTables.get(i);
-			String sortName = "sort" + ++sortId;
-			
-			// Sort tables
-			String orderby = sortName + " = ORDER " + tablename + " BY " + sortkey + " ;";
-			pigServer.registerQuery(orderby);
-			
-			String sortPath = new String(newPath.toString() + ++fileId);  // increment fileId suffix
-			
-			// Store sorted tables
-			pigJob = pigServer.store(sortName, sortPath, TableStorer.class.getCanonicalName() +
-				"('" + TABLE_STORAGE + "')");
-			Assert.assertNull(pigJob.getException());
-			
-			pathList.add(sortPath);  // add table path to list
-		}
-		
-		String paths = new String();
-		for (String path:pathList)
-			paths += path + ",";
-		paths = paths.substring(0, paths.lastIndexOf(","));  // remove trailing comma
-		
-		String queryLoad = "records1 = LOAD '"
-	        + paths
-	        +	"' USING org.apache.hadoop.zebra.pig.TableLoader('" + columns + "', 'sorted');";
-		
-		System.out.println("queryLoad: " + queryLoad);
-		pigServer.registerQuery(queryLoad);
-		
-		// Return iterator
-		Iterator<Tuple> it1 = pigServer.openIterator("records1");
-		return it1;
-	}
-	
-	@Test
-	public void test_sorted_union_multi_table() throws ExecException, IOException {
-		//
-		// Test sorted union
-		//
-		
-		// Create input tables for order preserve union
-		ArrayList<String> inputTables = new ArrayList<String>();  // Input tables
-		for (int i=0; i<NUMB_TABLE; ++i) {
-			inputTables.add("table" + i);  // add input table
-		}
-		
-		// Test with input tables and provided output columns
-		testOrderPreserveUnion(inputTables, "int1", "int1, source_table, str1, byte1");
-		
-		// Create results table for verification
-		HashMap<Integer, ArrayList<ArrayList<Object>>> resultTable = 
-      new HashMap<Integer, ArrayList<ArrayList<Object>>>();
-		for (int i=0; i<NUMB_TABLE; ++i) {
-		  ArrayList<ArrayList<Object>> rows = new ArrayList<ArrayList<Object>>();
-			for (int j=0; j<NUMB_TABLE_ROWS; ++j) {
-				ArrayList<Object> resultRow = new ArrayList<Object>();
-				resultRow.add(i);	// int1
-				resultRow.add(i);	// source_table
-				resultRow.add(new String("string" + j));	// str1
-				resultRow.add(new DataByteArray("byte" + (NUMB_TABLE_ROWS - j)));	// byte1
-				rows.add(resultRow);
-			}
-			resultTable.put(i, rows);
-		}
-		
-		// Verify union table
-		Iterator<Tuple> it = pigServer.openIterator("records1");
-		int numbRows = verifyTable(resultTable, 0, 1, it);
-		
-		Assert.assertEquals(totalTableRows, numbRows);
-		
-		// Print Table
-		//printTable("records1");
-	}	
-	
-	/**
-	 * Return the name of the routine that called getCurrentMethodName
-	 * 
-	 */
-	private String getCurrentMethodName() {
-		ByteArrayOutputStream baos = new ByteArrayOutputStream();
-		PrintWriter pw = new PrintWriter(baos);
-		(new Throwable()).printStackTrace(pw);
-		pw.flush();
-		String stackTrace = baos.toString();
-		pw.close();
-		
-		StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-		tok.nextToken(); // 'java.lang.Throwable'
-		tok.nextToken(); // 'at ...getCurrentMethodName'
-		String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-		// Parse line 3
-		tok = new StringTokenizer(l.trim(), " <(");
-		String t = tok.nextToken(); // 'at'
-		t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-		return t;
-	}
-	
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestOrderPreserveMultiTableGlob.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestOrderPreserveMultiTableGlob.java
deleted file mode 100644
index c10f57c40..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestOrderPreserveMultiTableGlob.java
+++ /dev/null
@@ -1,243 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.Iterator;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.StringTokenizer;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.pig.TableStorer;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.executionengine.ExecJob;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-
-import junit.framework.Assert;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-
-public class TestOrderPreserveMultiTableGlob extends BaseTestCase {
-	
-	final static int NUMB_TABLE = 10;  		// number of tables for stress test
-	final static int NUMB_TABLE_ROWS = 5;	// number of rows for each table
-	
-	final static String TABLE_SCHEMA = "int1:int,str1:string,byte1:bytes";
-	final static String TABLE_STORAGE = "[int1,str1,byte1]";
-	
-	static int fileId = 0;
-	static int sortId = 0;
-	
-	protected static ExecJob pigJob;
-	
-	private static ArrayList<Path> pathTables;
-	private static int totalTableRows =0;
-	
-	@BeforeClass
-	public static void setUp() throws Exception {
-		init();
-		
-
-		pathTables = new ArrayList<Path>();
-		for (int i=0; i<NUMB_TABLE; ++i) {
-			Path pathTable = getTableFullPath("TestOderPerserveMultiTable" + i);
-			pathTables.add(pathTable);
-			removeDir(pathTable);
-		}
-			
-		
-		// Create tables
-		for (int i=0; i<NUMB_TABLE; ++i) {
-			// Create table data
-			Object[][] table = new Object[NUMB_TABLE_ROWS][3];  // three columns
-			
-			for (int j=0; j<NUMB_TABLE_ROWS; ++j) {
-				table[j][0] = i;
-				table[j][1] = new String("string" + j);
-				table[j][2] = new DataByteArray("byte" + (NUMB_TABLE_ROWS - j));
-				++totalTableRows;
-			}
-			// Create table
-			createTable(pathTables.get(i), TABLE_SCHEMA, TABLE_STORAGE, table);
-			
-			// Load Table
-			String query = "table" + i + " = LOAD '" + pathTables.get(i).toString() + 
-					"' USING org.apache.hadoop.zebra.pig.TableLoader();";
-			pigServer.registerQuery(query);
-		}
-	}
-	
-	private static void createTable(Path path, String schemaString, String storageString, Object[][] tableData)
-			throws IOException {
-		//
-		// Create table from tableData array
-		//
-		BasicTable.Writer writer = new BasicTable.Writer(path, schemaString, storageString, conf);
-		
-		Schema schema = writer.getSchema();
-		Tuple tuple = TypesUtils.createTuple(schema);
-		TableInserter inserter = writer.getInserter("ins", false);
-		
-		for (int i = 0; i < tableData.length; ++i) {
-			TypesUtils.resetTuple(tuple);
-			for (int k = 0; k < tableData[i].length; ++k) {
-				tuple.set(k, tableData[i][k]);
-				System.out.println("DEBUG: setting tuple k=" + k + "value= " + tableData[i][k]);
-			}
-			inserter.insert(new BytesWritable(("key" + i).getBytes()), tuple);
-		}
-		inserter.close();
-		writer.close();
-	}
-
-	@AfterClass
-	public static void tearDown() throws Exception {
-		pigServer.shutdown();
-	}
-	
-	private Iterator<Tuple> testOrderPreserveUnion(ArrayList<String> inputTables, String sortkey, String columns)
-				throws IOException {
-		//
-		// Test order preserve union from input tables and provided output columns
-		//
-		Assert.assertTrue("Table union requires two or more input tables", inputTables.size() >= 2);
-		
-		Path newPath = new Path(getCurrentMethodName());
-		ArrayList<String> pathList = new ArrayList<String>();
-		
-		// Load and store each of the input tables
-		for (int i=0; i<inputTables.size(); ++i) {
-			String tablename = inputTables.get(i);
-			String sortName = "sort" + ++sortId;
-			
-			// Sort tables
-			String orderby = sortName + " = ORDER " + tablename + " BY " + sortkey + " ;";
-			pigServer.registerQuery(orderby);
-			
-			String sortPath = new String(newPath.toString() + ++fileId);  // increment fileId suffix
-			
-			// Store sorted tables
-			pigJob = pigServer.store(sortName, sortPath, TableStorer.class.getCanonicalName() +
-				"('" + TABLE_STORAGE + "')");
-			Assert.assertNull(pigJob.getException());
-			
-			pathList.add(sortPath);  // add table path to list
-		}
-		
-		String paths = new String();
-		
-    paths += newPath.toString() + "{";
-    fileId = 0;
-		for (String path:pathList)
-			paths += ++fileId + ",";
-		paths = paths.substring(0, paths.lastIndexOf(","));  // remove trailing comma
-    paths += "}";
-    
-		String queryLoad = "records1 = LOAD '"
-	        + paths
-	        +	"' USING org.apache.hadoop.zebra.pig.TableLoader('" + columns + "', 'sorted');";
-		
-		System.out.println("queryLoad: " + queryLoad);
-		pigServer.registerQuery(queryLoad);
-		
-		// Return iterator
-		Iterator<Tuple> it1 = pigServer.openIterator("records1");
-		return it1;
-	}
-	
-	@Test
-	public void test_sorted_union_multi_table() throws ExecException, IOException {
-		//
-		// Test sorted union
-		//
-		
-		// Create input tables for order preserve union
-		ArrayList<String> inputTables = new ArrayList<String>();  // Input tables
-		for (int i=0; i<NUMB_TABLE; ++i) {
-			inputTables.add("table" + i);  // add input table
-		}
-		
-		// Test with input tables and provided output columns
-		testOrderPreserveUnion(inputTables, "int1", "int1, str1, byte1, source_table");
-		
-		// Create results table for verification
-		HashMap<Integer, ArrayList<ArrayList<Object>>> resultTable = 
-		  new HashMap<Integer, ArrayList<ArrayList<Object>>>();
-		
-		// The ordering from FileInputFormat glob expansion.
-		int[] tblIndexList = {0, 9, 1, 2, 3, 4, 5, 6, 7, 8};
-		
-		for (int i=0; i<NUMB_TABLE; ++i) {
-		  ArrayList<ArrayList<Object>> rows = new ArrayList<ArrayList<Object>>();
-			for (int j=0; j<NUMB_TABLE_ROWS; ++j) {
-				ArrayList<Object> resultRow = new ArrayList<Object>();
-				resultRow.add(tblIndexList[i]);	// int1
-				resultRow.add(new String("string" + j));	// str1
-				resultRow.add(new DataByteArray("byte" + (NUMB_TABLE_ROWS - j)));	// byte1
-				rows.add(resultRow);
-			}
-			resultTable.put(i, rows);
-		}
-		
-		// Verify union table
-		Iterator<Tuple> it = pigServer.openIterator("records1");
-		int numbRows = verifyTable(resultTable, 0, 3, it);
-		
-		Assert.assertEquals(totalTableRows, numbRows);
-		
-		// Print Table
-		//printTable("records1");
-	}
-	
-	/**
-	 * Return the name of the routine that called getCurrentMethodName
-	 * 
-	 */
-	private String getCurrentMethodName() {
-		ByteArrayOutputStream baos = new ByteArrayOutputStream();
-		PrintWriter pw = new PrintWriter(baos);
-		(new Throwable()).printStackTrace(pw);
-		pw.flush();
-		String stackTrace = baos.toString();
-		pw.close();
-		
-		StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-		tok.nextToken(); // 'java.lang.Throwable'
-		tok.nextToken(); // 'at ...getCurrentMethodName'
-		String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-		// Parse line 3
-		tok = new StringTokenizer(l.trim(), " <(");
-		String t = tok.nextToken(); // 'at'
-		t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-		return t;
-	}
-	
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestOrderPreserveProjection.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestOrderPreserveProjection.java
deleted file mode 100644
index 82387dd0c..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestOrderPreserveProjection.java
+++ /dev/null
@@ -1,1012 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.io.StringWriter;
-import java.io.Writer;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.ArrayList;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.pig.TableStorer;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.executionengine.ExecJob;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-
-public class TestOrderPreserveProjection extends BaseTestCase {
-	
-	final static String TABLE1_SCHEMA = "a:int,b:float,c:long,d:double,e:string,f:bytes,m1:map(string)";
-	final static String TABLE1_STORAGE = "[a, b, c]; [d, e, f]; [m1#{a}]";
-	
-	final static String TABLE2_SCHEMA = "a:int,b:float,c:long,d:double,e:string,f:bytes,m1:map(string)";
-	final static String TABLE2_STORAGE = "[a, b, c]; [d, e, f]; [m1#{a}]";
-	
-	final static String TABLE3_SCHEMA = "e:string,str1:string,str2:string,int1:int,map1:map(string)";
-	final static String TABLE3_STORAGE = "[e, str1]; [str2,int1,map1#{a}]";
-	
-	final static String TABLE4_SCHEMA = "int1:int,str1:string,long1:long";
-	final static String TABLE4_STORAGE = "[int1]; [str1,long1]";
-	
-	final static String TABLE5_SCHEMA = "b:float,d:double,c:long,e:string,f:bytes,int1:int";
-	final static String TABLE5_STORAGE = "[b,d]; [c]; [e]; [f,int1]";
-	
-	final static String TABLE6_SCHEMA = "e:string,str3:string,str4:string";
-	final static String TABLE6_STORAGE = "[e,str3,str4]";
-	
-	final static String TABLE7_SCHEMA = "int2:int";
-	final static String TABLE7_STORAGE = "[int2]";
-	
-	static int fileId = 0;
-	static int sortId = 0;
-	
-	protected static ExecJob pigJob;
-	
-	private static Path pathTable1;
-	private static Path pathTable2;
-	private static Path pathTable3;
-	private static Path pathTable4;
-	private static Path pathTable5;
-	private static Path pathTable6;
-	private static Path pathTable7;
-	private static HashMap<String, String> tableStorage;
-	
-	private static ArrayList<String> tableList;
-	
-	private static Object[][] table1;
-	private static Object[][] table2;
-	private static Object[][] table3;
-	private static Object[][] table4;
-	private static Object[][] table5;
-	private static Object[][] table6;
-	private static Object[][] table7;
-	
-	private static Map<String, String> m1;
-	private static Map<String, String> m2;
-	private static Map<String, String> m3;
-	
-	@BeforeClass
-	public static void setUp() throws Exception {
-		init();
-		pathTable1 = getTableFullPath("table1");
-		pathTable2 = getTableFullPath("table2");
-		pathTable3 = getTableFullPath("table3");
-		pathTable4 = getTableFullPath("table4");
-		pathTable5 = getTableFullPath("table5");
-		pathTable6 = getTableFullPath("table6");
-		pathTable7 = getTableFullPath("table7");
-		
-		// Create table1 data
-		m1 = new HashMap<String, String>();
-		m1.put("a","m1-a");
-		m1.put("b","m1-b");
-		
-		table1 = new Object[][]{
-			{5,		-3.25f,	1001L,	51e+2,	"Zebra",	new DataByteArray("Zebra"),		m1},
-			{-1,	3.25f,	1000L,	50e+2,	"zebra",	new DataByteArray("zebra"),		m1},
-			{1001,	100.0f,	1003L,	50e+2,	"Apple",	new DataByteArray("Apple"),		m1},
-			{1001,	101.0f,	1001L,	50e+2,	"apple",	new DataByteArray("apple"),		m1},
-			{1001,	50.0f,	1000L,	50e+2,	"Pig",		new DataByteArray("Pig"),		m1},
-			{1001,	52.0f,	1001L,	50e+2,	"pig",		new DataByteArray("pig"),		m1},
-			{1002,	28.0f,	1000L,	50e+2,	"Hadoop",	new DataByteArray("Hadoop"),	m1},
-			{1000,	0.0f,	1002L,	52e+2,	"hadoop",	new DataByteArray("hadoop"),	m1} };
-		
-		// Create table1
-		createTable(pathTable1, TABLE1_SCHEMA, TABLE1_STORAGE, table1);
-		
-		// Create table2 data
-		m2 = new HashMap<String, String>();
-		m2.put("a","m2-a");
-		m2.put("b","m2-b");
-		
-		table2 = new Object[][] {
-			{15,	56.0f,	1004L,	50e+2,	"green",	new DataByteArray("green"),		m2},
-			{-1,	-99.0f,	1002L,	51e+2,	"orange",	new DataByteArray("orange"),	m2},
-			{1001,	100.0f,	1003L,	55e+2,	"white",	new DataByteArray("white"),		m2},
-			{1001,	102.0f,	1001L,	52e+2,	"purple",	new DataByteArray("purple"),	m2},
-			{1001,	50.0f,	1008L,	52e+2,	"gray",		new DataByteArray("gray"),		m2},
-			{1001,	53.0f,	1001L,	52e+2,	"brown",	new DataByteArray("brown"),		m2},
-			{2000,	33.0f,	1006L,	52e+2,	"beige",	new DataByteArray("beige"),		m2} };
-		
-		// Create table2
-		createTable(pathTable2, TABLE2_SCHEMA, TABLE2_STORAGE, table2);
-		
-		// Create table3 data
-		m3 = new HashMap<String, String>();
-		m3.put("a","m3-a");
-		m3.put("b","m3-b");
-		m3.put("c","m3-b");
-		
-		table3 = new Object[][] {
-			{"a 8",	"Cupertino",	"California",	1,	m3},
-			{"a 7",	"San Jose",		"California",	2,	m3},
-			{"a 6",	"Santa Cruz",	"California",	3,	m3},
-			{"a 5",	"Las Vegas",	"Nevada",		4,	m3},
-			{"a 4",	"New York",		"New York",		5,	m3},
-			{"a 3",	"Phoenix",		"Arizona",		6,	m3},
-			{"a 2",	"Dallas",		"Texas",		7,	m3},
-			{"a 1",	"Reno",			"Nevada",		8,	m3} };
-		
-		// Create table3
-		createTable(pathTable3, TABLE3_SCHEMA, TABLE3_STORAGE, table3);
-		
-		// Create table4 data
-		table4 = new Object[][] {
-			{1,	"Cupertino",	1001L},
-			{2,	"San Jose",		1008L},
-			{3,	"Santa Cruz",	1008L},
-			{4,	"Las Vegas",	1008L},
-			{5,	"Dallas",		1010L},
-			{6,	"Reno",			1000L} };
-		
-		// Create table4
-		createTable(pathTable4, TABLE4_SCHEMA, TABLE4_STORAGE, table4);
-		
-		// Create table5 data
-		table5 = new Object[][] {
-			{3.25f,		51e+2,	1001L,	"string1",	new DataByteArray("green"),	100},
-			{3.25f,		50e+2,	1000L,	"string1",	new DataByteArray("blue"),	100},
-			{3.26f,		51e+2,	1001L,	"string0",	new DataByteArray("purple"),100},
-			{3.24f,		53e+2,	1001L,	"string1",	new DataByteArray("yellow"),100},
-			{3.28f,		51e+2,	1001L,	"string2",	new DataByteArray("white"),	100},
-			{3.25f,		53e+2,	1000L,	"string0",	new DataByteArray("orange"),100} };
-		
-		// Create table5
-		createTable(pathTable5, TABLE5_SCHEMA, TABLE5_STORAGE, table5);
-		
-		// Create table6 data
-		table6 = new Object[][] {
-			{"a 8",	"Cupertino",	"California"},
-			{"a 7",	"San Jose",		"California"},
-			{"a 6",	"Santa Cruz",	"California"},
-			{"a 5",	"Las Vegas",	"Nevada"},
-			{"a 4",	"New York",		"New York"},
-			{"a 3",	"Phoenix",		"Arizona"},
-			{"a 2",	"Dallas",		"Texas"},
-			{"a 1",	"Reno",			"Nevada"} };
-		
-		// Create table6
-		createTable(pathTable6, TABLE6_SCHEMA, TABLE6_STORAGE, table6);
-		
-		// Create table7 data
-		table7 = new Object[][] {
-			{1001},
-			{1000},
-			{1010},
-			{1009},
-			{1001} };
-		
-		// Create table7
-		createTable(pathTable7, TABLE7_SCHEMA, TABLE7_STORAGE, table7);
-		
-		// Load tables
-		String query1 = "table1 = LOAD '" + pathTable1.toString() + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		pigServer.registerQuery(query1);
-		String query2 = "table2 = LOAD '" + pathTable2.toString() + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		pigServer.registerQuery(query2);
-		String query3 = "table3 = LOAD '" + pathTable3.toString() + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		pigServer.registerQuery(query3);
-		String query4 = "table4 = LOAD '" + pathTable4.toString() + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		pigServer.registerQuery(query4);
-		String query5 = "table5 = LOAD '" + pathTable5.toString() + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		pigServer.registerQuery(query5);
-		String query6 = "table6 = LOAD '" + pathTable6.toString() + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		pigServer.registerQuery(query6);
-		String query7 = "table7 = LOAD '" + pathTable7.toString() + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		pigServer.registerQuery(query7);
-		
-		// Map for table storage
-		tableStorage = new HashMap<String, String>();
-		tableStorage.put("table1", TABLE1_STORAGE);
-		tableStorage.put("table2", TABLE2_STORAGE);
-		tableStorage.put("table3", TABLE3_STORAGE);
-		tableStorage.put("table4", TABLE4_STORAGE);
-		tableStorage.put("table5", TABLE5_STORAGE);
-		tableStorage.put("table6", TABLE6_STORAGE);
-		tableStorage.put("table7", TABLE7_STORAGE);
-	}
-	
-	private static void createTable(Path path, String schemaString, String storageString, Object[][] tableData)
-			throws IOException {
-		//
-		// Create table from tableData array
-		//
-		BasicTable.Writer writer = new BasicTable.Writer(path, schemaString, storageString, conf);
-		
-		Schema schema = writer.getSchema();
-		Tuple tuple = TypesUtils.createTuple(schema);
-		TableInserter inserter = writer.getInserter("ins", false);
-		
-		for (int i = 0; i < tableData.length; ++i) {
-			TypesUtils.resetTuple(tuple);
-			for (int k = 0; k < tableData[i].length; ++k) {
-				tuple.set(k, tableData[i][k]);
-				System.out.println("DEBUG: setting tuple k=" + k + "value= " + tableData[i][k]);
-			}
-			inserter.insert(new BytesWritable(("key" + i).getBytes()), tuple);
-		}
-		inserter.close();
-		writer.close();
-	}
-	
-	@AfterClass
-	public static void tearDown() throws Exception {
-		pigServer.shutdown();
-	}
-	
-	private Iterator<Tuple> testOrderPreserveUnion(Map<String, String> inputTables, String columns) throws IOException {
-		//
-		// Test order preserve union from input tables and provided output columns
-		//
-		Assert.assertTrue("Table union requires two or more input tables", inputTables.size() >= 2);
-		
-		Path newPath = new Path(getCurrentMethodName());
-		ArrayList<String> pathList = new ArrayList<String>();
-		tableList = new ArrayList<String>(); // list of tables to load
-		
-		// Load and store each of the input tables
-		for (Iterator<String> it = inputTables.keySet().iterator(); it.hasNext();) {
-			
-			String tablename = it.next();
-			String sortkey = inputTables.get(tablename);
-			
-			String sortName = "sort" + ++sortId;
-			
-			// Sort tables
-			String orderby = sortName + " = ORDER " + tablename + " BY " + sortkey + " ;";   // need single quotes?
-			pigServer.registerQuery(orderby);
-			
-			String sortPath = new String(newPath.toString() + ++fileId);  // increment fileId suffix
-			
-			// Store sorted tables
-			pigJob = pigServer.store(sortName, sortPath, TableStorer.class.getCanonicalName() +
-				"('" + tableStorage.get(tablename) + "')");
-			Assert.assertNull(pigJob.getException());
-			
-			tableList.add(tablename); // add table name to list
-			pathList.add(sortPath);  // add table path to list
-		}
-		
-		String paths = new String();
-		for (String path:pathList)
-			paths += path + ",";
-		paths = paths.substring(0, paths.lastIndexOf(","));  // remove trailing comma
-		
-		String queryLoad = "records1 = LOAD '"
-	        + paths
-	        +	"' USING org.apache.hadoop.zebra.pig.TableLoader('" + columns + "', 'sorted');";
-		
-		System.out.println("queryLoad: " + queryLoad);
-		
-		pigServer.registerQuery(queryLoad);
-		
-		// Return iterator
-		Iterator<Tuple> it1 = pigServer.openIterator("records1");
-		return it1;
-	}
-	
-	
-	@Test
-	public void test_sorted_table_union_01() throws ExecException, IOException {
-		//
-		// Test sorted union
-		//
-		
-		// Create input tables for order preserve union
-		Map<String, String> inputTables = new HashMap<String, String>();  // Input two or more tables
-		inputTables.put("table1", "a,b,c");  // input table and sort keys
-		inputTables.put("table2", "a,b,c");  // input table and sort keys
-		
-		// Test with input tables and provided output columns
-		testOrderPreserveUnion(inputTables, "a,b,c,d,e,f,m1, source_table");
-		
-		// Verify union table
-		HashMap<Integer, ArrayList<ArrayList<Object>>> resultTable = new HashMap<Integer, ArrayList<ArrayList<Object>>>();
-		
-		ArrayList<ArrayList<Object>> rows = new ArrayList<ArrayList<Object>>();
-    addResultRow(rows, -1, 3.25f,  1000L,  50e+2,  "zebra",  new DataByteArray("zebra"), m1);
-    addResultRow(rows, 5,  -3.25f, 1001L,  51e+2,  "Zebra",  new DataByteArray("Zebra"), m1);
-    addResultRow(rows, 1000, 0.0f, 1002L,  52e+2,  "hadoop", new DataByteArray("hadoop"),m1);
-    addResultRow(rows, 1001, 50.0f,  1000L,  50e+2,  "Pig",    new DataByteArray("Pig"), m1);
-    addResultRow(rows, 1001, 52.0f,  1001L,  50e+2,  "pig",    new DataByteArray("pig"), m1);
-    addResultRow(rows, 1001, 100.0f, 1003L,  50e+2,  "Apple",  new DataByteArray("Apple"), m1);
-    addResultRow(rows, 1001, 101.0f, 1001L,  50e+2,  "apple",  new DataByteArray("apple"), m1);
-    addResultRow(rows, 1002, 28.0f,  1000L,  50e+2,  "Hadoop", new DataByteArray("Hadoop"),m1);
-		resultTable.put(0, rows);
-		
-		rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, -1, -99.0f, 1002L,  51e+2,  "orange", new DataByteArray("orange"),m2);
-    addResultRow(rows, 15, 56.0f,  1004L,  50e+2,  "green",  new DataByteArray("green"), m2);
-    addResultRow(rows, 1001, 50.0f,  1008L,  52e+2,  "gray",   new DataByteArray("gray"),  m2);
-    addResultRow(rows, 1001, 53.0f,  1001L,  52e+2,  "brown",  new DataByteArray("brown"), m2);
-    addResultRow(rows, 1001, 100.0f, 1003L,  55e+2,  "white",  new DataByteArray("white"), m2);
-    addResultRow(rows, 1001, 102.0f, 1001L,  52e+2,  "purple", new DataByteArray("purple"),m2);
-    addResultRow(rows, 2000, 33.0f,  1006L,  52e+2,  "beige",  new DataByteArray("beige"), m2);
-		resultTable.put(1, rows);
-    
-		// Verify union table
-		Iterator<Tuple> it = pigServer.openIterator("records1");
-		int numbRows = verifyTable(resultTable, 0, 7, it);
-		
-		Assert.assertEquals(numbRows, table1.length + table2.length);
-	}
-	
-	@Test
-	public void test_sorted_table_union_02() throws ExecException, IOException {
-		//
-		// Test sorted union
-		//
-		
-		// Create input tables for order preserve union
-		Map<String, String> inputTables = new HashMap<String, String>();  // Input two or more tables
-		inputTables.put("table1", "a,b,c");  // input table and sort keys
-		inputTables.put("table2", "a,b,c");  // input table and sort keys
-		
-		// Test with input tables and provided output columns
-		testOrderPreserveUnion(inputTables, "a,b,c,d,e,f,m1,source_table");
-		
-		// Verify union table
-		HashMap<Integer, ArrayList<ArrayList<Object>>> resultTable = new HashMap<Integer, ArrayList<ArrayList<Object>>>();
-		
-		ArrayList<ArrayList<Object>> rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, -1,	3.25f,	1000L,	50e+2,	"zebra",	new DataByteArray("zebra"),	m1,	0);
-		addResultRow(rows, 5,	-3.25f,	1001L,	51e+2,	"Zebra",	new DataByteArray("Zebra"),	m1,	0);
-		addResultRow(rows, 1000,	0.0f,	1002L,	52e+2,	"hadoop",	new DataByteArray("hadoop"),m1,	0);
-		addResultRow(rows, 1001,	50.0f,	1000L,	50e+2,	"Pig",		new DataByteArray("Pig"),	m1,	0);
-		addResultRow(rows, 1001,	52.0f,	1001L,	50e+2,	"pig",		new DataByteArray("pig"),	m1,	0);
-		addResultRow(rows, 1001,	100.0f,	1003L,	50e+2,	"Apple",	new DataByteArray("Apple"),	m1,	0);
-		addResultRow(rows, 1001,	101.0f,	1001L,	50e+2,	"apple",	new DataByteArray("apple"),	m1,	0);
-		addResultRow(rows, 1002,	28.0f,	1000L,	50e+2,	"Hadoop",	new DataByteArray("Hadoop"),m1,	0);
-		resultTable.put(0, rows);
-		
-		rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, -1,	-99.0f,	1002L,	51e+2,	"orange",	new DataByteArray("orange"),m2,	1);
-		addResultRow(rows, 15, 56.0f,  1004L,  50e+2,  "green",  new DataByteArray("green"), m2, 1);
-		addResultRow(rows, 1001, 50.0f,  1008L,  52e+2,  "gray",   new DataByteArray("gray"),  m2, 1);
-		addResultRow(rows, 1001, 53.0f,  1001L,  52e+2,  "brown",  new DataByteArray("brown"), m2, 1);
-		addResultRow(rows, 1001, 100.0f, 1003L,  55e+2,  "white",  new DataByteArray("white"), m2, 1);
-		addResultRow(rows, 1001, 102.0f, 1001L,  52e+2,  "purple", new DataByteArray("purple"),m2, 1);
-		addResultRow(rows, 2000, 33.0f,  1006L,  52e+2,  "beige",  new DataByteArray("beige"), m2, 1);
-		resultTable.put(1, rows);
-		
-		// Verify union table
-		Iterator<Tuple> it = pigServer.openIterator("records1");
-		int numbRows = verifyTable(resultTable, 0, 7, it);
-		
-		Assert.assertEquals(numbRows, table1.length + table2.length);
-	}
-	
-	/**
-	 * This is a negative test case, as we don't support partial sort key match.
-	 * 
-	 * @throws ExecException
-	 * @throws IOException
-	 */
-	@Test
-	public void test_sorted_table_union_03() throws ExecException, IOException {
-		//
-		// Test sorted union
-		//
-		
-		// Create input tables for order preserve union
-		Map<String, String> inputTables = new HashMap<String, String>();  // Input two or more tables
-		inputTables.put("table1", "a,b,c");  // input table and sort keys
-		inputTables.put("table2", "a,b");  // input table and sort keys
-		
-		IOException exception = null;
-		try {
-			// Test with input tables and provided output columns
-			testOrderPreserveUnion(inputTables, "a,b,c,d,e,f,m1,source_table");
-		} catch(IOException ex) {
-			exception = ex;
-		} finally {
-			Assert.assertTrue( exception != null );
-			Assert.assertTrue( getStackTrace( exception ).contains( "Tables of the table union do not possess the same sort property" ) );
-		}
-		
-//		// Verify union table
-//		ArrayList<ArrayList<Object>> resultTable = new ArrayList<ArrayList<Object>>();
-//		
-//		addResultRow(resultTable, -1,	-99.0f,	1002L,	51e+2,	"orange",	new DataByteArray("orange"),m2,	1);
-//		addResultRow(resultTable, -1,	3.25f,	1000L,	50e+2,	"zebra",	new DataByteArray("zebra"),	m1,	0);
-//		addResultRow(resultTable, 5,	-3.25f,	1001L,	51e+2,	"Zebra",	new DataByteArray("Zebra"),	m1,	0);
-//		addResultRow(resultTable, 15,	56.0f,	1004L,	50e+2,	"green",	new DataByteArray("green"),	m2,	1);
-//		
-//		addResultRow(resultTable, 1000,	0.0f,	1002L,	52e+2,	"hadoop",	new DataByteArray("hadoop"),m1,	0);
-//		addResultRow(resultTable, 1001,	50.0f,	1008L,	52e+2,	"gray",		new DataByteArray("gray"),	m2,	1);
-//		addResultRow(resultTable, 1001,	50.0f,	1000L,	50e+2,	"Pig",		new DataByteArray("Pig"),	m1,	0);
-//		addResultRow(resultTable, 1001,	52.0f,	1001L,	50e+2,	"pig",		new DataByteArray("pig"),	m1,	0);
-//		
-//		addResultRow(resultTable, 1001,	53.0f,	1001L,	52e+2,	"brown",	new DataByteArray("brown"),	m2,	1);
-//		addResultRow(resultTable, 1001,	100.0f,	1003L,	55e+2,	"white",	new DataByteArray("white"),	m2,	1);
-//		addResultRow(resultTable, 1001,	100.0f,	1003L,	50e+2,	"Apple",	new DataByteArray("Apple"),	m1,	0);
-//		addResultRow(resultTable, 1001,	101.0f,	1001L,	50e+2,	"apple",	new DataByteArray("apple"),	m1,	0);
-//		
-//		addResultRow(resultTable, 1001,	102.0f,	1001L,	52e+2,	"purple",	new DataByteArray("purple"),m2,	1);
-//		addResultRow(resultTable, 1002,	28.0f,	1000L,	50e+2,	"Hadoop",	new DataByteArray("Hadoop"),m1,	0);
-//		addResultRow(resultTable, 2000,	33.0f,	1006L,	52e+2,	"beige",	new DataByteArray("beige"),	m2,	1);
-//		
-//		// Verify union table
-//		Iterator<Tuple> it = pigServer.openIterator("records1");
-//		int numbRows = verifyTable(resultTable, 0, it);
-//		
-//		Assert.assertEquals(numbRows, table1.length + table2.length);
-	}
-	
-	/**
-	 * Get stack trace as string that includes nested exceptions
-	 * 
-	 */
-	private static String getStackTrace(Throwable throwable) {
-		Writer writer = new StringWriter();
-		PrintWriter printWriter = new PrintWriter(writer);
-		if (throwable != null)
-			throwable.printStackTrace(printWriter);
-		return writer.toString();
-	}
-
-	@Test
-	public void test_sorted_table_union_04() throws ExecException, IOException {
-		//
-		// Test sorted union
-		//
-		
-		// Create input tables for order preserve union
-		Map<String, String> inputTables = new HashMap<String, String>();  // Input two or more tables
-		inputTables.put("table1", "e");  // input table and sort keys
-		inputTables.put("table2", "e");  // input table and sort keys
-		
-		// Test with input tables and provided output columns
-		testOrderPreserveUnion(inputTables, "source_table,e,c,b,a");
-		
-		// Verify union table
-		HashMap<Integer, ArrayList<ArrayList<Object>>> resultTable = new HashMap<Integer, ArrayList<ArrayList<Object>>>();
-		
-		ArrayList<ArrayList<Object>> rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, 0,	"Apple",	1003L,	100.0f,	1001);
-		addResultRow(rows, 0,	"Hadoop",	1000L,	28.0f,	1002);
-		addResultRow(rows, 0,	"Pig",		1000L,	50.0f,	1001);
-		addResultRow(rows, 0,	"Zebra",	1001L,	-3.25f,	5);
-		
-		addResultRow(rows, 0,	"apple",	1001L,	101.0f,	1001);
-		addResultRow(rows, 0,	"hadoop",	1002L,	0.0f,	1000);
-		addResultRow(rows, 0,	"pig",		1001L,	52.0f,	1001);
-		addResultRow(rows, 0,	"zebra",	1000L,	3.25f,	-1);
-		resultTable.put(0, rows);
-		
-		rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, 1,  "beige",  1006L,  33.0f,  2000);
-    addResultRow(rows, 1,  "brown",  1001L,  53.0f,  1001);
-    addResultRow(rows, 1,  "gray",   1008L,  50.0f,  1001);
-    addResultRow(rows, 1,  "green",  1004L,  56.0f,  15);
-    addResultRow(rows, 1,  "orange", 1002L,  -99.0f, -1);
-    addResultRow(rows, 1,  "purple", 1001L,  102.0f, 1001);
-    addResultRow(rows, 1,  "white",  1003L,  100.0f, 1001);
-		resultTable.put(1, rows);
-    
-		// Verify union table
-		Iterator<Tuple> it = pigServer.openIterator("records1");
-		int numbRows = verifyTable(resultTable, 1, 0, it);
-		
-		Assert.assertEquals(numbRows, table1.length + table2.length);
-	}
-	
-	
-	@Test
-	public void test_sorted_table_union_05() throws ExecException, IOException {
-		//
-		// Test sorted union
-		//
-		
-		// Create input tables for order preserve union
-		Map<String, String> inputTables = new HashMap<String, String>();  // Input two or more tables
-		inputTables.put("table1", "e,c");  // input table and sort keys
-		inputTables.put("table2", "e,c");  // input table and sort keys
-		inputTables.put("table5", "e,c");  // input table and sort keys
-		
-		// Test with input tables and provided output columns
-		testOrderPreserveUnion(inputTables, "e,c,a,b,d,f,m1,source_table");
-		
-		// Verify union table
-		HashMap<Integer, ArrayList<ArrayList<Object>>> resultTable = new HashMap<Integer, ArrayList<ArrayList<Object>>>();
-		
-		ArrayList<ArrayList<Object>> rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, "Apple",	1003L,	1001,	100.0f,	50e+2,	new DataByteArray("Apple"),	m1,		0);
-		addResultRow(rows, "Hadoop",	1000L,	1002,	28.0f,	50e+2,	new DataByteArray("Hadoop"),m1,		0);
-		addResultRow(rows, "Pig",	1000L,	1001,	50.0f,	50e+2,	new DataByteArray("Pig"),	m1,		0);
-		addResultRow(rows, "Zebra",	1001L,	5,		-3.25f,	51e+2,	new DataByteArray("Zebra"),	m1,		0);
-		addResultRow(rows, "apple",	1001L,	1001,	101.0f,	50e+2,	new DataByteArray("apple"),	m1,		0);
-		addResultRow(rows, "hadoop",	1002L,	1000,	0.0f,	52e+2,	new DataByteArray("hadoop"),m1,		0);
-		addResultRow(rows, "pig",	1001L,	1001,	52.0f,	50e+2,	new DataByteArray("pig"),	m1,		0);
-		addResultRow(rows, "zebra",	1000L,	-1,		3.25f,	50e+2,	new DataByteArray("zebra"),	m1,		0);
-		resultTable.put(0, rows);
-		
-		rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, "beige",  1006L,  2000, 33.0f,  52e+2,  new DataByteArray("beige"), m2,   1);
-    addResultRow(rows, "brown",  1001L,  1001, 53.0f,  52e+2,  new DataByteArray("brown"), m2,   1);
-    addResultRow(rows, "gray", 1008L,  1001, 50.0f,  52e+2,  new DataByteArray("gray"),  m2,   1);
-    addResultRow(rows, "green",  1004L,  15,   56.0f,  50e+2,  new DataByteArray("green"), m2,   1);
-    addResultRow(rows, "orange", 1002L,  -1,   -99.0f, 51e+2,  new DataByteArray("orange"),m2,   1);
-    addResultRow(rows, "purple", 1001L,  1001, 102.0f, 52e+2,  new DataByteArray("purple"),m2,   1);
-    addResultRow(rows, "white",  1003L,  1001, 100.0f, 55e+2,  new DataByteArray("white"), m2,   1);
-		resultTable.put(1, rows);
-    
-		rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, "string0",1000L,  null, 3.25f,  53e+2,  new DataByteArray("orange"),null, 2);
-    addResultRow(rows, "string0",1001L,  null, 3.26f,  51e+2,  new DataByteArray("purple"),null, 2);
-    addResultRow(rows, "string1",1000L,  null, 3.25f,  50e+2,  new DataByteArray("blue"),  null, 2);
-    addResultRow(rows, "string1",1001L,  null, 3.25f,  51e+2,  new DataByteArray("green"), null, 2);
-    addResultRow(rows, "string1",1001L,  null, 3.24f,  53e+2,  new DataByteArray("yellow"),null, 2);
-    addResultRow(rows, "string2",1001L,  null, 3.28f,  51e+2,  new DataByteArray("white"), null, 2);
-		resultTable.put(2, rows);
-    
-		
-		// Verify union table
-		Iterator<Tuple> it = pigServer.openIterator("records1");
-		int numbRows = verifyTable(resultTable, 0, 7, it);
-		
-		Assert.assertEquals(numbRows, table1.length + table2.length + table5.length);
-	}
-	
-	@Test
-	public void test_sorted_table_union_06() throws ExecException, IOException {
-		//
-		// Test sorted union
-		//
-		
-		// Create input tables for order preserve union
-		Map<String, String> inputTables = new HashMap<String, String>();  // Input two or more tables
-		inputTables.put("table5", "e");  // input table and sort keys
-		inputTables.put("table1", "e");  // input table and sort keys
-		inputTables.put("table2", "e");  // input table and sort keys
-		
-		// Test with input tables and provided output columns
-		testOrderPreserveUnion(inputTables, "b,a,source_table,c,d,f,m1,e");
-		
-		// Verify union table
-		HashMap<Integer, ArrayList<ArrayList<Object>>> resultTable = new HashMap<Integer, ArrayList<ArrayList<Object>>>();
-		
-		ArrayList<ArrayList<Object>> rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows,	100.0f,	1001,	0,	1003L,	50e+2,	new DataByteArray("Apple"),	m1,	"Apple");
-		addResultRow(rows,	28.0f,	1002,	0,	1000L,	50e+2,	new DataByteArray("Hadoop"),m1,	"Hadoop");
-		addResultRow(rows,	50.0f,	1001,	0,	1000L,	50e+2,	new DataByteArray("Pig"),	m1,	"Pig");
-		addResultRow(rows,	-3.25f,	5,		0,	1001L,	51e+2,	new DataByteArray("Zebra"),	m1,	"Zebra");
-		addResultRow(rows,	101.0f,	1001,	0,	1001L,	50e+2,	new DataByteArray("apple"),	m1,	"apple");
-		addResultRow(rows,	0.0f,	1000,	0,	1002L,	52e+2,	new DataByteArray("hadoop"),m1,	"hadoop");
-		addResultRow(rows,	52.0f,	1001,	0,	1001L,	50e+2,	new DataByteArray("pig"),	m1,	"pig");
-		addResultRow(rows,	3.25f,	-1,		0,	1000L,	50e+2,	new DataByteArray("zebra"),	m1,	"zebra");
-		resultTable.put(0, rows);
-		
-		rows = new ArrayList<ArrayList<Object>>();
-    addResultRow(rows, 33.0f,  2000, 1,  1006L,  52e+2,  new DataByteArray("beige"), m2, "beige");
-    addResultRow(rows, 53.0f,  1001, 1,  1001L,  52e+2,  new DataByteArray("brown"), m2, "brown");
-    addResultRow(rows, 50.0f,  1001, 1,  1008L,  52e+2,  new DataByteArray("gray"),  m2, "gray");
-    addResultRow(rows, 56.0f,  15,   1,  1004L,  50e+2,  new DataByteArray("green"), m2, "green");
-    addResultRow(rows, -99.0f, -1,   1,  1002L,  51e+2,  new DataByteArray("orange"),m2, "orange");
-    addResultRow(rows, 102.0f, 1001, 1,  1001L,  52e+2,  new DataByteArray("purple"),m2, "purple");
-    addResultRow(rows, 100.0f, 1001, 1,  1003L,  55e+2,  new DataByteArray("white"), m2, "white");
-		resultTable.put(1, rows);
-    
-		rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, 3.26f,  null, 2,  1001L,  51e+2,  new DataByteArray("purple"),null,"string0");
-    addResultRow(rows, 3.25f,  null, 2,  1000L,  53e+2,  new DataByteArray("orange"),null,"string0");
-    addResultRow(rows, 3.25f,  null, 2,  1001L,  51e+2,  new DataByteArray("green"), null,"string1");
-    
-    addResultRow(rows, 3.25f,  null, 2,  1000L,  50e+2,  new DataByteArray("blue"),  null,"string1");
-    addResultRow(rows, 3.24f,  null, 2,  1001L,  53e+2,  new DataByteArray("yellow"),null,"string1");
-    addResultRow(rows, 3.28f,  null, 2,  1001L,  51e+2,  new DataByteArray("white"), null,"string2");
-		resultTable.put(2, rows);
-		
-		// Verify union table
-		Iterator<Tuple> it = pigServer.openIterator("records1");
-		int numbRows = verifyTable(resultTable, 7, 2, it);
-		
-		Assert.assertEquals(numbRows, table1.length + table2.length + table5.length);
-	}
-	
-	/**
-	 * Disable this test case as it also belongs to a negative test case, which is already covered in test case 03.
-	 * @throws ExecException
-	 * @throws IOException
-	 */
-	//@Test
-	public void test_sorted_table_union_07() throws ExecException, IOException {
-		//
-		// Test sorted union
-		//
-		
-		// Create input tables for order preserve union
-		Map<String, String> inputTables = new HashMap<String, String>();  // Input two or more tables
-		inputTables.put("table1", "e,b,c");  // input table and sort keys
-		inputTables.put("table2", "e,b");  // input table and sort keys
-		inputTables.put("table5", "e");  // input table and sort keys
-		
-		// Test with input tables and provided output columns
-		testOrderPreserveUnion(inputTables, "e,source_table,b,c,int1,f,m1");
-		
-		// Verify union table
-		HashMap<Integer, ArrayList<ArrayList<Object>>> resultTable = new HashMap<Integer, ArrayList<ArrayList<Object>>>();
-		
-		ArrayList<ArrayList<Object>> rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, "Apple",	0,	100.0f,	1003L,	null,	new DataByteArray("Apple"),	m1);
-		addResultRow(rows, "Hadoop",	0,	28.0f,	1000L,	null,	new DataByteArray("Hadoop"),m1);
-		addResultRow(rows, "Pig",	0,	50.0f,	1000L,	null,	new DataByteArray("Pig"),	m1);
-		addResultRow(rows, "Zebra",	0,	-3.25f,	1001L,	null,	new DataByteArray("Zebra"),	m1);
-		addResultRow(rows, "apple",	0,	101.0f,	1001L,	null,	new DataByteArray("apple"),	m1);
-		addResultRow(rows, "hadoop",	0,	0.0f,	1002L,	null,	new DataByteArray("hadoop"),m1);
-		addResultRow(rows, "pig",	0,	52.0f,	1001L,	null,	new DataByteArray("pig"),	m1);
-		addResultRow(rows, "zebra",	0,	3.25f,	1000L,	null,	new DataByteArray("zebra"),	m1);
-		resultTable.put(0, rows);
-		
-		addResultRow(rows, "beige",  1,  33.0f,  1006L,  null, new DataByteArray("beige"), m2);
-    addResultRow(rows, "brown",  1,  53.0f,  1001L,  null, new DataByteArray("brown"), m2);
-    addResultRow(rows, "gray", 1,  50.0f,  1008L,  null, new DataByteArray("gray"),  m2);
-    addResultRow(rows, "green",  1,  56.0f,  1004L,  null, new DataByteArray("green"), m2);
-    addResultRow(rows, "orange", 1,  -99.0f, 1002L,  null, new DataByteArray("orange"),m2);
-    addResultRow(rows, "purple", 1,  102.0f, 1001L,  null, new DataByteArray("purple"),m2);
-    addResultRow(rows, "white",  1,  100.0f, 1003L,  null, new DataByteArray("white"), m2);
-		resultTable.put(1, rows);
-		
-		addResultRow(rows, "string0",2,  3.26f,  1001L,  100,  new DataByteArray("purple"),null);
-    addResultRow(rows, "string0",2,  3.25f,  1000L,  100,  new DataByteArray("orange"),null);
-    addResultRow(rows, "string1",2,  3.25f,  1001L,  100,  new DataByteArray("green"),null);
-    addResultRow(rows, "string1",2,  3.25f,  1000L,  100,  new DataByteArray("blue"),null);
-    addResultRow(rows, "string1",2,  3.24f,  1001L,  100,  new DataByteArray("yellow"),null);
-    addResultRow(rows, "string2",2,  3.28f,  1001L,  100,  new DataByteArray("white"),null);
-		resultTable.put(2, rows);
-		
-		// Verify union table
-		Iterator<Tuple> it = pigServer.openIterator("records1");
-		int numbRows = verifyTable(resultTable, 0, 1, it);
-		
-		Assert.assertEquals(numbRows, table1.length + table2.length + table5.length);
-	}
-	
-	@Test
-	public void test_sorted_table_union_08() throws ExecException, IOException {
-		//
-		// Test sorted union
-		//
-		
-		// Create input tables for order preserve union
-		Map<String, String> inputTables = new HashMap<String, String>();  // Input two or more tables
-		inputTables.put("table1", "e");  // input table and sort keys
-		inputTables.put("table2", "e");  // input table and sort keys
-		inputTables.put("table3", "e");  // input table and sort keys
-		inputTables.put("table5", "e");  // input table and sort keys
-		inputTables.put("table6", "e");  // input table and sort keys
-		
-		// Test with input tables and provided output columns
-		testOrderPreserveUnion(inputTables, "e,source_table,a,b,c,m1,int1,str1,str2,str3,str4,map1");
-		
-		// Verify union table
-		HashMap<Integer, ArrayList<ArrayList<Object>>> resultTable = new HashMap<Integer, ArrayList<ArrayList<Object>>>();
-		
-		ArrayList<ArrayList<Object>> rows = new ArrayList<ArrayList<Object>>();
-    addResultRow(rows, "Apple",  0,  1001, 100.0f, 1003L,  m1,   null, null, null, null, null, null);
-    addResultRow(rows, "Hadoop", 0,  1002, 28.0f,  1000L,  m1,   null, null, null, null, null, null);
-    addResultRow(rows, "Pig",  0,  1001, 50.0f,  1000L,  m1,   null, null, null, null, null, null);
-    addResultRow(rows, "Zebra",  0,  5,    -3.25f, 1001L,  m1,   null, null, null, null, null, null);
-    addResultRow(rows, "apple",  0,  1001, 101.0f, 1001L,  m1,   null, null, null, null, null, null);
-    addResultRow(rows, "hadoop", 0,  1000, 0.0f, 1002L,  m1,   null, null, null, null, null, null);
-    addResultRow(rows, "pig",  0,  1001, 52.0f,  1001L,  m1,   null, null, null, null, null, null);
-    addResultRow(rows, "zebra",  0,  -1,   3.25f,  1000L,  m1,   null, null, null, null, null, null);
-		resultTable.put(0, rows);
-    
-		rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, "a 1",  1,  null, null, null, null, 8,    "Reno",   "Nevada", null,   null,         m3);
-    addResultRow(rows, "a 2",  1,  null, null, null, null, 7,    "Dallas", "Texas",  null,   null,         m3);
-    addResultRow(rows, "a 3",  1,  null, null, null, null, 6,    "Phoenix",  "Arizona",  null,   null,         m3);
-    addResultRow(rows, "a 4",  1,  null, null, null, null, 5,    "New York", "New York", null,   null,         m3);
-    addResultRow(rows, "a 5",  1,  null, null, null, null, 4,    "Las Vegas","Nevada", null,   null,         m3);
-    addResultRow(rows, "a 6",  1,  null, null, null, null, 3,    "Santa Cruz","California",  null, null,         m3);
-    addResultRow(rows, "a 7",  1,  null, null, null, null, 2,    "San Jose","California",null,   null,         m3);
-    addResultRow(rows, "a 8",  1,  null, null, null, null, 1,    "Cupertino","California",null,  null,         m3);
-		resultTable.put(1, rows);
-		
-		rows = new ArrayList<ArrayList<Object>>();
-    addResultRow(rows, "beige",  2,  2000, 33.0f,  1006L,  m2,   null, null, null, null, null, null);
-    addResultRow(rows, "brown",  2,  1001, 53.0f,  1001L,  m2,   null, null, null, null, null, null);
-    addResultRow(rows, "gray", 2,  1001, 50.0f,  1008L,  m2,   null, null, null, null, null, null);
-    addResultRow(rows, "green",  2,  15,   56.0f,  1004L,  m2,   null, null, null, null, null, null);
-    addResultRow(rows, "orange", 2,  -1,   -99.0f, 1002L,  m2,   null, null, null, null, null, null);
-    addResultRow(rows, "purple", 2,  1001, 102.0f, 1001L,  m2,   null, null, null, null, null, null);
-    addResultRow(rows, "white",  2,  1001, 100.0f, 1003L,  m2,   null, null, null, null, null, null);
-		resultTable.put(2, rows);
-    
-		rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, "string0",3,  null, 3.26f,  1001L,  null, 100,  null, null, null, null, null);
-    addResultRow(rows, "string0",3,  null, 3.25f,  1000L,  null, 100,  null, null, null, null, null);
-    addResultRow(rows, "string1",3,  null, 3.25f,  1001L,  null, 100,  null, null, null, null, null);
-    addResultRow(rows, "string1",3,  null, 3.25f,  1000L,  null, 100,  null, null, null, null, null);
-    addResultRow(rows, "string1",3,  null, 3.24f,  1001L,  null, 100,  null, null, null, null, null);
-    addResultRow(rows, "string2",3,  null, 3.28f,  1001L,  null, 100,  null, null, null, null, null);
-		resultTable.put(3, rows);
-		
-		rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, "a 1",  4,  null, null, null, null, null, null,   null,   "Reno",   "Nevada", null);
-		addResultRow(rows, "a 2",  4,  null, null, null, null, null, null,   null,   "Dallas", "Texas",    null);
-		addResultRow(rows, "a 3",  4,  null, null, null, null, null, null,   null,   "Phoenix",  "Arizona",  null);
-		addResultRow(rows, "a 4",  4,  null, null, null, null, null, null,   null,   "New York", "New York",   null);
-		addResultRow(rows, "a 5",  4,  null, null, null, null, null, null,   null,   "Las Vegas","Nevada", null);
-		addResultRow(rows, "a 6",  4,  null, null, null, null, null, null,   null,   "Santa Cruz","California",    null);
-		addResultRow(rows, "a 7",  4,  null, null, null, null, null, null,   null,   "San Jose","California",  null);
-		addResultRow(rows, "a 8",  4,  null, null, null, null, null, null,   null,   "Cupertino","California",   null);
-		resultTable.put(4, rows);
-		
-		// Verify union table
-		Iterator<Tuple> it = pigServer.openIterator("records1");
-		int numbRows = verifyTable(resultTable, 0, 1, it);
-		
-		Assert.assertEquals(numbRows, table1.length + table2.length + table3.length +
-				table5.length + table6.length);
-	}
-	
-	@Test
-	public void test_sorted_table_union_09() throws ExecException, IOException {
-		//
-		// Test sorted union
-		//
-		
-		// Create input tables for order preserve union
-		Map<String, String> inputTables = new HashMap<String, String>();  // Input two or more tables
-		inputTables.put("table1", "e");  // input table and sort keys
-		inputTables.put("table2", "e");  // input table and sort keys
-		
-		// Test with input tables and provided output columns
-		testOrderPreserveUnion(inputTables, "e,source_table,xx,yy,zz"); // extra undefined columns
-		
-		// Verify union table
-		HashMap<Integer, ArrayList<ArrayList<Object>>> resultTable = new HashMap<Integer, ArrayList<ArrayList<Object>>>();
-		
-		ArrayList<ArrayList<Object>> rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, "Apple",	0,	null, null, null);
-		addResultRow(rows, "Hadoop",	0,	null, null, null);
-		addResultRow(rows, "Pig",	0,	null, null, null);
-		addResultRow(rows, "Zebra",	0,	null, null, null);
-		addResultRow(rows, "apple",	0,	null, null, null);
-		addResultRow(rows, "hadoop",	0,	null, null, null);
-		addResultRow(rows, "pig",	0,	null, null, null);
-		addResultRow(rows, "zebra",	0,	null, null, null);
-		resultTable.put(0, rows);
-		
-		rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, "beige",  1,  null, null, null);
-    addResultRow(rows, "brown",  1,  null, null, null);
-    addResultRow(rows, "gray", 1,  null, null, null);
-    addResultRow(rows, "green",  1,  null, null, null);
-    addResultRow(rows, "orange", 1,  null, null, null);
-    addResultRow(rows, "purple", 1,  null, null, null);
-    addResultRow(rows, "white",  1,  null, null, null);
-		resultTable.put(1, rows);
-    
-		// Verify union table
-		Iterator<Tuple> it = pigServer.openIterator("records1");
-		int numbRows = verifyTable(resultTable, 0, 1, it);
-		
-		Assert.assertEquals(numbRows, table1.length + table2.length);
-	}
-	
-	@Test
-	public void test_sorted_table_union_10() throws ExecException, IOException {
-		//
-		// Test sorted union
-		//
-		
-		// Create input tables for order preserve union
-		Map<String, String> inputTables = new HashMap<String, String>();  // Input two or more tables
-		inputTables.put("table1", "a,b");  // input table and sort keys
-		inputTables.put("table2", "a,b");  // input table and sort keys
-		
-		// Test with input tables and provided output columns
-		testOrderPreserveUnion(inputTables, "source_table, a,  b,  c  ,,,  d ,  e,,,"); // extra commas and spaces
-		
-		// Verify union table
-		HashMap<Integer, ArrayList<ArrayList<Object>>> resultTable = new HashMap<Integer, ArrayList<ArrayList<Object>>>();
-		
-		ArrayList<ArrayList<Object>> rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, 0, -1,	3.25f,	1000L,	null, null,	50e+2,	"zebra");
-		addResultRow(rows, 0, 5,	-3.25f,	1001L,	null, null,	51e+2,	"Zebra");
-		addResultRow(rows, 0, 1000,	0.0f,	1002L,	null, null,	52e+2,	"hadoop");
-		addResultRow(rows, 0, 1001,	50.0f,	1000L,	null, null,	50e+2,	"Pig");
-		addResultRow(rows, 0, 1001,	52.0f,	1001L,	null, null,	50e+2,	"pig");
-		addResultRow(rows, 0, 1001,	100.0f,	1003L,	null, null,	50e+2,	"Apple");
-		addResultRow(rows, 0, 1001,	101.0f,	1001L,	null, null,	50e+2,	"apple");
-		addResultRow(rows, 0, 1002,	28.0f,	1000L,	null, null,	50e+2,	"Hadoop");
-		resultTable.put(0, rows);
-		
-		rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, 1, -1,	-99.0f,	1002L,	null, null,	51e+2,	"orange");
-		addResultRow(rows, 1, 15, 56.0f,  1004L,  null, null, 50e+2,  "green");
-		addResultRow(rows, 1, 1001, 50.0f,  1008L,  null, null, 52e+2,  "gray");
-		addResultRow(rows, 1, 1001, 53.0f,  1001L,  null, null, 52e+2,  "brown");
-		addResultRow(rows, 1, 1001, 100.0f, 1003L,  null, null, 55e+2,  "white");
-		addResultRow(rows, 1, 1001, 102.0f, 1001L,  null, null, 52e+2,  "purple");
-		addResultRow(rows, 1, 2000, 33.0f,  1006L,  null, null, 52e+2,  "beige");
-		resultTable.put(1, rows);
-		
-		// Verify union table
-		Iterator<Tuple> it = pigServer.openIterator("records1");
-		int numbRows = verifyTable(resultTable, 1, 0, it);
-		
-		Assert.assertEquals(numbRows, table1.length + table2.length);
-	}
-	
-	@Test
-	public void test_sorted_table_union_11() throws ExecException, IOException {
-		//
-		// Test sorted union
-		//
-		
-		// Create input tables for order preserve union
-		Map<String, String> inputTables = new HashMap<String, String>();  // Input two or more tables
-		inputTables.put("table1", "e");  // input table and sort keys
-		inputTables.put("table2", "e");  // input table and sort keys
-		
-		// Test with input tables and provided output columns
-		testOrderPreserveUnion(inputTables, "source_table,b,c,int1,f,m1");  // sort key e not in projection
-		
-		// Verify union table
-		HashMap<Integer, ArrayList<ArrayList<Object>>> resultTable = new HashMap<Integer, ArrayList<ArrayList<Object>>>();
-		
-		ArrayList<ArrayList<Object>> rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, 0,	100.0f,	1003L,	null,	new DataByteArray("Apple"),	m1);
-		addResultRow(rows, 0,	28.0f,	1000L,	null,	new DataByteArray("Hadoop"),m1);
-		addResultRow(rows, 0,	50.0f,	1000L,	null,	new DataByteArray("Pig"),	m1);
-		addResultRow(rows, 0,	-3.25f,	1001L,	null,	new DataByteArray("Zebra"),	m1);
-		addResultRow(rows, 0,	101.0f,	1001L,	null,	new DataByteArray("apple"),	m1);
-		addResultRow(rows, 0,	0.0f,	1002L,	null,	new DataByteArray("hadoop"),m1);
-		addResultRow(rows, 0,	52.0f,	1001L,	null,	new DataByteArray("pig"),	m1);
-		addResultRow(rows, 0,	3.25f,	1000L,	null,	new DataByteArray("zebra"),	m1);
-		resultTable.put(0, rows);
-		
-		rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, 1,  33.0f,  1006L,  null, new DataByteArray("beige"), m2);
-    addResultRow(rows, 1,  53.0f,  1001L,  null, new DataByteArray("brown"), m2);
-    addResultRow(rows, 1,  50.0f,  1008L,  null, new DataByteArray("gray"),  m2);
-    addResultRow(rows, 1,  56.0f,  1004L,  null, new DataByteArray("green"), m2);
-    addResultRow(rows, 1,  -99.0f, 1002L,  null, new DataByteArray("orange"),m2);
-    addResultRow(rows, 1,  102.0f, 1001L,  null, new DataByteArray("purple"),m2);
-    addResultRow(rows, 1,  100.0f, 1003L,  null, new DataByteArray("white"), m2);
-		resultTable.put(1, rows);
-    
-		// Verify union table
-		Iterator<Tuple> it = pigServer.openIterator("records1");
-		int numbRows = verifyTable(resultTable, 4, 0, it);
-		
-		Assert.assertEquals(numbRows, table1.length + table2.length);
-	}
-	
-	/**
-	 * Compare table rows
-	 * 
-	 */
-	private boolean compareRow(Tuple rowValues, ArrayList<Object> resultRow) throws IOException {
-		boolean result = true;
-		Assert.assertEquals(resultRow.size(), rowValues.size());
-		for (int i = 0; i < rowValues.size(); ++i) {
-			if (! compareObj(rowValues.get(i), resultRow.get(i)) ) {
-				result = false;
-				break;
-			}
-		}
-		return result;
-	}
-	
-	/**
-	 * Compare table values
-	 * 
-	 */
-	private boolean compareObj(Object object1, Object object2) {
-		if (object1 == null) {
-			if (object2 == null)
-				return true;
-			else
-				return false;
-		} else if (object1.equals(object2))
-			return true;
-		else
-			return false;
-	}
-	
-	/**
-	 *Add a row to expected results table
-	 * 
-	 */
-	private void addResultRow(ArrayList<ArrayList<Object>> resultTable, Object ... values) {
-		ArrayList<Object> resultRow = new ArrayList<Object>();
-		
-		for (int i = 0; i < values.length; i++) {
-			resultRow.add(values[i]);
-		}
-		resultTable.add(resultRow);
-	}
-	
-	/**
-	 * Print Pig Table (for debugging)
-	 * 
-	 */
-	private int printTable(String tablename) throws IOException {
-		Iterator<Tuple> it1 = pigServer.openIterator(tablename);
-		int numbRows = 0;
-		while (it1.hasNext()) {
-			Tuple RowValue1 = it1.next();
-			++numbRows;
-			System.out.println();
-			for (int i = 0; i < RowValue1.size(); ++i)
-				System.out.println("DEBUG: " + tablename + " RowValue.get(" + i + ") = " + RowValue1.get(i));
-		}
-		System.out.println("\nRow count : " + numbRows);
-		return numbRows;
-	}
-	
-	/**
-	 * Print loaded tables with indexes (for debugging)
-	 * 
-	 */
-	private void printTableList() {
-		System.out.println();
-		for (int i=0; i<tableList.size(); ++i) {
-			System.out.println("Load table  " + i + " : " + tableList.get(i));
-		}
-	}
-	
-	/**
-	 * Return the name of the routine that called getCurrentMethodName
-	 * 
-	 */
-	private String getCurrentMethodName() {
-		ByteArrayOutputStream baos = new ByteArrayOutputStream();
-		PrintWriter pw = new PrintWriter(baos);
-		(new Throwable()).printStackTrace(pw);
-		pw.flush();
-		String stackTrace = baos.toString();
-		pw.close();
-		
-		StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-		tok.nextToken(); // 'java.lang.Throwable'
-		tok.nextToken(); // 'at ...getCurrentMethodName'
-		String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-		// Parse line 3
-		tok = new StringTokenizer(l.trim(), " <(");
-		String t = tok.nextToken(); // 'at'
-		t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-		return t;
-	}
-	
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestOrderPreserveProjectionNegative.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestOrderPreserveProjectionNegative.java
deleted file mode 100644
index e259be01e..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestOrderPreserveProjectionNegative.java
+++ /dev/null
@@ -1,820 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.io.StringWriter;
-import java.io.Writer;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.ArrayList;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.pig.TableStorer;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.executionengine.ExecJob;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-
-public class TestOrderPreserveProjectionNegative extends BaseTestCase {
-	
-	final static String TABLE1_SCHEMA = "a:int,b:float,c:long,d:double,e:string,f:bytes,m1:map(string)";
-	final static String TABLE1_STORAGE = "[a, b, c]; [d, e, f]; [m1#{a}]";
-	
-	final static String TABLE2_SCHEMA = "a:int,b:float,c:long,d:double,e:string,f:bytes,m1:map(string)";
-	final static String TABLE2_STORAGE = "[a, b, c]; [d, e, f]; [m1#{a}]";
-	
-	final static String TABLE3_SCHEMA = "e:string,str1:string,str2:string,int1:int,map1:map(string)";
-	final static String TABLE3_STORAGE = "[e, str1]; [str2,int1,map1#{a}]";
-	
-	final static String TABLE4_SCHEMA = "int1:int,str1:string,long1:long";
-	final static String TABLE4_STORAGE = "[int1]; [str1,long1]";
-	
-	final static String TABLE5_SCHEMA = "b:float,d:double,c:long,e:string,f:bytes,int1:int";
-	final static String TABLE5_STORAGE = "[b,d]; [c]; [e]; [f,int1]";
-	
-	final static String TABLE6_SCHEMA = "a:string,b:string,str1:string";
-	final static String TABLE6_STORAGE = "[a,b,str1]";
-	
-	final static String TABLE7_SCHEMA = "int2:int";
-	final static String TABLE7_STORAGE = "[int2]";
-	
-	static int fileId = 0;
-	static int sortId = 0;
-	
-	protected static ExecJob pigJob;
-	
-	private static Path pathTable1;
-	private static Path pathTable2;
-	private static Path pathTable3;
-	private static Path pathTable4;
-	private static Path pathTable5;
-	private static Path pathTable6;
-	private static Path pathTable7;
-	private static HashMap<String, String> tableStorage;
-	
-	private static Object[][] table1;
-	private static Object[][] table2;
-	private static Object[][] table3;
-	private static Object[][] table4;
-	private static Object[][] table5;
-	private static Object[][] table6;
-	private static Object[][] table7;
-
-  @BeforeClass
-  public static void setUp() throws Exception {
-      init();
-      pathTable1 = getTableFullPath("TestOrderPerserveProjectionNegative1");
-      pathTable2 = getTableFullPath("TestOrderPerserveProjectionNegative2");
-      pathTable3 = getTableFullPath("TestOrderPerserveProjectionNegative3");
-      pathTable4 = getTableFullPath("TestOrderPerserveProjectionNegative4");
-      pathTable5 = getTableFullPath("TestOrderPerserveProjectionNegative5");
-      pathTable6 = getTableFullPath("TestOrderPerserveProjectionNegative6");
-      pathTable7 = getTableFullPath("TestOrderPerserveProjectionNegative7");
-      
-      removeDir(pathTable1);
-      removeDir(pathTable2);
-      removeDir(pathTable3);
-      removeDir(pathTable4);
-      removeDir(pathTable5);
-      removeDir(pathTable6);
-      removeDir(pathTable7);
-
-		// Create table1 data
-		Map<String, String> m1 = new HashMap<String, String>();
-		m1.put("a","m1-a");
-		m1.put("b","m1-b");
-		
-		table1 = new Object[][]{
-			{5,		-3.25f,	1001L,	51e+2,	"Zebra",	new DataByteArray("Zebra"),		m1},
-			{-1,	3.25f,	1000L,	50e+2,	"zebra",	new DataByteArray("zebra"),		m1},
-			{1001,	100.0f,	1003L,	50e+2,	"Apple",	new DataByteArray("Apple"),		m1},
-			{1001,	101.0f,	1001L,	50e+2,	"apple",	new DataByteArray("apple"),		m1},
-			{1001,	50.0f,	1000L,	50e+2,	"Pig",		new DataByteArray("Pig"),		m1},
-			{1001,	52.0f,	1001L,	50e+2,	"pig",		new DataByteArray("pig"),		m1},
-			{1002,	28.0f,	1000L,	50e+2,	"Hadoop",	new DataByteArray("Hadoop"),	m1},
-			{1000,	0.0f,	1002L,	52e+2,	"hadoop",	new DataByteArray("hadoop"),	m1} };
-		
-		// Create table1
-		createTable(pathTable1, TABLE1_SCHEMA, TABLE1_STORAGE, table1);
-		
-		// Create table2 data
-		Map<String, String> m2 = new HashMap<String, String>();
-		m2.put("a","m2-a");
-		m2.put("b","m2-b");
-		
-		table2 = new Object[][] {
-			{15,	56.0f,	1004L,	50e+2,	"green",	new DataByteArray("green"),		m2},
-			{-1,	-99.0f,	1002L,	51e+2,	"orange",	new DataByteArray("orange"),	m2},
-			{1001,	100.0f,	1003L,	55e+2,	"white",	new DataByteArray("white"),		m2},
-			{1001,	102.0f,	1001L,	52e+2,	"purple",	new DataByteArray("purple"),	m2},
-			{1001,	50.0f,	1008L,	52e+2,	"gray",		new DataByteArray("gray"),		m2},
-			{1001,	53.0f,	1001L,	52e+2,	"brown",	new DataByteArray("brown"),		m2},
-			{2000,	33.0f,	1006L,	52e+2,	"beige",	new DataByteArray("beige"),		m2} };
-		
-		// Create table2
-		createTable(pathTable2, TABLE2_SCHEMA, TABLE2_STORAGE, table2);
-		
-		// Create table3 data
-		Map<String, String> m3 = new HashMap<String, String>();
-		m3.put("a","m3-a");
-		m3.put("b","m3-b");
-		m3.put("c","m3-b");
-		
-		table3 = new Object[][] {
-			{"a 8",	"Cupertino",	"California",	1,	m3},
-			{"a 7",	"San Jose",		"California",	2,	m3},
-			{"a 6",	"Santa Cruz",	"California",	3,	m3},
-			{"a 5",	"Las Vegas",	"Nevada",		4,	m3},
-			{"a 4",	"New York",		"New York",		5,	m3},
-			{"a 3",	"Phoenix",		"Arizona",		6,	m3},
-			{"a 2",	"Dallas",		"Texas",		7,	m3},
-			{"a 1",	"Reno",			"Nevada",		8,	m3} };
-		
-		// Create table3
-		createTable(pathTable3, TABLE3_SCHEMA, TABLE3_STORAGE, table3);
-		
-		// Create table4 data
-		table4 = new Object[][] {
-			{1,	"Cupertino",	1001L},
-			{2,	"San Jose",		1008L},
-			{3,	"Santa Cruz",	1008L},
-			{4,	"Las Vegas",	1008L},
-			{5,	"Dallas",		1010L},
-			{6,	"Reno",			1000L} };
-		
-		// Create table4
-		createTable(pathTable4, TABLE4_SCHEMA, TABLE4_STORAGE, table4);
-		
-		// Create table5 data
-		table5 = new Object[][] {
-			{3.25f,		51e+2,	1001L,	"string1",	new DataByteArray("green"),	100},
-			{3.25f,		51e+2,	1001L,	"string1",	new DataByteArray("green"),	100},
-			{3.25f,		51e+2,	1001L,	"string1",	new DataByteArray("green"),	100},
-			{3.25f,		51e+2,	1001L,	"string1",	new DataByteArray("green"),	100},
-			{3.25f,		51e+2,	1001L,	"string1",	new DataByteArray("green"),	100},
-			{3.25f,		51e+2,	1001L,	"string1",	new DataByteArray("green"),	100} };
-		
-		// Create table5
-		createTable(pathTable5, TABLE5_SCHEMA, TABLE5_STORAGE, table5);
-		
-		// Create table6 data
-		table6 = new Object[][] {
-			{"a 8",	"Cupertino",	"California"},
-			{"a 7",	"San Jose",		"California"},
-			{"a 6",	"Santa Cruz",	"California"},
-			{"a 5",	"Las Vegas",	"Nevada"},
-			{"a 4",	"New York",		"New York"},
-			{"a 3",	"Phoenix",		"Arizona"},
-			{"a 2",	"Dallas",		"Texas"},
-			{"a 1",	"Reno",			"Nevada"} };
-		
-		// Create table6
-		createTable(pathTable6, TABLE6_SCHEMA, TABLE6_STORAGE, table6);
-		
-		// Create table7 data
-		table7 = new Object[][] {
-			{1001},
-			{1000},
-			{1010},
-			{1009},
-			{1001} };
-		
-		// Create table7
-		createTable(pathTable7, TABLE7_SCHEMA, TABLE7_STORAGE, table7);
-		
-		// Load tables
-		String query1 = "table1 = LOAD '" + pathTable1.toString() + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		pigServer.registerQuery(query1);
-		String query2 = "table2 = LOAD '" + pathTable2.toString() + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		pigServer.registerQuery(query2);
-		String query3 = "table3 = LOAD '" + pathTable3.toString() + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		pigServer.registerQuery(query3);
-		String query4 = "table4 = LOAD '" + pathTable4.toString() + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		pigServer.registerQuery(query4);
-		String query5 = "table5 = LOAD '" + pathTable5.toString() + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		pigServer.registerQuery(query5);
-		String query6 = "table6 = LOAD '" + pathTable6.toString() + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		pigServer.registerQuery(query6);
-		String query7 = "table7 = LOAD '" + pathTable7.toString() + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		pigServer.registerQuery(query7);
-		String query8 = "table8 = LOAD '" + pathTable7.toString() + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		pigServer.registerQuery(query8);
-		
-		// Map for table storage
-		tableStorage = new HashMap<String, String>();
-		tableStorage.put("table1", TABLE1_STORAGE);
-		tableStorage.put("table2", TABLE2_STORAGE);
-		tableStorage.put("table3", TABLE3_STORAGE);
-		tableStorage.put("table4", TABLE4_STORAGE);
-		tableStorage.put("table5", TABLE5_STORAGE);
-		tableStorage.put("table6", TABLE6_STORAGE);
-		tableStorage.put("table7", TABLE7_STORAGE);
-	}
-	
-	private static void createTable(Path path, String schemaString, String storageString, Object[][] tableData)
-			throws IOException {
-		//
-		// Create table from tableData array
-		//
-		BasicTable.Writer writer = new BasicTable.Writer(path, schemaString, storageString, conf);
-		
-		Schema schema = writer.getSchema();
-		Tuple tuple = TypesUtils.createTuple(schema);
-		TableInserter inserter = writer.getInserter("ins", false);
-		
-		for (int i = 0; i < tableData.length; ++i) {
-			TypesUtils.resetTuple(tuple);
-			for (int k = 0; k < tableData[i].length; ++k) {
-				tuple.set(k, tableData[i][k]);
-				System.out.println("DEBUG: setting tuple k=" + k + "value= " + tableData[i][k]);
-			}
-			inserter.insert(new BytesWritable(("key" + i).getBytes()), tuple);
-		}
-		inserter.close();
-		writer.close();
-	}
-	
-	@AfterClass
-	public static void tearDown() throws Exception {
-		pigServer.shutdown();
-	}
-	
-	private Iterator<Tuple> testOrderPreserveUnion(Map<String, String> inputTables, String columns) throws IOException {
-		//
-		// Test order preserve union from input tables and provided output columns
-		//
-		Assert.assertTrue("Table union requires two or more input tables", inputTables.size() >= 2);
-		
-		Path newPath = new Path(getCurrentMethodName());
-		ArrayList<String> pathList = new ArrayList<String>();
-		
-		// Load and store each of the input tables
-		for (Iterator<String> it = inputTables.keySet().iterator(); it.hasNext();) {
-			
-			String tablename = it.next();
-			String sortkey = inputTables.get(tablename);
-			
-			String sortName = "sort" + ++sortId;
-			
-			// Sort tables
-			String orderby = sortName + " = ORDER " + tablename + " BY " + sortkey + " ;";   // need single quotes?
-			pigServer.registerQuery(orderby);
-			
-			String sortPath = new String(newPath.toString() + ++fileId);  // increment fileId suffix
-			
-			// Store sorted tables
-			pigJob = pigServer.store(sortName, sortPath, TableStorer.class.getCanonicalName() +
-				"('" + tableStorage.get(tablename) + "')");
-			Assert.assertNull(pigJob.getException());
-			
-			pathList.add(sortPath);  // add table path to list
-		}
-		
-		String paths = new String();
-		for (String path:pathList)
-			paths += path + ",";
-		paths = paths.substring(0, paths.lastIndexOf(","));  // remove trailing comma
-		
-		String queryLoad = "records1 = LOAD '"
-	        + paths
-	        +	"' USING org.apache.hadoop.zebra.pig.TableLoader('" + columns + "', 'sorted');";
-		
-		System.out.println("queryLoad: " + queryLoad);
-		
-		pigServer.registerQuery(queryLoad);
-		
-		// Return iterator
-		Iterator<Tuple> it1 = pigServer.openIterator("records1");
-		return it1;
-	}
-	
-	@Test
-	public void union_error_exist_source_table() throws ExecException {
-		//
-		// Test that we do not allow user to create table with column "source_table" (Negative test)
-		//
-		IOException exception = null;
-		
-		String STR_SCHEMA_TEST = "a:string,b:string,source_table:int";
-		String STR_STORAGE_TEST = "[a, b]; [source_table]";
-		
-		// Create table1 data
-		Object[][] table_test = new Object[][] {
-			{"a1",	"z",	5},
-			{"a2",	"r",	4},
-			{"a3",	"e",	3},
-			{"a4",	"a",	1} };
-		
-//		Path pathTable1 = new Path(pathWorking, "table_test");
-		
-		try {
-			// Create table1
-			createTable(pathTable1, STR_SCHEMA_TEST, STR_STORAGE_TEST, table_test);
-			
-		} catch (IOException e) {
-			exception = e;
-		} finally {
-			//System.out.println(getStackTrace(exception));
-			Assert.assertNotNull(exception);
-			Assert.assertTrue(exception.toString().contains("[source_table] is a reserved virtual column name"));
-		}
-	}
-	
-	@Test
-	public void union_error_diff_type() throws ExecException {
-		//
-		// Test sorted union error handling when tables have same sort key of different types (Negative test)
-		//
-		IOException exception = null;
-		
-		// Create input tables for order preserve union
-		Map<String, String> inputTables = new HashMap<String, String>();  // Input two or more tables
-		inputTables.put("table1", "a");  // input table and sort keys
-		inputTables.put("table6", "a");  // input table and sort keys
-		
-		try {
-			// Test with input tables and provided output columns
-			testOrderPreserveUnion(inputTables, "a,b,c,d,e,f,m1");
-			
-		} catch (IOException e) {
-			exception = e;
-		} finally {
-			//System.out.println(getStackTrace(exception));
-			Assert.assertNotNull(exception);
-			Assert.assertTrue(getStackTrace(exception).contains("Different types of column"));
-		}
-	}
-	
-	@Test
-	public void union_error_complex_type() throws ExecException {
-		//
-		// Test sorted union error handling when tables have complex map sort keys (Negative test)
-		//
-		IOException exception = null;
-		
-		// Create input tables for order preserve union
-		Map<String, String> inputTables = new HashMap<String, String>();  // Input two or more tables
-		inputTables.put("table1", "m1");  // input table and sort keys
-		inputTables.put("table2", "m1");  // input table and sort keys
-		
-		try {
-			// Test with input tables and provided output columns
-			testOrderPreserveUnion(inputTables, "a,b,c,d,e,f,m1");
-			
-		} catch (IOException e) {
-			exception = e;
-		} finally {
-			//System.out.println(getStackTrace(exception));
-			Assert.assertNotNull(exception);
-			Assert.assertTrue(getStackTrace(exception).contains("not of simple type as required for a sort column now"));
-		}
-	}
-	
-	@Test
-	public void union_error_invalid_path1() throws ExecException {
-		//
-		// Test sorted union error handling when one of the table paths is invalid (Negative test)
-		//
-		IOException exception = null;
-		
-		try {
-			// Sort tables
-			String orderby1 = "sort1 = ORDER table1 BY " + "a" + " ;";
-			pigServer.registerQuery(orderby1);
-			
-			Path newPath = new Path(getCurrentMethodName());
-			
-			// Store sorted tables
-			String pathSort1 = newPath.toString() + "1";
-			pigJob = pigServer.store("sort1", pathSort1, TableStorer.class.getCanonicalName() +
-				"('" + TABLE1_STORAGE + "')");
-			Assert.assertNull(pigJob.getException());
-			
-			String pathSort2 = newPath.toString() + "2";  // invalid path
-			
-			String queryLoad = "records1 = LOAD '"
-		        + pathSort1 + ","
-		        + pathSort2
-		        +	"' USING org.apache.hadoop.zebra.pig.TableLoader('a,b,c', 'sorted');";
-			pigServer.registerQuery(queryLoad);
-			
-		} catch (IOException e) {
-			exception = e;
-		} finally {
-			//System.out.println(getStackTrace(exception));
-			Assert.assertNotNull(exception);
-                        Assert.assertTrue(getStackTrace(exception).contains("Input path does not exist: "));
-		}
-	}
-	
-	@Test
-	public void union_error_invalid_path2() throws ExecException {
-		//
-		// Test sorted union error handling when one of the table paths is invalid (Negative test)
-		//
-		IOException exception = null;
-		String pathSort2 = null;
-		
-		try {
-			// Sort tables
-			String orderby1 = "sort1 = ORDER table1 BY " + "a" + " ;";
-			pigServer.registerQuery(orderby1);
-			
-			Path newPath = new Path(getCurrentMethodName());
-			
-			// Store sorted tables
-			String pathSort1 = newPath.toString() + "1";
-			pigJob = pigServer.store("sort1", pathSort1, TableStorer.class.getCanonicalName() +
-				"('" + TABLE1_STORAGE + "')");
-			Assert.assertNull(pigJob.getException());
-			
-			pathSort2 = newPath.toString() + "2";  // invalid path
-			
-			String queryLoad = "records1 = LOAD '"
-		        + pathSort1 + ","
-		        + pathSort2
-		        +	"' USING org.apache.hadoop.zebra.pig.TableLoader('a,b,c,source_table', 'sorted');";
-			pigServer.registerQuery(queryLoad);
-			
-			printTable("records1");
-			
-		} catch (IOException e) {
-			exception = e;
-		} finally {
-			System.out.println(getStackTrace(exception));
-			//Assert.assertNotNull(exception);
-			//Assert.assertTrue(getStackTrace(exception).contains("Schema file doesn't exist"));
-		}
-	}
-	
-	@Test
-	public void union_error_invalid_column() throws ExecException {
-		//
-		// Test sorted union error handling when some of the output columns are not in any
-		// of the tables (Negative test)
-		//
-		IOException exception = null;
-		
-		// Create input tables for order preserve union
-		Map<String, String> inputTables = new HashMap<String, String>();  // Input two or more tables
-		inputTables.put("table1", "e");  // input table and sort keys
-		inputTables.put("table2", "e");  // input table and sort keys
-		
-		try {
-			// Test with input tables and provided output columns
-			testOrderPreserveUnion(inputTables, "e,source_table,a,xx,yy,zz");
-			
-			printTable("records1");
-			
-		} catch (IOException e) {
-			exception = e;
-		} finally {
-			System.out.println(getStackTrace(exception));
-			//Assert.assertNotNull(exception);
-			//Assert.assertTrue(getStackTrace(exception).contains("Using Map as key not supported"));
-		}
-	}
-	
-	@Test
-	public void union_error_invalid_key() throws ExecException {
-		//
-		// Test sorted union error handling where column sort keys are missing from
-		// projection (Negative test)
-		//
-		IOException exception = null;
-		
-		// Create input tables for order preserve union
-		Map<String, String> inputTables = new HashMap<String, String>();  // Input two or more tables
-		inputTables.put("table1", "e");  // input table and sort keys
-		inputTables.put("table2", "e");  // input table and sort keys
-		
-		try {
-			// Test with input tables and provided output columns
-			testOrderPreserveUnion(inputTables, "a");
-			
-			printTable("records1");
-			
-		} catch (IOException e) {
-			exception = e;
-		} finally {
-			System.out.println(getStackTrace(exception));
-			//Assert.assertNotNull(exception);
-			//Assert.assertTrue(getStackTrace(exception).contains("Using Map as key not supported"));
-		}
-	}
-	
-	@Test
-	public void union_error_unsorted_left() throws ExecException {
-		//
-		// Test sorted union error handling where left table is not sorted (Negative test)
-		//
-		IOException exception = null;
-		
-		try {
-			// Sort tables
-			String orderby2 = "sort2 = ORDER table2 BY " + "a" + " ;";
-			pigServer.registerQuery(orderby2);
-			
-			Path newPath = new Path(getCurrentMethodName());
-			
-			// Store sorted tables
-			String pathSort1 = newPath.toString() + "1";
-			pigJob = pigServer.store("table1", pathSort1, TableStorer.class.getCanonicalName() +
-				"('" + TABLE1_STORAGE + "')");
-			Assert.assertNull(pigJob.getException());
-			
-			String pathSort2 = newPath.toString() + "2";
-			pigJob = pigServer.store("sort2", pathSort2, TableStorer.class.getCanonicalName() +
-				"('" + TABLE2_STORAGE + "')");
-			Assert.assertNull(pigJob.getException());
-			
-			String queryLoad = "records1 = LOAD '"
-		        + pathSort1 + ","
-		        + pathSort2
-		        +	"' USING org.apache.hadoop.zebra.pig.TableLoader('a,source_table,b,c,d,e,f,m1', 'sorted');";
-			pigServer.registerQuery(queryLoad);
-			
-		} catch (IOException e) {
-			exception = e;
-		} finally {
-			//System.out.println(getStackTrace(exception));
-			Assert.assertNotNull(exception);
-			Assert.assertTrue(getStackTrace(exception).contains("The table is not sorted"));
-		}
-	}
-	
-	@Test
-	public void union_error_unsorted_right() throws ExecException {
-		//
-		// Test sorted union error handling where right table is not sorted (Negative test)
-		//
-		IOException exception = null;
-		
-		try {
-			// Sort tables
-			String orderby1 = "sort1 = ORDER table1 BY " + "a" + " ;";
-			pigServer.registerQuery(orderby1);
-			
-			Path newPath = new Path(getCurrentMethodName());
-			
-			// Store sorted tables
-			String pathSort1 = newPath.toString() + "1";
-			pigJob = pigServer.store("sort1", pathSort1, TableStorer.class.getCanonicalName() +
-				"('" + TABLE1_STORAGE + "')");
-			Assert.assertNull(pigJob.getException());
-			
-			String pathSort2 = newPath.toString() + "2";
-			pigJob = pigServer.store("table2", pathSort2, TableStorer.class.getCanonicalName() +
-				"('" + TABLE2_STORAGE + "')");
-			Assert.assertNull(pigJob.getException());
-			
-			String queryLoad = "records1 = LOAD '"
-		        + pathSort1 + ","
-		        + pathSort2
-		        +	"' USING org.apache.hadoop.zebra.pig.TableLoader('a,source_table,b,c,d,e,f,m1', 'sorted');";
-			pigServer.registerQuery(queryLoad);
-			
-		} catch (IOException e) {
-			exception = e;
-		} finally {
-			//System.out.println(getStackTrace(exception));
-			Assert.assertNotNull(exception);
-			Assert.assertTrue(getStackTrace(exception).contains("The table is not sorted"));
-		}
-	}
-	
-	@Test
-	public void union_error_unsorted_middle() throws ExecException {
-		//
-		// Test sorted union error handling where middle table is not sorted (Negative test)
-		//
-		IOException exception = null;
-		
-		try {
-			// Sort tables
-			String orderby1 = "sort1 = ORDER table1 BY " + "e" + " ;";
-			pigServer.registerQuery(orderby1);
-			
-			String orderby3 = "sort3 = ORDER table3 BY " + "e" + " ;";
-			pigServer.registerQuery(orderby3);
-			
-			Path newPath = new Path(getCurrentMethodName());
-			
-			// Store sorted tables
-			String pathSort1 = newPath.toString() + "1";
-			pigJob = pigServer.store("sort1", pathSort1, TableStorer.class.getCanonicalName() +
-				"('" + TABLE1_STORAGE + "')");
-			Assert.assertNull(pigJob.getException());
-			
-			String pathSort2 = newPath.toString() + "2";
-			pigJob = pigServer.store("table2", pathSort2, TableStorer.class.getCanonicalName() +
-				"('" + TABLE2_STORAGE + "')");
-			Assert.assertNull(pigJob.getException());
-			
-			String pathSort3 = newPath.toString() + "3";
-			pigJob = pigServer.store("sort3", pathSort3, TableStorer.class.getCanonicalName() +
-				"('" + TABLE3_STORAGE + "')");
-			Assert.assertNull(pigJob.getException());
-			
-			String queryLoad = "records1 = LOAD '"
-		        + pathSort1 + ","
-		        + pathSort2 + ","
-		        + pathSort3
-		        +	"' USING org.apache.hadoop.zebra.pig.TableLoader('e,source_table,a,b,c,d,f,m1,str1', 'sorted');";
-			pigServer.registerQuery(queryLoad);
-			
-		} catch (IOException e) {
-			exception = e;
-		} finally {
-			//System.out.println(getStackTrace(exception));
-			Assert.assertNotNull(exception);
-			Assert.assertTrue(getStackTrace(exception).contains("The table is not sorted"));
-		}
-	}
-	
-	@Test
-	public void union_error_sort_key() throws ExecException {
-		//
-		// Test sorted union error handling with mixed sort keys (Negative test)
-		//
-		IOException exception = null;
-		
-		// Create input tables for order preserve union
-		Map<String, String> inputTables = new HashMap<String, String>();  // Input two or more tables
-		inputTables.put("table1", "a");  // input table and sort keys
-		inputTables.put("table2", "b");  // input table and sort keys
-		inputTables.put("table3", "e");  // input table and sort keys
-		
-		try {
-			// Test with input tables and provided output columns
-			testOrderPreserveUnion(inputTables, "source_table,a,b,e");
-			
-		} catch (IOException e) {
-			exception = e;
-		} finally {
-			//System.out.println(getStackTrace(exception));
-			Assert.assertNotNull(exception);
-			Assert.assertTrue(getStackTrace(exception).contains("does not exist in schema"));
-		}
-	}
-	
-	@Test
-	public void union_error_sort_key_partial() throws ExecException {
-		//
-		// Test sorted union error handling with mixed multiple sort keys (Negative test)
-		//
-		IOException exception = null;
-		
-		// Create input tables for order preserve union
-		Map<String, String> inputTables = new HashMap<String, String>();  // Input two or more tables
-		inputTables.put("table1", "a,e");  // input table and sort keys
-		inputTables.put("table2", "a,b");  // input table and sort keys
-		
-		try {
-			// Test with input tables and provided output columns
-			testOrderPreserveUnion(inputTables, "a,e,b");
-			
-		} catch (IOException e) {
-			exception = e;
-		} finally {
-			//System.out.println(getStackTrace(exception));
-			Assert.assertNotNull(exception);
-			Assert.assertTrue(getStackTrace(exception).contains("The table is not properly sorted"));
-		}
-	}
-	
-	@Test
-	public void test_pig_foreach_error() throws ExecException, IOException {
-		//
-		// Test sorted union after Pig foreach that does not change the sort order (Negative test)
-		// The union will fail as no sort info is added when store is done after Pig foreach statement
-		//
-		IOException exception = null;
-		
-		// Sort tables
-		String orderby1 = "sort1 = ORDER table1 BY " + "a" + " ;";
-		pigServer.registerQuery(orderby1);
-		
-		String orderby2 = "sort2 = ORDER table2 BY " + "a" + " ;";
-		pigServer.registerQuery(orderby2);
-		
-		// Table after pig foreach
-		String foreach2 = "foreachsort2 = FOREACH sort2 GENERATE a as a, b as b, c as c, d as d, e as e, f as f, m1 as m1;";
-		pigServer.registerQuery(foreach2);
-		
-		Path newPath = new Path(getCurrentMethodName());
-		
-		// Store sorted tables
-		String pathSort1 = newPath.toString() + "1";
-		pigJob = pigServer.store("sort1", pathSort1, TableStorer.class.getCanonicalName() +
-			"('" + TABLE1_STORAGE + "')");
-		Assert.assertNull(pigJob.getException());
-		
-		String pathSort2 = newPath.toString() + "2";
-		pigJob = pigServer.store("foreachsort2", pathSort2, TableStorer.class.getCanonicalName() +
-			"('" + TABLE2_STORAGE + "')");
-		Assert.assertNull(pigJob.getException());
-		
-		try {
-			String queryLoad = "records1 = LOAD '"
-				+ pathSort1 + ","
-				+ pathSort2
-				+	"' USING org.apache.hadoop.zebra.pig.TableLoader('a,source_table,b,c,d,e,f,m1', 'sorted');";
-			
-			pigServer.registerQuery(queryLoad);
-		} catch (IOException e) {
-			exception = e;
-		} finally {
-			//System.out.println(getStackTrace(exception));
-			Assert.assertNotNull(exception);
-			Assert.assertTrue(getStackTrace(exception).contains("The table is not sorted"));
-		}
-	}
-	
-	/**
-	 * Get stack trace as string that includes nested exceptions
-	 * 
-	 */
-	private static String getStackTrace(Throwable throwable) {
-		Writer writer = new StringWriter();
-		PrintWriter printWriter = new PrintWriter(writer);
-		if (throwable != null)
-			throwable.printStackTrace(printWriter);
-		return writer.toString();
-	}
-	
-	/**
-	 * Print Pig Table (for debugging)
-	 * 
-	 */
-	private int printTable(String tablename) throws IOException {
-		Iterator<Tuple> it1 = pigServer.openIterator(tablename);
-		int numbRows = 0;
-		while (it1.hasNext()) {
-			Tuple RowValue1 = it1.next();
-			++numbRows;
-			System.out.println();
-			for (int i = 0; i < RowValue1.size(); ++i)
-				System.out.println("DEBUG: " + tablename + " RowValue.get(" + i + ") = " + RowValue1.get(i));
-		}
-		System.out.println("\nRow count : " + numbRows);
-		return numbRows;
-	}
-	
-	/**
-	 * Return the name of the routine that called getCurrentMethodName
-	 * 
-	 */
-	private String getCurrentMethodName() {
-		ByteArrayOutputStream baos = new ByteArrayOutputStream();
-		PrintWriter pw = new PrintWriter(baos);
-		(new Throwable()).printStackTrace(pw);
-		pw.flush();
-		String stackTrace = baos.toString();
-		pw.close();
-		
-		StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-		tok.nextToken(); // 'java.lang.Throwable'
-		tok.nextToken(); // 'at ...getCurrentMethodName'
-		String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-		// Parse line 3
-		tok = new StringTokenizer(l.trim(), " <(");
-		String t = tok.nextToken(); // 'at'
-		t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-		return t;
-	}
-	
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestOrderPreserveSimple.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestOrderPreserveSimple.java
deleted file mode 100644
index e05d9ebe3..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestOrderPreserveSimple.java
+++ /dev/null
@@ -1,519 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.ArrayList;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.pig.TableStorer;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.executionengine.ExecJob;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-
-public class TestOrderPreserveSimple extends BaseTestCase {
-	
-	final static String STR_SCHEMA1 = "a:int,b:float,c:long,d:double,e:string,f:bytes,m1:map(string)";
-	final static String STR_STORAGE1 = "[a, b, c]; [d, e, f]; [m1#{a}]";
-	
-	static int fileId = 0;
-	
-	protected static ExecJob pigJob;
-	
-	private static Path pathTable1;
-	private static Path pathTable2;
-	
-	private static Object[][] table1;
-	private static Object[][] table2;
-	
-	private static Map<String, String> m1;
-	private static Map<String, String> m2;
-
-  @BeforeClass
-  public static void setUp() throws Exception {
-    init();
-    
-    pathTable1 = getTableFullPath("TestOrderPreserveSimple1");
-    pathTable2 = getTableFullPath("TestOrderPreserveSimple2");    
-    removeDir(pathTable1);
-    removeDir(pathTable2);
-    
-		// Create table1 data
-		m1 = new HashMap<String, String>();
-		m1.put("a","m1-a");
-		m1.put("b","m1-b");
-		
-		table1 = new Object[][]{
-			{5,		-3.25f,	1001L,	51e+2,	"Zebra",	new DataByteArray("Zebra"),		m1},
-			{-1,	3.25f,	1000L,	50e+2,	"zebra",	new DataByteArray("zebra"),		m1},
-			{1001,	100.0f,	1003L,	50e+2,	"Apple",	new DataByteArray("Apple"),		m1},
-			{1001,	101.0f,	1001L,	50e+2,	"apple",	new DataByteArray("apple"),		m1},
-			{1001,	101.0f,	1001L,	50e+2,	"apple",	new DataByteArray("apple"),		m1},
-			{1001,	50.0f,	1000L,	50e+2,	"Pig",		new DataByteArray("Pig"),		m1},
-			{1001,	52.0f,	1001L,	50e+2,	"pig",		new DataByteArray("pig"),		m1},
-			{1002,	28.0f,	1000L,	50e+2,	"Hadoop",	new DataByteArray("Hadoop"),	m1},
-			{1000,	0.0f,	1002L,	52e+2,	"hadoop",	new DataByteArray("hadoop"),	m1} };
-		
-		// Create table1
-		createTable(pathTable1, STR_SCHEMA1, STR_STORAGE1, table1);
-		
-		// Create table2 data
-		m2 = new HashMap<String, String>();
-		m2.put("a","m2-a");
-		m2.put("b","m2-b");
-		
-		table2 = new Object[][] {
-			{15,	56.0f,	1004L,	50e+2,	"green",	new DataByteArray("green"),		m2},
-			{-1,	-99.0f,	1002L,	51e+2,	"orange",	new DataByteArray("orange"),	m2},
-			{1001,	100.0f,	1003L,	55e+2,	"white",	new DataByteArray("white"),		m2},
-			{1001,	102.0f,	1001L,	52e+2,	"purple",	new DataByteArray("purple"),	m2},
-			{1001,	50.0f,	1008L,	52e+2,	"gray",		new DataByteArray("gray"),		m2},
-			{1001,	53.0f,	1001L,	52e+2,	"brown",	new DataByteArray("brown"),		m2},
-			{2000,	33.0f,	1006L,	52e+2,	"beige",	new DataByteArray("beige"),		m2} };
-		
-		// Create table2
-		createTable(pathTable2, STR_SCHEMA1, STR_STORAGE1, table2);
-	}
-	
-	private static void createTable(Path path, String schemaString, String storageString, Object[][] tableData)
-			throws IOException {
-		//
-		// Create table from tableData array
-		//
-		BasicTable.Writer writer = new BasicTable.Writer(path, schemaString, storageString, conf);
-		
-		Schema schema = writer.getSchema();
-		Tuple tuple = TypesUtils.createTuple(schema);
-		TableInserter inserter = writer.getInserter("ins", false);
-		
-		for (int i = 0; i < tableData.length; ++i) {
-			TypesUtils.resetTuple(tuple);
-			for (int k = 0; k < tableData[i].length; ++k) {
-				tuple.set(k, tableData[i][k]);
-				System.out.println("DEBUG: setting tuple k=" + k + "value= " + tableData[i][k]);
-			}
-			inserter.insert(new BytesWritable(("key" + i).getBytes()), tuple);
-		}
-		inserter.close();
-		writer.close();
-	}
-	
-	@AfterClass
-	public static void tearDown() throws Exception {
-		pigServer.shutdown();
-	}
- 
-	private Iterator<Tuple> orderPreserveUnion(String sortkey, String columns) throws IOException {
-		//
-		// Test sorted union with two tables that are different
-		//
-		
-		// Load tables
-		String query1 = "table1 = LOAD '" + pathTable1.toString() + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		pigServer.registerQuery(query1);
-		
-		String query2 = "table2 = LOAD '" + pathTable2.toString() + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		pigServer.registerQuery(query2);
-		
-		// Sort tables
-		String orderby1 = "sort1 = ORDER table1 BY " + sortkey + " ;";
-		pigServer.registerQuery(orderby1);
-		
-		String orderby2 = "sort2 = ORDER table2 BY " + sortkey + " ;";
-		pigServer.registerQuery(orderby2);
-		
-		Path newPath = new Path(getCurrentMethodName());
-		
-		// Store sorted tables
-		String pathSort1 = newPath.toString() + ++fileId;  // increment fileId suffix
-		pigJob = pigServer.store("sort1", pathSort1, TableStorer.class.getCanonicalName() +
-			"('" + STR_STORAGE1 + "')");
-		Assert.assertNull(pigJob.getException());
-		
-		String pathSort2 = newPath.toString() + ++fileId;  // increment fileId suffix
-		pigJob = pigServer.store("sort2", pathSort2, TableStorer.class.getCanonicalName() +
-			"('" + STR_STORAGE1 + "')");
-		Assert.assertNull(pigJob.getException());
-		
-		String queryLoad = "records1 = LOAD '"
-	        + pathSort1 + ","
-	        + pathSort2
-	        +	"' USING org.apache.hadoop.zebra.pig.TableLoader('" + columns + "', 'sorted');";
-		
-		System.out.println("queryLoad: " + queryLoad);
-		
-		pigServer.registerQuery(queryLoad);
-		
-		// Return iterator
-		Iterator<Tuple> it1 = pigServer.openIterator("records1");
-		return it1;
-	}
-	
-	@Test
-	public void test_union_int() throws ExecException, IOException {
-		//
-		// Test sorted union with two tables with: int
-		//
-		orderPreserveUnion("a", "a,b,c,d,e,f,m1, source_table");
-		
-		// Verify union table
-		HashMap<Integer, ArrayList<ArrayList<Object>>> resultTable
-	  		= new HashMap<Integer, ArrayList<ArrayList<Object>>>();
-		
-		ArrayList<ArrayList<Object>> rows = new ArrayList<ArrayList<Object>>();
-    addResultRow(rows, -1,   3.25f,  1000L,  50e+2,  "zebra",  new DataByteArray("zebra"), m1);
-    addResultRow(rows,  5,    -3.25f, 1001L,  51e+2,  "Zebra",  new DataByteArray("Zebra"), m1);  
-    addResultRow(rows, 1000, 0.0f, 1002L,  52e+2,  "hadoop", new DataByteArray("hadoop"),m1);
-    addResultRow(rows, 1001, 100.0f, 1003L,  50e+2,  "Apple",  new DataByteArray("Apple"), m1);
-    addResultRow(rows, 1001, 101.0f, 1001L,  50e+2,  "apple",  new DataByteArray("apple"), m1);
-    addResultRow(rows, 1001, 101.0f, 1001L,  50e+2,  "apple",  new DataByteArray("apple"), m1);
-    addResultRow(rows, 1001, 50.0f,  1000L,  50e+2,  "Pig",    new DataByteArray("Pig"), m1);
-    addResultRow(rows, 1001, 52.0f,  1001L,  50e+2,  "pig",    new DataByteArray("pig"), m1);
-    addResultRow(rows, 1002, 28.0f,  1000L,  50e+2,  "Hadoop", new DataByteArray("Hadoop"),m1);
-		resultTable.put(0, rows);
-    
-		rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, -1,   -99.0f, 1002L,  51e+2,  "orange", new DataByteArray("orange"),m2);
-    addResultRow(rows, 15,   56.0f,  1004L,  50e+2,  "green",  new DataByteArray("green"), m2);
-    addResultRow(rows, 1001, 100.0f, 1003L,  55e+2,  "white",  new DataByteArray("white"), m2);
-    addResultRow(rows, 1001, 102.0f, 1001L,  52e+2,  "purple", new DataByteArray("purple"),m2);
-    addResultRow(rows, 1001, 50.0f,  1008L,  52e+2,  "gray",   new DataByteArray("gray"),  m2);
-    addResultRow(rows, 1001, 53.0f,  1001L,  52e+2,  "brown",  new DataByteArray("brown"), m2);
-    addResultRow(rows, 2000, 33.0f,  1006L,  52e+2,  "beige",  new DataByteArray("beige"), m2);
-    resultTable.put(1, rows);
-    
-		// Verify union table
-		Iterator<Tuple> it = pigServer.openIterator("records1");
-		int numbRows = verifyTable(resultTable, 0, 7, it);
-		
-		Assert.assertEquals(numbRows, table1.length + table2.length);
-	}
-	
-	@Test
-	public void test_union_int_source() throws ExecException, IOException {
-		//
-		// Test sorted union with two tables with: int
-		//
-		orderPreserveUnion("a", "source_table,a,b,c,d,e,f,m1");
-		
-		// Verify union table
-		HashMap<Integer, ArrayList<ArrayList<Object>>> resultTable
-		  = new HashMap<Integer, ArrayList<ArrayList<Object>>>();
-		
-		ArrayList<ArrayList<Object>> rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, 0,  -1,   3.25f,  1000L,  50e+2,  "zebra",  new DataByteArray("zebra"), m1);
-		addResultRow(rows, 0,	5,		-3.25f,	1001L,	51e+2,	"Zebra",	new DataByteArray("Zebra"),	m1);	
-		addResultRow(rows, 0,	1000,	0.0f,	1002L,	52e+2,	"hadoop",	new DataByteArray("hadoop"),m1);
-		addResultRow(rows, 0,	1001,	100.0f,	1003L,	50e+2,	"Apple",	new DataByteArray("Apple"),	m1);
-		addResultRow(rows, 0,	1001,	101.0f,	1001L,	50e+2,	"apple",	new DataByteArray("apple"),	m1);
-		addResultRow(rows, 0,	1001,	101.0f,	1001L,	50e+2,	"apple",	new DataByteArray("apple"),	m1);
-		addResultRow(rows, 0,	1001,	50.0f,	1000L,	50e+2,	"Pig",		new DataByteArray("Pig"),	m1);
-		addResultRow(rows, 0,	1001,	52.0f,	1001L,	50e+2,	"pig",		new DataByteArray("pig"),	m1);
-		addResultRow(rows, 0,	1002,	28.0f,	1000L,	50e+2,	"Hadoop",	new DataByteArray("Hadoop"),m1);
-		resultTable.put(0, rows);
-		
-		rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, 1,	-1,		-99.0f,	1002L,	51e+2,	"orange",	new DataByteArray("orange"),m2);
-		addResultRow(rows, 1,  15,   56.0f,  1004L,  50e+2,  "green",  new DataByteArray("green"), m2);
-		addResultRow(rows, 1,  1001, 100.0f, 1003L,  55e+2,  "white",  new DataByteArray("white"), m2);
-		addResultRow(rows, 1,  1001, 102.0f, 1001L,  52e+2,  "purple", new DataByteArray("purple"),m2);
-		addResultRow(rows, 1,  1001, 50.0f,  1008L,  52e+2,  "gray",   new DataByteArray("gray"),  m2);
-		addResultRow(rows, 1,  1001, 53.0f,  1001L,  52e+2,  "brown",  new DataByteArray("brown"), m2);
-		addResultRow(rows, 1,  2000, 33.0f,  1006L,  52e+2,  "beige",  new DataByteArray("beige"), m2);
-		resultTable.put(1, rows);
-		
-		
-		// Verify union table
-		Iterator<Tuple> it = pigServer.openIterator("records1");
-		int numbRows = verifyTable(resultTable, 1, 0, it);
-		
-		Assert.assertEquals(numbRows, table1.length + table2.length);
-	}
-	
-	@Test
-	public void test_union_float_source() throws ExecException, IOException {
-		//
-		// Test sorted union with two tables with: float
-		//
-		orderPreserveUnion("b", "source_table,b,a,c,d,e,f,m1");
-		
-		// Verify union table
-		HashMap<Integer, ArrayList<ArrayList<Object>>> resultTable =
-      new HashMap<Integer, ArrayList<ArrayList<Object>>>();
-		
-		ArrayList<ArrayList<Object>> rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, 0,	-3.25f,	5,		1001L,	51e+2,	"Zebra",	new DataByteArray("Zebra"),	m1);
-		addResultRow(rows, 0,	0.0f,	1000,	1002L,	52e+2,	"hadoop",	new DataByteArray("hadoop"),m1);
-		addResultRow(rows, 0,	3.25f,	-1,		1000L,	50e+2,	"zebra",	new DataByteArray("zebra"),	m1);	
-		addResultRow(rows, 0,	28.0f,	1002,	1000L,	50e+2,	"Hadoop",	new DataByteArray("Hadoop"),m1);
-		addResultRow(rows, 0,	50.0f,	1001,	1000L,	50e+2,	"Pig",		new DataByteArray("Pig"),	m1);	
-		addResultRow(rows, 0,	52.0f,	1001,	1001L,	50e+2,	"pig",		new DataByteArray("pig"),	m1);
-		addResultRow(rows, 0,	100.0f,	1001,	1003L,	50e+2,	"Apple",	new DataByteArray("Apple"),	m1);
-		addResultRow(rows, 0,	101.0f,	1001,	1001L,	50e+2,	"apple",	new DataByteArray("apple"),	m1);
-		addResultRow(rows, 0,	101.0f,	1001,	1001L,	50e+2,	"apple",	new DataByteArray("apple"),	m1);
-		resultTable.put(0, rows);
-		
-		rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, 1,	-99.0f,	-1,		1002L,	51e+2,	"orange",	new DataByteArray("orange"),m2);
-		addResultRow(rows, 1,  33.0f,  2000, 1006L,  52e+2,  "beige",  new DataByteArray("beige"), m2);
-		addResultRow(rows, 1,  50.0f,  1001, 1008L,  52e+2,  "gray",   new DataByteArray("gray"),  m2);
-		addResultRow(rows, 1,  53.0f,  1001, 1001L,  52e+2,  "brown",  new DataByteArray("brown"), m2);
-    addResultRow(rows, 1,  56.0f,  15,   1004L,  50e+2,  "green",  new DataByteArray("green"), m2);
-    addResultRow(rows, 1,  100.0f, 1001, 1003L,  55e+2,  "white",  new DataByteArray("white"), m2);
-    addResultRow(rows, 1,  102.0f, 1001, 1001L,  52e+2,  "purple", new DataByteArray("purple"),m2);
-		resultTable.put(1, rows);
-    
-		// Verify union table
-		Iterator<Tuple> it = pigServer.openIterator("records1");
-		int numbRows = verifyTable(resultTable, 1, 0, it);
-		
-		Assert.assertEquals(numbRows, table1.length + table2.length);
-	}
-	
-	@Test
-	public void test_union_long_source() throws ExecException, IOException {
-		//
-		// Test sorted union with two tables with: long
-		//
-		orderPreserveUnion("c", "source_table,c,a,b,d,e,f,m1");
-		
-		// Verify union table
-		HashMap<Integer, ArrayList<ArrayList<Object>>> resultTable = new HashMap<Integer, ArrayList<ArrayList<Object>>>();
-		
-		ArrayList<ArrayList<Object>> rows = new ArrayList<ArrayList<Object>>();
-    addResultRow(rows, 0,  1000L,  -1,   3.25f,  50e+2,  "zebra",  new DataByteArray("zebra"), m1);
-    addResultRow(rows, 0,  1000L,  1001, 50.0f,  50e+2,  "Pig",    new DataByteArray("Pig"), m1);
-    addResultRow(rows, 0,  1000L,  1002, 28.0f,  50e+2,  "Hadoop", new DataByteArray("Hadoop"),m1);
-		addResultRow(rows, 0,	1001L,	5,		-3.25f,	51e+2,	"Zebra",	new DataByteArray("Zebra"),	m1);
-		addResultRow(rows, 0,	1001L,	1001,	101.0f,	50e+2,	"apple",	new DataByteArray("apple"),	m1);
-		addResultRow(rows, 0,	1001L,	1001,	101.0f,	50e+2,	"apple",	new DataByteArray("apple"),	m1);
-		addResultRow(rows, 0,	1001L,	1001,	52.0f,	50e+2,	"pig",		new DataByteArray("pig"),	m1);
-		addResultRow(rows, 0,	1002L,	1000,	0.0f,	52e+2,	"hadoop",	new DataByteArray("hadoop"),m1);
-		addResultRow(rows, 0,	1003L,	1001,	100.0f,	50e+2,	"Apple",	new DataByteArray("Apple"),	m1);
-		resultTable.put(0, rows);
-		
-		rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, 1,	1001L,	1001,	102.0f,	52e+2,	"purple",	new DataByteArray("purple"),m2);
-		addResultRow(rows, 1,  1001L,  1001, 53.0f,  52e+2,  "brown",  new DataByteArray("brown"), m2);
-		addResultRow(rows, 1,  1002L,  -1,   -99.0f, 51e+2,  "orange", new DataByteArray("orange"),m2);
-		addResultRow(rows, 1,  1003L,  1001, 100.0f, 55e+2,  "white",  new DataByteArray("white"), m2);
-		addResultRow(rows, 1,  1004L,  15,   56.0f,  50e+2,  "green",  new DataByteArray("green"), m2);
-    addResultRow(rows, 1,  1006L,  2000, 33.0f,  52e+2,  "beige",  new DataByteArray("beige"), m2);
-    addResultRow(rows, 1,  1008L,  1001, 50.0f,  52e+2,  "gray",   new DataByteArray("gray"),  m2);
-		resultTable.put(1, rows);
-		
-		// Verify union table
-		Iterator<Tuple> it = pigServer.openIterator("records1");
-		int numbRows = verifyTable(resultTable, 1, 0, it);
-		
-		Assert.assertEquals(numbRows, table1.length + table2.length);
-	}
-	
-	@Test
-	public void test_union_double_source() throws ExecException, IOException {
-		//
-		// Test sorted union with two tables with: double
-		//
-		orderPreserveUnion("d", "source_table,d,a,b,c,e,f,m1");
-		
-		// Verify union table
-		HashMap<Integer, ArrayList<ArrayList<Object>>> resultTable = new HashMap<Integer, ArrayList<ArrayList<Object>>>();
-		
-		ArrayList<ArrayList<Object>> rows = new ArrayList<ArrayList<Object>>();
-    addResultRow(rows, 0,50e+2,  -1,   3.25f,  1000L,  "zebra",  new DataByteArray("zebra"), m1);
-		addResultRow(rows, 0,50e+2,	1001,	100.0f,	1003L,	"Apple",	new DataByteArray("Apple"),	m1);
-		addResultRow(rows, 0,50e+2,	1001,	101.0f,	1001L,	"apple",	new DataByteArray("apple"),	m1);
-		addResultRow(rows, 0,50e+2,	1001,	101.0f,	1001L,	"apple",	new DataByteArray("apple"),	m1);
-		addResultRow(rows, 0,50e+2,	1001,	50.0f,	1000L,	"Pig",		new DataByteArray("Pig"),	m1);
-		addResultRow(rows, 0,50e+2,	1001,	52.0f,	1001L,	"pig",		new DataByteArray("pig"),	m1);
-		addResultRow(rows, 0,50e+2,	1002,	28.0f,	1000L,	"Hadoop",	new DataByteArray("Hadoop"),m1);
-		addResultRow(rows, 0,51e+2,	5,		-3.25f,	1001L,	"Zebra",	new DataByteArray("Zebra"),	m1);
-		addResultRow(rows, 0,52e+2,	1000,	0.0f,	1002L,	"hadoop",	new DataByteArray("hadoop"),m1);
-		resultTable.put(0, rows);
-		
-		rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, 1,50e+2,	15,		56.0f,	1004L,	"green",	new DataByteArray("green"),	m2);
-		addResultRow(rows, 1,51e+2,  -1,   -99.0f, 1002L,  "orange", new DataByteArray("orange"),m2);
-		addResultRow(rows, 1,52e+2,  1001, 102.0f, 1001L,  "purple", new DataByteArray("purple"),m2);
-		addResultRow(rows, 1,52e+2,  1001, 50.0f,  1008L,  "gray",   new DataByteArray("gray"),  m2);
-    addResultRow(rows, 1,52e+2,  1001, 53.0f,  1001L,  "brown",  new DataByteArray("brown"), m2);
-    addResultRow(rows, 1,52e+2,  2000, 33.0f,  1006L,  "beige",  new DataByteArray("beige"), m2);
-    addResultRow(rows, 1,55e+2,  1001, 100.0f, 1003L,  "white",  new DataByteArray("white"), m2);
-		resultTable.put(1, rows);
-		
-		// Verify union table
-		Iterator<Tuple> it = pigServer.openIterator("records1");
-		int numbRows = verifyTable(resultTable, 1, 0, it);
-		
-		Assert.assertEquals(numbRows, table1.length + table2.length);
-	}
-	
-	@Test
-	public void test_union_string_source() throws ExecException, IOException {
-		//
-		// Test sorted union with two tables with: string
-		//
-		orderPreserveUnion("e", "source_table,e,a,b,c,d,f,m1");
-		
-		// Verify union table
-		HashMap<Integer, ArrayList<ArrayList<Object>>> resultTable = new HashMap<Integer, ArrayList<ArrayList<Object>>>();
-		
-		ArrayList<ArrayList<Object>> rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, 0,"Apple",	1001,	100.0f,	1003L,	50e+2,	new DataByteArray("Apple"),	m1);
-		addResultRow(rows, 0,"Hadoop",	1002,	28.0f,	1000L,	50e+2,	new DataByteArray("Hadoop"),m1);
-		addResultRow(rows, 0,"Pig",		1001,	50.0f,	1000L,	50e+2,	new DataByteArray("Pig"),	m1);
-		addResultRow(rows, 0,"Zebra",	5,		-3.25f,	1001L,	51e+2,	new DataByteArray("Zebra"),	m1);
-		addResultRow(rows, 0,"apple",	1001,	101.0f,	1001L,	50e+2,	new DataByteArray("apple"),	m1);
-		addResultRow(rows, 0,"apple",	1001,	101.0f,	1001L,	50e+2,	new DataByteArray("apple"),	m1);
-		addResultRow(rows, 0,"hadoop",	1000,	0.0f,	1002L,	52e+2,	new DataByteArray("hadoop"),m1);
-		addResultRow(rows, 0,"pig",		1001,	52.0f,	1001L,	50e+2,	new DataByteArray("pig"),	m1);
-		addResultRow(rows, 0,"zebra",	-1,		3.25f,	1000L,	50e+2,	new DataByteArray("zebra"),	m1);
-		resultTable.put(0, rows);
-		
-		rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, 1,"beige",  2000, 33.0f,  1006L,  52e+2,  new DataByteArray("beige"), m2);
-    addResultRow(rows, 1,"brown",  1001, 53.0f,  1001L,  52e+2,  new DataByteArray("brown"), m2);
-    addResultRow(rows, 1,"gray",   1001, 50.0f,  1008L,  52e+2,  new DataByteArray("gray"),  m2);
-    addResultRow(rows, 1,"green",  15,   56.0f,  1004L,  50e+2,  new DataByteArray("green"), m2);
-    addResultRow(rows, 1,"orange", -1,   -99.0f, 1002L,  51e+2,  new DataByteArray("orange"),m2);
-    addResultRow(rows, 1,"purple", 1001, 102.0f, 1001L,  52e+2,  new DataByteArray("purple"),m2);
-    addResultRow(rows, 1,"white",  1001, 100.0f, 1003L,  55e+2,  new DataByteArray("white"), m2);
-		resultTable.put(1, rows);
-    
-		// Verify union table
-		Iterator<Tuple> it = pigServer.openIterator("records1");
-		int numbRows = verifyTable(resultTable, 1, 0, it);
-		
-		Assert.assertEquals(numbRows, table1.length + table2.length);
-	}
-	
-	@Test
-	public void test_union_bytes_source() throws ExecException, IOException {
-		//
-		// Test sorted union with two tables with: bytes
-		//
-		orderPreserveUnion("f", "source_table,f,a,b,c,d,e,m1");
-		
-		// Verify union table
-		HashMap<Integer, ArrayList<ArrayList<Object>>> resultTable = new HashMap<Integer, ArrayList<ArrayList<Object>>>();
-		
-		ArrayList<ArrayList<Object>> rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, 0,new DataByteArray("Apple"),	1001,	100.0f,	1003L,	50e+2,	"Apple",	m1);
-		addResultRow(rows, 0,new DataByteArray("Hadoop"),1002,	28.0f,	1000L,	50e+2,	"Hadoop",	m1);
-		addResultRow(rows, 0,new DataByteArray("Pig"),	1001,	50.0f,	1000L,	50e+2,	"Pig",		m1);
-		addResultRow(rows, 0,new DataByteArray("Zebra"),	5,		-3.25f,	1001L,	51e+2,	"Zebra",	m1);	
-		addResultRow(rows, 0,new DataByteArray("apple"),	1001,	101.0f,	1001L,	50e+2,	"apple",	m1);
-		addResultRow(rows, 0,new DataByteArray("apple"),	1001,	101.0f,	1001L,	50e+2,	"apple",	m1);
-		addResultRow(rows, 0,new DataByteArray("hadoop"),1000,	0.0f,	1002L,	52e+2,	"hadoop",	m1);
-		addResultRow(rows, 0,new DataByteArray("pig"),	1001,	52.0f,	1001L,	50e+2,	"pig",		m1);
-		addResultRow(rows, 0,new DataByteArray("zebra"),	-1,		3.25f,	1000L,	50e+2,	"zebra",	m1);
-		resultTable.put(0, rows);
-		
-		rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, 1,new DataByteArray("beige"), 2000, 33.0f,  1006L,  52e+2,  "beige",  m2);
-    addResultRow(rows, 1,new DataByteArray("brown"), 1001, 53.0f,  1001L,  52e+2,  "brown",  m2);
-    
-    addResultRow(rows, 1,new DataByteArray("gray"),  1001, 50.0f,  1008L,  52e+2,  "gray",   m2);
-    addResultRow(rows, 1,new DataByteArray("green"), 15,   56.0f,  1004L,  50e+2,  "green",  m2);
-    addResultRow(rows, 1,new DataByteArray("orange"),-1,   -99.0f, 1002L,  51e+2,  "orange", m2);
-    addResultRow(rows, 1,new DataByteArray("purple"),1001, 102.0f, 1001L,  52e+2,  "purple", m2);
-    addResultRow(rows, 1,new DataByteArray("white"), 1001, 100.0f, 1003L,  55e+2,  "white",  m2);
-		resultTable.put(1, rows);
-    
-		// Verify union table
-		Iterator<Tuple> it = pigServer.openIterator("records1");
-		int numbRows = verifyTable(resultTable, 1, 0, it);
-		
-		Assert.assertEquals(numbRows, table1.length + table2.length);
-	}	
-
-	/**
-	 *Add a row to expected results table
-	 * 
-	 */
-	private void addResultRow(ArrayList<ArrayList<Object>> resultTable, Object ... values) {
-		ArrayList<Object> resultRow = new ArrayList<Object>();
-		
-		for (int i = 0; i < values.length; i++) {
-			resultRow.add(values[i]);
-		}
-		resultTable.add(resultRow);
-	}
-	
-	/**
-	 * Print Pig Table (for debugging)
-	 * 
-	 */
-	private int printTable(String tablename) throws IOException {
-		Iterator<Tuple> it1 = pigServer.openIterator(tablename);
-		int numbRows = 0;
-		while (it1.hasNext()) {
-			Tuple RowValue1 = it1.next();
-			++numbRows;
-			System.out.println();
-			for (int i = 0; i < RowValue1.size(); ++i)
-				System.out.println("DEBUG: " + tablename + " RowValue.get(" + i + ") = " + RowValue1.get(i));
-		}
-		System.out.println("\nRow count : " + numbRows);
-		return numbRows;
-	}
-	
-	/**
-	 * Return the name of the routine that called getCurrentMethodName
-	 * 
-	 */
-	public String getCurrentMethodName() {
-		ByteArrayOutputStream baos = new ByteArrayOutputStream();
-		PrintWriter pw = new PrintWriter(baos);
-		(new Throwable()).printStackTrace(pw);
-		pw.flush();
-		String stackTrace = baos.toString();
-		pw.close();
-		
-		StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-		tok.nextToken(); // 'java.lang.Throwable'
-		tok.nextToken(); // 'at ...getCurrentMethodName'
-		String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-		// Parse line 3
-		tok = new StringTokenizer(l.trim(), " <(");
-		String t = tok.nextToken(); // 'at'
-		t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-		return t;
-	}
-	
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestOrderPreserveUnion.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestOrderPreserveUnion.java
deleted file mode 100644
index eccd38600..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestOrderPreserveUnion.java
+++ /dev/null
@@ -1,214 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.pig.TableStorer;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * Note:
- * 
- * Make sure you add the build/pig-0.1.0-dev-core.jar to the Classpath of the
- * app/debug configuration, when run this from inside the Eclipse.
- * 
- */
-public class TestOrderPreserveUnion extends BaseTestCase {
-  private static Path pathTable;
-
-  @BeforeClass
-  public static void setUp() throws Exception {
-    init();
-    
-    pathTable = getTableFullPath("TestOrderPreserveUnion");    
-    removeDir(pathTable);
-    BasicTable.Writer writer = new BasicTable.Writer(pathTable,
-        "SF_a:string,SF_b:string,SF_c:string,SF_d:string,SF_e:string,SF_f:string,SF_g:string",
-        "[SF_a, SF_b, SF_c]; [SF_e, SF_f, SF_g]", conf);
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    final int numsBatch = 10;
-    final int numsInserters = 1;
-    TableInserter[] inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tuple);
-        for (int k = 0; k < tuple.size(); ++k) {
-          try {
-            tuple.set(k, (9-b) + "_" + i + "" + k);
-          } catch (ExecException e) {
-            e.printStackTrace();
-          }
-        }
-        inserters[i].insert(new BytesWritable(("key" + i).getBytes()), tuple);
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDown() throws Exception {
-    pigServer.shutdown();
-    removeDir(pathTable);
-  }
-
-  /**
-   * Return the name of the routine that called getCurrentMethodName
-   * 
-   */
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    return t;
-  }
-
-  @Test
-  public void testStorer() throws ExecException, IOException {
-    /*
-     * Use pig LOAD to load testing data for store
-     */
-    String query = "records = LOAD '" + pathTable.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    pigServer.registerQuery(query);
-
-    /*
-    Iterator<Tuple> it2 = pigServer.openIterator("records");
-    int row0 = 0;
-    Tuple RowValue2 = null;
-    while (it2.hasNext()) {
-      // Last row value
-      RowValue2 = it2.next();
-      row0++;
-      if (row0 == 10) {
-        Assert.assertEquals("0_01", RowValue2.get(1));
-        Assert.assertEquals("0_00", RowValue2.get(0));
-      }
-    }
-    Assert.assertEquals(10, row0);
-    */
-
-    String orderby = "srecs = ORDER records BY SF_a;";
-    pigServer.registerQuery(orderby);
-
-    /*
-     * Use pig STORE to store testing data BasicTable.Writer writer = new
-     * BasicTable.Writer(pathTable, "SF_a,SF_b,SF_c,SF_d,SF_e,SF_f,SF_g",
-     * "[SF_a, SF_b, SF_c]; [SF_e, SF_f, SF_g]", false, conf);
-     */
-    Path newPath = new Path(getCurrentMethodName());
-
-    /*
-     * Table1 creation
-     */
-    removeDir(getTableFullPath(newPath.toString()+"1"));
-    pigServer
-        .store(
-            "srecs",
-            newPath.toString()+"1",
-            TableStorer.class.getCanonicalName()
-                + "('[SF_a, SF_b, SF_c]; [SF_e]')");
-    removeDir(getTableFullPath(newPath.toString()+"2"));
-    pigServer
-        .store(
-            "srecs",
-            newPath.toString()+"2",
-            TableStorer.class.getCanonicalName()
-                + "('[SF_a, SF_b, SF_c]; [SF_e]')");
-
-
-    String query4 = "records2 = LOAD '"
-        + newPath.toString() + "1,"
-        + newPath.toString() + "2"
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('SF_a, source_table, SF_b', 'sorted');";
-    pigServer.registerQuery(query4);
-
-    HashMap<Integer, ArrayList<ArrayList<Object>>> resultTable
-    = new HashMap<Integer, ArrayList<ArrayList<Object>>>();
-
-    ArrayList<ArrayList<Object>> rows = new ArrayList<ArrayList<Object>>();
-    
-    for (int i = 0; i < 10; i++)
-      addResultRow(rows, i+"_00", 0, i+"_01");
-    resultTable.put(0, rows);
-    
-    rows = new ArrayList<ArrayList<Object>>();
-    for (int i = 0; i < 10; i++)
-      addResultRow(rows, i+"_00", 1,  i+"_01");
-    resultTable.put(1, rows);
-
- // Verify union table
-    Iterator<Tuple> it = pigServer.openIterator("records2");
-    int row = verifyTable(resultTable, 0, 1, it);
-    
-    Assert.assertEquals(20, row);
-  }
-  
-  /**
-   *Add a row to expected results table
-   * 
-   */
-  private void addResultRow(ArrayList<ArrayList<Object>> resultTable, Object ... values) {
-    ArrayList<Object> resultRow = new ArrayList<Object>();
-    
-    for (int i = 0; i < values.length; i++) {
-      resultRow.add(values[i]);
-    }
-    resultTable.add(resultRow);
-  }
-} 
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestOrderPreserveUnionHDFS.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestOrderPreserveUnionHDFS.java
deleted file mode 100644
index 69f51c171..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestOrderPreserveUnionHDFS.java
+++ /dev/null
@@ -1,199 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.ArrayList;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.pig.TableStorer;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.executionengine.ExecJob;
-import org.apache.pig.data.Tuple;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-
-public class TestOrderPreserveUnionHDFS extends BaseTestCase {
-	
-	final static String STR_SCHEMA1 = "a:string,b:string,c:string";
-	final static String STR_STORAGE1 = "[a, b]; [c]";
-	
-	static int fileId = 0;
-	
-	protected static ExecJob pigJob;
-	
-	private static Path pathTable1;
-	private static Path pathTable2;
-	
-	private static Object[][] table1;
-	private static Object[][] table2;	
-	
-	@BeforeClass
-	public static void setUp() throws Exception {
-	  init();
-	  
-    pathTable1 = getTableFullPath("TestOrderPerserveSimple1");
-    pathTable2 = getTableFullPath("TestOrderPerserveSimple2");
-	  removeDir(pathTable1);
-		removeDir(pathTable2);
-				
-		// Create table1 data
-		table1 = new Object[][] {
-			{"a1",	"z",	"5"},
-			{"a2",	"r",	"4"},
-			{"a3",	"e",	"3"},
-			{"a4",	"a",	"1"} };
-		
-		// Create table1
-		createTable(pathTable1, STR_SCHEMA1, STR_STORAGE1, table1);
-		
-		// Create table2 data
-		table2 = new Object[][] {
-			{"b1",	"a",	"a"},
-			{"b2",	"a",	"a"},
-			{"b3",	"a",	"a"},
-			{"b4",	"a",	"a"} };
-		
-		// Create table2
-		createTable(pathTable2, STR_SCHEMA1, STR_STORAGE1, table2);
-		
-		// Load table1
-		String query1 = "table1 = LOAD '" + pathTable1.toString() + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		pigServer.registerQuery(query1);
-		
-		// Load table2
-		String query2 = "table2 = LOAD '" + pathTable2.toString() + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		pigServer.registerQuery(query2);
-	}
-	
-	private static void createTable(Path path, String schemaString, String storageString, Object[][] tableData)
-			throws IOException {
-		//
-		// Create table from tableData array
-		//
-		BasicTable.Writer writer = new BasicTable.Writer(path, schemaString, storageString, conf);
-		
-		Schema schema = writer.getSchema();
-		Tuple tuple = TypesUtils.createTuple(schema);
-		TableInserter inserter = writer.getInserter("ins", false);
-		
-		for (int i = 0; i < tableData.length; ++i) {
-			TypesUtils.resetTuple(tuple);
-			for (int k = 0; k < tableData[i].length; ++k) {
-				tuple.set(k, tableData[i][k]);
-				System.out.println("DEBUG: setting tuple k=" + k + "value= " + tableData[i][k]);
-			}
-			inserter.insert(new BytesWritable(("key" + i).getBytes()), tuple);
-		}
-		inserter.close();
-		writer.close();
-	}
-	
-	@AfterClass
-	public static void tearDown() throws Exception {
-		pigServer.shutdown();
-	}
-	
-	@Test
-	public void test_sorted_table_union_hdfs() throws ExecException, IOException {
-		//
-		// Test sorted union with two tables that are different and that use
-		// hdfs file URLs in Pig LOAD statement
-		//
-		
-		// Sort tables
-		String orderby1 = "sort1 = ORDER table1 BY " + "a" + " ;";
-		pigServer.registerQuery(orderby1);
-		
-		String orderby2 = "sort2 = ORDER table2 BY " + "a" + " ;";
-		pigServer.registerQuery(orderby2);
-		
-		// Store sorted tables
-		++fileId;  // increment filename suffix
-		String pathSort1 = "TestOrderPerserveSimple1" + Integer.toString(fileId);
-	  removeDir(getTableFullPath(pathSort1));
-		pigJob = pigServer.store("sort1", pathSort1, TableStorer.class.getCanonicalName() +
-			"('[a, b]; [c]')");
-		Assert.assertNull(pigJob.getException());
-		
-		String pathSort2 = "TestOrderPerserveSimple2" + Integer.toString(fileId);
-	  removeDir(getTableFullPath(pathSort2));
-		pigJob = pigServer.store("sort2", pathSort2, TableStorer.class.getCanonicalName() +
-			"('[a, b]; [c]')");
-		Assert.assertNull(pigJob.getException());
-		
-		String queryLoad = "records1 = LOAD '"
-	        + pathSort1 + ","
-	        + pathSort2
-	        +	"' USING org.apache.hadoop.zebra.pig.TableLoader('a,b,c, source_table', 'sorted');";
-		
-		System.out.println("queryLoad: " + queryLoad);
-		
-		pigServer.registerQuery(queryLoad);
-		
-		// Verify union table
-		HashMap<Integer, ArrayList<ArrayList<Object>>> resultTable
-    = new HashMap<Integer, ArrayList<ArrayList<Object>>>();
-		ArrayList<ArrayList<Object>> rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, "a1",	"z",	"5", 0);
-		addResultRow(rows, "a2",	"r",	"4", 0);
-		addResultRow(rows, "a3",	"e",	"3", 0);
-		addResultRow(rows, "a4",	"a",	"1", 0);
-		resultTable.put(0, rows);
-		
-		rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, "b1",	"a",	"a", 1);
-		addResultRow(rows, "b2",	"a",	"a", 1);
-		addResultRow(rows, "b3",	"a",	"a", 1);
-		addResultRow(rows, "b4",	"a",	"a", 1);
-		resultTable.put(1, rows);
-		
-		// Verify union table
-		Iterator<Tuple> it = pigServer.openIterator("records1");
-		int numbRows = verifyTable(resultTable, 0, 3, it);
-		
-		Assert.assertEquals(numbRows, table1.length + table2.length);
-	}
-	
-	/**
-	 *Add a row to expected results table
-	 * 
-	 */
-	private void addResultRow(ArrayList<ArrayList<Object>> resultTable, Object ... values) {
-		ArrayList<Object> resultRow = new ArrayList<Object>();
-		
-		for (int i = 0; i < values.length; i++) {
-			resultRow.add(values[i]);
-		}
-		resultTable.add(resultRow);
-	}
-	
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestOrderPreserveVariableTable.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestOrderPreserveVariableTable.java
deleted file mode 100644
index 7c84b3ff2..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestOrderPreserveVariableTable.java
+++ /dev/null
@@ -1,784 +0,0 @@
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.ArrayList;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.pig.TableStorer;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.executionengine.ExecJob;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-
-public class TestOrderPreserveVariableTable extends BaseTestCase {
-	
-	final static String TABLE1_SCHEMA = "a:int,b:float,c:long,d:double,e:string,f:bytes,m1:map(string)";
-	final static String TABLE1_STORAGE = "[a, b, c]; [d, e, f]; [m1#{a}]";
-	
-	final static String TABLE2_SCHEMA = "a:int,b:float,c:long,d:double,e:string,f:bytes,m1:map(string)";
-	final static String TABLE2_STORAGE = "[a, b, c]; [d, e, f]; [m1#{a}]";
-	
-	final static String TABLE3_SCHEMA = "b:float,d:double,c:long,e:string,f:bytes,int1:int";
-	final static String TABLE3_STORAGE = "[b,d]; [c]; [e]; [f,int1]";
-	
-	final static String TABLE4_SCHEMA = "e:string,str3:string,str4:string";
-	final static String TABLE4_STORAGE = "[e,str3,str4]";
-	
-	static int fileId = 0;
-	static int sortId = 0;
-	
-	protected static ExecJob pigJob;
-	
-	private static Path pathTable1;
-	private static Path pathTable2;
-	private static Path pathTable3;
-	private static Path pathTable4;
-	private static HashMap<String, String> tableStorage;
-	
-	private static Object[][] table1;
-	private static Object[][] table2;
-	private static Object[][] table3;
-	private static Object[][] table4;
-	
-	private static Map<String, String> m1;
-	private static Map<String, String> m2;
-
-  @BeforeClass
-  public static void setUp() throws Exception {
-    init();
-    
-    pathTable1 = getTableFullPath("TestOrderPreserveVariableTable1");
-    pathTable2 = getTableFullPath("TestOrderPreserveVariableTable2");
-    pathTable3 = getTableFullPath("TestOrderPreserveVariableTable3");
-    pathTable4 = getTableFullPath("TestOrderPreserveVariableTable4");
-  
-    removeDir(pathTable1);
-    removeDir(pathTable2);
-    removeDir(pathTable3);
-    removeDir(pathTable4);
-    
-		// Create table1 data
-		m1 = new HashMap<String, String>();
-		m1.put("a","m1-a");
-		m1.put("b","m1-b");
-		
-		table1 = new Object[][]{
-			{5,		-3.25f,	1001L,	51e+2,	"Zebra",	new DataByteArray("Zebra"),		m1},
-			{-1,	3.25f,	1000L,	50e+2,	"zebra",	new DataByteArray("zebra"),		m1},
-			{1001,	100.0f,	1003L,	50e+2,	"Apple",	new DataByteArray("Apple"),		m1},
-			{1001,	101.0f,	1001L,	50e+2,	"apple",	new DataByteArray("apple"),		m1},
-			{1001,	50.0f,	1000L,	50e+2,	"Pig",		new DataByteArray("Pig"),		m1},
-			{1001,	52.0f,	1001L,	50e+2,	"pig",		new DataByteArray("pig"),		m1},
-			{1002,	28.0f,	1000L,	50e+2,	"Hadoop",	new DataByteArray("Hadoop"),	m1},
-			{1000,	0.0f,	1002L,	52e+2,	"hadoop",	new DataByteArray("hadoop"),	m1} };
-		
-		// Create table1
-		createTable(pathTable1, TABLE1_SCHEMA, TABLE1_STORAGE, table1);
-		
-		// Create table2 data
-		m2 = new HashMap<String, String>();
-		m2.put("a","m2-a");
-		m2.put("b","m2-b");
-		
-		table2 = new Object[][] {
-			{15,	56.0f,	1004L,	50e+2,	"green",	new DataByteArray("green"),		m2},
-			{-1,	-99.0f,	1002L,	51e+2,	"orange",	new DataByteArray("orange"),	m2},
-			{1001,	100.0f,	1003L,	55e+2,	"white",	new DataByteArray("white"),		m2},
-			{1001,	102.0f,	1001L,	52e+2,	"purple",	new DataByteArray("purple"),	m2},
-			{1001,	50.0f,	1008L,	52e+2,	"gray",		new DataByteArray("gray"),		m2},
-			{1001,	53.0f,	1001L,	52e+2,	"brown",	new DataByteArray("brown"),		m2},
-			{2000,	33.0f,	1006L,	52e+2,	"beige",	new DataByteArray("beige"),		m2} };
-		
-		// Create table2
-		createTable(pathTable2, TABLE2_SCHEMA, TABLE2_STORAGE, table2);
-		
-		// Create table3 data
-		table3 = new Object[][] {
-			{3.25f,		51e+2,	1001L,	"string1",	new DataByteArray("green"),	100}, };
-		
-		// Create table3
-		createTable(pathTable3, TABLE3_SCHEMA, TABLE3_STORAGE, table3);
-		
-		// Create table4 data
-		table4 = new Object[][] {
-			{"a 8",	"Cupertino",	"California"},
-			{"a 7",	"San Jose",		"California"},
-			{"a 6",	"Santa Cruz",	"California"},
-			{"a 5",	"Las Vegas",	"Nevada"},
-			{"a 4",	"New York",		"New York"},
-			{"a 3",	"Phoenix",		"Arizona"},
-			{"a 2",	"Dallas",		"Texas"},
-			{"a 1",	"Reno",			"Nevada"} };
-		
-		// Create table4
-		createTable(pathTable4, TABLE4_SCHEMA, TABLE4_STORAGE, table4);
-		
-		// Load tables
-		String query1 = "table1 = LOAD '" + pathTable1.toString() + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		pigServer.registerQuery(query1);
-		String query2 = "table2 = LOAD '" + pathTable2.toString() + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		pigServer.registerQuery(query2);
-		String query3 = "table3 = LOAD '" + pathTable3.toString() + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		pigServer.registerQuery(query3);
-		String query4 = "table4 = LOAD '" + pathTable4.toString() + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-		pigServer.registerQuery(query4);
-		
-		// Map for table storage
-		tableStorage = new HashMap<String, String>();
-		tableStorage.put("table1", TABLE1_STORAGE);
-		tableStorage.put("table2", TABLE2_STORAGE);
-		tableStorage.put("table3", TABLE3_STORAGE);
-		tableStorage.put("table4", TABLE4_STORAGE);
-	}
-	
-	private static void createTable(Path path, String schemaString, String storageString, Object[][] tableData)
-			throws IOException {
-		//
-		// Create table from tableData array
-		//
-		BasicTable.Writer writer = new BasicTable.Writer(path, schemaString, storageString, conf);
-		
-		Schema schema = writer.getSchema();
-		Tuple tuple = TypesUtils.createTuple(schema);
-		TableInserter inserter = writer.getInserter("ins", false);
-		
-		for (int i = 0; i < tableData.length; ++i) {
-			TypesUtils.resetTuple(tuple);
-			for (int k = 0; k < tableData[i].length; ++k) {
-				tuple.set(k, tableData[i][k]);
-				System.out.println("DEBUG: setting tuple k=" + k + "value= " + tableData[i][k]);
-			}
-			inserter.insert(new BytesWritable(("key" + i).getBytes()), tuple);
-		}
-		inserter.close();
-		writer.close();
-	}
-	
-	@AfterClass
-	public static void tearDown() throws Exception {
-		pigServer.shutdown();
-	}
-	
-	@Test
-	public void test_union_empty_left_table() throws ExecException, IOException {
-		//
-		// Test sorted union with two tables and left table is empty
-		//
-		
-		// Sort tables
-		String orderby1 = "sort1 = ORDER table1 BY " + "a,b" + " ;";
-		pigServer.registerQuery(orderby1);
-		
-		String orderby2 = "sort2 = ORDER table2 BY " + "a,b" + " ;";
-		pigServer.registerQuery(orderby2);
-		
-		// Filter to remove all rows in table
-		String filter = "filtersort1 = FILTER sort1 BY a == -99 ;";
-		
-		pigServer.registerQuery(filter);
-		
-		Path newPath = new Path(getCurrentMethodName());
-		
-		// Store sorted tables
-		String pathSort1 = newPath.toString() + "1";
-		pigJob = pigServer.store("filtersort1", pathSort1, TableStorer.class.getCanonicalName() +
-			"('" + TABLE1_STORAGE + "')");
-		Assert.assertNull(pigJob.getException());
-		
-		String pathSort2 = newPath.toString() + "2";
-		pigJob = pigServer.store("sort2", pathSort2, TableStorer.class.getCanonicalName() +
-			"('" + TABLE2_STORAGE + "')");
-		Assert.assertNull(pigJob.getException());
-		
-		String queryLoad = "records1 = LOAD '"
-	        + pathSort1 + ","
-	        + pathSort2
-	        +	"' USING org.apache.hadoop.zebra.pig.TableLoader('a,source_table,b,c,d,e,f,m1', 'sorted');";
-		
-		System.out.println("queryLoad: " + queryLoad);
-		
-		pigServer.registerQuery(queryLoad);
-		
-		// Verify union table
-		HashMap<Integer, ArrayList<ArrayList<Object>>> resultTable = new HashMap<Integer, ArrayList<ArrayList<Object>>>();
-		
-		ArrayList<ArrayList<Object>> rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, -1,	1,	-99.0f,	1002L,	51e+2,	"orange",	new DataByteArray("orange"),m2);
-		addResultRow(rows, 15,	1,	56.0f,	1004L,	50e+2,	"green",	new DataByteArray("green"),	m2);
-		addResultRow(rows, 1001,	1,	100.0f,	1003L,	55e+2,	"white",	new DataByteArray("white"),	m2);
-		addResultRow(rows, 1001,	1,	102.0f,	1001L,	52e+2,	"purple",	new DataByteArray("purple"),m2);
-		
-		addResultRow(rows, 1001,	1,	50.0f,	1008L,	52e+2,	"gray",		new DataByteArray("gray"),	m2);
-		addResultRow(rows, 1001,	1,	53.0f,	1001L,	52e+2,	"brown",	new DataByteArray("brown"),	m2);
-		addResultRow(rows, 2000,	1,	33.0f,	1006L,	52e+2,	"beige",	new DataByteArray("beige"),	m2);
-		resultTable.put(1, rows);
-		
-		// Verify union table
-		Iterator<Tuple> it = pigServer.openIterator("records1");
-		int numbRows = verifyTable(resultTable, 0, 1, it);
-		
-		Assert.assertEquals(numbRows, table2.length);
-	}
-	
-	@Test
-	public void test_union_empty_right_table() throws ExecException, IOException {
-		//
-		// Test sorted union with two tables and right table is empty
-		//
-		
-		// Sort tables
-		String orderby1 = "sort1 = ORDER table1 BY " + "e" + " ;";
-		pigServer.registerQuery(orderby1);
-		
-		String orderby4 = "sort4 = ORDER table4 BY " + "e" + " ;";
-		pigServer.registerQuery(orderby4);
-		
-		// Filter to remove all rows in table
-		String filter = "filtersort4 = FILTER sort4 BY e == 'teststring' ;";
-		
-		pigServer.registerQuery(filter);
-		
-		Path newPath = new Path(getCurrentMethodName());
-		
-		// Store sorted tables
-		String pathSort1 = newPath.toString() + "1";
-		pigJob = pigServer.store("sort1", pathSort1, TableStorer.class.getCanonicalName() +
-			"('" + TABLE1_STORAGE + "')");
-		Assert.assertNull(pigJob.getException());
-		
-		String pathSort4 = newPath.toString() + "4";
-		pigJob = pigServer.store("filtersort4", pathSort4, TableStorer.class.getCanonicalName() +
-			"('" + TABLE4_STORAGE + "')");
-		Assert.assertNull(pigJob.getException());
-		
-		String queryLoad = "records1 = LOAD '"
-	        + pathSort1 + ","
-	        + pathSort4
-	        +	"' USING org.apache.hadoop.zebra.pig.TableLoader('e,source_table,a,b,c,d,f,m1,str3,str4', 'sorted');";
-		
-		System.out.println("queryLoad: " + queryLoad);
-		
-		pigServer.registerQuery(queryLoad);
-		
-		// Verify union table
-		HashMap<Integer, ArrayList<ArrayList<Object>>> resultTable = new HashMap<Integer, ArrayList<ArrayList<Object>>>();
-		
-		ArrayList<ArrayList<Object>> rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, "Apple",	0,	1001,	100.0f,	1003L,	50e+2,	new DataByteArray("Apple"),	m1,	null,	null);
-		addResultRow(rows, "Hadoop",	0,	1002,	28.0f,	1000L,	50e+2,	new DataByteArray("Hadoop"),m1,	null,	null);
-		addResultRow(rows, "Pig",	0,	1001,	50.0f,	1000L,	50e+2,	new DataByteArray("Pig"),	m1,	null,	null);
-		addResultRow(rows, "Zebra",	0,	5,		-3.25f,	1001L,	51e+2,	new DataByteArray("Zebra"),	m1,	null,	null);
-		
-		addResultRow(rows, "apple",	0,	1001,	101.0f,	1001L,	50e+2,	new DataByteArray("apple"),	m1,	null,	null);
-		addResultRow(rows, "hadoop",	0,	1000,	0.0f,	1002L,	52e+2,	new DataByteArray("hadoop"),m1,	null,	null);
-		addResultRow(rows, "pig",	0,	1001,	52.0f,	1001L,	50e+2,	new DataByteArray("pig"),	m1,	null,	null);
-		addResultRow(rows, "zebra",	0,	-1,		3.25f,	1000L,	50e+2,	new DataByteArray("zebra"),	m1,	null,	null);
-		resultTable.put(0, rows);
-		
-		// Verify union table
-		Iterator<Tuple> it = pigServer.openIterator("records1");
-		int numbRows = verifyTable(resultTable, 0, 1, it);
-		
-		Assert.assertEquals(numbRows, table1.length);
-	}
-	
-	@Test
-	public void test_union_empty_middle_table() throws ExecException, IOException {
-		//
-		// Test sorted union with three tables and middle table is empty
-		//
-		
-		// Sort tables
-		String orderby1 = "sort1 = ORDER table1 BY " + "e" + " ;";
-		pigServer.registerQuery(orderby1);
-		
-		String orderby2 = "sort2 = ORDER table2 BY " + "e" + " ;";
-		pigServer.registerQuery(orderby2);
-		
-		String orderby4 = "sort4 = ORDER table4 BY " + "e" + " ;";
-		pigServer.registerQuery(orderby4);
-		
-		// Filter to remove all rows in table
-		String filter = "filtersort2 = FILTER sort2 BY e == 'teststring' ;";
-		
-		pigServer.registerQuery(filter);
-		
-		Path newPath = new Path(getCurrentMethodName());
-		
-		// Store sorted tables
-		String pathSort1 = newPath.toString() + "1";
-		pigJob = pigServer.store("sort1", pathSort1, TableStorer.class.getCanonicalName() +
-			"('" + TABLE1_STORAGE + "')");
-		Assert.assertNull(pigJob.getException());
-		
-		String pathSort2 = newPath.toString() + "2";
-		pigJob = pigServer.store("filtersort2", pathSort2, TableStorer.class.getCanonicalName() +
-			"('" + TABLE2_STORAGE + "')");
-		Assert.assertNull(pigJob.getException());
-		
-		String pathSort4 = newPath.toString() + "4";
-		pigJob = pigServer.store("sort4", pathSort4, TableStorer.class.getCanonicalName() +
-			"('" + TABLE4_STORAGE + "')");
-		Assert.assertNull(pigJob.getException());
-		
-		String queryLoad = "records1 = LOAD '"
-	        + pathSort1 + ","
-	        + pathSort2 + ","
-	        + pathSort4
-	        +	"' USING org.apache.hadoop.zebra.pig.TableLoader('e,source_table,a,b,c,d,f,m1,str3,str4', 'sorted');";
-		
-		System.out.println("queryLoad: " + queryLoad);
-		
-		pigServer.registerQuery(queryLoad);
-		
-		// Verify union table
-		HashMap<Integer, ArrayList<ArrayList<Object>>> resultTable = new HashMap<Integer, ArrayList<ArrayList<Object>>>();
-		
-		ArrayList<ArrayList<Object>> rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, "Apple",  0,  1001, 100.0f, 1003L,  50e+2,  new DataByteArray("Apple"), m1, null, null);
-    addResultRow(rows, "Hadoop", 0,  1002, 28.0f,  1000L,  50e+2,  new DataByteArray("Hadoop"),m1, null, null);
-    addResultRow(rows, "Pig",  0,  1001, 50.0f,  1000L,  50e+2,  new DataByteArray("Pig"), m1, null, null);
-    addResultRow(rows, "Zebra",  0,  5,    -3.25f, 1001L,  51e+2,  new DataByteArray("Zebra"), m1, null, null);
-    
-    addResultRow(rows, "apple",  0,  1001, 101.0f, 1001L,  50e+2,  new DataByteArray("apple"), m1, null, null);
-    addResultRow(rows, "hadoop", 0,  1000, 0.0f, 1002L,  52e+2,  new DataByteArray("hadoop"),m1, null, null);
-    addResultRow(rows, "pig",  0,  1001, 52.0f,  1001L,  50e+2,  new DataByteArray("pig"), m1, null, null);
-    addResultRow(rows, "zebra",  0,  -1,   3.25f,  1000L,  50e+2,  new DataByteArray("zebra"), m1, null, null);
-		resultTable.put(0, rows);
-		
-		rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, "a 1",	2,	null,	null,	null,	null,	null,	null,	"Reno",		"Nevada");
-		addResultRow(rows, "a 2",	2,	null,	null,	null,	null,	null,	null,	"Dallas",	"Texas");
-		addResultRow(rows, "a 3",	2,	null,	null,	null,	null,	null,	null,	"Phoenix",	"Arizona");
-		addResultRow(rows, "a 4",	2,	null,	null,	null,	null,	null,	null,	"New York",	"New York");
-		
-		addResultRow(rows, "a 5",	2,	null,	null,	null,	null,	null,	null,	"Las Vegas","Nevada");
-		addResultRow(rows, "a 6",	2,	null,	null,	null,	null,	null,	null,	"Santa Cruz","California");
-		addResultRow(rows, "a 7",	2,	null,	null,	null,	null,	null,	null,	"San Jose",	"California");
-		addResultRow(rows, "a 8",	2,	null,	null,	null,	null,	null,	null,	"Cupertino","California");
-		resultTable.put(2, rows);
-		
-		// Verify union table
-		Iterator<Tuple> it = pigServer.openIterator("records1");
-		int numbRows = verifyTable(resultTable, 0, 1, it);
-		
-		Assert.assertEquals(numbRows, table1.length + table4.length);
-	}
-	
-	@Test
-	public void test_union_empty_many_table() throws ExecException, IOException {
-		//
-		// Test sorted union with three tables with left and right table empty
-		//
-		
-		// Sort tables
-		String orderby1 = "sort1 = ORDER table1 BY " + "e" + " ;";
-		pigServer.registerQuery(orderby1);
-		
-		String orderby2 = "sort2 = ORDER table2 BY " + "e" + " ;";
-		pigServer.registerQuery(orderby2);
-		
-		String orderby4 = "sort4 = ORDER table4 BY " + "e" + " ;";
-		pigServer.registerQuery(orderby4);
-		
-		// Filter to remove all rows in table
-		String filter1 = "filtersort1 = FILTER sort1 BY e == 'teststring' ;";
-		pigServer.registerQuery(filter1);
-		
-		// Filter to remove all rows in table
-		String filter4 = "filtersort4 = FILTER sort4 BY e == 'teststring' ;";
-		pigServer.registerQuery(filter4);
-		
-		Path newPath = new Path(getCurrentMethodName());
-		
-		// Store sorted tables
-		String pathSort1 = newPath.toString() + "1";
-		pigJob = pigServer.store("filtersort1", pathSort1, TableStorer.class.getCanonicalName() +
-			"('" + TABLE1_STORAGE + "')");
-		Assert.assertNull(pigJob.getException());
-		
-		String pathSort2 = newPath.toString() + "2";
-		pigJob = pigServer.store("sort2", pathSort2, TableStorer.class.getCanonicalName() +
-			"('" + TABLE2_STORAGE + "')");
-		Assert.assertNull(pigJob.getException());
-		
-		String pathSort4 = newPath.toString() + "4";
-		pigJob = pigServer.store("filtersort4", pathSort4, TableStorer.class.getCanonicalName() +
-			"('" + TABLE4_STORAGE + "')");
-		Assert.assertNull(pigJob.getException());
-		
-		String queryLoad = "records1 = LOAD '"
-	        + pathSort1 + ","
-	        + pathSort2 + ","
-	        + pathSort4
-	        +	"' USING org.apache.hadoop.zebra.pig.TableLoader('e,source_table,a,b,c,d,f,m1,str3,str4', 'sorted');";
-		
-		System.out.println("queryLoad: " + queryLoad);
-		
-		pigServer.registerQuery(queryLoad);
-		
-		// Verify union table
-		HashMap<Integer, ArrayList<ArrayList<Object>>> resultTable = new HashMap<Integer, ArrayList<ArrayList<Object>>>();
-		
-		ArrayList<ArrayList<Object>> rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, "beige",	1,	2000,	33.0f,	1006L,	52e+2,	new DataByteArray("beige"),	m2, null, null);
-		addResultRow(rows, "brown",	1,	1001,	53.0f,	1001L,	52e+2,	new DataByteArray("brown"),	m2, null, null);
-		addResultRow(rows, "gray",	1,	1001,	50.0f,	1008L,	52e+2,	new DataByteArray("gray"),	m2, null, null);
-		addResultRow(rows, "green",	1,	15,		56.0f,	1004L,	50e+2,	new DataByteArray("green"),	m2, null, null);
-		
-		addResultRow(rows, "orange",	1,	-1,		-99.0f,	1002L,	51e+2,	new DataByteArray("orange"),m2, null, null);
-		addResultRow(rows, "purple",	1,	1001,	102.0f,	1001L,	52e+2,	new DataByteArray("purple"),m2, null, null);
-		addResultRow(rows, "white",	1,	1001,	100.0f,	1003L,	55e+2,	new DataByteArray("white"),	m2, null, null);
-		resultTable.put(1, rows);
-		
-		// Verify union table
-		Iterator<Tuple> it = pigServer.openIterator("records1");
-		int numbRows = verifyTable(resultTable, 0, 1, it);
-		
-		Assert.assertEquals(numbRows, table2.length);
-	}
-	
-	@Test
-	public void test_union_empty_all_table() throws ExecException, IOException {
-		//
-		// Test sorted union with two tables and both are empty
-		//
-		
-		// Sort tables
-		String orderby1 = "sort1 = ORDER table1 BY " + "e" + " ;";
-		pigServer.registerQuery(orderby1);
-		
-		String orderby4 = "sort4 = ORDER table4 BY " + "e" + " ;";
-		pigServer.registerQuery(orderby4);
-		
-		// Filter to remove all rows in table
-		String filter1 = "filtersort1 = FILTER sort1 BY e == 'teststring' ;";
-		pigServer.registerQuery(filter1);
-		
-		// Filter to remove all rows in table
-		String filter4 = "filtersort4 = FILTER sort4 BY e == 'teststring' ;";
-		pigServer.registerQuery(filter4);
-		
-		Path newPath = new Path(getCurrentMethodName());
-		
-		// Store sorted tables
-		String pathSort1 = newPath.toString() + "1";
-		pigJob = pigServer.store("filtersort1", pathSort1, TableStorer.class.getCanonicalName() +
-			"('" + TABLE1_STORAGE + "')");
-		Assert.assertNull(pigJob.getException());
-		
-		String pathSort4 = newPath.toString() + "4";
-		pigJob = pigServer.store("filtersort4", pathSort4, TableStorer.class.getCanonicalName() +
-			"('" + TABLE4_STORAGE + "')");
-		Assert.assertNull(pigJob.getException());
-		
-		String queryLoad = "records1 = LOAD '"
-	        + pathSort1 + ","
-	        + pathSort4
-	        +	"' USING org.apache.hadoop.zebra.pig.TableLoader('e,source_table,a,b,c,d,f,m1,str3,str4', 'sorted');";
-		
-		System.out.println("queryLoad: " + queryLoad);
-		
-		pigServer.registerQuery(queryLoad);
-		
-		int numbRows = printTable("records1");
-		
-		Assert.assertEquals(numbRows, 0);
-	}
-	
-	@Test
-	public void test_pig_statements() throws ExecException, IOException {
-		//
-		// Test sorted union with two tables after pig filter and pig foreach statements
-		//
-		
-		// Sort tables
-		String orderby1 = "sort1 = ORDER table1 BY " + "e" + " ;";
-		pigServer.registerQuery(orderby1);
-		
-		String orderby4 = "sort4 = ORDER table4 BY " + "e" + " ;";
-		pigServer.registerQuery(orderby4);
-		
-		// Table after pig filter
-		String filter1 = "filtersort1 = FILTER sort1 BY a == 1001 ;";
-		pigServer.registerQuery(filter1);
-		
-		Path newPath = new Path(getCurrentMethodName());
-		
-		// Store sorted tables
-		String pathSort1 = newPath.toString() + "1";
-		pigJob = pigServer.store("filtersort1", pathSort1, TableStorer.class.getCanonicalName() +
-			"('" + TABLE1_STORAGE + "')");
-		Assert.assertNull(pigJob.getException());
-		
-		String pathSort4 = newPath.toString() + "4";
-		pigJob = pigServer.store("sort4", pathSort4, TableStorer.class.getCanonicalName() +
-			"('" + TABLE4_STORAGE + "')");
-		Assert.assertNull(pigJob.getException());
-		
-		String queryLoad = "records1 = LOAD '"
-	        + pathSort1 + ","
-	        + pathSort4
-	        +	"' USING org.apache.hadoop.zebra.pig.TableLoader('e,source_table,a,b,c,d,f,m1,str3,str4', 'sorted');";
-		
-		System.out.println("queryLoad: " + queryLoad);
-		
-		pigServer.registerQuery(queryLoad);
-		
-		// Verify union table
-		HashMap<Integer, ArrayList<ArrayList<Object>>> resultTable = new HashMap<Integer, ArrayList<ArrayList<Object>>>();
-		
-		ArrayList<ArrayList<Object>> rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, "Apple",  0,  1001, 100.0f, 1003L,  50e+2,  new DataByteArray("Apple"), m1, null, null);
-    addResultRow(rows, "Pig",  0,  1001, 50.0f,  1000L,  50e+2,  new DataByteArray("Pig"), m1, null, null);
-		addResultRow(rows, "apple",	0,	1001,	101.0f,	1001L,	50e+2,	new DataByteArray("apple"),	m1,	null,	null);
-		addResultRow(rows, "pig",	0,	1001,	52.0f,	1001L,	50e+2,	new DataByteArray("pig"),	m1,	null,	null);
-		resultTable.put(0, rows);
-		
-		rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, "a 1",	1,	null,	null,	null,	null,	null,	null,	"Reno",		"Nevada");
-		addResultRow(rows, "a 2",	1,	null,	null,	null,	null,	null,	null,	"Dallas",	"Texas");
-		
-		addResultRow(rows, "a 3",	1,	null,	null,	null,	null,	null,	null,	"Phoenix",	"Arizona");
-		addResultRow(rows, "a 4",	1,	null,	null,	null,	null,	null,	null,	"New York",	"New York");
-		addResultRow(rows, "a 5",	1,	null,	null,	null,	null,	null,	null,	"Las Vegas","Nevada");
-		addResultRow(rows, "a 6",	1,	null,	null,	null,	null,	null,	null,	"Santa Cruz","California");
-		
-		addResultRow(rows, "a 7",	1,	null,	null,	null,	null,	null,	null,	"San Jose",	"California");
-		addResultRow(rows, "a 8",	1,	null,	null,	null,	null,	null,	null,	"Cupertino","California");
-		resultTable.put(1, rows);
-		
-		// Verify union table
-		Iterator<Tuple> it = pigServer.openIterator("records1");
-		int numbRows = verifyTable(resultTable, 0, 1, it);
-		
-		Assert.assertEquals(12, numbRows);
-	}
-	
-	@Test
-	public void test_union_as_input() throws ExecException, IOException {
-		//
-		// Test sorted union with two tables, one of the tables is from previous sorted union
-		//
-		
-		// Sort tables
-		String orderby1 = "sort1 = ORDER table1 BY " + "e" + " ;";
-		pigServer.registerQuery(orderby1);
-		
-		String orderby2 = "sort2 = ORDER table2 BY " + "e" + " ;";
-		pigServer.registerQuery(orderby2);
-		
-		String orderby4 = "sort4 = ORDER table4 BY " + "e" + " ;";
-		pigServer.registerQuery(orderby4);
-		
-		Path newPath = new Path(getCurrentMethodName());
-		
-		// Store sorted tables
-		String pathSort1 = newPath.toString() + ++fileId;
-		pigJob = pigServer.store("sort1", pathSort1, TableStorer.class.getCanonicalName() +
-			"('" + TABLE1_STORAGE + "')");
-		Assert.assertNull(pigJob.getException());
-		
-		String pathSort2 = newPath.toString() + ++fileId;
-		pigJob = pigServer.store("sort2", pathSort2, TableStorer.class.getCanonicalName() +
-			"('" + TABLE2_STORAGE + "')");
-		Assert.assertNull(pigJob.getException());
-		
-		String pathSort4 = newPath.toString() + ++fileId;
-		pigJob = pigServer.store("sort4", pathSort4, TableStorer.class.getCanonicalName() +
-			"('" + TABLE4_STORAGE + "')");
-		Assert.assertNull(pigJob.getException());
-		
-		// Create first sorted union
-		String queryLoad1 = "records1 = LOAD '"
-	        + pathSort1 + ","
-	        + pathSort2
-	        +	"' USING org.apache.hadoop.zebra.pig.TableLoader('e,a,b,c,d,f,m1', 'sorted');";
-		pigServer.registerQuery(queryLoad1);
-		
-		// Add sort info to union table before store
-		String sortunion = "sortrecords1 = ORDER records1 BY " + "e" + " ;";
-		pigServer.registerQuery(sortunion);
-		
-		// Store union table
-		String pathUnion1 = newPath.toString() + ++fileId;
-		pigJob = pigServer.store("sortrecords1", pathUnion1, TableStorer.class.getCanonicalName() +
-			"('" + TABLE1_STORAGE + "')");
-		Assert.assertNull(pigJob.getException());
-		
-		// Create second sorted union
-		String queryLoad2 = "records2 = LOAD '"
-	        + pathUnion1 + ","
-	        + pathSort4
-	        +	"' USING org.apache.hadoop.zebra.pig.TableLoader('e,source_table,a,b,str3,str4', 'sorted');";
-		
-		System.out.println("queryLoad: " + queryLoad2);
-		pigServer.registerQuery(queryLoad2);
-		
-		// Verify union table
-		HashMap<Integer, ArrayList<ArrayList<Object>>> resultTable = new HashMap<Integer, ArrayList<ArrayList<Object>>>();
-		
-		ArrayList<ArrayList<Object>> rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, "Apple",  0,  1001, 100.0f, null, null);
-    addResultRow(rows, "Hadoop", 0,  1002, 28.0f,  null, null);
-    addResultRow(rows, "Pig",  0,  1001, 50.0f,  null, null);
-    addResultRow(rows, "Zebra",  0,  5,    -3.25f, null, null);
-    
-    addResultRow(rows, "apple",  0,  1001, 101.0f, null, null);
-    addResultRow(rows, "beige",  0,  2000, 33.0f,  null, null);
-    addResultRow(rows, "brown",  0,  1001, 53.0f,  null, null);
-    addResultRow(rows, "gray", 0,  1001, 50.0f,  null, null);
-    
-    addResultRow(rows, "green",  0,  15,   56.0f,  null, null);
-    addResultRow(rows, "hadoop", 0,  1000, 0.0f, null, null);
-    addResultRow(rows, "orange", 0,  -1,   -99.0f, null, null);
-    addResultRow(rows, "pig",  0,  1001, 52.0f,  null, null);
-    
-    addResultRow(rows, "purple", 0,  1001, 102.0f, null, null);
-    addResultRow(rows, "white",  0,  1001, 100.0f, null, null);
-    addResultRow(rows, "zebra",  0,  -1,   3.25f,  null, null);
-		resultTable.put(0, rows);
-		
-		rows = new ArrayList<ArrayList<Object>>();
-		addResultRow(rows, "a 1",	1,	null,	null,	"Reno",		"Nevada");
-		addResultRow(rows, "a 2",	1,	null,	null,	"Dallas",	"Texas");
-		addResultRow(rows, "a 3",	1,	null,	null,	"Phoenix",	"Arizona");
-		addResultRow(rows, "a 4",	1,	null,	null,	"New York",	"New York");
-		
-		addResultRow(rows, "a 5",	1,	null,	null,	"Las Vegas","Nevada");
-		addResultRow(rows, "a 6",	1,	null,	null,	"Santa Cruz","California");
-		addResultRow(rows, "a 7",	1,	null,	null,	"San Jose","California");
-		addResultRow(rows, "a 8",	1,	null,	null,	"Cupertino","California");
-		resultTable.put(1, rows);
-		
-		// Verify union table
-		Iterator<Tuple> it = pigServer.openIterator("records2");
-		int numbRows = verifyTable(resultTable, 0, 1, it);
-		
-		Assert.assertEquals(23, numbRows);
-	}
-	
-	
-	/**
-	 * Compare table rows
-	 * 
-	 */
-	private boolean compareRow(Tuple rowValues, ArrayList<Object> resultRow) throws IOException {
-		boolean result = true;
-		Assert.assertEquals(resultRow.size(), rowValues.size());
-		for (int i = 0; i < rowValues.size(); ++i) {
-			if (! compareObj(rowValues.get(i), resultRow.get(i)) ) {
-				result = false;
-				break;
-			}
-		}
-		return result;
-	}
-	
-	/**
-	 * Compare table values
-	 * 
-	 */
-	private boolean compareObj(Object object1, Object object2) {
-		if (object1 == null) {
-			if (object2 == null)
-				return true;
-			else
-				return false;
-		} else if (object1.equals(object2))
-			return true;
-		else
-			return false;
-	}
-	
-	/**
-	 *Add a row to expected results table
-	 * 
-	 */
-	private void addResultRow(ArrayList<ArrayList<Object>> resultTable, Object ... values) {
-		ArrayList<Object> resultRow = new ArrayList<Object>();
-		
-		for (int i = 0; i < values.length; i++) {
-			resultRow.add(values[i]);
-		}
-		resultTable.add(resultRow);
-	}
-	
-	/**
-	 * Print Pig Table (for debugging)
-	 * 
-	 */
-	private int printTable(String tablename) throws IOException {
-		Iterator<Tuple> it1 = pigServer.openIterator(tablename);
-		int numbRows = 0;
-		while (it1.hasNext()) {
-			Tuple RowValue1 = it1.next();
-			++numbRows;
-			System.out.println();
-			for (int i = 0; i < RowValue1.size(); ++i)
-				System.out.println("DEBUG: " + tablename + " RowValue.get(" + i + ") = " + RowValue1.get(i));
-		}
-		System.out.println("\nRow count : " + numbRows);
-		return numbRows;
-	}
-	
-	/**
-	 * Return the name of the routine that called getCurrentMethodName
-	 * 
-	 */
-	private String getCurrentMethodName() {
-		ByteArrayOutputStream baos = new ByteArrayOutputStream();
-		PrintWriter pw = new PrintWriter(baos);
-		(new Throwable()).printStackTrace(pw);
-		pw.flush();
-		String stackTrace = baos.toString();
-		pw.close();
-		
-		StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-		tok.nextToken(); // 'java.lang.Throwable'
-		tok.nextToken(); // 'at ...getCurrentMethodName'
-		String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-		// Parse line 3
-		tok = new StringTokenizer(l.trim(), " <(");
-		String t = tok.nextToken(); // 'at'
-		t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-		return t;
-	}
-	
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestSimpleType.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestSimpleType.java
deleted file mode 100644
index df18ab529..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestSimpleType.java
+++ /dev/null
@@ -1,529 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.pig;
-
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.Iterator;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.pig.TableStorer;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.backend.executionengine.ExecJob;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * 
- * Test projections on complicated column types.
- * 
- */
-public class TestSimpleType extends BaseTestCase {
-
-  final static String STR_SCHEMA = "s1:bool, s2:int, s3:long, s4:double, s5:string, s6:bytes";
-  final static String STR_STORAGE = "[s1, s2]; [s3, s4]; [s5, s6]";
-  private static Path path;
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException, Exception {
-    init();
-
-
-    path = getTableFullPath("TesMapType");
-    removeDir(path);
-
-    BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-        STR_STORAGE, conf);
-    Schema schema = writer.getSchema();
-
-    BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-
-    Tuple tuple = TypesUtils.createTuple(schema);
-    TypesUtils.resetTuple(tuple);
-
-    // insert data in row 1
-    int row = 0;
-    tuple.set(0, true); // bool
-    tuple.set(1, 1); // int
-    tuple.set(2, 1001L); // long
-    tuple.set(3, 1.1); // float
-    tuple.set(4, "hello world 1"); // string
-    tuple.set(5, new DataByteArray("hello byte 1")); // byte
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    row++;
-    TypesUtils.resetTuple(tuple);
-
-    tuple.set(0, false);
-    tuple.set(1, 2); // int
-    tuple.set(2, 1002L); // long
-    tuple.set(3, 3.1); // float
-    tuple.set(4, "hello world 2"); // string
-    tuple.set(5, new DataByteArray("hello byte 2")); // byte
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    inserter.close();
-    writer1.finish();
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-    BasicTable.drop(path, conf);
-  }
-
-  /**
-   * Return the name of the routine that called getCurrentMethodName
-   * 
-   */
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    return t;
-  }
-  
-  // @Test
-  public void testReadSimpleStitch() throws IOException, ParseException {
-    String query = "records = LOAD '" + path.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('s5,s1');";
-    System.out.println(query);
-    pigServer.registerQuery(query);
-    Iterator<Tuple> it = pigServer.openIterator("records");
-    BytesWritable key = new BytesWritable();
-    int row = 0;
-    while (it.hasNext()) {
-      Tuple RowValue = it.next();
-      System.out.println(RowValue);
-      row++;
-      if (row == 1) {
-        Assert.assertEquals("hello world 1", RowValue.get(0));
-        Assert.assertEquals(true, RowValue.get(1));
-      }
-      if (row == 2) {
-        Assert.assertEquals("hello world 2", RowValue.get(0));
-        Assert.assertEquals(false, RowValue.get(1));
-      }
-    }
-  }
-
-  // @Test
-  // Test reader
-  public void testReadSimple1() throws IOException, ParseException {
-    String query = "records = LOAD '"
-        + path.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('s6,s5,s4,s3,s2,s1');";
-    System.out.println(query);
-    pigServer.registerQuery(query);
-    Iterator<Tuple> it = pigServer.openIterator("records");
-    BytesWritable key = new BytesWritable();
-    int row = 0;
-    while (it.hasNext()) {
-      Tuple RowValue = it.next();
-      System.out.println(RowValue);
-      row++;
-      if (row == 1) {
-        // Assert.assertEquals(key, new
-        // BytesWritable("k11".getBytes()));
-        Assert.assertEquals(true, RowValue.get(5));
-        Assert.assertEquals(1, RowValue.get(4));
-        Assert.assertEquals(1001L, RowValue.get(3));
-        Assert.assertEquals(1.1, RowValue.get(2));
-        Assert.assertEquals("hello world 1", RowValue.get(1));
-        Assert.assertEquals("hello byte 1", RowValue.get(0).toString());
-      }
-      if (row == 2) {
-        Assert.assertEquals(false, RowValue.get(5));
-        Assert.assertEquals(2, RowValue.get(4));
-        Assert.assertEquals(1002L, RowValue.get(3));
-        Assert.assertEquals(3.1, RowValue.get(2));
-        Assert.assertEquals("hello world 2", RowValue.get(1));
-        Assert.assertEquals("hello byte 2", RowValue.get(0).toString());
-      }
-    }
-  }
-
-  // @Test
-  // Test reader, negative. not exist field in the projection
-  public void testRead2() throws IOException, ParseException {
-    try {
-      String query = "records = LOAD '" + path.toString()
-          + "' USING org.apache.hadoop.zebra.pig.TableLoader('s7');";
-      Assert.fail("Project should not take non-existent fields");
-    } catch (Exception e) {
-      System.out.println(e);
-    }
-
-  }
-
-  @Test
-  // Store same table
-  public void testStorer() throws ExecException, IOException {
-    //
-    // Use pig LOAD to load testing data for store
-    //
-    String query = "records = LOAD '"
-        + path.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader() as (s1,s2,s3,s4,s5,s6);";
-    pigServer.registerQuery(query);
-
-    Iterator<Tuple> it = pigServer.openIterator("records");
-    while (it.hasNext()) {
-      Tuple RowValue = it.next();
-      System.out.println(RowValue);
-    }
-
-    //
-    // Use pig STORE to store testing data
-    //
-    Path newPath = new Path(getCurrentMethodName());
-    pigServer
-        .store(
-            "records",
-            new Path(newPath, "store").toString(),
-            TableStorer.class.getCanonicalName()
-                + "('[s1, s2]; [s3, s4]')");
-
-  }
-
-  @Test
-  // store different records, second row of the previous table
-  public void testStorer2() throws ExecException, IOException {
-    // Load original table
-    String query = "records = LOAD '"
-        + path.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader() as (s1,s2,s3,s4,s5,s6);";
-    System.out.println(query);
-    pigServer.registerQuery(query);
-
-    // filter the original table
-    String query2 = "newRecord = FILTER records BY (s2 >= 2);";
-    pigServer.registerQuery(query2);
-
-    // store the new records to new table
-    Path newPath = new Path(getCurrentMethodName());
-    pigServer
-        .store(
-            "newRecord",
-            newPath.toString(),
-            TableStorer.class.getCanonicalName()
-                + "('[s1, s2]; [s3, s4]')");
-
-    // check new table content
-    String query3 = "newRecords = LOAD '"
-        + newPath.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('s6,s5,s4,s3,s2,s1');";
-    System.out.println(query3);
-    // newRecords = LOAD
-    // 'org.apache.hadoop.zebra.pig.TestSimpleType.testStorer2' USING
-    // org.apache.hadoop.zebra.pig.TableLoader() as (s1,s2,s3,s4,s5,s6);
-    pigServer.registerQuery(query3);
-
-    Iterator<Tuple> it3 = pigServer.openIterator("newRecords");
-    // BytesWritable key2 = new BytesWritable();
-    int row = 0;
-    Tuple RowValue2 = null;
-    while (it3.hasNext()) {
-      // Last row value
-      RowValue2 = it3.next();
-      row++;
-      if (row == 1) {
-        Assert.assertEquals(false, RowValue2.get(5));
-        Assert.assertEquals(2, RowValue2.get(4));
-        Assert.assertEquals(1002L, RowValue2.get(3));
-        Assert.assertEquals(3.1, RowValue2.get(2));
-        Assert.assertEquals("hello world 2", RowValue2.get(1));
-        Assert.assertEquals("hello byte 2", RowValue2.get(0).toString());
-      }
-    }
-    Assert.assertEquals(1, row);
-  }
-
-  @Test
-  // store different records, with storage hint is empty
-  public void testStorer3() throws ExecException, IOException {
-    // Load original table
-    String query = "records = LOAD '"
-        + path.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader() as (s1,s2,s3,s4,s5,s6);";
-    System.out.println(query);
-    pigServer.registerQuery(query);
-
-    // filter the original table
-    String query2 = "newRecord = FILTER records BY (s2 >= 2);";
-    pigServer.registerQuery(query2);
-
-    // store the new records to new table
-    Path newPath = new Path(getCurrentMethodName());
-    pigServer.store("newRecord", newPath.toString(), TableStorer.class
-        .getCanonicalName()
-        + "('')");
-
-    // check new table content
-    String query3 = "newRecords = LOAD '"
-        + newPath.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('s6,s5,s4,s3,s2,s1');";
-    System.out.println(query3);
-    // newRecords = LOAD
-    // 'org.apache.hadoop.zebra.pig.TestSimpleType.testStorer2' USING
-    // org.apache.hadoop.zebra.pig.TableLoader() as (s1,s2,s3,s4,s5,s6);
-    pigServer.registerQuery(query3);
-
-    Iterator<Tuple> it3 = pigServer.openIterator("newRecords");
-    // BytesWritable key2 = new BytesWritable();
-    int row = 0;
-    Tuple RowValue2 = null;
-    while (it3.hasNext()) {
-      // Last row value
-      RowValue2 = it3.next();
-      row++;
-      if (row == 1) {
-        Assert.assertEquals(false, RowValue2.get(5));
-        Assert.assertEquals(2, RowValue2.get(4));
-        Assert.assertEquals(1002L, RowValue2.get(3));
-        Assert.assertEquals(3.1, RowValue2.get(2));
-        Assert.assertEquals("hello world 2", RowValue2.get(1));
-        Assert.assertEquals("hello byte 2", RowValue2.get(0).toString());
-      }
-    }
-    Assert.assertEquals(1, row);
-  }
-
-  @Test
-  // store different records, with column group is empty
-  public void testStorer4() throws ExecException, IOException {
-    // Load original table
-    String query = "records = LOAD '"
-        + path.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader() as (s1,s2,s3,s4,s5,s6);";
-    System.out.println(query);
-    pigServer.registerQuery(query);
-
-    // filter the original table
-    String query2 = "newRecord = FILTER records BY (s2 >= 2);";
-    pigServer.registerQuery(query2);
-
-    // store the new records to new table
-    Path newPath = new Path(getCurrentMethodName());
-    pigServer.store("newRecord", newPath.toString(), TableStorer.class
-        .getCanonicalName()
-        + "('[]')");
-
-    // check new table content
-    String query3 = "newRecords = LOAD '"
-        + newPath.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('s6,s5,s4,s3,s2,s1');";
-    System.out.println(query3);
-    // newRecords = LOAD
-    // 'org.apache.hadoop.zebra.pig.TestSimpleType.testStorer2' USING
-    // org.apache.hadoop.zebra.pig.TableLoader() as (s1,s2,s3,s4,s5,s6);
-    pigServer.registerQuery(query3);
-
-    Iterator<Tuple> it3 = pigServer.openIterator("newRecords");
-    // BytesWritable key2 = new BytesWritable();
-    int row = 0;
-    Tuple RowValue2 = null;
-    while (it3.hasNext()) {
-      // Last row value
-      RowValue2 = it3.next();
-      row++;
-      if (row == 1) {
-        Assert.assertEquals(false, RowValue2.get(5));
-        Assert.assertEquals(2, RowValue2.get(4));
-        Assert.assertEquals(1002L, RowValue2.get(3));
-        Assert.assertEquals(3.1, RowValue2.get(2));
-        Assert.assertEquals("hello world 2", RowValue2.get(1));
-        Assert.assertEquals("hello byte 2", RowValue2.get(0).toString());
-      }
-    }
-    Assert.assertEquals(1, row);
-  }
-
-  @Test
-  // negative, schema description is different from input tuple, less column
-  // numbers
-  public void testStorerNegative1() throws ExecException, IOException {
-
-    String query = "records = LOAD '" + path.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    pigServer.registerQuery(query);
-
-    Iterator<Tuple> it = pigServer.openIterator("records");
-    while (it.hasNext()) {
-      Tuple RowValue = it.next();
-      System.out.println(RowValue);
-    }
-
-    try {
-        Path newPath = new Path(getCurrentMethodName());
-        ExecJob pigJob = pigServer
-            .store(
-                "records",
-                new Path(newPath, "store").toString(),
-                TableStorer.class.getCanonicalName()
-                    + "('[s7, s2]; [s3, s4]')");
-    } catch (Exception e) {
-        System.out.println(e);
-        return;
-    }
-    Assert.fail("Exception expected");
-  }
-
-  @Test
-  // negative, storage hint duplicate the columns
-  public void testStorerNegative2() throws ExecException, IOException {
-
-    String query = "records = LOAD '" + path.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    pigServer.registerQuery(query);
-
-    Iterator<Tuple> it = pigServer.openIterator("records");
-    while (it.hasNext()) {
-      Tuple RowValue = it.next();
-      System.out.println(RowValue);
-    }
-
-    try {
-        Path newPath = new Path(getCurrentMethodName());
-        
-        ExecJob pigJob = pigServer
-              .store(
-                  "records",
-                  new Path(newPath, "store").toString(),
-                  TableStorer.class.getCanonicalName()
-                      + "('[s1, s2]; [s1, s4]')");
-    } catch(Exception e) {
-        System.out.println(e);
-        return;
-    }
-    Assert.fail("Exception expected");
-  }
-
-  @Test
-  // negative, storage hint duplicate the column groups
-  public void testStorerNegative3() throws ExecException, IOException {
-
-    String query = "records = LOAD '" + path.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    pigServer.registerQuery(query);
-
-    Iterator<Tuple> it = pigServer.openIterator("records");
-    while (it.hasNext()) {
-      Tuple RowValue = it.next();
-      System.out.println(RowValue);
-    }
-    try{
-        Path newPath = new Path(getCurrentMethodName());
-    
-        ExecJob pigJob = pigServer
-            .store(
-                "records",
-                new Path(newPath, "store").toString(),
-                TableStorer.class.getCanonicalName()
-                    + "('[s1]; [s1]')");
-    } catch(Exception e) {
-        System.out.println(e);
-        return;
-    }
-    Assert.fail("Exception expected");
-
-  }
-
-  // @Test
-  // negative, schema description is different from input tuple, different
-  // data types for columns
-  public void testStorerNegative4() throws ExecException, IOException {
-
-    String query = "records = LOAD '" + path.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    pigServer.registerQuery(query);
-
-    Iterator<Tuple> it = pigServer.openIterator("records");
-    while (it.hasNext()) {
-      Tuple RowValue = it.next();
-      System.out.println(RowValue);
-    }
-
-    Path newPath = new Path(getCurrentMethodName());
-    ExecJob pigJob = pigServer
-        .store(
-            "records",
-            new Path(newPath, "store").toString(),
-            TableStorer.class.getCanonicalName()
-                + "('[s1, s2]; [s3, s4]')");
-    Assert.assertNotNull(pigJob.getException());
-    System.out.println(pigJob.getException());
-  }
-
-  @Test
-  // Store negative, store to same path. Store should fail
-  public void testStorer5() throws ExecException, IOException {   
-    //
-    // Use pig LOAD to load testing data for store
-    //
-    String query = "records = LOAD '"
-        + path.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader() as (s1,s2,s3,s4,s5,s6);";
-    pigServer.registerQuery(query);
-
-    //
-    // Use pig STORE to store testing data
-    //
-    System.out.println("path = " + path);
-    try {
-        ExecJob pigJob = pigServer
-            .store(
-                "records",
-                path.toString(),
-                TableStorer.class.getCanonicalName()
-                    + "('[s1, s2]; [s3, s4]')");
-    } catch(Exception e) {
-        System.out.println(e);
-        return;
-    }
-    Assert.fail("Exception expected");
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestSortedTableUnion.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestSortedTableUnion.java
deleted file mode 100644
index 7b32d7964..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestSortedTableUnion.java
+++ /dev/null
@@ -1,231 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.Iterator;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.pig.TableStorer;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-import org.apache.hadoop.zebra.BaseTestCase;
-
-
-/**
- * Note:
- * 
- * Make sure you add the build/pig-0.1.0-dev-core.jar to the Classpath of the
- * app/debug configuration, when run this from inside the Eclipse.
- * 
- */
-public class TestSortedTableUnion extends BaseTestCase{
-  private static Path pathTable;
-
-  @BeforeClass
-  public static void setUp() throws Exception {
-
-    init();
-    pathTable = getTableFullPath("TestBasicTableUnionLoader1");
-    removeDir(pathTable);
-    
-    BasicTable.Writer writer = new BasicTable.Writer(pathTable,
-        "SF_a:string,SF_b:string,SF_c:string,SF_d:string,SF_e:string,SF_f:string,SF_g:string",
-        "[SF_a, SF_b, SF_c]; [SF_e, SF_f, SF_g]", conf);
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    final int numsBatch = 10;
-    final int numsInserters = 1;
-    TableInserter[] inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tuple);
-        for (int k = 0; k < tuple.size(); ++k) {
-          try {
-            tuple.set(k, (9-b) + "_" + i + "" + k);
-          } catch (ExecException e) {
-            e.printStackTrace();
-          }
-        }
-        inserters[i].insert(new BytesWritable(("key" + i).getBytes()), tuple);
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDown() throws Exception {
-    pigServer.shutdown();
-  }
-
-  /**
-   * Return the name of the routine that called getCurrentMethodName
-   * 
-   */
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    return t;
-  }
-
-  @Test
-  public void testStorer() throws ExecException, IOException {
-    /*
-     * Use pig LOAD to load testing data for store
-     */
-    String query = "records = LOAD '" + pathTable.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    pigServer.registerQuery(query);
-
-    /*
-    Iterator<Tuple> it2 = pigServer.openIterator("records");
-    int row0 = 0;
-    Tuple RowValue2 = null;
-    while (it2.hasNext()) {
-      // Last row value
-      RowValue2 = it2.next();
-      row0++;
-      if (row0 == 10) {
-        Assert.assertEquals("0_01", RowValue2.get(1));
-        Assert.assertEquals("0_00", RowValue2.get(0));
-      }
-    }
-    Assert.assertEquals(10, row0);
-    */
-
-    String orderby = "srecs = ORDER records BY SF_a;";
-    pigServer.registerQuery(orderby);
-
-    /*
-     * Use pig STORE to store testing data BasicTable.Writer writer = new
-     * BasicTable.Writer(pathTable, "SF_a,SF_b,SF_c,SF_d,SF_e,SF_f,SF_g",
-     * "[SF_a, SF_b, SF_c]; [SF_e, SF_f, SF_g]", false, conf);
-     */
-    Path newPath = new Path(getCurrentMethodName());
-
-    removeDir(getTableFullPath(newPath.toString()+"1"));
-    removeDir(getTableFullPath(newPath.toString()+"2"));
-    /*
-     * Table1 creation
-     */
-    pigServer
-        .store(
-            "srecs",
-            newPath.toString()+"1",
-            TableStorer.class.getCanonicalName()
-                + "('[SF_a, SF_b, SF_c]; [SF_e]')");
-    pigServer
-        .store(
-            "srecs",
-            newPath.toString()+"2",
-            TableStorer.class.getCanonicalName()
-                + "('[SF_a, SF_b, SF_c]; [SF_e]')");
-
-
-    String query4 = "records2 = LOAD '"
-        + newPath.toString() + "1,"
-        + newPath.toString() + "2"
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('', 'sorted');";
-    pigServer.registerQuery(query4);
-
-    String query5 = "records3 = ORDER records2 BY SF_a;";
-    pigServer.registerQuery(query5);
-
-    // check JOIN content
-    Iterator<Tuple> it3 = pigServer.openIterator("records3");
-    int row = 0, index;
-    Tuple RowValue3 = null;
-    while (it3.hasNext()) {
-      // Last row value
-      RowValue3 = it3.next();
-      Assert.assertEquals(7, RowValue3.size());
-      row++;
-      index = (row-1)/2;
-      Assert.assertEquals(index+"_01", RowValue3.get(1));
-      Assert.assertEquals(index+"_00", RowValue3.get(0));
-      Assert.assertEquals(index+"_06", RowValue3.get(6));
-      Assert.assertEquals(index+"_05", RowValue3.get(5));
-      Assert.assertEquals(index+"_04", RowValue3.get(4));
-      Assert.assertEquals(index+"_03", RowValue3.get(3));
-      Assert.assertEquals(index+"_02", RowValue3.get(2));
-    }
-    Assert.assertEquals(20, row);
-    
-    /*
-     * Test source table index on a single sorted table
-     */
-    String query6 = "records4 = LOAD '"
-      + newPath.toString() + "1"
-      + "' USING org.apache.hadoop.zebra.pig.TableLoader('SF_a, SF_b, SF_c," +
-      		"source_table, SF_d, SF_e, SF_f, SF_g', 'sorted');";
-    pigServer.registerQuery(query6);
-    Iterator<Tuple> it4 = pigServer.openIterator("records4");
-    row = 0;
-    while (it4.hasNext()) {
-      RowValue3 = it4.next();
-      Assert.assertEquals(8, RowValue3.size());
-      row++;
-      index = row-1;
-      Assert.assertEquals(0, RowValue3.get(3));
-      Assert.assertEquals(index+"_01", RowValue3.get(1));
-      Assert.assertEquals(index+"_00", RowValue3.get(0));
-      Assert.assertEquals(index+"_06", RowValue3.get(7));
-      Assert.assertEquals(index+"_05", RowValue3.get(6));
-      Assert.assertEquals(index+"_04", RowValue3.get(5));
-      Assert.assertEquals(index+"_03", RowValue3.get(4));
-      Assert.assertEquals(index+"_02", RowValue3.get(2));
-    }
-    Assert.assertEquals(10, row);
-  }
-  
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestSortedTableUnionMergeJoin.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestSortedTableUnionMergeJoin.java
deleted file mode 100644
index d7f0cc7aa..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestSortedTableUnionMergeJoin.java
+++ /dev/null
@@ -1,222 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.Iterator;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.pig.TableStorer;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.ExecType;
-import org.apache.pig.PigServer;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.test.MiniCluster;
-import org.junit.After;
-import org.junit.AfterClass;
-import org.junit.Before;
-import org.junit.BeforeClass;
-import org.junit.Test;
-import org.apache.hadoop.zebra.BaseTestCase;
-
-/**
- * Note:
- * 
- * Make sure you add the build/pig-0.1.0-dev-core.jar to the Classpath of the
- * app/debug configuration, when run this from inside the Eclipse.
- * 
- */
-public class TestSortedTableUnionMergeJoin extends BaseTestCase {
-  private static Path pathTable;
-
-  @BeforeClass
-  public static void setUp() throws Exception {
-
-    init();
-    pathTable = getTableFullPath("TestTableStorer");
-    removeDir(pathTable);
-
-
-    BasicTable.Writer writer = new BasicTable.Writer(pathTable,
-        "SF_a:string,SF_b:string,SF_c:string,SF_d:string,SF_e:string,SF_f:string,SF_g:string",
-        "[SF_a, SF_b, SF_c]; [SF_e, SF_f, SF_g]", conf);
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    final int numsBatch = 10;
-    final int numsInserters = 1;
-    TableInserter[] inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tuple);
-        for (int k = 0; k < tuple.size(); ++k) {
-          try {
-            tuple.set(k, (9-b) + "_" + i + "" + k);
-          } catch (ExecException e) {
-            e.printStackTrace();
-          }
-        }
-        inserters[i].insert(new BytesWritable(("key" + i).getBytes()), tuple);
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDown() throws Exception {
-    pigServer.shutdown();
-  }
-
-  /**
-   * Return the name of the routine that called getCurrentMethodName
-   * 
-   */
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    return t;
-  }
-
-  @Test
-  public void testStorer() throws ExecException, IOException {
-    /*
-     * Use pig LOAD to load testing data for store
-     */
-    String query = "records = LOAD '" + pathTable.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    pigServer.registerQuery(query);
-
-    /*
-    Iterator<Tuple> it2 = pigServer.openIterator("records");
-    int row0 = 0;
-    Tuple RowValue2 = null;
-    while (it2.hasNext()) {
-      // Last row value
-      RowValue2 = it2.next();
-      row0++;
-      if (row0 == 10) {
-        Assert.assertEquals("0_01", RowValue2.get(1));
-        Assert.assertEquals("0_00", RowValue2.get(0));
-      }
-    }
-    Assert.assertEquals(10, row0);
-    */
-
-    String orderby = "srecs = ORDER records BY SF_a;";
-    pigServer.registerQuery(orderby);
-
-    /*
-     * Use pig STORE to store testing data BasicTable.Writer writer = new
-     * BasicTable.Writer(pathTable, "SF_a,SF_b,SF_c,SF_d,SF_e,SF_f,SF_g",
-     * "[SF_a, SF_b, SF_c]; [SF_e, SF_f, SF_g]", false, conf);
-     */
-    Path newPath = new Path(getCurrentMethodName());
-
-    removeDir(getTableFullPath(newPath.toString()+"1"));
-    removeDir(getTableFullPath(newPath.toString()+"2"));
-    
-    /*
-     * Table1 creation
-     */
-    pigServer
-        .store(
-            "srecs",
-            newPath.toString()+"1",
-            TableStorer.class.getCanonicalName()
-                + "('[SF_a, SF_b, SF_c]; [SF_e]')");
-    pigServer
-        .store(
-            "srecs",
-            newPath.toString()+"2",
-            TableStorer.class.getCanonicalName()
-                + "('[SF_a, SF_b, SF_c]; [SF_e]')");
-
-    String query3 = "records1 = LOAD '"
-        + newPath.toString() + "1"
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('', 'sorted');";
-    pigServer.registerQuery(query3);
-
-    String query4 = "records2 = LOAD '"
-        + newPath.toString() + "1,"
-        + newPath.toString() + "2"
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('', 'sorted');";
-    pigServer.registerQuery(query4);
-
-    String join = "joinRecords = JOIN records1 BY SF_a, records2 BY SF_a USING \"merge\";";
-    pigServer.registerQuery(join);
-
-    // check JOIN content
-    String query5 = "records3 = ORDER records2 BY SF_a;";
-    pigServer.registerQuery(query5);
-
-    Iterator<Tuple> it3 = pigServer.openIterator("records3");
-    int row = 0, index;
-    Tuple RowValue3 = null;
-    while (it3.hasNext()) {
-      // Last row value
-      RowValue3 = it3.next();
-      Assert.assertEquals(7, RowValue3.size());
-      row++;
-      index = (row-1)/2;
-      Assert.assertEquals(index+"_01", RowValue3.get(1));
-      Assert.assertEquals(index+"_00", RowValue3.get(0));
-      Assert.assertEquals(index+"_06", RowValue3.get(6));
-      Assert.assertEquals(index+"_05", RowValue3.get(5));
-      Assert.assertEquals(index+"_04", RowValue3.get(4));
-      Assert.assertEquals(index+"_03", RowValue3.get(3));
-      Assert.assertEquals(index+"_02", RowValue3.get(2));
-    }
-    Assert.assertEquals(20, row);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestSortedTableUnionSourceTableProj.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestSortedTableUnionSourceTableProj.java
deleted file mode 100644
index d09b2a689..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestSortedTableUnionSourceTableProj.java
+++ /dev/null
@@ -1,212 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.Iterator;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.pig.TableStorer;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.ExecType;
-import org.apache.pig.PigServer;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.test.MiniCluster;
-import org.junit.After;
-import org.junit.AfterClass;
-import org.junit.Before;
-import org.junit.BeforeClass;
-import org.junit.Test;
-import org.apache.hadoop.zebra.BaseTestCase;
-
-
-/**
- * Note:
- * 
- * Make sure you add the build/pig-0.1.0-dev-core.jar to the Classpath of the
- * app/debug configuration, when run this from inside the Eclipse.
- * 
- */
-public class TestSortedTableUnionSourceTableProj extends BaseTestCase{
-  private static Path pathTable;
-
-  @BeforeClass
-  public static void setUp() throws Exception {
-
-    init();
-    pathTable = getTableFullPath("TestBasicTableUnionLoader1");
-    removeDir(pathTable);
-    
-    BasicTable.Writer writer = new BasicTable.Writer(pathTable,
-        "SF_a:string,SF_b:string,SF_c:string,SF_d:string,SF_e:string,SF_f:string,SF_g:string",
-        "[SF_a, SF_b, SF_c]; [SF_e, SF_f, SF_g]", conf);
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    final int numsBatch = 10;
-    final int numsInserters = 1;
-    TableInserter[] inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tuple);
-        for (int k = 0; k < tuple.size(); ++k) {
-          try {
-            tuple.set(k, (9-b) + "_" + i + "" + k);
-          } catch (ExecException e) {
-            e.printStackTrace();
-          }
-        }
-        inserters[i].insert(new BytesWritable(("key" + i).getBytes()), tuple);
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDown() throws Exception {
-    pigServer.shutdown();
-  }
-
-  /**
-   * Return the name of the routine that called getCurrentMethodName
-   * 
-   */
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    return t;
-  }
-
-  @Test
-  public void testStorer() throws ExecException, IOException {
-    /*
-     * Use pig LOAD to load testing data for store
-     */
-    String query = "records = LOAD '" + pathTable.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    pigServer.registerQuery(query);
-
-    /*
-    Iterator<Tuple> it2 = pigServer.openIterator("records");
-    int row0 = 0;
-    Tuple RowValue2 = null;
-    while (it2.hasNext()) {
-      // Last row value
-      RowValue2 = it2.next();
-      row0++;
-      if (row0 == 10) {
-        Assert.assertEquals("0_01", RowValue2.get(1));
-        Assert.assertEquals("0_00", RowValue2.get(0));
-      }
-    }
-    Assert.assertEquals(10, row0);
-    */
-
-    String orderby = "srecs = ORDER records BY SF_a;";
-    pigServer.registerQuery(orderby);
-
-    /*
-     * Use pig STORE to store testing data BasicTable.Writer writer = new
-     * BasicTable.Writer(pathTable, "SF_a,SF_b,SF_c,SF_d,SF_e,SF_f,SF_g",
-     * "[SF_a, SF_b, SF_c]; [SF_e, SF_f, SF_g]", false, conf);
-     */
-    Path newPath = new Path(getCurrentMethodName());
-
-    /*
-     * Table1 creation
-     */
-    removeDir(getTableFullPath(newPath.toString()+"1"));
-    pigServer
-        .store(
-            "srecs",
-            newPath.toString()+"1",
-            TableStorer.class.getCanonicalName()
-                + "('[SF_a, SF_b, SF_c]; [SF_e]')");
-    removeDir(getTableFullPath(newPath.toString()+"2"));
-    pigServer
-        .store(
-            "srecs",
-            newPath.toString()+"2",
-            TableStorer.class.getCanonicalName()
-                + "('[SF_a, SF_b, SF_c]; [SF_e]')");
-
-
-    String query4 = "records2 = LOAD '"
-        + newPath.toString() + "1,"
-        + newPath.toString() + "2"
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('SF_a, SF_b, source_table', 'sorted');";
-    pigServer.registerQuery(query4);
-    
-    String foreach2 = "foreachsort2 = FOREACH records2 GENERATE SF_a, $2;";
-    pigServer.registerQuery(foreach2);
-
-    // check JOIN content
-    Iterator<Tuple> it4 = pigServer.openIterator("foreachsort2");
-    int row = 0, index;
-    Tuple RowValue3 = null;
-    while (it4.hasNext()) {
-      // Last row value
-      RowValue3 = it4.next();
-      Assert.assertEquals(2, RowValue3.size());
-      row++;
-      if (row < 11)
-        index = row-1;
-      else
-        index = row-11;
-      Assert.assertEquals(index+"_00", RowValue3.get(0));
-    }
-    Assert.assertEquals(20, row);
-  }
-  
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestTableLoader.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestTableLoader.java
deleted file mode 100644
index 25807bf27..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestTableLoader.java
+++ /dev/null
@@ -1,132 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.File;
-import java.io.IOException;
-import java.util.Iterator;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.ExecType;
-import org.apache.pig.PigServer;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.test.MiniCluster;
-import org.junit.Test;
-import org.junit.BeforeClass;
-import org.junit.AfterClass;
-import junit.framework.Assert;
-import org.apache.hadoop.zebra.BaseTestCase;
-
-/**
- * Note:
- * 
- * Make sure you add the build/pig-0.1.0-dev-core.jar to the Classpath of the
- * app/debug configuration, when run this from inside the Eclipse.
- * 
- */
-public class TestTableLoader extends BaseTestCase {
-  static private Path pathTable;
-
-  @BeforeClass
-  public static void setUp() throws Exception {
-
-    init();
-    pathTable = getTableFullPath("TestTableLoader");
-    removeDir(pathTable);
-
-    BasicTable.Writer writer = new BasicTable.Writer(pathTable,
-        "a:string,b:string,c:string,d:string,e:string,f:string,g:string", "[a,b,c];[d,e,f,g]", conf);
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    final int numsBatch = 10;
-    final int numsInserters = 2;
-    TableInserter[] inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tuple);
-        for (int k = 0; k < tuple.size(); ++k) {
-          try {
-            tuple.set(k, b + "_" + i + "" + k);
-          } catch (ExecException e) {
-            e.printStackTrace();
-          }
-        }
-        inserters[i].insert(new BytesWritable(("key" + i).getBytes()), tuple);
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-    writer.close();
-  }
-
-  @AfterClass
-  static public void tearDown() throws Exception {
-    pigServer.shutdown();
-  }
-
-  @Test
-  public void testReader1() throws ExecException, IOException {
-    String query = "records = LOAD '" + pathTable.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('c, a');";
-    System.out.println(query);
-    pigServer.registerQuery(query);
-    Iterator<Tuple> it = pigServer.openIterator("records");
-    while (it.hasNext()) {
-      Tuple cur = it.next();
-      System.out.println(cur);
-    }
-  }
-
-  @Test
-  public void testReader2() throws ExecException, IOException {
-    String query = "records = LOAD '" + pathTable.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('a, aa, b, c');";
-    System.out.println(query);
-    pigServer.registerQuery(query);
-    Iterator<Tuple> it = pigServer.openIterator("records");
-    int cnt = 0;
-    while (it.hasNext()) {
-      Tuple cur = it.next();
-      cnt++;
-      if (cnt == 1) {
-        Assert.assertEquals(4, cur.size());
-        Assert.assertEquals("0_00", cur.get(0));
-        Assert.assertEquals(null, cur.get(1));
-        Assert.assertEquals("0_01", cur.get(2));
-        Assert.assertEquals("0_02", cur.get(3));
-      }
-      System.out.println(cur);
-    }
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestTableLoaderPrune.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestTableLoaderPrune.java
deleted file mode 100644
index 0d15bb2b4..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestTableLoaderPrune.java
+++ /dev/null
@@ -1,143 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.File;
-import java.io.IOException;
-import java.util.Iterator;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.ExecType;
-import org.apache.pig.PigServer;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.test.MiniCluster;
-import org.junit.Test;
-import org.junit.BeforeClass;
-import org.junit.AfterClass;
-import junit.framework.Assert;
-import org.apache.hadoop.zebra.BaseTestCase;
-
-
-/**
- * Note:
- * 
- * Make sure you add the build/pig-0.1.0-dev-core.jar to the Classpath of the
- * app/debug configuration, when run this from inside the Eclipse.
- * 
- */
-public class TestTableLoaderPrune extends BaseTestCase {
-  static private Path pathTable;
-
-  @BeforeClass
-  public static void setUp() throws Exception {
-
-    init();
-    pathTable = getTableFullPath("TestMapType");
-    removeDir(pathTable);
-
-
-    BasicTable.Writer writer = new BasicTable.Writer(pathTable,
-        "a:string,b:string,c:string,d:string,e:string,f:string,g:string", "[a,b,c];[d,e,f,g]", conf);
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    final int numsBatch = 10;
-    final int numsInserters = 2;
-    TableInserter[] inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tuple);
-        for (int k = 0; k < tuple.size(); ++k) {
-          try {
-            tuple.set(k, b + "_" + i + "" + k);
-          } catch (ExecException e) {
-            e.printStackTrace();
-          }
-        }
-        inserters[i].insert(new BytesWritable(("key" + i).getBytes()), tuple);
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-    writer.close();
-  }
-
-  @AfterClass
-  static public void tearDown() throws Exception {
-    pigServer.shutdown();
-  }
-
-//  @Test
-  public void testReader1() throws ExecException, IOException {
-    String query = "records = LOAD '" + pathTable.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('c, a');";
-    
-  //  String query = "records = LOAD '/homes/jing1234/grid/pig-table20/pig-table/contrib/zebra/src/keyGenerator/test3' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    
-
-    System.out.println(query);
-    pigServer.registerQuery(query);
-    Iterator<Tuple> it = pigServer.openIterator("records");
-    while (it.hasNext()) {
-      Tuple cur = it.next();
-      System.out.println(cur);
-    }
-  }
-
-
-  @Test
-  public void testReader2() throws ExecException, IOException {
-    String query = "records = LOAD '" + pathTable.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('a, aa, b, c');";
-    System.out.println(query);
-    
-    String prune = "pruneR = foreach records generate $0, $2;";
-    
-    pigServer.registerQuery(query);
-    pigServer.registerQuery(prune);  
-    Iterator<Tuple> it = pigServer.openIterator("pruneR");
-//    Iterator<Tuple> it = pigServer.openIterator("records");
-
-    int cnt = 0;
-    while (it.hasNext()) {
-      Tuple cur = it.next();
-      cnt++;
-      if (cnt == 1) {
-        Assert.assertEquals(2, cur.size());
-        Assert.assertEquals("0_00", cur.get(0));
-        Assert.assertEquals("0_01", cur.get(1));
-      }
-      System.out.println(cur);
-    }
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestTableMergeJoin.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestTableMergeJoin.java
deleted file mode 100644
index 521063eb2..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestTableMergeJoin.java
+++ /dev/null
@@ -1,240 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.Iterator;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.pig.TableStorer;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.ExecType;
-import org.apache.pig.PigServer;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.test.MiniCluster;
-import org.junit.After;
-import org.junit.AfterClass;
-import org.junit.Before;
-import org.junit.BeforeClass;
-import org.junit.Test;
-import org.apache.hadoop.zebra.BaseTestCase;
-
-
-/**
- * Note:
- * 
- * Make sure you add the build/pig-0.1.0-dev-core.jar to the Classpath of the
- * app/debug configuration, when run this from inside the Eclipse.
- * 
- */
-public class TestTableMergeJoin extends BaseTestCase {
-  private static Path pathTable;
-
-
-  @BeforeClass
-  public static void setUp() throws Exception {
-
-    init();
-    pathTable = getTableFullPath("TestTableMergeJoin");
-    removeDir(pathTable);
-
-    BasicTable.Writer writer = new BasicTable.Writer(pathTable,
-        "SF_a:string,SF_b:string,SF_c:string,SF_d:string,SF_e:string,SF_f:string,SF_g:string",
-        "[SF_a, SF_b, SF_c]; [SF_e, SF_f, SF_g]", conf);
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    final int numsBatch = 10;
-    final int numsInserters = 1;
-    TableInserter[] inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tuple);
-        for (int k = 0; k < tuple.size(); ++k) {
-          try {
-            tuple.set(k, (9-b) + "_" + i + "" + k);
-          } catch (ExecException e) {
-            e.printStackTrace();
-          }
-        }
-        inserters[i].insert(new BytesWritable(("key" + i).getBytes()), tuple);
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDown() throws Exception {
-    pigServer.shutdown();
-  }
-
-  /**
-   * Return the name of the routine that called getCurrentMethodName
-   * 
-   */
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    return t;
-  }
-
-  @Test
-  public void testStorer() throws ExecException, IOException {
-    /*
-     * Use pig LOAD to load testing data for store
-     */
-    String query = "records = LOAD '" + pathTable.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    pigServer.registerQuery(query);
-
-    /*
-    Iterator<Tuple> it2 = pigServer.openIterator("records");
-    int row0 = 0;
-    Tuple RowValue2 = null;
-    while (it2.hasNext()) {
-      // Last row value
-      RowValue2 = it2.next();
-      row0++;
-      if (row0 == 10) {
-        Assert.assertEquals("0_01", RowValue2.get(1));
-        Assert.assertEquals("0_00", RowValue2.get(0));
-      }
-    }
-    Assert.assertEquals(10, row0);
-    */
-
-    String orderby = "srecs = ORDER records BY SF_a;";
-    pigServer.registerQuery(orderby);
-
-    /*
-     * Use pig STORE to store testing data BasicTable.Writer writer = new
-     * BasicTable.Writer(pathTable, "SF_a,SF_b,SF_c,SF_d,SF_e,SF_f,SF_g",
-     * "[SF_a, SF_b, SF_c]; [SF_e, SF_f, SF_g]", false, conf);
-     */
-    Path newPath = new Path(getCurrentMethodName());
-    removeDir(getTableFullPath(newPath.toString()+"1"));
-    /*
-     * Table1 creation
-     */
-    pigServer
-        .store(
-            "srecs",
-            newPath.toString()+"1",
-            TableStorer.class.getCanonicalName()
-                + "('[SF_a, SF_b, SF_c]; [SF_e]')");
-
-    String query3 = "records1 = LOAD '"
-        + newPath.toString() + "1"
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('', 'sorted');";
-    pigServer.registerQuery(query3);
-
-    /*
-    Iterator<Tuple> it3 = pigServer.openIterator("records1");
-    int row = 0;
-    Tuple RowValue3 = null;
-    while (it3.hasNext()) {
-      // Last row value
-      RowValue3 = it3.next();
-      Assert.assertEquals(2, RowValue3.size());
-      row++;
-      if (row == 10) {
-        Assert.assertEquals("9_01", RowValue3.get(1));
-        Assert.assertEquals("9_00", RowValue3.get(0));
-      }
-    }
-    Assert.assertEquals(10, row);
-    */
-    removeDir(getTableFullPath(newPath.toString()+"2"));
-    /*
-     * Table2 creation
-     */
-    pigServer
-        .store(
-            "srecs",
-            newPath.toString()+"2",
-            TableStorer.class.getCanonicalName()
-                + "('[SF_a, SF_b, SF_c]; [SF_e]')");
-
-
-    String query4 = "records2 = LOAD '"
-        + newPath.toString() + "2"
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    pigServer.registerQuery(query4);
-
-    String join = "joinRecords = JOIN records1 BY SF_a, records2 BY SF_a USING \"merge\";";
-    pigServer.registerQuery(join);
-    // check JOIN content
-    Iterator<Tuple> it3 = pigServer.openIterator("joinRecords");
-    int row = 0;
-    Tuple RowValue3 = null;
-    while (it3.hasNext()) {
-      // Last row value
-      RowValue3 = it3.next();
-      Assert.assertEquals(14, RowValue3.size());
-      row++;
-      if (row == 10) {
-        Assert.assertEquals("9_01", RowValue3.get(1));
-        Assert.assertEquals("9_00", RowValue3.get(0));
-        Assert.assertEquals("9_01", RowValue3.get(8));
-        Assert.assertEquals("9_00", RowValue3.get(7));
-        Assert.assertEquals("9_06", RowValue3.get(6));
-        Assert.assertEquals("9_05", RowValue3.get(5));
-        Assert.assertEquals("9_04", RowValue3.get(4));
-        Assert.assertEquals("9_03", RowValue3.get(3));
-        Assert.assertEquals("9_02", RowValue3.get(2));
-      }
-    }
-    Assert.assertEquals(10, row);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestTableMergeJoinAfterFilter.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestTableMergeJoinAfterFilter.java
deleted file mode 100644
index b905db2b9..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestTableMergeJoinAfterFilter.java
+++ /dev/null
@@ -1,243 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.Iterator;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.pig.TableStorer;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.ExecType;
-import org.apache.pig.PigServer;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.test.MiniCluster;
-import org.junit.After;
-import org.junit.AfterClass;
-import org.junit.Before;
-import org.junit.BeforeClass;
-import org.junit.Test;
-import org.apache.hadoop.zebra.BaseTestCase;
-
-
-/**
- * Note:
- * 
- * Make sure you add the build/pig-0.1.0-dev-core.jar to the Classpath of the
- * app/debug configuration, when run this from inside the Eclipse.
- * 
- */
-public class TestTableMergeJoinAfterFilter extends BaseTestCase {
-  private static Path pathTable;
-
-  @BeforeClass
-  public static void setUp() throws Exception {
-
-    init();
-    pathTable = getTableFullPath("TestTableMergeJoinAfterFilter");
-    removeDir(pathTable);
-
-
-    BasicTable.Writer writer = new BasicTable.Writer(pathTable,
-        "SF_a:string,SF_b:string,SF_c:string,SF_d:string,SF_e:string,SF_f:string,SF_g:string",
-        "[SF_a, SF_b, SF_c]; [SF_e, SF_f, SF_g]", conf);
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    final int numsBatch = 10;
-    final int numsInserters = 1;
-    TableInserter[] inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tuple);
-        for (int k = 0; k < tuple.size(); ++k) {
-          try {
-            tuple.set(k, (9-b) + "_" + i + "" + k);
-          } catch (ExecException e) {
-            e.printStackTrace();
-          }
-        }
-        inserters[i].insert(new BytesWritable(("key" + i).getBytes()), tuple);
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDown() throws Exception {
-    pigServer.shutdown();
-  }
-
-  /**
-   * Return the name of the routine that called getCurrentMethodName
-   * 
-   */
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    return t;
-  }
-
-  @Test
-  public void testStorer() throws ExecException, IOException {
-    /*
-     * Use pig LOAD to load testing data for store
-     */
-    String query = "records = LOAD '" + pathTable.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    pigServer.registerQuery(query);
-
-    /*
-    Iterator<Tuple> it2 = pigServer.openIterator("records");
-    int row0 = 0;
-    Tuple RowValue2 = null;
-    while (it2.hasNext()) {
-      // Last row value
-      RowValue2 = it2.next();
-      row0++;
-      if (row0 == 10) {
-        Assert.assertEquals("0_01", RowValue2.get(1));
-        Assert.assertEquals("0_00", RowValue2.get(0));
-      }
-    }
-    Assert.assertEquals(10, row0);
-    */
-
-    String orderby = "srecs = ORDER records BY SF_a;";
-    pigServer.registerQuery(orderby);
-
-    /*
-     * Use pig STORE to store testing data BasicTable.Writer writer = new
-     * BasicTable.Writer(pathTable, "SF_a,SF_b,SF_c,SF_d,SF_e,SF_f,SF_g",
-     * "[SF_a, SF_b, SF_c]; [SF_e, SF_f, SF_g]", false, conf);
-     */
-    Path newPath = new Path(getCurrentMethodName());
-
-    /*
-     * Table1 creation
-     */
-    removeDir(getTableFullPath(newPath.toString()+"1"));
-    pigServer
-        .store(
-            "srecs",
-            newPath.toString()+"1",
-            TableStorer.class.getCanonicalName()
-                + "('[SF_a, SF_b, SF_c]; [SF_e]')");
-
-    String query3 = "records1 = LOAD '"
-        + newPath.toString() + "1"
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('SF_a, SF_b', 'sorted');";
-    pigServer.registerQuery(query3);
-
-    /*
-     * Table2 creation
-     */
-    removeDir(getTableFullPath(newPath.toString()+"2"));
-    pigServer
-        .store(
-            "srecs",
-            newPath.toString()+"2",
-            TableStorer.class.getCanonicalName()
-                + "('[SF_a, SF_b, SF_c]; [SF_e]')");
-
-
-    String query4 = "records2 = LOAD '"
-        + newPath.toString() + "2"
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    pigServer.registerQuery(query4);
-
-    String filter = "records3 = FILTER records2 BY SF_a > '4';";
-    pigServer.registerQuery(filter);
-
-    Iterator<Tuple> it2 = pigServer.openIterator("records3");
-    int row2 = 0;
-    Tuple RowValue2 = null;
-    while (it2.hasNext()) {
-      // Last row value
-      RowValue2 = it2.next();
-      Assert.assertEquals(7, RowValue2.size());
-      row2++;
-      if (row2 == 5) {
-        Assert.assertEquals("8_01", RowValue2.get(1));
-        Assert.assertEquals("8_00", RowValue2.get(0));
-      }
-    }
-    Assert.assertEquals(6, row2);
-    
-    String join = "joinRecords = JOIN records1 BY SF_a, records3 BY SF_a USING \"merge\";";
-    pigServer.registerQuery(join);
-    // check JOIN content
-    Iterator<Tuple> it3 = pigServer.openIterator("joinRecords");
-    int row = 0;
-    Tuple RowValue3 = null;
-    while (it3.hasNext()) {
-      // Last row value
-      RowValue3 = it3.next();
-      Assert.assertEquals(9, RowValue3.size());
-      row++;
-      if (row == 6) {
-        Assert.assertEquals("9_01", RowValue3.get(1));
-        Assert.assertEquals("9_00", RowValue3.get(0));
-        Assert.assertEquals("9_06", RowValue3.get(8));
-        Assert.assertEquals("9_05", RowValue3.get(7));
-        Assert.assertEquals("9_04", RowValue3.get(6));
-        Assert.assertEquals("9_03", RowValue3.get(5));
-        Assert.assertEquals("9_02", RowValue3.get(4));
-        Assert.assertEquals("9_01", RowValue3.get(3));
-        Assert.assertEquals("9_00", RowValue3.get(2));
-      }
-    }
-    Assert.assertEquals(6, row);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestTableMergeJoinFloat.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestTableMergeJoinFloat.java
deleted file mode 100644
index 2fbbf2dd2..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestTableMergeJoinFloat.java
+++ /dev/null
@@ -1,210 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.Iterator;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.pig.TableStorer;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.ExecType;
-import org.apache.pig.PigServer;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.test.MiniCluster;
-import org.junit.After;
-import org.junit.AfterClass;
-import org.junit.Before;
-import org.junit.BeforeClass;
-import org.junit.Test;
-import org.apache.hadoop.zebra.BaseTestCase;
-
-
-/**
- * Note:
- * 
- * Make sure you add the build/pig-0.1.0-dev-core.jar to the Classpath of the
- * app/debug configuration, when run this from inside the Eclipse.
- * 
- */
-public class TestTableMergeJoinFloat extends BaseTestCase {
-  private static Path pathTable;
-
-  @BeforeClass
-  public static void setUp() throws Exception {
-
-    init();
-    pathTable = getTableFullPath("TestTableMergeJoinFloat");
-    removeDir(pathTable);
-
-    BasicTable.Writer writer = new BasicTable.Writer(pathTable,
-        "SF_a:float,SF_b:string,SF_c:string,SF_d:string,SF_e:string,SF_f:string,SF_g:string",
-        "[SF_a, SF_b, SF_c]; [SF_e, SF_f, SF_g]", conf);
-    Schema schema = writer.getSchema();
-    System.out.println("typeName" + schema.getColumn("SF_a").getType().pigDataType());
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    final int numsBatch = 10;
-    final int numsInserters = 1;
-    TableInserter[] inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tuple);
-        for (int k = 0; k < tuple.size(); ++k) {
-          try {
-        	if(k==0) {
-        		Float f = 32.987f;
-        		tuple.set(0, k+b-f);
-        	} else {
-        		tuple.set(k, b + "_" + i + "" + k);
-        	}
-          } catch (ExecException e) {
-            e.printStackTrace();
-          }
-        }
-        inserters[i].insert(new BytesWritable(("key" + i).getBytes()), tuple);
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDown() throws Exception {
-    pigServer.shutdown();
-  }
-
-  /**
-   * Return the name of the routine that called getCurrentMethodName
-   * 
-   */
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    return t;
-  }
-
-  @Test
-  public void testStorer() throws ExecException, IOException {
-    /*
-     * Use pig LOAD to load testing data for store
-     */
-    String query = "records = LOAD '" + pathTable.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    pigServer.registerQuery(query);
-
-
-    String orderby = "srecs = ORDER records BY SF_a;";
-    pigServer.registerQuery(orderby);
-
-    Path newPath = new Path(getCurrentMethodName());
-
-    /*
-     * Table1 creation
-     */
-
-    
-    pigServer
-        .store(
-            "srecs",
-            newPath.toString()+"1",
-            TableStorer.class.getCanonicalName()
-                + "('[SF_a, SF_b, SF_c]; [SF_e]')");
-
-    String query3 = "records1 = LOAD '"
-        + newPath.toString() + "1"
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('SF_a, SF_b', 'sorted');";
-    pigServer.registerQuery(query3);
-    /*
-     * Table2 creation
-     */
-
-    pigServer
-        .store(
-            "srecs",
-            newPath.toString()+"2",
-            TableStorer.class.getCanonicalName()
-                + "('[SF_a, SF_b, SF_c]; [SF_e]')");
-
-
-    String query4 = "records2 = LOAD '"
-        + newPath.toString() + "2"
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    pigServer.registerQuery(query4);
-
-    String join = "joinRecords = JOIN records1 BY SF_a, records2 BY SF_a USING \"merge\";";
-    pigServer.registerQuery(join);
-    // check JOIN content
-    Iterator<Tuple> it3 = pigServer.openIterator("joinRecords");
-    int row = 0;
-    Tuple RowValue3 = null;
-    while (it3.hasNext()) {
-      // Last row value
-      RowValue3 = it3.next();
-      Assert.assertEquals(9, RowValue3.size());
-      row++;
-      if (row == 10) {
-        Assert.assertEquals("9_01", RowValue3.get(1));
-        Assert.assertEquals(-23.987f, RowValue3.get(0));
-        Assert.assertEquals("9_06", RowValue3.get(8));
-        Assert.assertEquals("9_05", RowValue3.get(7));
-        Assert.assertEquals("9_04", RowValue3.get(6));
-        Assert.assertEquals("9_03", RowValue3.get(5));
-        Assert.assertEquals("9_02", RowValue3.get(4));
-        Assert.assertEquals("9_01", RowValue3.get(3));
-        Assert.assertEquals(-23.987f, RowValue3.get(2));
-      }
-    }
-    Assert.assertEquals(10, row);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestTableMergeJoinInteger.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestTableMergeJoinInteger.java
deleted file mode 100644
index 10507ab24..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestTableMergeJoinInteger.java
+++ /dev/null
@@ -1,210 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.Iterator;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.pig.TableStorer;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.ExecType;
-import org.apache.pig.PigServer;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.test.MiniCluster;
-import org.junit.After;
-import org.junit.AfterClass;
-import org.junit.Before;
-import org.junit.BeforeClass;
-import org.junit.Test;
-import org.apache.hadoop.zebra.BaseTestCase;
-
-
-/**
- * Note:
- * 
- * Make sure you add the build/pig-0.1.0-dev-core.jar to the Classpath of the
- * app/debug configuration, when run this from inside the Eclipse.
- * 
- */
-public class TestTableMergeJoinInteger extends BaseTestCase {
-  private static Path pathTable;
-
-  @BeforeClass
-  public static void setUp() throws Exception {
-
-    init();
-    pathTable = getTableFullPath("TestTableMergeJoinInteger");
-    removeDir(pathTable);
-
-
-    BasicTable.Writer writer = new BasicTable.Writer(pathTable,
-        "SF_a:int,SF_b:string,SF_c:string,SF_d:string,SF_e:string,SF_f:string,SF_g:string",
-        "[SF_a, SF_b, SF_c]; [SF_e, SF_f, SF_g]", conf);
-    Schema schema = writer.getSchema();
-    System.out.println("typeName" + schema.getColumn("SF_a").getType().pigDataType());
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    final int numsBatch = 10;
-    final int numsInserters = 1;
-    TableInserter[] inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tuple);
-        for (int k = 0; k < tuple.size(); ++k) {
-          try {
-        	if(k==0) {
-        		tuple.set(0, k+b);
-        	} else {
-        		tuple.set(k, b + "_" + i + "" + k);
-        	}
-          } catch (ExecException e) {
-            e.printStackTrace();
-          }
-        }
-        inserters[i].insert(new BytesWritable(("key" + i).getBytes()), tuple);
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDown() throws Exception {
-    pigServer.shutdown();
-  }
-
-  /**
-   * Return the name of the routine that called getCurrentMethodName
-   * 
-   */
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    return t;
-  }
-
-  @Test
-  public void testStorer() throws ExecException, IOException {
-    /*
-     * Use pig LOAD to load testing data for store
-     */
-    String query = "records = LOAD '" + pathTable.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    pigServer.registerQuery(query);
-
-
-    String orderby = "srecs = ORDER records BY SF_a;";
-    pigServer.registerQuery(orderby);
-
-    Path newPath = new Path(getCurrentMethodName());
-
-    /*
-     * Table1 creation
-     */
-
-    
-    pigServer
-        .store(
-            "srecs",
-            newPath.toString()+"1",
-            TableStorer.class.getCanonicalName()
-                + "('[SF_a, SF_b, SF_c]; [SF_e]')");
-
-    String query3 = "records1 = LOAD '"
-        + newPath.toString() + "1"
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('SF_a, SF_b', 'sorted');";
-    pigServer.registerQuery(query3);
-    /*
-     * Table2 creation
-     */
-
-    pigServer
-        .store(
-            "srecs",
-            newPath.toString()+"2",
-            TableStorer.class.getCanonicalName()
-                + "('[SF_a, SF_b, SF_c]; [SF_e]')");
-
-
-    String query4 = "records2 = LOAD '"
-        + newPath.toString() + "2"
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    pigServer.registerQuery(query4);
-
-    String join = "joinRecords = JOIN records1 BY SF_a, records2 BY SF_a USING \"merge\";";
-    pigServer.registerQuery(join);
-    // check JOIN content
-    Iterator<Tuple> it3 = pigServer.openIterator("joinRecords");
-    int row = 0;
-    Tuple RowValue3 = null;
-    while (it3.hasNext()) {
-      // Last row value
-      RowValue3 = it3.next();
-      Assert.assertEquals(9, RowValue3.size());
-      row++;
-      if (row == 10) {
-        Assert.assertEquals("9_01", RowValue3.get(1));
-        Assert.assertEquals(9, RowValue3.get(0));
-        Assert.assertEquals("9_06", RowValue3.get(8));
-        Assert.assertEquals("9_05", RowValue3.get(7));
-        Assert.assertEquals("9_04", RowValue3.get(6));
-        Assert.assertEquals("9_03", RowValue3.get(5));
-        Assert.assertEquals("9_02", RowValue3.get(4));
-        Assert.assertEquals("9_01", RowValue3.get(3));
-        Assert.assertEquals(9, RowValue3.get(2));
-      }
-    }
-    Assert.assertEquals(10, row);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestTableMergeJoinMultipleColsSort.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestTableMergeJoinMultipleColsSort.java
deleted file mode 100644
index 073b7790e..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestTableMergeJoinMultipleColsSort.java
+++ /dev/null
@@ -1,213 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.Iterator;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.pig.TableStorer;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.ExecType;
-import org.apache.pig.PigServer;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.test.MiniCluster;
-import org.junit.After;
-import org.junit.AfterClass;
-import org.junit.Before;
-import org.junit.BeforeClass;
-import org.junit.Test;
-import org.apache.hadoop.zebra.BaseTestCase;
-
-
-/**
- * Note:
- * 
- * Make sure you add the build/pig-0.1.0-dev-core.jar to the Classpath of the
- * app/debug configuration, when run this from inside the Eclipse.
- * 
- */
-public class TestTableMergeJoinMultipleColsSort extends BaseTestCase {
-  private static Path pathTable;
-
-  @BeforeClass
-  public static void setUp() throws Exception {
-
-
-    init();
-    pathTable = getTableFullPath("TestTableMergeJoinMultipleColsSort");
-    removeDir(pathTable);
-
-
-    BasicTable.Writer writer = new BasicTable.Writer(pathTable,
-        "SF_a:int,SF_b:int,SF_c:string,SF_d:string,SF_e:string,SF_f:string,SF_g:string",
-        "[SF_a, SF_b, SF_c]; [SF_e, SF_f, SF_g]", conf);
-    Schema schema = writer.getSchema();
-    System.out.println("typeName" + schema.getColumn("SF_a").getType().pigDataType());
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    final int numsBatch = 10;
-    final int numsInserters = 1;
-    TableInserter[] inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tuple);
-        for (int k = 0; k < tuple.size(); ++k) {
-          try {
-        	if(k==0) {
-        		tuple.set(0, k+b);
-        	} else if(k==1) {
-        		tuple.set(1, numsBatch - b);
-        	} else {
-        		tuple.set(k, b + "_" + i + "" + k);
-        	}
-          } catch (ExecException e) {
-            e.printStackTrace();
-          }
-        }
-        inserters[i].insert(new BytesWritable(("key" + i).getBytes()), tuple);
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDown() throws Exception {
-    pigServer.shutdown();
-  }
-
-  /**
-   * Return the name of the routine that called getCurrentMethodName
-   * 
-   */
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    return t;
-  }
-
-  @Test
-  public void testStorer() throws ExecException, IOException {
-    /*
-     * Use pig LOAD to load testing data for store
-     */
-    String query = "records = LOAD '" + pathTable.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    pigServer.registerQuery(query);
-
-
-    String orderby = "srecs = ORDER records BY SF_a,SF_b;";
-    pigServer.registerQuery(orderby);
-
-    Path newPath = new Path(getCurrentMethodName());
-
-    /*
-     * Table1 creation
-     */
-
-    removeDir(getTableFullPath(newPath.toString()+"1"));    
-    pigServer
-        .store(
-            "srecs",
-            newPath.toString()+"1",
-            TableStorer.class.getCanonicalName()
-                + "('[SF_a, SF_b, SF_c]; [SF_e]')");
-
-    String query3 = "records1 = LOAD '"
-        + newPath.toString() + "1"
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('SF_a, SF_b', 'sorted');";
-    pigServer.registerQuery(query3);
-    /*
-     * Table2 creation
-     */
-    removeDir(getTableFullPath(newPath.toString()+"2"));
-    pigServer
-        .store(
-            "srecs",
-            newPath.toString()+"2",
-            TableStorer.class.getCanonicalName()
-                + "('[SF_a, SF_b, SF_c]; [SF_e]')");
-
-
-    String query4 = "records2 = LOAD '"
-        + newPath.toString() + "2"
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    pigServer.registerQuery(query4);
-
-    String join = "joinRecords = JOIN records1 BY (SF_a, SF_b), records2 BY (SF_a,SF_b) USING \"merge\";";
-    pigServer.registerQuery(join);
-    // check JOIN content
-    Iterator<Tuple> it3 = pigServer.openIterator("joinRecords");
-    int row = 0;
-    Tuple RowValue3 = null;
-    while (it3.hasNext()) {
-      // Last row value
-      RowValue3 = it3.next();
-      Assert.assertEquals(9, RowValue3.size());
-      row++;
-      if (row == 10) {
-        Assert.assertEquals(1, RowValue3.get(1));
-        Assert.assertEquals(9, RowValue3.get(0));
-        Assert.assertEquals("9_06", RowValue3.get(8));
-        Assert.assertEquals("9_05", RowValue3.get(7));
-        Assert.assertEquals("9_04", RowValue3.get(6));
-        Assert.assertEquals("9_03", RowValue3.get(5));
-        Assert.assertEquals("9_02", RowValue3.get(4));
-        Assert.assertEquals(1, RowValue3.get(3));
-        Assert.assertEquals(9, RowValue3.get(2));
-      }
-    }
-    Assert.assertEquals(10, row);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestTableSortStorer.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestTableSortStorer.java
deleted file mode 100644
index 785ec775d..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestTableSortStorer.java
+++ /dev/null
@@ -1,191 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.Iterator;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.pig.TableStorer;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.ExecType;
-import org.apache.pig.PigServer;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.test.MiniCluster;
-import org.junit.After;
-import org.junit.AfterClass;
-import org.junit.Before;
-import org.junit.BeforeClass;
-import org.junit.Test;
-import org.apache.hadoop.zebra.BaseTestCase;
-
-
-/**
- * Note:
- * 
- * Make sure you add the build/pig-0.1.0-dev-core.jar to the Classpath of the
- * app/debug configuration, when run this from inside the Eclipse.
- * 
- */
-public class TestTableSortStorer extends BaseTestCase {
-  private static Path pathTable;
-
-  @BeforeClass
-  public static void setUp() throws Exception {
-
-    init();
-    pathTable = getTableFullPath("TestTableSortStorer");
-    removeDir(pathTable);
-
-    BasicTable.Writer writer = new BasicTable.Writer(pathTable,
-        "SF_a:string,SF_b:string,SF_c:string,SF_d:string,SF_e:string,SF_f:string,SF_g:string",
-        "[SF_a, SF_b, SF_c]; [SF_e, SF_f, SF_g]", conf);
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    final int numsBatch = 10;
-    final int numsInserters = 1;
-    TableInserter[] inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tuple);
-        for (int k = 0; k < tuple.size(); ++k) {
-          try {
-            tuple.set(k, (9-b) + "_" + i + "" + k);
-          } catch (ExecException e) {
-            e.printStackTrace();
-          }
-        }
-        inserters[i].insert(new BytesWritable(("key" + i).getBytes()), tuple);
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDown() throws Exception {
-    pigServer.shutdown();
-  }
-
-  /**
-   * Return the name of the routine that called getCurrentMethodName
-   * 
-   */
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    return t;
-  }
-
-  @Test
-  public void testStorer() throws ExecException, IOException {
-    /*
-     * Use pig LOAD to load testing data for store
-     */
-    String query = "records = LOAD '" + pathTable.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    pigServer.registerQuery(query);
-
-    Iterator<Tuple> it2 = pigServer.openIterator("records");
-    int row0 = 0;
-    Tuple RowValue2 = null;
-    while (it2.hasNext()) {
-      // Last row value
-      RowValue2 = it2.next();
-      row0++;
-      if (row0 == 10) {
-        Assert.assertEquals("0_01", RowValue2.get(1));
-        Assert.assertEquals("0_00", RowValue2.get(0).toString());
-      }
-    }
-    Assert.assertEquals(10, row0);
-
-    String orderby = "srecs = ORDER records BY SF_a;";
-    pigServer.registerQuery(orderby);
-
-    /*
-     * Use pig STORE to store testing data BasicTable.Writer writer = new
-     * BasicTable.Writer(pathTable, "SF_a,SF_b,SF_c,SF_d,SF_e,SF_f,SF_g",
-     * "[SF_a, SF_b, SF_c]; [SF_e, SF_f, SF_g]", false, conf);
-     */
-    Path newPath = getTableFullPath(getCurrentMethodName());
-    removeDir(newPath);
-    pigServer
-        .store(
-            "srecs",
-            newPath.toString(),
-            TableStorer.class.getCanonicalName()
-                + "('[SF_a, SF_b, SF_c]; [SF_e]')");
-
-    // check new table content
-    String query3 = "newRecords = LOAD '"
-        + newPath.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('SF_a, SF_b');";
-    pigServer.registerQuery(query3);
-
-    Iterator<Tuple> it3 = pigServer.openIterator("newRecords");
-    int row = 0;
-    Tuple RowValue3 = null;
-    while (it3.hasNext()) {
-      // Last row value
-      RowValue3 = it3.next();
-      row++;
-      if (row == 10) {
-        Assert.assertEquals("9_01", RowValue3.get(1));
-        Assert.assertEquals("9_00", RowValue3.get(0).toString());
-      }
-    }
-    Assert.assertEquals(10, row);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestTableSortStorerDesc.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestTableSortStorerDesc.java
deleted file mode 100644
index 829574896..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestTableSortStorerDesc.java
+++ /dev/null
@@ -1,187 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.util.Iterator;
-import java.util.StringTokenizer;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.pig.TableStorer;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.ExecType;
-import org.apache.pig.PigServer;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.test.MiniCluster;
-import org.junit.After;
-import org.junit.AfterClass;
-import org.junit.Before;
-import org.junit.BeforeClass;
-import org.junit.Test;
-import org.apache.hadoop.zebra.BaseTestCase;
-
-
-/**
- * Note:
- * 
- * Make sure you add the build/pig-0.1.0-dev-core.jar to the Classpath of the
- * app/debug configuration, when run this from inside the Eclipse.
- * 
- */
-public class TestTableSortStorerDesc extends BaseTestCase {
-  private static Path pathTable;
-  
-  @BeforeClass
-  public static void setUp() throws Exception {
-
-    init();
-    pathTable = getTableFullPath("TestTableSortStorerDesc");
-    removeDir(pathTable);
-    
-    BasicTable.Writer writer = new BasicTable.Writer(pathTable,
-        "SF_a:string,SF_b:string,SF_c:string,SF_d:string,SF_e:string,SF_f:string,SF_g:string",
-        "[SF_a, SF_b, SF_c]; [SF_e, SF_f, SF_g]", conf);
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    final int numsBatch = 10;
-    final int numsInserters = 1;
-    TableInserter[] inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tuple);
-        for (int k = 0; k < tuple.size(); ++k) {
-          try {
-            tuple.set(k, (9-b) + "_" + i + "" + k);
-          } catch (ExecException e) {
-            e.printStackTrace();
-          }
-        }
-        inserters[i].insert(new BytesWritable(("key" + i).getBytes()), tuple);
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDown() throws Exception {
-    pigServer.shutdown();
-  }
-
-  /**
-   * Return the name of the routine that called getCurrentMethodName
-   * 
-   */
-  public String getCurrentMethodName() {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    PrintWriter pw = new PrintWriter(baos);
-    (new Throwable()).printStackTrace(pw);
-    pw.flush();
-    String stackTrace = baos.toString();
-    pw.close();
-
-    StringTokenizer tok = new StringTokenizer(stackTrace, "\n");
-    tok.nextToken(); // 'java.lang.Throwable'
-    tok.nextToken(); // 'at ...getCurrentMethodName'
-    String l = tok.nextToken(); // 'at ...<caller to getCurrentRoutine>'
-    // Parse line 3
-    tok = new StringTokenizer(l.trim(), " <(");
-    String t = tok.nextToken(); // 'at'
-    t = tok.nextToken(); // '...<caller to getCurrentRoutine>'
-    return t;
-  }
-
-  @Test
-  public void testStorer() throws ExecException, IOException {
-    /*
-     * Use pig LOAD to load testing data for store
-     */
-    String query = "records = LOAD '" + pathTable.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    pigServer.registerQuery(query);
-
-    Iterator<Tuple> it2 = pigServer.openIterator("records");
-    int row0 = 0;
-    Tuple RowValue2 = null;
-    while (it2.hasNext()) {
-      // Last row value
-      RowValue2 = it2.next();
-      row0++;
-      if (row0 == 10) {
-        Assert.assertEquals("0_01", RowValue2.get(1));
-        Assert.assertEquals("0_00", RowValue2.get(0).toString());
-      }
-    }
-    Assert.assertEquals(10, row0);
-
-    String orderby = "srecs = ORDER records BY SF_a DESC;";
-    pigServer.registerQuery(orderby);
-
-    /*
-     * Use pig STORE to store testing data BasicTable.Writer writer = new
-     * BasicTable.Writer(pathTable, "SF_a,SF_b,SF_c,SF_d,SF_e,SF_f,SF_g",
-     * "[SF_a, SF_b, SF_c]; [SF_e, SF_f, SF_g]", false, conf);
-     */
-    Path newPath = getTableFullPath(getCurrentMethodName());
-    removeDir(newPath);
-    pigServer
-        .store(
-            "srecs",
-            newPath.toString(),
-            TableStorer.class.getCanonicalName()
-                + "('[SF_a, SF_b, SF_c]; [SF_e]')");
-
-    // check new table content
-    String query3 = "newRecords = LOAD '"
-        + newPath.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('SF_a, SF_b');";
-    pigServer.registerQuery(query3);
-
-    Iterator<Tuple> it3 = pigServer.openIterator("newRecords");
-    int row = 0;
-    Tuple RowValue3 = null;
-    while (it3.hasNext()) {
-      // Last row value
-      RowValue3 = it3.next();
-      row++;
-    }
-    Assert.assertEquals(10, row);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestTableStorer.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestTableStorer.java
deleted file mode 100644
index cc5ac4302..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestTableStorer.java
+++ /dev/null
@@ -1,135 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.File;
-import java.io.IOException;
-import java.util.Iterator;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.pig.TableStorer;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.ExecType;
-import org.apache.pig.PigServer;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.executionengine.ExecJob;
-import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.test.MiniCluster;
-import org.junit.After;
-import org.junit.AfterClass;
-import org.junit.Before;
-import org.junit.BeforeClass;
-import org.junit.Test;
-import org.apache.hadoop.zebra.BaseTestCase;
-
-/**
- * Note:
- * 
- * Make sure you add the build/pig-0.1.0-dev-core.jar to the Classpath of the
- * app/debug configuration, when run this from inside the Eclipse.
- * 
- */
-public class TestTableStorer extends BaseTestCase {
-  private static Path pathTable;
-
-  @BeforeClass
-  public static void setUp() throws Exception {
-
-    init();
-    pathTable = getTableFullPath("TestTableStorer");
-    removeDir(pathTable);
-
-
-    BasicTable.Writer writer = new BasicTable.Writer(pathTable,
-        "SF_a:string,SF_b:string,SF_c:string,SF_d:string,SF_e:string,SF_f:string,SF_g:string",
-        "[SF_a, SF_b, SF_c]; [SF_e, SF_f, SF_g]", conf);
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    final int numsBatch = 10;
-    final int numsInserters = 2;
-    TableInserter[] inserters = new TableInserter[numsInserters];
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i] = writer.getInserter("ins" + i, false);
-    }
-
-    for (int b = 0; b < numsBatch; b++) {
-      for (int i = 0; i < numsInserters; i++) {
-        TypesUtils.resetTuple(tuple);
-        for (int k = 0; k < tuple.size(); ++k) {
-          try {
-            tuple.set(k, b + "_" + i + "" + k);
-          } catch (ExecException e) {
-            e.printStackTrace();
-          }
-        }
-        inserters[i].insert(new BytesWritable(("key" + i).getBytes()), tuple);
-      }
-    }
-    for (int i = 0; i < numsInserters; i++) {
-      inserters[i].close();
-    }
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDown() throws Exception {
-    pigServer.shutdown();
-  }
-
-  @Test
-  public void testStorer() throws ExecException, IOException {
-    /*
-     * Use pig LOAD to load testing data for store
-     */
-    String query = "records = LOAD '" + pathTable.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader();";
-    pigServer.registerQuery(query);
-
-    Iterator<Tuple> it = pigServer.openIterator("records");
-    while (it.hasNext()) {
-      Tuple cur = it.next();
-      System.out.println(cur);
-    }
-
-    /*
-     * Use pig STORE to store testing data BasicTable.Writer writer = new
-     * BasicTable.Writer(pathTable, "SF_a,SF_b,SF_c,SF_d,SF_e,SF_f,SF_g",
-     * "[SF_a, SF_b, SF_c]; [SF_e, SF_f, SF_g]", false, conf);
-     */
-    ExecJob pigJob = pigServer
-        .store(
-            "records",
-            new Path(pathTable, "store").toString(),
-            TableStorer.class.getCanonicalName()
-                + "('[SF_a, SF_b, SF_c]; [SF_e]')");
-
-    Assert.assertNull(pigJob.getException());
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestUnionMixedTypes.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestUnionMixedTypes.java
deleted file mode 100644
index 03fd48249..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/pig/TestUnionMixedTypes.java
+++ /dev/null
@@ -1,433 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.pig;
-
-import java.io.File;
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.io.TableScanner;
-import org.apache.hadoop.zebra.io.BasicTable.Reader.RangeSplit;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.ExecType;
-import org.apache.pig.PigServer;
-import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.apache.pig.test.MiniCluster;
-import org.junit.After;
-import org.junit.AfterClass;
-import org.junit.Before;
-import org.junit.BeforeClass;
-import org.junit.Test;
-import org.apache.hadoop.zebra.BaseTestCase;
-
-
-/**
- * Note:
- * 
- * Make sure you add the build/pig-0.1.0-dev-core.jar to the Classpath of the
- * app/debug configuration, when run this from inside the Eclipse.
- * 
- */
-public class TestUnionMixedTypes extends BaseTestCase {
-  private static Path pathWorking, pathTable1, pathTable2;
-  final static String STR_SCHEMA1 = "a:collection(record(a:string, b:string)),b:map(string),c:record(f1:string, f2:string),d:string";
-  final static String STR_STORAGE1 = "[a,d];[b#{k1|k2}];[c]";
-  final static String STR_SCHEMA2 = "a:collection(record(a:string, b:string)),b:map(string),c:record(f1:string, f2:string),e:string";
-  final static String STR_STORAGE2 = "[a,e];[b#{k1}];[c.f1]";
-
-  @BeforeClass
-  public static void setUpOnce() throws Exception {
-
-    init();
-    pathTable1 = getTableFullPath("TestUnionMixedTypes1");
-    pathTable2 = getTableFullPath("TestUnionMixedTypes2");
-    removeDir(pathTable1);
-    removeDir(pathTable2);
-
-
-    /*
-     * create 1st basic table;
-     */
-
-    BasicTable.Writer writer = new BasicTable.Writer(pathTable1, STR_SCHEMA1,
-        STR_STORAGE1, conf);
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-
-    BasicTable.Writer writer1 = new BasicTable.Writer(pathTable1, conf);
-    int part = 0;
-    TableInserter inserter = writer1.getInserter("part" + part, true);
-
-    TypesUtils.resetTuple(tuple);
-    DataBag bag1 = TypesUtils.createBag();
-    Schema schColl = schema.getColumn(0).getSchema().getColumn(0).getSchema();
-    Tuple tupColl1 = TypesUtils.createTuple(schColl);
-    Tuple tupColl2 = TypesUtils.createTuple(schColl);
-
-    int row = 0;
-    tupColl1.set(0, "1.1");
-    tupColl1.set(1, "1.11");
-    bag1.add(tupColl1);
-    tupColl2.set(0, "1.111");
-    tupColl2.set(1, "1.1111");
-    bag1.add(tupColl2);
-    tuple.set(0, bag1);
-
-    Map<String, String> m1 = new HashMap<String, String>();
-    m1.put("k1", "k11");
-    m1.put("b", "b1");
-    m1.put("c", "c1");
-    tuple.set(1, m1);
-
-    Tuple tupRecord1;
-    try {
-      tupRecord1 = TypesUtils.createTuple(schema.getColumnSchema("c")
-          .getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    tupRecord1.set(0, "1");
-    tupRecord1.set(1, "hello1");
-    tuple.set(2, tupRecord1);
-    tuple.set(3, "world1");
-
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // second row
-    row++;
-    TypesUtils.resetTuple(tuple);
-    TypesUtils.resetTuple(tupRecord1);
-    TypesUtils.resetTuple(tupColl1);
-    TypesUtils.resetTuple(tupColl2);
-    m1.clear();
-    bag1.clear();
-
-    row++;
-    tupColl1.set(0, "2.2");
-    tupColl1.set(1, "2.22");
-    bag1.add(tupColl1);
-    tupColl2.set(0, "2.222");
-    tupColl2.set(1, "2.2222");
-    bag1.add(tupColl2);
-    tuple.set(0, bag1);
-
-    m1.put("k2", "k22");
-    m1.put("k3", "k32");
-    m1.put("k1", "k12");
-    m1.put("k4", "k42");
-    tuple.set(1, m1);
-
-    tupRecord1.set(0, "2");
-    tupRecord1.set(1, "hello2");
-    tuple.set(2, tupRecord1);
-    tuple.set(3, "world2");
-
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-    inserter.close();
-    writer1.finish();
-    writer.close();
-
-    /*
-     * create 2nd basic table;
-     */
-
-    BasicTable.Writer writer2 = new BasicTable.Writer(pathTable2, STR_SCHEMA2,
-        STR_STORAGE2, conf);
-    Schema schema2 = writer.getSchema();
-
-    Tuple tuple2 = TypesUtils.createTuple(schema2);
-
-    BasicTable.Writer writer22 = new BasicTable.Writer(pathTable2, conf);
-    part = 0;
-    TableInserter inserter2 = writer22.getInserter("part" + part, true);
-
-    TypesUtils.resetTuple(tuple2);
-    TypesUtils.resetTuple(tuple);
-    TypesUtils.resetTuple(tupRecord1);
-    TypesUtils.resetTuple(tupColl1);
-    TypesUtils.resetTuple(tupColl2);
-    m1.clear();
-    bag1.clear();
-
-    row = 0;
-    tupColl1.set(0, "3.3");
-    tupColl1.set(1, "3.33");
-    bag1.add(tupColl1);
-    tupColl2.set(0, "3.333");
-    tupColl2.set(1, "3.3333");
-    bag1.add(tupColl2);
-    tuple2.set(0, bag1);
-
-    m1.put("k1", "k13");
-    m1.put("b", "b3");
-    m1.put("c", "c3");
-    tuple2.set(1, m1);
-
-    tupRecord1.set(0, "3");
-    tupRecord1.set(1, "hello3");
-    tuple2.set(2, tupRecord1);
-    tuple2.set(3, "world13");
-
-    inserter2.insert(new BytesWritable(String
-        .format("k%d%d", part + 1, row + 1).getBytes()), tuple2);
-
-    // second row
-    row++;
-    TypesUtils.resetTuple(tuple2);
-    TypesUtils.resetTuple(tupRecord1);
-    TypesUtils.resetTuple(tupColl1);
-    TypesUtils.resetTuple(tupColl2);
-    bag1.clear();
-    m1.clear();
-
-    row++;
-    tupColl1.set(0, "4.4");
-    tupColl1.set(1, "4.44");
-    bag1.add(tupColl1);
-    tupColl2.set(0, "4.444");
-    tupColl2.set(1, "4.4444");
-    bag1.add(tupColl2);
-    tuple2.set(0, bag1);
-
-    m1.put("k2", "k24");
-    m1.put("k3", "k34");
-    m1.put("k1", "k14");
-    m1.put("k4", "k44");
-    tuple2.set(1, m1);
-
-    tupRecord1.set(0, "4");
-    tupRecord1.set(1, "hello4");
-    tuple2.set(2, tupRecord1);
-    tuple2.set(3, "world4");
-
-    inserter2.insert(new BytesWritable(String
-        .format("k%d%d", part + 1, row + 1).getBytes()), tuple2);
-    inserter2.close();
-    writer2.finish();
-    writer22.close();
-
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws Exception {
-    pigServer.shutdown();
-  }
-
-  @Test
-  // all fields
-  public void testReader1() throws ExecException, IOException {
-    String str1 = pathTable1.toString();
-    String str2 = pathTable2.toString();
-
-    
-    String query = "records = LOAD '"
-        + str1
-        + ","
-        + str2
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('a,b#{k1|k2},c.f1');";
-    System.out.println(query);
-
-    pigServer.registerQuery(query);
-    Iterator<Tuple> it = pigServer.openIterator("records");
-
-    Tuple cur = null;
-    int i = 0;
-    int j = 0;
-    // total 4 lines
-    while (it.hasNext()) {
-      cur = it.next();
-
-      i++;
-      System.out.println(" line : " + i + " : " + cur.toString());
-      /*
-       * line : 1 : ({(3.3,3.33),(3.333,3.3333)},[k1#k13,k2#],3) line : 2 :
-       * ({(4,4,4.44),(4.444,4,4444),(4,4,4.44),(4.444,4,4444)},[k1#k14,k2
-       * #k24],4)
-       */
-      // first line
-      Iterator<Tuple> it2 = ((DataBag) cur.get(0)).iterator();
-      while (it2.hasNext()) {
-
-        Tuple cur2 = it2.next();
-        j++;
-
-        if (j == 1) {
-          System.out.println("j is : " + j);
-          /* The order of t1 and t2 in the union result can vary across runs. */
-          Assert.assertTrue("1.1".equals(cur2.get(0)) || "3.3".equals(cur2.get(0)));
-          Assert.assertTrue("1.11".equals(cur2.get(1)) || "3.33".equals(cur2.get(1)));
-        }
-        if (j == 2) {
-          System.out.println("j is : " + j);
-
-          Assert.assertTrue("1.111".equals(cur2.get(0)) || "3.333".equals(cur2.get(0)));
-          Assert.assertTrue("1.1111".equals(cur2.get(1)) || "3.3333".equals(cur2.get(1)));
-        }
-
-        TypesUtils.resetTuple(cur2);
-
-      }// inner while
-      if (i == 1) {
-        System.out.println("i is : " + i);
-        Assert.assertTrue("k11".equals(((Map) cur.get(1)).get("k1")) || "k13".equals(((Map) cur.get(1)).get("k1")));
-        Assert.assertEquals(null, ((Map) cur.get(1)).get("k2"));
-        Assert.assertTrue("1".equals(cur.get(2)) || "3".equals(cur.get(2)));
-      }
-
-      if (i == 2) {
-        System.out.println("i should see this line. ");
-        Assert.assertTrue("k12".equals(((Map) cur.get(1)).get("k1")) || "k14".equals(((Map) cur.get(1)).get("k1")));
-        Assert.assertTrue("k22".equals(((Map) cur.get(1)).get("k2")) || "k24".equals(((Map) cur.get(1)).get("k2")));
-        Assert.assertTrue("2".equals(cur.get(2)) || "4".equals(cur.get(2)));
-      }
-      if (i == 3) {
-        System.out.println("i is : " + i);
-
-        Assert.assertTrue("k11".equals(((Map) cur.get(1)).get("k1")) || "k13".equals(((Map) cur.get(1)).get("k1")));
-        Assert.assertEquals(null, ((Map) cur.get(1)).get("k2"));
-        Assert.assertTrue("1".equals(cur.get(2)) || "3".equals(cur.get(2)));
-      }
-
-      if (i == 4) {
-        System.out.println("i should see this line. ");
-        Assert.assertTrue("k12".equals(((Map) cur.get(1)).get("k1")) || "k14".equals(((Map) cur.get(1)).get("k1")));
-        Assert.assertTrue("k22".equals(((Map) cur.get(1)).get("k2")) || "k24".equals(((Map) cur.get(1)).get("k2")));
-        Assert.assertTrue("2".equals(cur.get(2)) || "4".equals(cur.get(2)));
-      }
-    }// outer while
-
-    Assert.assertEquals(4, i);
-  }
-
-
-  @Test
-  // one common field only
-  public void testReader2() throws ExecException, IOException {
-    String query = "records = LOAD '" + pathTable1.toString() + "," + pathTable2.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('b#{k1}');";
-    System.out.println(query);
-
-    pigServer.registerQuery(query);
-    Iterator<Tuple> it = pigServer.openIterator("records");
-
-    Tuple cur = null;
-    String[] expected1 = new String[]{"k11", "k12", "k13", "k14" };
-    String[] expected2 = new String[]{"k13", "k14", "k11", "k12" };
-    int i = 0;
-    // total 4 lines
-    while (it.hasNext()) {
-      cur = it.next();
-      String key = ((Map<String, String>) cur.get(0)).get("k1");
-      Assert.assertTrue( expected1[i].equals( key ) || expected2[i].equals( key ) );
-      i++;
-      System.out.println(" line : " + i + " : " + cur.toString());
-
-    }
-    Assert.assertEquals(4, i);
-  }
-
-  @Test
-  // one field which exists in one table only
-  public void testReader3() throws ExecException, IOException {
-    String query = "records = LOAD '" + pathTable1.toString() + "," + pathTable2.toString()
-        + "' USING org.apache.hadoop.zebra.pig.TableLoader('d');";
-    System.out.println(query);
-
-    pigServer.registerQuery(query);
-    Iterator<Tuple> it = pigServer.openIterator("records");
-
-    Tuple cur = null;
-    int i = 0;
-    while (it.hasNext()) {
-      cur = it.next();
-
-      i++;
-      System.out.println(" line : " + i + " : " + cur.toString());
-      
-      
-      if (i == 1) {
-        System.out.println("i is : " + i);
-
-        Assert.assertTrue("world1".equals(cur.get(0)) || cur.get(0) == null);
-        try {
-          cur.get(1);
-          Assert.fail("should throw index out of bound exception");
-        } catch (Exception e) {
-          e.printStackTrace();
-        }
-      }
-
-      if (i == 2) {
-        Assert.assertTrue("world2".equals(cur.get(0)) || cur.get(0) == null);
-        try {
-          cur.get(1);
-          Assert.fail("should throw index out of bound exception");
-        } catch (Exception e) {
-          e.printStackTrace();
-        }
-      }
-      if (i == 3) {
-
-        Assert.assertTrue("world1".equals(cur.get(0)) || cur.get(0) == null);
-        try {
-          cur.get(1);
-          Assert.fail("should throw index out of bound exception");
-        } catch (Exception e) {
-          e.printStackTrace();
-        }
-      }
-
-      if (i == 4) {
-
-        Assert.assertTrue("world2".equals(cur.get(0)) || cur.get(0) == null);
-        try {
-          cur.get(1);
-          Assert.fail("should throw index out of bound exception");
-        } catch (Exception e) {
-          e.printStackTrace();
-        }
-      }
-    }// outer while
-
-    Assert.assertEquals(4, i);
-  }
-
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/KVGenerator.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/KVGenerator.java
deleted file mode 100644
index 4fa7a24eb..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/KVGenerator.java
+++ /dev/null
@@ -1,105 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.tfile;
-
-import java.util.Random;
-
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.WritableComparator;
-import org.apache.hadoop.zebra.tfile.RandomDistribution.DiscreteRNG;
-
-/**
- * Generate random <key, value> pairs.
- */
-class KVGenerator {
-  private final Random random;
-  private final byte[][] dict;
-  private final boolean sorted;
-  private final DiscreteRNG keyLenRNG, valLenRNG;
-  private BytesWritable lastKey;
-  private static final int MIN_KEY_LEN = 4;
-  private final byte prefix[] = new byte[MIN_KEY_LEN];
-
-  public KVGenerator(Random random, boolean sorted, DiscreteRNG keyLenRNG,
-      DiscreteRNG valLenRNG, DiscreteRNG wordLenRNG, int dictSize) {
-    this.random = random;
-    dict = new byte[dictSize][];
-    this.sorted = sorted;
-    this.keyLenRNG = keyLenRNG;
-    this.valLenRNG = valLenRNG;
-    for (int i = 0; i < dictSize; ++i) {
-      int wordLen = wordLenRNG.nextInt();
-      dict[i] = new byte[wordLen];
-      random.nextBytes(dict[i]);
-    }
-    lastKey = new BytesWritable();
-    fillKey(lastKey);
-  }
-  
-  private void fillKey(BytesWritable o) {
-    int len = keyLenRNG.nextInt();
-    if (len < MIN_KEY_LEN) len = MIN_KEY_LEN;
-    o.setSize(len);
-    int n = MIN_KEY_LEN;
-    while (n < len) {
-      byte[] word = dict[random.nextInt(dict.length)];
-      int l = Math.min(word.length, len - n);
-      System.arraycopy(word, 0, o.get(), n, l);
-      n += l;
-    }
-    if (sorted
-        && WritableComparator.compareBytes(lastKey.get(), MIN_KEY_LEN, lastKey
-            .getSize()
-            - MIN_KEY_LEN, o.get(), MIN_KEY_LEN, o.getSize() - MIN_KEY_LEN) > 0) {
-      incrementPrefix();
-    }
-
-    System.arraycopy(prefix, 0, o.get(), 0, MIN_KEY_LEN);
-    lastKey.set(o);
-  }
-
-  private void fillValue(BytesWritable o) {
-    int len = valLenRNG.nextInt();
-    o.setSize(len);
-    int n = 0;
-    while (n < len) {
-      byte[] word = dict[random.nextInt(dict.length)];
-      int l = Math.min(word.length, len - n);
-      System.arraycopy(word, 0, o.get(), n, l);
-      n += l;
-    }
-  }
-  
-  private void incrementPrefix() {
-    for (int i = MIN_KEY_LEN - 1; i >= 0; --i) {
-      ++prefix[i];
-      if (prefix[i] != 0) return;
-    }
-    
-    throw new RuntimeException("Prefix overflown");
-  }
-  
-  public void next(BytesWritable key, BytesWritable value, boolean dupKey) {
-    if (dupKey) {
-      key.set(lastKey);
-    }
-    else {
-      fillKey(key);
-    }
-    fillValue(value);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/KeySampler.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/KeySampler.java
deleted file mode 100644
index 38cfdc7e6..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/KeySampler.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.tfile;
-
-import java.io.IOException;
-import java.util.Random;
-
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.tfile.RandomDistribution.DiscreteRNG;
-
-class KeySampler {
-  Random random;
-  int min, max;
-  DiscreteRNG keyLenRNG;
-  private static final int MIN_KEY_LEN = 4;
-
-  public KeySampler(Random random, RawComparable first, RawComparable last,
-      DiscreteRNG keyLenRNG) throws IOException {
-    this.random = random;
-    min = keyPrefixToInt(first);
-    max = keyPrefixToInt(last);
-    this.keyLenRNG = keyLenRNG;
-  }
-
-  private int keyPrefixToInt(RawComparable key) throws IOException {
-    byte[] b = key.buffer();
-    int o = key.offset();
-    return (b[o] & 0xff) << 24 | (b[o + 1] & 0xff) << 16
-        | (b[o + 2] & 0xff) << 8 | (b[o + 3] & 0xff);
-  }
-  
-  public void next(BytesWritable key) {
-    key.setSize(Math.max(MIN_KEY_LEN, keyLenRNG.nextInt()));
-    random.nextBytes(key.get());
-    int n = random.nextInt(max - min) + min;
-    byte[] b = key.get();
-    b[0] = (byte) (n >> 24);
-    b[1] = (byte) (n >> 16);
-    b[2] = (byte) (n >> 8);
-    b[3] = (byte) n;
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/NanoTimer.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/NanoTimer.java
deleted file mode 100644
index 8edf304c4..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/NanoTimer.java
+++ /dev/null
@@ -1,193 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.tfile;
-
-/**
- * A nano-second timer.
- */
-public class NanoTimer {
-  private long last = -1;
-  private boolean started = false;
-  private long cumulate = 0;
-
-  /**
-   * Constructor
-   * 
-   * @param start
-   *          Start the timer upon construction.
-   */
-  public NanoTimer(boolean start) {
-    if (start) this.start();
-  }
-
-  /**
-   * Start the timer.
-   * 
-   * Note: No effect if timer is already started.
-   */
-  public void start() {
-    if (!this.started) {
-      this.last = System.nanoTime();
-      this.started = true;
-    }
-  }
-
-  /**
-   * Stop the timer.
-   * 
-   * Note: No effect if timer is already stopped.
-   */
-  public void stop() {
-    if (this.started) {
-      this.started = false;
-      this.cumulate += System.nanoTime() - this.last;
-    }
-  }
-
-  /**
-   * Read the timer.
-   * 
-   * @return the elapsed time in nano-seconds. Note: If the timer is never
-   *         started before, -1 is returned.
-   */
-  public long read() {
-    if (!readable()) return -1;
-
-    return this.cumulate;
-  }
-
-  /**
-   * Reset the timer.
-   */
-  public void reset() {
-    this.last = -1;
-    this.started = false;
-    this.cumulate = 0;
-  }
-
-  /**
-   * Checking whether the timer is started
-   * 
-   * @return true if timer is started.
-   */
-  public boolean isStarted() {
-    return this.started;
-  }
-
-  /**
-   * Format the elapsed time to a human understandable string.
-   * 
-   * Note: If timer is never started, "ERR" will be returned.
-   */
-  public String toString() {
-    if (!readable()) {
-      return "ERR";
-    }
-
-    return NanoTimer.nanoTimeToString(this.cumulate);
-  }
-
-  /**
-   * A utility method to format a time duration in nano seconds into a human
-   * understandable stirng.
-   * 
-   * @param t
-   *          Time duration in nano seconds.
-   * @return String representation.
-   */
-  public static String nanoTimeToString(long t) {
-    if (t < 0) return "ERR";
-
-    if (t == 0) return "0";
-
-    if (t < 1000) {
-      return t + "ns";
-    }
-
-    double us = (double) t / 1000;
-    if (us < 1000) {
-      return String.format("%.2fus", us);
-    }
-
-    double ms = us / 1000;
-    if (ms < 1000) {
-      return String.format("%.2fms", ms);
-    }
-
-    double ss = ms / 1000;
-    if (ss < 1000) {
-      return String.format("%.2fs", ss);
-    }
-
-    long mm = (long) ss / 60;
-    ss -= mm * 60;
-    long hh = mm / 60;
-    mm -= hh * 60;
-    long dd = hh / 24;
-    hh -= dd * 24;
-
-    if (dd > 0) {
-      return String.format("%dd%dh", dd, hh);
-    }
-
-    if (hh > 0) {
-      return String.format("%dh%dm", hh, mm);
-    }
-
-    if (mm > 0) {
-      return String.format("%dm%.1fs", mm, ss);
-    }
-
-    return String.format("%.2fs", ss);
-
-    /**
-     * StringBuilder sb = new StringBuilder(); String sep = "";
-     * 
-     * if (dd > 0) { String unit = (dd > 1) ? "days" : "day";
-     * sb.append(String.format("%s%d%s", sep, dd, unit)); sep = " "; }
-     * 
-     * if (hh > 0) { String unit = (hh > 1) ? "hrs" : "hr";
-     * sb.append(String.format("%s%d%s", sep, hh, unit)); sep = " "; }
-     * 
-     * if (mm > 0) { String unit = (mm > 1) ? "mins" : "min";
-     * sb.append(String.format("%s%d%s", sep, mm, unit)); sep = " "; }
-     * 
-     * if (ss > 0) { String unit = (ss > 1) ? "secs" : "sec";
-     * sb.append(String.format("%s%.3f%s", sep, ss, unit)); sep = " "; }
-     * 
-     * return sb.toString();
-     */
-  }
-
-  private boolean readable() {
-    return this.last != -1;
-  }
-
-  /**
-   * Simple tester.
-   * 
-   * @param args
-   */
-  public static void main(String[] args) {
-    long i = 7;
-
-    for (int x = 0; x < 20; ++x, i *= 7) {
-      System.out.println(NanoTimer.nanoTimeToString(i));
-    }
-  }
-}
-
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/RandomDistribution.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/RandomDistribution.java
deleted file mode 100644
index afbfca253..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/RandomDistribution.java
+++ /dev/null
@@ -1,266 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.tfile;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.Random;
-
-/**
- * A class that generates random numbers that follow some distribution.
- */
-public class RandomDistribution {
-  /**
-   * Interface for discrete (integer) random distributions.
-   */
-  public static interface DiscreteRNG {
-    /**
-     * Get the next random number
-     * 
-     * @return the next random number.
-     */
-    public int nextInt();
-  }
-
-  /**
-   * P(i)=1/(max-min)
-   */
-  public static final class Flat implements DiscreteRNG {
-    private final Random random;
-    private final int min;
-    private final int max;
-
-    /**
-     * Generate random integers from min (inclusive) to max (exclusive)
-     * following even distribution.
-     * 
-     * @param random
-     *          The basic random number generator.
-     * @param min
-     *          Minimum integer
-     * @param max
-     *          maximum integer (exclusive).
-     * 
-     */
-    public Flat(Random random, int min, int max) {
-      if (min >= max) {
-        throw new IllegalArgumentException("Invalid range");
-      }
-      this.random = random;
-      this.min = min;
-      this.max = max;
-    }
-    
-    /**
-     * @see DiscreteRNG#nextInt()
-     */
-    @Override
-    public int nextInt() {
-      return random.nextInt(max - min) + min;
-    }
-  }
-
-  /**
-   * Zipf distribution. The ratio of the probabilities of integer i and j is
-   * defined as follows:
-   * 
-   * P(i)/P(j)=((j-min+1)/(i-min+1))^sigma.
-   */
-  public static final class Zipf implements DiscreteRNG {
-    private static final double DEFAULT_EPSILON = 0.001;
-    private final Random random;
-    private final ArrayList<Integer> k;
-    private final ArrayList<Double> v;
-
-    /**
-     * Constructor
-     * 
-     * @param r
-     *          The random number generator.
-     * @param min
-     *          minimum integer (inclusvie)
-     * @param max
-     *          maximum integer (exclusive)
-     * @param sigma
-     *          parameter sigma. (sigma > 1.0)
-     */
-    public Zipf(Random r, int min, int max, double sigma) {
-      this(r, min, max, sigma, DEFAULT_EPSILON);
-    }
-
-    /**
-     * Constructor.
-     * 
-     * @param r
-     *          The random number generator.
-     * @param min
-     *          minimum integer (inclusvie)
-     * @param max
-     *          maximum integer (exclusive)
-     * @param sigma
-     *          parameter sigma. (sigma > 1.0)
-     * @param epsilon
-     *          Allowable error percentage (0 < epsilon < 1.0).
-     */
-    public Zipf(Random r, int min, int max, double sigma, double epsilon) {
-      if ((max <= min) || (sigma <= 1) || (epsilon <= 0)
-          || (epsilon >= 0.5)) {
-        throw new IllegalArgumentException("Invalid arguments");
-      }
-      random = r;
-      k = new ArrayList<Integer>();
-      v = new ArrayList<Double>();
-
-      double sum = 0;
-      int last = -1;
-      for (int i = min; i < max; ++i) {
-        sum += Math.exp(-sigma * Math.log(i - min + 1));
-        if ((last == -1) || i * (1 - epsilon) > last) {
-          k.add(i);
-          v.add(sum);
-          last = i;
-        }
-      }
-
-      if (last != max - 1) {
-        k.add(max - 1);
-        v.add(sum);
-      }
-
-      v.set(v.size() - 1, 1.0);
-
-      for (int i = v.size() - 2; i >= 0; --i) {
-        v.set(i, v.get(i) / sum);
-      }
-    }
-
-    /**
-     * @see DiscreteRNG#nextInt()
-     */
-    @Override
-    public int nextInt() {
-      double d = random.nextDouble();
-      int idx = Collections.binarySearch(v, d);
-
-      if (idx > 0) {
-        ++idx;
-      }
-      else {
-        idx = -(idx + 1);
-      }
-
-      if (idx >= v.size()) {
-        idx = v.size() - 1;
-      }
-
-      if (idx == 0) {
-        return k.get(0);
-      }
-
-      int ceiling = k.get(idx);
-      int lower = k.get(idx - 1);
-
-      return ceiling - random.nextInt(ceiling - lower);
-    }
-  }
-
-  /**
-   * Binomial distribution.
-   * 
-   * P(k)=select(n, k)*p^k*(1-p)^(n-k) (k = 0, 1, ..., n)
-   * 
-   * P(k)=select(max-min-1, k-min)*p^(k-min)*(1-p)^(k-min)*(1-p)^(max-k-1)
-   */
-  public static final class Binomial implements DiscreteRNG {
-    private final Random random;
-    private final int min;
-    private final int n;
-    private final double[] v;
-
-    private static double select(int n, int k) {
-      double ret = 1.0;
-      for (int i = k + 1; i <= n; ++i) {
-        ret *= (double) i / (i - k);
-      }
-      return ret;
-    }
-    
-    private static double power(double p, int k) {
-      return Math.exp(k * Math.log(p));
-    }
-
-    /**
-     * Generate random integers from min (inclusive) to max (exclusive)
-     * following Binomial distribution.
-     * 
-     * @param random
-     *          The basic random number generator.
-     * @param min
-     *          Minimum integer
-     * @param max
-     *          maximum integer (exclusive).
-     * @param p
-     *          parameter.
-     * 
-     */
-    public Binomial(Random random, int min, int max, double p) {
-      if (min >= max) {
-        throw new IllegalArgumentException("Invalid range");
-      }
-      this.random = random;
-      this.min = min;
-      this.n = max - min - 1;
-      if (n > 0) {
-        v = new double[n + 1];
-        double sum = 0.0;
-        for (int i = 0; i <= n; ++i) {
-          sum += select(n, i) * power(p, i) * power(1 - p, n - i);
-          v[i] = sum;
-        }
-        for (int i = 0; i <= n; ++i) {
-          v[i] /= sum;
-        }
-      }
-      else {
-        v = null;
-      }
-    }
-
-    /**
-     * @see DiscreteRNG#nextInt()
-     */
-    @Override
-    public int nextInt() {
-      if (v == null) {
-        return min;
-      }
-      double d = random.nextDouble();
-      int idx = Arrays.binarySearch(v, d);
-      if (idx > 0) {
-        ++idx;
-      } else {
-        idx = -(idx + 1);
-      }
-
-      if (idx >= v.length) {
-        idx = v.length - 1;
-      }
-      return idx + min;
-    }
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFile.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFile.java
deleted file mode 100644
index 5a839efef..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFile.java
+++ /dev/null
@@ -1,431 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-package org.apache.hadoop.zebra.tfile;
-
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.util.Arrays;
-
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.zebra.tfile.TFile.Reader;
-import org.apache.hadoop.zebra.tfile.TFile.Writer;
-import org.apache.hadoop.zebra.tfile.TFile.Reader.Scanner;
-
-/**
- * test tfile features.
- * 
- */
-public class TestTFile extends TestCase {
-  private static String ROOT =
-      System.getProperty("test.build.data", "/tmp/tfile-test");
-  private FileSystem fs;
-  private Configuration conf;
-  private static final int minBlockSize = 512;
-  private static final int largeVal = 3 * 1024 * 1024;
-  private static final String localFormatter = "%010d";
-
-  @Override
-  public void setUp() throws IOException {
-    conf = new Configuration();
-    fs = FileSystem.get(conf);
-  }
-
-  @Override
-  public void tearDown() throws IOException {
-    // do nothing
-  }
-
-  // read a key from the scanner
-  public byte[] readKey(Scanner scanner) throws IOException {
-    int keylen = scanner.entry().getKeyLength();
-    byte[] read = new byte[keylen];
-    scanner.entry().getKey(read);
-    return read;
-  }
-
-  // read a value from the scanner
-  public byte[] readValue(Scanner scanner) throws IOException {
-    int valueLen = scanner.entry().getValueLength();
-    byte[] read = new byte[valueLen];
-    scanner.entry().getValue(read);
-    return read;
-  }
-
-  // read a long value from the scanner
-  public byte[] readLongValue(Scanner scanner, int len) throws IOException {
-    DataInputStream din = scanner.entry().getValueStream();
-    byte[] b = new byte[len];
-    din.readFully(b);
-    din.close();
-    return b;
-  }
-
-  // write some records into the tfile
-  // write them twice
-  private int writeSomeRecords(Writer writer, int start, int n)
-      throws IOException {
-    String value = "value";
-    for (int i = start; i < (start + n); i++) {
-      String key = String.format(localFormatter, i);
-      writer.append(key.getBytes(), (value + key).getBytes());
-      writer.append(key.getBytes(), (value + key).getBytes());
-    }
-    return (start + n);
-  }
-
-  // read the records and check
-  private int readAndCheckbytes(Scanner scanner, int start, int n)
-      throws IOException {
-    String value = "value";
-    for (int i = start; i < (start + n); i++) {
-      byte[] key = readKey(scanner);
-      byte[] val = readValue(scanner);
-      String keyStr = String.format(localFormatter, i);
-      String valStr = value + keyStr;
-      assertTrue("btyes for keys do not match " + keyStr + " "
-          + new String(key), Arrays.equals(keyStr.getBytes(), key));
-      assertTrue("bytes for vals do not match " + valStr + " "
-          + new String(val), Arrays.equals(
-          valStr.getBytes(), val));
-      assertTrue(scanner.advance());
-      key = readKey(scanner);
-      val = readValue(scanner);
-      assertTrue("btyes for keys do not match", Arrays.equals(
-          keyStr.getBytes(), key));
-      assertTrue("bytes for vals do not match", Arrays.equals(
-          valStr.getBytes(), val));
-      assertTrue(scanner.advance());
-    }
-    return (start + n);
-  }
-
-  // write some large records
-  // write them twice
-  private int writeLargeRecords(Writer writer, int start, int n)
-      throws IOException {
-    byte[] value = new byte[largeVal];
-    for (int i = start; i < (start + n); i++) {
-      String key = String.format(localFormatter, i);
-      writer.append(key.getBytes(), value);
-      writer.append(key.getBytes(), value);
-    }
-    return (start + n);
-  }
-
-  // read large records
-  // read them twice since its duplicated
-  private int readLargeRecords(Scanner scanner, int start, int n)
-      throws IOException {
-    for (int i = start; i < (start + n); i++) {
-      byte[] key = readKey(scanner);
-      String keyStr = String.format(localFormatter, i);
-      assertTrue("btyes for keys do not match", Arrays.equals(
-          keyStr.getBytes(), key));
-      scanner.advance();
-      key = readKey(scanner);
-      assertTrue("btyes for keys do not match", Arrays.equals(
-          keyStr.getBytes(), key));
-      scanner.advance();
-    }
-    return (start + n);
-  }
-
-  // write empty keys and values
-  private void writeEmptyRecords(Writer writer, int n) throws IOException {
-    byte[] key = new byte[0];
-    byte[] value = new byte[0];
-    for (int i = 0; i < n; i++) {
-      writer.append(key, value);
-    }
-  }
-
-  // read empty keys and values
-  private void readEmptyRecords(Scanner scanner, int n) throws IOException {
-    byte[] key = new byte[0];
-    byte[] value = new byte[0];
-    byte[] readKey = null;
-    byte[] readValue = null;
-    for (int i = 0; i < n; i++) {
-      readKey = readKey(scanner);
-      readValue = readValue(scanner);
-      assertTrue("failed to match keys", Arrays.equals(readKey, key));
-      assertTrue("failed to match values", Arrays.equals(readValue, value));
-      assertTrue("failed to advance cursor", scanner.advance());
-    }
-  }
-
-  private int writePrepWithKnownLength(Writer writer, int start, int n)
-      throws IOException {
-    // get the length of the key
-    String key = String.format(localFormatter, start);
-    int keyLen = key.getBytes().length;
-    String value = "value" + key;
-    int valueLen = value.getBytes().length;
-    for (int i = start; i < (start + n); i++) {
-      DataOutputStream out = writer.prepareAppendKey(keyLen);
-      String localKey = String.format(localFormatter, i);
-      out.write(localKey.getBytes());
-      out.close();
-      out = writer.prepareAppendValue(valueLen);
-      String localValue = "value" + localKey;
-      out.write(localValue.getBytes());
-      out.close();
-    }
-    return (start + n);
-  }
-
-  private int readPrepWithKnownLength(Scanner scanner, int start, int n)
-      throws IOException {
-    for (int i = start; i < (start + n); i++) {
-      String key = String.format(localFormatter, i);
-      byte[] read = readKey(scanner);
-      assertTrue("keys not equal", Arrays.equals(key.getBytes(), read));
-      String value = "value" + key;
-      read = readValue(scanner);
-      assertTrue("values not equal", Arrays.equals(value.getBytes(), read));
-      scanner.advance();
-    }
-    return (start + n);
-  }
-
-  private int writePrepWithUnkownLength(Writer writer, int start, int n)
-      throws IOException {
-    for (int i = start; i < (start + n); i++) {
-      DataOutputStream out = writer.prepareAppendKey(-1);
-      String localKey = String.format(localFormatter, i);
-      out.write(localKey.getBytes());
-      out.close();
-      String value = "value" + localKey;
-      out = writer.prepareAppendValue(-1);
-      out.write(value.getBytes());
-      out.close();
-    }
-    return (start + n);
-  }
-
-  private int readPrepWithUnknownLength(Scanner scanner, int start, int n)
-      throws IOException {
-    for (int i = start; i < start; i++) {
-      String key = String.format(localFormatter, i);
-      byte[] read = readKey(scanner);
-      assertTrue("keys not equal", Arrays.equals(key.getBytes(), read));
-      try {
-        read = readValue(scanner);
-        assertTrue(false);
-      }
-      catch (IOException ie) {
-        // should have thrown exception
-      }
-      String value = "value" + key;
-      read = readLongValue(scanner, value.getBytes().length);
-      assertTrue("values nto equal", Arrays.equals(read, value.getBytes()));
-      scanner.advance();
-    }
-    return (start + n);
-  }
-
-  private byte[] getSomeKey(int rowId) {
-    return String.format(localFormatter, rowId).getBytes();
-  }
-
-  private void writeRecords(Writer writer) throws IOException {
-    writeEmptyRecords(writer, 10);
-    int ret = writeSomeRecords(writer, 0, 100);
-    ret = writeLargeRecords(writer, ret, 1);
-    ret = writePrepWithKnownLength(writer, ret, 40);
-    ret = writePrepWithUnkownLength(writer, ret, 50);
-    writer.close();
-  }
-
-  private void readAllRecords(Scanner scanner) throws IOException {
-    readEmptyRecords(scanner, 10);
-    int ret = readAndCheckbytes(scanner, 0, 100);
-    ret = readLargeRecords(scanner, ret, 1);
-    ret = readPrepWithKnownLength(scanner, ret, 40);
-    ret = readPrepWithUnknownLength(scanner, ret, 50);
-  }
-
-  private FSDataOutputStream createFSOutput(Path name) throws IOException {
-    if (fs.exists(name)) fs.delete(name, true);
-    FSDataOutputStream fout = fs.create(name);
-    return fout;
-  }
-
-  /**
-   * test none codecs
-   */
-  void basicWithSomeCodec(String codec) throws IOException {
-    Path ncTFile = new Path(ROOT, "basic.tfile");
-    FSDataOutputStream fout = createFSOutput(ncTFile);
-    Writer writer = new Writer(fout, minBlockSize, codec, "memcmp", conf);
-    writeRecords(writer);
-    fout.close();
-    FSDataInputStream fin = fs.open(ncTFile);
-    Reader reader =
-        new Reader(fs.open(ncTFile), fs.getFileStatus(ncTFile).getLen(), conf);
-
-    Scanner scanner = reader.createScanner();
-    readAllRecords(scanner);
-    scanner.seekTo(getSomeKey(50));
-    assertTrue("location lookup failed", scanner.seekTo(getSomeKey(50)));
-    // read the key and see if it matches
-    byte[] readKey = readKey(scanner);
-    assertTrue("seeked key does not match", Arrays.equals(getSomeKey(50),
-        readKey));
-
-    scanner.seekTo(new byte[0]);
-    byte[] val1 = readValue(scanner);
-    scanner.seekTo(new byte[0]);
-    byte[] val2 = readValue(scanner);
-    assertTrue(Arrays.equals(val1, val2));
-    
-    // check for lowerBound
-    scanner.lowerBound(getSomeKey(50));
-    assertTrue("locaton lookup failed", scanner.currentLocation
-        .compareTo(reader.end()) < 0);
-    readKey = readKey(scanner);
-    assertTrue("seeked key does not match", Arrays.equals(readKey,
-        getSomeKey(50)));
-
-    // check for upper bound
-    scanner.upperBound(getSomeKey(50));
-    assertTrue("location lookup failed", scanner.currentLocation
-        .compareTo(reader.end()) < 0);
-    readKey = readKey(scanner);
-    assertTrue("seeked key does not match", Arrays.equals(readKey,
-        getSomeKey(51)));
-
-    scanner.close();
-    // test for a range of scanner
-    scanner = reader.createScannerByKey(getSomeKey(10), getSomeKey(60));
-    readAndCheckbytes(scanner, 10, 50);
-    assertFalse(scanner.advance());
-    scanner.close();
-    reader.close();
-    fin.close();
-    fs.delete(ncTFile, true);
-  }
-
-  // unsorted with some codec
-  void unsortedWithSomeCodec(String codec) throws IOException {
-    Path uTfile = new Path(ROOT, "unsorted.tfile");
-    FSDataOutputStream fout = createFSOutput(uTfile);
-    Writer writer = new Writer(fout, minBlockSize, codec, null, conf);
-    writeRecords(writer);
-    writer.close();
-    fout.close();
-    FSDataInputStream fin = fs.open(uTfile);
-    Reader reader =
-        new Reader(fs.open(uTfile), fs.getFileStatus(uTfile).getLen(), conf);
-
-    Scanner scanner = reader.createScanner();
-    readAllRecords(scanner);
-    scanner.close();
-    reader.close();
-    fin.close();
-    fs.delete(uTfile, true);
-  }
-
-  public void testTFileFeatures() throws IOException {
-    basicWithSomeCodec("none");
-    basicWithSomeCodec("gz");
-  }
-
-  // test unsorted t files.
-  public void testUnsortedTFileFeatures() throws IOException {
-    unsortedWithSomeCodec("none");
-    unsortedWithSomeCodec("gz");
-  }
-
-  private void writeNumMetablocks(Writer writer, String compression, int n)
-      throws IOException {
-    for (int i = 0; i < n; i++) {
-      DataOutputStream dout =
-          writer.prepareMetaBlock("TfileMeta" + i, compression);
-      byte[] b = ("something to test" + i).getBytes();
-      dout.write(b);
-      dout.close();
-    }
-  }
-
-  private void someTestingWithMetaBlock(Writer writer, String compression)
-      throws IOException {
-    DataOutputStream dout = null;
-    writeNumMetablocks(writer, compression, 10);
-    try {
-      dout = writer.prepareMetaBlock("TfileMeta1", compression);
-      assertTrue(false);
-    }
-    catch (MetaBlockAlreadyExists me) {
-      // avoid this exception
-    }
-    dout = writer.prepareMetaBlock("TFileMeta100", compression);
-    dout.close();
-  }
-
-  private void readNumMetablocks(Reader reader, int n) throws IOException {
-    int len = ("something to test" + 0).getBytes().length;
-    for (int i = 0; i < n; i++) {
-      DataInputStream din = reader.getMetaBlock("TfileMeta" + i);
-      byte b[] = new byte[len];
-      din.readFully(b);
-      assertTrue("faield to match metadata", Arrays.equals(
-          ("something to test" + i).getBytes(), b));
-      din.close();
-    }
-  }
-
-  private void someReadingWithMetaBlock(Reader reader) throws IOException {
-    DataInputStream din = null;
-    readNumMetablocks(reader, 10);
-    try {
-      din = reader.getMetaBlock("NO ONE");
-      assertTrue(false);
-    }
-    catch (MetaBlockDoesNotExist me) {
-      // should catch
-    }
-    din = reader.getMetaBlock("TFileMeta100");
-    int read = din.read();
-    assertTrue("check for status", (read == -1));
-    din.close();
-  }
-
-  // test meta blocks for tfiles
-  public void testMetaBlocks() throws IOException {
-    Path mFile = new Path(ROOT, "meta.tfile");
-    FSDataOutputStream fout = createFSOutput(mFile);
-    Writer writer = new Writer(fout, minBlockSize, "none", null, conf);
-    someTestingWithMetaBlock(writer, "none");
-    writer.close();
-    fout.close();
-    FSDataInputStream fin = fs.open(mFile);
-    Reader reader = new Reader(fin, fs.getFileStatus(mFile).getLen(), conf);
-    someReadingWithMetaBlock(reader);
-    fs.delete(mFile, true);
-    reader.close();
-    fin.close();
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileByteArrays.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileByteArrays.java
deleted file mode 100644
index d1372adcc..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileByteArrays.java
+++ /dev/null
@@ -1,790 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-package org.apache.hadoop.zebra.tfile;
-
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.EOFException;
-import java.io.IOException;
-import java.util.Random;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.WritableUtils;
-import org.apache.hadoop.zebra.tfile.TFile.Reader;
-import org.apache.hadoop.zebra.tfile.TFile.Writer;
-import org.apache.hadoop.zebra.tfile.TFile.Reader.Location;
-import org.apache.hadoop.zebra.tfile.TFile.Reader.Scanner;
-
-/**
- * 
- * Byte arrays test case class using GZ compression codec, base class of none
- * and LZO compression classes.
- * 
- */
-public class TestTFileByteArrays extends TestCase {
-  private static String ROOT =
-      System.getProperty("test.build.data", "/tmp/tfile-test");
-  private final static int BLOCK_SIZE = 512;
-  private final static int BUF_SIZE = 64;
-  private final static int K = 1024;
-  protected boolean skip = false;
-
-  private static final String KEY = "key";
-  private static final String VALUE = "value";
-
-  private FileSystem fs;
-  private Configuration conf;
-  private Path path;
-  private FSDataOutputStream out;
-  private Writer writer;
-
-  private String compression = Compression.Algorithm.GZ.getName();
-  private String comparator = "memcmp";
-  private String outputFile = "TFileTestByteArrays";
-  /*
-   * pre-sampled numbers of records in one block, based on the given the
-   * generated key and value strings
-   */
-  // private int records1stBlock = 4314;
-  // private int records2ndBlock = 4108;
-  private int records1stBlock = 4480;
-  private int records2ndBlock = 4263;
-
-  public void init(String compression, String comparator, String outputFile,
-      int numRecords1stBlock, int numRecords2ndBlock) {
-    this.compression = compression;
-    this.comparator = comparator;
-    this.outputFile = outputFile;
-    this.records1stBlock = numRecords1stBlock;
-    this.records2ndBlock = numRecords2ndBlock;
-  }
-
-  @Override
-  public void setUp() throws IOException {
-    conf = new Configuration();
-    path = new Path(ROOT, outputFile);
-    fs = path.getFileSystem(conf);
-    out = fs.create(path);
-    writer = new Writer(out, BLOCK_SIZE, compression, comparator, conf);
-  }
-
-  @Override
-  public void tearDown() throws IOException {
-    if (!skip)
-    fs.delete(path, true);
-  }
-
-  public void testNoDataEntry() throws IOException {
-    if (skip) 
-      return;
-    closeOutput();
-
-    Reader reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);
-    Assert.assertTrue(reader.isSorted());
-    Scanner scanner = reader.createScanner();
-    Assert.assertTrue(scanner.atEnd());
-    scanner.close();
-    reader.close();
-  }
-
-  public void testOneDataEntry() throws IOException {
-    if (skip)
-      return;
-    writeRecords(1);
-    readRecords(1);
-
-    checkBlockIndex(1, 0, 0);
-    readValueBeforeKey(1, 0);
-    readKeyWithoutValue(1, 0);
-    readValueWithoutKey(1, 0);
-    readKeyManyTimes(1, 0);
-  }
-
-  public void testTwoDataEntries() throws IOException {
-    if (skip)
-      return;
-    writeRecords(2);
-    readRecords(2);
-  }
-
-  /**
-   * Fill up exactly one block.
-   * 
-   * @throws IOException
-   */
-  public void testOneBlock() throws IOException {
-    if (skip)
-      return;
-    // just under one block
-    writeRecords(records1stBlock);
-    readRecords(records1stBlock);
-    // last key should be in the first block (block 0)
-    checkBlockIndex(records1stBlock, records1stBlock - 1, 0);
-  }
-
-  /**
-   * One block plus one record.
-   * 
-   * @throws IOException
-   */
-  public void testOneBlockPlusOneEntry() throws IOException {
-    if (skip)
-      return;
-    writeRecords(records1stBlock + 1);
-    readRecords(records1stBlock + 1);
-    checkBlockIndex(records1stBlock + 1, records1stBlock - 1, 0);
-    checkBlockIndex(records1stBlock + 1, records1stBlock, 1);
-  }
-
-  public void testTwoBlocks() throws IOException {
-    if (skip)
-      return;
-    writeRecords(records1stBlock + 5);
-    readRecords(records1stBlock + 5);
-    checkBlockIndex(records1stBlock + 5, records1stBlock + 4, 1);
-  }
-
-  public void testThreeBlocks() throws IOException {
-    if (skip) 
-      return;
-    writeRecords(2 * records1stBlock + 5);
-    readRecords(2 * records1stBlock + 5);
-
-    checkBlockIndex(2 * records1stBlock + 5, 2 * records1stBlock + 4, 2);
-    // 1st key in file
-    readValueBeforeKey(2 * records1stBlock + 5, 0);
-    readKeyWithoutValue(2 * records1stBlock + 5, 0);
-    readValueWithoutKey(2 * records1stBlock + 5, 0);
-    readKeyManyTimes(2 * records1stBlock + 5, 0);
-    // last key in file
-    readValueBeforeKey(2 * records1stBlock + 5, 2 * records1stBlock + 4);
-    readKeyWithoutValue(2 * records1stBlock + 5, 2 * records1stBlock + 4);
-    readValueWithoutKey(2 * records1stBlock + 5, 2 * records1stBlock + 4);
-    readKeyManyTimes(2 * records1stBlock + 5, 2 * records1stBlock + 4);
-
-    // 1st key in mid block, verify block indexes then read
-    checkBlockIndex(2 * records1stBlock + 5, records1stBlock - 1, 0);
-    checkBlockIndex(2 * records1stBlock + 5, records1stBlock, 1);
-    readValueBeforeKey(2 * records1stBlock + 5, records1stBlock);
-    readKeyWithoutValue(2 * records1stBlock + 5, records1stBlock);
-    readValueWithoutKey(2 * records1stBlock + 5, records1stBlock);
-    readKeyManyTimes(2 * records1stBlock + 5, records1stBlock);
-
-    // last key in mid block, verify block indexes then read
-    checkBlockIndex(2 * records1stBlock + 5, records1stBlock + records2ndBlock
-        - 1, 1);
-    checkBlockIndex(2 * records1stBlock + 5, records1stBlock + records2ndBlock,
-        2);
-    readValueBeforeKey(2 * records1stBlock + 5, records1stBlock
-        + records2ndBlock - 1);
-    readKeyWithoutValue(2 * records1stBlock + 5, records1stBlock
-        + records2ndBlock - 1);
-    readValueWithoutKey(2 * records1stBlock + 5, records1stBlock
-        + records2ndBlock - 1);
-    readKeyManyTimes(2 * records1stBlock + 5, records1stBlock + records2ndBlock
-        - 1);
-
-    // mid in mid block
-    readValueBeforeKey(2 * records1stBlock + 5, records1stBlock + 10);
-    readKeyWithoutValue(2 * records1stBlock + 5, records1stBlock + 10);
-    readValueWithoutKey(2 * records1stBlock + 5, records1stBlock + 10);
-    readKeyManyTimes(2 * records1stBlock + 5, records1stBlock + 10);
-  }
-
-  Location locate(Scanner scanner, byte[] key) throws IOException {
-    if (scanner.seekTo(key) == true) {
-      return scanner.currentLocation;
-    }
-    return scanner.endLocation;
-  }
-  
-  public void testLocate() throws IOException {
-    if (skip)
-      return;
-    writeRecords(3 * records1stBlock);
-    Reader reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);
-    Scanner scanner = reader.createScanner();
-    Location loc2 =
-        locate(scanner, composeSortedKey(KEY, 3 * records1stBlock, 2)
-            .getBytes());
-    Location locLastIn1stBlock =
-        locate(scanner, composeSortedKey(KEY, 3 * records1stBlock,
-            records1stBlock - 1).getBytes());
-    Location locFirstIn2ndBlock =
-        locate(scanner, composeSortedKey(KEY, 3 * records1stBlock,
-            records1stBlock).getBytes());
-    Location locX = locate(scanner, "keyX".getBytes());
-    Assert.assertEquals(scanner.endLocation, locX);
-    scanner.close();
-    reader.close();
-  }
-
-  public void testFailureWriterNotClosed() throws IOException {
-    if (skip)
-      return;
-    Reader reader = null;
-    try {
-      reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);
-      Assert.fail("Cannot read before closing the writer.");
-    }
-    catch (IOException e) {
-      // noop, expecting exceptions
-    }
-    finally {
-      if (reader != null) {
-        reader.close();
-      }
-    }
-  }
-
-  public void testFailureWriteMetaBlocksWithSameName() throws IOException {
-    if (skip)
-      return;
-    writer.append("keyX".getBytes(), "valueX".getBytes());
-
-    // create a new metablock
-    DataOutputStream outMeta =
-        writer.prepareMetaBlock("testX", Compression.Algorithm.GZ.getName());
-    outMeta.write(123);
-    outMeta.write("foo".getBytes());
-    outMeta.close();
-    // add the same metablock
-    try {
-      DataOutputStream outMeta2 =
-          writer.prepareMetaBlock("testX", Compression.Algorithm.GZ.getName());
-      Assert.fail("Cannot create metablocks with the same name.");
-    }
-    catch (Exception e) {
-      // noop, expecting exceptions
-    }
-    closeOutput();
-  }
-
-  public void testFailureGetNonExistentMetaBlock() throws IOException {
-    if (skip)
-      return;
-    writer.append("keyX".getBytes(), "valueX".getBytes());
-
-    // create a new metablock
-    DataOutputStream outMeta =
-        writer.prepareMetaBlock("testX", Compression.Algorithm.GZ.getName());
-    outMeta.write(123);
-    outMeta.write("foo".getBytes());
-    outMeta.close();
-    closeOutput();
-
-    Reader reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);
-    DataInputStream mb = reader.getMetaBlock("testX");
-    Assert.assertNotNull(mb);
-    mb.close();
-    try {
-      DataInputStream mbBad = reader.getMetaBlock("testY");
-      Assert.assertNull(mbBad);
-      Assert.fail("Error on handling non-existent metablocks.");
-    }
-    catch (Exception e) {
-      // noop, expecting exceptions
-    }
-    reader.close();
-  }
-
-  public void testFailureWriteRecordAfterMetaBlock() throws IOException {
-    if (skip)
-      return;
-    // write a key/value first
-    writer.append("keyX".getBytes(), "valueX".getBytes());
-    // create a new metablock
-    DataOutputStream outMeta =
-        writer.prepareMetaBlock("testX", Compression.Algorithm.GZ.getName());
-    outMeta.write(123);
-    outMeta.write("dummy".getBytes());
-    outMeta.close();
-    // add more key/value
-    try {
-      writer.append("keyY".getBytes(), "valueY".getBytes());
-      Assert.fail("Cannot add key/value after start adding meta blocks.");
-    }
-    catch (Exception e) {
-      // noop, expecting exceptions
-    }
-    closeOutput();
-  }
-
-  public void testFailureReadValueManyTimes() throws IOException {
-    if (skip)
-      return;
-    writeRecords(5);
-
-    Reader reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);
-    Scanner scanner = reader.createScanner();
-
-    byte[] vbuf = new byte[BUF_SIZE];
-    int vlen = scanner.entry().getValueLength();
-    scanner.entry().getValue(vbuf);
-    Assert.assertEquals(new String(vbuf, 0, vlen), VALUE + 0);
-    try {
-      scanner.entry().getValue(vbuf);
-      Assert.fail("Cannot get the value mlutiple times.");
-    }
-    catch (Exception e) {
-      // noop, expecting exceptions
-    }
-
-    scanner.close();
-    reader.close();
-  }
-
-  public void testFailureBadCompressionCodec() throws IOException {
-    if (skip)
-      return;
-    closeOutput();
-    out = fs.create(path);
-    try {
-      writer = new Writer(out, BLOCK_SIZE, "BAD", comparator, conf);
-      Assert.fail("Error on handling invalid compression codecs.");
-    }
-    catch (Exception e) {
-      // noop, expecting exceptions
-      // e.printStackTrace();
-    }
-  }
-
-  public void testFailureOpenEmptyFile() throws IOException {
-    if (skip)
-      return;
-    closeOutput();
-    // create an absolutely empty file
-    path = new Path(fs.getWorkingDirectory(), outputFile);
-    out = fs.create(path);
-    out.close();
-    try {
-      Reader reader =
-          new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);
-      Assert.fail("Error on handling empty files.");
-    }
-    catch (EOFException e) {
-      // noop, expecting exceptions
-    }
-  }
-
-  public void testFailureOpenRandomFile() throws IOException {
-    if (skip)
-      return;
-    closeOutput();
-    // create an random file
-    path = new Path(fs.getWorkingDirectory(), outputFile);
-    out = fs.create(path);
-    Random rand = new Random();
-    byte[] buf = new byte[K];
-    // fill with > 1MB data
-    for (int nx = 0; nx < K + 2; nx++) {
-      rand.nextBytes(buf);
-      out.write(buf);
-    }
-    out.close();
-    try {
-      Reader reader =
-          new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);
-      Assert.fail("Error on handling random files.");
-    }
-    catch (IOException e) {
-      // noop, expecting exceptions
-    }
-  }
-
-  public void testFailureKeyLongerThan64K() throws IOException {
-    if (skip)
-      return;
-    byte[] buf = new byte[64 * K + 1];
-    Random rand = new Random();
-    rand.nextBytes(buf);
-    try {
-      writer.append(buf, "valueX".getBytes());
-    }
-    catch (IndexOutOfBoundsException e) {
-      // noop, expecting exceptions
-    }
-    closeOutput();
-  }
-
-  public void testFailureOutOfOrderKeys() throws IOException {
-    if (skip)
-      return;
-    try {
-      writer.append("keyM".getBytes(), "valueM".getBytes());
-      writer.append("keyA".getBytes(), "valueA".getBytes());
-      Assert.fail("Error on handling out of order keys.");
-    }
-    catch (Exception e) {
-      // noop, expecting exceptions
-      // e.printStackTrace();
-    }
-
-    closeOutput();
-  }
-
-  public void testFailureNegativeOffset() throws IOException {
-    if (skip)
-      return;
-    try {
-      writer.append("keyX".getBytes(), -1, 4, "valueX".getBytes(), 0, 6);
-      Assert.fail("Error on handling negative offset.");
-    }
-    catch (Exception e) {
-      // noop, expecting exceptions
-    }
-    closeOutput();
-  }
-
-  public void testFailureNegativeOffset_2() throws IOException {
-    if (skip)
-      return;
-    closeOutput();
-
-    Reader reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);
-    Scanner scanner = reader.createScanner();
-    try {
-      scanner.lowerBound("keyX".getBytes(), -1, 4);
-      Assert.fail("Error on handling negative offset.");
-    }
-    catch (Exception e) {
-      // noop, expecting exceptions
-    }
-    finally {
-      reader.close();
-      scanner.close();
-    }
-    closeOutput();
-  }
-
-  public void testFailureNegativeLength() throws IOException {
-    if (skip)
-      return;
-    try {
-      writer.append("keyX".getBytes(), 0, -1, "valueX".getBytes(), 0, 6);
-      Assert.fail("Error on handling negative length.");
-    }
-    catch (Exception e) {
-      // noop, expecting exceptions
-    }
-    closeOutput();
-  }
-
-  public void testFailureNegativeLength_2() throws IOException {
-    if (skip)
-      return;
-    closeOutput();
-
-    Reader reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);
-    Scanner scanner = reader.createScanner();
-    try {
-      scanner.lowerBound("keyX".getBytes(), 0, -1);
-      Assert.fail("Error on handling negative length.");
-    }
-    catch (Exception e) {
-      // noop, expecting exceptions
-    }
-    finally {
-      scanner.close();
-      reader.close();
-    }
-    closeOutput();
-  }
-
-  public void testFailureNegativeLength_3() throws IOException {
-    if (skip)
-      return;
-    writeRecords(3);
-
-    Reader reader =
-        new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);
-    Scanner scanner = reader.createScanner();
-    try {
-      // test negative array offset
-      try {
-        scanner.seekTo("keyY".getBytes(), -1, 4);
-        Assert.fail("Failed to handle negative offset.");
-      } catch (Exception e) {
-        // noop, expecting exceptions
-      }
-
-      // test negative array length
-      try {
-        scanner.seekTo("keyY".getBytes(), 0, -2);
-        Assert.fail("Failed to handle negative key length.");
-      } catch (Exception e) {
-        // noop, expecting exceptions
-      }
-    } finally {
-      reader.close();
-      scanner.close();
-    }
-  }
-
-  public void testFailureCompressionNotWorking() throws IOException {
-    if (skip)
-      return;
-    long rawDataSize = writeRecords(10 * records1stBlock, false);
-    if (!compression.equalsIgnoreCase(Compression.Algorithm.NONE.getName())) {
-      Assert.assertTrue(out.getPos() < rawDataSize);
-    }
-    closeOutput();
-  }
-
-  public void testFailureFileWriteNotAt0Position() throws IOException {
-    if (skip)
-      return;
-    closeOutput();
-    out = fs.create(path);
-    out.write(123);
-
-    try {
-      writer = new Writer(out, BLOCK_SIZE, compression, comparator, conf);
-      Assert.fail("Failed to catch file write not at position 0.");
-    }
-    catch (Exception e) {
-      // noop, expecting exceptions
-    }
-    closeOutput();
-  }
-
-  private long writeRecords(int count) throws IOException {
-    return writeRecords(count, true);
-  }
-
-  private long writeRecords(int count, boolean close) throws IOException {
-    long rawDataSize = writeRecords(writer, count);
-    if (close) {
-      closeOutput();
-    }
-    return rawDataSize;
-  }
-
-  static long writeRecords(Writer writer, int count) throws IOException {
-    long rawDataSize = 0;
-    int nx;
-    for (nx = 0; nx < count; nx++) {
-      byte[] key = composeSortedKey(KEY, count, nx).getBytes();
-      byte[] value = (VALUE + nx).getBytes();
-      writer.append(key, value);
-      rawDataSize +=
-          WritableUtils.getVIntSize(key.length) + key.length
-              + WritableUtils.getVIntSize(value.length) + value.length;
-    }
-    return rawDataSize;
-  }
-
-  /**
-   * Insert some leading 0's in front of the value, to make the keys sorted.
-   * 
-   * @param prefix
-   * @param total
-   * @param value
-   * @return
-   */
-  static String composeSortedKey(String prefix, int total, int value) {
-    return String.format("%s%010d", prefix, value);
-  }
-
-  /**
-   * Calculate how many digits are in the 10-based integer.
-   * 
-   * @param value
-   * @return
-   */
-  private static int numberDigits(int value) {
-    int digits = 0;
-    while ((value = value / 10) > 0) {
-      digits++;
-    }
-    return digits;
-  }
-
-  private void readRecords(int count) throws IOException {
-    readRecords(fs, path, count, conf);
-  }
-
-  static void readRecords(FileSystem fs, Path path, int count,
-      Configuration conf) throws IOException {
-    Reader reader =
-        new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);
-    Scanner scanner = reader.createScanner();
-
-    try {
-      for (int nx = 0; nx < count; nx++, scanner.advance()) {
-        Assert.assertFalse(scanner.atEnd());
-        // Assert.assertTrue(scanner.next());
-
-        byte[] kbuf = new byte[BUF_SIZE];
-        int klen = scanner.entry().getKeyLength();
-        scanner.entry().getKey(kbuf);
-        Assert.assertEquals(new String(kbuf, 0, klen), composeSortedKey(KEY,
-            count, nx));
-
-        byte[] vbuf = new byte[BUF_SIZE];
-        int vlen = scanner.entry().getValueLength();
-        scanner.entry().getValue(vbuf);
-        Assert.assertEquals(new String(vbuf, 0, vlen), VALUE + nx);
-      }
-
-      Assert.assertTrue(scanner.atEnd());
-      Assert.assertFalse(scanner.advance());
-    }
-    finally {
-      scanner.close();
-      reader.close();
-    }
-  }
-
-  private void checkBlockIndex(int count, int recordIndex,
-      int blockIndexExpected) throws IOException {
-    Reader reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);
-    Scanner scanner = reader.createScanner();
-    scanner.seekTo(composeSortedKey(KEY, count, recordIndex).getBytes());
-    Assert.assertEquals(blockIndexExpected, scanner.currentLocation
-        .getBlockIndex());
-    scanner.close();
-    reader.close();
-  }
-
-  private void readValueBeforeKey(int count, int recordIndex)
-      throws IOException {
-    Reader reader =
-        new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);
-    Scanner scanner =
-        reader.createScannerByKey(composeSortedKey(KEY, count, recordIndex)
-            .getBytes(), null);
-
-    try {
-      byte[] vbuf = new byte[BUF_SIZE];
-      int vlen = scanner.entry().getValueLength();
-      scanner.entry().getValue(vbuf);
-      Assert.assertEquals(new String(vbuf, 0, vlen), VALUE + recordIndex);
-
-      byte[] kbuf = new byte[BUF_SIZE];
-      int klen = scanner.entry().getKeyLength();
-      scanner.entry().getKey(kbuf);
-      Assert.assertEquals(new String(kbuf, 0, klen), composeSortedKey(KEY,
-          count, recordIndex));
-    }
-    finally {
-      scanner.close();
-      reader.close();
-    }
-  }
-
-  private void readKeyWithoutValue(int count, int recordIndex)
-      throws IOException {
-    Reader reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);
-    Scanner scanner =
-        reader.createScannerByKey(composeSortedKey(KEY, count, recordIndex)
-            .getBytes(), null);
-
-    try {
-      // read the indexed key
-      byte[] kbuf1 = new byte[BUF_SIZE];
-      int klen1 = scanner.entry().getKeyLength();
-      scanner.entry().getKey(kbuf1);
-      Assert.assertEquals(new String(kbuf1, 0, klen1), composeSortedKey(KEY,
-          count, recordIndex));
-
-      if (scanner.advance() && !scanner.atEnd()) {
-        // read the next key following the indexed
-        byte[] kbuf2 = new byte[BUF_SIZE];
-        int klen2 = scanner.entry().getKeyLength();
-        scanner.entry().getKey(kbuf2);
-        Assert.assertEquals(new String(kbuf2, 0, klen2), composeSortedKey(KEY,
-            count, recordIndex + 1));
-      }
-    }
-    finally {
-      scanner.close();
-      reader.close();
-    }
-  }
-
-  private void readValueWithoutKey(int count, int recordIndex)
-      throws IOException {
-    Reader reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);
-
-    Scanner scanner =
-        reader.createScannerByKey(composeSortedKey(KEY, count, recordIndex)
-            .getBytes(), null);
-
-    byte[] vbuf1 = new byte[BUF_SIZE];
-    int vlen1 = scanner.entry().getValueLength();
-    scanner.entry().getValue(vbuf1);
-    Assert.assertEquals(new String(vbuf1, 0, vlen1), VALUE + recordIndex);
-
-    if (scanner.advance() && !scanner.atEnd()) {
-      byte[] vbuf2 = new byte[BUF_SIZE];
-      int vlen2 = scanner.entry().getValueLength();
-      scanner.entry().getValue(vbuf2);
-      Assert.assertEquals(new String(vbuf2, 0, vlen2), VALUE
-          + (recordIndex + 1));
-    }
-
-    scanner.close();
-    reader.close();
-  }
-
-  private void readKeyManyTimes(int count, int recordIndex) throws IOException {
-    Reader reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);
-
-    Scanner scanner =
-        reader.createScannerByKey(composeSortedKey(KEY, count, recordIndex)
-            .getBytes(), null);
-
-    // read the indexed key
-    byte[] kbuf1 = new byte[BUF_SIZE];
-    int klen1 = scanner.entry().getKeyLength();
-    scanner.entry().getKey(kbuf1);
-    Assert.assertEquals(new String(kbuf1, 0, klen1), composeSortedKey(KEY,
-        count, recordIndex));
-
-    klen1 = scanner.entry().getKeyLength();
-    scanner.entry().getKey(kbuf1);
-    Assert.assertEquals(new String(kbuf1, 0, klen1), composeSortedKey(KEY,
-        count, recordIndex));
-
-    klen1 = scanner.entry().getKeyLength();
-    scanner.entry().getKey(kbuf1);
-    Assert.assertEquals(new String(kbuf1, 0, klen1), composeSortedKey(KEY,
-        count, recordIndex));
-
-    scanner.close();
-    reader.close();
-  }
-
-  private void closeOutput() throws IOException {
-    if (writer != null) {
-      writer.close();
-      writer = null;
-    }
-    if (out != null) {
-      out.close();
-      out = null;
-    }
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileComparators.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileComparators.java
deleted file mode 100644
index c586395d7..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileComparators.java
+++ /dev/null
@@ -1,122 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-package org.apache.hadoop.zebra.tfile;
-
-import java.io.IOException;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.zebra.tfile.TFile.Writer;
-
-/**
- * 
- * Byte arrays test case class using GZ compression codec, base class of none
- * and LZO compression classes.
- * 
- */
-public class TestTFileComparators extends TestCase {
-  private static String ROOT =
-      System.getProperty("test.build.data", "/tmp/tfile-test");
-
-  private final static int BLOCK_SIZE = 512;
-  private FileSystem fs;
-  private Configuration conf;
-  private Path path;
-  private FSDataOutputStream out;
-  private Writer writer;
-
-  private String compression = Compression.Algorithm.GZ.getName();
-  private String outputFile = "TFileTestComparators";
-  /*
-   * pre-sampled numbers of records in one block, based on the given the
-   * generated key and value strings
-   */
-  // private int records1stBlock = 4314;
-  // private int records2ndBlock = 4108;
-  private int records1stBlock = 4480;
-  private int records2ndBlock = 4263;
-
-  @Override
-  public void setUp() throws IOException {
-    conf = new Configuration();
-    path = new Path(ROOT, outputFile);
-    fs = path.getFileSystem(conf);
-    out = fs.create(path);
-  }
-
-  @Override
-  public void tearDown() throws IOException {
-    fs.delete(path, true);
-  }
-
-  // bad comparator format
-  public void testFailureBadComparatorNames() throws IOException {
-    try {
-      writer = new Writer(out, BLOCK_SIZE, compression, "badcmp", conf);
-      Assert.fail("Failed to catch unsupported comparator names");
-    }
-    catch (Exception e) {
-      // noop, expecting exceptions
-      e.printStackTrace();
-    }
-  }
-
-  // jclass that doesn't exist
-  public void testFailureBadJClassNames() throws IOException {
-    try {
-      writer =
-          new Writer(out, BLOCK_SIZE, compression,
-              "jclass: some.non.existence.clazz", conf);
-      Assert.fail("Failed to catch unsupported comparator names");
-    }
-    catch (Exception e) {
-      // noop, expecting exceptions
-      e.printStackTrace();
-    }
-  }
-
-  // class exists but not a RawComparator
-  public void testFailureBadJClasses() throws IOException {
-    try {
-      writer =
-          new Writer(out, BLOCK_SIZE, compression,
-              "jclass:org.apache.hadoop.zebra.tfile.Chunk", conf);
-      Assert.fail("Failed to catch unsupported comparator names");
-    }
-    catch (Exception e) {
-      // noop, expecting exceptions
-      e.printStackTrace();
-    }
-  }
-
-  private void closeOutput() throws IOException {
-    if (writer != null) {
-      writer.close();
-      writer = null;
-    }
-    if (out != null) {
-      out.close();
-      out = null;
-    }
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileJClassComparatorByteArrays.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileJClassComparatorByteArrays.java
deleted file mode 100644
index dacca6079..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileJClassComparatorByteArrays.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-package org.apache.hadoop.zebra.tfile;
-
-import java.io.IOException;
-import java.io.Serializable;
-
-import org.apache.hadoop.io.RawComparator;
-import org.apache.hadoop.io.WritableComparator;
-
-/**
- * 
- * Byte arrays test case class using GZ compression codec, base class of none
- * and LZO compression classes.
- * 
- */
-
-public class TestTFileJClassComparatorByteArrays extends TestTFileByteArrays {
-  /**
-   * Test non-compression codec, using the same test cases as in the ByteArrays.
-   */
-  @Override
-  public void setUp() throws IOException {
-    init(Compression.Algorithm.GZ.getName(),
-        "jclass: org.apache.hadoop.zebra.tfile.MyComparator",
-        "TFileTestJClassComparator", 4480, 4263);
-    super.setUp();
-  }
-}
-
-class MyComparator implements RawComparator<byte[]>, Serializable {
-
-  @Override
-  public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
-    return WritableComparator.compareBytes(b1, s1, l1, b2, s2, l2);
-  }
-
-  @Override
-  public int compare(byte[] o1, byte[] o2) {
-    return WritableComparator.compareBytes(o1, 0, o1.length, o2, 0, o2.length);
-  }
-  
-}
-
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileLzoCodecsByteArrays.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileLzoCodecsByteArrays.java
deleted file mode 100644
index 45f4df788..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileLzoCodecsByteArrays.java
+++ /dev/null
@@ -1,42 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.tfile;
-
-import java.io.IOException;
-
-import org.apache.hadoop.zebra.tfile.Compression.Algorithm;
-
-public class TestTFileLzoCodecsByteArrays extends TestTFileByteArrays {
-  /**
-   * Test LZO compression codec, using the same test cases as in the ByteArrays.
-   */
-  @Override
-  public void setUp() throws IOException {
-    skip = !(Algorithm.LZO.isSupported());
-    if (skip) {
-      System.out.println("Skipped");
-    }
-
-    // TODO: sample the generated key/value records, and put the numbers below
-    init(Compression.Algorithm.LZO.getName(), "memcmp", "TFileTestCodecsLzo",
-        2605, 2558);
-    if (!skip)
-      super.setUp();
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileLzoCodecsStreams.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileLzoCodecsStreams.java
deleted file mode 100644
index 0885a5e78..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileLzoCodecsStreams.java
+++ /dev/null
@@ -1,39 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.tfile;
-
-import java.io.IOException;
-
-import org.apache.hadoop.zebra.tfile.Compression.Algorithm;
-
-public class TestTFileLzoCodecsStreams extends TestTFileStreams {
-  /**
-   * Test LZO compression codec, using the same test cases as in the ByteArrays.
-   */
-  @Override
-  public void setUp() throws IOException {
-    skip = !(Algorithm.LZO.isSupported());
-    if (skip) {
-      System.out.println("Skipped");
-    }
-    init(Compression.Algorithm.LZO.getName(), "memcmp", "TFileTestCodecsLzo");
-    if (!skip) 
-      super.setUp();
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileNoneCodecsByteArrays.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileNoneCodecsByteArrays.java
deleted file mode 100644
index 7f7fa518b..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileNoneCodecsByteArrays.java
+++ /dev/null
@@ -1,32 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-package org.apache.hadoop.zebra.tfile;
-
-import java.io.IOException;
-
-public class TestTFileNoneCodecsByteArrays extends TestTFileByteArrays {
-  /**
-   * Test non-compression codec, using the same test cases as in the ByteArrays.
-   */
-  @Override
-  public void setUp() throws IOException {
-    init(Compression.Algorithm.NONE.getName(), "memcmp", "TFileTestCodecsNone",
-        24, 24);
-    super.setUp();
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileNoneCodecsJClassComparatorByteArrays.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileNoneCodecsJClassComparatorByteArrays.java
deleted file mode 100644
index 35cc8a62b..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileNoneCodecsJClassComparatorByteArrays.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-package org.apache.hadoop.zebra.tfile;
-
-import java.io.IOException;
-
-import org.apache.hadoop.io.RawComparator;
-import org.apache.hadoop.io.WritableComparator;
-
-/**
- * 
- * Byte arrays test case class using GZ compression codec, base class of none
- * and LZO compression classes.
- * 
- */
-
-public class TestTFileNoneCodecsJClassComparatorByteArrays extends TestTFileByteArrays {
-  /**
-   * Test non-compression codec, using the same test cases as in the ByteArrays.
-   */
-  @Override
-  public void setUp() throws IOException {
-    init(Compression.Algorithm.NONE.getName(),
-        "jclass: org.apache.hadoop.zebra.tfile.MyComparator",
-        "TestTFileNoneCodecsJClassComparatorByteArrays", 24, 24);
-    super.setUp();
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileNoneCodecsStreams.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileNoneCodecsStreams.java
deleted file mode 100644
index a80196290..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileNoneCodecsStreams.java
+++ /dev/null
@@ -1,32 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.tfile;
-
-import java.io.IOException;
-
-public class TestTFileNoneCodecsStreams extends TestTFileStreams {
-  /**
-   * Test non-compression codec, using the same test cases as in the ByteArrays.
-   */
-  @Override
-  public void setUp() throws IOException {
-    init(Compression.Algorithm.NONE.getName(), "memcmp", "TFileTestCodecsNone");
-    super.setUp();
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileSeek.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileSeek.java
deleted file mode 100644
index ee2049582..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileSeek.java
+++ /dev/null
@@ -1,504 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-package org.apache.hadoop.zebra.tfile;
-
-import java.io.IOException;
-import java.util.Random;
-import java.util.StringTokenizer;
-
-import junit.framework.TestCase;
-
-import org.apache.commons.cli.CommandLine;
-import org.apache.commons.cli.CommandLineParser;
-import org.apache.commons.cli.GnuParser;
-import org.apache.commons.cli.HelpFormatter;
-import org.apache.commons.cli.Option;
-import org.apache.commons.cli.OptionBuilder;
-import org.apache.commons.cli.Options;
-import org.apache.commons.cli.ParseException;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.tfile.RandomDistribution.DiscreteRNG;
-import org.apache.hadoop.zebra.tfile.TFile.Reader;
-import org.apache.hadoop.zebra.tfile.TFile.Writer;
-import org.apache.hadoop.zebra.tfile.TFile.Reader.Scanner;
-
-/**
- * test the performance for seek.
- *
- */
-public class TestTFileSeek extends TestCase { 
-  private MyOptions options;
-  private Configuration conf;
-  private Path path;
-  private FileSystem fs;
-  private NanoTimer timer;
-  private Random rng;
-  private DiscreteRNG keyLenGen;
-  private KVGenerator kvGen;
-
-  @Override
-  public void setUp() throws IOException {
-    if (options == null) {
-      options = new MyOptions(new String[0]);
-    }
-
-    conf = new Configuration();
-    conf.setInt("tfile.fs.input.buffer.size", options.fsInputBufferSize);
-    conf.setInt("tfile.fs.output.buffer.size", options.fsOutputBufferSize);
-    path = new Path(new Path(options.rootDir), options.file);
-    fs = path.getFileSystem(conf);
-    timer = new NanoTimer(false);
-    rng = new Random(options.seed);
-    keyLenGen =
-        new RandomDistribution.Zipf(new Random(rng.nextLong()),
-            options.minKeyLen, options.maxKeyLen, 1.2);
-    DiscreteRNG valLenGen =
-        new RandomDistribution.Flat(new Random(rng.nextLong()),
-            options.minValLength, options.maxValLength);
-    DiscreteRNG wordLenGen =
-        new RandomDistribution.Flat(new Random(rng.nextLong()),
-            options.minWordLen, options.maxWordLen);
-    kvGen =
-        new KVGenerator(rng, true, keyLenGen, valLenGen, wordLenGen,
-            options.dictSize);
-  }
-  
-  @Override
-  public void tearDown() throws IOException {
-    fs.delete(path, true);
-  }
-  
-  private static FSDataOutputStream createFSOutput(Path name, FileSystem fs)
-    throws IOException {
-    if (fs.exists(name)) {
-      fs.delete(name, true);
-    }
-    FSDataOutputStream fout = fs.create(name);
-    return fout;
-  }
-
-  private void createTFile() throws IOException {
-    long totalBytes = 0;
-    FSDataOutputStream fout = createFSOutput(path, fs);
-    try {
-      Writer writer =
-          new Writer(fout, options.minBlockSize, options.compress, "memcmp",
-              conf);
-      try {
-        BytesWritable key = new BytesWritable();
-        BytesWritable val = new BytesWritable();
-        timer.start();
-        for (long i = 0; true; ++i) {
-          if (i % 1000 == 0) { // test the size for every 1000 rows.
-            if (fs.getFileStatus(path).getLen() >= options.fileSize) {
-              break;
-            }
-          }
-          kvGen.next(key, val, false);
-          writer.append(key.get(), 0, key.getSize(), val.get(), 0, val
-              .getSize());
-          totalBytes += key.getSize();
-          totalBytes += val.getSize();
-        }
-        timer.stop();
-      }
-      finally {
-        writer.close();
-      }
-    }
-    finally {
-      fout.close();
-    }
-    double duration = (double)timer.read()/1000; // in us.
-    long fsize = fs.getFileStatus(path).getLen();
-
-    System.out.printf(
-        "time: %s...uncompressed: %.2fMB...raw thrpt: %.2fMB/s\n",
-        timer.toString(), (double) totalBytes / 1024 / 1024, totalBytes
-            / duration);
-    System.out.printf("time: %s...file size: %.2fMB...disk thrpt: %.2fMB/s\n",
-        timer.toString(), (double) fsize / 1024 / 1024, fsize / duration);
-  }
-  
-  public void seekTFile() throws IOException {
-    int miss = 0;
-    long totalBytes = 0;
-    FSDataInputStream fsdis = fs.open(path);
-    Reader reader =
-      new Reader(fsdis, fs.getFileStatus(path).getLen(), conf);
-    KeySampler kSampler =
-        new KeySampler(rng, reader.getFirstKey(), reader.getLastKey(),
-            keyLenGen);
-    Scanner scanner = reader.createScanner();
-    BytesWritable key = new BytesWritable();
-    BytesWritable val = new BytesWritable();
-    timer.reset();
-    timer.start();
-    for (int i = 0; i < options.seekCount; ++i) {
-      kSampler.next(key);
-      scanner.lowerBound(key.get(), 0, key.getSize());
-      if (!scanner.atEnd()) {
-        scanner.entry().get(key, val);
-        totalBytes += key.getSize();
-        totalBytes += val.getSize();
-      }
-      else {
-        ++miss;
-      }
-    }
-    timer.stop();
-    double duration = (double) timer.read() / 1000; // in us.
-    System.out.printf(
-        "time: %s...avg seek: %s...%d hit...%d miss...avg I/O size: %.2fKB\n",
-        timer.toString(), NanoTimer.nanoTimeToString(timer.read()
-            / options.seekCount), options.seekCount - miss, miss,
-        (double) totalBytes / 1024 / (options.seekCount - miss));
-
-  }
-  
-  public void testSeeks() throws IOException {
-    String[] supported = TFile.getSupportedCompressionAlgorithms();
-    boolean proceed = false;
-    for (String c : supported) {
-      if (c.equals(options.compress)) {
-        proceed = true;
-        break;
-      }
-    }
-
-    if (!proceed) {
-      System.out.println("Skipped for " + options.compress);
-      return;
-    }
-
-    if (options.doCreate()) {
-      createTFile();
-    }
-
-    if (options.doRead()) {
-      seekTFile();
-    }
-  }
-  
-  private static class IntegerRange {
-    private final int from, to;
-
-    public IntegerRange(int from, int to) {
-      this.from = from;
-      this.to = to;
-    }
-
-    public static IntegerRange parse(String s) throws ParseException {
-      StringTokenizer st = new StringTokenizer(s, " \t,");
-      if (st.countTokens() != 2) {
-        throw new ParseException("Bad integer specification: " + s);
-      }
-      int from = Integer.parseInt(st.nextToken());
-      int to = Integer.parseInt(st.nextToken());
-      return new IntegerRange(from, to);
-    }
-
-    public int from() {
-      return from;
-    }
-
-    public int to() {
-      return to;
-    }
-  }
-
-  private static class MyOptions {
-    // hard coded constants
-    int dictSize = 1000;
-    int minWordLen = 5;
-    int maxWordLen = 20;
-    int osInputBufferSize = 64 * 1024;
-    int osOutputBufferSize = 64 * 1024;
-    int fsInputBufferSizeNone = 0;
-    int fsInputBufferSizeLzo = 0;
-    int fsInputBufferSizeGz = 0;
-    int fsOutputBufferSizeNone = 1;
-    int fsOutputBufferSizeLzo = 1;
-    int fsOutputBufferSizeGz = 1;
-   
-    String rootDir =
-        System.getProperty("test.build.data", "/tmp/tfile-test");
-    String file = "TestTFileSeek";
-    String compress = "gz";
-    int minKeyLen = 10;
-    int maxKeyLen = 50;
-    int minValLength = 100;
-    int maxValLength = 200;
-    int minBlockSize = 64 * 1024;
-    int fsOutputBufferSize = 1;
-    int fsInputBufferSize = 0;
-    long fileSize = 3 * 1024 * 1024;
-    long seekCount = 1000;
-    long seed;
-
-    static final int OP_CREATE = 1;
-    static final int OP_READ = 2;
-    int op = OP_CREATE | OP_READ;
-
-    boolean proceed = false;
-
-    public MyOptions(String[] args) {
-      seed = System.nanoTime();
-
-      try {
-        Options opts = buildOptions();
-        CommandLineParser parser = new GnuParser();
-        CommandLine line = parser.parse(opts, args, true);
-        processOptions(line, opts);
-        validateOptions();
-      }
-      catch (ParseException e) {
-        System.out.println(e.getMessage());
-        System.out.println("Try \"--help\" option for details.");
-        setStopProceed();
-      }
-    }
-
-    public boolean proceed() {
-      return proceed;
-    }
-
-    private Options buildOptions() {
-      Option compress =
-          OptionBuilder.withLongOpt("compress").withArgName("[none|lzo|gz]")
-              .hasArg().withDescription("compression scheme").create('c');
-
-      Option fileSize =
-          OptionBuilder.withLongOpt("file-size").withArgName("size-in-MB")
-              .hasArg().withDescription("target size of the file (in MB).")
-              .create('s');
-
-      Option fsInputBufferSz =
-          OptionBuilder.withLongOpt("fs-input-buffer").withArgName("size")
-              .hasArg().withDescription(
-                  "size of the file system input buffer (in bytes).").create(
-                  'i');
-
-      Option fsOutputBufferSize =
-          OptionBuilder.withLongOpt("fs-output-buffer").withArgName("size")
-              .hasArg().withDescription(
-                  "size of the file system output buffer (in bytes).").create(
-                  'o');
-
-      Option keyLen =
-          OptionBuilder
-              .withLongOpt("key-length")
-              .withArgName("min,max")
-              .hasArg()
-              .withDescription(
-                  "the length range of the key (in bytes)")
-              .create('k');
-
-      Option valueLen =
-          OptionBuilder
-              .withLongOpt("value-length")
-              .withArgName("min,max")
-              .hasArg()
-              .withDescription(
-                  "the length range of the value (in bytes)")
-              .create('v');
-
-      Option blockSz =
-          OptionBuilder.withLongOpt("block").withArgName("size-in-KB").hasArg()
-              .withDescription("minimum block size (in KB)").create('b');
-
-      Option seed =
-          OptionBuilder.withLongOpt("seed").withArgName("long-int").hasArg()
-              .withDescription("specify the seed").create('S');
-
-      Option operation =
-          OptionBuilder.withLongOpt("operation").withArgName("r|w|rw").hasArg()
-              .withDescription(
-                  "action: seek-only, create-only, seek-after-create").create(
-                  'x');
-
-      Option rootDir =
-          OptionBuilder.withLongOpt("root-dir").withArgName("path").hasArg()
-              .withDescription(
-                  "specify root directory where files will be created.")
-              .create('r');
-
-      Option file =
-          OptionBuilder.withLongOpt("file").withArgName("name").hasArg()
-              .withDescription("specify the file name to be created or read.")
-              .create('f');
-
-      Option seekCount =
-          OptionBuilder
-              .withLongOpt("seek")
-              .withArgName("count")
-              .hasArg()
-              .withDescription(
-                  "specify how many seek operations we perform (requires -x r or -x rw.")
-              .create('n');
-
-      Option help =
-          OptionBuilder.withLongOpt("help").hasArg(false).withDescription(
-              "show this screen").create("h");
-
-      return new Options().addOption(compress).addOption(fileSize).addOption(
-          fsInputBufferSz).addOption(fsOutputBufferSize).addOption(keyLen)
-          .addOption(blockSz).addOption(rootDir).addOption(valueLen).addOption(
-              operation).addOption(seekCount).addOption(file).addOption(help);
-
-    }
-
-    private void processOptions(CommandLine line, Options opts)
-        throws ParseException {
-      // --help -h and --version -V must be processed first.
-      if (line.hasOption('h')) {
-        HelpFormatter formatter = new HelpFormatter();
-        System.out.println("TFile and SeqFile benchmark.");
-        System.out.println();
-        formatter.printHelp(100,
-            "java ... TestTFileSeqFileComparison [options]",
-            "\nSupported options:", opts, "");
-        return;
-      }
-
-      if (line.hasOption('c')) {
-        compress = line.getOptionValue('c');
-      }
-
-      if (line.hasOption('d')) {
-        dictSize = Integer.parseInt(line.getOptionValue('d'));
-      }
-
-      if (line.hasOption('s')) {
-        fileSize = Long.parseLong(line.getOptionValue('s')) * 1024 * 1024;
-      }
-
-      if (line.hasOption('i')) {
-        fsInputBufferSize = Integer.parseInt(line.getOptionValue('i'));
-      }
-
-      if (line.hasOption('o')) {
-        fsOutputBufferSize = Integer.parseInt(line.getOptionValue('o'));
-      }
-      
-      if (line.hasOption('n')) {
-        seekCount = Integer.parseInt(line.getOptionValue('n'));
-      }
-
-      if (line.hasOption('k')) {
-        IntegerRange ir = IntegerRange.parse(line.getOptionValue('k'));
-        minKeyLen = ir.from();
-        maxKeyLen = ir.to();
-      }
-
-      if (line.hasOption('v')) {
-        IntegerRange ir = IntegerRange.parse(line.getOptionValue('v'));
-        minValLength = ir.from();
-        maxValLength = ir.to();
-      }
-
-      if (line.hasOption('b')) {
-        minBlockSize = Integer.parseInt(line.getOptionValue('b')) * 1024;
-      }
-
-      if (line.hasOption('r')) {
-        rootDir = line.getOptionValue('r');
-      }
-      
-      if (line.hasOption('f')) {
-        file = line.getOptionValue('f');
-      }
-
-      if (line.hasOption('S')) {
-        seed = Long.parseLong(line.getOptionValue('S'));
-      }
-
-      if (line.hasOption('x')) {
-        String strOp = line.getOptionValue('x');
-        if (strOp.equals("r")) {
-          op = OP_READ;
-        }
-        else if (strOp.equals("w")) {
-          op = OP_CREATE;
-        }
-        else if (strOp.equals("rw")) {
-          op = OP_CREATE | OP_READ;
-        }
-        else {
-          throw new ParseException("Unknown action specifier: " + strOp);
-        }
-      }
-
-      proceed = true;
-    }
-
-    private void validateOptions() throws ParseException {
-      if (!compress.equals("none") && !compress.equals("lzo")
-          && !compress.equals("gz")) {
-        throw new ParseException("Unknown compression scheme: " + compress);
-      }
-
-      if (minKeyLen >= maxKeyLen) {
-        throw new ParseException(
-            "Max key length must be greater than min key length.");
-      }
-
-      if (minValLength >= maxValLength) {
-        throw new ParseException(
-            "Max value length must be greater than min value length.");
-      }
-
-      if (minWordLen >= maxWordLen) {
-        throw new ParseException(
-            "Max word length must be greater than min word length.");
-      }
-      return;
-    }
-
-    private void setStopProceed() {
-      proceed = false;
-    }
-
-    public boolean doCreate() {
-      return (op & OP_CREATE) != 0;
-    }
-
-    public boolean doRead() {
-      return (op & OP_READ) != 0;
-    }
-  }
-  
-  public static void main(String[] argv) throws IOException {
-    TestTFileSeek testCase = new TestTFileSeek();
-    MyOptions options = new MyOptions(argv);
-    
-    if (options.proceed == false) {
-      return;
-    }
-
-    testCase.options = options;
-    testCase.setUp();
-    testCase.testSeeks();
-    testCase.tearDown();
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileSeqFileComparison.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileSeqFileComparison.java
deleted file mode 100644
index 2507bfa7f..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileSeqFileComparison.java
+++ /dev/null
@@ -1,782 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-package org.apache.hadoop.zebra.tfile;
-
-import java.io.IOException;
-import java.text.DateFormat;
-import java.text.SimpleDateFormat;
-import java.util.Random;
-import java.util.StringTokenizer;
-
-import junit.framework.TestCase;
-
-import org.apache.commons.cli.CommandLine;
-import org.apache.commons.cli.CommandLineParser;
-import org.apache.commons.cli.GnuParser;
-import org.apache.commons.cli.HelpFormatter;
-import org.apache.commons.cli.Option;
-import org.apache.commons.cli.OptionBuilder;
-import org.apache.commons.cli.Options;
-import org.apache.commons.cli.ParseException;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.SequenceFile;
-import org.apache.hadoop.io.compress.CompressionCodec;
-import org.apache.hadoop.zebra.tfile.TFile.Reader.Scanner.Entry;
-
-public class TestTFileSeqFileComparison extends TestCase {
-  MyOptions options;
-
-  private FileSystem fs;
-  private Configuration conf;
-  private long startTimeEpoch;
-  private long finishTimeEpoch;
-  private DateFormat formatter;
-  byte[][] dictionary;
-
-  @Override
-  public void setUp() throws IOException {
-    if (options == null) {
-      options = new MyOptions(new String[0]);
-    }
-
-    conf = new Configuration();
-    conf.setInt("tfile.fs.input.buffer.size", options.fsInputBufferSize);
-    conf.setInt("tfile.fs.output.buffer.size", options.fsOutputBufferSize);
-    Path path = new Path(options.rootDir);
-    fs = path.getFileSystem(conf);
-    formatter = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
-    setUpDictionary();
-  }
-
-  private void setUpDictionary() {
-    Random rng = new Random();
-    dictionary = new byte[options.dictSize][];
-    for (int i = 0; i < options.dictSize; ++i) {
-      int len =
-          rng.nextInt(options.maxWordLen - options.minWordLen)
-              + options.minWordLen;
-      dictionary[i] = new byte[len];
-      rng.nextBytes(dictionary[i]);
-    }
-  }
-
-  @Override
-  public void tearDown() throws IOException {
-    // do nothing
-  }
-
-  public void startTime() throws IOException {
-    startTimeEpoch = System.currentTimeMillis();
-    System.out.println(formatTime() + " Started timing.");
-  }
-
-  public void stopTime() throws IOException {
-    finishTimeEpoch = System.currentTimeMillis();
-    System.out.println(formatTime() + " Stopped timing.");
-  }
-
-  public long getIntervalMillis() throws IOException {
-    return finishTimeEpoch - startTimeEpoch;
-  }
-
-  public void printlnWithTimestamp(String message) throws IOException {
-    System.out.println(formatTime() + "  " + message);
-  }
-
-  /*
-   * Format millis into minutes and seconds.
-   */
-  public String formatTime(long milis) {
-    return formatter.format(milis);
-  }
-
-  public String formatTime() {
-    return formatTime(System.currentTimeMillis());
-  }
-
-  private interface KVAppendable {
-    public void append(BytesWritable key, BytesWritable value)
-        throws IOException;
-
-    public void close() throws IOException;
-  }
-
-  private interface KVReadable {
-    public byte[] getKey();
-
-    public byte[] getValue();
-
-    public int getKeyLength();
-
-    public int getValueLength();
-
-    public boolean next() throws IOException;
-
-    public void close() throws IOException;
-  }
-
-  static class TFileAppendable implements KVAppendable {
-    private FSDataOutputStream fsdos;
-    private TFile.Writer writer;
-
-    public TFileAppendable(FileSystem fs, Path path, String compress,
-        int minBlkSize, int osBufferSize, Configuration conf)
-        throws IOException {
-      this.fsdos = fs.create(path, true, osBufferSize);
-      this.writer = new TFile.Writer(fsdos, minBlkSize, compress, null, conf);
-    }
-
-    public void append(BytesWritable key, BytesWritable value)
-        throws IOException {
-      writer.append(key.get(), 0, key.getSize(), value.get(), 0, value
-          .getSize());
-    }
-
-    public void close() throws IOException {
-      writer.close();
-      fsdos.close();
-    }
-  }
-
-  static class TFileReadable implements KVReadable {
-    private FSDataInputStream fsdis;
-    private TFile.Reader reader;
-    private TFile.Reader.Scanner scanner;
-    private byte[] keyBuffer;
-    private int keyLength;
-    private byte[] valueBuffer;
-    private int valueLength;
-
-    public TFileReadable(FileSystem fs, Path path, int osBufferSize,
-        Configuration conf) throws IOException {
-      this.fsdis = fs.open(path, osBufferSize);
-      this.reader =
-          new TFile.Reader(fsdis, fs.getFileStatus(path).getLen(), conf);
-      this.scanner = reader.createScanner();
-      keyBuffer = new byte[32];
-      valueBuffer = new byte[32];
-    }
-
-    private void checkKeyBuffer(int size) {
-      if (size <= keyBuffer.length) {
-        return;
-      }
-      keyBuffer =
-          new byte[Math.max(2 * keyBuffer.length, 2 * size - keyBuffer.length)];
-    }
-
-    private void checkValueBuffer(int size) {
-      if (size <= valueBuffer.length) {
-        return;
-      }
-      valueBuffer =
-          new byte[Math.max(2 * valueBuffer.length, 2 * size
-              - valueBuffer.length)];
-    }
-
-    public byte[] getKey() {
-      return keyBuffer;
-    }
-
-    public int getKeyLength() {
-      return keyLength;
-    }
-
-    public byte[] getValue() {
-      return valueBuffer;
-    }
-
-    public int getValueLength() {
-      return valueLength;
-    }
-
-    public boolean next() throws IOException {
-      if (scanner.atEnd()) return false;
-      Entry entry = scanner.entry();
-      keyLength = entry.getKeyLength();
-      checkKeyBuffer(keyLength);
-      entry.getKey(keyBuffer);
-      valueLength = entry.getValueLength();
-      checkValueBuffer(valueLength);
-      entry.getValue(valueBuffer);
-      scanner.advance();
-      return true;
-    }
-
-    public void close() throws IOException {
-      scanner.close();
-      reader.close();
-      fsdis.close();
-    }
-  }
-
-  static class SeqFileAppendable implements KVAppendable {
-    private FSDataOutputStream fsdos;
-    private SequenceFile.Writer writer;
-
-    public SeqFileAppendable(FileSystem fs, Path path, int osBufferSize,
-        String compress, int minBlkSize) throws IOException {
-      Configuration conf = new Configuration();
-      conf.setBoolean("hadoop.native.lib", true);
-
-      CompressionCodec codec = null;
-      if ("lzo".equals(compress)) {
-        codec = Compression.Algorithm.LZO.getCodec();
-      }
-      else if ("gz".equals(compress)) {
-        codec = Compression.Algorithm.GZ.getCodec();
-      }
-      else if (!"none".equals(compress))
-        throw new IOException("Codec not supported.");
-
-      this.fsdos = fs.create(path, true, osBufferSize);
-
-      if (!"none".equals(compress)) {
-        writer =
-            SequenceFile.createWriter(conf, fsdos, BytesWritable.class,
-                BytesWritable.class, SequenceFile.CompressionType.BLOCK, codec);
-      }
-      else {
-        writer =
-            SequenceFile.createWriter(conf, fsdos, BytesWritable.class,
-                BytesWritable.class, SequenceFile.CompressionType.NONE, null);
-      }
-    }
-
-    public void append(BytesWritable key, BytesWritable value)
-        throws IOException {
-      writer.append(key, value);
-    }
-
-    public void close() throws IOException {
-      writer.close();
-      fsdos.close();
-    }
-  }
-
-  static class SeqFileReadable implements KVReadable {
-    private SequenceFile.Reader reader;
-    private BytesWritable key;
-    private BytesWritable value;
-
-    public SeqFileReadable(FileSystem fs, Path path, int osBufferSize)
-        throws IOException {
-      Configuration conf = new Configuration();
-      conf.setInt("io.file.buffer.size", osBufferSize);
-      reader = new SequenceFile.Reader(fs, path, conf);
-      key = new BytesWritable();
-      value = new BytesWritable();
-    }
-
-    public byte[] getKey() {
-      return key.get();
-    }
-
-    public int getKeyLength() {
-      return key.getSize();
-    }
-
-    public byte[] getValue() {
-      return value.get();
-    }
-
-    public int getValueLength() {
-      return value.getSize();
-    }
-
-    public boolean next() throws IOException {
-      return reader.next(key, value);
-    }
-
-    public void close() throws IOException {
-      reader.close();
-    }
-  }
-
-  private void reportStats(Path path, long totalBytes) throws IOException {
-    long duration = getIntervalMillis();
-    long fsize = fs.getFileStatus(path).getLen();
-    printlnWithTimestamp(String.format(
-        "Duration: %dms...total size: %.2fMB...raw thrpt: %.2fMB/s", duration,
-        (double) totalBytes / 1024 / 1024, (double) totalBytes / duration
-            * 1000 / 1024 / 1024));
-    printlnWithTimestamp(String.format(
-        "Compressed size: %.2fMB...compressed thrpt: %.2fMB/s.",
-        (double) fsize / 1024 / 1024, (double) fsize / duration * 1000 / 1024
-            / 1024));
-  }
-
-  private void fillBuffer(Random rng, BytesWritable bw, byte[] tmp, int len) {
-    int n = 0;
-    while (n < len) {
-      byte[] word = dictionary[rng.nextInt(dictionary.length)];
-      int l = Math.min(word.length, len - n);
-      System.arraycopy(word, 0, tmp, n, l);
-      n += l;
-    }
-    bw.set(tmp, 0, len);
-  }
-
-  private void timeWrite(Path path, KVAppendable appendable, int baseKlen,
-      int baseVlen, long fileSize) throws IOException {
-    int maxKlen = baseKlen * 2;
-    int maxVlen = baseVlen * 2;
-    BytesWritable key = new BytesWritable();
-    BytesWritable value = new BytesWritable();
-    byte[] keyBuffer = new byte[maxKlen];
-    byte[] valueBuffer = new byte[maxVlen];
-    Random rng = new Random(options.seed);
-    long totalBytes = 0;
-    printlnWithTimestamp("Start writing: " + path.getName() + "...");
-    startTime();
-
-    for (long i = 0; true; ++i) {
-      if (i % 1000 == 0) { // test the size for every 1000 rows.
-        if (fs.getFileStatus(path).getLen() >= fileSize) {
-          break;
-        }
-      }
-      int klen = rng.nextInt(baseKlen) + baseKlen;
-      int vlen = rng.nextInt(baseVlen) + baseVlen;
-      fillBuffer(rng, key, keyBuffer, klen);
-      fillBuffer(rng, value, valueBuffer, vlen);
-      key.set(keyBuffer, 0, klen);
-      value.set(valueBuffer, 0, vlen);
-      appendable.append(key, value);
-      totalBytes += klen;
-      totalBytes += vlen;
-    }
-    stopTime();
-    appendable.close();
-    reportStats(path, totalBytes);
-  }
-
-  private void timeRead(Path path, KVReadable readable) throws IOException {
-    printlnWithTimestamp("Start reading: " + path.getName() + "...");
-    long totalBytes = 0;
-    startTime();
-    for (; readable.next();) {
-      totalBytes += readable.getKeyLength();
-      totalBytes += readable.getValueLength();
-    }
-    stopTime();
-    readable.close();
-    reportStats(path, totalBytes);
-  }
-
-  private void createTFile(String parameters, String compress)
-      throws IOException {
-    System.out.println("=== TFile: Creation (" + parameters + ") === ");
-    Path path = new Path(options.rootDir, "TFile.Performance");
-    KVAppendable appendable =
-        new TFileAppendable(fs, path, compress, options.minBlockSize,
-            options.osOutputBufferSize, conf);
-    timeWrite(path, appendable, options.keyLength, options.valueLength,
-        options.fileSize);
-  }
-
-  private void readTFile(String parameters, boolean delFile) throws IOException {
-    System.out.println("=== TFile: Reading (" + parameters + ") === ");
-    {
-      Path path = new Path(options.rootDir, "TFile.Performance");
-      KVReadable readable =
-          new TFileReadable(fs, path, options.osInputBufferSize, conf);
-      timeRead(path, readable);
-      if (delFile) {
-        if (fs.exists(path)) {
-          fs.delete(path, true);
-        }
-      }
-    }
-  }
-
-  private void createSeqFile(String parameters, String compress)
-      throws IOException {
-    System.out.println("=== SeqFile: Creation (" + parameters + ") === ");
-    Path path = new Path(options.rootDir, "SeqFile.Performance");
-    KVAppendable appendable =
-        new SeqFileAppendable(fs, path, options.osOutputBufferSize, compress,
-            options.minBlockSize);
-    timeWrite(path, appendable, options.keyLength, options.valueLength,
-        options.fileSize);
-  }
-
-  private void readSeqFile(String parameters, boolean delFile)
-      throws IOException {
-    System.out.println("=== SeqFile: Reading (" + parameters + ") === ");
-    Path path = new Path(options.rootDir, "SeqFile.Performance");
-    KVReadable readable =
-        new SeqFileReadable(fs, path, options.osInputBufferSize);
-    timeRead(path, readable);
-    if (delFile) {
-      if (fs.exists(path)) {
-        fs.delete(path, true);
-      }
-    }
-  }
-
-  private void compareRun(String compress) throws IOException {
-    String[] supported = TFile.getSupportedCompressionAlgorithms();
-    boolean proceed = false;
-    for (String c : supported) {
-      if (c.equals(compress)) {
-        proceed = true;
-        break;
-      }
-    }
-
-    if (!proceed) {
-      System.out.println("Skipped for " + compress);
-      return;
-    }
-    
-    options.compress = compress;
-    String parameters = parameters2String(options);
-    createSeqFile(parameters, compress);
-    readSeqFile(parameters, true);
-    createTFile(parameters, compress);
-    readTFile(parameters, true);
-    createTFile(parameters, compress);
-    readTFile(parameters, true);
-    createSeqFile(parameters, compress);
-    readSeqFile(parameters, true);
-  }
-
-  public void testRunComparisons() throws IOException {
-    String[] compresses = new String[] { "none", "lzo", "gz" };
-    for (String compress : compresses) {
-      if (compress.equals("none")) {
-        conf
-            .setInt("tfile.fs.input.buffer.size", options.fsInputBufferSizeNone);
-        conf.setInt("tfile.fs.output.buffer.size",
-            options.fsOutputBufferSizeNone);
-      }
-      else if (compress.equals("lzo")) {
-        conf.setInt("tfile.fs.input.buffer.size", options.fsInputBufferSizeLzo);
-        conf.setInt("tfile.fs.output.buffer.size",
-            options.fsOutputBufferSizeLzo);
-      }
-      else {
-        conf.setInt("tfile.fs.input.buffer.size", options.fsInputBufferSizeGz);
-        conf
-            .setInt("tfile.fs.output.buffer.size", options.fsOutputBufferSizeGz);
-      }
-      compareRun(compress);
-    }
-  }
-
-  private static String parameters2String(MyOptions options) {
-    return String
-        .format(
-            "KLEN: %d-%d... VLEN: %d-%d...MinBlkSize: %.2fKB...Target Size: %.2fMB...Compression: ...%s",
-            options.keyLength, options.keyLength * 2, options.valueLength,
-            options.valueLength * 2, (double) options.minBlockSize / 1024,
-            (double) options.fileSize / 1024 / 1024, options.compress);
-  }
-
-  private static class MyOptions {
-    String rootDir =
-        System
-            .getProperty("test.build.data", "/tmp/tfile-test");
-    String compress = "gz";
-    String format = "tfile";
-    int dictSize = 1000;
-    int minWordLen = 5;
-    int maxWordLen = 20;
-    int keyLength = 50;
-    int valueLength = 100;
-    int minBlockSize = 256 * 1024;
-    int fsOutputBufferSize = 1;
-    int fsInputBufferSize = 0;
-    // special variable only for unit testing.
-    int fsInputBufferSizeNone = 0;
-    int fsInputBufferSizeGz = 0;
-    int fsInputBufferSizeLzo = 0;
-    int fsOutputBufferSizeNone = 1;
-    int fsOutputBufferSizeGz = 1;
-    int fsOutputBufferSizeLzo = 1;
-
-    // un-exposed parameters.
-    int osInputBufferSize = 64 * 1024;
-    int osOutputBufferSize = 64 * 1024;
-
-    long fileSize = 3 * 1024 * 1024;
-    long seed;
-
-    static final int OP_CREATE = 1;
-    static final int OP_READ = 2;
-    int op = OP_READ;
-
-    boolean proceed = false;
-
-    public MyOptions(String[] args) {
-      seed = System.nanoTime();
-
-      try {
-        Options opts = buildOptions();
-        CommandLineParser parser = new GnuParser();
-        CommandLine line = parser.parse(opts, args, true);
-        processOptions(line, opts);
-        validateOptions();
-      }
-      catch (ParseException e) {
-        System.out.println(e.getMessage());
-        System.out.println("Try \"--help\" option for details.");
-        setStopProceed();
-      }
-    }
-
-    public boolean proceed() {
-      return proceed;
-    }
-
-    private Options buildOptions() {
-      Option compress =
-          OptionBuilder.withLongOpt("compress").withArgName("[none|lzo|gz]")
-              .hasArg().withDescription("compression scheme").create('c');
-
-      Option ditSize =
-          OptionBuilder.withLongOpt("dict").withArgName("size").hasArg()
-              .withDescription("number of dictionary entries").create('d');
-
-      Option fileSize =
-          OptionBuilder.withLongOpt("file-size").withArgName("size-in-MB")
-              .hasArg().withDescription("target size of the file (in MB).")
-              .create('s');
-
-      Option format =
-          OptionBuilder.withLongOpt("format").withArgName("[tfile|seqfile]")
-              .hasArg().withDescription("choose TFile or SeqFile").create('f');
-
-      Option fsInputBufferSz =
-          OptionBuilder.withLongOpt("fs-input-buffer").withArgName("size")
-              .hasArg().withDescription(
-                  "size of the file system input buffer (in bytes).").create(
-                  'i');
-
-      Option fsOutputBufferSize =
-          OptionBuilder.withLongOpt("fs-output-buffer").withArgName("size")
-              .hasArg().withDescription(
-                  "size of the file system output buffer (in bytes).").create(
-                  'o');
-
-      Option keyLen =
-          OptionBuilder
-              .withLongOpt("key-length")
-              .withArgName("length")
-              .hasArg()
-              .withDescription(
-                  "base length of the key (in bytes), actual length varies in [base, 2*base)")
-              .create('k');
-
-      Option valueLen =
-          OptionBuilder
-              .withLongOpt("value-length")
-              .withArgName("length")
-              .hasArg()
-              .withDescription(
-                  "base length of the value (in bytes), actual length varies in [base, 2*base)")
-              .create('v');
-
-      Option wordLen =
-          OptionBuilder.withLongOpt("word-length").withArgName("min,max")
-              .hasArg().withDescription(
-                  "range of dictionary word length (in bytes)").create('w');
-
-      Option blockSz =
-          OptionBuilder.withLongOpt("block").withArgName("size-in-KB").hasArg()
-              .withDescription("minimum block size (in KB)").create('b');
-
-      Option seed =
-          OptionBuilder.withLongOpt("seed").withArgName("long-int").hasArg()
-              .withDescription("specify the seed").create('S');
-
-      Option operation =
-          OptionBuilder.withLongOpt("operation").withArgName("r|w|rw").hasArg()
-              .withDescription(
-                  "action: read-only, create-only, read-after-create").create(
-                  'x');
-
-      Option rootDir =
-          OptionBuilder.withLongOpt("root-dir").withArgName("path").hasArg()
-              .withDescription(
-                  "specify root directory where files will be created.")
-              .create('r');
-
-      Option help =
-          OptionBuilder.withLongOpt("help").hasArg(false).withDescription(
-              "show this screen").create("h");
-
-      return new Options().addOption(compress).addOption(ditSize).addOption(
-          fileSize).addOption(format).addOption(fsInputBufferSz).addOption(
-          fsOutputBufferSize).addOption(keyLen).addOption(wordLen).addOption(
-          blockSz).addOption(rootDir).addOption(valueLen).addOption(operation)
-          .addOption(help);
-
-    }
-
-    private void processOptions(CommandLine line, Options opts)
-        throws ParseException {
-      // --help -h and --version -V must be processed first.
-      if (line.hasOption('h')) {
-        HelpFormatter formatter = new HelpFormatter();
-        System.out.println("TFile and SeqFile benchmark.");
-        System.out.println();
-        formatter.printHelp(100,
-            "java ... TestTFileSeqFileComparison [options]",
-            "\nSupported options:", opts, "");
-        return;
-      }
-
-      if (line.hasOption('c')) {
-        compress = line.getOptionValue('c');
-      }
-
-      if (line.hasOption('d')) {
-        dictSize = Integer.parseInt(line.getOptionValue('d'));
-      }
-
-      if (line.hasOption('s')) {
-        fileSize = Long.parseLong(line.getOptionValue('s')) * 1024 * 1024;
-      }
-
-      if (line.hasOption('f')) {
-        format = line.getOptionValue('f');
-      }
-
-      if (line.hasOption('i')) {
-        fsInputBufferSize = Integer.parseInt(line.getOptionValue('i'));
-      }
-
-      if (line.hasOption('o')) {
-        fsOutputBufferSize = Integer.parseInt(line.getOptionValue('o'));
-      }
-
-      if (line.hasOption('k')) {
-        keyLength = Integer.parseInt(line.getOptionValue('k'));
-      }
-
-      if (line.hasOption('v')) {
-        valueLength = Integer.parseInt(line.getOptionValue('v'));
-      }
-
-      if (line.hasOption('b')) {
-        minBlockSize = Integer.parseInt(line.getOptionValue('b')) * 1024;
-      }
-
-      if (line.hasOption('r')) {
-        rootDir = line.getOptionValue('r');
-      }
-
-      if (line.hasOption('S')) {
-        seed = Long.parseLong(line.getOptionValue('S'));
-      }
-
-      if (line.hasOption('w')) {
-        String min_max = line.getOptionValue('w');
-        StringTokenizer st = new StringTokenizer(min_max, " \t,");
-        if (st.countTokens() != 2) {
-          throw new ParseException("Bad word length specification: " + min_max);
-        }
-        minWordLen = Integer.parseInt(st.nextToken());
-        maxWordLen = Integer.parseInt(st.nextToken());
-      }
-
-      if (line.hasOption('x')) {
-        String strOp = line.getOptionValue('x');
-        if (strOp.equals("r")) {
-          op = OP_READ;
-        }
-        else if (strOp.equals("w")) {
-          op = OP_CREATE;
-        }
-        else if (strOp.equals("rw")) {
-          op = OP_CREATE | OP_READ;
-        }
-        else {
-          throw new ParseException("Unknown action specifier: " + strOp);
-        }
-      }
-
-      proceed = true;
-    }
-
-    private void validateOptions() throws ParseException {
-      if (!compress.equals("none") && !compress.equals("lzo")
-          && !compress.equals("gz")) {
-        throw new ParseException("Unknown compression scheme: " + compress);
-      }
-
-      if (!format.equals("tfile") && !format.equals("seqfile")) {
-        throw new ParseException("Unknown file format: " + format);
-      }
-
-      if (minWordLen >= maxWordLen) {
-        throw new ParseException(
-            "Max word length must be greater than min word length.");
-      }
-      return;
-    }
-
-    private void setStopProceed() {
-      proceed = false;
-    }
-
-    public boolean doCreate() {
-      return (op & OP_CREATE) != 0;
-    }
-
-    public boolean doRead() {
-      return (op & OP_READ) != 0;
-    }
-  }
-
-  public static void main(String[] args) throws IOException {
-    TestTFileSeqFileComparison testCase = new TestTFileSeqFileComparison();
-    MyOptions options = new MyOptions(args);
-    if (options.proceed == false) {
-      return;
-    }
-    testCase.options = options;
-    String parameters = parameters2String(options);
-
-    testCase.setUp();
-    if (testCase.options.format.equals("tfile")) {
-      if (options.doCreate()) {
-        testCase.createTFile(parameters, options.compress);
-      }
-      if (options.doRead()) {
-        testCase.readTFile(parameters, options.doCreate());
-      }
-    }
-    else {
-      if (options.doCreate()) {
-        testCase.createSeqFile(parameters, options.compress);
-      }
-      if (options.doRead()) {
-        testCase.readSeqFile(parameters, options.doCreate());
-      }
-    }
-    testCase.tearDown();
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileSplit.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileSplit.java
deleted file mode 100644
index 826fa97d6..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileSplit.java
+++ /dev/null
@@ -1,192 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.tfile;
-
-import java.io.IOException;
-import java.util.Random;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.tfile.TFile.Reader;
-import org.apache.hadoop.zebra.tfile.TFile.Writer;
-import org.apache.hadoop.zebra.tfile.TFile.Reader.Scanner;
-
-public class TestTFileSplit extends TestCase {
-  private static String ROOT =
-      System.getProperty("test.build.data", "/tmp/tfile-test");
-
-  private final static int BLOCK_SIZE = 64 * 1024;
-
-  private static final String KEY = "key";
-  private static final String VALUE = "value";
-
-  private FileSystem fs;
-  private Configuration conf;
-  private Path path;
-  private Random random = new Random();
-
-  private String comparator = "memcmp";
-  private String outputFile = "TestTFileSplit";
-
-  void createFile(int count, String compress) throws IOException {
-    conf = new Configuration();
-    path = new Path(ROOT, outputFile + "." + compress);
-    fs = path.getFileSystem(conf);
-    FSDataOutputStream out = fs.create(path);
-    Writer writer = new Writer(out, BLOCK_SIZE, compress, comparator, conf);
-
-    int nx;
-    for (nx = 0; nx < count; nx++) {
-      byte[] key = composeSortedKey(KEY, count, nx).getBytes();
-      byte[] value = (VALUE + nx).getBytes();
-      writer.append(key, value);
-    }
-    writer.close();
-    out.close();
-  }
-
-  void readFile() throws IOException {
-    long fileLength = fs.getFileStatus(path).getLen();
-    int numSplit = 10;
-    long splitSize = fileLength / numSplit + 1;
-
-    Reader reader =
-        new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);
-    long offset = 0;
-    long rowCount = 0;
-    BytesWritable key, value;
-    for (int i = 0; i < numSplit; ++i, offset += splitSize) {
-      Scanner scanner = reader.createScannerByByteRange(offset, splitSize);
-      int count = 0;
-      key = new BytesWritable();
-      value = new BytesWritable();
-      while (!scanner.atEnd()) {
-        scanner.entry().get(key, value);
-        ++count;
-        scanner.advance();
-      }
-      scanner.close();
-      Assert.assertTrue(count > 0);
-      rowCount += count;
-    }
-    Assert.assertEquals(rowCount, reader.getEntryCount());
-    reader.close();
-  }
-
-  /* Similar to readFile(), tests the scanner created 
-   * by record numbers rather than the offsets.
-   */
-  void readRowSplits(int numSplits) throws IOException {
-
-    Reader reader =
-      new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);
-    
-    long totalRecords = reader.getEntryCount();
-    for (int i=0; i<numSplits; i++) {
-      long startRec = i*totalRecords/numSplits;
-      long endRec = (i+1)*totalRecords/numSplits;
-      if (i == numSplits-1) {
-        endRec = totalRecords;
-      }
-      Scanner scanner = reader.createScannerByRecordNum(startRec, endRec);
-      int count = 0;
-      BytesWritable key = new BytesWritable();
-      BytesWritable value = new BytesWritable();
-      long x=startRec;
-      while (!scanner.atEnd()) {
-        assertEquals("Incorrect RecNum returned by scanner", scanner.getRecordNum(), x);
-        scanner.entry().get(key, value);
-        ++count;
-        assertEquals("Incorrect RecNum returned by scanner", scanner.getRecordNum(), x);
-        scanner.advance();
-        ++x;
-      }
-      scanner.close();
-      Assert.assertTrue(count == (endRec - startRec));
-    }
-    // make sure specifying range at the end gives zero records.
-    Scanner scanner = reader.createScannerByRecordNum(totalRecords, -1);
-    Assert.assertTrue(scanner.atEnd());
-  }
-  
-  static String composeSortedKey(String prefix, int total, int value) {
-    return String.format("%s%010d", prefix, value);
-  }
-  
-  void checkRecNums() throws IOException {
-    long fileLen = fs.getFileStatus(path).getLen();
-    Reader reader = new Reader(fs.open(path), fileLen, conf);
-    long totalRecs = reader.getEntryCount();
-    long begin = random.nextLong() % (totalRecs / 2);
-    if (begin < 0)
-      begin += (totalRecs / 2);
-    long end = random.nextLong() % (totalRecs / 2);
-    if (end < 0)
-      end += (totalRecs / 2);
-    end += (totalRecs / 2) + 1;
-
-    assertEquals("RecNum for offset=0 should be 0", 0, reader
-        .getRecordNumNear(0));
-    for (long x : new long[] { fileLen, fileLen + 1, 2 * fileLen }) {
-      assertEquals("RecNum for offset>=fileLen should be total entries",
-          totalRecs, reader.getRecordNumNear(x));
-    }
-
-    for (long i = 0; i < 100; ++i) {
-      assertEquals("Locaton to RecNum conversion not symmetric", i, reader
-          .getRecordNumByLocation(reader.getLocationByRecordNum(i)));
-    }
-
-    for (long i = 1; i < 100; ++i) {
-      long x = totalRecs - i;
-      assertEquals("Locaton to RecNum conversion not symmetric", x, reader
-          .getRecordNumByLocation(reader.getLocationByRecordNum(x)));
-    }
-
-    for (long i = begin; i < end; ++i) {
-      assertEquals("Locaton to RecNum conversion not symmetric", i, reader
-          .getRecordNumByLocation(reader.getLocationByRecordNum(i)));
-    }
-
-    for (int i = 0; i < 1000; ++i) {
-      long x = random.nextLong() % totalRecs;
-      if (x < 0) x += totalRecs;
-      assertEquals("Locaton to RecNum conversion not symmetric", x, reader
-          .getRecordNumByLocation(reader.getLocationByRecordNum(x)));
-    }
-  }
-  
-  public void testSplit() throws IOException {
-    System.out.println("testSplit");
-    createFile(100000, Compression.Algorithm.NONE.getName());
-    checkRecNums();   
-    readFile();
-    readRowSplits(10);
-    fs.delete(path, true);
-    createFile(500000, Compression.Algorithm.GZ.getName());
-    checkRecNums();
-    readFile();
-    readRowSplits(83);
-    fs.delete(path, true);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileStreams.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileStreams.java
deleted file mode 100644
index 79692b8e0..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileStreams.java
+++ /dev/null
@@ -1,423 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-package org.apache.hadoop.zebra.tfile;
-
-import java.io.DataOutputStream;
-import java.io.EOFException;
-import java.io.IOException;
-import java.util.Random;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.WritableUtils;
-import org.apache.hadoop.zebra.tfile.TFile.Reader;
-import org.apache.hadoop.zebra.tfile.TFile.Writer;
-import org.apache.hadoop.zebra.tfile.TFile.Reader.Scanner;
-
-/**
- * 
- * Streaming interfaces test case class using GZ compression codec, base class
- * of none and LZO compression classes.
- * 
- */
-
-public class TestTFileStreams extends TestCase {
-  private static String ROOT =
-      System.getProperty("test.build.data", "/tmp/tfile-test");
-
-  private final static int BLOCK_SIZE = 512;
-  private final static int K = 1024;
-  private final static int M = K * K;
-  protected boolean skip = false;
-  private FileSystem fs;
-  private Configuration conf;
-  private Path path;
-  private FSDataOutputStream out;
-  Writer writer;
-
-  private String compression = Compression.Algorithm.GZ.getName();
-  private String comparator = "memcmp";
-  private String outputFile = "TFileTestStreams";
-
-  public void init(String compression, String comparator, String outputFile) {
-    this.compression = compression;
-    this.comparator = comparator;
-    this.outputFile = outputFile;
-  }
-
-  @Override
-  public void setUp() throws IOException {
-    conf = new Configuration();
-    path = new Path(ROOT, outputFile);
-    fs = path.getFileSystem(conf);
-    out = fs.create(path);
-    writer = new Writer(out, BLOCK_SIZE, compression, comparator, conf);
-  }
-
-  @Override
-  public void tearDown() throws IOException {
-    if (!skip) {
-      try {
-        closeOutput();
-      } catch (Exception e) {
-        // no-op
-      }
-      fs.delete(path, true);
-    }
-  }
-
-  public void testNoEntry() throws IOException {
-    if (skip)
-      return;
-    closeOutput();
-    TestTFileByteArrays.readRecords(fs, path, 0, conf);
-  }
-
-  public void testOneEntryKnownLength() throws IOException {
-    if (skip)
-      return;
-    writeRecords(1, true, true);
-
-    TestTFileByteArrays.readRecords(fs, path, 1, conf);
-  }
-
-  public void testOneEntryUnknownLength() throws IOException {
-    if (skip)
-      return;
-    writeRecords(1, false, false);
-
-    // TODO: will throw exception at getValueLength, it's inconsistent though;
-    // getKeyLength returns a value correctly, though initial length is -1
-    TestTFileByteArrays.readRecords(fs, path, 1, conf);
-  }
-
-  // known key length, unknown value length
-  public void testOneEntryMixedLengths1() throws IOException {
-    if (skip)
-      return;
-    writeRecords(1, true, false);
-
-    TestTFileByteArrays.readRecords(fs, path, 1, conf);
-  }
-
-  // unknown key length, known value length
-  public void testOneEntryMixedLengths2() throws IOException {
-    if (skip)
-      return;
-    writeRecords(1, false, true);
-
-    TestTFileByteArrays.readRecords(fs, path, 1, conf);
-  }
-
-  public void testTwoEntriesKnownLength() throws IOException {
-    if (skip)
-      return;
-    writeRecords(2, true, true);
-
-    TestTFileByteArrays.readRecords(fs, path, 2, conf);
-  }
-
-  // Negative test
-  public void testFailureAddKeyWithoutValue() throws IOException {
-    if (skip)
-      return;
-    DataOutputStream dos = writer.prepareAppendKey(-1);
-    dos.write("key0".getBytes());
-    try {
-      closeOutput();
-      fail("Cannot add only a key without a value. ");
-    }
-    catch (IllegalStateException e) {
-      // noop, expecting an exception
-    }
-  }
-
-  public void testFailureAddValueWithoutKey() throws IOException {
-    if (skip)
-      return;
-    DataOutputStream outValue = null;
-    try {
-      outValue = writer.prepareAppendValue(6);
-      outValue.write("value0".getBytes());
-      fail("Cannot add a value without adding key first. ");
-    }
-    catch (Exception e) {
-      // noop, expecting an exception
-    }
-    finally {
-      if (outValue != null) {
-        outValue.close();
-      }
-    }
-  }
-
-  public void testFailureOneEntryKnownLength() throws IOException {
-    if (skip)
-      return;
-    DataOutputStream outKey = writer.prepareAppendKey(2);
-    try {
-      outKey.write("key0".getBytes());
-      fail("Specified key length mismatched the actual key length.");
-    }
-    catch (IOException e) {
-      // noop, expecting an exception
-    }
-
-    DataOutputStream outValue = null;
-    try {
-      outValue = writer.prepareAppendValue(6);
-      outValue.write("value0".getBytes());
-    }
-    catch (Exception e) {
-      // noop, expecting an exception
-    }
-  }
-
-  public void testFailureKeyTooLong() throws IOException {
-    if (skip)
-      return;
-    DataOutputStream outKey = writer.prepareAppendKey(2);
-    try {
-      outKey.write("key0".getBytes());
-      outKey.close();
-      Assert.fail("Key is longer than requested.");
-    }
-    catch (Exception e) {
-      // noop, expecting an exception
-    }
-    finally {
-    }
-  }
-
-  public void testFailureKeyTooShort() throws IOException {
-    if (skip)
-      return;
-    DataOutputStream outKey = writer.prepareAppendKey(4);
-    outKey.write("key0".getBytes());
-    outKey.close();
-    DataOutputStream outValue = writer.prepareAppendValue(15);
-    try {
-      outValue.write("value0".getBytes());
-      outValue.close();
-      Assert.fail("Value is shorter than expected.");
-    }
-    catch (Exception e) {
-      // noop, expecting an exception
-    }
-    finally {
-    }
-  }
-
-  public void testFailureValueTooLong() throws IOException {
-    if (skip)
-      return;
-    DataOutputStream outKey = writer.prepareAppendKey(4);
-    outKey.write("key0".getBytes());
-    outKey.close();
-    DataOutputStream outValue = writer.prepareAppendValue(3);
-    try {
-      outValue.write("value0".getBytes());
-      outValue.close();
-      Assert.fail("Value is longer than expected.");
-    }
-    catch (Exception e) {
-      // noop, expecting an exception
-    }
-
-    try {
-      outKey.close();
-      outKey.close();
-    }
-    catch (Exception e) {
-      Assert.fail("Second or more close() should have no effect.");
-    }
-  }
-
-  public void testFailureValueTooShort() throws IOException {
-    if (skip)
-      return;
-    DataOutputStream outKey = writer.prepareAppendKey(8);
-    try {
-      outKey.write("key0".getBytes());
-      outKey.close();
-      Assert.fail("Key is shorter than expected.");
-    }
-    catch (Exception e) {
-      // noop, expecting an exception
-    }
-    finally {
-    }
-  }
-
-  public void testFailureCloseKeyStreamManyTimesInWriter() throws IOException {
-    if (skip)
-      return;
-    DataOutputStream outKey = writer.prepareAppendKey(4);
-    try {
-      outKey.write("key0".getBytes());
-      outKey.close();
-    }
-    catch (Exception e) {
-      // noop, expecting an exception
-    }
-    finally {
-      try {
-        outKey.close();
-      }
-      catch (Exception e) {
-        // no-op
-      }
-    }
-    outKey.close();
-    outKey.close();
-    Assert.assertTrue("Multiple close should have no effect.", true);
-  }
-
-  public void testFailureKeyLongerThan64K() throws IOException {
-    if (skip)
-      return;
-    try {
-      DataOutputStream outKey = writer.prepareAppendKey(64 * K + 1);
-      Assert.fail("Failed to handle key longer than 64K.");
-    }
-    catch (IndexOutOfBoundsException e) {
-      // noop, expecting exceptions
-    }
-    closeOutput();
-  }
-
-  public void testFailureKeyLongerThan64K_2() throws IOException {
-    if (skip)
-      return;
-    DataOutputStream outKey = writer.prepareAppendKey(-1);
-    try {
-      byte[] buf = new byte[K];
-      Random rand = new Random();
-      for (int nx = 0; nx < K + 2; nx++) {
-        rand.nextBytes(buf);
-        outKey.write(buf);
-      }
-      outKey.close();
-      Assert.fail("Failed to handle key longer than 64K.");
-    }
-    catch (EOFException e) {
-      // noop, expecting exceptions
-    }
-    finally {
-      try {
-        closeOutput();
-      }
-      catch (Exception e) {
-        // no-op
-      }
-    }
-  }
-
-  public void testFailureNegativeOffset() throws IOException {
-    if (skip)
-      return;
-    writeRecords(2, true, true);
-
-    Reader reader = new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);
-    Scanner scanner = reader.createScanner();
-    byte[] buf = new byte[K];
-    try {
-      scanner.entry().getKey(buf, -1);
-      Assert.fail("Failed to handle key negative offset.");
-    }
-    catch (Exception e) {
-      // noop, expecting exceptions
-    }
-    finally {
-    }
-    scanner.close();
-    reader.close();
-  }
-
-  /**
-   * Verify that the compressed data size is less than raw data size.
-   * 
-   * @throws IOException
-   */
-  public void testFailureCompressionNotWorking() throws IOException {
-    if (skip)
-      return;
-    long rawDataSize = writeRecords(10000, false, false, false);
-    if (!compression.equalsIgnoreCase(Compression.Algorithm.NONE.getName())) {
-      Assert.assertTrue(out.getPos() < rawDataSize);
-    }
-    closeOutput();
-  }
-
-  public void testFailureCompressionNotWorking2() throws IOException {
-    if (skip)
-      return;
-    long rawDataSize = writeRecords(10000, true, true, false);
-    if (!compression.equalsIgnoreCase(Compression.Algorithm.NONE.getName())) {
-      Assert.assertTrue(out.getPos() < rawDataSize);
-    }
-    closeOutput();
-  }
-
-  private long writeRecords(int count, boolean knownKeyLength,
-      boolean knownValueLength, boolean close) throws IOException {
-    long rawDataSize = 0;
-    for (int nx = 0; nx < count; nx++) {
-      String key = TestTFileByteArrays.composeSortedKey("key", count, nx);
-      DataOutputStream outKey =
-          writer.prepareAppendKey(knownKeyLength ? key.length() : -1);
-      outKey.write(key.getBytes());
-      outKey.close();
-      String value = "value" + nx;
-      DataOutputStream outValue =
-          writer.prepareAppendValue(knownValueLength ? value.length() : -1);
-      outValue.write(value.getBytes());
-      outValue.close();
-      rawDataSize +=
-          WritableUtils.getVIntSize(key.getBytes().length)
-              + key.getBytes().length
-              + WritableUtils.getVIntSize(value.getBytes().length)
-              + value.getBytes().length;
-    }
-    if (close) {
-      closeOutput();
-    }
-    return rawDataSize;
-  }
-
-  private long writeRecords(int count, boolean knownKeyLength,
-      boolean knownValueLength) throws IOException {
-    return writeRecords(count, knownKeyLength, knownValueLength, true);
-  }
-
-  private void closeOutput() throws IOException {
-    if (writer != null) {
-      writer.close();
-      writer = null;
-    }
-    if (out != null) {
-      out.close();
-      out = null;
-    }
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileUnsortedByteArrays.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileUnsortedByteArrays.java
deleted file mode 100644
index 90e7b7c9d..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestTFileUnsortedByteArrays.java
+++ /dev/null
@@ -1,238 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-package org.apache.hadoop.zebra.tfile;
-
-import java.io.IOException;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.zebra.tfile.TFile.Reader;
-import org.apache.hadoop.zebra.tfile.TFile.Writer;
-import org.apache.hadoop.zebra.tfile.TFile.Reader.Scanner;
-
-public class TestTFileUnsortedByteArrays extends TestCase {
-  private static String ROOT =
-      System.getProperty("test.build.data", "/tmp/tfile-test");
-
-
-  private final static int BLOCK_SIZE = 512;
-  private final static int BUF_SIZE = 64;
-
-  private FileSystem fs;
-  private Configuration conf;
-  private Path path;
-  private FSDataOutputStream out;
-  private Writer writer;
-
-  private String compression = Compression.Algorithm.GZ.getName();
-  private String outputFile = "TFileTestUnsorted";
-  /*
-   * pre-sampled numbers of records in one block, based on the given the
-   * generated key and value strings
-   */
-  private int records1stBlock = 4314;
-  private int records2ndBlock = 4108;
-
-  public void init(String compression, String outputFile,
-      int numRecords1stBlock, int numRecords2ndBlock) {
-    this.compression = compression;
-    this.outputFile = outputFile;
-    this.records1stBlock = numRecords1stBlock;
-    this.records2ndBlock = numRecords2ndBlock;
-  }
-
-  @Override
-  public void setUp() throws IOException {
-    conf = new Configuration();
-    path = new Path(ROOT, outputFile);
-    fs = path.getFileSystem(conf);
-    out = fs.create(path);
-    writer = new Writer(out, BLOCK_SIZE, compression, null, conf);
-    writer.append("keyZ".getBytes(), "valueZ".getBytes());
-    writer.append("keyM".getBytes(), "valueM".getBytes());
-    writer.append("keyN".getBytes(), "valueN".getBytes());
-    writer.append("keyA".getBytes(), "valueA".getBytes());
-    closeOutput();
-  }
-
-  @Override
-  public void tearDown() throws IOException {
-    fs.delete(path, true);
-  }
-
-  // we still can scan records in an unsorted TFile
-  public void testFailureScannerWithKeys() throws IOException {
-    Reader reader =
-        new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);
-    Assert.assertFalse(reader.isSorted());
-    Assert.assertEquals((int) reader.getEntryCount(), 4);
-
-    try {
-      Scanner scanner =
-          reader.createScannerByKey("aaa".getBytes(), "zzz".getBytes());
-      Assert
-          .fail("Failed to catch creating scanner with keys on unsorted file.");
-    }
-    catch (RuntimeException e) {
-    }
-    finally {
-      reader.close();
-    }
-  }
-
-  // we still can scan records in an unsorted TFile
-  public void testScan() throws IOException {
-    Reader reader =
-        new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);
-    Assert.assertFalse(reader.isSorted());
-    Assert.assertEquals((int) reader.getEntryCount(), 4);
-
-    Scanner scanner = reader.createScanner();
-
-    try {
-
-      // read key and value
-      byte[] kbuf = new byte[BUF_SIZE];
-      int klen = scanner.entry().getKeyLength();
-      scanner.entry().getKey(kbuf);
-      Assert.assertEquals(new String(kbuf, 0, klen), "keyZ");
-
-      byte[] vbuf = new byte[BUF_SIZE];
-      int vlen = scanner.entry().getValueLength();
-      scanner.entry().getValue(vbuf);
-      Assert.assertEquals(new String(vbuf, 0, vlen), "valueZ");
-
-      scanner.advance();
-
-      // now try get value first
-      vbuf = new byte[BUF_SIZE];
-      vlen = scanner.entry().getValueLength();
-      scanner.entry().getValue(vbuf);
-      Assert.assertEquals(new String(vbuf, 0, vlen), "valueM");
-
-      kbuf = new byte[BUF_SIZE];
-      klen = scanner.entry().getKeyLength();
-      scanner.entry().getKey(kbuf);
-      Assert.assertEquals(new String(kbuf, 0, klen), "keyM");
-    }
-    finally {
-      scanner.close();
-      reader.close();
-    }
-  }
-
-  // we still can scan records in an unsorted TFile
-  public void testScanRange() throws IOException {
-    Reader reader =
-        new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);
-    Assert.assertFalse(reader.isSorted());
-    Assert.assertEquals((int) reader.getEntryCount(), 4);
-
-    Scanner scanner = reader.createScanner();
-
-    try {
-
-      // read key and value
-      byte[] kbuf = new byte[BUF_SIZE];
-      int klen = scanner.entry().getKeyLength();
-      scanner.entry().getKey(kbuf);
-      Assert.assertEquals(new String(kbuf, 0, klen), "keyZ");
-
-      byte[] vbuf = new byte[BUF_SIZE];
-      int vlen = scanner.entry().getValueLength();
-      scanner.entry().getValue(vbuf);
-      Assert.assertEquals(new String(vbuf, 0, vlen), "valueZ");
-
-      scanner.advance();
-
-      // now try get value first
-      vbuf = new byte[BUF_SIZE];
-      vlen = scanner.entry().getValueLength();
-      scanner.entry().getValue(vbuf);
-      Assert.assertEquals(new String(vbuf, 0, vlen), "valueM");
-
-      kbuf = new byte[BUF_SIZE];
-      klen = scanner.entry().getKeyLength();
-      scanner.entry().getKey(kbuf);
-      Assert.assertEquals(new String(kbuf, 0, klen), "keyM");
-    }
-    finally {
-      scanner.close();
-      reader.close();
-    }
-  }
-
-  public void testFailureSeek() throws IOException {
-    Reader reader =
-        new Reader(fs.open(path), fs.getFileStatus(path).getLen(), conf);
-    Scanner scanner = reader.createScanner();
-
-    try {
-      // can't find ceil
-      try {
-        scanner.lowerBound("keyN".getBytes());
-        Assert.fail("Cannot search in a unsorted TFile!");
-      }
-      catch (Exception e) {
-        // noop, expecting excetions
-      }
-      finally {
-      }
-
-      // can't find higher
-      try {
-        scanner.upperBound("keyA".getBytes());
-        Assert.fail("Cannot search higher in a unsorted TFile!");
-      }
-      catch (Exception e) {
-        // noop, expecting excetions
-      }
-      finally {
-      }
-
-      // can't seek
-      try {
-        scanner.seekTo("keyM".getBytes());
-        Assert.fail("Cannot search a unsorted TFile!");
-      }
-      catch (Exception e) {
-        // noop, expecting excetions
-      }
-      finally {
-      }
-    }
-    finally {
-      scanner.close();
-      reader.close();
-    }
-  }
-
-  private void closeOutput() throws IOException {
-    if (writer != null) {
-      writer.close();
-      writer = null;
-      out.close();
-      out = null;
-    }
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestVLong.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestVLong.java
deleted file mode 100644
index e071ddc27..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/TestVLong.java
+++ /dev/null
@@ -1,161 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.tfile;
-
-import java.io.IOException;
-import java.util.Random;
-
-import junit.framework.Assert;
-import junit.framework.TestCase;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-
-public class TestVLong extends TestCase {
-  private static String ROOT =
-      System.getProperty("test.build.data", "/tmp/tfile-test");
-  private Configuration conf;
-  private FileSystem fs;
-  private Path path;
-  private String outputFile = "TestVLong";
-
-  @Override
-  public void setUp() throws IOException {
-    conf = new Configuration();
-    path = new Path(ROOT, outputFile);
-    fs = path.getFileSystem(conf);
-    if (fs.exists(path)) {
-      fs.delete(path, false);
-    }
-  }
-
-  @Override
-  public void tearDown() throws IOException {
-    if (fs.exists(path)) {
-      fs.delete(path, false);
-    }
-  }
-
-  public void testVLongByte() throws IOException {
-    FSDataOutputStream out = fs.create(path);
-    for (int i = Byte.MIN_VALUE; i <= Byte.MAX_VALUE; ++i) {
-      Utils.writeVLong(out, i);
-    }
-    out.close();
-    Assert.assertEquals("Incorrect encoded size", (1 << Byte.SIZE) + 96, fs
-        .getFileStatus(
-        path).getLen());
-
-    FSDataInputStream in = fs.open(path);
-    for (int i = Byte.MIN_VALUE; i <= Byte.MAX_VALUE; ++i) {
-      long n = Utils.readVLong(in);
-      Assert.assertEquals(n, i);
-    }
-    in.close();
-    fs.delete(path, false);
-  }
-
-  private long writeAndVerify(int shift) throws IOException {
-    FSDataOutputStream out = fs.create(path);
-    for (int i = Short.MIN_VALUE; i <= Short.MAX_VALUE; ++i) {
-      Utils.writeVLong(out, ((long) i) << shift);
-    }
-    out.close();
-    FSDataInputStream in = fs.open(path);
-    for (int i = Short.MIN_VALUE; i <= Short.MAX_VALUE; ++i) {
-      long n = Utils.readVLong(in);
-      Assert.assertEquals(n, ((long) i) << shift);
-    }
-    in.close();
-    long ret = fs.getFileStatus(path).getLen();
-    fs.delete(path, false);
-    return ret;
-  }
-  
-  public void testVLongShort() throws IOException {
-    long size = writeAndVerify(0);
-    Assert.assertEquals("Incorrect encoded size", (1 << Short.SIZE) * 2
-        + ((1 << Byte.SIZE) - 40)
-        * (1 << Byte.SIZE) - 128 - 32, size);
-  }
-
-  public void testVLong3Bytes() throws IOException {
-    long size = writeAndVerify(Byte.SIZE);
-    Assert.assertEquals("Incorrect encoded size", (1 << Short.SIZE) * 3
-        + ((1 << Byte.SIZE) - 32) * (1 << Byte.SIZE) - 40 - 1, size);
-  }
-
-  public void testVLong4Bytes() throws IOException {
-    long size = writeAndVerify(Byte.SIZE * 2);
-    Assert.assertEquals("Incorrect encoded size", (1 << Short.SIZE) * 4
-        + ((1 << Byte.SIZE) - 16) * (1 << Byte.SIZE) - 32 - 2, size);
-  }
-
-  public void testVLong5Bytes() throws IOException {
-    long size = writeAndVerify(Byte.SIZE * 3);
-     Assert.assertEquals("Incorrect encoded size", (1 << Short.SIZE) * 6 - 256
-        - 16 - 3, size);
-  }
-
-  private void verifySixOrMoreBytes(int bytes) throws IOException {
-    long size = writeAndVerify(Byte.SIZE * (bytes - 2));
-    Assert.assertEquals("Incorrect encoded size", (1 << Short.SIZE)
-        * (bytes + 1) - 256 - bytes + 1, size);
-  }
-  public void testVLong6Bytes() throws IOException {
-    verifySixOrMoreBytes(6);
-  }
-  
-  public void testVLong7Bytes() throws IOException {
-    verifySixOrMoreBytes(7);
-  }
-
-  public void testVLong8Bytes() throws IOException {
-    verifySixOrMoreBytes(8);
-  }
-
-  public void testVLongRandom() throws IOException {
-    int count = 1024 * 1024;
-    long data[] = new long[count];
-    Random rng = new Random();
-    for (int i = 0; i < data.length; ++i) {
-      int shift = rng.nextInt(Long.SIZE) + 1;
-      long mask = (1L << shift) - 1;
-      long a = ((long) rng.nextInt()) << 32;
-      long b = ((long) rng.nextInt()) & 0xffffffff;
-      data[i] = (a + b) & mask;
-    }
-    
-    FSDataOutputStream out = fs.create(path);
-    for (int i = 0; i < data.length; ++i) {
-      Utils.writeVLong(out, data[i]);
-    }
-    out.close();
-
-    FSDataInputStream in = fs.open(path);
-    for (int i = 0; i < data.length; ++i) {
-      Assert.assertEquals(Utils.readVLong(in), data[i]);
-    }
-    in.close();
-    fs.delete(path, false);
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/Timer.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/Timer.java
deleted file mode 100644
index f4fff5236..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/tfile/Timer.java
+++ /dev/null
@@ -1,63 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.tfile;
-
-import java.io.IOException;
-import java.text.DateFormat;
-import java.text.SimpleDateFormat;
-
-/**
- * this class is a time class to 
- * measure to measure the time 
- * taken for some event.
- */
-public  class Timer {
-  long startTimeEpoch;
-  long finishTimeEpoch;
-  private DateFormat formatter = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
-  
-  public void startTime() throws IOException {
-      startTimeEpoch = System.currentTimeMillis();
-    }
-
-    public void stopTime() throws IOException {
-      finishTimeEpoch = System.currentTimeMillis();
-    }
-
-    public long getIntervalMillis() throws IOException {
-      return finishTimeEpoch - startTimeEpoch;
-    }
-  
-    public void printlnWithTimestamp(String message) throws IOException {
-      System.out.println(formatCurrentTime() + "  " + message);
-    }
-  
-    public String formatTime(long millis) {
-      return formatter.format(millis);
-    }
-    
-    public String getIntervalString() throws IOException {
-      long time = getIntervalMillis();
-      return formatTime(time);
-    }
-    
-    public String formatCurrentTime() {
-      return formatTime(System.currentTimeMillis());
-    }
-
-}
-
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestCheckin.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestCheckin.java
deleted file mode 100644
index 348e17afa..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestCheckin.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.types;
-
-import org.junit.runner.RunWith;
-import org.junit.runners.Suite;
- 
-@RunWith(Suite.class)
-@Suite.SuiteClasses({
-  TestColumnGroupName.class,
-  TestSchemaCollection.class,
-  TestSchemaMap.class,
-  TestSchemaPrimitive.class,
-  TestSchemaRecord.class,
-  TestStorageCollection.class,
-  TestStorageMap.class,
-  TestStorageMisc1.class,
-  TestStorageMisc2.class,
-  TestStorageMisc3.class,
-  TestStorageRecord.class,
-  TestStorePrimitive.class,
-  TestZebraTupleTostring.class
-})
-
-public class TestCheckin {
-  // the class remains completely empty, 
-  // being used only as a holder for the above annotations
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestColumnGroupName.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestColumnGroupName.java
deleted file mode 100644
index ddf24d796..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestColumnGroupName.java
+++ /dev/null
@@ -1,369 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.types;
-
-import java.io.StringReader;
-import java.io.IOException;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.HashSet;
-import junit.framework.Assert;
-
-import org.apache.hadoop.zebra.types.CGSchema;
-import org.apache.hadoop.zebra.schema.ColumnType;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Partition;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.parser.TableSchemaParser;
-import org.apache.hadoop.zebra.schema.Schema.ColumnSchema;
-import org.junit.Before;
-import org.junit.Test;
-
-public class TestColumnGroupName {
-  String strSch = "f1:int, f2:long, f3:float, f4:bool, f5:string, f6:bytes";
-  TableSchemaParser parser;
-  Schema schema;
-
-  @Before
-  public void init() throws ParseException {
-    parser = new TableSchemaParser(new StringReader(strSch));
-    schema = parser.RecordSchema(null);
-  }
-
-  @Test
-  public void testSchema() throws ParseException {
-    ColumnSchema f1 = schema.getColumn(0);
-    Assert.assertEquals("f1", f1.getName());
-    Assert.assertEquals(ColumnType.INT, f1.getType());
-    ColumnSchema f2 = schema.getColumn(1);
-    Assert.assertEquals("f2", f2.getName());
-    Assert.assertEquals(ColumnType.LONG, f2.getType());
-    ColumnSchema f3 = schema.getColumn(2);
-    Assert.assertEquals("f3", f3.getName());
-    Assert.assertEquals(ColumnType.FLOAT, f3.getType());
-    ColumnSchema f4 = schema.getColumn(3);
-    Assert.assertEquals("f4", f4.getName());
-    Assert.assertEquals(ColumnType.BOOL, f4.getType());
-    ColumnSchema f5 = schema.getColumn(4);
-    Assert.assertEquals("f5", f5.getName());
-    Assert.assertEquals(ColumnType.STRING, f5.getType());
-    ColumnSchema f6 = schema.getColumn(5);
-    Assert.assertEquals("f6", f6.getName());
-    Assert.assertEquals(ColumnType.BYTES, f6.getType());
-
-    System.out.println(schema.toString());
-  }
-
-  @Test
-  public void testStorageValid1() {
-    try {
-      String strStorage = "[f1, f2] as PI; [f3, f4] as General secure by uid:joe gid:secure perm:640 COMPRESS BY gz SERIALIZE BY avro; [f5, f6] as ULT";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      CGSchema[] cgschemas = p.getCGSchemas();
-
-      // 3 column group;
-      int size = cgschemas.length;
-      Assert.assertEquals(size, 3);
-      System.out.println("********** Column Groups **********");
-      for (int i = 0; i < cgschemas.length; i++) {
-        System.out.println(cgschemas[i]);
-        System.out.println("--------------------------------");
-      }
-      CGSchema cgs1 = cgschemas[0];
-      CGSchema cgs2 = cgschemas[1];
-      CGSchema cgs3 = cgschemas[2];
-
-      ColumnSchema f11 = cgs1.getSchema().getColumn(0);
-      Assert.assertEquals("f1", f11.getName());
-      Assert.assertEquals(ColumnType.INT, f11.getType());
-      ColumnSchema f12 = cgs1.getSchema().getColumn(1);
-      Assert.assertEquals("f2", f12.getName());
-      Assert.assertEquals(ColumnType.LONG, f12.getType());
-      ColumnSchema f21 = cgs2.getSchema().getColumn(0);
-      Assert.assertEquals("f3", f21.getName());
-      Assert.assertEquals(ColumnType.FLOAT, f21.getType());
-      ColumnSchema f22 = cgs2.getSchema().getColumn(1);
-      Assert.assertEquals("f4", f22.getName());
-      Assert.assertEquals(ColumnType.BOOL, f22.getType());
-      ColumnSchema f31 = cgs3.getSchema().getColumn(0);
-      Assert.assertEquals("f5", f31.getName());
-      Assert.assertEquals(ColumnType.STRING, f31.getType());
-      ColumnSchema f32 = cgs3.getSchema().getColumn(1);
-      Assert.assertEquals("f6", f32.getName());
-      Assert.assertEquals(ColumnType.BYTES, f32.getType());
-
-      Assert.assertEquals(cgs1.getCompressor(), "gz");
-      Assert.assertEquals(cgs1.getSerializer(), "pig");
-      Assert.assertEquals(cgs2.getCompressor(), "gz");
-      Assert.assertEquals(cgs2.getSerializer(), "avro");
-      Assert.assertEquals(cgs3.getCompressor(), "gz");
-      Assert.assertEquals(cgs3.getSerializer(), "pig");
-      
-      //Assert.assertEquals(cgs2.getOwner(), "joe");
-      //Assert.assertEquals(cgs2.getGroup(), "secure");
-      //Assert.assertEquals(cgs2.getPerm(), (short) Short.parseShort("640", 8));
-      Assert.assertEquals(cgs1.getName(), "PI");
-      Assert.assertEquals(cgs2.getName(), "General");
-      Assert.assertEquals(cgs3.getName(), "ULT");
-      
-      System.out.println("*********** Column Map **********");
-      Map<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>> colmap = p
-          .getPartitionInfo().getColMap();
-      Assert.assertEquals(colmap.size(), 6);
-      Iterator<Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>>> it = colmap
-          .entrySet().iterator();
-      for (int i = 0; i < colmap.size(); i++) {
-        Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>> entry = (Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>>) it
-            .next();
-        String name = entry.getKey();
-        HashSet<Partition.PartitionInfo.ColumnMappingEntry> hs = entry
-            .getValue();
-        Iterator<Partition.PartitionInfo.ColumnMappingEntry> it1 = hs
-            .iterator();
-        for (int j = 0; j < hs.size(); j++) {
-          Partition.PartitionInfo.ColumnMappingEntry cme = (Partition.PartitionInfo.ColumnMappingEntry) it1
-              .next();
-          System.out.println("[Column = " + name + " CG = " + cme.getCGIndex()
-              + "." + cme.getFieldIndex() + "]");
-          if (i == 0 && j == 0) {
-            Assert.assertEquals(name, "f6");
-            Assert.assertEquals(cme.getCGIndex(), 2);
-            Assert.assertEquals(cme.getFieldIndex(), 1);
-          } else if (i == 1 && j == 0) {
-            Assert.assertEquals(name, "f1");
-            Assert.assertEquals(cme.getCGIndex(), 0);
-            Assert.assertEquals(cme.getFieldIndex(), 0);
-          } else if (i == 2 && j == 0) {
-            Assert.assertEquals(name, "f3");
-            Assert.assertEquals(cme.getCGIndex(), 1);
-            Assert.assertEquals(cme.getFieldIndex(), 0);
-          } else if (i == 3 && j == 0) {
-            Assert.assertEquals(name, "f2");
-            Assert.assertEquals(cme.getCGIndex(), 0);
-            Assert.assertEquals(cme.getFieldIndex(), 1);
-          } else if (i == 4 && j == 0) {
-            Assert.assertEquals(name, "f5");
-            Assert.assertEquals(cme.getCGIndex(), 2);
-            Assert.assertEquals(cme.getFieldIndex(), 0);
-          } else if (i == 5 && j == 0) {
-            Assert.assertEquals(name, "f4");
-            Assert.assertEquals(cme.getCGIndex(), 1);
-            Assert.assertEquals(cme.getFieldIndex(), 1);
-          }
-        }
-      }
-    } catch (Exception e) {
-      Assert.assertTrue(false);
-    }
-  }
-
-  @Test
-  public void testStorageValid2() {
-    try {
-      String strStorage = "[f1, f2] serialize by avro compress by gz; [f3, f4] SERIALIZE BY avro COMPRESS BY gz; [f5, f6]";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      CGSchema[] cgschemas = p.getCGSchemas();
-
-      Assert.assertEquals(cgschemas.length, 3);
-      CGSchema cgs1 = cgschemas[0];
-      System.out.println(cgs1);
-      CGSchema cgs2 = cgschemas[1];
-      System.out.println(cgs2);
-      CGSchema cgs3 = cgschemas[2];
-      System.out.println(cgs3);
-
-      Assert.assertEquals(cgs1.getName(), "CG0");
-      Assert.assertEquals(cgs2.getName(), "CG1");
-      Assert.assertEquals(cgs3.getName(), "CG2");
-    } catch (Exception e) {
-      e.printStackTrace();
-    }
-  }
-  
-  @Test
-  public void testStorageValid3() {
-    try {
-      String strStorage = "[f1, f2] as PI serialize by avro compress by gz; [f3, f4] as General SERIALIZE BY avro COMPRESS BY gz; [f5, f6]";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      CGSchema[] cgschemas = p.getCGSchemas();
-
-      Assert.assertEquals(cgschemas.length, 3);
-      CGSchema cgs1 = cgschemas[0];
-      System.out.println(cgs1);
-      CGSchema cgs2 = cgschemas[1];
-      System.out.println(cgs2);
-      CGSchema cgs3 = cgschemas[2];
-      System.out.println(cgs3);
-
-      Assert.assertEquals(cgs1.getName(), "PI");
-      Assert.assertEquals(cgs2.getName(), "General");
-      Assert.assertEquals(cgs3.getName(), "CG0");
-    } catch (Exception e) {
-      e.printStackTrace();
-    }
-  }
-  
-  @Test
-  public void testStorageValid4() {
-    try {
-      String strStorage = "[f1, f2] as C1 serialize by avro compress by gz; [f3, f4] as C2 SERIALIZE BY avro COMPRESS BY gz; as C3";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      CGSchema[] cgschemas = p.getCGSchemas();
-
-      Assert.assertEquals(cgschemas.length, 3);
-      CGSchema cgs1 = cgschemas[0];
-      System.out.println(cgs1);
-      CGSchema cgs2 = cgschemas[1];
-      System.out.println(cgs2);
-      CGSchema cgs3 = cgschemas[2];
-      System.out.println(cgs3);
-
-      Assert.assertEquals(cgs1.getName(), "C1");
-      Assert.assertEquals(cgs2.getName(), "C2");
-      Assert.assertEquals(cgs3.getName(), "C3");
-    } catch (Exception e) {
-      e.printStackTrace();
-    }
-  }
-
-  @Test
-  public void testStorageValid5() {
-    try {
-      String strStorage = "[f1, f2] as C1 serialize by avro compress by gz; [f3, f4] as C2 SERIALIZE BY avro COMPRESS BY gz;";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      CGSchema[] cgschemas = p.getCGSchemas();
-
-      Assert.assertEquals(cgschemas.length, 3);
-      CGSchema cgs1 = cgschemas[0];
-      System.out.println(cgs1);
-      CGSchema cgs2 = cgschemas[1];
-      System.out.println(cgs2);
-      CGSchema cgs3 = cgschemas[2];
-      System.out.println(cgs3);
-
-      Assert.assertEquals(cgs1.getName(), "C1");
-      Assert.assertEquals(cgs2.getName(), "C2");
-      Assert.assertEquals(cgs3.getName(), "CG0");
-    } catch (Exception e) {
-      e.printStackTrace();
-    }
-  }
-
-  @Test
-  public void testStorageValid6() {
-    try {
-      String strStorage = "[f1, f2] as PI serialize by avro compress by gz; [f3, f4] SERIALIZE BY avro COMPRESS BY gz; [f5, f6] as CG0";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      CGSchema[] cgschemas = p.getCGSchemas();
-
-      Assert.assertEquals(cgschemas.length, 3);
-      CGSchema cgs1 = cgschemas[0];
-      System.out.println(cgs1);
-      CGSchema cgs2 = cgschemas[1];
-      System.out.println(cgs2);
-      CGSchema cgs3 = cgschemas[2];
-      System.out.println(cgs3);
-
-      Assert.assertEquals(cgs1.getName(), "PI");
-      Assert.assertEquals(cgs2.getName(), "CG1");
-      Assert.assertEquals(cgs3.getName(), "CG0");
-    } catch (Exception e) {
-      e.printStackTrace();
-    }
-  }
-  
-  @Test
-  public void testStorageValid7() {
-    try {
-      String strStorage = "[f1, f2] as PI serialize by avro compress by gz; [f3, f4] as Pi SERIALIZE BY avro COMPRESS BY gz; [f5, f6] as CG100";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      CGSchema[] cgschemas = p.getCGSchemas();
-
-      Assert.assertEquals(cgschemas.length, 3);
-      CGSchema cgs1 = cgschemas[0];
-      System.out.println(cgs1);
-      CGSchema cgs2 = cgschemas[1];
-      System.out.println(cgs2);
-      CGSchema cgs3 = cgschemas[2];
-      System.out.println(cgs3);
-
-      Assert.assertEquals(cgs1.getName(), "PI");
-      Assert.assertEquals(cgs2.getName(), "Pi");
-      Assert.assertEquals(cgs3.getName(), "CG100");
-    } catch (Exception e) {
-      e.printStackTrace();
-    }
-  }
-  
-  /*
-   *  There is no default column group and C3 is skipped.
-   */
-  @Test
-  public void testStorageValid8() {
-    try {
-      String strStorage = "[f1, f2] as C1 serialize by avro compress by gz; [f3, f4, f5, f6] as C2 SERIALIZE BY avro COMPRESS BY gz; as C3 compress by gz";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      CGSchema[] cgschemas = p.getCGSchemas();
-      Assert.assertEquals(cgschemas.length, 2);
-      CGSchema cgs1 = cgschemas[0];
-      System.out.println(cgs1);
-      CGSchema cgs2 = cgschemas[1];
-      System.out.println(cgs2);
-
-      Assert.assertEquals(cgs1.getName(), "C1");
-      Assert.assertEquals(cgs2.getName(), "C2");
-    } catch (Exception e) {
-      e.printStackTrace();
-    }
-  }  
-
-  @Test
-  public void testStorageInvalid1() {
-    try {
-      String strStorage = "[f1, f2] as C1 serialize by avro compress by gz; [f3, f4] as C1 SERIALIZE BY avro COMPRESS BY gz; [f5, f6] as C3";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      CGSchema[] cgschemas = p.getCGSchemas();
-      CGSchema cgs1 = cgschemas[0];
-      System.out.println(cgs1);
-
-      Assert.assertTrue(false);
-    } catch (Exception e) {
-      String errMsg = e.getMessage();
-      String str = "Duplicate column group names.";
-      Assert.assertEquals(errMsg, str);
-    }
-  }
-
-  @Test
-  public void testStorageInvalid2() {
-    try {
-      String strStorage = "[f1, f2] serialize by avro compress by gz as C1; [f3, f4] as C2 SERIALIZE BY avro COMPRESS BY gz; [f5, f6] as C3";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      CGSchema[] cgschemas = p.getCGSchemas();
-      CGSchema cgs1 = cgschemas[0];
-      System.out.println(cgs1);
-
-      Assert.assertTrue(false);
-    } catch (ParseException e) {
-      System.out.println(e.getMessage());
-    } catch (IOException e) {
-      Assert.assertTrue(false);
-    }
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestSchemaAnonymousCollection.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestSchemaAnonymousCollection.java
deleted file mode 100644
index 3f3b02e3a..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestSchemaAnonymousCollection.java
+++ /dev/null
@@ -1,83 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.types;
-
-import java.io.StringReader;
-import junit.framework.Assert;
-
-import org.apache.hadoop.zebra.schema.ColumnType;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.parser.TableSchemaParser;
-import org.apache.hadoop.zebra.schema.Schema.ColumnSchema;
-import org.junit.Test;
-
-public class TestSchemaAnonymousCollection {
-  @Test
-  public void testSchemaValid1() throws ParseException {
-    String strSch = "c1:collection(Record(f1:int, f2:int)), c2:collection(record(f5:collection(record(f3:float, f4))))";
-    TableSchemaParser parser;
-    Schema schema;
-
-    parser = new TableSchemaParser(new StringReader(strSch));
-    schema = parser.RecordSchema(null);
-    System.out.println(schema);
-
-    // test 1st level schema;
-    ColumnSchema f1 = schema.getColumn(0);
-    Assert.assertEquals("c1", f1.getName());
-    Assert.assertEquals(ColumnType.COLLECTION, f1.getType());
-
-    ColumnSchema f2 = schema.getColumn(1);
-    Assert.assertEquals("c2", f2.getName());
-    Assert.assertEquals(ColumnType.COLLECTION, f2.getType());
-
-    // test 2nd level schema;
-    Schema f1Schema = f1.getSchema();
-    ColumnSchema f11 = f1Schema.getColumn(0).getSchema().getColumn(0);
-    Assert.assertEquals("f1", f11.getName());
-    Assert.assertEquals(ColumnType.INT, f11.getType());
-    ColumnSchema f12 = f1Schema.getColumn(0).getSchema().getColumn(1);
-    Assert.assertEquals("f2", f12.getName());
-    Assert.assertEquals(ColumnType.INT, f12.getType());
-
-    Schema f2Schema = f2.getSchema();
-    ColumnSchema f21 = f2Schema.getColumn(0);
-    Assert.assertNull(f21.getName());
-    Assert.assertEquals(ColumnType.RECORD, f21.getType());
-
-    // test 3rd level schema;
-    Schema f21Schema = f21.getSchema();
-    ColumnSchema f211 = f21Schema.getColumn(0);
-    Assert.assertTrue(f211.getName().equals("f5"));
-    Assert.assertEquals(ColumnType.COLLECTION, f211.getType());
-    Schema f211Schema = f211.getSchema();
-    
-    ColumnSchema f212 = f211Schema.getColumn(0);
-    Assert.assertNull(f212.getName());
-    Assert.assertEquals(ColumnType.RECORD, f212.getType());
-    Schema f212Schema = f212.getSchema();
-    ColumnSchema f213 = f212Schema.getColumn(0);
-    Assert.assertEquals("f3", f213.getName());
-    Assert.assertEquals(ColumnType.FLOAT, f213.getType());
-    ColumnSchema f214 = f212Schema.getColumn(1);
-    Assert.assertEquals("f4", f214.getName());
-    Assert.assertEquals(ColumnType.BYTES, f214.getType());
-  }
-}
\ No newline at end of file
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestSchemaCollection.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestSchemaCollection.java
deleted file mode 100644
index 5f2abc16c..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestSchemaCollection.java
+++ /dev/null
@@ -1,124 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.types;
-
-import java.io.StringReader;
-import junit.framework.Assert;
-
-import org.apache.hadoop.zebra.schema.ColumnType;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.parser.TableSchemaParser;
-import org.apache.hadoop.zebra.schema.Schema.ColumnSchema;
-import org.junit.Test;
-
-public class TestSchemaCollection {
-  @Test
-  public void testSchemaValid1() throws ParseException {
-    String strSch = "c1:collection(record(f1:int, f2:int)), c2:collection(record(c3:collection(record(f3:float, f4))))";
-    TableSchemaParser parser;
-    Schema schema;
-
-    parser = new TableSchemaParser(new StringReader(strSch));
-    schema = parser.RecordSchema(null);
-    System.out.println(schema);
-
-    // test 1st level schema;
-    ColumnSchema f1 = schema.getColumn(0);
-    Assert.assertEquals("c1", f1.getName());
-    Assert.assertEquals(ColumnType.COLLECTION, f1.getType());
-
-    ColumnSchema f2 = schema.getColumn(1);
-    Assert.assertEquals("c2", f2.getName());
-    Assert.assertEquals(ColumnType.COLLECTION, f2.getType());
-
-    // test 2nd level schema;
-    Schema f1Schema = f1.getSchema();
-    ColumnSchema f11 = f1Schema.getColumn(0).getSchema().getColumn(0);
-    Assert.assertEquals("f1", f11.getName());
-    Assert.assertEquals(ColumnType.INT, f11.getType());
-    ColumnSchema f12 = f1Schema.getColumn(0).getSchema().getColumn(1);
-    Assert.assertEquals("f2", f12.getName());
-    Assert.assertEquals(ColumnType.INT, f12.getType());
-
-    Schema f2Schema = f2.getSchema();
-    ColumnSchema f21 = f2Schema.getColumn(0).getSchema().getColumn(0);
-    Assert.assertEquals("c3", f21.getName());
-    Assert.assertEquals(ColumnType.COLLECTION, f21.getType());
-
-    // test 3rd level schema;
-    Schema f21Schema = f21.getSchema();
-    ColumnSchema f211 = f21Schema.getColumn(0).getSchema().getColumn(0);
-    Assert.assertEquals("f3", f211.getName());
-    Assert.assertEquals(ColumnType.FLOAT, f211.getType());
-    ColumnSchema f212 = f21Schema.getColumn(0).getSchema().getColumn(1);
-    Assert.assertEquals("f4", f212.getName());
-    Assert.assertEquals(ColumnType.BYTES, f212.getType());
-  }
-
-  @Test
-  public void testSchemaInvalid1() throws ParseException, Exception {
-    try {
-      String strSch = "c1:collection(record(f1:int, f2:int)), c2:collection(record(c3:collection(record(f3:float, f4)))))";
-      TableSchemaParser parser;
-      Schema schema;
-
-      parser = new TableSchemaParser(new StringReader(strSch));
-      schema = parser.RecordSchema(null);
-      System.out.println(schema);
-    } catch (Exception e) {
-      String errMsg = e.getMessage();
-      String str = "Encountered \" \")\" \") \"\" at line 1, column 98.";
-      System.out.println(errMsg);
-      System.out.println(str);
-      Assert.assertEquals(errMsg.startsWith(str), true);
-    }
-  }
-  
-  @Test
-  public void testInvalidCollectionSchema() throws ParseException {
-	    String[] strSchs = { "c0:int, c1:collection(f1:int, f2:string)",
-	    		             "c0:int, c1:collection(f1:int)",
-	    		             "c0:int, c1:collection(int)" };
-	    for( String strSch : strSchs ) {
-		    TableSchemaParser parser = new TableSchemaParser( new StringReader( strSch ) );
-		    try {
-		    	parser.RecordSchema(null);
-		    } catch(ParseException ex) {
-		    	System.out.println( "Catch expected exception for schema: " + strSch );
-		    	
-		    	continue;
-		    }
-		    Assert.fail( "Exception expected for invalid schemes: "+  strSch );
-	    }
-  }
-  
-//  private void printSchema(Schema schema, int indent) {
-//	    for( int i = 0; i < schema.getNumColumns(); i++ ) {
-//	    	ColumnSchema cs = schema.getColumn( i );
-//	    	for( int j = 0; j < indent; j++ )
-//	    		System.out.print( "\t" );
-//	    	System.out.println( cs.getName() + ": " + cs.getType().toString() );
-//	    	if( cs.getSchema() != null )
-//	    		printSchema( cs.getSchema(), indent + 1 );
-//	    }
-//	  
-//  }
-  
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestSchemaMap.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestSchemaMap.java
deleted file mode 100644
index 4bfcb65cd..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestSchemaMap.java
+++ /dev/null
@@ -1,222 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.types;
-
-import java.io.StringReader;
-import junit.framework.Assert;
-
-import org.apache.hadoop.zebra.schema.ColumnType;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.parser.TableSchemaParser;
-import org.apache.hadoop.zebra.schema.Schema.ColumnSchema;
-import org.junit.Test;
-
-public class TestSchemaMap {
-  @Test
-  public void testSchemaValid1() throws ParseException {
-    String strSch = "f1:int, m1:map(int)";
-    TableSchemaParser parser;
-    Schema schema;
-
-    parser = new TableSchemaParser(new StringReader(strSch));
-    schema = parser.RecordSchema(null);
-    System.out.println(schema);
-
-    // test 1st level schema;
-    ColumnSchema f1 = schema.getColumn(0);
-    Assert.assertEquals("f1", f1.getName());
-    Assert.assertEquals(ColumnType.INT, f1.getType());
-
-    ColumnSchema f2 = schema.getColumn(1);
-    Assert.assertEquals("m1", f2.getName());
-    Assert.assertEquals(ColumnType.MAP, f2.getType());
-    
-    Schema temp = f2.getSchema();
-    System.out.println(temp.getNumColumns());
-    ColumnSchema cs = temp.getColumn(0);
-    System.out.println(cs.getType());
-    
-  }
-
-  @Test
-  public void testSchemaValid2() throws ParseException {
-    String strSch = "f1:int, m1:map(map(float))";
-    TableSchemaParser parser;
-    Schema schema;
-
-    parser = new TableSchemaParser(new StringReader(strSch));
-    schema = parser.RecordSchema(null);
-    System.out.println(schema);
-
-    // test 1st level schema;
-    ColumnSchema f1 = schema.getColumn(0);
-    Assert.assertEquals("f1", f1.getName());
-    Assert.assertEquals(ColumnType.INT, f1.getType());
-
-    ColumnSchema f2 = schema.getColumn(1);
-    Assert.assertEquals("m1", f2.getName());
-    Assert.assertEquals(ColumnType.MAP, f2.getType());
-
-    // test 2nd level schema;
-    Schema f2Schema = f2.getSchema();
-    ColumnSchema f21 = f2Schema.getColumn(0);
-    // Assert.assertEquals("m1", f2.getName());
-    Assert.assertEquals(ColumnType.MAP, f21.getType());
-
-    // test 3rd level schema;
-    Schema f21Schema = f21.getSchema();
-    ColumnSchema f211 = f21Schema.getColumn(0);
-    // Assert.assertEquals("m1", f2.getName());
-    Assert.assertEquals(ColumnType.FLOAT, f211.getType());
-  }
-
-  @Test
-  public void testSchemaValid3() throws ParseException {
-    String strSch = "m1:map(map(float)), m2:map(bool), f3";
-    TableSchemaParser parser;
-    Schema schema;
-
-    parser = new TableSchemaParser(new StringReader(strSch));
-    schema = parser.RecordSchema(null);
-    System.out.println(schema);
-
-    // test 1st level schema;
-    ColumnSchema f1 = schema.getColumn(0);
-    Assert.assertEquals("m1", f1.getName());
-    Assert.assertEquals(ColumnType.MAP, f1.getType());
-
-    ColumnSchema f2 = schema.getColumn(1);
-    Assert.assertEquals("m2", f2.getName());
-    Assert.assertEquals(ColumnType.MAP, f2.getType());
-
-    ColumnSchema f3 = schema.getColumn(2);
-    Assert.assertEquals("f3", f3.getName());
-    Assert.assertEquals(ColumnType.BYTES, f3.getType());
-
-    // test 2nd level schema;
-    Schema f1Schema = f1.getSchema();
-    ColumnSchema f11 = f1Schema.getColumn(0);
-    Assert.assertEquals(ColumnType.MAP, f11.getType());
-
-    Schema f2Schema = f2.getSchema();
-    ColumnSchema f21 = f2Schema.getColumn(0);
-    Assert.assertEquals(ColumnType.BOOL, f21.getType());
-
-    // test 3rd level schema;
-    Schema f11Schema = f11.getSchema();
-    ColumnSchema f111 = f11Schema.getColumn(0);
-    Assert.assertEquals(ColumnType.FLOAT, f111.getType());
-  }
-
-  @Test
-  public void testSchemaInvalid1() throws ParseException {
-    try {
-      String strSch = "m1:abc";
-      TableSchemaParser parser;
-      Schema schema;
-
-      parser = new TableSchemaParser(new StringReader(strSch));
-      schema = parser.RecordSchema(null);
-      System.out.println(schema);
-    } catch (Exception e) {
-      String errMsg = e.getMessage();
-      String str = "Encountered \" <IDENTIFIER> \"abc \"\" at line 1, column 4.";
-      System.out.println(errMsg);
-      System.out.println(str);
-      Assert.assertEquals(errMsg.startsWith(str), true);
-    }
-  }
-
-  @Test
-  public void testSchemaInvalid2() throws ParseException {
-    try {
-      String strSch = "m1:map(int";
-      TableSchemaParser parser;
-      Schema schema;
-
-      parser = new TableSchemaParser(new StringReader(strSch));
-      schema = parser.RecordSchema(null);
-      System.out.println(schema);
-    } catch (Exception e) {
-      String errMsg = e.getMessage();
-      String str = "Encountered \"<EOF>\" at line 1, column 10.";
-      System.out.println(errMsg);
-      System.out.println(str);
-      Assert.assertEquals(errMsg.startsWith(str), true);
-    }
-  }
-
-  @Test
-  public void testSchemaInvalid3() throws ParseException {
-    try {
-      String strSch = "m1:map(int, f2:int";
-      TableSchemaParser parser;
-      Schema schema;
-
-      parser = new TableSchemaParser(new StringReader(strSch));
-      schema = parser.RecordSchema(null);
-      System.out.println(schema);
-    } catch (Exception e) {
-      String errMsg = e.getMessage();
-      String str = "Encountered \" \",\" \", \"\" at line 1, column 11.";
-      System.out.println(errMsg);
-      System.out.println(str);
-      Assert.assertEquals(errMsg.startsWith(str), true);
-    }
-  }
-
-  @Test
-  public void testSchemaInvalid4() throws ParseException {
-    try {
-      String strSch = "m1:map(m2:int)";
-      TableSchemaParser parser;
-      Schema schema;
-
-      parser = new TableSchemaParser(new StringReader(strSch));
-      schema = parser.RecordSchema(null);
-      System.out.println(schema);
-    } catch (Exception e) {
-      String errMsg = e.getMessage();
-      String str = "Encountered \" <IDENTIFIER> \"m2 \"\" at line 1, column 8.";
-      System.out.println(errMsg);
-      System.out.println(str);
-      Assert.assertEquals(errMsg.startsWith(str), true);
-    }
-  }
-
-  @Test
-  public void testSchemaInvalid5() throws ParseException {
-    try {
-      String strSch = "m1:map(abc)";
-      TableSchemaParser parser;
-      Schema schema;
-
-      parser = new TableSchemaParser(new StringReader(strSch));
-      schema = parser.RecordSchema(null);
-      System.out.println(schema);
-    } catch (Exception e) {
-      String errMsg = e.getMessage();
-      String str = "Encountered \" <IDENTIFIER> \"abc \"\" at line 1, column 8.";
-      System.out.println(errMsg);
-      System.out.println(str);
-      Assert.assertEquals(errMsg.startsWith(str), true);
-    }
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestSchemaPrimitive.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestSchemaPrimitive.java
deleted file mode 100644
index dc0b52e40..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestSchemaPrimitive.java
+++ /dev/null
@@ -1,183 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.types;
-
-import java.io.StringReader;
-import junit.framework.Assert;
-
-import org.apache.hadoop.zebra.schema.ColumnType;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.parser.TableSchemaParser;
-import org.apache.hadoop.zebra.schema.Schema.ColumnSchema;
-import org.junit.Test;
-
-public class TestSchemaPrimitive {
-  @Test
-  public void testSchemaValid1() throws ParseException {
-    String strSch = "f1:int, f2:long, f3:float, f4:bool, f5:string, f6:bytes";
-    TableSchemaParser parser;
-    Schema schema;
-
-    parser = new TableSchemaParser(new StringReader(strSch));
-    schema = parser.RecordSchema(null);
-    System.out.println(schema);
-
-    ColumnSchema f2 = schema.getColumn(1);
-    Assert.assertEquals("f2", f2.getName());
-    Assert.assertEquals(ColumnType.LONG, f2.getType());
-  }
-
-  @Test
-  public void testSchemaValid2() throws ParseException {
-    String strSch = "f1:int, f2, f3:float, f4, f5:string, f6::f61, f7::f71:map";
-    TableSchemaParser parser;
-    Schema schema;
-
-    parser = new TableSchemaParser(new StringReader(strSch));
-    schema = parser.RecordSchema(null);
-    System.out.println(schema);
-
-    ColumnSchema f2 = schema.getColumn(1);
-    Assert.assertEquals("f2", f2.getName());
-    Assert.assertEquals(ColumnType.BYTES, f2.getType());
-
-    ColumnSchema f4 = schema.getColumn(3);
-    Assert.assertEquals("f4", f4.getName());
-    Assert.assertEquals(ColumnType.BYTES, f4.getType());
-    
-    ColumnSchema f6 = schema.getColumn(5);
-    Assert.assertEquals("f6::f61", f6.getName());
-    Assert.assertEquals(ColumnType.BYTES, f6.getType());
-    
-    ColumnSchema f7 = schema.getColumn(6);
-    Assert.assertEquals("f7::f71", f7.getName());
-    Assert.assertEquals(ColumnType.MAP, f7.getType());
-  }
-
-  /*
-   * @Test public void testSchemaValid3() throws ParseException { try { String
-   * strSch = ",f1:int"; TableSchemaParser parser; Schema schema;
-   * 
-   * parser = new TableSchemaParser(new StringReader(strSch)); schema =
-   * parser.RecordSchema(null); System.out.println(schema);
-   * 
-   * ColumnSchema f1 = schema.getColumn(0); //Assert.assertEquals("f2",
-   * f2.getName()); //Assert.assertEquals(ColumnType.BYTES, f2.getType());
-   * 
-   * ColumnSchema f2 = schema.getColumn(1); //Assert.assertEquals("f1",
-   * f4.getName()); //Assert.assertEquals(ColumnType.BYTES, f4.getType()); } catch
-   * (Exception e) { System.out.println(e.getMessage()); } }
-   */
-
-  @Test
-  public void testSchemaInvalid1() {
-    try {
-      String strSch = "f1:int, f2:xyz, f3:float";
-      TableSchemaParser parser;
-      Schema schema;
-
-      parser = new TableSchemaParser(new StringReader(strSch));
-      schema = parser.RecordSchema(null);
-      System.out.println(schema);
-    } catch (Exception e) {
-      String errMsg = e.getMessage();
-      String str = "Encountered \" <IDENTIFIER> \"xyz \"\" at line 1, column 12.";
-      System.out.println(errMsg);
-      System.out.println(str);
-      Assert.assertEquals(errMsg.startsWith(str), true);
-    }
-  }
-
-  @Test
-  public void testSchemaInvalid2() {
-    try {
-      String strSch = "f1:";
-      TableSchemaParser parser;
-      Schema schema;
-
-      parser = new TableSchemaParser(new StringReader(strSch));
-      schema = parser.RecordSchema(null);
-      System.out.println(schema);
-    } catch (Exception e) {
-      String errMsg = e.getMessage();
-      String str = "Encountered \"<EOF>\" at line 1, column 3.";
-      System.out.println(errMsg);
-      System.out.println(str);
-      Assert.assertEquals(errMsg.startsWith(str), true);
-    }
-  }
-
-  @Test
-  public void testSchemaInvalid3() {
-    try {
-      String strSch = ":";
-      TableSchemaParser parser;
-      Schema schema;
-
-      parser = new TableSchemaParser(new StringReader(strSch));
-      schema = parser.RecordSchema(null);
-      System.out.println(schema);
-    } catch (Exception e) {
-      String errMsg = e.getMessage();
-      String str = "Encountered \" \":\" \": \"\" at line 1, column 1.";
-      System.out.println(errMsg);
-      System.out.println(str);
-      Assert.assertEquals(errMsg.startsWith(str), true);
-    }
-  }
-
-  @Test
-  public void testSchemaInvalid4() {
-    try {
-      String strSch = "f1:int abc";
-      TableSchemaParser parser;
-      Schema schema;
-
-      parser = new TableSchemaParser(new StringReader(strSch));
-      schema = parser.RecordSchema(null);
-      System.out.println(schema);
-    } catch (Exception e) {
-      String errMsg = e.getMessage();
-      String str = "Encountered \" <IDENTIFIER> \"abc \"\" at line 1, column 8.";
-      System.out.println(errMsg);
-      System.out.println(str);
-      Assert.assertEquals(errMsg.startsWith(str), true);
-    }
-  }
-  
-  @Test
-  public void testSchemaInvalid5() {
-    try {
-      String strSch = "f1:f11";
-      TableSchemaParser parser;
-      Schema schema;
-
-      parser = new TableSchemaParser(new StringReader(strSch));
-      schema = parser.RecordSchema(null);
-      System.out.println(schema);
-    } catch (Exception e) {
-      String errMsg = e.getMessage();
-      String str = "Encountered \" <IDENTIFIER> \"f11 \"\" at line 1, column 4.";
-      System.out.println(errMsg);
-      System.out.println(str);
-      Assert.assertEquals(errMsg.startsWith(str), true);
-    }
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestSchemaRecord.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestSchemaRecord.java
deleted file mode 100644
index 6e26791a8..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestSchemaRecord.java
+++ /dev/null
@@ -1,86 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.types;
-
-import java.io.StringReader;
-import junit.framework.Assert;
-
-import org.apache.hadoop.zebra.schema.ColumnType;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.parser.TableSchemaParser;
-import org.apache.hadoop.zebra.schema.Schema.ColumnSchema;
-import org.junit.Test;
-
-public class TestSchemaRecord {
-  @Test
-  public void testSchemaValid1() throws ParseException {
-    String strSch = "r1:record(f1:int, f2:int), r2:record(r3:record(f3:float, f4))";
-    TableSchemaParser parser;
-    Schema schema;
-
-    parser = new TableSchemaParser(new StringReader(strSch));
-    schema = parser.RecordSchema(null);
-    System.out.println(schema);
-
-    // test 1st level schema;
-    ColumnSchema f1 = schema.getColumn(0);
-    Assert.assertEquals("r1", f1.getName());
-    Assert.assertEquals(ColumnType.RECORD, f1.getType());
-
-    ColumnSchema f2 = schema.getColumn(1);
-    Assert.assertEquals("r2", f2.getName());
-    Assert.assertEquals(ColumnType.RECORD, f2.getType());
-
-    // test 2nd level schema;
-    Schema f1Schema = f1.getSchema();
-    ColumnSchema f11 = f1Schema.getColumn(0);
-    Assert.assertEquals("f1", f11.getName());
-    Assert.assertEquals(ColumnType.INT, f11.getType());
-    ColumnSchema f12 = f1Schema.getColumn(1);
-    Assert.assertEquals("f2", f12.getName());
-    Assert.assertEquals(ColumnType.INT, f12.getType());
-
-    Schema f2Schema = f2.getSchema();
-    ColumnSchema f21 = f2Schema.getColumn(0);
-    Assert.assertEquals("r3", f21.getName());
-    Assert.assertEquals(ColumnType.RECORD, f21.getType());
-
-    // test 3rd level schema;
-    Schema f21Schema = f21.getSchema();
-    ColumnSchema f211 = f21Schema.getColumn(0);
-    Assert.assertEquals("f3", f211.getName());
-    Assert.assertEquals(ColumnType.FLOAT, f211.getType());
-    ColumnSchema f212 = f21Schema.getColumn(1);
-    Assert.assertEquals("f4", f212.getName());
-    Assert.assertEquals(ColumnType.BYTES, f212.getType());
-  }
-
-  /*
-   * @Test public void testSchemaInvalid1() throws ParseException { try { String
-   * strSch = "m1:abc"; TableSchemaParser parser; Schema schema;
-   * 
-   * parser = new TableSchemaParser(new StringReader(strSch)); schema =
-   * parser.RecordSchema(); System.out.println(schema); } catch (Exception e) {
-   * String errMsg = e.getMessage(); String str =
-   * "Encountered \" <IDENTIFIER> \"abc \"\" at line 1, column 4.";
-   * System.out.println(errMsg); System.out.println(str);
-   * Assert.assertEquals(errMsg.startsWith(str), true); } }
-   */
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestStorageCollection.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestStorageCollection.java
deleted file mode 100644
index efddb3ad6..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestStorageCollection.java
+++ /dev/null
@@ -1,174 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.types;
-
-import java.io.StringReader;
-import java.util.Map;
-import java.util.HashSet;
-import java.util.Iterator;
-import junit.framework.Assert;
-
-import org.apache.hadoop.zebra.types.CGSchema;
-import org.apache.hadoop.zebra.schema.ColumnType;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Partition;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.parser.TableSchemaParser;
-import org.apache.hadoop.zebra.schema.Schema.ColumnSchema;
-import org.junit.Before;
-import org.junit.Test;
-
-public class TestStorageCollection {
-  String strSch = "c1:collection(record(f1:int, f2:int)), c2:collection(record(c3:collection(record(f3:float, f4))))";
-  TableSchemaParser parser;
-  Schema schema;
-
-  @Before
-  public void init() throws ParseException {
-    parser = new TableSchemaParser(new StringReader(strSch));
-    schema = parser.RecordSchema(null);
-  }
-
-  @Test
-  public void testStorageValid1() {
-    try {
-      String strStorage = "[c1]; [c2]";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      CGSchema[] cgschemas = p.getCGSchemas();
-
-      // 2 column group;
-      int size = cgschemas.length;
-      Assert.assertEquals(size, 2);
-      System.out.println("********** Column Groups **********");
-      for (int i = 0; i < cgschemas.length; i++) {
-        System.out.println(cgschemas[i]);
-        System.out.println("--------------------------------");
-      }
-      CGSchema cgs1 = cgschemas[0];
-      CGSchema cgs2 = cgschemas[1];
-
-      ColumnSchema f11 = cgs1.getSchema().getColumn(0);
-      Assert.assertEquals("c1", f11.getName());
-      Assert.assertEquals(ColumnType.COLLECTION, f11.getType());
-      ColumnSchema f21 = cgs2.getSchema().getColumn(0);
-      Assert.assertEquals("c2", f21.getName());
-      Assert.assertEquals(ColumnType.COLLECTION, f21.getType());
-
-      System.out.println("*********** Column Map **********");
-      Map<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>> colmap = p
-          .getPartitionInfo().getColMap();
-      Assert.assertEquals(colmap.size(), 2);
-      Iterator<Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>>> it = colmap
-          .entrySet().iterator();
-      for (int i = 0; i < colmap.size(); i++) {
-        Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>> entry = (Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>>) it
-            .next();
-        String name = entry.getKey();
-        HashSet<Partition.PartitionInfo.ColumnMappingEntry> hs = entry
-            .getValue();
-        Iterator<Partition.PartitionInfo.ColumnMappingEntry> it1 = hs
-            .iterator();
-        while (it1.hasNext()) {
-          Partition.PartitionInfo.ColumnMappingEntry cme = (Partition.PartitionInfo.ColumnMappingEntry) it1
-              .next();
-          System.out.println("[Column = " + name + " CG = " + cme.getCGIndex()
-              + "." + cme.getFieldIndex() + "]");
-          if (i == 0) {
-            Assert.assertEquals(name, "c1");
-            Assert.assertEquals(cme.getCGIndex(), 0);
-            Assert.assertEquals(cme.getFieldIndex(), 0);
-          } else if (i == 1) {
-            Assert.assertEquals(name, "c2");
-            Assert.assertEquals(cme.getCGIndex(), 1);
-            Assert.assertEquals(cme.getFieldIndex(), 0);
-          }
-        }
-      }
-    } catch (Exception e) {
-      Assert.assertTrue(false);
-    }
-  }
-
-  @Test
-  public void testStorageValid2() {
-    try {
-      String strStorage = "[c1.f1]";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      Assert.assertTrue(false);
-      CGSchema[] cgschemas = p.getCGSchemas();
-
-      // 2 column group;
-      int size = cgschemas.length;
-      Assert.assertEquals(size, 2);
-      System.out.println("********** Column Groups **********");
-      for (int i = 0; i < cgschemas.length; i++) {
-        System.out.println(cgschemas[i]);
-        System.out.println("--------------------------------");
-      }
-      CGSchema cgs1 = cgschemas[0];
-      CGSchema cgs2 = cgschemas[1];
-
-      ColumnSchema f11 = cgs1.getSchema().getColumn(0);
-      Assert.assertEquals("c1.f1", f11.getName());
-      Assert.assertEquals(ColumnType.INT, f11.getType());
-      ColumnSchema f21 = cgs2.getSchema().getColumn(0);
-      Assert.assertEquals("c1.f2", f21.getName());
-      Assert.assertEquals(ColumnType.INT, f21.getType());
-      ColumnSchema f22 = cgs2.getSchema().getColumn(1);
-      Assert.assertEquals("c2", f22.getName());
-      Assert.assertEquals(ColumnType.COLLECTION, f22.getType());
-
-      System.out.println("*********** Column Map **********");
-      Map<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>> colmap = p
-          .getPartitionInfo().getColMap();
-      Assert.assertEquals(colmap.size(), 3);
-      Iterator<Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>>> it = colmap
-          .entrySet().iterator();
-      for (int i = 0; i < colmap.size(); i++) {
-        Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>> entry = (Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>>) it
-            .next();
-        String name = entry.getKey();
-        HashSet<Partition.PartitionInfo.ColumnMappingEntry> hs = entry
-            .getValue();
-        Iterator<Partition.PartitionInfo.ColumnMappingEntry> it1 = hs
-            .iterator();
-        for (int j = 0; j < hs.size(); j++) {
-          Partition.PartitionInfo.ColumnMappingEntry cme = (Partition.PartitionInfo.ColumnMappingEntry) it1
-              .next();
-          System.out.println("[Column = " + name + " CG = " + cme.getCGIndex()
-              + "." + cme.getFieldIndex() + "]");
-          if (i == 0 && j == 0) {
-            Assert.assertEquals(name, "c1.f2");
-            Assert.assertEquals(cme.getCGIndex(), 1);
-            Assert.assertEquals(cme.getFieldIndex(), 0);
-          } else if (i == 1 && j == 0) {
-            Assert.assertEquals(name, "c1.f1");
-            Assert.assertEquals(cme.getCGIndex(), 0);
-            Assert.assertEquals(cme.getFieldIndex(), 0);
-          } else if (i == 2 && j == 0) {
-            Assert.assertEquals(name, "c2");
-            Assert.assertEquals(cme.getCGIndex(), 1);
-            Assert.assertEquals(cme.getFieldIndex(), 1);
-          }
-        }
-      }
-    } catch (Exception e) {
-    }
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestStorageGrammar.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestStorageGrammar.java
deleted file mode 100644
index 451d634e7..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestStorageGrammar.java
+++ /dev/null
@@ -1,860 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.types;
-
-import java.io.BufferedReader;
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.InputStreamReader;
-import java.io.PrintStream;
-import java.util.HashMap;
-import java.util.Map;
-import javax.security.auth.login.LoginException;
-
-import junit.framework.Assert;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocalFileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RawLocalFileSystem;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.zebra.io.BasicTable;
-import org.apache.hadoop.zebra.io.TableInserter;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * 
- * Test projections on complicated column types.
- * 
- */
-public class TestStorageGrammar {
-
-  final static String STR_SCHEMA = "s1:bool, s2:int, s3:long, s4:double, s5:string, s6:bytes, r1:record(f1:int, f2:long), r2:record(r3:record(f3:double, f4)), m1:map(string),m2:map(map(int)), c:collection(record(f13:double, f14:double, f15:bytes)),s7:string, s8:string, s9:string, s10:string, s11:string, s12:string, s13:string, s14:string, s15:string, s16:string, s17:string, s18:string, s19:string, s20:string, s21:string, s22:string, s23:string";
-  static String STR_STORAGE = null;
-  final private static Configuration conf = new Configuration();
-  private static Path path;
-  private static FileSystem fs;
-  private static String user;
-  private static String group;
-  private static String defaultUser;
-
-  static {
-    try {
-      String command = "whoami";
-      Process process = new ProcessBuilder(command).start();
-      InputStream is = process.getInputStream();
-      InputStreamReader isr = new InputStreamReader(is);
-      BufferedReader br = new BufferedReader(isr);
-      System.out.printf("Output of running %s is:", command);
-      String line;
-      while ((line = br.readLine()) != null) {
-        System.out.println("default user0:" + line);
-        defaultUser = line;
-
-      }
-    } catch (Exception e) {
-      e.printStackTrace();
-    }
-  }
-
-  @BeforeClass
-  public static void setUpOnce() throws IOException, LoginException {
-    if (System.getProperty("user") == null) {
-      System.setProperty("user", defaultUser);
-    }
-    user = System.getProperty("user");
-    if (System.getProperty("group") == null) {
-      System.setProperty("group", "users");
-    }
-    group = System.getProperty("group");
-    System.out.println("uid:" + user + " gid: " + group);
-    STR_STORAGE = "[s1, s2] COMPRESS BY gz SECURE BY uid:"
-        + user
-        + " gid:"
-        + group
-        + " perm:777 SERIALIZE BY pig; [m1#{a}] SERIALIZE BY pig COMPRESS BY gz SECURE BY uid:"
-        + user
-        + " gid:"
-        + group
-        + " perm:777 ; [r1.f1] SECURE BY uid:"
-        + user
-        + " gid:"
-        + group
-        + " perm:777 SERIALIZE BY pig COMPRESS BY gz ; [s3, s4, r2.r3.f3] SERIALIZE BY pig SECURE BY uid:"
-        + user
-        + " gid: "
-        + group
-        + " perm:777 COMPRESS BY gz ; [s5, s6, m2#{x|y}] compREss by gz secURe by uid:"
-        + user
-        + " gid:"
-        + group
-        + " perm:777 SerialIZE BY pig; [r1.f2, m1#{b}]; [r2.r3.f4, m2#{z}] SERIALIZE BY avro;[s7,s8] SERIALIZE BY AVRO;[s9,s10] COMPRESS BY gz ";
-    System.out.println("storage: " + STR_STORAGE);
-    conf.setInt("table.output.tfile.minBlock.size", 64 * 1024);
-    conf.setInt("table.input.split.minSize", 64 * 1024);
-    conf.set("table.output.tfile.compression", "none");
-    // String STR_STORAGE1 =
-    // "[s1, s2] COMPRESS BY gz SECURE BY uid:"+USER1.getUserName()+" gid:"+USER1.getGroupNames()[0]+" perm:777 SERIALIZE BY pig";
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    path = new Path(fs.getWorkingDirectory(), "TestStorageGrammar");
-    System.out.println("path: " + path.toString());
-    // Process p = Runtime.getRuntime().exec("rm -rf "+path.toString()+"*");
-
-    fs = path.getFileSystem(conf);
-    // drop any previous tables
-    BasicTable.drop(path, conf);
-
-    BasicTable.Writer writer = new BasicTable.Writer(path, STR_SCHEMA,
-        STR_STORAGE, conf);
-
-    writer.finish();
-
-    Schema schema = writer.getSchema();
-    Tuple tuple = TypesUtils.createTuple(schema);
-    BasicTable.Writer writer1 = new BasicTable.Writer(path, conf);
-    int part = 0;
-    TableInserter inserter = null;
-    try {
-      inserter = writer1.getInserter("part" + part, true);
-    } catch (Exception e) {
-      e.printStackTrace();
-    }
-    TypesUtils.resetTuple(tuple);
-    Tuple tupRecord1 = null;
-    try {
-      tupRecord1 = TypesUtils.createTuple(schema.getColumnSchema("r1")
-          .getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-    }
-
-    Tuple tupRecord2 = null;
-    try {
-      tupRecord2 = TypesUtils.createTuple(schema.getColumnSchema("r2")
-          .getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-    }
-
-    Tuple tupRecord3 = null;
-    try {
-      tupRecord3 = TypesUtils.createTuple(new Schema("f3:float, f4"));
-    } catch (ParseException e) {
-      e.printStackTrace();
-    }
-
-    // row 1
-    tuple.set(0, true); // bool
-    tuple.set(1, 1); // int
-    tuple.set(2, 1001L); // long
-    tuple.set(3, 1.1); // float
-    tuple.set(4, "hello world 1"); // string
-    tuple.set(5, new DataByteArray("hello byte 1")); // byte
-
-    // r1:record(f1:int, f2:long
-    tupRecord1.set(0, 1);
-    tupRecord1.set(1, 1001L);
-    tuple.set(6, tupRecord1);
-
-    // r2:record(r3:record(f3:float, f4))
-    tupRecord2.set(0, tupRecord3);
-    tupRecord3.set(0, 1.3);
-    tupRecord3.set(1, new DataByteArray("r3 row 1 byte array "));
-    tuple.set(7, tupRecord2);
-
-    // m1:map(string)
-    Map<String, String> m1 = new HashMap<String, String>();
-    m1.put("a", "A");
-    m1.put("b", "B");
-    m1.put("c", "C");
-    tuple.set(8, m1);
-
-    // m2:map(map(int))
-    HashMap<String, Map> m2 = new HashMap<String, Map>();
-    Map<String, Integer> m3 = new HashMap<String, Integer>();
-    m3.put("m311", 311);
-    m3.put("m321", 321);
-    m3.put("m331", 331);
-    Map<String, Integer> m4 = new HashMap<String, Integer>();
-    m4.put("m411", 411);
-    m4.put("m421", 421);
-    m4.put("m431", 431);
-    m2.put("x", m3);
-    m2.put("y", m4);
-    tuple.set(9, m2);
-
-    // c:collection(f13:double, f14:float, f15:bytes)
-    DataBag bagColl = TypesUtils.createBag();
-    Schema schColl = schema.getColumn(10).getSchema().getColumn(0).getSchema();
-    Tuple tupColl1 = TypesUtils.createTuple(schColl);
-    Tuple tupColl2 = TypesUtils.createTuple(schColl);
-    byte[] abs1 = new byte[3];
-    byte[] abs2 = new byte[4];
-    tupColl1.set(0, 3.1415926);
-    tupColl1.set(1, 1.6);
-    abs1[0] = 11;
-    abs1[1] = 12;
-    abs1[2] = 13;
-    tupColl1.set(2, new DataByteArray(abs1));
-    bagColl.add(tupColl1);
-    tupColl2.set(0, 123.456789);
-    tupColl2.set(1, 100);
-    abs2[0] = 21;
-    abs2[1] = 22;
-    abs2[2] = 23;
-    abs2[3] = 24;
-    tupColl2.set(2, new DataByteArray(abs2));
-    bagColl.add(tupColl2);
-    tuple.set(10, bagColl);
-
-    // set s7 to s23
-    for (int i = 7; i <= 23; i++) {
-      tuple.set(i + 4, "s" + "i" + ", line1");
-    }
-
-    int row = 0;
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    // row 2
-    row++;
-    TypesUtils.resetTuple(tuple);
-    TypesUtils.resetTuple(tupRecord1);
-    TypesUtils.resetTuple(tupRecord2);
-    TypesUtils.resetTuple(tupRecord3);
-    m1.clear();
-    m2.clear();
-    m3.clear();
-    m4.clear();
-    tuple.set(0, false);
-    tuple.set(1, 2); // int
-    tuple.set(2, 1002L); // long
-    tuple.set(3, 3.1); // float
-    tuple.set(4, "hello world 2"); // string
-    tuple.set(5, new DataByteArray("hello byte 2")); // byte
-
-    // r1:record(f1:int, f2:long
-    tupRecord1.set(0, 2);
-    tupRecord1.set(1, 1002L);
-    tuple.set(6, tupRecord1);
-
-    // r2:record(r3:record(f3:float, f4))
-    tupRecord2.set(0, tupRecord3);
-    tupRecord3.set(0, 2.3);
-    tupRecord3.set(1, new DataByteArray("r3 row2  byte array"));
-    tuple.set(7, tupRecord2);
-
-    // m1:map(string)
-    m1.put("a2", "A2");
-    m1.put("b2", "B2");
-    m1.put("c2", "C2");
-    tuple.set(8, m1);
-
-    // m2:map(map(int))
-    m3.put("m321", 321);
-    m3.put("m322", 322);
-    m3.put("m323", 323);
-    m2.put("z", m3);
-    tuple.set(9, m2);
-
-    // c:collection(f13:double, f14:float, f15:bytes)
-    bagColl.clear();
-    TypesUtils.resetTuple(tupColl1);
-    TypesUtils.resetTuple(tupColl2);
-    tupColl1.set(0, 7654.321);
-    tupColl1.set(1, 0.0001);
-    abs1[0] = 31;
-    abs1[1] = 32;
-    abs1[2] = 33;
-    tupColl1.set(2, new DataByteArray(abs1));
-    bagColl.add(tupColl1);
-    tupColl2.set(0, 0.123456789);
-    tupColl2.set(1, 0.3333);
-    abs2[0] = 41;
-    abs2[1] = 42;
-    abs2[2] = 43;
-    abs2[3] = 44;
-    tupColl2.set(2, new DataByteArray(abs2));
-    bagColl.add(tupColl2);
-    tuple.set(10, bagColl);
-    // set s7 to s23
-    for (int i = 7; i <= 23; i++) {
-      tuple.set(i + 4, "s" + "i" + ", line2");
-    }
-    inserter.insert(new BytesWritable(String.format("k%d%d", part + 1, row + 1)
-        .getBytes()), tuple);
-
-    inserter.close();
-    writer1.finish();
-
-    writer.close();
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-  }
-
-  // @Test
-  /*
-   * group1 is user specific
-   */
-  public void test1() throws IOException, ParseException {
-
-    String schema = "s1:string, s2:string";
-    String storage = "[s1, s2]COMPRESS BY gz SECURE BY uid:user1 gid:users perm:744 SERIALIZE BY pig";
-    RawLocalFileSystem rawLFS = new RawLocalFileSystem();
-    fs = new LocalFileSystem(rawLFS);
-    Path path1 = new Path(path.toString() + "1");
-    Runtime.getRuntime().exec("rm -rf " + path1.toString());
-
-    fs = path.getFileSystem(conf);
-    BasicTable.Writer writer = new BasicTable.Writer(path1, schema, storage,
-        conf);
-    writer.finish();
-    ByteArrayOutputStream bos = new ByteArrayOutputStream();
-    PrintStream ps = new PrintStream(bos);
-    System.out.println("start dumpinfo ===========");
-    BasicTable.dumpInfo(path1.toString(), ps, conf);
-
-    Assert.assertEquals(true, bos.toString().contains("Serializer: pig"));
-    Assert.assertEquals(true, bos.toString().contains("Compressor: gz"));
-    Assert.assertEquals(true, bos.toString().contains(
-        "Schema : s1:string,s2:string"));
-  }
-
-  @Test
-  public void test2() throws IOException, ParseException {
-    String schema = "m1:map(string),m2:map(map(int))";
-    String storage = "[m1#{a}] SERIALIZE BY pig COMPRESS BY gz SECURE BY uid:"
-        + user + " gid:" + group + " perm:770 ";
-    Path path1 = new Path(path.toString() + "2");
-    BasicTable.Writer writer = null;
-    try {
-      BasicTable.drop(path1, conf);
-      writer = new BasicTable.Writer(path1, schema, storage, conf);
-    } catch (Exception e) {
-      e.printStackTrace();
-    }
-    writer.close();
-    ByteArrayOutputStream bos = new ByteArrayOutputStream();
-    PrintStream ps = new PrintStream(bos);
-
-    BasicTable.dumpInfo(path1.toString(), ps, conf);
-    System.out.println("start dumpinfo ===========\n" + bos.toString());
-    Assert.assertEquals(true, bos.toString().contains("Serializer: pig"));
-    Assert.assertEquals(true, bos.toString().contains("Compressor: gz"));
-    Assert.assertEquals(true, bos.toString().contains(
-        "Schema : m1:map(string),m2:map(map(int))"));
-  }
-
-  @Test
-  public void test3() throws IOException, ParseException {
-    String schema = "r1:record(f1:int, f2:long)";
-    String storage = "[r1.f1] SECURE BY uid:" + user + " gid:" + group
-        + " perm:777 SERIALIZE BY pig COMPRESS BY gz";
-    Path path1 = new Path(path.toString() + "3");
-    BasicTable.Writer writer = null;
-    try {
-      BasicTable.drop(path1, conf);
-      writer = new BasicTable.Writer(path1, schema, storage, conf);
-    } catch (Exception e) {
-      e.printStackTrace();
-    }
-    writer.close();
-    ByteArrayOutputStream bos = new ByteArrayOutputStream();
-    PrintStream ps = new PrintStream(bos);
-    BasicTable.dumpInfo(path1.toString(), ps, conf);
-    System.out.println("start dumpinfo ===========\n" + bos.toString());
-    Assert.assertEquals(true, bos.toString().contains("Serializer: pig"));
-    Assert.assertEquals(true, bos.toString().contains("Compressor: gz"));
-    // Assert.assertEquals(true,bos.toString().contains("Schema : r1:record(f1:int, f2:long)"));
-  }
-
-  @Test
-  public void test4() throws IOException, ParseException {
-    String schema = "s3:string, s4:string, r2:record(r3:record(f3:float, f4)), c:collection(record(f13:double, f14:float, f15:bytes))";
-    String storage = "[s3, s4, r2.r3.f3, c] SERIALIZE BY pig SECURE BY uid:"
-        + user + " gid:" + group + " perm:777 COMPRESS BY gz";
-    Path path1 = new Path(path.toString() + "4");
-    BasicTable.Writer writer = null;
-    try {
-      BasicTable.drop(path1, conf);
-      writer = new BasicTable.Writer(path1, schema, storage, conf);
-    } catch (Exception e) {
-      e.printStackTrace();
-    }
-    writer.close();
-    ByteArrayOutputStream bos = new ByteArrayOutputStream();
-    PrintStream ps = new PrintStream(bos);
-    BasicTable.dumpInfo(path1.toString(), ps, conf);
-    System.out.println("start dumpinfo ===========\n" + bos.toString());
-    Assert.assertEquals(true, bos.toString().contains("Serializer: pig"));
-    Assert.assertEquals(true, bos.toString().contains("Compressor: gz"));
-    // Assert.assertEquals(true,bos.toString().contains("s3:string,s4:string,r2:record(r3:record(f3:float, f4)),c:collection(f13:double,f14:float,f15:bytes)"));
-  }
-
-  @Test
-  public void test5() throws IOException, ParseException {
-    String schema = "s5:string, s6:string, m2:map(map(int))";
-    String storage = "[s5, s6, m2#{x|y}] compREss by gz secURe by uid:" + user
-        + " gid:" + group + " perm:777 SerialIZE BY pig";
-    Path path1 = new Path(path.toString() + "5");
-    BasicTable.Writer writer = null;
-    try {
-      BasicTable.drop(path1, conf);
-      writer = new BasicTable.Writer(path1, schema, storage, conf);
-    } catch (Exception e) {
-      e.printStackTrace();
-    }
-    writer.close();
-    ByteArrayOutputStream bos = new ByteArrayOutputStream();
-    PrintStream ps = new PrintStream(bos);
-    BasicTable.dumpInfo(path1.toString(), ps, conf);
-    System.out.println("start dumpinfo ===========\n" + bos.toString());
-    Assert.assertEquals(true, bos.toString().contains("Serializer: pig"));
-    // Assert.assertEquals(true,bos.toString().contains("Compressor: gz"));
-    Assert.assertEquals(true, bos.toString().contains(
-        "Schema : s5:string,s6:string,m2:map(map(int))"));
-  }
-
-  @Test
-  public void test6() throws IOException, ParseException {
-    String schema = "r1:record(f1:int, f2:long),m1:map(string)";
-    String storage = "[r1.f2, m1#{b}]";
-    Path path1 = new Path(path.toString() + "6");
-    BasicTable.Writer writer = null;
-    try {
-      BasicTable.drop(path1, conf);
-      writer = new BasicTable.Writer(path1, schema, storage, conf);
-    } catch (Exception e) {
-      e.printStackTrace();
-    }
-    writer.close();
-    ByteArrayOutputStream bos = new ByteArrayOutputStream();
-    PrintStream ps = new PrintStream(bos);
-    BasicTable.dumpInfo(path1.toString(), ps, conf);
-    System.out.println("start dumpinfo ===========\n" + bos.toString());
-    Assert.assertEquals(true, bos.toString().contains("Serializer: pig"));
-    // Assert.assertEquals(true,bos.toString().contains("Compressor: gz"));
-    // Assert.assertEquals(true,bos.toString().contains("Schema : r1:record(f1:int, f2:long),m1:map(string)"));
-  }
-
-  @Test
-  public void test7() throws IOException, ParseException {
-    System.out.println("========7777");
-    String schema = "r2:record(r3:record(f3:float, f4)),m2:map(map(int))";
-    String storage = "[r2.r3.f4, m2#{z}] SERIALIZE BY avro";
-    Path path1 = new Path(path.toString() + "7");
-    BasicTable.Writer writer = null;
-    try {
-      BasicTable.drop(path1, conf);
-      writer = new BasicTable.Writer(path1, schema, storage, conf);
-    } catch (Exception e) {
-      e.printStackTrace();
-    }
-    writer.close();
-    ByteArrayOutputStream bos = new ByteArrayOutputStream();
-    PrintStream ps = new PrintStream(bos);
-    BasicTable.dumpInfo(path1.toString(), ps, conf);
-    System.out.println("start dumpinfo ===========\n" + bos.toString());
-    Assert.assertEquals(true, bos.toString().contains("Serializer: avro"));
-    Assert.assertEquals(true, bos.toString().contains("Compressor: gz"));
-    // Assert.assertEquals(true,bos.toString().contains("Schema : r2:record(r3:record(f3:float, f4)),m2:map(map(int))"));
-  }
-
-  @Test
-  public void test8() throws IOException, ParseException {
-    String schema = "s7:string, s8:string";
-    String storage = "[s7,s8] SERIALIZE BY AvRO";
-    Path path1 = new Path(path.toString() + "8");
-    BasicTable.Writer writer = null;
-    try {
-      BasicTable.drop(path1, conf);
-      writer = new BasicTable.Writer(path1, schema, storage, conf);
-    } catch (Exception e) {
-      e.printStackTrace();
-    }
-    writer.close();
-    ByteArrayOutputStream bos = new ByteArrayOutputStream();
-    PrintStream ps = new PrintStream(bos);
-    BasicTable.dumpInfo(path1.toString(), ps, conf);
-    System.out.println("start dumpinfo ===========\n" + bos.toString());
-    Assert.assertEquals(true, bos.toString().contains("Serializer: AvRO"));
-    Assert.assertEquals(true, bos.toString().contains("Compressor: gz"));
-    Assert.assertEquals(true, bos.toString().contains(
-        "Schema : s7:string,s8:string"));
-  }
-
-  @Test(expected = IOException.class)
-  public void test9() throws IOException, ParseException {
-    String schema = "some1:string, some2:string";
-    String storage = "[some1,some2] SERIALIZE BY something";
-    Path path1 = new Path(path.toString() + "9");
-    BasicTable.Writer writer = null;
-
-    BasicTable.drop(path1, conf);
-    writer = new BasicTable.Writer(path1, schema, storage, conf);
-    Assert.fail("should throw exception, none defined serializer");
-    writer.finish();
-  }
-
-  @Test(expected = IOException.class)
-  public void test10() throws IOException, ParseException {
-    String schema = "some1:string, some2:string";
-    String storage = "[some1,some2] COMPRESS BY gz SERIALLLLIZE BY pig";
-    Path path1 = new Path(path.toString() + "10");
-    BasicTable.Writer writer = null;
-
-    BasicTable.drop(path1, conf);
-    writer = new BasicTable.Writer(path1, schema, storage, conf);
-    Assert.fail("should throw exception, none defined serializer");
-    writer.finish();
-  }
-
-  @Test(expected = IOException.class)
-  public void test11() throws IOException, ParseException {
-    String schema = "some1:string, some2:string";
-    String storage = "[some1,some2] COMPREEEEESS BY gz SERIALIZE BY pig";
-    Path path1 = new Path(path.toString() + "11");
-    BasicTable.Writer writer = null;
-
-    BasicTable.drop(path1, conf);
-    writer = new BasicTable.Writer(path1, schema, storage, conf);
-    Assert.fail("should throw exception, none defined serializer");
-    writer.finish();
-  }
-
-  @Test(expected = IOException.class)
-  public void test12() throws IOException, ParseException {
-    String schema = "some1:string, some2:string";
-    String storage = "[some1,some2] COMPRESS BY gz SECURrrrrrE BY uid:user1 gid:grop1 perm:760 SERIALIZE BY pig";
-    Path path1 = new Path(path.toString() + "12");
-    BasicTable.Writer writer = null;
-
-    BasicTable.drop(path1, conf);
-    writer = new BasicTable.Writer(path1, schema, storage, conf);
-    Assert.fail("should throw exception, none defined serializer");
-    writer.finish();
-  }
-
-  @Test(expected = IOException.class)
-  public void test13() throws IOException, ParseException {
-    String schema = "some1:string, some2:string";
-    String storage = "[some1,some2] COMPRESS gz SECURE BY uid:user1 gid:users perm:760 SERIALIZE BY pig";
-    Path path1 = new Path(path.toString() + "13");
-    BasicTable.Writer writer = null;
-
-    BasicTable.drop(path1, conf);
-    writer = new BasicTable.Writer(path1, schema, storage, conf);
-    Assert.fail("should throw exception, none defined serializer");
-    writer.finish();
-  }
-
-  @Test(expected = IOException.class)
-  public void test14() throws IOException, ParseException {
-    String schema = "some1:string, some2:string";
-    String storage = "[some1,some2] COMPRESS BY gz SECURE BY usssser:user1 gid:users perm:760 SERIALIZE BY pig";
-    Path path1 = new Path(path.toString() + "14");
-    
-    BasicTable.drop(path1, conf);
-    BasicTable.Writer writer = null;
-    writer = new BasicTable.Writer(path1, schema, storage, conf);
-    Assert.fail("should throw exception, none defined serializer");
-    writer.finish();
-  }
-
-  @Test(expected = IOException.class)
-  public void test15() throws IOException, ParseException {
-    String schema = "some1:string, some2:string";
-    String storage = "[some1,some2] COMPRESS BY gz SECURE BY uid:user1 grouuuup:group1 perm:760 SERIALIZE BY pig";
-    Path path1 = new Path(path.toString() + "15");
-    BasicTable.Writer writer = null;
-    BasicTable.drop(path1, conf);
-    writer = new BasicTable.Writer(path1, schema, storage, conf);
-    Assert.fail("should throw exception, none defined serializer");
-    writer.finish();
-  }
-
-  @Test(expected = IOException.class)
-  public void test16() throws IOException, ParseException {
-    String schema = "some1:string, some2:string";
-    String storage = "[some1,some2] COMPRESS BY gz SECURE BY uid:user1 gid:users perrrrrm:760 SERIALIZE BY pig";
-    Path path1 = new Path(path.toString() + "16");
-    BasicTable.Writer writer = null;
-    BasicTable.drop(path1, conf);
-    writer = new BasicTable.Writer(path1, schema, storage, conf);
-    Assert.fail("should throw exception, none defined serializer");
-    writer.finish();
-  }
-
-  @Test
-  public void test17() throws IOException, ParseException {
-    String schema = "s9:string, s10:string";
-    String storage = "[s9,s10] COMPRESS BY lzo";
-
-    Path path1 = new Path(path.toString() + "17");
-    Runtime.getRuntime().exec("rm -rf " + path1.toString());
-    fs = path.getFileSystem(conf);
-    BasicTable.drop(path1, conf);
-    BasicTable.Writer writer = new BasicTable.Writer(path1, schema, storage,
-        conf);
-    writer.close();
-    ByteArrayOutputStream bos = new ByteArrayOutputStream();
-    PrintStream ps = new PrintStream(bos);
-    System.out.println("start dumpinfo 17 ===========");
-    BasicTable.dumpInfo(path1.toString(), ps, conf);
-    System.out.println("bos: " + bos.toString());
-    Assert.assertEquals(true, bos.toString().contains("Serializer: pig"));
-    Assert.assertEquals(true, bos.toString().contains("Compressor: lzo"));
-    Assert.assertEquals(true, bos.toString().contains(
-        "Schema : s9:string,s10:string"));
-  }
-
-  // @Test
-  public void test18() throws IOException, ParseException {
-    String schema = "s9:string, s10:string";
-    String storage = "[s9,s10] COMPRESS BY gZ";
-
-    Path path1 = new Path(path.toString() + "18");
-    Runtime.getRuntime().exec("rm -rf " + path1.toString());
-    fs = path.getFileSystem(conf);
-    BasicTable.drop(path1, conf);
-    BasicTable.Writer writer = new BasicTable.Writer(path1, schema, storage,
-        conf);
-    writer.finish();
-    ByteArrayOutputStream bos = new ByteArrayOutputStream();
-    PrintStream ps = new PrintStream(bos);
-    System.out.println("start dumpinfo 18===========" + bos.toString());
-    BasicTable.dumpInfo(path1.toString(), ps, conf);
-    System.out.println("bos: " + bos.toString());
-
-    Assert.assertEquals(true, bos.toString().contains("Serializer: pig"));
-
-    Assert.assertEquals(true, bos.toString().contains("Compressor: gz"));
-    Assert.assertEquals(true, bos.toString().contains(
-        "Schema : s9:string,s10:string"));
-  }
-
-  @Test(expected = IOException.class)
-  public void test19() throws IOException, ParseException {
-    String schema = "some1:string, some2:string";
-    String storage = "[some1,some2] COMPRESS BY something";
-    Path path1 = new Path(path.toString() + "19");
-    BasicTable.drop(path1, conf);
-    BasicTable.Writer writer = null;
-    writer = new BasicTable.Writer(path1, schema, storage, conf);
-    Assert.fail("should throw exception, none defined serializer");
-    writer.finish();
-  }
-
-  // @Test
-  /*
-   * group1 is user specific
-   */
-  public void test20() throws IOException, ParseException {
-    String schema = "s9:string, s10:string, s11:string";
-    String storage = "[s9,s10] COMPRESS BY gZ; secure by uid:user1 gid:users perm:755";
-
-    Path path1 = new Path(path.toString() + "20");
-    Runtime.getRuntime().exec("rm -rf " + path1.toString());
-    fs = path.getFileSystem(conf);
-    BasicTable.drop(path1, conf);
-    BasicTable.Writer writer = new BasicTable.Writer(path1, schema, storage,
-        conf);
-    writer.finish();
-    ByteArrayOutputStream bos = new ByteArrayOutputStream();
-    PrintStream ps = new PrintStream(bos);
-    System.out.println("start dumpinfo 20===========" + bos.toString());
-    BasicTable.dumpInfo(path1.toString(), ps, conf);
-    System.out.println("bos: " + bos.toString());
-
-    Assert.assertEquals(true, bos.toString().contains("Serializer: pig"));
-    Assert.assertEquals(true, bos.toString().contains("Compressor: gz"));
-    Assert.assertEquals(true, bos.toString().contains(
-        "Schema : s9:string,s10:string"));
-  }
-
-  @Test(expected = IOException.class)
-  public void test21() throws IOException, ParseException {
-    String schema = "s9:string, s10:string, s11:string";
-    String storage = "[s9,s10] COMPRESS BY gZ; secure by uid:user1 gid:users perm:755;secure by user:user1 gid:users perm:755";
-    Path path1 = new Path(path.toString() + "16");
-    BasicTable.drop(path1, conf);
-    BasicTable.Writer writer = null;
-    writer = new BasicTable.Writer(path1, schema, storage, conf);
-    Assert.fail("should throw exception, none defined serializer");
-    writer.finish();
-  }
-
-  @Test
-  public void test22() throws IOException, ParseException {
-    String schema = "s9:string, s10:string";
-    String storage = "[s9,s10] secure by uid:user3";
-
-    Path path1 = new Path(path.toString() + "22");
-    BasicTable.drop(path1, conf);
-    Runtime.getRuntime().exec("rm -rf " + path1.toString());
-    fs = path.getFileSystem(conf);
-    BasicTable.Writer writer = new BasicTable.Writer(path1, schema, storage,
-        conf);
-    writer.close();
-    ByteArrayOutputStream bos = new ByteArrayOutputStream();
-    PrintStream ps = new PrintStream(bos);
-    System.out.println("start dumpinfo 22===========" + bos.toString());
-    BasicTable.dumpInfo(path1.toString(), ps, conf);
-    System.out.println("bos: " + bos.toString());
-
-    Assert.assertEquals(true, bos.toString().contains("Serializer: pig"));
-    Assert.assertEquals(true, bos.toString().contains("Compressor: gz"));
-    Assert.assertEquals(true, bos.toString().contains(
-        "Schema : s9:string,s10:string"));
-    Assert.assertEquals(true, bos.toString().contains("Group: null"));
-    Assert.assertEquals(true, bos.toString().contains("Perm: -1"));
-  }
-
-  // @Test
-  /*
-   * group1 is user specific
-   */
-  public void test23() throws IOException, ParseException {
-    String schema = "s9:string, s10:string";
-    String storage = "[s9,s10] secure by gid:users";
-
-    Path path1 = new Path(path.toString() + "23");
-    BasicTable.drop(path1, conf);
-    Runtime.getRuntime().exec("rm -rf " + path1.toString());
-    fs = path.getFileSystem(conf);
-    BasicTable.Writer writer = new BasicTable.Writer(path1, schema, storage,
-        conf);
-    writer.finish();
-    ByteArrayOutputStream bos = new ByteArrayOutputStream();
-    PrintStream ps = new PrintStream(bos);
-    System.out.println("start dumpinfo 23===========" + bos.toString());
-    BasicTable.dumpInfo(path1.toString(), ps, conf);
-    System.out.println("bos: " + bos.toString());
-
-    Assert.assertEquals(true, bos.toString().contains("Serializer: pig"));
-    Assert.assertEquals(true, bos.toString().contains("Compressor: gz"));
-    Assert.assertEquals(true, bos.toString().contains(
-        "Schema : s9:string,s10:string"));
-    Assert.assertEquals(true, bos.toString().contains("Group: users"));
-    Assert.assertEquals(true, bos.toString().contains("Perm: -1"));
-  }
-
-  // @Test
-  public void test24() throws IOException, ParseException {
-    String schema = "s9:string, s10:string";
-    String storage = "[s9,s10] secure by perm:755";
-
-    Path path1 = new Path(path.toString() + "24");
-    BasicTable.drop(path1, conf);
-    Runtime.getRuntime().exec("rm -rf " + path1.toString());
-    fs = path.getFileSystem(conf);
-    BasicTable.Writer writer = new BasicTable.Writer(path1, schema, storage,
-        conf);
-    writer.finish();
-    ByteArrayOutputStream bos = new ByteArrayOutputStream();
-    PrintStream ps = new PrintStream(bos);
-    System.out.println("start dumpinfo 24===========" + bos.toString());
-    BasicTable.dumpInfo(path1.toString(), ps, conf);
-    System.out.println("bos: " + bos.toString());
-
-    Assert.assertEquals(true, bos.toString().contains("Serializer: pig"));
-    Assert.assertEquals(true, bos.toString().contains("Compressor: gz"));
-    Assert.assertEquals(true, bos.toString().contains(
-        "Schema : s9:string,s10:string"));
-    Assert.assertEquals(true, bos.toString().contains("Group:"));
-    Assert.assertEquals(true, bos.toString().contains("Perm: 0755"));
-  }
-
-  // @Test
-  /*
-   * group1 is user specific
-   */
-  public void test25() throws IOException, ParseException {
-    String schema = "s9:string, s10:string";
-    String storage = "[s9,s10] secure by uid:user1 gid:users";
-
-    Path path1 = new Path(path.toString() + "25");
-    BasicTable.drop(path1, conf);
-    Runtime.getRuntime().exec("rm -rf " + path1.toString());
-    fs = path.getFileSystem(conf);
-    BasicTable.Writer writer = new BasicTable.Writer(path1, schema, storage,
-        conf);
-    writer.finish();
-    ByteArrayOutputStream bos = new ByteArrayOutputStream();
-    PrintStream ps = new PrintStream(bos);
-    System.out.println("start dumpinfo 25===========" + bos.toString());
-    BasicTable.dumpInfo(path1.toString(), ps, conf);
-    System.out.println("bos: " + bos.toString());
-
-    Assert.assertEquals(true, bos.toString().contains("Serializer: pig"));
-    Assert.assertEquals(true, bos.toString().contains("Compressor: gz"));
-    Assert.assertEquals(true, bos.toString().contains(
-        "Schema : s9:string,s10:string"));
-    Assert.assertEquals(true, bos.toString().contains("Group: users"));
-    Assert.assertEquals(true, bos.toString().contains("Perm: -1"));
-  }
-
-  // @Test
-  public void test26() throws IOException, ParseException {
-    String schema = "s9:string, s10:string";
-    String storage = "[s9,s10] secure by perm:755 gid:users";
-
-    Path path1 = new Path(path.toString() + "26");
-    BasicTable.drop(path1, conf);
-    Runtime.getRuntime().exec("rm -rf " + path1.toString());
-    fs = path.getFileSystem(conf);
-    BasicTable.Writer writer = new BasicTable.Writer(path1, schema, storage,
-        conf);
-    writer.finish();
-    ByteArrayOutputStream bos = new ByteArrayOutputStream();
-    PrintStream ps = new PrintStream(bos);
-    System.out.println("start dumpinfo 26===========" + bos.toString());
-    BasicTable.dumpInfo(path1.toString(), ps, conf);
-    System.out.println("bos: " + bos.toString());
-
-    Assert.assertEquals(true, bos.toString().contains("Serializer: pig"));
-    Assert.assertEquals(true, bos.toString().contains("Compressor: gz"));
-    Assert.assertEquals(true, bos.toString().contains(
-        "Schema : s9:string,s10:string"));
-    Assert.assertEquals(true, bos.toString().contains("Group: users"));
-    Assert.assertEquals(true, bos.toString().contains("Perm: 755"));
-  }
-
-  @Test(expected = IOException.class)
-  public void test27() throws IOException, ParseException {
-    String schema = "s9:string, s10:string";
-    String storage = "[s9,s10] COMPRESS BY gZ; secure by ";
-    Path path1 = new Path(path.toString() + "27");
-    BasicTable.drop(path1, conf);
-    BasicTable.Writer writer = null;
-    writer = new BasicTable.Writer(path1, schema, storage, conf);
-    Assert.fail("should throw exception, none defined serializer");
-    writer.finish();
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestStorageMap.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestStorageMap.java
deleted file mode 100644
index 04668bb7c..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestStorageMap.java
+++ /dev/null
@@ -1,271 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.types;
-
-import java.io.StringReader;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.HashSet;
-import java.util.TreeSet;
-import junit.framework.Assert;
-
-import org.apache.hadoop.zebra.types.CGSchema;
-import org.apache.hadoop.zebra.schema.ColumnType;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Partition;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.parser.TableSchemaParser;
-import org.apache.hadoop.zebra.schema.Schema.ColumnSchema;
-import org.junit.Before;
-import org.junit.Test;
-
-public class TestStorageMap {
-  String strSch = "m1:map(map(float)), m2:map(bool), f3:int";
-  TableSchemaParser parser;
-  Schema schema;
-
-  @Before
-  public void init() throws ParseException {
-    parser = new TableSchemaParser(new StringReader(strSch));
-    schema = parser.RecordSchema(null);
-
-  }
-
-  @Test
-  public void testStorageValid1() {
-    try {
-      String strStorage = "[m1#{k1}]; [m2#{k1}, f3]";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      CGSchema[] cgschemas = p.getCGSchemas();
-
-      // 3 column group;
-      int size = cgschemas.length;
-      Assert.assertEquals(size, 3);
-      System.out.println("********** Column Groups **********");
-      for (int i = 0; i < cgschemas.length; i++) {
-        System.out.println(cgschemas[i]);
-        System.out.println("--------------------------------");
-      }
-      CGSchema cgs1 = cgschemas[0];
-      CGSchema cgs2 = cgschemas[1];
-      CGSchema cgs3 = cgschemas[2];
-
-      ColumnSchema f11 = cgs1.getSchema().getColumn(0);
-      Assert.assertEquals(f11.getName(), "m1");
-      Assert.assertEquals(ColumnType.MAP, f11.getType());
-
-      ColumnSchema f21 = cgs2.getSchema().getColumn(0);
-      Assert.assertEquals(f21.getName(), "m2");
-      // TODO: type should be MAP!
-      Assert.assertEquals(ColumnType.MAP, f21.getType());
-
-      ColumnSchema f22 = cgs2.getSchema().getColumn(1);
-      Assert.assertEquals(f22.getName(), "f3");
-      Assert.assertEquals(ColumnType.INT, f22.getType());
-      ColumnSchema f31 = cgs3.getSchema().getColumn(0);
-      Assert.assertEquals(f31.getName(), "m1");
-      Assert.assertEquals(ColumnType.MAP, f31.getType());
-      ColumnSchema f32 = cgs3.getSchema().getColumn(1);
-      Assert.assertEquals(f32.getName(), "m2");
-      Assert.assertEquals(ColumnType.MAP, f32.getType());
-
-      System.out.println("*********** Column Map **********");
-      Map<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>> colmap = p
-          .getPartitionInfo().getColMap();
-      Assert.assertEquals(colmap.size(), 3);
-      Iterator<Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>>> it = colmap
-          .entrySet().iterator();
-      for (int i = 0; i < colmap.size(); i++) {
-        Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>> entry = (Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>>) it
-            .next();
-        String name = entry.getKey();
-        HashSet<Partition.PartitionInfo.ColumnMappingEntry> hs = entry
-            .getValue();
-        TreeSet<Partition.PartitionInfo.ColumnMappingEntry> ts = new TreeSet<Partition.PartitionInfo.ColumnMappingEntry>(
-            hs);
-        Iterator<Partition.PartitionInfo.ColumnMappingEntry> it1 = ts
-            .iterator();
-        for (int j = 0; j < ts.size(); j++) {
-          Partition.PartitionInfo.ColumnMappingEntry cme = (Partition.PartitionInfo.ColumnMappingEntry) it1
-              .next();
-          System.out.println("[Column = " + name + " CG = " + cme.getCGIndex()
-              + "." + cme.getFieldIndex() + "]");
-          if (i == 0 && j == 0) {
-            Assert.assertEquals(name, "m1");
-            Assert.assertEquals(cme.getCGIndex(), 0);
-            Assert.assertEquals(cme.getFieldIndex(), 0);
-          } else if (i == 0 && j == 1) {
-            Assert.assertEquals(name, "m1");
-            Assert.assertEquals(cme.getCGIndex(), 2);
-            Assert.assertEquals(cme.getFieldIndex(), 0);
-          } else if (i == 1 && j == 0) {
-            Assert.assertEquals(name, "m2");
-            Assert.assertEquals(cme.getCGIndex(), 1);
-            Assert.assertEquals(cme.getFieldIndex(), 0);
-          } else if (i == 1 && j == 1) {
-            Assert.assertEquals(name, "m2");
-            Assert.assertEquals(cme.getCGIndex(), 2);
-            Assert.assertEquals(cme.getFieldIndex(), 1);
-          } else if (i == 2 && j == 0) {
-            Assert.assertEquals(name, "f3");
-            Assert.assertEquals(cme.getCGIndex(), 1);
-            Assert.assertEquals(cme.getFieldIndex(), 1);
-          }
-        }
-      }
-    } catch (Exception e) {
-      Assert.assertTrue(false);
-    }
-  }
-
-  @Test
-  public void testStorageValid2() {
-    try {
-      String strStorage = "[m1#{k1}]; [m1#{k2}, f3]";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      CGSchema[] cgschemas = p.getCGSchemas();
-
-      // 3 column group;
-      int size = cgschemas.length;
-      Assert.assertEquals(size, 3);
-      System.out.println("********** Column Groups **********");
-      for (int i = 0; i < cgschemas.length; i++) {
-        System.out.println(cgschemas[i]);
-        System.out.println("--------------------------------");
-      }
-      CGSchema cgs1 = cgschemas[0];
-      CGSchema cgs2 = cgschemas[1];
-      CGSchema cgs3 = cgschemas[2];
-
-      ColumnSchema f11 = cgs1.getSchema().getColumn(0);
-      Assert.assertEquals(f11.getName(), "m1");
-      Assert.assertEquals(ColumnType.MAP, f11.getType());
-      ColumnSchema f21 = cgs2.getSchema().getColumn(0);
-      Assert.assertEquals(f21.getName(), "m1");
-      Assert.assertEquals(ColumnType.MAP, f21.getType());
-      ColumnSchema f22 = cgs2.getSchema().getColumn(1);
-      Assert.assertEquals(f22.getName(), "f3");
-      Assert.assertEquals(ColumnType.INT, f22.getType());
-      ColumnSchema f31 = cgs3.getSchema().getColumn(0);
-      Assert.assertEquals(f31.getName(), "m1");
-      Assert.assertEquals(ColumnType.MAP, f31.getType());
-      ColumnSchema f32 = cgs3.getSchema().getColumn(1);
-      Assert.assertEquals(f32.getName(), "m2");
-      Assert.assertEquals(ColumnType.MAP, f32.getType());
-
-      System.out.println("*********** Column Map **********");
-      Map<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>> colmap = p
-          .getPartitionInfo().getColMap();
-      Assert.assertEquals(colmap.size(), 3);
-      Iterator<Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>>> it = colmap
-          .entrySet().iterator();
-      for (int i = 0; i < colmap.size(); i++) {
-        Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>> entry = (Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>>) it
-            .next();
-        String name = entry.getKey();
-        HashSet<Partition.PartitionInfo.ColumnMappingEntry> hs = entry
-            .getValue();
-        TreeSet<Partition.PartitionInfo.ColumnMappingEntry> ts = new TreeSet<Partition.PartitionInfo.ColumnMappingEntry>(
-            hs);
-        Iterator<Partition.PartitionInfo.ColumnMappingEntry> it1 = ts
-            .iterator();
-        for (int j = 0; j < ts.size(); j++) {
-          Partition.PartitionInfo.ColumnMappingEntry cme = (Partition.PartitionInfo.ColumnMappingEntry) it1
-              .next();
-          System.out.println("[Column = " + name + " CG = " + cme.getCGIndex()
-              + "." + cme.getFieldIndex() + "]");
-          if (i == 0 && j == 0) {
-            Assert.assertEquals(name, "m1");
-            Assert.assertEquals(cme.getCGIndex(), 0);
-            Assert.assertEquals(cme.getFieldIndex(), 0);
-          } else if (i == 0 && j == 1) {
-            Assert.assertEquals(name, "m1");
-            Assert.assertEquals(cme.getCGIndex(), 1);
-            Assert.assertEquals(cme.getFieldIndex(), 0);
-          } else if (i == 0 && j == 1) {
-            Assert.assertEquals(name, "m1");
-            Assert.assertEquals(cme.getCGIndex(), 2);
-            Assert.assertEquals(cme.getFieldIndex(), 0);
-          } else if (i == 1 && j == 0) {
-            Assert.assertEquals(name, "m2");
-            Assert.assertEquals(cme.getCGIndex(), 2);
-            Assert.assertEquals(cme.getFieldIndex(), 1);
-          } else if (i == 2 && j == 0) {
-            Assert.assertEquals(name, "f3");
-            Assert.assertEquals(cme.getCGIndex(), 1);
-            Assert.assertEquals(cme.getFieldIndex(), 1);
-          }
-        }
-      }
-    } catch (Exception e) {
-      Assert.assertTrue(false);
-    }
-  }
-
-  @Test
-  public void testStorageInvalid1() {
-    try {
-      String strStorage = "m1#{k1}";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      CGSchema[] cgschemas = p.getCGSchemas();
-      CGSchema cgs1 = cgschemas[0];
-      System.out.println(cgs1);
-    } catch (Exception e) {
-      String errMsg = e.getMessage();
-      String str = "Encountered \" <IDENTIFIER> \"m1 \"\" at line 1, column 1.";
-      System.out.println(errMsg);
-      System.out.println(str);
-      Assert.assertEquals(errMsg.startsWith(str), true);
-    }
-  }
-
-  @Test
-  public void testStorageInvalid2() {
-    try {
-      String strStorage = "[m1#{k1}] abc; [m1#{k2}, f3] xyz";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      CGSchema[] cgschemas = p.getCGSchemas();
-      CGSchema cgs1 = cgschemas[0];
-      System.out.println(cgs1);
-    } catch (Exception e) {
-      String errMsg = e.getMessage();
-      String str = "Encountered \" <IDENTIFIER> \"abc \"\" at line 1, column 11.";
-      System.out.println(errMsg);
-      System.out.println(str);
-      Assert.assertEquals(errMsg.startsWith(str), true);
-    }
-  }
-
-  @Test
-  public void testStorageInvalid3() {
-    try {
-      String strStorage = "[m1{#k1}{#k2}]";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      CGSchema[] cgschemas = p.getCGSchemas();
-      CGSchema cgs1 = cgschemas[0];
-      System.out.println(cgs1);
-    } catch (Exception e) {
-      String errMsg = e.getMessage();
-      String str = "Encountered \" \"{\" \"{ \"\" at line 1, column 4.";
-      System.out.println(errMsg);
-      System.out.println(str);
-      Assert.assertEquals(errMsg.startsWith(str), true);
-    }
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestStorageMisc1.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestStorageMisc1.java
deleted file mode 100644
index 55912dc73..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestStorageMisc1.java
+++ /dev/null
@@ -1,168 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.types;
-
-import java.io.StringReader;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.HashSet;
-import junit.framework.Assert;
-
-import org.apache.hadoop.zebra.types.CGSchema;
-import org.apache.hadoop.zebra.schema.ColumnType;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Partition;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.parser.TableSchemaParser;
-import org.apache.hadoop.zebra.schema.Schema.ColumnSchema;
-import org.junit.Before;
-import org.junit.Test;
-
-public class TestStorageMisc1 {
-  String strSch = "r:record(r:record(f1:int, f2:int), f2:map)";
-  TableSchemaParser parser;
-  Schema schema;
-
-  @Before
-  public void init() throws ParseException {
-    parser = new TableSchemaParser(new StringReader(strSch));
-    schema = parser.RecordSchema(null);
-  }
-
-  @Test
-  public void testSchema() throws ParseException {
-    System.out.println(schema);
-
-    // test 1st level schema;
-    ColumnSchema f1 = schema.getColumn(0);
-    Assert.assertEquals("r", f1.getName());
-    Assert.assertEquals(ColumnType.RECORD, f1.getType());
-
-    // test 2nd level schema;
-    Schema f1Schema = f1.getSchema();
-    ColumnSchema f11 = f1Schema.getColumn(0);
-    Assert.assertEquals("r", f11.getName());
-    Assert.assertEquals(ColumnType.RECORD, f11.getType());
-    ColumnSchema f12 = f1Schema.getColumn(1);
-    Assert.assertEquals("f2", f12.getName());
-    Assert.assertEquals(ColumnType.MAP, f12.getType());
-
-    // test 3rd level schema;
-    Schema f11Schema = f11.getSchema();
-    ColumnSchema f111 = f11Schema.getColumn(0);
-    Assert.assertEquals("f1", f111.getName());
-    Assert.assertEquals(ColumnType.INT, f111.getType());
-    ColumnSchema f112 = f11Schema.getColumn(1);
-    Assert.assertEquals("f2", f112.getName());
-    Assert.assertEquals(ColumnType.INT, f112.getType());
-
-    Schema f12Schema = f12.getSchema();
-    ColumnSchema f121 = f12Schema.getColumn(0);
-    // Assert.assertEquals("", f121.getName());
-    Assert.assertEquals(ColumnType.BYTES, f121.getType());
-  }
-
-  @Test
-  public void testStorageValid1() {
-    try {
-        String strStorage = "[r.r.f1,r.f2#{k1}] COMPRESS BY gz SECURE BY uid:root; [r.r.f2, r.f2#{k2}] COMPRESS BY lzo SERIALIZE BY avro";
-//      String strStorage = "[r.r.f1,r.f2#{k1}] COMPRESS BY gzip SECURE BY uid:root; [r.r.f2, r.f2#{k2}] COMPRESS BY lzo SERIALIZE BY avro";
-//      String strStorage = "[r.r.f1,r.f2#{k1}] COMPRESS BY gzip SECURE BY uid:root group:data perm:0766; [r.r.f2, r.f2#{k2}] COMPRESS BY lzo SERIALIZE BY avro";
-//      String strStorage = "[r.r.f1,r.f2#{k1}] COMPRESS BY gzip SECURE BY uid:root group:data perm:966; [r.r.f2, r.f2#{k2}] COMPRESS BY lzo SERIALIZE BY avro";
-//      String strStorage = "[r.r.f1,r.f2#{k1}] COMPRESS BY gzip SECURE BY; [r.r.f2, r.f2#{k2}] COMPRESS BY lzo SERIALIZE BY avro";
-//      String strStorage = "[r.r.f1,r.f2#{k1}] COMPRESS BY gzip SECURE BY uid:ggg SECURE BY group:fff; [r.r.f2, r.f2#{k2}] COMPRESS BY lzo SERIALIZE BY avro";
-//      String strStorage = "[r.r.f1,r.f2#{k1}] COMPRESS BY gzip SECURE BY uid:root user:root; [r.r.f2, r.f2#{k2}] COMPRESS BY lzo SERIALIZE BY avro";
-
-    	Partition p = new Partition(schema.toString(), strStorage, null);
-      CGSchema[] cgschemas = p.getCGSchemas();
-
-      // 3 column group;
-      int size = cgschemas.length;
-      Assert.assertEquals(size, 3);
-      System.out.println("********** Column Groups **********");
-      for (int i = 0; i < cgschemas.length; i++) {
-        System.out.println(cgschemas[i]);
-        System.out.println("--------------------------------");
-      }
-      CGSchema cgs1 = cgschemas[0];
-      CGSchema cgs2 = cgschemas[1];
-      CGSchema cgs3 = cgschemas[2];
-
-      ColumnSchema f11 = cgs1.getSchema().getColumn(0);
-      Assert.assertEquals("r.r.f1", f11.getName());
-      Assert.assertEquals(ColumnType.INT, f11.getType());
-      ColumnSchema f12 = cgs1.getSchema().getColumn(1);
-      Assert.assertEquals("r.f2", f12.getName());
-      Assert.assertEquals(ColumnType.MAP, f12.getType());
-
-      ColumnSchema f21 = cgs2.getSchema().getColumn(0);
-      Assert.assertEquals("r.r.f2", f21.getName());
-      Assert.assertEquals(ColumnType.INT, f21.getType());
-      ColumnSchema f22 = cgs2.getSchema().getColumn(1);
-      Assert.assertEquals("r.f2", f22.getName());
-      Assert.assertEquals(ColumnType.MAP, f22.getType());
-
-      ColumnSchema f31 = cgs3.getSchema().getColumn(0);
-      Assert.assertEquals("r.f2", f31.getName());
-      Assert.assertEquals(ColumnType.MAP, f31.getType());
-
-      System.out.println("*********** Column Map **********");
-      Map<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>> colmap = p
-          .getPartitionInfo().getColMap();
-      Assert.assertEquals(colmap.size(), 3);
-      Iterator<Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>>> it = colmap
-          .entrySet().iterator();
-      for (int i = 0; i < colmap.size(); i++) {
-        Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>> entry = (Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>>) it
-            .next();
-        String name = entry.getKey();
-        HashSet<Partition.PartitionInfo.ColumnMappingEntry> hs = entry
-            .getValue();
-        Iterator<Partition.PartitionInfo.ColumnMappingEntry> it1 = hs
-            .iterator();
-        for (int j = 0; j < hs.size(); j++) {
-          Partition.PartitionInfo.ColumnMappingEntry cme = (Partition.PartitionInfo.ColumnMappingEntry) it1
-              .next();
-          System.out.println("[Column = " + name + " CG = " + cme.getCGIndex()
-              + "." + cme.getFieldIndex() + "]");
-          /*
-           * if (i == 0 && j == 0) { Assert.assertEquals(name, "r.r.f1");
-           * Assert.assertEquals(cme.getCGIndex(), 0);
-           * Assert.assertEquals(cme.getFieldIndex(), 0); } else if (i == 1 && j
-           * == 0) { Assert.assertEquals(name, "r.r.f2");
-           * Assert.assertEquals(cme.getCGIndex(), 1);
-           * Assert.assertEquals(cme.getFieldIndex(), 0); } else if (i == 2 && j
-           * == 0) { Assert.assertEquals(name, "r.f2");
-           * Assert.assertEquals(cme.getCGIndex(), 1);
-           * Assert.assertEquals(cme.getFieldIndex(), 1); } else if (i == 2 && j
-           * == 1) { Assert.assertEquals(name, "r.f2");
-           * Assert.assertEquals(cme.getCGIndex(), 2);
-           * Assert.assertEquals(cme.getFieldIndex(), 0); } else if (i == 2 && j
-           * == 2) { Assert.assertEquals(name, "r.f2");
-           * Assert.assertEquals(cme.getCGIndex(), 0);
-           * Assert.assertEquals(cme.getFieldIndex(), 1); }
-           */
-        }
-      }
-    } catch (Exception e) {
-      System.out.println("Error is [" + e.getMessage() + "]");
-      Assert.assertTrue(false);
-    }
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestStorageMisc2.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestStorageMisc2.java
deleted file mode 100644
index 48bba81a4..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestStorageMisc2.java
+++ /dev/null
@@ -1,226 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.types;
-
-import java.io.StringReader;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.HashSet;
-import junit.framework.Assert;
-
-import org.apache.hadoop.zebra.types.CGSchema;
-import org.apache.hadoop.zebra.schema.ColumnType;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Partition;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.parser.TableSchemaParser;
-import org.apache.hadoop.zebra.schema.Schema.ColumnSchema;
-import org.junit.Before;
-import org.junit.Test;
-
-public class TestStorageMisc2 {
-  String strSch = "c:collection(r:record(r:record(f1:int, f2:int), f2:map)), m1:map(int)";
-  TableSchemaParser parser;
-  Schema schema;
-
-  @Before
-  public void init() throws ParseException {
-    parser = new TableSchemaParser(new StringReader(strSch));
-    schema = parser.RecordSchema(null);
-  }
-
-  @Test
-  public void testSchema() throws ParseException {
-    System.out.println(schema);
-
-    // test 1st level schema;
-    ColumnSchema f1 = schema.getColumn(0);
-    Assert.assertEquals("c", f1.getName());
-    Assert.assertEquals(ColumnType.COLLECTION, f1.getType());
-    ColumnSchema f2 = schema.getColumn(1);
-    Assert.assertEquals("m1", f2.getName());
-    Assert.assertEquals(ColumnType.MAP, f2.getType());
-
-    // test 2nd level schema;
-    Schema f1Schema = f1.getSchema();
-    ColumnSchema f11 = f1Schema.getColumn(0);
-    Assert.assertEquals("r", f11.getName());
-    Assert.assertEquals(ColumnType.RECORD, f11.getType());
-
-    Schema f2Schema = f2.getSchema();
-    ColumnSchema f21 = f2Schema.getColumn(0);
-    // Assert.assertEquals("", f21.getName());
-    Assert.assertEquals(ColumnType.INT, f21.getType());
-
-    // test 3rd level schema;
-    Schema f11Schema = f11.getSchema();
-    ColumnSchema f111 = f11Schema.getColumn(0);
-    Assert.assertEquals("r", f111.getName());
-    Assert.assertEquals(ColumnType.RECORD, f111.getType());
-    ColumnSchema f112 = f11Schema.getColumn(1);
-    Assert.assertEquals("f2", f112.getName());
-    Assert.assertEquals(ColumnType.MAP, f112.getType());
-
-    // test 4th level schema;
-    Schema f111Schema = f111.getSchema();
-    ColumnSchema f1111 = f111Schema.getColumn(0);
-    Assert.assertEquals("f1", f1111.getName());
-    Assert.assertEquals(ColumnType.INT, f1111.getType());
-    ColumnSchema f1112 = f111Schema.getColumn(1);
-    Assert.assertEquals("f2", f1112.getName());
-    Assert.assertEquals(ColumnType.INT, f1112.getType());
-
-    Schema f112Schema = f112.getSchema();
-    ColumnSchema f1121 = f112Schema.getColumn(0);
-    // Assert.assertEquals("", f1121.getName());
-    Assert.assertEquals(ColumnType.BYTES, f1121.getType());
-  }
-
-  @Test
-  public void testStorageValid1() {
-    try {
-      String strStorage = "[c] compress by gz; [m1] serialize by avro";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      CGSchema[] cgschemas = p.getCGSchemas();
-
-      // 2 column group;
-      int size = cgschemas.length;
-      Assert.assertEquals(size, 2);
-      System.out.println("********** Column Groups **********");
-      for (int i = 0; i < cgschemas.length; i++) {
-        System.out.println(cgschemas[i]);
-        System.out.println("--------------------------------");
-      }
-      CGSchema cgs1 = cgschemas[0];
-      CGSchema cgs2 = cgschemas[1];
-
-      ColumnSchema f11 = cgs1.getSchema().getColumn(0);
-      Assert.assertEquals("c", f11.getName());
-      Assert.assertEquals(ColumnType.COLLECTION, f11.getType());
-
-      ColumnSchema f21 = cgs2.getSchema().getColumn(0);
-      Assert.assertEquals("m1", f21.getName());
-      Assert.assertEquals(ColumnType.MAP, f21.getType());
-
-      Assert.assertEquals(cgs1.getCompressor(), "gz");
-      Assert.assertEquals(cgs2.getSerializer(), "avro");
-
-      System.out.println("*********** Column Map **********");
-      Map<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>> colmap = p
-          .getPartitionInfo().getColMap();
-      Assert.assertEquals(colmap.size(), 2);
-      Iterator<Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>>> it = colmap
-          .entrySet().iterator();
-      for (int i = 0; i < colmap.size(); i++) {
-        Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>> entry = (Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>>) it
-            .next();
-        String name = entry.getKey();
-        HashSet<Partition.PartitionInfo.ColumnMappingEntry> hs = entry
-            .getValue();
-        Iterator<Partition.PartitionInfo.ColumnMappingEntry> it1 = hs
-            .iterator();
-        for (int j = 0; j < hs.size(); j++) {
-          Partition.PartitionInfo.ColumnMappingEntry cme = (Partition.PartitionInfo.ColumnMappingEntry) it1
-              .next();
-          System.out.println("[Column = " + name + " CG = " + cme.getCGIndex()
-              + "." + cme.getFieldIndex() + "]");
-          if (i == 0 && j == 0) {
-            Assert.assertEquals(name, "c");
-            Assert.assertEquals(cme.getCGIndex(), 0);
-            Assert.assertEquals(cme.getFieldIndex(), 0);
-          } else if (i == 1 && j == 0) {
-            Assert.assertEquals(name, "m1");
-            Assert.assertEquals(cme.getCGIndex(), 1);
-            Assert.assertEquals(cme.getFieldIndex(), 0);
-          }
-        }
-      }
-    } catch (Exception e) {
-      Assert.assertTrue(false);
-    }
-  }
-
-  @Test
-  public void testStorageValid2() {
-    try {
-      String strStorage = "[c] compress by gz; [m1#{k1}] serialize by avro";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      CGSchema[] cgschemas = p.getCGSchemas();
-
-      // 3 column group;
-      int size = cgschemas.length;
-      Assert.assertEquals(size, 3);
-      System.out.println("********** Column Groups **********");
-      for (int i = 0; i < cgschemas.length; i++) {
-        System.out.println(cgschemas[i]);
-        System.out.println("--------------------------------");
-      }
-      CGSchema cgs1 = cgschemas[0];
-      CGSchema cgs2 = cgschemas[1];
-      CGSchema cgs3 = cgschemas[2];
-
-      ColumnSchema f11 = cgs1.getSchema().getColumn(0);
-      Assert.assertEquals("c", f11.getName());
-      Assert.assertEquals(ColumnType.COLLECTION, f11.getType());
-
-      ColumnSchema f21 = cgs2.getSchema().getColumn(0);
-      Assert.assertEquals("m1", f21.getName());
-      Assert.assertEquals(ColumnType.MAP, f21.getType());
-
-      ColumnSchema f31 = cgs3.getSchema().getColumn(0);
-      Assert.assertEquals("m1", f31.getName());
-      Assert.assertEquals(ColumnType.MAP, f31.getType());
-
-      System.out.println("*********** Column Map **********");
-      Map<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>> colmap = p
-          .getPartitionInfo().getColMap();
-      Assert.assertEquals(colmap.size(), 2);
-      Iterator<Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>>> it = colmap
-          .entrySet().iterator();
-      for (int i = 0; i < colmap.size(); i++) {
-        Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>> entry = (Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>>) it
-            .next();
-        String name = entry.getKey();
-        HashSet<Partition.PartitionInfo.ColumnMappingEntry> hs = entry
-            .getValue();
-        Iterator<Partition.PartitionInfo.ColumnMappingEntry> it1 = hs
-            .iterator();
-        for (int j = 0; j < hs.size(); j++) {
-          Partition.PartitionInfo.ColumnMappingEntry cme = (Partition.PartitionInfo.ColumnMappingEntry) it1
-              .next();
-          System.out.println("[Column = " + name + " CG = " + cme.getCGIndex()
-              + "." + cme.getFieldIndex() + "]");
-          /*
-           * if (i == 0 && j == 0) { Assert.assertEquals(name, "c");
-           * Assert.assertEquals(cme.getCGIndex(), 0);
-           * Assert.assertEquals(cme.getFieldIndex(), 0); } else if (i == 0 && j
-           * == 1) { Assert.assertEquals(name, "m1");
-           * Assert.assertEquals(cme.getCGIndex(), 1);
-           * Assert.assertEquals(cme.getFieldIndex(), 0); } else if (i == 1 && j
-           * == 0) { Assert.assertEquals(name, "m1");
-           * Assert.assertEquals(cme.getCGIndex(), 2);
-           * Assert.assertEquals(cme.getFieldIndex(), 0); }
-           */
-        }
-      }
-    } catch (Exception e) {
-      Assert.assertTrue(false);
-    }
-  }
-}
\ No newline at end of file
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestStorageMisc3.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestStorageMisc3.java
deleted file mode 100644
index 22c2f0401..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestStorageMisc3.java
+++ /dev/null
@@ -1,252 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.types;
-
-import java.io.StringReader;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.HashSet;
-import junit.framework.Assert;
-
-import org.apache.hadoop.zebra.types.CGSchema;
-import org.apache.hadoop.zebra.schema.ColumnType;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Partition;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.parser.TableSchemaParser;
-import org.apache.hadoop.zebra.schema.Schema.ColumnSchema;
-import org.junit.Before;
-import org.junit.Test;
-
-public class TestStorageMisc3 {
-  String strSch = "c:collection(record(f1:int, f2:int)), m1:map(int)";
-  TableSchemaParser parser;
-  Schema schema;
-
-  @Before
-  public void init() throws ParseException {
-    parser = new TableSchemaParser(new StringReader(strSch));
-    schema = parser.RecordSchema(null);
-  }
-
-  @Test
-  public void testStorageValid1() {
-    try {
-      String strStorage = "[c] compress by gz; [m1] serialize by avro";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      CGSchema[] cgschemas = p.getCGSchemas();
-
-      // 2 column group;
-      int size = cgschemas.length;
-      Assert.assertEquals(size, 2);
-      System.out.println("********** Column Groups **********");
-      for (int i = 0; i < cgschemas.length; i++) {
-        System.out.println(cgschemas[i]);
-        System.out.println("--------------------------------");
-      }
-      CGSchema cgs1 = cgschemas[0];
-      CGSchema cgs2 = cgschemas[1];
-
-      ColumnSchema f11 = cgs1.getSchema().getColumn(0);
-      Assert.assertEquals("c", f11.getName());
-      Assert.assertEquals(ColumnType.COLLECTION, f11.getType());
-
-      ColumnSchema f21 = cgs2.getSchema().getColumn(0);
-      Assert.assertEquals("m1", f21.getName());
-      Assert.assertEquals(ColumnType.MAP, f21.getType());
-
-      Assert.assertEquals(cgs1.getCompressor(), "gz");
-      Assert.assertEquals(cgs2.getSerializer(), "avro");
-
-      System.out.println("*********** Column Map **********");
-      Map<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>> colmap = p
-          .getPartitionInfo().getColMap();
-      Assert.assertEquals(colmap.size(), 2);
-      Iterator<Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>>> it = colmap
-          .entrySet().iterator();
-      for (int i = 0; i < colmap.size(); i++) {
-        Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>> entry = (Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>>) it
-            .next();
-        String name = entry.getKey();
-        HashSet<Partition.PartitionInfo.ColumnMappingEntry> hs = entry
-            .getValue();
-        Iterator<Partition.PartitionInfo.ColumnMappingEntry> it1 = hs
-            .iterator();
-        for (int j = 0; j < hs.size(); j++) {
-          Partition.PartitionInfo.ColumnMappingEntry cme = (Partition.PartitionInfo.ColumnMappingEntry) it1
-              .next();
-          System.out.println("[Column = " + name + " CG = " + cme.getCGIndex()
-              + "." + cme.getFieldIndex() + "]");
-          if (i == 0 && j == 0) {
-            Assert.assertEquals(name, "c");
-            Assert.assertEquals(cme.getCGIndex(), 0);
-            Assert.assertEquals(cme.getFieldIndex(), 0);
-          } else if (i == 1 && j == 0) {
-            Assert.assertEquals(name, "m1");
-            Assert.assertEquals(cme.getCGIndex(), 1);
-            Assert.assertEquals(cme.getFieldIndex(), 0);
-          }
-        }
-      }
-    } catch (Exception e) {
-      Assert.assertTrue(false);
-    }
-  }
-
-  @Test
-  public void testStorageValid2() {
-    try {
-      String strStorage = "[c.r] compress by gz; [m1] serialize by avro";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      Assert.assertTrue(false);
-      CGSchema[] cgschemas = p.getCGSchemas();
-
-      // 2 column group;
-      int size = cgschemas.length;
-      Assert.assertEquals(size, 2);
-      System.out.println("********** Column Groups **********");
-      for (int i = 0; i < cgschemas.length; i++) {
-        System.out.println(cgschemas[i]);
-        System.out.println("--------------------------------");
-      }
-      CGSchema cgs1 = cgschemas[0];
-      CGSchema cgs2 = cgschemas[1];
-
-      ColumnSchema f11 = cgs1.getSchema().getColumn(0);
-      Assert.assertEquals("c.r", f11.getName());
-      Assert.assertEquals(ColumnType.RECORD, f11.getType());
-
-      ColumnSchema f21 = cgs2.getSchema().getColumn(0);
-      Assert.assertEquals("m1", f21.getName());
-      Assert.assertEquals(ColumnType.MAP, f21.getType());
-
-      Assert.assertEquals(cgs1.getCompressor(), "gz");
-      Assert.assertEquals(cgs2.getSerializer(), "avro");
-
-      System.out.println("*********** Column Map **********");
-      Map<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>> colmap = p
-          .getPartitionInfo().getColMap();
-      Assert.assertEquals(colmap.size(), 2);
-      Iterator<Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>>> it = colmap
-          .entrySet().iterator();
-      for (int i = 0; i < colmap.size(); i++) {
-        Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>> entry = (Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>>) it
-            .next();
-        String name = entry.getKey();
-        HashSet<Partition.PartitionInfo.ColumnMappingEntry> hs = entry
-            .getValue();
-        Iterator<Partition.PartitionInfo.ColumnMappingEntry> it1 = hs
-            .iterator();
-        for (int j = 0; j < hs.size(); j++) {
-          Partition.PartitionInfo.ColumnMappingEntry cme = (Partition.PartitionInfo.ColumnMappingEntry) it1
-              .next();
-          System.out.println("[Column = " + name + " CG = " + cme.getCGIndex()
-              + "." + cme.getFieldIndex() + "]");
-          if (i == 0 && j == 0) {
-            Assert.assertEquals(name, "m1");
-            Assert.assertEquals(cme.getCGIndex(), 1);
-            Assert.assertEquals(cme.getFieldIndex(), 0);
-          } else if (i == 1 && j == 0) {
-            Assert.assertEquals(name, "c.r");
-            Assert.assertEquals(cme.getCGIndex(), 0);
-            Assert.assertEquals(cme.getFieldIndex(), 0);
-          }
-        }
-      }
-    } catch (Exception e) {
-    }
-  }
-
-  @Test
-  public void testStorageValid3() {
-    try {
-      String strStorage = "[c.r.f1] compress by gz; [m1] serialize by avro";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      Assert.assertTrue(false);
-      CGSchema[] cgschemas = p.getCGSchemas();
-
-      // 3 column group;
-      int size = cgschemas.length;
-      Assert.assertEquals(size, 3);
-      System.out.println("********** Column Groups **********");
-      for (int i = 0; i < cgschemas.length; i++) {
-        System.out.println(cgschemas[i]);
-        System.out.println("--------------------------------");
-      }
-      CGSchema cgs1 = cgschemas[0];
-      CGSchema cgs2 = cgschemas[1];
-      CGSchema cgs3 = cgschemas[2];
-
-      ColumnSchema f11 = cgs1.getSchema().getColumn(0);
-      Assert.assertEquals("c.r.f1", f11.getName());
-      Assert.assertEquals(ColumnType.INT, f11.getType());
-
-      ColumnSchema f21 = cgs2.getSchema().getColumn(0);
-      Assert.assertEquals("m1", f21.getName());
-      Assert.assertEquals(ColumnType.MAP, f21.getType());
-
-      ColumnSchema f31 = cgs3.getSchema().getColumn(0);
-      Assert.assertEquals("c.r.f2", f31.getName());
-      Assert.assertEquals(ColumnType.INT, f31.getType());
-
-      Assert.assertEquals(cgs1.getCompressor(), "gz");
-      Assert.assertEquals(cgs1.getSerializer(), "pig");
-      Assert.assertEquals(cgs2.getCompressor(), "lzo");
-      Assert.assertEquals(cgs2.getSerializer(), "avro");
-      Assert.assertEquals(cgs3.getCompressor(), "lzo");
-      Assert.assertEquals(cgs3.getSerializer(), "pig");
-
-      System.out.println("*********** Column Map **********");
-      Map<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>> colmap = p
-          .getPartitionInfo().getColMap();
-      Assert.assertEquals(colmap.size(), 3);
-      Iterator<Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>>> it = colmap
-          .entrySet().iterator();
-      for (int i = 0; i < colmap.size(); i++) {
-        Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>> entry = (Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>>) it
-            .next();
-        String name = entry.getKey();
-        HashSet<Partition.PartitionInfo.ColumnMappingEntry> hs = entry
-            .getValue();
-        Iterator<Partition.PartitionInfo.ColumnMappingEntry> it1 = hs
-            .iterator();
-        for (int j = 0; j < hs.size(); j++) {
-          Partition.PartitionInfo.ColumnMappingEntry cme = (Partition.PartitionInfo.ColumnMappingEntry) it1
-              .next();
-          System.out.println("[Column = " + name + " CG = " + cme.getCGIndex()
-              + "." + cme.getFieldIndex() + "]");
-          if (i == 0 && j == 0) {
-            Assert.assertEquals(name, "c.r.f1");
-            Assert.assertEquals(cme.getCGIndex(), 0);
-            Assert.assertEquals(cme.getFieldIndex(), 0);
-          } else if (i == 1 && j == 0) {
-            Assert.assertEquals(name, "c.r.f2");
-            Assert.assertEquals(cme.getCGIndex(), 2);
-            Assert.assertEquals(cme.getFieldIndex(), 0);
-          } else if (i == 2 && j == 0) {
-            Assert.assertEquals(name, "m1");
-            Assert.assertEquals(cme.getCGIndex(), 1);
-            Assert.assertEquals(cme.getFieldIndex(), 0);
-          }
-        }
-      }
-    } catch (Exception e) {
-    }
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestStorageRecord.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestStorageRecord.java
deleted file mode 100644
index 747323920..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestStorageRecord.java
+++ /dev/null
@@ -1,133 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.types;
-
-import java.io.StringReader;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.HashSet;
-import junit.framework.Assert;
-
-import org.apache.hadoop.zebra.types.CGSchema;
-import org.apache.hadoop.zebra.schema.ColumnType;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Partition;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.parser.TableSchemaParser;
-import org.apache.hadoop.zebra.schema.Schema.ColumnSchema;
-import org.junit.Before;
-import org.junit.Test;
-
-public class TestStorageRecord {
-  String strSch = "r1:record(f1:int, f2:int), r2:record(r3:record(f3:float, f4))";
-  TableSchemaParser parser;
-  Schema schema;
-
-  @Before
-  public void init() throws ParseException {
-    parser = new TableSchemaParser(new StringReader(strSch));
-    schema = parser.RecordSchema(null);
-  }
-
-  @Test
-  public void testStorageValid1() {
-    try {
-      String strStorage = "[r1.f1, r2.r3.f3]; [r1.f2, r2.r3.f4]";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      CGSchema[] cgschemas = p.getCGSchemas();
-
-      // 2 column group;
-      int size = cgschemas.length;
-      Assert.assertEquals(size, 2);
-      System.out.println("********** Column Groups **********");
-      for (int i = 0; i < cgschemas.length; i++) {
-        System.out.println(cgschemas[i]);
-        System.out.println("--------------------------------");
-      }
-      CGSchema cgs1 = cgschemas[0];
-      CGSchema cgs2 = cgschemas[1];
-
-      ColumnSchema f11 = cgs1.getSchema().getColumn(0);
-      Assert.assertEquals("r1.f1", f11.getName());
-      Assert.assertEquals(ColumnType.INT, f11.getType());
-      ColumnSchema f12 = cgs1.getSchema().getColumn(1);
-      Assert.assertEquals("r2.r3.f3", f12.getName());
-      Assert.assertEquals(ColumnType.FLOAT, f12.getType());
-
-      ColumnSchema f21 = cgs2.getSchema().getColumn(0);
-      Assert.assertEquals("r1.f2", f21.getName());
-      Assert.assertEquals(ColumnType.INT, f21.getType());
-      ColumnSchema f22 = cgs2.getSchema().getColumn(1);
-      Assert.assertEquals("r2.r3.f4", f22.getName());
-      Assert.assertEquals(ColumnType.BYTES, f22.getType());
-
-      System.out.println("*********** Column Map **********");
-      Map<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>> colmap = p
-          .getPartitionInfo().getColMap();
-      Assert.assertEquals(colmap.size(), 4);
-      Iterator<Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>>> it = colmap
-          .entrySet().iterator();
-      for (int i = 0; i < colmap.size(); i++) {
-        Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>> entry = (Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>>) it
-            .next();
-        String name = entry.getKey();
-        HashSet<Partition.PartitionInfo.ColumnMappingEntry> hs = entry
-            .getValue();
-        Iterator<Partition.PartitionInfo.ColumnMappingEntry> it1 = hs
-            .iterator();
-        for (int j = 0; j < hs.size(); j++) {
-          Partition.PartitionInfo.ColumnMappingEntry cme = (Partition.PartitionInfo.ColumnMappingEntry) it1
-              .next();
-          System.out.println("[Column = " + name + " CG = " + cme.getCGIndex()
-              + "." + cme.getFieldIndex() + "]");
-          if (i == 0 && j == 0) {
-            Assert.assertEquals(name, "r1.f1");
-            Assert.assertEquals(cme.getCGIndex(), 0);
-            Assert.assertEquals(cme.getFieldIndex(), 0);
-          } else if (i == 1 && j == 0) {
-            Assert.assertEquals(name, "r1.f2");
-            Assert.assertEquals(cme.getCGIndex(), 1);
-            Assert.assertEquals(cme.getFieldIndex(), 0);
-          } else if (i == 2 && j == 0) {
-            Assert.assertEquals(name, "r2.r3.f3");
-            Assert.assertEquals(cme.getCGIndex(), 0);
-            Assert.assertEquals(cme.getFieldIndex(), 1);
-          } else if (i == 3 && j == 0) {
-            Assert.assertEquals(name, "r2.r3.f4");
-            Assert.assertEquals(cme.getCGIndex(), 1);
-            Assert.assertEquals(cme.getFieldIndex(), 1);
-          }
-        }
-      }
-    } catch (Exception e) {
-      Assert.assertTrue(false);
-    }
-  }
-
-  /*
-   * @Test public void testStorageInvalid1() { try { String strStorage =
-   * "m1#k1"; TableStorageParser parser = new TableStorageParser(new
-   * ByteArrayInputStream(strStorage.getBytes("UTF-8")), null, schema);
-   * ArrayList<CGSchema> schemas = parser.StorageSchema(); CGSchema cgs1 =
-   * schemas.get(0); } catch (Exception e) { String errMsg = e.getMessage();
-   * String str = "Encountered \" <IDENTIFIER> \"m1 \"\" at line 1, column 1.";
-   * System.out.println(errMsg); System.out.println(str);
-   * Assert.assertEquals(errMsg.startsWith(str), true); } }
-   */
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestStorageRecord2.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestStorageRecord2.java
deleted file mode 100644
index 1addd6b48..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestStorageRecord2.java
+++ /dev/null
@@ -1,137 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.types;
-
-import java.io.StringReader;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.HashSet;
-import junit.framework.Assert;
-
-import org.apache.hadoop.zebra.types.CGSchema;
-import org.apache.hadoop.zebra.schema.ColumnType;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Partition;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.parser.TableSchemaParser;
-import org.apache.hadoop.zebra.schema.Schema.ColumnSchema;
-import org.junit.Before;
-import org.junit.Test;
-
-public class TestStorageRecord2 {
-  String strSch = "r1:record(f1:int, f2:int), r2:record(f5:int, r3:record(f3:float, f4))";
-  TableSchemaParser parser;
-  Schema schema;
-
-  @Before
-  public void init() throws ParseException {
-    parser = new TableSchemaParser(new StringReader(strSch));
-    schema = parser.RecordSchema(null);
-  }
-
-  @Test
-  public void testStorageValid1() {
-    try {
-      String strStorage = "[r1.f1, r2.r3.f3, r2.f5]; [r1.f2, r2.r3.f4]";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      CGSchema[] cgschemas = p.getCGSchemas();
-
-      // 2 column group;
-      int size = cgschemas.length;
-      Assert.assertEquals(size, 2);
-      System.out.println("********** Column Groups **********");
-      for (int i = 0; i < cgschemas.length; i++) {
-        System.out.println(cgschemas[i]);
-        System.out.println("--------------------------------");
-      }
-      CGSchema cgs1 = cgschemas[0];
-      CGSchema cgs2 = cgschemas[1];
-
-      ColumnSchema f11 = cgs1.getSchema().getColumn(0);
-      Assert.assertEquals("r1.f1", f11.getName());
-      Assert.assertEquals(ColumnType.INT, f11.getType());
-      ColumnSchema f12 = cgs1.getSchema().getColumn(1);
-      Assert.assertEquals("r2.r3.f3", f12.getName());
-      Assert.assertEquals(ColumnType.FLOAT, f12.getType());
-
-      ColumnSchema f21 = cgs2.getSchema().getColumn(0);
-      Assert.assertEquals("r1.f2", f21.getName());
-      Assert.assertEquals(ColumnType.INT, f21.getType());
-      ColumnSchema f22 = cgs2.getSchema().getColumn(1);
-      Assert.assertEquals("r2.r3.f4", f22.getName());
-      Assert.assertEquals(ColumnType.BYTES, f22.getType());
-
-      System.out.println("*********** Column Map **********");
-      Map<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>> colmap = p
-          .getPartitionInfo().getColMap();
-      Assert.assertEquals(colmap.size(), 5);
-      Iterator<Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>>> it = colmap
-          .entrySet().iterator();
-      for (int i = 0; i < colmap.size(); i++) {
-        Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>> entry = (Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>>) it
-            .next();
-        String name = entry.getKey();
-        HashSet<Partition.PartitionInfo.ColumnMappingEntry> hs = entry
-            .getValue();
-        Iterator<Partition.PartitionInfo.ColumnMappingEntry> it1 = hs
-            .iterator();
-        for (int j = 0; j < hs.size(); j++) {
-          Partition.PartitionInfo.ColumnMappingEntry cme = (Partition.PartitionInfo.ColumnMappingEntry) it1
-              .next();
-          System.out.println("[Column = " + name + " CG = " + cme.getCGIndex()
-              + "." + cme.getFieldIndex() + "]");
-          if (i == 0 && j == 0) {
-            Assert.assertEquals(name, "r2.f5");
-            Assert.assertEquals(cme.getCGIndex(), 0);
-            Assert.assertEquals(cme.getFieldIndex(), 2);
-          } else if (i == 1 && j == 0) {
-            Assert.assertEquals(name, "r1.f1");
-            Assert.assertEquals(cme.getCGIndex(), 0);
-            Assert.assertEquals(cme.getFieldIndex(), 0);
-          } else if (i == 2 && j == 0) {
-            Assert.assertEquals(name, "r1.f2");
-            Assert.assertEquals(cme.getCGIndex(), 1);
-            Assert.assertEquals(cme.getFieldIndex(), 0);
-          } else if (i == 3 && j == 0) {
-            Assert.assertEquals(name, "r2.r3.f3");
-            Assert.assertEquals(cme.getCGIndex(), 0);
-            Assert.assertEquals(cme.getFieldIndex(), 1);
-          } else if (i == 4 && j == 0) {
-            Assert.assertEquals(name, "r2.r3.f4");
-            Assert.assertEquals(cme.getCGIndex(), 1);
-            Assert.assertEquals(cme.getFieldIndex(), 1);
-          }
-        }
-      }
-    } catch (Exception e) {
-      Assert.assertTrue(false);
-    }
-  }
-
-  /*
-   * @Test public void testStorageInvalid1() { try { String strStorage =
-   * "m1#k1"; TableStorageParser parser = new TableStorageParser(new
-   * ByteArrayInputStream(strStorage.getBytes("UTF-8")), null, schema);
-   * ArrayList<CGSchema> schemas = parser.StorageSchema(); CGSchema cgs1 =
-   * schemas.get(0); } catch (Exception e) { String errMsg = e.getMessage();
-   * String str = "Encountered \" <IDENTIFIER> \"m1 \"\" at line 1, column 1.";
-   * System.out.println(errMsg); System.out.println(str);
-   * Assert.assertEquals(errMsg.startsWith(str), true); } }
-   */
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestStorePrimitive.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestStorePrimitive.java
deleted file mode 100644
index 2498f4f02..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestStorePrimitive.java
+++ /dev/null
@@ -1,332 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.zebra.types;
-
-import java.io.StringReader;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.HashSet;
-import junit.framework.Assert;
-
-import org.apache.hadoop.zebra.types.CGSchema;
-import org.apache.hadoop.zebra.schema.ColumnType;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.types.Partition;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.parser.TableSchemaParser;
-import org.apache.hadoop.zebra.schema.Schema.ColumnSchema;
-import org.junit.Before;
-import org.junit.Test;
-
-public class TestStorePrimitive {
-  String strSch = "f1:int, f2:long, f3:float, f4:bool, f5:string, f6:bytes";
-  TableSchemaParser parser;
-  Schema schema;
-
-  @Before
-  public void init() throws ParseException {
-    parser = new TableSchemaParser(new StringReader(strSch));
-    schema = parser.RecordSchema(null);
-  }
-
-  @Test
-  public void testSchema() throws ParseException {
-    ColumnSchema f1 = schema.getColumn(0);
-    Assert.assertEquals("f1", f1.getName());
-    Assert.assertEquals(ColumnType.INT, f1.getType());
-    ColumnSchema f2 = schema.getColumn(1);
-    Assert.assertEquals("f2", f2.getName());
-    Assert.assertEquals(ColumnType.LONG, f2.getType());
-    ColumnSchema f3 = schema.getColumn(2);
-    Assert.assertEquals("f3", f3.getName());
-    Assert.assertEquals(ColumnType.FLOAT, f3.getType());
-    ColumnSchema f4 = schema.getColumn(3);
-    Assert.assertEquals("f4", f4.getName());
-    Assert.assertEquals(ColumnType.BOOL, f4.getType());
-    ColumnSchema f5 = schema.getColumn(4);
-    Assert.assertEquals("f5", f5.getName());
-    Assert.assertEquals(ColumnType.STRING, f5.getType());
-    ColumnSchema f6 = schema.getColumn(5);
-    Assert.assertEquals("f6", f6.getName());
-    Assert.assertEquals(ColumnType.BYTES, f6.getType());
-
-    System.out.println(schema.toString());
-  }
-
-  @Test
-  public void testStorageValid1() {
-    try {
-      String strStorage = "[f1, f2]; [f3, f4] COMPRESS BY gz SERIALIZE BY avro";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      CGSchema[] cgschemas = p.getCGSchemas();
-
-      // 3 column group;
-      int size = cgschemas.length;
-      Assert.assertEquals(size, 3);
-      System.out.println("********** Column Groups **********");
-      for (int i = 0; i < cgschemas.length; i++) {
-        System.out.println(cgschemas[i]);
-        System.out.println("--------------------------------");
-      }
-      CGSchema cgs1 = cgschemas[0];
-      CGSchema cgs2 = cgschemas[1];
-      CGSchema cgs3 = cgschemas[2];
-
-      ColumnSchema f11 = cgs1.getSchema().getColumn(0);
-      Assert.assertEquals("f1", f11.getName());
-      Assert.assertEquals(ColumnType.INT, f11.getType());
-      ColumnSchema f12 = cgs1.getSchema().getColumn(1);
-      Assert.assertEquals("f2", f12.getName());
-      Assert.assertEquals(ColumnType.LONG, f12.getType());
-      ColumnSchema f21 = cgs2.getSchema().getColumn(0);
-      Assert.assertEquals("f3", f21.getName());
-      Assert.assertEquals(ColumnType.FLOAT, f21.getType());
-      ColumnSchema f22 = cgs2.getSchema().getColumn(1);
-      Assert.assertEquals("f4", f22.getName());
-      Assert.assertEquals(ColumnType.BOOL, f22.getType());
-      ColumnSchema f31 = cgs3.getSchema().getColumn(0);
-      Assert.assertEquals("f5", f31.getName());
-      Assert.assertEquals(ColumnType.STRING, f31.getType());
-      ColumnSchema f32 = cgs3.getSchema().getColumn(1);
-      Assert.assertEquals("f6", f32.getName());
-      Assert.assertEquals(ColumnType.BYTES, f32.getType());
-
-      Assert.assertEquals(cgs1.getCompressor(), "gz");
-      Assert.assertEquals(cgs1.getSerializer(), "pig");
-      Assert.assertEquals(cgs2.getCompressor(), "gz");
-      Assert.assertEquals(cgs2.getSerializer(), "avro");
-      Assert.assertEquals(cgs3.getCompressor(), "gz");
-      Assert.assertEquals(cgs3.getSerializer(), "pig");
-
-      System.out.println("*********** Column Map **********");
-      Map<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>> colmap = p
-          .getPartitionInfo().getColMap();
-      Assert.assertEquals(colmap.size(), 6);
-      Iterator<Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>>> it = colmap
-          .entrySet().iterator();
-      for (int i = 0; i < colmap.size(); i++) {
-        Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>> entry = (Map.Entry<String, HashSet<Partition.PartitionInfo.ColumnMappingEntry>>) it
-            .next();
-        String name = entry.getKey();
-        HashSet<Partition.PartitionInfo.ColumnMappingEntry> hs = entry
-            .getValue();
-        Iterator<Partition.PartitionInfo.ColumnMappingEntry> it1 = hs
-            .iterator();
-        for (int j = 0; j < hs.size(); j++) {
-          Partition.PartitionInfo.ColumnMappingEntry cme = (Partition.PartitionInfo.ColumnMappingEntry) it1
-              .next();
-          System.out.println("[Column = " + name + " CG = " + cme.getCGIndex()
-              + "." + cme.getFieldIndex() + "]");
-          if (i == 0 && j == 0) {
-            Assert.assertEquals(name, "f6");
-            Assert.assertEquals(cme.getCGIndex(), 2);
-            Assert.assertEquals(cme.getFieldIndex(), 1);
-          } else if (i == 1 && j == 0) {
-            Assert.assertEquals(name, "f1");
-            Assert.assertEquals(cme.getCGIndex(), 0);
-            Assert.assertEquals(cme.getFieldIndex(), 0);
-          } else if (i == 2 && j == 0) {
-            Assert.assertEquals(name, "f3");
-            Assert.assertEquals(cme.getCGIndex(), 1);
-            Assert.assertEquals(cme.getFieldIndex(), 0);
-          } else if (i == 3 && j == 0) {
-            Assert.assertEquals(name, "f2");
-            Assert.assertEquals(cme.getCGIndex(), 0);
-            Assert.assertEquals(cme.getFieldIndex(), 1);
-          } else if (i == 4 && j == 0) {
-            Assert.assertEquals(name, "f5");
-            Assert.assertEquals(cme.getCGIndex(), 2);
-            Assert.assertEquals(cme.getFieldIndex(), 0);
-          } else if (i == 5 && j == 0) {
-            Assert.assertEquals(name, "f4");
-            Assert.assertEquals(cme.getCGIndex(), 1);
-            Assert.assertEquals(cme.getFieldIndex(), 1);
-          }
-        }
-      }
-    } catch (Exception e) {
-      Assert.assertTrue(false);
-    }
-  }
-
-  @Test
-  public void testStorageValid2() {
-    try {
-      String strStorage = "[f1, f2] serialize by avro compress by gz; [f3, f4] SERIALIZE BY avro COMPRESS BY gz";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      CGSchema[] cgschemas = p.getCGSchemas();
-
-      Assert.assertEquals(cgschemas.length, 3);
-      CGSchema cgs1 = cgschemas[0];
-      System.out.println(cgs1);
-    } catch (Exception e) {
-      e.printStackTrace();
-    }
-  }
-
-  @Test
-  public void testStorageValid3() {
-    try {
-      String strStorage = "";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      CGSchema[] cgschemas = p.getCGSchemas();
-      Assert.assertEquals(cgschemas.length, 1);
-      CGSchema cgs1 = cgschemas[0];
-      System.out.println(cgs1);
-    } catch (Exception e) {
-      e.printStackTrace();
-    }
-  }
-
-  @Test
-  public void testStorageInvalid1() {
-    try {
-      String strStorage = "f1";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      CGSchema[] cgschemas = p.getCGSchemas();
-      CGSchema cgs1 = cgschemas[0];
-      System.out.println(cgs1);
-    } catch (Exception e) {
-      String errMsg = e.getMessage();
-      String str = "Encountered \" <IDENTIFIER> \"f1 \"\" at line 1, column 1.\n";
-      System.out.println(errMsg);
-      System.out.println(str);
-      Assert.assertEquals(errMsg.startsWith(str), true);
-    }
-  }
-
-  @Test
-  public void testStorageInvalid2() {
-    try {
-      String strStorage = "[f100]";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      CGSchema[] cgschemas = p.getCGSchemas();
-      CGSchema cgs1 = cgschemas[0];
-      System.out.println(cgs1);
-    } catch (Exception e) {
-      String errMsg = e.getMessage();
-      String str = "Column f100 not defined in schema";
-      System.out.println(errMsg);
-      System.out.println(str);
-      Assert.assertEquals(errMsg, str);
-    }
-  }
-
-  @Test
-  public void testStorageInvalid3() {
-    try {
-      String strStorage = "f1:long";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      CGSchema[] cgschemas = p.getCGSchemas();
-      CGSchema cgs1 = cgschemas[0];
-      System.out.println(cgs1);
-    } catch (Exception e) {
-      String errMsg = e.getMessage();
-      String str = "Encountered \" <IDENTIFIER> \"f1 \"\" at line 1, column 1.\nWas expecting one of:\n";
-      System.out.println(errMsg);
-      System.out.println(str);
-      Assert.assertEquals(errMsg.startsWith(str), true);
-    }
-  }
-
-  @Test
-  public void testStorageInvalid4() {
-    try {
-      String strStorage = "[";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      CGSchema[] cgschemas = p.getCGSchemas();
-      CGSchema cgs1 = cgschemas[0];
-      System.out.println(cgs1);
-    } catch (Exception e) {
-      String errMsg = e.getMessage();
-      String str = "Encountered \"<EOF>\" at line 1, column 1.\nWas expecting one of:\n";
-      System.out.println(errMsg);
-      System.out.println(str);
-      Assert.assertEquals(errMsg.startsWith(str), true);
-    }
-  }
-
-  @Test
-  public void testStorageInvalid5() {
-    try {
-      String strStorage = "[f1, f2]; [f1, f4]";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      CGSchema[] cgschemas = p.getCGSchemas();
-      CGSchema cgs1 = cgschemas[0];
-      System.out.println(cgs1);
-    } catch (Exception e) {
-      String errMsg = e.getMessage();
-      String str = "Column f1 specified more than once!";
-      System.out.println(errMsg);
-      System.out.println(str);
-      Assert.assertEquals(errMsg, str);
-    }
-  }
-
-  @Test
-  public void testStorageInvalid6() {
-    try {
-      String strStorage = ":";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      CGSchema[] cgschemas = p.getCGSchemas();
-      CGSchema cgs1 = cgschemas[0];
-      System.out.println(cgs1);
-    } catch (Exception e) {
-      String errMsg = e.getMessage();
-      String str = "Encountered \" \":\" \": \"\" at line 1, column 1.\n";
-      System.out.println(errMsg);
-      System.out.println(str);
-      Assert.assertEquals(errMsg.startsWith(str), true);
-    }
-  }
-
-  @Test
-  public void testStorageInvalid7() {
-    try {
-      String strStorage = "[f1, f2] serialize by xyz compress by gz; [f3, f4] SERIALIZE BY avro COMPRESS BY lzo";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      CGSchema[] cgschemas = p.getCGSchemas();
-      CGSchema cgs1 = cgschemas[0];
-      System.out.println(cgs1);
-    } catch (Exception e) {
-      String errMsg = e.getMessage();
-      String str = "Encountered \" <IDENTIFIER> \"xyz \"\" at line 1, column 23.";
-      System.out.println(errMsg);
-      System.out.println(str);
-      Assert.assertEquals(errMsg.startsWith(str), true);
-    }
-  }
-
-  @Test
-  public void testStorageInvalid8() {
-    try {
-      String strStorage = "[f1, f2] serialize by avro compress by xyz; [f3, f4] SERIALIZE BY avro COMPRESS BY lzo";
-      Partition p = new Partition(schema.toString(), strStorage, null);
-      CGSchema[] cgschemas = p.getCGSchemas();
-      CGSchema cgs1 = cgschemas[0];
-      System.out.println(cgs1);
-    } catch (Exception e) {
-      String errMsg = e.getMessage();
-      String str = "Encountered \" <IDENTIFIER> \"xyz \"\" at line 1, column 40.";
-      System.out.println(errMsg);
-      System.out.println(str);
-      Assert.assertEquals(errMsg.startsWith(str), true);
-    }
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestTypeCheck.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestTypeCheck.java
deleted file mode 100644
index 8b2f3a2ea..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestTypeCheck.java
+++ /dev/null
@@ -1,421 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.types;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-import junit.framework.Assert;
-
-import org.apache.hadoop.zebra.BaseTestCase;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * This is to test type check for writes in Zebra.
- */
-public class TestTypeCheck extends BaseTestCase {
-
-  @BeforeClass
-  public static void setUpOnce() throws Exception {
-    init();
-  }
-  
-  @Test
-  public void testPositive1() throws ParseException, IOException {
-    String STR_SCHEMA = "s1:bool, s2:int, s3:long, s4:float, s5:double, s6:string, s7:bytes";
-    Schema schema = new Schema(STR_SCHEMA);
-
-    Tuple tuple = TypesUtils.createTuple(schema);
-    TypesUtils.resetTuple(tuple);
-
-    tuple.set(0, true);
-    tuple.set(1, 1);
-    tuple.set(2, 1001L);
-    tuple.set(3, 1.1f);
-    tuple.set(4, 2.2d);
-    tuple.set(5, "hello world 1");
-    tuple.set(6, new DataByteArray("hello byte 1"));
-
-    try {
-      TypesUtils.checkCompatible(tuple, schema);
-    } catch (Exception e) {
-      Assert.fail("Should not see this: " + e.getMessage());
-    }
-  }
-  
-  @Test
-  public void testPositive2() throws ParseException, IOException {
-    String STR_SCHEMA = "f1:bool, r:record(f11:int, f12:long), m:map(string), c:collection(record(f13:double, f14:float, f15:bytes))";
-    Schema schema = new Schema(STR_SCHEMA);
-
-    Tuple tuple = TypesUtils.createTuple(schema);    
-    TypesUtils.resetTuple(tuple);
-
-    tuple.set(0, true);
-
-    Tuple tupRecord;
-    try {
-      tupRecord = TypesUtils.createTuple(schema.getColumnSchema("r").getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-    tupRecord.set(0, 1);
-    tupRecord.set(1, 1001L);
-    tuple.set(1, tupRecord);
-
-    Map<String, String> map = new HashMap<String, String>();
-    map.put("a", "x");
-    map.put("b", "y");
-    map.put("c", "z");
-    tuple.set(2, map);
-
-    DataBag bagColl = TypesUtils.createBag();
-    Schema schColl = schema.getColumn(3).getSchema().getColumn(0).getSchema();
-    Tuple tupColl1 = TypesUtils.createTuple(schColl);
-    Tuple tupColl2 = TypesUtils.createTuple(schColl);
-    byte[] abs1 = new byte[3];
-    byte[] abs2 = new byte[4];
-    tupColl1.set(0, 3.1415926);
-    tupColl1.set(1, 1.6f);
-    abs1[0] = 11;
-    abs1[1] = 12;
-    abs1[2] = 13;
-    tupColl1.set(2, new DataByteArray(abs1));
-    bagColl.add(tupColl1);
-    tupColl2.set(0, 123.456789);
-    tupColl2.set(1, 100f);
-    abs2[0] = 21;
-    abs2[1] = 22;
-    abs2[2] = 23;
-    abs2[3] = 24;
-    tupColl2.set(2, new DataByteArray(abs2));
-    bagColl.add(tupColl2);
-    tuple.set(3, bagColl);
-    
-    try {
-      TypesUtils.checkCompatible(tuple, schema);
-    } catch (Exception e) {
-      Assert.fail("Should not see this: " + e.getMessage());
-    }
-  }
-  
-  @Test
-  public void testPositive3() throws IOException, ParseException {
-    String STR_SCHEMA = "m1:map(string),m2:map(map(int))";
-    Schema schema = new Schema(STR_SCHEMA);
-    Tuple tuple = TypesUtils.createTuple(schema);
-    TypesUtils.resetTuple(tuple);
-
-    // m1:map(string)
-    Map<String, String> m1 = new HashMap<String, String>();
-    m1.put("a", "A");
-    m1.put("b", "B");
-    m1.put("c", "C");
-    tuple.set(0, m1);
-
-    // m2:map(map(int))
-    HashMap<String, Map> m2 = new HashMap<String, Map>();
-    Map<String, Integer> m3 = new HashMap<String, Integer>();
-    m3.put("m311", 311);
-    m3.put("m321", 321);
-    m3.put("m331", 331);
-    Map<String, Integer> m4 = new HashMap<String, Integer>();
-    m4.put("m411", 411);
-    m4.put("m421", 421);
-    m4.put("m431", 431);
-    m2.put("x", m3);
-    m2.put("y", m4);
-    tuple.set(1, m2);
-    
-    try {
-      TypesUtils.checkCompatible(tuple, schema);
-    } catch (Exception e) {
-      Assert.fail("Should not see this: " + e.getMessage());
-    }
-  }
-  
-  @Test
-  public void testPositive4() throws ParseException, IOException {
-    String STR_SCHEMA = "s1:bool, s2:int, s3:long, s4:float, s5:double, s6:string, s7:bytes";
-    Schema schema = new Schema(STR_SCHEMA);
-
-    Tuple tuple = TypesUtils.createTuple(schema);
-    TypesUtils.resetTuple(tuple);
-
-    tuple.set(0, true);
-    tuple.set(1, 1);
-    tuple.set(2, 1001);
-    tuple.set(3, 1.1f);
-    tuple.set(4, 2.2f);
-    tuple.set(5, "hello world 1");
-    tuple.set(6, new DataByteArray("hello byte 1"));
-
-    try {
-      TypesUtils.checkCompatible(tuple, schema);
-    } catch (Exception e) {
-      Assert.fail("Should not see this: " + e.getMessage());
-    }
-  }
-  
-  // null is ok
-  @Test
-  public void testPositive5() throws ParseException, IOException {
-    String STR_SCHEMA = "s1:bool, s2:int, s3:long, s4:float, s5:double, s6:string, s7:bytes";
-    Schema schema = new Schema(STR_SCHEMA);
-
-    Tuple tuple = TypesUtils.createTuple(schema);
-    TypesUtils.resetTuple(tuple);
-
-    tuple.set(0, null);
-    tuple.set(1, 1);
-    tuple.set(2, null);
-    tuple.set(3, 1.1f);
-    tuple.set(4, null);
-    tuple.set(5, "hello world 1");
-    tuple.set(6, new DataByteArray("hello byte 1"));
-
-    try {
-      TypesUtils.checkCompatible(tuple, schema);
-    } catch (Exception e) {
-      Assert.fail("Should not see this: " + e.getMessage());
-    }
-  }
-  
-  @Test
-  public void testPositive6() throws ParseException, IOException {
-    String STR_SCHEMA = "f1:bool, r:record(f11:int, f12:long), m:map(string), c:collection(record(f13:double, f14:float, m:map(string)))";
-    Schema schema = new Schema(STR_SCHEMA);
-
-    Tuple tuple = TypesUtils.createTuple(schema);    
-    TypesUtils.resetTuple(tuple);
-
-    tuple.set(0, true);
-
-    Tuple tupRecord;
-    try {
-      tupRecord = TypesUtils.createTuple(schema.getColumnSchema("r").getSchema());
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-    tupRecord.set(0, 1);
-    tupRecord.set(1, 1001L);
-    tuple.set(1, tupRecord);
-
-    Map<String, String> map = new HashMap<String, String>();
-    map.put("a", "x");
-    map.put("b", "y");
-    map.put("c", "z");
-    tuple.set(2, map);
-
-    DataBag bagColl = TypesUtils.createBag();
-    Schema schColl = schema.getColumn(3).getSchema().getColumn(0).getSchema();
-    Tuple tupColl1 = TypesUtils.createTuple(schColl);
-    Tuple tupColl2 = TypesUtils.createTuple(schColl);
-    tupColl1.set(0, 3.1415926);
-    tupColl1.set(1, 1.6f);
-    tupColl1.set(2, map);
-    bagColl.add(tupColl1);
-    tupColl2.set(0, 123.456789);
-    tupColl2.set(1, 100f);
-    tupColl2.set(2, map);
-    bagColl.add(tupColl2);
-    tuple.set(3, bagColl);
-    
-    try {
-      TypesUtils.checkCompatible(tuple, schema);
-    } catch (Exception e) {
-      Assert.fail("Should not see this: " + e.getMessage());
-    }
-  }
-
-  
-  // sees an long for an integer column; 
-  @Test (expected = IOException.class)
-  public void testNegative1() throws ParseException, IOException {
-    String STR_SCHEMA = "s1:bool, s2:int, s3:long, s4:float, s5:double, s6:string, s7:bytes";
-    Schema schema = new Schema(STR_SCHEMA);
-
-    Tuple tuple = TypesUtils.createTuple(schema);
-    TypesUtils.resetTuple(tuple);
-
-    tuple.set(0, true);
-    tuple.set(1, 1L);
-    tuple.set(2, 1001);
-    tuple.set(3, 1.1f);
-    tuple.set(4, 2.2f);
-    tuple.set(5, "hello world 1");
-    tuple.set(6, new DataByteArray("hello byte 1"));
-
-    TypesUtils.checkCompatible(tuple, schema);
-  }
-  
-  // sees an float for an integer column; 
-  @Test (expected = IOException.class)
-  public void testNegative2() throws ParseException, IOException {
-    String STR_SCHEMA = "s1:bool, s2:int, s3:long, s4:float, s5:double, s6:string, s7:bytes";
-    Schema schema = new Schema(STR_SCHEMA);
-
-    Tuple tuple = TypesUtils.createTuple(schema);
-    TypesUtils.resetTuple(tuple);
-
-    tuple.set(0, true);
-    tuple.set(1, 1.1f);
-    tuple.set(2, 1001);
-    tuple.set(3, 1.1f);
-    tuple.set(4, 2.2f);
-    tuple.set(5, "hello world 1");
-    tuple.set(6, new DataByteArray("hello byte 1"));
-
-    TypesUtils.checkCompatible(tuple, schema);
-  }
-  
-  // sees a double for an integer column; 
-  @Test (expected = IOException.class)
-  public void testNegative3() throws ParseException, IOException {
-    String STR_SCHEMA = "s1:bool, s2:int, s3:long, s4:float, s5:double, s6:string, s7:bytes";
-    Schema schema = new Schema(STR_SCHEMA);
-
-    Tuple tuple = TypesUtils.createTuple(schema);
-    TypesUtils.resetTuple(tuple);
-
-    tuple.set(0, true);
-    tuple.set(1, 1.1);
-    tuple.set(2, 1001);
-    tuple.set(3, 1.1f);
-    tuple.set(4, 2.2f);
-    tuple.set(5, "hello world 1");
-    tuple.set(6, new DataByteArray("hello byte 1"));
-
-    TypesUtils.checkCompatible(tuple, schema);
-  }
-
-  // sees an float for an long column; 
-  @Test (expected = IOException.class)
-  public void testNegative4() throws ParseException, IOException {
-    String STR_SCHEMA = "s1:bool, s2:int, s3:long, s4:float, s5:double, s6:string, s7:bytes";
-    Schema schema = new Schema(STR_SCHEMA);
-
-    Tuple tuple = TypesUtils.createTuple(schema);
-    TypesUtils.resetTuple(tuple);
-
-    tuple.set(0, true);
-    tuple.set(1, 1);
-    tuple.set(2, 1.1f);
-    tuple.set(3, 1.1f);
-    tuple.set(4, 2.2f);
-    tuple.set(5, "hello world 1");
-    tuple.set(6, new DataByteArray("hello byte 1"));
-
-    TypesUtils.checkCompatible(tuple, schema);
-  }
-
-  // sees a double for an long column; 
-  @Test (expected = IOException.class)
-  public void testNegative5() throws ParseException, IOException {
-    String STR_SCHEMA = "s1:bool, s2:int, s3:long, s4:float, s5:double, s6:string, s7:bytes";
-    Schema schema = new Schema(STR_SCHEMA);
-
-    Tuple tuple = TypesUtils.createTuple(schema);
-    TypesUtils.resetTuple(tuple);
-
-    tuple.set(0, true);
-    tuple.set(1, 1);
-    tuple.set(2, 1.1);
-    tuple.set(3, 1.1f);
-    tuple.set(4, 2.2f);
-    tuple.set(5, "hello world 1");
-    tuple.set(6, new DataByteArray("hello byte 1"));
-
-    TypesUtils.checkCompatible(tuple, schema);
-  }
-  
-  // sees a double for an float column; 
-  @Test (expected = IOException.class)
-  public void testNegative6() throws ParseException, IOException {
-    String STR_SCHEMA = "s1:bool, s2:int, s3:long, s4:float, s5:double, s6:string, s7:bytes";
-    Schema schema = new Schema(STR_SCHEMA);
-
-    Tuple tuple = TypesUtils.createTuple(schema);
-    TypesUtils.resetTuple(tuple);
-
-    tuple.set(0, true);
-    tuple.set(1, 1);
-    tuple.set(2, 1);
-    tuple.set(3, 1.1);
-    tuple.set(4, 2.2);
-    tuple.set(5, "hello world 1");
-    tuple.set(6, new DataByteArray("hello byte 1"));
-
-    TypesUtils.checkCompatible(tuple, schema);
-  }
-
-  // sees an float for a boolean column;
-  @Test (expected = IOException.class)
-  public void testNegative7() throws ParseException, IOException {
-    String STR_SCHEMA = "s1:bool, s2:int, s3:long, s4:float, s5:double, s6:string, s7:bytes";
-    Schema schema = new Schema(STR_SCHEMA);
-
-    Tuple tuple = TypesUtils.createTuple(schema);
-    TypesUtils.resetTuple(tuple);
-
-    tuple.set(0, 1.1f);
-    tuple.set(1, 1);
-    tuple.set(2, 1.1f);
-    tuple.set(3, 1.1f);
-    tuple.set(4, 2.2f);
-    tuple.set(5, "hello world 1");
-    tuple.set(6, new DataByteArray("hello byte 1"));
-
-    TypesUtils.checkCompatible(tuple, schema);
-  }
-  
-  // no. of columns is wrong;
-  @Test (expected = IOException.class)
-  public void testNegative8() throws ParseException, IOException {
-    String STR_SCHEMA = "s1:bool, s2:int, s3:long, s4:float, s5:double, s6:string, s7:bytes";
-    String STR_SCHEMA1 = "s1:bool, s2:int, s3:long, s4:float, s5:double, s6:string";
-    Schema schema = new Schema(STR_SCHEMA);
-    Schema schema1 = new Schema(STR_SCHEMA1);
-
-    Tuple tuple = TypesUtils.createTuple(schema1);
-    TypesUtils.resetTuple(tuple);
-
-    tuple.set(0, true);
-    tuple.set(1, 1);
-    tuple.set(2, 1L);
-    tuple.set(3, 1.1f);
-    tuple.set(4, 2.2f);
-    tuple.set(5, "hello world 1");
-
-    TypesUtils.checkCompatible(tuple, schema);
-  }
-
-  @AfterClass
-  public static void tearDownOnce() throws IOException {
-    
-  }
-}
diff --git a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestZebraTupleTostring.java b/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestZebraTupleTostring.java
deleted file mode 100644
index 4eb6df39e..000000000
--- a/contrib/zebra/src/test/org/apache/hadoop/zebra/types/TestZebraTupleTostring.java
+++ /dev/null
@@ -1,256 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.zebra.types;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-import junit.framework.Assert;
-import org.apache.hadoop.zebra.parser.ParseException;
-import org.apache.hadoop.zebra.schema.Schema;
-import org.apache.hadoop.zebra.types.TypesUtils;
-import org.apache.pig.data.DataBag;
-import org.apache.pig.data.DataByteArray;
-import org.apache.pig.data.Tuple;
-import org.junit.Test;
-
-/**
- * 
- * Test toString() methold of ZebraTuple.
- * It should generate string representations of Zebra tuples of CSV format.
- * 
- */
-public class TestZebraTupleTostring {
-  @Test
-  public void testSimple() throws IOException, ParseException {
-    String STR_SCHEMA = "s1:bool, s2:int, s3:long, s4:float, s5:string, s6:bytes";
-    Schema schema = new Schema(STR_SCHEMA);
-    Tuple tuple = TypesUtils.createTuple(schema);
-    Assert.assertTrue(tuple instanceof ZebraTuple);
-    TypesUtils.resetTuple(tuple);
-
-    tuple.set(0, true); // bool
-    tuple.set(1, 1); // int
-    tuple.set(2, 1001L); // long
-    tuple.set(3, 1.1); // float
-    tuple.set(4, "hello\r world, 1\n"); // string
-    tuple.set(5, new DataByteArray("hello byte, 1")); // bytes
-
-    String str = tuple.toString();
-    Assert.assertTrue(str.equals("T,1,1001,1.1,'hello%0D world%2C 1%0A,#hello byte, 1"));
-  }
-  
-  @Test
-  public void testRecord() throws IOException, ParseException {
-    String STR_SCHEMA = "r1:record(f1:int, f2:long), r2:record(r3:record(f3:float, f4))";
-    Schema schema = new Schema(STR_SCHEMA);
-    Tuple tuple = TypesUtils.createTuple(schema);
-    TypesUtils.resetTuple(tuple);
-
-    Tuple tupRecord1;
-    Schema r1Schema = new Schema("f1:int, f2:long");
-    tupRecord1 = TypesUtils.createTuple(r1Schema);
-    TypesUtils.resetTuple(tupRecord1);
-    
-    Tuple tupRecord2;
-    Schema r2Schema = new Schema("r3:record(f3:float, f4)");
-    tupRecord2 = TypesUtils.createTuple(r2Schema);
-    TypesUtils.resetTuple(tupRecord2);
-    
-    Tuple tupRecord3;
-    Schema r3Schema = new Schema("f3:float, f4");
-    tupRecord3 = TypesUtils.createTuple(r3Schema);
-    TypesUtils.resetTuple(tupRecord3);
-    
-    // r1:record(f1:int, f2:long)
-    tupRecord1.set(0, 1);
-    tupRecord1.set(1, 1001L);
-    Assert.assertTrue(tupRecord1.toString().equals("1,1001"));
-    tuple.set(0, tupRecord1);
-
-    // r2:record(r3:record(f3:float, f4))
-    tupRecord2.set(0, tupRecord3);
-    tupRecord3.set(0, 1.3);
-    tupRecord3.set(1, new DataByteArray("r3 row1 byte array"));
-    Assert.assertTrue(tupRecord3.toString().equals("1.3,#r3 row1 byte array"));
-    Assert.assertTrue(tupRecord2.toString().equals("1.3,#r3 row1 byte array"));
-    tuple.set(1, tupRecord2);
-    Assert.assertTrue(tuple.toString().equals("1,1001,1.3,#r3 row1 byte array"));
-  }
-
-  @Test
-  public void testMap() throws IOException, ParseException {
-    String STR_SCHEMA = "m1:map(string),m2:map(map(int))";
-    Schema schema = new Schema(STR_SCHEMA);
-    Tuple tuple = TypesUtils.createTuple(schema);
-    TypesUtils.resetTuple(tuple);
-
-    // m1:map(string)
-    Map<String, String> m1 = new HashMap<String, String>();
-    m1.put("a", "'A");
-    m1.put("b", "B,B");
-    m1.put("c", "C\n");
-    tuple.set(0, m1);
-
-    // m2:map(map(int))
-    HashMap<String, Map<String, Integer>> m2 = new HashMap<String, Map<String, Integer>>();
-    Map<String, Integer> m3 = new HashMap<String, Integer>();
-    m3.put("m311", 311);
-    m3.put("m321", 321);
-    m3.put("m331", 331);
-    Map<String, Integer> m4 = new HashMap<String, Integer>();
-    m4.put("m411", 411);
-    m4.put("m421", 421);
-    m4.put("m431", 431);
-    m2.put("x", m3);
-    m2.put("y", m4);
-    tuple.set(1, m2);
-
-    Assert.assertTrue(tuple.toString().equals(
-        "m{'b,'B%2CB,'c,'C%0A,'a,''A},m{'y,m{'m421,421,'m411,411,'m431,431},'x,m{'m311,311,'m321,321,'m331,331}}"));
-  }
-  
-  @Test
-  public void testCollection() throws IOException, ParseException {
-    String STR_SCHEMA = 
-      "c1:collection(record(a:double, b:float, c:bytes)),c2:collection(record(r1:record(f1:int, f2:string), d:string)),c3:collection(record(c3_1:collection(record(e:int,f:bool))))";
-    Schema schema = new Schema(STR_SCHEMA);
-    Tuple tuple = TypesUtils.createTuple(schema);
-    TypesUtils.resetTuple(tuple);
-    
-    DataBag bag1 = TypesUtils.createBag();
-    Schema schColl = new Schema("a:double, b:float, c:bytes");
-    Tuple tupColl1 = TypesUtils.createTuple(schColl);
-    Tuple tupColl2 = TypesUtils.createTuple(schColl);
-
-    DataBag bag2 = TypesUtils.createBag();
-    Schema schColl2 = new Schema("r1:record(f1:int, f2:string), d:string");
-    Tuple tupColl2_1 = TypesUtils.createTuple(schColl2);
-    Tuple tupColl2_2 = TypesUtils.createTuple(schColl2);
-    
-    Tuple collRecord1;
-    try {
-      collRecord1 = TypesUtils.createTuple(new Schema("f1:int, f2:string"));
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-    Tuple collRecord2;
-    try {
-      collRecord2 = TypesUtils.createTuple(new Schema("f1:int, f2:string"));
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    // c3:collection(c3_1:collection(e:int,f:bool))
-    DataBag bag3 = TypesUtils.createBag();
-    DataBag bag3_1 = TypesUtils.createBag();
-    DataBag bag3_2 = TypesUtils.createBag();
-
-    Tuple tupColl3_1 = null;
-    try {
-      tupColl3_1 = TypesUtils.createTuple(new Schema("e:int,f:bool"));
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-    Tuple tupColl3_2;
-    try {
-      tupColl3_2 = TypesUtils.createTuple(new Schema("e:int,f:bool"));
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    Tuple tupColl3_3 = null;
-    try {
-      tupColl3_3 = TypesUtils.createTuple(new Schema("e:int,f:bool"));
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-    Tuple tupColl3_4;
-    try {
-      tupColl3_4 = TypesUtils.createTuple(new Schema("e:int,f:bool"));
-    } catch (ParseException e) {
-      e.printStackTrace();
-      throw new IOException(e);
-    }
-
-    byte[] abs1 = new byte[3];
-    byte[] abs2 = new byte[4];
-    tupColl1.set(0, 3.1415926);
-    tupColl1.set(1, 1.6);
-    abs1[0] = 'a';
-    abs1[1] = 'a';
-    abs1[2] = 'a';
-    tupColl1.set(2, new DataByteArray(abs1));
-    bag1.add(tupColl1);
-    tupColl2.set(0, 123.456789);
-    tupColl2.set(1, 100);
-    abs2[0] = 'b';
-    abs2[1] = 'c';
-    abs2[2] = 'd';
-    abs2[3] = 'e';
-    tupColl2.set(2, new DataByteArray(abs2));
-    bag1.add(tupColl2);
-    tuple.set(0, bag1);
-
-    collRecord1.set(0, 1);
-    collRecord1.set(1, "record1_string1");
-    tupColl2_1.set(0, collRecord1);
-    tupColl2_1.set(1, "hello1");
-    bag2.add(tupColl2_1);
-
-    collRecord2.set(0, 2);
-    collRecord2.set(1, "record2_string1");
-    tupColl2_2.set(0, collRecord2);
-    tupColl2_2.set(1, "hello2");
-    bag2.add(tupColl2_2);
-    tuple.set(1, bag2);
-
-    TypesUtils.resetTuple(tupColl3_1);
-    TypesUtils.resetTuple(tupColl3_2);
-    tupColl3_1.set(0, 1);
-    tupColl3_1.set(1, true);
-    tupColl3_2.set(0, 2);
-    tupColl3_2.set(1, false);
-    bag3_1.add(tupColl3_1);
-    bag3_1.add(tupColl3_2);
-    bag3.addAll(bag3_1);
-
-    tupColl3_3.set(0, 3);
-    tupColl3_3.set(1, true);
-    tupColl3_4.set(0, 4);
-    tupColl3_4.set(1, false);
-    bag3_2.add(tupColl3_3);
-    bag3_2.add(tupColl3_4);
-    bag3.addAll(bag3_2);
-    tuple.set(2, bag3);
-
-    Assert.assertTrue(tuple.toString().equals("3.1415926,1.6,#aaa\n" +
-    		"123.456789,100,#bcde\n" +
-    		"1,'record1_string1,'hello1\n"+
-    		"2,'record2_string1,'hello2\n"+
-    		"1,T\n"+
-    		"2,F\n"+
-    		"3,T\n"+
-    		"4,F\n"));
-  }
-}
-
diff --git a/contrib/zebra/src/test/smoke/zebra_smoke_run.pl b/contrib/zebra/src/test/smoke/zebra_smoke_run.pl
deleted file mode 100644
index 4f0f7c8f5..000000000
--- a/contrib/zebra/src/test/smoke/zebra_smoke_run.pl
+++ /dev/null
@@ -1,93 +0,0 @@
-#!/usr/local/bin/perl
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#  http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-
-use File::Basename;
-use Getopt::Long;
-if (@ARGV > 0 ){
-  GetOptions( \%options,
-  'cmd:s',
-  'smoke:s',
-  'help:i'
-  );
-}
-if (defined $options{'help'} or !defined $options{'smoke'}){
-  print ("perl run.pl --smoke\n");
-  print ("***************Enviroment Variables Needed***************\n");
-  print ("HADOOP_HOME\n");
-  print ("USER\n");
-  print ("ZEBRA_JAR\n");
-  print ("PIG_JAR\n");
-  print ("ZEBRA_SMOKE_JUNIT_JAR\n");
-  print ("CLASSPATH\n");
-  print ("HADOOP_CLASSPATH\n");
-  print ("ZEBRA_SMOKE_PIG_CLASS\n");
-  print ("ZEBRA_SMOKE_MAPRED_CLASS\n");
-  exit;
-}
-
-#make an output directory
-$zebraqa= $ENV{ZEBRA_SMOKE_DIR};
-mkdir("$zebraqa/output");
-$logfile="$zebraqa/output/zebra.out";
-
-if (defined $options{'smoke'}){
-  $my_hadoop_home=$ENV{HADOOP_HOME};
-  defined($my_hadoop_home) || die("HADOOP_HOME not defined");
-  $my_user=$ENV{USER};
-  defined($my_user) || die("USER not defined");
-  $my_zebra_jar=$ENV{ZEBRA_JAR};
-  defined($my_zebra_jar) || die("ZEBRA_JAR not defined");
-  $my_pig_jar=$ENV{PIG_JAR};
-  defined($my_pig_jar) || die("PIG_JAR not defined");
-  $my_junit_jar=$ENV{ZEBRA_SMOKE_JUNIT_JAR};
-  defined($my_junit_jar) || die("ZEBRA_SMOKE_JUNIT_JAR not defined");
-  $my_classpath=$ENV{CLASSPATH};
-  defined($my_classpath) || die("CLASSPATH not defined");
-  $my_smoke_pig_class=$ENV{ZEBRA_SMOKE_PIG_CLASS};
-  defined($my_smoke_pig_class) || die("ZEBRA_SMOKE_PIG_CLASS not defined");
-  $my_mapred_class=$ENV{ZEBRA_SMOKE_MAPRED_CLASS};
-  defined($my_mapred_class) || die("ZEBRA_SMOKE_MAPRED_CLASS not defined");
-  $my_hadoop_classpath=$ENV{HADOOP_CLASSPATH};
-  defined($my_hadoop_classpath) || die("HADOOP_CLASSPATH not defined");
-	
-  #execute pig job
-  write_to_log("..... STARTING ZEBRA PIG JOB TESTING .....\n");
-  $cmd="java -cp $my_classpath -DwhichCluster=\"realCluster\" -DHADOOP_HOME=$my_hadoop_home -DUSER=$my_user org.junit.runner.JUnitCore $my_smoke_pig_class > $zebraqa/output/zebra.out 2>&1";
-  exec_cmd($cmd);
-
-  #execute mapred job
-  write_to_log("..... STARTING ZEBRA MAP REDUCE JOB TESTING .....\n");	
-  $cmd="$my_hadoop_home/bin/hadoop jar $zebraqa/lib/zebra_smoke.jar $my_mapred_class -libjars $my_pig_jar,$my_zebra_jar,$my_junit_jar >> $zebraqa/output/zebra.out 2>&1";
-  exec_cmd($cmd);
-}
-
-sub exec_cmd {
-  my $cmd = shift or die "exec_cmd: Command not supplied!\n";
-  print ($cmd."\n");
-  my $rc = system($cmd);
-  !$rc or die "ERROR($rc): command failed: $cmd\n";
-  return $rc;
-}
-sub write_to_log {
-  ($whatToWrite,$ignore) = @_;
-  open(MYLOG, ">>$logfile") or die "can't open $logfile : $!\n";
-  print MYLOG $whatToWrite."\n";
-  close MYLOG;
-}
-
-
diff --git a/contrib/zebra/src/test/stress/bad_join.pig b/contrib/zebra/src/test/stress/bad_join.pig
deleted file mode 100644
index 85781c767..000000000
--- a/contrib/zebra/src/test/stress/bad_join.pig
+++ /dev/null
@@ -1,41 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register $zebraJar;
---fs -rmr $outputDir
-
-
---a1 = LOAD '$inputDir/unsorted1' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,byte2');
---a2 = LOAD '$inputDir/unsorted2' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,byte2');
-
---sort1 = order a1 by byte2;
---sort2 = order a2 by byte2;
-
---store sort1 into '$outputDir/100Msortedbyte21' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2];[byte2]');
---store sort2 into '$outputDir/100Msortedbyte22' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2];[byte2]');
-
-rec1 = load '$outputDir/100Msortedbyte21' using org.apache.hadoop.zebra.pig.TableLoader('','sorted');
-rec2 = load '$outputDir/100Msortedbyte22' using org.apache.hadoop.zebra.pig.TableLoader('','sorted');
-
-joina = join rec1 by byte2, rec2 by byte2 using "merge" ;
-
-E = foreach joina  generate $0 as count,  $1 as seed,  $2 as int1,  $3 as str2, $4 as byte2;
-
-store E into '$outputDir/bad3' using org.apache.hadoop.zebra.pig.TableStorer('');
-
-
diff --git a/contrib/zebra/src/test/stress/collecion4.pig b/contrib/zebra/src/test/stress/collecion4.pig
deleted file mode 100644
index 2796110fa..000000000
--- a/contrib/zebra/src/test/stress/collecion4.pig
+++ /dev/null
@@ -1,34 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-
-a1 = LOAD '/data/SDS_HTable' USING org.apache.hadoop.zebra.pig.TableLoader('MLF_viewinfo');
---limitedVals = LIMIT a1 10;
---dump limitedVals;
-
-store a1 into '/data/collection_viewinfo1' using org.apache.hadoop.zebra.pig.TableStorer('[MLF_viewinfo]');    
-
-a2 = LOAD '/data/collection_viewinfo1' USING org.apache.hadoop.zebra.pig.TableLoader('MLF_viewinfo');
---limitedVals = LIMIT a2 10;
---dump limitedVals;
-                      
-
-store a2 into '/data/collection_viewinfo2' using org.apache.hadoop.zebra.pig.TableStorer('[MLF_viewinfo]');    
-
-             
diff --git a/contrib/zebra/src/test/stress/config b/contrib/zebra/src/test/stress/config
deleted file mode 100644
index 8969d6647..000000000
--- a/contrib/zebra/src/test/stress/config
+++ /dev/null
@@ -1,5 +0,0 @@
-zebraJar=/grid/0/dev/hadoopqa/jars/zebra.jar
-inputDir=/data/zebraStress/input
-outputDir=/data/zebraStress/output
-unsorted1=unsorted1
-unsorted2=unsorted2
diff --git a/contrib/zebra/src/test/stress/join.pig b/contrib/zebra/src/test/stress/join.pig
deleted file mode 100644
index 9cd03b0c7..000000000
--- a/contrib/zebra/src/test/stress/join.pig
+++ /dev/null
@@ -1,36 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register $zebraJar;
---fs -rmr $outputDir
-
-
-
-rec1 = load '$outputDir/u1' using org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1', 'sorted');
-rec2 = load '$outputDir/u2' using org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1', 'sorted');
-
-
-joina = join rec1 by long1, rec2 by long1 using "merge" ;
-
-E = foreach joina  generate $0 as count,  $1 as seed,  $2 as int1,  $3 as str2, $4 as long1;
-joinE = order E by long1 parallel 25;
-
-
-
-store joinE into '$outputDir/j1' using org.apache.hadoop.zebra.pig.TableStorer('');
-                                                 
diff --git a/contrib/zebra/src/test/stress/join2.pig b/contrib/zebra/src/test/stress/join2.pig
deleted file mode 100644
index ba311270e..000000000
--- a/contrib/zebra/src/test/stress/join2.pig
+++ /dev/null
@@ -1,36 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register $zebraJar;
---fs -rmr $outputDir
-
-
-
-rec1 = load '$outputDir/u3' using org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1', 'sorted');
-rec2 = load '$outputDir/u4' using org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1', 'sorted');
-
-
-joina = join rec1 by long1, rec2 by long1 using "merge" ;
-
-E = foreach joina  generate $0 as count,  $1 as seed,  $2 as int1,  $3 as str2, $4 as long1;
-joinE = order E by long1 parallel 25;
-
-
-
-store joinE into '$outputDir/j2' using org.apache.hadoop.zebra.pig.TableStorer('');
-                                                 
diff --git a/contrib/zebra/src/test/stress/join_after_union.pig b/contrib/zebra/src/test/stress/join_after_union.pig
deleted file mode 100644
index 03b10c3d7..000000000
--- a/contrib/zebra/src/test/stress/join_after_union.pig
+++ /dev/null
@@ -1,57 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register $zebraJar;
---fs -rmr $outputDir
-
-
-a1 = LOAD '$inputDir/25Munsorted1' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1');
-a2 = LOAD '$inputDir/25Munsorted2' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1');    
-a3 = LOAD '$inputDir/25Munsorted3' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1');
-a4 = LOAD '$inputDir/25Munsorted4' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1');
-
-sort1 = order a1 by long1;
-sort2 = order a2 by long1;  
-sort3 = order a3 by long1;
-sort4 = order a4 by long1;
-
-store sort1 into '$outputDir/25Msorted1' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
-store sort2 into '$outputDir/25Msorted2' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]'); 
-store sort3 into '$outputDir/25Msorted3' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
-store sort4 into '$outputDir/25Msorted4' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');     
-                                                                                                                        
-
-joinl = LOAD '$outputDir/25Msorted1,$outputDir/25Msorted2' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1', 'sorted');
-joinll = order joinl by long1; 
-store joinll into '$outputDir/unionl' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');          
-
-
-joinr = LOAD '$outputDir/25Msorted3,$outputDir/25Msorted4' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1', 'sorted');
-joinrr = order joinr by long1; 
-store joinrr into '$outputDir/unionr' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
-
-
-rec1 = load '$outputDir/unionl' using org.apache.hadoop.zebra.pig.TableLoader();
-rec2 = load '$outputDir/unionr' using org.apache.hadoop.zebra.pig.TableLoader();   
-
-
-joina = join rec1 by long1, rec2 by long1 using "merge" ;
-
-E = foreach joina  generate $0 as count,  $1 as seed,  $2 as int1,  $3 as str2, $4 as long1;
-
-store E into '$outputDir/join_after_union_1' using org.apache.hadoop.zebra.pig.TableStorer('');                  
diff --git a/contrib/zebra/src/test/stress/join_after_union10k.pig b/contrib/zebra/src/test/stress/join_after_union10k.pig
deleted file mode 100644
index 4410941c2..000000000
--- a/contrib/zebra/src/test/stress/join_after_union10k.pig
+++ /dev/null
@@ -1,61 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register $zebraJar;
---fs -rmr $outputDir
-
-
-a1 = LOAD '$inputDir/10k1' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1');
-a2 = LOAD '$inputDir/10k2' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1');
-a3 = LOAD '$inputDir/10k3' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1');
-a4 = LOAD '$inputDir/10k4' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1');
-
-sort1 = order a1 by long1 parallel 6;
-sort2 = order a2 by long1 parallel 5;
-sort3 = order a3 by long1 parallel 7;
-sort4 = order a4 by long1 parallel 4;
-
-store sort1 into '$outputDir/sortedlong110k1' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
-store sort2 into '$outputDir/sortedlong110k2' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
-store sort3 into '$outputDir/sortedlong110k3' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
-store sort4 into '$outputDir/sortedlong110k4' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
-
-
-joinl = LOAD '$outputDir/sortedlong110k1,$outputDir/sortedlong110k2' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1', 'sorted');
-joinll = order joinl by long1 parallel 7;
-store joinll into '$outputDir/union10kl' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
-
-
-joinr = LOAD '$outputDir/sortedlong110k3,$outputDir/sortedlong110k4' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1', 'sorted');
-joinrr = order joinr by long1 parallel 4;
-store joinrr into '$outputDir/union10kr' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
-
-
-rec1 = load '$outputDir/union10kl' using org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1', 'sorted');
-rec2 = load '$outputDir/union10kr' using org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1', 'sorted');
-
-
-joina = join rec1 by long1, rec2 by long1 using "merge" ;
-
-E = foreach joina  generate $0 as count,  $1 as seed,  $2 as int1,  $3 as str2, $4 as long1;
-joinE = order E by long1 parallel 25;
-
-limitedVals = LIMIT joina 10;
-dump limitedVals;
-
-store joinE into '$outputDir/join_after_union_10k' using org.apache.hadoop.zebra.pig.TableStorer('');  
diff --git a/contrib/zebra/src/test/stress/join_after_union2.pig b/contrib/zebra/src/test/stress/join_after_union2.pig
deleted file mode 100644
index e69b3a2f5..000000000
--- a/contrib/zebra/src/test/stress/join_after_union2.pig
+++ /dev/null
@@ -1,57 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register $zebraJar;
---fs -rmr $outputDir
-
-
---a1 = LOAD '$inputDir/25Munsorted1' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1');
---a2 = LOAD '$inputDir/25Munsorted2' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1');
---a3 = LOAD '$inputDir/25Munsorted3' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1');
---a4 = LOAD '$inputDir/25Munsorted4' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1');
-
---sort1 = order a1 by long1 parallel 6;
---sort2 = order a2 by long1 parallel 5;
---sort3 = order a3 by long1 parallel 7;
---sort4 = order a4 by long1 parallel 4;
-
---store sort1 into '$outputDir/25Msorted11' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
---store sort2 into '$outputDir/25Msorted21' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
---store sort3 into '$outputDir/25Msorted31' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
---store sort4 into '$outputDir/25Msorted41' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
-
-
-joinl = LOAD '$outputDir/25Msorted11,$outputDir/25Msorted21' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1', 'sorted');
-joinll = order joinl by long1 parallel 7;
-store joinll into '$outputDir/unionl1' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
-
-
-joinr = LOAD '$outputDir/25Msorted31,$outputDir/25Msorted41' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1', 'sorted');
-joinrr = order joinr by long1 parallel 4;
-store joinrr into '$outputDir/unionr1' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
-
-
-rec1 = load '$outputDir/unionl1' using org.apache.hadoop.zebra.pig.TableLoader();
-rec2 = load '$outputDir/unionr1' using org.apache.hadoop.zebra.pig.TableLoader();
-
-
-joina = join rec1 by long1, rec2 by long1 using "merge" ;
-
-E = foreach joina  generate $0 as count,  $1 as seed,  $2 as int1,  $3 as str2, $4 as long1;
-joinE = order E by long1 parallel 25; 
-store joinE into '$outputDir/join_after_union_11' using org.apache.hadoop.zebra.pig.TableStorer(''); 
diff --git a/contrib/zebra/src/test/stress/join_after_union3.pig b/contrib/zebra/src/test/stress/join_after_union3.pig
deleted file mode 100644
index 2d60a1932..000000000
--- a/contrib/zebra/src/test/stress/join_after_union3.pig
+++ /dev/null
@@ -1,61 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register $zebraJar;
---fs -rmr $outputDir
-
-
---a1 = LOAD '$inputDir/25Munsorted1' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1');
---a2 = LOAD '$inputDir/25Munsorted2' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1');
---a3 = LOAD '$inputDir/25Munsorted3' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1');
---a4 = LOAD '$inputDir/25Munsorted4' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1');
-
---sort1 = order a1 by long1 parallel 6;
---sort2 = order a2 by long1 parallel 5;
---sort3 = order a3 by long1 parallel 7;
---sort4 = order a4 by long1 parallel 4;
-
---store sort1 into '$outputDir/25Msorted11' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
---store sort2 into '$outputDir/25Msorted21' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
---store sort3 into '$outputDir/25Msorted31' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
---store sort4 into '$outputDir/25Msorted41' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
-
-
---joinl = LOAD '$outputDir/25Msorted11,$outputDir/25Msorted21' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1', 'sorted');
---joinll = order joinl by long1 parallel 7;
---store joinll into '$outputDir/unionl1' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
-
-
---joinr = LOAD '$outputDir/25Msorted31,$outputDir/25Msorted41' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1', 'sorted');
---joinrr = order joinr by long1 parallel 4;
---store joinrr into '$outputDir/unionr1' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
-
-
-rec1 = load '$outputDir/unionl1' using org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1', 'sorted');
-rec2 = load '$outputDir/unionr1' using org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1', 'sorted');
-
-
-joina = join rec1 by long1, rec2 by long1 using "merge" ;
-
-E = foreach joina  generate $0 as count,  $1 as seed,  $2 as int1,  $3 as str2, $4 as long1;
-joinE = order E by long1 parallel 25;
-
---limitedVals = LIMIT joina 10;
---dump limitedVals;
-
-store joinE into '$outputDir/join_after_union_13' using org.apache.hadoop.zebra.pig.TableStorer('');             
diff --git a/contrib/zebra/src/test/stress/join_jira.pig b/contrib/zebra/src/test/stress/join_jira.pig
deleted file mode 100644
index 67523d02e..000000000
--- a/contrib/zebra/src/test/stress/join_jira.pig
+++ /dev/null
@@ -1,58 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register $zebraJar;
---fs -rmr $outputDir
-
---a1 = LOAD '$inputDir/10k1' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1');
---a2 = LOAD '$inputDir/10k2' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1');
---a3 = LOAD '$inputDir/10k3' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1');
---a4 = LOAD '$inputDir/10k4' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1');
-
---sort1 = order a1 by long1 parallel 6;
---sort2 = order a2 by long1 parallel 5;
---sort3 = order a3 by long1 parallel 7;
---sort4 = order a4 by long1 parallel 4;
-
---store sort1 into '$outputDir/sortedlong110k1' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
---store sort2 into '$outputDir/sortedlong110k2' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
---store sort3 into '$outputDir/sortedlong110k3' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
---store sort4 into '$outputDir/sortedlong110k4' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
-
-joinl = LOAD '$outputDir/sortedlong110k1,$outputDir/sortedlong110k2' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1', 'sorted');
---joinll = order joinl by long1 parallel 7;
---store joinll into '$outputDir/union10kl' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
-
-joinr = LOAD '$outputDir/sortedlong110k3,$outputDir/sortedlong110k4' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1', 'sorted');
---joinrr = order joinr by long1 parallel 4;
---store joinrr into '$outputDir/union10kr' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
-
---rec1 = load '$outputDir/union10kl' using org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1', 'sorted');
---rec2 = load '$outputDir/union10kr' using org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1', 'sorted');
-
---joina = join rec1 by long1, rec2 by long1 using "merge" ; 
-
-joina = join joinl by long1, joinr by long1 using "merge" ; 
-
-E = foreach joina  generate $0 as count,  $1 as seed,  $2 as int1,  $3 as str2, $4 as long1;
-joinE = order E by long1 parallel 25;
-
-limitedVals = LIMIT joinE 10;
-dump limitedVals;
-
-store joinE into '$outputDir/join_jira' using org.apache.hadoop.zebra.pig.TableStorer('');     
diff --git a/contrib/zebra/src/test/stress/join_jira1.pig b/contrib/zebra/src/test/stress/join_jira1.pig
deleted file mode 100644
index 689ec485c..000000000
--- a/contrib/zebra/src/test/stress/join_jira1.pig
+++ /dev/null
@@ -1,45 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-
---a1 = load '1.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
---a2 = load '2.txt' as (a:int, b:float,c:long,d:double,e:chararray,f:bytearray,r1(f1:chararray,f2:chararray),m1:map[]);
- 
---sort1 = order a1 by a parallel 6;
---sort2 = order a2 by a parallel 5;
-
---store sort1 into 'asort1' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c,d]');
---store sort2 into 'asort2' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c,d]');
---store sort1 into 'asort3' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c,d]');
---store sort2 into 'asort4' using org.apache.hadoop.zebra.pig.TableStorer('[a,b,c,d]');
-
-joinl = LOAD 'asort1,asort2' USING org.apache.hadoop.zebra.pig.TableLoader('a,b,c,d', 'sorted');
-
-joinr = LOAD 'asort3,asort4' USING org.apache.hadoop.zebra.pig.TableLoader('a,b,c,d', 'sorted');
-
-
-joina = join joinl by a, joinr by a using "merge" ;
-dump joina;
---E = foreach joina  generate $0 as count,  $1 as seed,  $2 as int1,  $3 as str2, $4 as long1;
---joinE = order E by long1 parallel 25;
-
---limitedVals = LIMIT joinE 10;
---dump limitedVals;
-
---store joinE into 'join_jira' using org.apache.hadoop.zebra.pig.TableStorer('');                     
diff --git a/contrib/zebra/src/test/stress/readme b/contrib/zebra/src/test/stress/readme
deleted file mode 100644
index 885ab36aa..000000000
--- a/contrib/zebra/src/test/stress/readme
+++ /dev/null
@@ -1,10 +0,0 @@
-1. run pig scritp,for example,
-java -cp /grid/0/dev/hadoopqa/jing1234/conf:/grid/0/dev/hadoopqa/jars/pig.jar:/grid/0/dev/hadoopqa/jars/tfile.jar:/grid/0/dev/hadoopqa/jars/zebra.jar org.apache.pig.Main -m config -M stress_union_02.pig
-
-2.compile ToolTestComparator.java from contrib/zebra/src/test/org/apache/hadoop/zebra/mapred/ToolTestComparator.java
-and ship the ToolTestComparator.class to cluster 
-run the tool to verifiy, for example,
-java -DwhichCluster="realCluster" -DHADOOP_HOME=$HADOOP_HOME -DUSER=$USER org.apache.hadoop.zebra.mapred.ToolTestComparator -verifyOption merge-join -pathTable1 /data/zebraStress/output/join4 -sortCol 4 -numbCols 5 -sortString byte1,int1
-
-3. For details on how to run,please refer to 
-http://twiki.corp.yahoo.com/pub/Grid/Release2TestPlan/zebra_stress_test.html
diff --git a/contrib/zebra/src/test/stress/sortSimpleString.pig b/contrib/zebra/src/test/stress/sortSimpleString.pig
deleted file mode 100644
index c4a3d0f2e..000000000
--- a/contrib/zebra/src/test/stress/sortSimpleString.pig
+++ /dev/null
@@ -1,27 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-
---test case : unsort table is /data/all1, sort table is /data/bcookie_sort, sort on bcookie
-
-a1 = LOAD '/data/all1' USING org.apache.hadoop.zebra.pig.TableLoader();
-                      
-a1order = order a1 by SF_bcookie;
-
-STORE a1order INTO '/data/bcookie_sort' USING org.apache.hadoop.zebra.pig.TableStorer('[SF_bcookie,SF_yuid,SF_ip];[SF_action,SF_afcookie,SF_browser,SF_bucket,SF_cbrn,SF_csc,SF_datestamp,SF_dst_spaceid,SF_dstid,SF_dstpvid,SF_error,SF_match_ts,SF_media,SF_ms,SF_os,SF_pcookie,SF_pg_load_time,SF_pg_size,SF_pg_spaceid,SF_query_term,SF_referrer,SF_server_code,SF_src_spaceid,SF_srcid,SF_srcpvid,SF_timestamp,SF_type,SF_ultspaceid,SF_ydod,MF_demog];[MF_page_params,MF_clickinfo,MLF_viewinfo]');
diff --git a/contrib/zebra/src/test/stress/stress_join_01.pig b/contrib/zebra/src/test/stress/stress_join_01.pig
deleted file mode 100644
index e6e2204fe..000000000
--- a/contrib/zebra/src/test/stress/stress_join_01.pig
+++ /dev/null
@@ -1,40 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register $zebraJar;
---fs -rmr $outputDir
-
-
---a1 = LOAD '$inputDir/unsorted1' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2');
---a2 = LOAD '$inputDir/unsorted2' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2');
-
---sort1 = order a1 by str2;
---sort2 = order a2 by str2;
-
---store sort1 into '$outputDir/sorted11' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2]');
---store sort2 into '$outputDir/sorted21' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2]');
-
-rec1 = load '$outputDir/sorted11' using org.apache.hadoop.zebra.pig.TableLoader();
-rec2 = load '$outputDir/sorted21' using org.apache.hadoop.zebra.pig.TableLoader();
-
-joina = join rec1 by str2, rec2 by str2 using "merge" ;
-
-E = foreach joina  generate $0 as count,  $1 as seed,  $2 as int1,  $3 as str2;
-
-
-store E into '$outputDir/join1' using org.apache.hadoop.zebra.pig.TableStorer('');
diff --git a/contrib/zebra/src/test/stress/stress_join_02.pig b/contrib/zebra/src/test/stress/stress_join_02.pig
deleted file mode 100644
index 7cb98292f..000000000
--- a/contrib/zebra/src/test/stress/stress_join_02.pig
+++ /dev/null
@@ -1,39 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register $zebraJar;
---fs -rmr $outputDir
-
-
-a1 = LOAD '$inputDir/unsorted1' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2');
-a2 = LOAD '$inputDir/unsorted2' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2');
-
-sort1 = order a1 by str2;
-sort2 = order a2 by str2;
-
-store sort1 into '$outputDir/100Msortedstr21' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2]');
-store sort2 into '$outputDir/100Msortedstr22' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2]');
-
-rec1 = load '$outputDir/100Msortedstr21' using org.apache.hadoop.zebra.pig.TableLoader('','sorted');
-rec2 = load '$outputDir/100Msortedstr22' using org.apache.hadoop.zebra.pig.TableLoader('','sorted');
-
-joina = join rec1 by str2, rec2 by str2 using "merge" ;
-
-E = foreach joina  generate $0 as count,  $1 as seed,  $2 as int1,  $3 as str2;
-
-store E into '$outputDir/join2' using org.apache.hadoop.zebra.pig.TableStorer('');
diff --git a/contrib/zebra/src/test/stress/stress_join_03.pig b/contrib/zebra/src/test/stress/stress_join_03.pig
deleted file mode 100644
index 58892b31d..000000000
--- a/contrib/zebra/src/test/stress/stress_join_03.pig
+++ /dev/null
@@ -1,39 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register $zebraJar;
---fs -rmr $outputDir
-
-
-a1 = LOAD '$inputDir/unsorted1' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,byte2');
-a2 = LOAD '$inputDir/unsorted2' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,byte2');
-
-sort1 = order a1 by byte2;
-sort2 = order a2 by byte2;
-
-store sort1 into '$outputDir/100Msortedbyte21' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,byte2]');
-store sort2 into '$outputDir/100Msortedbyte22' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,byte2]');
-
-rec1 = load '$outputDir/100Msortedbyte21' using org.apache.hadoop.zebra.pig.TableLoader('','sorted');
-rec2 = load '$outputDir/100Msortedbyte22' using org.apache.hadoop.zebra.pig.TableLoader('','sorted');
-
-joina = join rec1 by byte2, rec2 by byte2 using "merge" ;
-
-E = foreach joina  generate $0 as count,  $1 as seed,  $2 as int1,  $3 as str2, $4 as byte2;
-
-store E into '$outputDir/join3' using org.apache.hadoop.zebra.pig.TableStorer('');
diff --git a/contrib/zebra/src/test/stress/stress_join_04.pig b/contrib/zebra/src/test/stress/stress_join_04.pig
deleted file mode 100644
index 28cf2086f..000000000
--- a/contrib/zebra/src/test/stress/stress_join_04.pig
+++ /dev/null
@@ -1,49 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register $zebraJar;
---fs -rmr $outputDir
-
-
---a1 = LOAD '$inputDir/unsorted1' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,byte1');
---a2 = LOAD '$inputDir/unsorted2' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,byte1');
-
---sort1 = order a1 by byte1,int1;
---sort2 = order a2 by byte1,int1;
-
---store sort1 into '$outputDir/sortedbyteint1' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,byte1]');
---store sort2 into '$outputDir/sortedbyteint2' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,byte1]');
-
-rec1 = load '$outputDir/sortedbyteint1' using org.apache.hadoop.zebra.pig.TableLoader();
-rec2 = load '$outputDir/sortedbyteint2' using org.apache.hadoop.zebra.pig.TableLoader();
-
-joina = join rec1 by (byte1,int1), rec2 by (byte1,int1) using "merge" ;
-
-E = foreach joina  generate $0 as count,  $1 as seed,  $2 as int1,  $3 as str2, $4 as byte1;
-
-
---limitedVals = LIMIT E 5;
---dump limitedVals;
-
---store E into '$outputDir/join4' using org.apache.hadoop.zebra.pig.TableStorer('');
-
-
-join4 = load '$outputDir/join4' using org.apache.hadoop.zebra.pig.TableLoader();
-orderjoin = order join4 by byte1,int1;
-store orderjoin into '$outputDir/join4_order' using org.apache.hadoop.zebra.pig.TableStorer('');
-
diff --git a/contrib/zebra/src/test/stress/stress_join_05.pig b/contrib/zebra/src/test/stress/stress_join_05.pig
deleted file mode 100644
index 2962fb9cf..000000000
--- a/contrib/zebra/src/test/stress/stress_join_05.pig
+++ /dev/null
@@ -1,38 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-
-a1 = LOAD '/user/hadoopqa/zebra/data/unsorted1' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2');
-a2 = LOAD '/user/hadoopqa/zebra/data/unsorted2' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2');
-
-sort1 = order a1 by str2;
-sort2 = order a2 by str2;
-
-store sort1 into '/user/hadoopqa/zebra/temp/sorted1' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2]');
-store sort2 into '/user/hadoopqa/zebra/temp/sorted2' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2]');
-
-rec1 = load '/user/hadoopqa/zebra/temp/sorted1' using org.apache.hadoop.zebra.pig.TableLoader();
-rec2 = load '/user/hadoopqa/zebra/temp/sorted2' using org.apache.hadoop.zebra.pig.TableLoader();
-
-joina = join rec1 by str2, rec2 by str2 using "merge" ;
-
-limitedVals = LIMIT joina 5;
-dump limitedVals;
-
---store joina into '/user/hadoopqa/zebra/temp/join1' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2];[count,seed,int1,str2]');
diff --git a/contrib/zebra/src/test/stress/stress_load_store_00.pig b/contrib/zebra/src/test/stress/stress_load_store_00.pig
deleted file mode 100644
index 4a280f0bc..000000000
--- a/contrib/zebra/src/test/stress/stress_load_store_00.pig
+++ /dev/null
@@ -1,35 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register $zebraJar;
-fs -rmr $outputDir
-
-a1 = LOAD '$inputDir/$unsorted1' USING org.apache.hadoop.zebra.pig.TableLoader('m1');
---limitedVals = LIMIT a1 10;
---dump limitedVals;
-
-store a1 into '$outputDir/store1' using org.apache.hadoop.zebra.pig.TableStorer('[m1]');    
-
-a2 = LOAD '$outputDir/store1' USING org.apache.hadoop.zebra.pig.TableLoader('m1');
---limitedVals = LIMIT a2 10;
---dump limitedVals;
-
-
-store a2 into '$outputDir/store2' using org.apache.hadoop.zebra.pig.TableStorer('[m1]');    
-
-                
diff --git a/contrib/zebra/src/test/stress/stress_load_store_01.pig b/contrib/zebra/src/test/stress/stress_load_store_01.pig
deleted file mode 100644
index 81067a719..000000000
--- a/contrib/zebra/src/test/stress/stress_load_store_01.pig
+++ /dev/null
@@ -1,35 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register $zebraJar;
-fs -rmr $outputDir
-
-a1 = LOAD '$inputDir/$unsorted1' USING org.apache.hadoop.zebra.pig.TableLoader();
---limitedVals = LIMIT a1 10;
---dump limitedVals;
-
-store a1 into '$outputDir/store1' using org.apache.hadoop.zebra.pig.TableStorer('');    
-
-a2 = LOAD '$outputDir/store1' USING org.apache.hadoop.zebra.pig.TableLoader();
---limitedVals = LIMIT a2 10;
---dump limitedVals;
-
-
-store a2 into '$outputDir/store2' using org.apache.hadoop.zebra.pig.TableStorer('');    
-
-                
diff --git a/contrib/zebra/src/test/stress/stress_load_store_02.pig b/contrib/zebra/src/test/stress/stress_load_store_02.pig
deleted file mode 100644
index 5902b1179..000000000
--- a/contrib/zebra/src/test/stress/stress_load_store_02.pig
+++ /dev/null
@@ -1,45 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register $zebraJar;
---fs -rmr $outputDir
-
-
-
---a1 = LOAD '$inputDir/unsorted1' USING org.apache.hadoop.zebra.pig.TableLoader('int1,int2,str1,str2,byte1,byte2');
---limitedVals = LIMIT a1 10;
---dump limitedVals;
-
---store a1 into '$outputDir/mix1' using org.apache.hadoop.zebra.pig.TableStorer('[int1];[int2];[byte2];[str2,str1]');
-
---a2 = LOAD '$outputDir/mix1' USING org.apache.hadoop.zebra.pig.TableLoader('byte2,int2,int1,str1,str2');
---limitedVals = LIMIT a2 10;
---dump limitedVals;
-
-
---store a2 into '$outputDir/mix1_2' using org.apache.hadoop.zebra.pig.TableStorer('[int1];[int2];[byte2];[str2,str1]');      
-
-a3 = LOAD '$outputDir/mix1_2' USING org.apache.hadoop.zebra.pig.TableLoader('byte2,int2,int1,str1,str2');
---limitedVals = LIMIT a2 10;
---dump limitedVals;
-
-
-store a3 into '$outputDir/mix1_1' using org.apache.hadoop.zebra.pig.TableStorer('[int1];[int2];[byte2];[str2,str1]');   
-
---if only store once, and compare mix1 with mix1_1, table one has column number 6, table two has 5 (default column for table one)
---now we should compare mix1_1 and mix1_2 . they should be idential        
diff --git a/contrib/zebra/src/test/stress/stress_load_store_03.pig b/contrib/zebra/src/test/stress/stress_load_store_03.pig
deleted file mode 100644
index ed5f36589..000000000
--- a/contrib/zebra/src/test/stress/stress_load_store_03.pig
+++ /dev/null
@@ -1,34 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register $zebraJar;
---fs -rmr $outputDir
-
-
-a1 = LOAD '$inputDir/unsorted1' USING org.apache.hadoop.zebra.pig.TableLoader('m1');
---limitedVals = LIMIT a1 10;
---dump limitedVals;
-
-store a1 into '$outputDir/store1' using org.apache.hadoop.zebra.pig.TableStorer('[m1]');
-
-a2 = LOAD '$outputDir/store1' USING org.apache.hadoop.zebra.pig.TableLoader('m1');
---limitedVals = LIMIT a2 10;
---dump limitedVals;
-
-
-store a2 into '$outputDir/store2' using org.apache.hadoop.zebra.pig.TableStorer('[m1]'); 
diff --git a/contrib/zebra/src/test/stress/stress_load_store_04.pig b/contrib/zebra/src/test/stress/stress_load_store_04.pig
deleted file mode 100644
index 7c5aa1baf..000000000
--- a/contrib/zebra/src/test/stress/stress_load_store_04.pig
+++ /dev/null
@@ -1,36 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register $zebraJar;
---fs -rmr $outputDir
-
-
-
-a1 = LOAD '$inputDir/unsorted1' USING org.apache.hadoop.zebra.pig.TableLoader('c1');
---limitedVals = LIMIT a1 10;
---dump limitedVals;
-
-store a1 into '$outputDir/c1' using org.apache.hadoop.zebra.pig.TableStorer('[c1]');    
-
-a2 = LOAD '$outputDir/c1' USING org.apache.hadoop.zebra.pig.TableLoader('c1');
---limitedVals = LIMIT a2 10;
---dump limitedVals;
-
-
-store a2 into '$outputDir/c1_2' using org.apache.hadoop.zebra.pig.TableStorer('[c1]');    
-  
diff --git a/contrib/zebra/src/test/stress/stress_load_store_05.pig b/contrib/zebra/src/test/stress/stress_load_store_05.pig
deleted file mode 100644
index 2a6f01528..000000000
--- a/contrib/zebra/src/test/stress/stress_load_store_05.pig
+++ /dev/null
@@ -1,45 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register $zebraJar;
---fs -rmr $outputDir
-
-
-
-a1 = LOAD '$inputDir/unsorted1' USING org.apache.hadoop.zebra.pig.TableLoader('int1,int2,str1,str2,byte1,byte2,r1');
---limitedVals = LIMIT a1 10;
---dump limitedVals;
-
-store a1 into '$outputDir/r1' using org.apache.hadoop.zebra.pig.TableStorer('[int1];[int2];[byte2];[str2,str1,r1]');
-
-a2 = LOAD '$outputDir/r1' USING org.apache.hadoop.zebra.pig.TableLoader('byte2,int2,int1,str1,str2,r1');
---limitedVals = LIMIT a2 10;
---dump limitedVals;
-
-
-store a2 into '$outputDir/r1_2' using org.apache.hadoop.zebra.pig.TableStorer('[int1];[int2];[byte2];[str2,str1,r1]');  
-
-a3 = LOAD '$outputDir/r1_2' USING org.apache.hadoop.zebra.pig.TableLoader('byte2,int2,int1,str1,str2,r1');
---limitedVals = LIMIT a2 10;
---dump limitedVals;
-
-
-store a3 into '$outputDir/r1_1' using org.apache.hadoop.zebra.pig.TableStorer('[int1];[int2];[byte2];[str2,str1,r1]');
-
---if only store once, and compare r1 with r1_1, table one has column number 6, table two has 5 (default column for table one)
---now we should compare r1_1 and r1_2 . they should be identia
diff --git a/contrib/zebra/src/test/stress/stress_sort_01.pig b/contrib/zebra/src/test/stress/stress_sort_01.pig
deleted file mode 100644
index 139e76ade..000000000
--- a/contrib/zebra/src/test/stress/stress_sort_01.pig
+++ /dev/null
@@ -1,29 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register $zebraJar;
---fs -rmr $outputDir
-
-
-a1 = LOAD '$inputDir/$unsorted1' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,int2,str1,str2,byte1,byte2,float1,long1,double1,m1,r1,c1');
-
-store a1 into '$outputDir/unsorted1' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,int2,str1,str2,byte1,byte2,float1,long1,double1];[m1#{a}];[r1,c1]');
-
-sort1 = ORDER a1 BY int2;
-
-store sort1 into '$outputDir/sorted111' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,int2,str1,str2,byte1,byte2,float1,long1,double1];[m1#{a}];[r1,c1]'); 
diff --git a/contrib/zebra/src/test/stress/stress_sort_01_save.pig b/contrib/zebra/src/test/stress/stress_sort_01_save.pig
deleted file mode 100644
index e98efcb3d..000000000
--- a/contrib/zebra/src/test/stress/stress_sort_01_save.pig
+++ /dev/null
@@ -1,27 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register /grid/0/dev/hadoopqa/jars/zebra.jar;
-
-a1 = LOAD '/user/hadoopqa/zebra/data/unsorted1' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2');
-
-store a1 into '/user/hadoopqa/zebra/temp/unsorted1' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2]');
-
-sort1 = ORDER a1 BY str2;
-
-store sort1 into '/user/hadoopqa/zebra/temp/sorted1' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2]'); 
diff --git a/contrib/zebra/src/test/stress/stress_sort_02.pig b/contrib/zebra/src/test/stress/stress_sort_02.pig
deleted file mode 100644
index 4a0464d3a..000000000
--- a/contrib/zebra/src/test/stress/stress_sort_02.pig
+++ /dev/null
@@ -1,29 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register $zebraJar;
---fs -rmr $outputDir
-
-
-a1 = LOAD '$inputDir/$unsorted1' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,int2,str1,str2,byte1,byte2,float1,long1,double1,m1,r1,c1');
-
---store a1 into '$outputDir/unsortedbyte2' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,int2,str1,str2,byte1,byte2,float1,long1,double1];[m1#{a}];[r1,c1]');
-
-sort1 = ORDER a1 BY byte2;
-
-store sort1 into '$outputDir/sortedbyte2_1' using org.apache.hadoop.zebra.pig.TableStorer('[seed,int1,int2,str1,str2,byte1,byte2,float1,long1,double1];[m1#{a},r1,c1]'); 
diff --git a/contrib/zebra/src/test/stress/stress_sort_03.pig b/contrib/zebra/src/test/stress/stress_sort_03.pig
deleted file mode 100644
index 4a0464d3a..000000000
--- a/contrib/zebra/src/test/stress/stress_sort_03.pig
+++ /dev/null
@@ -1,29 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register $zebraJar;
---fs -rmr $outputDir
-
-
-a1 = LOAD '$inputDir/$unsorted1' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,int2,str1,str2,byte1,byte2,float1,long1,double1,m1,r1,c1');
-
---store a1 into '$outputDir/unsortedbyte2' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,int2,str1,str2,byte1,byte2,float1,long1,double1];[m1#{a}];[r1,c1]');
-
-sort1 = ORDER a1 BY byte2;
-
-store sort1 into '$outputDir/sortedbyte2_1' using org.apache.hadoop.zebra.pig.TableStorer('[seed,int1,int2,str1,str2,byte1,byte2,float1,long1,double1];[m1#{a},r1,c1]'); 
diff --git a/contrib/zebra/src/test/stress/stress_union_01.pig b/contrib/zebra/src/test/stress/stress_union_01.pig
deleted file mode 100644
index 60c7b9da7..000000000
--- a/contrib/zebra/src/test/stress/stress_union_01.pig
+++ /dev/null
@@ -1,38 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register $zebraJar;
---fs -rmr $outputDir
-
-
---a1 = LOAD '$inputDir/25Munsorted1' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int2,str2,byte2');
---a2 = LOAD '$inputDir/25Munsorted2' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int2,str2,byte2');
-
---sort1 = order a1 by int2;
---sort2 = order a2 by int2;
-
---store sort1 into '$outputDir/sortedint21' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int2,str2,byte2]');
---store sort2 into '$outputDir/sortedint22' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int2,str2,byte2]');
-
-rec1 = load '$outputDir/sortedint21' using org.apache.hadoop.zebra.pig.TableLoader();
-rec2 = load '$outputDir/sortedint22' using org.apache.hadoop.zebra.pig.TableLoader();
-
-joina = LOAD '$outputDir/sortedint21,$outputDir/sortedint22' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int2,str2,byte2', 'sorted');
-
-joinaa = order joina by int2;
-store joinaa into '$outputDir/union1' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int2,str2,byte2]');
diff --git a/contrib/zebra/src/test/stress/stress_union_02.pig b/contrib/zebra/src/test/stress/stress_union_02.pig
deleted file mode 100644
index 93ca770c1..000000000
--- a/contrib/zebra/src/test/stress/stress_union_02.pig
+++ /dev/null
@@ -1,38 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register $zebraJar;
---fs -rmr $outputDir
-
-
-a1 = LOAD '$inputDir/25Munsorted1' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,byte1');
-a2 = LOAD '$inputDir/25Munsorted2' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,byte1');
-
-sort1 = order a1 by byte1;
-sort2 = order a2 by byte1;
-
---store sort1 into '$outputDir/sortedbyte1' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,byte1]');
---store sort2 into '$outputDir/sortedbyte2' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,byte1]');
-
-rec1 = load '$outputDir/sortedbyte1' using org.apache.hadoop.zebra.pig.TableLoader();
-rec2 = load '$outputDir/sortedbyte2' using org.apache.hadoop.zebra.pig.TableLoader();
-
-joina = LOAD '$outputDir/sortedbyte1,$outputDir/sortedbyte2' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,byte1', 'sorted');
-    
-joinaa = order joina by byte1;
-store joinaa into '$outputDir/union2' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,byte1]');
diff --git a/contrib/zebra/src/test/stress/stress_union_02_2.pig b/contrib/zebra/src/test/stress/stress_union_02_2.pig
deleted file mode 100644
index 54ed0df21..000000000
--- a/contrib/zebra/src/test/stress/stress_union_02_2.pig
+++ /dev/null
@@ -1,38 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register $zebraJar;
---fs -rmr $outputDir
-
-
-a1 = LOAD '$inputDir/25Munsorted3' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,byte1');
-a2 = LOAD '$inputDir/25Munsorted4' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,byte1');
-
-sort1 = order a1 by byte1;
-sort2 = order a2 by byte1;
-
-store sort1 into '$outputDir/sortedbyte3' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,byte1]');
-store sort2 into '$outputDir/sortedbyte4' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,byte1]');
-
-rec1 = load '$outputDir/sortedbyte3' using org.apache.hadoop.zebra.pig.TableLoader();
-rec2 = load '$outputDir/sortedbyte4' using org.apache.hadoop.zebra.pig.TableLoader();
-
-joina = LOAD '$outputDir/sortedbyte3,$outputDir/sortedbyte4' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,byte1', 'sorted');
-    
-
-store joina into '$outputDir/union2_2' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,byte1]');
diff --git a/contrib/zebra/src/test/stress/stress_union_03.pig b/contrib/zebra/src/test/stress/stress_union_03.pig
deleted file mode 100644
index 22c0d4e8e..000000000
--- a/contrib/zebra/src/test/stress/stress_union_03.pig
+++ /dev/null
@@ -1,40 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register $zebraJar;
---fs -rmr $outputDir
-
-
-a1 = LOAD '$inputDir/25Munsorted3' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2');
-a2 = LOAD '$inputDir/25Munsorted4' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2');
-
-sort1 = order a1 by str2;
-sort2 = order a2 by str2;
-
---store sort1 into '$outputDir/strsorted1' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2]');
---store sort2 into '$outputDir/strsorted2' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2]');
-
-rec1 = load '$outputDir/strsorted1' using org.apache.hadoop.zebra.pig.TableLoader();
-rec2 = load '$outputDir/strsorted2' using org.apache.hadoop.zebra.pig.TableLoader();
-
-joina = LOAD '$outputDir/strsorted1,$outputDir/strsorted2' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2', 'sorted');
-    
-joinaa = order joina by str2;
-
-store joinaa into '$outputDir/union3' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2]');
-
diff --git a/contrib/zebra/src/test/stress/stress_union_04.pig b/contrib/zebra/src/test/stress/stress_union_04.pig
deleted file mode 100644
index f843496ca..000000000
--- a/contrib/zebra/src/test/stress/stress_union_04.pig
+++ /dev/null
@@ -1,38 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register $zebraJar;
---fs -rmr $outputDir
-
-
---a1 = LOAD '$inputDir/25Munsorted1' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int2,str2,byte2');
---a2 = LOAD '$inputDir/25Munsorted2' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int2,str2,byte2');
-
---sort1 = order a1 by int2,byte2;
---sort2 = order a2 by int2,byte2;
-
---store sort1 into '$outputDir/sortedintbyte21' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int2,str2,byte2]');
---store sort2 into '$outputDir/sortedintbyte22' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int2,str2,byte2]');
-
-rec1 = load '$outputDir/sortedintbyte21' using org.apache.hadoop.zebra.pig.TableLoader();
-rec2 = load '$outputDir/sortedintbyte22' using org.apache.hadoop.zebra.pig.TableLoader();
-
-joina = LOAD '$outputDir/sortedintbyte21,$outputDir/sortedintbyte22' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int2,str2,byte2', 'sorted');
-
-joinaa = order joina by int2,byte2;
-store joinaa into '$outputDir/union4' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int2,str2,byte2]');
diff --git a/contrib/zebra/src/test/stress/testjoing1.pig b/contrib/zebra/src/test/stress/testjoing1.pig
deleted file mode 100644
index f51e2afd7..000000000
--- a/contrib/zebra/src/test/stress/testjoing1.pig
+++ /dev/null
@@ -1,40 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register $zebraJar;
---fs -rmr $outputDir
-
-
---a1 = LOAD '$inputDir/unsorted1' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2');
---a2 = LOAD '$inputDir/unsorted2' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2');
-
---sort1 = order a1 by str2;
---sort2 = order a2 by str2;
-
---store sort1 into '$outputDir/sorted11' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2]');
---store sort2 into '$outputDir/sorted21' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2]');
-
-rec1 = load '$outputDir/sorted11' using org.apache.hadoop.zebra.pig.TableLoader('','sorted');
-rec2 = load '$outputDir/sorted21' using org.apache.hadoop.zebra.pig.TableLoader('','sorted');
-
-joina = join rec1 by str2, rec2 by str2 using "merge" ;
-
-E = foreach joina  generate $0 as count,  $1 as seed,  $2 as int1,  $3 as str2;
-
-
-store E into '$outputDir/testjoin21' using org.apache.hadoop.zebra.pig.TableStorer('');
diff --git a/contrib/zebra/src/test/stress/union1.pig b/contrib/zebra/src/test/stress/union1.pig
deleted file mode 100644
index 4537d7aef..000000000
--- a/contrib/zebra/src/test/stress/union1.pig
+++ /dev/null
@@ -1,35 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register $zebraJar;
---fs -rmr $outputDir
-
-
-a1 = LOAD '$inputDir/25Munsorted1' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1');
-a2 = LOAD '$inputDir/25Munsorted2' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1');
-
-sort1 = order a1 by long1 parallel 6;
-sort2 = order a2 by long1 parallel 5;
-
-store sort1 into '$outputDir/25MS1' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
-store sort2 into '$outputDir/25MS2' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
-
-
-union1 = LOAD '$outputDir/25MS1,$outputDir/25MS2' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1', 'sorted');
-orderunion1 = order union1 by long1 parallel 7;
-store orderunion1 into '$outputDir/u1' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');    
diff --git a/contrib/zebra/src/test/stress/union2.pig b/contrib/zebra/src/test/stress/union2.pig
deleted file mode 100644
index c13e397ca..000000000
--- a/contrib/zebra/src/test/stress/union2.pig
+++ /dev/null
@@ -1,35 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register $zebraJar;
---fs -rmr $outputDir
-
-
-a1 = LOAD '$inputDir/25Munsorted3' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1');
-a2 = LOAD '$inputDir/25Munsorted4' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1');
-
-sort1 = order a1 by long1 parallel 6;
-sort2 = order a2 by long1 parallel 5;
-
-store sort1 into '$outputDir/25MS3' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
-store sort2 into '$outputDir/25MS4' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
-
-
-union1 = LOAD '$outputDir/25MS3,$outputDir/25MS4' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1', 'sorted');
-orderunion1 = order union1 by long1 parallel 7;
-store orderunion1 into '$outputDir/u2' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');        
diff --git a/contrib/zebra/src/test/stress/union3.pig b/contrib/zebra/src/test/stress/union3.pig
deleted file mode 100644
index 3197d67de..000000000
--- a/contrib/zebra/src/test/stress/union3.pig
+++ /dev/null
@@ -1,35 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register $zebraJar;
---fs -rmr $outputDir
-
-
-a1 = LOAD '$inputDir/50Munsorted1' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1');
-a2 = LOAD '$inputDir/50Munsorted2' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1');
-
-sort1 = order a1 by long1 parallel 6;
-sort2 = order a2 by long1 parallel 5;
-
-store sort1 into '$outputDir/50MS1' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
-store sort2 into '$outputDir/50MS2' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
-
-
-union1 = LOAD '$outputDir/50MS1,$outputDir/50MS2' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1', 'sorted');
-orderunion1 = order union1 by long1 parallel 7;
-store orderunion1 into '$outputDir/u3' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');    
diff --git a/contrib/zebra/src/test/stress/union4.pig b/contrib/zebra/src/test/stress/union4.pig
deleted file mode 100644
index f99a95229..000000000
--- a/contrib/zebra/src/test/stress/union4.pig
+++ /dev/null
@@ -1,35 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-register $zebraJar;
---fs -rmr $outputDir
-
-
-a1 = LOAD '$inputDir/50Munsorted3' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1');
-a2 = LOAD '$inputDir/50Munsorted4' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1');
-
-sort1 = order a1 by long1 parallel 6;
-sort2 = order a2 by long1 parallel 5;
-
-store sort1 into '$outputDir/50MS3' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
-store sort2 into '$outputDir/50MS4' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');
-
-
-union1 = LOAD '$outputDir/50MS3,$outputDir/50MS4' USING org.apache.hadoop.zebra.pig.TableLoader('count,seed,int1,str2,long1', 'sorted');
-orderunion1 = order union1 by long1 parallel 7;
-store orderunion1 into '$outputDir/u4' using org.apache.hadoop.zebra.pig.TableStorer('[count,seed,int1,str2,long1]');    
diff --git a/src/org/apache/pig/data/DataReaderWriter.java b/src/org/apache/pig/data/DataReaderWriter.java
index 380e46a4f..5b0934844 100644
--- a/src/org/apache/pig/data/DataReaderWriter.java
+++ b/src/org/apache/pig/data/DataReaderWriter.java
@@ -37,7 +37,7 @@ import org.joda.time.DateTimeZone;
  * This class was used to handle reading and writing of intermediate
  *  results of data types. Now that functionality is in {@link BinInterSedes}
  *  This class could also be used for storing permanent results, it used
- *  by BinStorage and Zebra through DefaultTuple class.
+ *  by BinStorage through DefaultTuple class.
  */
 @InterfaceAudience.Private
 @InterfaceStability.Stable
diff --git a/test/e2e/pig/drivers/TestDriverScript.pm b/test/e2e/pig/drivers/TestDriverScript.pm
index bf23172a1..d16c172f4 100644
--- a/test/e2e/pig/drivers/TestDriverScript.pm
+++ b/test/e2e/pig/drivers/TestDriverScript.pm
@@ -91,13 +91,6 @@ sub runTest
 
         my %result;
 
-        # extract the current zebra.jar file path from the classpath
-        # and enter it in the hash for use in the substitution of :ZEBRAJAR:
-        my $zebrajar = $testCmd->{'cp'};
-        $zebrajar =~ s/zebra.jar.*/zebra.jar/;
-        $zebrajar =~ s/.*://;
-        $testCmd->{'zebrajar'} = $zebrajar;
-
         if(  $testCmd->{'pig'} ){
            return runPig( $self, $testCmd, $log );
         } elsif(  $testCmd->{'pigsql'} ){
@@ -126,7 +119,6 @@ sub runPig
     $pigcmd =~ s/:OUTPATH:/$outfile/g;
     $pigcmd =~ s/:FUNCPATH:/$testCmd->{'funcjarPath'}/g;
     $pigcmd =~ s/:PIGGYBANKPATH:/$testCmd->{'piggybankjarPath'}/g;
-    $pigcmd =~ s/:ZEBRAJAR:/$testCmd->{'zebrajar'}/g;
     $pigcmd =~ s/:RUNID:/$testCmd->{'UID'}/g;
     $pigcmd =~ s/:PIGHARNESS:/$ENV{PIG_HARNESS_ROOT}/g;
     $pigcmd =~ s/:USRHOMEPATH:/$testCmd->{'userhomePath'}/g;
