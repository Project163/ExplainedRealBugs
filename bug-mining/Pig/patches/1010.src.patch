diff --git a/CHANGES.txt b/CHANGES.txt
index ae732fddf..0931e9205 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -24,6 +24,8 @@ INCOMPATIBLE CHANGES
 
 IMPROVEMENTS
 
+PIG-2779: Refactoring the code for setting number of reducers (jay23jack via billgraham)
+
 PIG-2765: Implementing RollupDimensions UDF and adding ROLLUP clause in CUBE operator (prasanth_j via dvryaboy)
 
 PIG-2814: Fix issues with Sample operator documentation (prasanth_j via dvryaboy)
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/InputSizeReducerEstimator.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/InputSizeReducerEstimator.java
index 192fff82e..aaad67e49 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/InputSizeReducerEstimator.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/InputSizeReducerEstimator.java
@@ -78,6 +78,9 @@ public class InputSizeReducerEstimator implements PigReducerEstimator {
         log.info("BytesPerReducer=" + bytesPerReducer + " maxReducers="
             + maxReducers + " totalInputFileSize=" + totalInputFileSize);
 
+        // if totalInputFileSize == -1, we couldn't get the input size so we can't estimate.
+        if (totalInputFileSize == -1) { return -1; }
+
         int reducers = (int)Math.ceil((double)totalInputFileSize / bytesPerReducer);
         reducers = Math.max(1, reducers);
         reducers = Math.min(maxReducers, reducers);
@@ -92,8 +95,10 @@ public class InputSizeReducerEstimator implements PigReducerEstimator {
     static long getTotalInputFileSize(Configuration conf,
                                       List<POLoad> lds, Job job) throws IOException {
         long totalInputFileSize = 0;
+        boolean foundSize = false;
         for (POLoad ld : lds) {
             long size = getInputSizeFromLoader(ld, job);
+            if (size > -1) { foundSize = true; }
             if (size > 0) {
                 totalInputFileSize += size;
                 continue;
@@ -108,12 +113,13 @@ public class InputSizeReducerEstimator implements PigReducerEstimator {
                     if (status != null) {
                         for (FileStatus s : status) {
                             totalInputFileSize += Utils.getPathLength(fs, s);
+                            foundSize = true;
                         }
                     }
                 }
             }
         }
-        return totalInputFileSize;
+        return foundSize ? totalInputFileSize : -1;
     }
 
     /**
@@ -121,7 +127,7 @@ public class InputSizeReducerEstimator implements PigReducerEstimator {
      * loaders that implement @{link LoadMetadata}.
      * @param ld
      * @param job
-     * @return total input size in bytes, or 0 if unknown or incomplete
+     * @return total input size in bytes, or -1 if unknown or incomplete
      * @throws IOException on error
      */
     static long getInputSizeFromLoader(POLoad ld, Job job) throws IOException {
@@ -129,7 +135,7 @@ public class InputSizeReducerEstimator implements PigReducerEstimator {
                 || !(ld.getLoadFunc() instanceof LoadMetadata)
                 || ld.getLFile() == null
                 || ld.getLFile().getFileName() == null) {
-            return 0;
+            return -1;
         }
 
         ResourceStatistics statistics;
@@ -138,11 +144,11 @@ public class InputSizeReducerEstimator implements PigReducerEstimator {
                         .getStatistics(ld.getLFile().getFileName(), job);
         } catch (Exception e) {
             log.warn("Couldn't get statistics from LoadFunc: " + ld.getLoadFunc(), e);
-            return 0;
+            return -1;
         }
 
         if (statistics == null || statistics.getSizeInBytes() == null) {
-            return 0;
+            return -1;
         }
 
         return statistics.getSizeInBytes();
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java
index aaa181a2c..5d328cd9b 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java
@@ -756,40 +756,85 @@ public class JobControlCompiler{
             throw new JobCreationException(msg, errCode, PigException.BUG, e);
         }
     }
-
+    
+    /**
+     * Adjust the number of reducers based on the default_parallel, requested parallel and estimated
+     * parallel. For sampler jobs, we also adjust the next job in advance to get its runtime parallel as
+     * the number of partitions used in the sampler.
+     * @param plan the MR plan
+     * @param mro the MR operator
+     * @param conf the configuration
+     * @param nwJob the current job
+     * @throws IOException
+     */
     public void adjustNumReducers(MROperPlan plan, MapReduceOper mro, Configuration conf,
             org.apache.hadoop.mapreduce.Job nwJob) throws IOException {
+        int jobParallelism = calculateRuntimeReducers(mro, conf, nwJob);
+
+        if (mro.isSampler()) {
+            // We need to calculate the final number of reducers of the next job (order-by or skew-join)
+            // to generate the quantfile.
+            MapReduceOper nextMro = plan.getSuccessors(mro).get(0);
+
+            // Here we use the same conf and Job to calculate the runtime #reducers of the next job
+            // which is fine as the statistics comes from the nextMro's POLoads
+            int nPartitions = calculateRuntimeReducers(nextMro, conf, nwJob);
+            
+            // set the runtime #reducer of the next job as the #partition
+            ParallelConstantVisitor visitor =
+              new ParallelConstantVisitor(mro.reducePlan, nPartitions);
+            visitor.visit();
+        }
+        log.info("Setting Parallelism to " + jobParallelism);
+        
+        // set various parallelism into the job conf for later analysis, PIG-2779
+        conf.setInt("pig.info.reducers.default.parallel", pigContext.defaultParallel);
+        conf.setInt("pig.info.reducers.requested.parallel", mro.requestedParallelism);
+        conf.setInt("pig.info.reducers.estimated.parallel", mro.estimatedParallelism);
+
+        // this is for backward compatibility, and we encourage to use runtimeParallelism at runtime
+        mro.requestedParallelism = jobParallelism;
+        
+        // finally set the number of reducers
+        conf.setInt("mapred.reduce.tasks", jobParallelism);
+    }
+    
+    /**
+     * Calculate the runtime #reducers based on the default_parallel, requested parallel and estimated
+     * parallel, and save it to MapReduceOper's runtimeParallelism. 
+     * @return the runtimeParallelism
+     * @throws IOException 
+     */
+    private int calculateRuntimeReducers(MapReduceOper mro, Configuration conf,
+            org.apache.hadoop.mapreduce.Job nwJob) throws IOException{
+        // we don't recalculate for the same job
+        if (mro.runtimeParallelism != -1) {
+            return mro.runtimeParallelism;
+        }
         List<PhysicalOperator> loads = mro.mapPlan.getRoots();
         List<POLoad> lds = new ArrayList<POLoad>();
         for (PhysicalOperator ld : loads) {
             lds.add((POLoad)ld);
         }
+        
         int jobParallelism = -1;
-        int estimatedParallelism = estimateNumberOfReducers(conf, lds, nwJob);
+        
         if (mro.requestedParallelism > 0) {
             jobParallelism = mro.requestedParallelism;
         } else if (pigContext.defaultParallel > 0) {
             jobParallelism = pigContext.defaultParallel;
         } else {
-            jobParallelism = estimatedParallelism;
-        }
-        // Special case: Skewed Join and Order set parallelism to 1 even when no parallelism is specified.
-        if ((mro.isSkewedJoin() || mro.isGlobalSort()) && jobParallelism == 1) {
-            jobParallelism = estimatedParallelism;
-        }
-        if (mro.isSampler() && jobParallelism == 1) {
-            // Note: this is suboptimal, as the number of reducers communicated to the
-            // sampler is only based on the sampler inputs, meaning, the left side in the case
-            // of a skewed join. Ideally, we'd take into account the right side, as well.
-            ParallelConstantVisitor visitor =
-              new ParallelConstantVisitor(mro.reducePlan, estimatedParallelism);
-            visitor.visit();
+            mro.estimatedParallelism = estimateNumberOfReducers(conf, lds, nwJob);
+            // reducer estimation could return -1 if it couldn't estimate
+            log.info("Could not estimate number of reducers and no requested or default " +
+                     "parallelism set. Defaulting to 1 reducer.");
+            jobParallelism = mro.estimatedParallelism > 0 ? mro.estimatedParallelism : 1;
         }
-        log.info("Setting Parallelism to " + jobParallelism);
-        mro.requestedParallelism = jobParallelism;
-        conf.setInt("mapred.reduce.tasks", jobParallelism);
-    }
 
+        // save it
+        mro.runtimeParallelism = jobParallelism;
+        return jobParallelism;
+    }
     /**
      * Looks up the estimator from REDUCER_ESTIMATOR_KEY and invokes it to find the number of
      * reducers to use. If REDUCER_ESTIMATOR_KEY isn't set, defaults to InputSizeReducerEstimator.
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRCompiler.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRCompiler.java
index 310040f76..16eb00c62 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRCompiler.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRCompiler.java
@@ -47,7 +47,6 @@ import org.apache.pig.PigException;
 import org.apache.pig.PigWarning;
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
-import org.apache.pig.backend.hadoop.executionengine.HExecutionEngine;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.MROpPlanVisitor;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.MROperPlan;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.ScalarPhyFinder;
@@ -2474,28 +2473,10 @@ public class MRCompiler extends PhyPlanVisitor {
         PhysicalPlan rpep = new PhysicalPlan();
         ConstantExpression rpce = new ConstantExpression(new OperatorKey(scope,nig.getNextNodeId(scope)));
         rpce.setRequestedParallelism(rp);
-        int val = rp;
-        if(val<=0){
-            HExecutionEngine eng = pigContext.getExecutionEngine();
-            if(pigContext.getExecType() != ExecType.LOCAL){
-                try {
-                    if(val<=0)
-                        val = pigContext.defaultParallel;
-                    if (val<=0)
-                        val = eng.getJobConf().getNumReduceTasks();
-                    if (val<=0)
-                        val = 1;
-                } catch (Exception e) {
-                    int errCode = 6015;
-                    String msg = "Problem getting the default number of reduces from the Job Client.";
-                    throw new MRCompilerException(msg, errCode, PigException.REMOTE_ENVIRONMENT, e);
-                }
-            } else {
-            	val = 1; // local mode, set it to 1
-            }
-        }
-        int parallelismForSort = (rp <= 0 ? val : rp);
-        rpce.setValue(parallelismForSort);
+        
+        // We temporarily set it to rp and will adjust it at runtime, because the final degree of parallelism
+        // is unknown until we are ready to submit it. See PIG-2779.
+        rpce.setValue(rp);
         
         rpce.setResultType(DataType.INTEGER);
         rpep.add(rpce);
@@ -2549,7 +2530,7 @@ public class MRCompiler extends PhyPlanVisitor {
         mro.setReduceDone(true);
         mro.requestedParallelism = 1;
         mro.markSampler();
-        return new Pair<MapReduceOper, Integer>(mro, parallelismForSort);
+        return new Pair<MapReduceOper, Integer>(mro, rp);
     }
 
     static class LastInputStreamingOptimizer extends MROpPlanVisitor {
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceOper.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceOper.java
index 74680e26e..b87c20970 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceOper.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceOper.java
@@ -118,6 +118,12 @@ public class MapReduceOper extends Operator<MROpPlanVisitor> {
     
     int requestedParallelism = -1;
     
+    // estimated at runtime
+    int estimatedParallelism = -1;
+    
+    // calculated at runtime 
+    int runtimeParallelism = -1;
+    
     /* Name of the Custom Partitioner used */ 
     String customPartitioner = null;
     
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigReducerEstimator.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigReducerEstimator.java
index ae640edfa..a4e179c00 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigReducerEstimator.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigReducerEstimator.java
@@ -49,7 +49,7 @@ public interface PigReducerEstimator {
      * @param conf the job configuration
      * @param poLoadList list of POLoads used in the jobs physical plan
      * @param job job instance
-     * @return the number of reducers to use
+     * @return the number of reducers to use, or -1 if the count couldn't be estimated
      * @throws IOException
      */
     public int estimateNumberOfReducers(Configuration conf, List<POLoad> poLoadList, Job job)
diff --git a/test/org/apache/pig/test/TestJobSubmission.java b/test/org/apache/pig/test/TestJobSubmission.java
index 28139552f..89ac132dc 100644
--- a/test/org/apache/pig/test/TestJobSubmission.java
+++ b/test/org/apache/pig/test/TestJobSubmission.java
@@ -27,7 +27,6 @@ import java.util.Random;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.HBaseTestingUtility;
 import org.apache.hadoop.hbase.MiniHBaseCluster;
-import org.apache.hadoop.hbase.client.HTable;
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.mapred.jobcontrol.Job;
 import org.apache.hadoop.mapred.jobcontrol.JobControl;
@@ -476,16 +475,18 @@ public class TestJobSubmission {
         Job job = jobControl.getWaitingJobs().get(0);
         int parallel = job.getJobConf().getNumReduceTasks();
 
-        assertTrue(parallel==100);
-        
+        assertEquals(100, parallel);
+        Util.assertParallelValues(100, -1, -1, 100, job.getJobConf());
+
         pc.defaultParallel = -1;        
     }
-    
+
     @Test
     public void testDefaultParallelInSort() throws Throwable {
-        pc.defaultParallel = 100;
-        
-        String query = "a = load 'input';" + "b = order a by $0;" + "store b into 'output';";
+        // default_parallel is considered only at runtime, so here we only test requested parallel
+        // more thorough tests can be found in TestNumberOfReducers.java
+
+        String query = "a = load 'input';" + "b = order a by $0 parallel 100;" + "store b into 'output';";
         PigServer ps = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
         PhysicalPlan pp = Util.buildPp(ps, query);
         MROperPlan mrPlan = Util.buildMRPlan(pp, pc);
@@ -507,11 +508,11 @@ public class TestJobSubmission {
     
     @Test
     public void testDefaultParallelInSkewJoin() throws Throwable {
-        pc.defaultParallel = 100;
-        
+        // default_parallel is considered only at runtime, so here we only test requested parallel
+        // more thorough tests can be found in TestNumberOfReducers.java
         String query = "a = load 'input';" + 
                        "b = load 'input';" + 
-                       "c = join a by $0, b by $0 using 'skewed';" +
+                       "c = join a by $0, b by $0 using 'skewed' parallel 100;" +
                        "store c into 'output';";
         PigServer ps = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
         PhysicalPlan pp = Util.buildPp(ps, query);
@@ -558,8 +559,9 @@ public class TestJobSubmission {
         JobControl jc=jcc.compile(mrPlan, "Test");
         Job job = jc.getWaitingJobs().get(0);
         long reducer=Math.min((long)Math.ceil(new File("test/org/apache/pig/test/data/passwd").length()/100.0), 10);
-        assertEquals(job.getJobConf().getLong("mapred.reduce.tasks",10), reducer);
-        
+
+        Util.assertParallelValues(-1, -1, reducer, reducer, job.getJobConf());
+
         // use the PARALLEL key word, it will override the estimated reducer number
         query = "a = load '/passwd';" +
                 "b = group a by $0 PARALLEL 2;" +
@@ -574,14 +576,14 @@ public class TestJobSubmission {
         jcc = new JobControlCompiler(pc, conf);
         jc=jcc.compile(mrPlan, "Test");
         job = jc.getWaitingJobs().get(0);
-        assertEquals(job.getJobConf().getLong("mapred.reduce.tasks",10), 2);
-        
+
+        Util.assertParallelValues(-1, 2, -1, 2, job.getJobConf());
+
         final byte[] COLUMNFAMILY = Bytes.toBytes("pig");
-        HTable table = util.createTable(Bytes.toBytesBinary("passwd"),
-                COLUMNFAMILY);
+        util.createTable(Bytes.toBytesBinary("test_table"), COLUMNFAMILY);
         
         // the estimation won't take effect when it apply to non-dfs or the files doesn't exist, such as hbase
-        query = "a = load 'hbase://passwd' using org.apache.pig.backend.hadoop.hbase.HBaseStorage('c:f1 c:f2');" +
+        query = "a = load 'hbase://test_table' using org.apache.pig.backend.hadoop.hbase.HBaseStorage('c:f1 c:f2');" +
                 "b = group a by $0 ;" +
                 "store b into 'output';";
         pp = Util.buildPp(ps, query);
@@ -595,8 +597,10 @@ public class TestJobSubmission {
         jcc = new JobControlCompiler(pc, conf);
         jc=jcc.compile(mrPlan, "Test");
         job = jc.getWaitingJobs().get(0);
-        assertEquals(job.getJobConf().getLong("mapred.reduce.tasks",10), 1);
-        util.deleteTable(Bytes.toBytesBinary("passwd"));
+
+        Util.assertParallelValues(-1, -1, -1, 1, job.getJobConf());
+
+        util.deleteTable(Bytes.toBytesBinary("test_table"));
         // In HBase 0.90.1 and above we can use util.shutdownMiniHBaseCluster()
         // here instead.
         MiniHBaseCluster hbc = util.getHBaseCluster();
@@ -624,18 +628,24 @@ public class TestJobSubmission {
         JobControlCompiler jcc = new JobControlCompiler(pc, conf);
         JobControl jobControl = jcc.compile(mrPlan, query);
 
-        assertEquals(2, mrPlan.size());     
-        
+        assertEquals(2, mrPlan.size());
+
+        // first job uses a single reducer for the sampling
+        Util.assertParallelValues(-1, 1, -1, 1, jobControl.getWaitingJobs().get(0).getJobConf());
+
         // Simulate the first job having run so estimation kicks in.
         MapReduceOper sort = mrPlan.getLeaves().get(0);        
         jcc.updateMROpPlan(jobControl.getReadyJobs());
         FileLocalizer.create(sort.getQuantFile(), pc);
-        jcc.compile(mrPlan, query);
+        jobControl = jcc.compile(mrPlan, query);
 
         sort = mrPlan.getLeaves().get(0);
         long reducer=Math.min((long)Math.ceil(new File("test/org/apache/pig/test/data/passwd").length()/100.0), 10);
         assertEquals(reducer, sort.getRequestedParallelism());
-        
+
+        // the second job estimates reducers
+        Util.assertParallelValues(-1, -1, reducer, reducer, jobControl.getWaitingJobs().get(0).getJobConf());
+
         // use the PARALLEL key word, it will override the estimated reducer number
         query = "a = load '/passwd';" + "b = order a by $0 PARALLEL 2;" +
                 "store b into 'output';";
@@ -643,7 +653,7 @@ public class TestJobSubmission {
         
         mrPlan = Util.buildMRPlanWithOptimizer(pp, pc);               
 
-        assertEquals(2, mrPlan.size());     
+        assertEquals(2, mrPlan.size());
         
         sort = mrPlan.getLeaves().get(0);        
         assertEquals(2, sort.getRequestedParallelism());
@@ -659,7 +669,9 @@ public class TestJobSubmission {
         
         sort = mrPlan.getLeaves().get(0);
         
-        assertEquals(1, sort.getRequestedParallelism());
+        // the requested parallel will be -1 if users don't set any of default_parallel, paralllel
+        // and the estimation doesn't take effect. MR framework will finally set it to 1.
+        assertEquals(-1, sort.getRequestedParallelism());
         
         // test order by with three jobs (after optimization)
         query = "a = load '/passwd';" +
@@ -677,12 +689,22 @@ public class TestJobSubmission {
 
         jobControl = jcc.compile(mrPlan, query);
         Util.copyFromLocalToCluster(cluster, "test/org/apache/pig/test/data/passwd", ((POLoad) sort.mapPlan.getRoots().get(0)).getLFile().getFileName());
+
+        //First job is just foreach with projection, mapper-only job, so estimate gets ignored
+        Util.assertParallelValues(-1, -1, reducer, 0, jobControl.getWaitingJobs().get(0).getJobConf());
+
         jcc.updateMROpPlan(jobControl.getReadyJobs());
         jobControl = jcc.compile(mrPlan, query);
         jcc.updateMROpPlan(jobControl.getReadyJobs());
 
+        //Second job is a sampler, which requests and gets 1 reducer
+        Util.assertParallelValues(-1, 1, -1, 1, jobControl.getWaitingJobs().get(0).getJobConf());
+
         jobControl = jcc.compile(mrPlan, query);
         sort = mrPlan.getLeaves().get(0);       
         assertEquals(reducer, sort.getRequestedParallelism());
+
+        //Third job is the order, which uses the estimated number of reducers
+        Util.assertParallelValues(-1, -1, reducer, reducer, jobControl.getWaitingJobs().get(0).getJobConf());
     }
 }
diff --git a/test/org/apache/pig/test/TestNumberOfReducers.java b/test/org/apache/pig/test/TestNumberOfReducers.java
new file mode 100644
index 000000000..2f52f4b02
--- /dev/null
+++ b/test/org/apache/pig/test/TestNumberOfReducers.java
@@ -0,0 +1,266 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.pig.test;
+
+import java.io.File;
+import java.io.FileWriter;
+import java.io.IOException;
+import java.io.PrintWriter;
+
+import org.apache.pig.PigRunner;
+import org.apache.pig.PigServer;
+import org.apache.pig.impl.PigContext;
+import org.apache.pig.tools.pigstats.JobStats;
+import org.apache.pig.tools.pigstats.PigStats;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.JUnit4;
+
+import static org.junit.Assert.*;
+
+/**
+ * This class tests whether the number of reducers is correctly set for queries
+ * involving a sample job. Note the sample job should use the actual #reducers
+ * of the following order-by or skew-join job to generate the partition file,
+ * and the challenge is that whether we can get the actual #reducer of the
+ * order-by/skew-join job in advance.
+ */
+@RunWith(JUnit4.class)
+public class TestNumberOfReducers {
+
+    private static final String LOCAL_INPUT_FILE = "test/org/apache/pig/test/data/passwd";
+    private static final String HDFS_INPUT_FILE = "passwd";
+    private static final String OUTPUT_FILE = "output";
+    private static final String PIG_FILE = "test.pig";
+
+    static PigContext pc;
+    static PigServer pigServer;
+    private static MiniCluster cluster;
+
+    @BeforeClass
+    public static void setUpBeforeClass() throws Exception {
+        cluster = MiniCluster.buildCluster();
+
+        Util.copyFromLocalToCluster(cluster, LOCAL_INPUT_FILE, HDFS_INPUT_FILE);
+    }
+
+    @AfterClass
+    public static void oneTimeTearDown() throws Exception {
+        cluster.shutDown();
+    }
+
+    /**
+     * Specifying bytes_per_reducer, default_parallel and parallel, verifying
+     * whether it uses the actual #reducers as actual_parallel.
+     * 
+     * Note the input file size is 555B.
+     * 
+     */
+    private void verifyOrderBy(int bytes_per_reducer, int default_parallel,
+            int parallel, int actual_parallel) throws IOException {
+        PrintWriter w = new PrintWriter(new FileWriter(PIG_FILE));
+
+        // the file is 555B
+        w.println("SET pig.exec.reducers.bytes.per.reducer " + bytes_per_reducer + ";");
+        
+        w.println("SET default_parallel " + default_parallel + ";");
+        w.println("A = load '" + HDFS_INPUT_FILE + "';");
+        w.print("B = order A by $0 ");
+        if (parallel > 0) 
+            w.print("parallel " + parallel);
+        w.println(";");
+        w.println("store B into '" + OUTPUT_FILE + "';");
+        w.close();
+
+        doTest(bytes_per_reducer, default_parallel, parallel, actual_parallel);
+    }
+    
+    /**
+     * Specifying bytes_per_reducer, default_parallel and parallel, verifying
+     * whether it uses the actual #reducers as actual_parallel.
+     * 
+     * Note the input file size is 555B.
+     * 
+     */
+    private void verifySkewJoin(int bytes_per_reducer, int default_parallel,
+            int parallel, int actual_parallel) throws IOException {
+        PrintWriter w = new PrintWriter(new FileWriter(PIG_FILE));
+
+        // the file is 555B
+        w.println("SET pig.exec.reducers.bytes.per.reducer " + bytes_per_reducer + ";");
+        w.println("SET default_parallel " + default_parallel + ";");
+        w.println("A1 = load '" + HDFS_INPUT_FILE + "';");
+        w.println("A2 = load '" + HDFS_INPUT_FILE + "';");
+        w.print("B = join A1 by $0,  A2 by $0 using 'skewed' ");
+        if (parallel > 0) 
+            w.print("parallel " + parallel);
+        w.println(";");
+        w.println("store B into '" + OUTPUT_FILE + "';");
+        w.close();
+
+        doTest(bytes_per_reducer, default_parallel, parallel, actual_parallel);
+    }
+
+    private void doTest(double bytes_per_reducer, int default_parallel,
+                        int parallel, int actual_parallel) throws IOException {
+        try {
+            String[] args = { PIG_FILE };
+            PigStats stats = PigRunner.run(args, null);
+
+            assertTrue(stats.isSuccessful());
+
+            // get the skew-join job stat
+            JobStats js = (JobStats) stats.getJobGraph().getSinks().get(0);
+            assertEquals(actual_parallel, js.getNumberReduces());
+
+            // estimation should only kick in if parallel and default_parallel are not set
+            long estimatedReducers = -1;
+            if (parallel < 1 && default_parallel < 1) {
+                double fileSize = (double)(new File("test/org/apache/pig/test/data/passwd").length());
+                int inputFiles = js.getInputs().size();
+                estimatedReducers = Math.min((long)Math.ceil(fileSize/(double)bytes_per_reducer) * inputFiles, 999);
+            }
+
+            Util.assertParallelValues(default_parallel, parallel, estimatedReducers,
+                    actual_parallel, js.getInputs().get(0).getConf());
+
+        } catch (Exception e) {
+            assertNull("Exception thrown during verifySkewJoin", e);
+        } finally {
+            new File(PIG_FILE).delete();
+            Util.deleteFile(cluster, OUTPUT_FILE);
+        }
+    }
+
+    /**
+     * For 550B file and 300B per reducer, it estimates 2 #reducers.
+     * @throws Exception
+     */
+    @Test
+    public void testOrderByEstimate() throws Exception{
+        verifyOrderBy(300, -1, -1, 2);
+    }
+    
+    /**
+     * For two 550B files and 600B per reducer, it estimates 2 #reducers.
+     * @throws Exception
+     */
+    @Test
+    public void testSkewJoinEstimate() throws Exception{
+        verifySkewJoin(600, -1, -1, 2);
+    }
+    
+    
+    /**
+     * Pig will estimate 2 reducers but we specify parallel 1, so it should use
+     * 1 reducer.
+     * 
+     * @throws Exception
+     */
+    @Test
+    public void testOrderByEstimate2Parallel1() throws Exception {
+        verifyOrderBy(300, -1, 1, 1);
+    }
+    
+    /**
+     * Pig will estimate 2 reducers but we specify parallel 1, so it should use
+     * 1 reducer.
+     * 
+     * @throws Exception
+     */
+    @Test
+    public void testSkewJoinEstimate2Parallel1() throws Exception {
+        verifySkewJoin(600, -1, 1, 1);
+    }
+
+    /**
+     * Pig will estimate 2 reducers but we specify default parallel 1, so it
+     * should use 1 reducer.
+     * 
+     * @throws Exception
+     */
+    @Test
+    public void testOrderByEstimate2Default1() throws Exception {
+        verifyOrderBy(300,  1, -1, 1);
+    }
+    
+    /**
+     * Pig will estimate 2 reducers but we specify default parallel 1, so it
+     * should use 1 reducer.
+     * 
+     * @throws Exception
+     */
+    @Test
+    public void testSkewJoinEstimate2Default1() throws Exception {
+        verifySkewJoin(600, 1, -1, 1);
+    }
+
+    /**
+     * Pig will estimate 2 reducers but we specify parallel 4, so it should use
+     * 4 reducer.
+     * 
+     * TODO we need verify that the sample job generates 4 partitions instead of
+     * 2.
+     * 
+     * @throws Exception
+     */
+    @Test
+    public void testOrderByEstimate2Parallel4() throws Exception {
+        verifyOrderBy(300, -1, 4, 4);
+    }
+    
+    /**
+     * Pig will estimate 2 reducers but we specify parallel 4, so it should use
+     * 4 reducer.
+     * 
+     * TODO we need verify that the sample job generates 4 partitions instead of
+     * 2.
+     * 
+     * @throws Exception
+     */
+    @Test
+    public void testSkewJoinEstimate2Parallel4() throws Exception {
+        verifySkewJoin(600, -1, 4, 4);
+    }
+
+    /**
+     * Pig will estimate 6 reducers but we specify default parallel 2, so it
+     * should use 2 reducer. Also, the sampler should generate 2 partitions,
+     * instead of 6, otherwise the next job will fail.
+     * 
+     * @throws Exception
+     */
+    @Test
+    public void testOrderByEstimate6Default2() throws Exception {
+        verifyOrderBy(100, 2, -1, 2);
+    }
+    
+    /**
+     * Pig will estimate 6 reducers but we specify default parallel 2, so it
+     * should use 2 reducer. Also, the sampler should generate 2 partitions,
+     * instead of 6, otherwise the next job will fail.
+     * 
+     * @throws Exception
+     */
+    @Test
+    public void testSkewJoinEstimate6Default2() throws Exception {
+        verifySkewJoin(200, 2, -1, 2);
+    }
+}
diff --git a/test/org/apache/pig/test/Util.java b/test/org/apache/pig/test/Util.java
index 3913692bd..ef72b066b 100644
--- a/test/org/apache/pig/test/Util.java
+++ b/test/org/apache/pig/test/Util.java
@@ -18,6 +18,7 @@
 package org.apache.pig.test;
 
 import static java.util.regex.Matcher.quoteReplacement;
+import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.fail;
 
 import java.io.BufferedReader;
@@ -1160,4 +1161,19 @@ public class Util {
             return true;
         return false;
     }
+
+    public static void assertParallelValues(long defaultParallel,
+                                             long requestedParallel,
+                                             long estimatedParallel,
+                                             long runtimeParallel,
+                                             Configuration conf) {
+        assertConfLong(conf, "pig.info.reducers.default.parallel", defaultParallel);
+        assertConfLong(conf, "pig.info.reducers.requested.parallel", requestedParallel);
+        assertConfLong(conf, "pig.info.reducers.estimated.parallel", estimatedParallel);
+        assertConfLong(conf, "mapred.reduce.tasks", runtimeParallel);
+    }
+
+    private static void assertConfLong(Configuration conf, String param, long expected) {
+        assertEquals("Unexpected value found in configs for " + param, expected, conf.getLong(param, -1));
+    }
 }
