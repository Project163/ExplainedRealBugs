diff --git a/CHANGES.txt b/CHANGES.txt
index 92399dde3..f5ca47601 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -34,6 +34,8 @@ OPTIMIZATIONS
  
 BUG FIXES
 
+PIG-3524: Clean up Launcher and MapReduceLauncher after PIG-3419 (cheolsoo)
+
 PIG-3515: Shell commands are limited from OS buffer (andronat via cheolsoo)
 
 PIG-3520: Provide backward compatibility for PigRunner and PPNL after PIG-3419 (daijy via cheolsoo)
diff --git a/src/org/apache/pig/backend/executionengine/ExecutionEngine.java b/src/org/apache/pig/backend/executionengine/ExecutionEngine.java
index 7d480d760..56a958b83 100644
--- a/src/org/apache/pig/backend/executionengine/ExecutionEngine.java
+++ b/src/org/apache/pig/backend/executionengine/ExecutionEngine.java
@@ -37,7 +37,7 @@ import org.apache.pig.tools.pigstats.PigStats;
 import org.apache.pig.tools.pigstats.ScriptState;
 
 /**
- * 
+ *
  * The main interface bridging the front end and back end of Pig. This allows Pig
  * to be ran on multiple Execution Engines, and not being limited to only Hadoop
  * MapReduce. The ExecutionEngines must support the following methods as these
@@ -46,7 +46,7 @@ import org.apache.pig.tools.pigstats.ScriptState;
  * The ExecutionEngine instance comes from the ExecType, and it can choose to reuse
  * ExecutionEngine instances. All specifications for methods are listed below
  * as well as expected behavior, and the ExecutionEngine must conform to these.
- * 
+ *
  */
 
 
@@ -67,12 +67,12 @@ public interface ExecutionEngine {
      * another call to init() if appropriate. This decision is delegated to the
      * ExecutionEngine -- that is, the caller will not call init() after
      * updating the properties.
-     * 
+     *
      * The Properties passed in should replace any configuration that occurred
      * from previous Properties object. The Properties object should also be
      * updated to reflect the deprecation/modifications that the ExecutionEngine
      * may trigger.
-     * 
+     *
      * @param newConfiguration -- Properties object holding all configuration vals
      */
     public void setConfiguration(Properties newConfiguration)
@@ -82,12 +82,12 @@ public interface ExecutionEngine {
      * Responsible for setting a specific property and value. This method may be
      * called as a result of a user "SET" command in the script or elsewhere in
      * Pig to set certain properties.
-     * 
+     *
      * The properties object of the PigContext should be updated with the
      * property and value with deprecation/other configuration done by the
      * ExecutionEngine reflected. The ExecutionEngine should also update its
      * internal configuration view as well.
-     * 
+     *
      * @param property to update
      * @param value to set for property
      */
@@ -96,7 +96,7 @@ public interface ExecutionEngine {
     /**
      * Returns the Properties representation of the ExecutionEngine
      * configuration. The Properties object returned does not have to be the
-     * same object between distinct calls to getConfiguration(). The ExecutionEngine 
+     * same object between distinct calls to getConfiguration(). The ExecutionEngine
      * may create a new Properties object populated with all the properties
      * each time.
      */
@@ -110,11 +110,11 @@ public interface ExecutionEngine {
      * executing the logical plan and returns an implementation of PigStats that
      * contains the relevant information/statistics of the execution of the
      * script.
-     * 
+     *
      * @param lp -- plan to compile
      * @param grpName -- group name for submission
      * @param pc -- context for execution
-     * @throws ExecException 
+     * @throws ExecException
      */
     public PigStats launchPig(LogicalPlan lp, String grpName, PigContext pc)
             throws FrontendException, ExecException;
@@ -131,16 +131,16 @@ public interface ExecutionEngine {
      * on a PhysicalPlan and an ExecutionPlan then there would be two separate
      * files detailing each. The suffix param indicates the suffix of each of
      * these file names.
-     * 
+     *
      * @param lp -- plan to explain
      * @param pc -- context for explain processing
      * @param ps -- print stream to write all output to (if dir param is null)
-     * @param format -- format to print explain 
-     * @param verbose 
+     * @param format -- format to print explain
+     * @param verbose
      * @param dir -- directory to write output to. if not null, write to files
      * @param suffix -- if writing to files, suffix to be used for each file
-     * 
-     * 
+     *
+     *
      * @throws PlanException
      * @throws VisitorException
      * @throws IOException
@@ -151,7 +151,7 @@ public interface ExecutionEngine {
 
     /**
      * Returns the DataStorage the ExecutionEngine is using.
-     * 
+     *
      * @return DataStorage the ExecutionEngine is using.
      */
     public DataStorage getDataStorage();
@@ -161,35 +161,27 @@ public interface ExecutionEngine {
      * variable inside the ScriptState class. This method is called when first
      * initializing the ScriptState as to delegate to the ExecutionEngine the
      * version of ScriptState to use to manage the execution at hand.
-     * 
+     *
      * @return ScriptState object to manage execution of the script
      */
     public ScriptState instantiateScriptState();
 
     /**
      * Returns the ExecutableManager to be used in Pig Streaming.
-     * 
+     *
      * @return ExecutableManager to be used in Pig Streaming.
      */
     public ExecutableManager getExecutableManager();
 
-    /**
-     * This method is called whenever a kill signal has been triggered in the
-     * current process, indicating that the user wishes to kill and exit the
-     * current processing.. It is necessary to implement this method to ensure
-     * that we kill running jobs, and any other necessary cleanup.
-     */
-    public void kill() throws BackendException;
-
     /**
      * This method is called when a user requests to kill a job associated with
      * the given job id. If it is not possible for a user to kill a job, throw a
      * exception. It is imperative for the job id's being displayed to be unique
      * such that the correct jobs are being killed when the user supplies the
      * id.
-     * 
+     *
      * @throws BackendException
      */
     public void killJob(String jobID) throws BackendException;
 
-}
\ No newline at end of file
+}
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/HExecutionEngine.java b/src/org/apache/pig/backend/hadoop/executionengine/HExecutionEngine.java
index b886f9de7..e0cedd2ff 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/HExecutionEngine.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/HExecutionEngine.java
@@ -415,12 +415,6 @@ public abstract class HExecutionEngine implements ExecutionEngine {
         return new HadoopExecutableManager();
     }
 
-    public void kill() throws BackendException {
-        if (launcher != null) {
-            launcher.kill();
-        }
-    }
-
     public void killJob(String jobID) throws BackendException {
         if (launcher != null) {
             launcher.killJob(jobID, jobConf);
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/Launcher.java b/src/org/apache/pig/backend/hadoop/executionengine/Launcher.java
index c6faa072e..bb7ec9725 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/Launcher.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/Launcher.java
@@ -23,17 +23,17 @@ import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.HashSet;
 import java.util.List;
+import java.util.Map;
 import java.util.Set;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.JobID;
 import org.apache.hadoop.mapred.RunningJob;
-import org.apache.hadoop.mapred.TIPStatus;
 import org.apache.hadoop.mapred.TaskReport;
 import org.apache.hadoop.mapred.jobcontrol.Job;
 import org.apache.hadoop.mapred.jobcontrol.JobControl;
@@ -41,35 +41,58 @@ import org.apache.pig.FuncSpec;
 import org.apache.pig.PigException;
 import org.apache.pig.backend.BackendException;
 import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobCreationException;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
 import org.apache.pig.backend.hadoop.executionengine.shims.HadoopShims;
 import org.apache.pig.impl.PigContext;
+import org.apache.pig.impl.io.FileSpec;
 import org.apache.pig.impl.plan.PlanException;
 import org.apache.pig.impl.plan.VisitorException;
 import org.apache.pig.impl.util.LogUtils;
+import org.apache.pig.impl.util.Utils;
 import org.apache.pig.tools.pigstats.PigStats;
 
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
 
 /**
- * 
- * Provides core processing implementation for the backend of Pig 
+ *
+ * Provides core processing implementation for the backend of Pig
  * if ExecutionEngine chosen decides to delegate it's work to this class.
- * Also contains set of utility methods, including ones centered around 
+ * Also contains set of utility methods, including ones centered around
  * Hadoop.
- * 
+ *
  */
 public abstract class Launcher {
     private static final Log log = LogFactory.getLog(Launcher.class);
+    private static final String OOM_ERR = "OutOfMemoryError";
+    private boolean pigException = false;
+    private boolean outOfMemory = false;
+    private String newLine = "\n";
+
+    // Used to track the exception thrown by the job control which is run in a
+    // separate thread
+    protected String jobControlExceptionStackTrace = null;
+    protected Exception jobControlException = null;
+    protected long totalHadoopTimeSpent;
 
-    long totalHadoopTimeSpent;
-    String newLine = "\n";
-    boolean pigException = false;
-    boolean outOfMemory = false;
-    static final String OOM_ERR = "OutOfMemoryError";
+    protected Map<FileSpec, Exception> failureMap;
+    protected JobControl jc = null;
+
+    class HangingJobKiller extends Thread {
+        public HangingJobKiller() {}
+
+        @Override
+        public void run() {
+            try {
+                kill();
+            } catch (Exception e) {
+                log.warn("Error in killing Execution Engine: " + e);
+            }
+        }
+    }
 
     protected Launcher() {
-        totalHadoopTimeSpent = 0;
+        Runtime.getRuntime().addShutdownHook(new HangingJobKiller());
         // handle the windows portion of \r
         if (System.getProperty("os.name").toUpperCase().startsWith("WINDOWS")) {
             newLine = "\r\n";
@@ -81,7 +104,9 @@ public abstract class Launcher {
      * Resets the state after a launch
      */
     public void reset() {
-
+        failureMap = Maps.newHashMap();
+        totalHadoopTimeSpent = 0;
+        jc = null;
     }
 
     /**
@@ -100,22 +125,18 @@ public abstract class Launcher {
      * while respecting the dependency information. The parent thread monitors
      * the submitted jobs' progress and after it is complete, stops the
      * JobControl thread.
-     * 
+     *
      * @param php
      * @param grpName
      * @param pc
-     * @throws PlanException
-     * @throws VisitorException
-     * @throws IOException
-     * @throws ExecException
-     * @throws JobCreationException
+     * @throws Exception
      */
     public abstract PigStats launchPig(PhysicalPlan php, String grpName,
             PigContext pc) throws Exception;
 
     /**
      * Explain how a pig job will be executed on the underlying infrastructure.
-     * 
+     *
      * @param pp
      *            PhysicalPlan to explain
      * @param pc
@@ -135,56 +156,13 @@ public abstract class Launcher {
 
     public abstract void kill() throws BackendException;
 
-    public abstract void killJob(String jobID, JobConf jobConf)
+    public abstract void killJob(String jobID, Configuration conf)
             throws BackendException;
 
     protected boolean isComplete(double prog) {
         return (int) (Math.ceil(prog)) == 1;
     }
 
-    protected void getStats(Job job, JobClient jobClient, boolean errNotDbg,
-            PigContext pigContext) throws ExecException {
-        JobID MRJobID = job.getAssignedJobID();
-        String jobMessage = job.getMessage();
-        Exception backendException = null;
-        if (MRJobID == null) {
-            try {
-                LogUtils.writeLog(
-                        "Backend error message during job submission",
-                        jobMessage,
-                        pigContext.getProperties().getProperty("pig.logfile"),
-                        log);
-                backendException = getExceptionFromString(jobMessage);
-            } catch (Exception e) {
-                int errCode = 2997;
-                String msg = "Unable to recreate exception from backend error: "
-                        + jobMessage;
-                throw new ExecException(msg, errCode, PigException.BUG);
-            }
-            throw new ExecException(backendException);
-        }
-        try {
-            TaskReport[] mapRep = jobClient.getMapTaskReports(MRJobID);
-            getErrorMessages(mapRep, "map", errNotDbg, pigContext);
-            totalHadoopTimeSpent += computeTimeSpent(mapRep);
-            mapRep = null;
-            TaskReport[] redRep = jobClient.getReduceTaskReports(MRJobID);
-            getErrorMessages(redRep, "reduce", errNotDbg, pigContext);
-            totalHadoopTimeSpent += computeTimeSpent(redRep);
-            redRep = null;
-        } catch (IOException e) {
-            if (job.getState() == Job.SUCCESS) {
-                // if the job succeeded, let the user know that
-                // we were unable to get statistics
-                log.warn("Unable to get job related diagnostics");
-            } else {
-                throw new ExecException(e);
-            }
-        } catch (Exception e) {
-            throw new ExecException(e);
-        }
-    }
-
     protected long computeTimeSpent(TaskReport[] taskReports) {
         long timeSpent = 0;
         for (TaskReport r : taskReports) {
@@ -268,7 +246,7 @@ public abstract class Launcher {
     /**
      * Compute the progress of the current job submitted through the JobControl
      * object jc to the JobClient jobClient
-     * 
+     *
      * @param jc
      *            - The JobControl object that has been submitted
      * @param jobClient
@@ -293,7 +271,7 @@ public abstract class Launcher {
      * Returns the progress of a Job j which is part of a submitted JobControl
      * object. The progress is for this Job. So it has to be scaled down by the
      * num of jobs that are present in the JobControl.
-     * 
+     *
      * @param j
      *            - The Job for which progress is required
      * @param jobClient
@@ -321,7 +299,27 @@ public abstract class Launcher {
     }
 
     /**
-     * 
+     * An exception handler class to handle exceptions thrown by the job controller thread
+     * Its a local class. This is the only mechanism to catch unhandled thread exceptions
+     * Unhandled exceptions in threads are handled by the VM if the handler is not registered
+     * explicitly or if the default handler is null
+     */
+    public class JobControlThreadExceptionHandler implements Thread.UncaughtExceptionHandler {
+        @Override
+        public void uncaughtException(Thread thread, Throwable throwable) {
+            jobControlExceptionStackTrace = Utils.getStackStraceStr(throwable);
+            try {
+                jobControlException = getExceptionFromString(jobControlExceptionStackTrace);
+            } catch (Exception e) {
+                String errMsg = "Could not resolve error that occured when launching job: "
+                        + jobControlExceptionStackTrace;
+                jobControlException = new RuntimeException(errMsg, throwable);
+            }
+        }
+    }
+
+    /**
+     *
      * @param stackTraceLine
      *            The string representation of
      *            {@link Throwable#printStackTrace() printStackTrace} Handles
@@ -358,7 +356,7 @@ public abstract class Launcher {
     }
 
     /**
-     * 
+     *
      * @param stackTraceLine
      *            An array of strings that represent
      *            {@link Throwable#printStackTrace() printStackTrace} output,
@@ -436,7 +434,7 @@ public abstract class Launcher {
             // the exceptionName should not be null
             if (exceptionName != null) {
 
-                ArrayList<StackTraceElement> stackTraceElements = new ArrayList<StackTraceElement>();
+                ArrayList<StackTraceElement> stackTraceElements = Lists.newArrayList();
 
                 // Create stack trace elements for the remaining lines
                 String stackElementRegex = "\\s+at\\s+(\\w+(\\$\\w+)?\\.)+(\\<)?\\w+(\\>)?";
@@ -575,7 +573,7 @@ public abstract class Launcher {
     }
 
     /**
-     * 
+     *
      * @param line
      *            the string representation of a stack trace returned by
      *            {@link Throwable#printStackTrace() printStackTrace}
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java
index 570139de6..0c650bd4e 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java
@@ -17,7 +17,6 @@
  */
 package org.apache.pig.backend.hadoop.executionengine.mapReduceLayer;
 
-import java.io.ByteArrayOutputStream;
 import java.io.IOException;
 import java.io.PrintStream;
 import java.util.ArrayList;
@@ -40,7 +39,7 @@ import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.JobID;
 import org.apache.hadoop.mapred.RunningJob;
 import org.apache.hadoop.mapred.jobcontrol.Job;
-import org.apache.hadoop.mapred.jobcontrol.JobControl;
+import org.apache.hadoop.mapred.TaskReport;
 import org.apache.pig.ExecType;
 import org.apache.pig.PigConfiguration;
 import org.apache.pig.PigException;
@@ -72,6 +71,7 @@ import org.apache.pig.impl.plan.VisitorException;
 import org.apache.pig.impl.util.ConfigurationValidator;
 import org.apache.pig.impl.util.LogUtils;
 import org.apache.pig.impl.util.UDFContext;
+import org.apache.pig.impl.util.Utils;
 import org.apache.pig.tools.pigstats.OutputStats;
 import org.apache.pig.tools.pigstats.PigStats;
 import org.apache.pig.tools.pigstats.PigStatsUtil;
@@ -86,21 +86,14 @@ import org.apache.pig.tools.pigstats.mapreduce.MRScriptState;
 public class MapReduceLauncher extends Launcher{
 
     public static final String SUCCEEDED_FILE_NAME = "_SUCCESS";
-    
+
     public static final String SUCCESSFUL_JOB_OUTPUT_DIR_MARKER =
         "mapreduce.fileoutputcommitter.marksuccessfuljobs";
 
     private static final Log log = LogFactory.getLog(MapReduceLauncher.class);
- 
-    //used to track the exception thrown by the job control which is run in a separate thread
-    private Exception jobControlException = null;
-    private String jobControlExceptionStackTrace = null;
+
     private boolean aggregateWarning = false;
 
-    private Map<FileSpec, Exception> failureMap;
-    
-    private JobControl jc=null;
-    
     public void kill() {
         try {
             log.debug("Receive kill signal");
@@ -116,10 +109,11 @@ public class MapReduceLauncher extends Launcher{
             log.warn("Encounter exception on cleanup:" + e);
         }
     }
-    
-    public void killJob(String jobID, JobConf jobConf) throws BackendException {
+
+    public void killJob(String jobID, Configuration conf) throws BackendException {
         try {
-            if (jobConf != null) {
+            if (conf != null) {
+                JobConf jobConf = new JobConf(conf);
                 JobClient jc = new JobClient(jobConf);
                 JobID id = JobID.forName(jobID);
                 RunningJob job = jc.getJob(id);
@@ -130,12 +124,12 @@ public class MapReduceLauncher extends Launcher{
                     job.killJob();
                     log.info("Kill " + id + " submitted.");
                 }
-            }        
+            }
         } catch (IOException e) {
             throw new BackendException(e);
         }
     }
-    
+
     /**
      * Get the exception that caused a failure on the backend for a
      * store location (if any).
@@ -144,12 +138,6 @@ public class MapReduceLauncher extends Launcher{
         return failureMap.get(spec);
     }
 
-    @Override
-    public void reset() {  
-        failureMap = new HashMap<FileSpec, Exception>();
-        super.reset();
-    }
-   
     @SuppressWarnings("deprecation")
     @Override
     public PigStats launchPig(PhysicalPlan php,
@@ -163,25 +151,25 @@ public class MapReduceLauncher extends Launcher{
         long sleepTime = 500;
         aggregateWarning = "true".equalsIgnoreCase(pc.getProperties().getProperty("aggregate.warning"));
         MROperPlan mrp = compile(php, pc);
-                
+
         ConfigurationValidator.validatePigProperties(pc.getProperties());
         Configuration conf = ConfigurationUtil.toConfiguration(pc.getProperties());
-        
+
         MRExecutionEngine exe = (MRExecutionEngine) pc.getExecutionEngine();
         JobClient jobClient = new JobClient(exe.getJobConf());
-        
+
         JobControlCompiler jcc = new JobControlCompiler(pc, conf);
 
         MRScriptState.get().addWorkflowAdjacenciesToConf(mrp, conf);
 
         // start collecting statistics
-        MRPigStatsUtil.startCollection(pc, jobClient, jcc, mrp); 
-        
+        MRPigStatsUtil.startCollection(pc, jobClient, jcc, mrp);
+
         // Find all the intermediate data stores. The plan will be destroyed during compile/execution
         // so this needs to be done before.
         MRIntermediateDataVisitor intermediateVisitor = new MRIntermediateDataVisitor(mrp);
         intermediateVisitor.visit();
-        
+
         List<Job> failedJobs = new LinkedList<Job>();
         List<NativeMapReduceOper> failedNativeMR = new LinkedList<NativeMapReduceOper>();
         List<Job> completeFailedJobsInThisRun = new LinkedList<Job>();
@@ -190,21 +178,21 @@ public class MapReduceLauncher extends Launcher{
         int numMRJobsCompl = 0;
         double lastProg = -1;
         long scriptSubmittedTimestamp = System.currentTimeMillis();
-        
+
         //create the exception handler for the job control thread
         //and register the handler with the job control thread
         JobControlThreadExceptionHandler jctExceptionHandler = new JobControlThreadExceptionHandler();
-        
-        boolean stop_on_failure = 
+
+        boolean stop_on_failure =
             pc.getProperties().getProperty("stop.on.failure", "false").equals("true");
-        
+
         // jc is null only when mrp.size == 0
         while(mrp.size() != 0) {
             jc = jcc.compile(mrp, grpName);
             if(jc == null) {
                 List<MapReduceOper> roots = new LinkedList<MapReduceOper>();
                 roots.addAll(mrp.getRoots());
-                
+
                 // run the native mapreduce roots first then run the rest of the roots
                 for(MapReduceOper mro: roots) {
                     if(mro instanceof NativeMapReduceOper) {
@@ -214,28 +202,28 @@ public class MapReduceLauncher extends Launcher{
                             natOp.runJob();
                             numMRJobsCompl++;
                         } catch (IOException e) {
-                            
+
                             mrp.trimBelow(natOp);
                             failedNativeMR.add(natOp);
-                            
+
                             String msg = "Error running native mapreduce" +
                             " operator job :" + natOp.getJobId() + e.getMessage();
-                            
-                            String stackTrace = getStackStraceStr(e);
+
+                            String stackTrace = Utils.getStackStraceStr(e);
                             LogUtils.writeLog(msg,
                                     stackTrace,
                                     pc.getProperties().getProperty("pig.logfile"),
                                     log
-                            );     
+                            );
                             log.info(msg);
-                            
+
                             if (stop_on_failure) {
                                 int errCode = 6017;
-                               
+
                                 throw new ExecException(msg, errCode,
                                         PigException.REMOTE_ENVIRONMENT);
                             }
-                            
+
                         }
                         double prog = ((double)numMRJobsCompl)/totalMRJobs;
                         notifyProgress(prog, lastProg);
@@ -245,33 +233,33 @@ public class MapReduceLauncher extends Launcher{
                 }
                 continue;
             }
-        	// Initially, all jobs are in wait state.
+            // Initially, all jobs are in wait state.
             List<Job> jobsWithoutIds = jc.getWaitingJobs();
             log.info(jobsWithoutIds.size() +" map-reduce job(s) waiting for submission.");
             //notify listeners about jobs submitted
             MRScriptState.get().emitJobsSubmittedNotification(jobsWithoutIds.size());
-            
+
             // update Pig stats' job DAG with just compiled jobs
             MRPigStatsUtil.updateJobMroMap(jcc.getJobMroMap());
-            
-            // determine job tracker url 
+
+            // determine job tracker url
             String jobTrackerLoc;
             JobConf jobConf = jobsWithoutIds.get(0).getJobConf();
             try {
                 String port = jobConf.get("mapred.job.tracker.http.address");
                 String jobTrackerAdd = jobConf.get(MRExecutionEngine.JOB_TRACKER_LOCATION);
-                
-                jobTrackerLoc = jobTrackerAdd.substring(0,jobTrackerAdd.indexOf(":")) 
+
+                jobTrackerLoc = jobTrackerAdd.substring(0,jobTrackerAdd.indexOf(":"))
                 + port.substring(port.indexOf(":"));
             }
             catch(Exception e){
                 // Could not get the job tracker location, most probably we are running in local mode.
                 // If it is the case, we don't print out job tracker location,
                 // because it is meaningless for local mode.
-            	jobTrackerLoc = null;
+                jobTrackerLoc = null;
                 log.debug("Failed to get job tracker location.");
             }
-            
+
             completeFailedJobsInThisRun.clear();
 
             // Set the thread UDFContext so registered classes are available.
@@ -285,7 +273,7 @@ public class MapReduceLauncher extends Launcher{
             };
 
             jcThread.setUncaughtExceptionHandler(jctExceptionHandler);
-            
+
             jcThread.setContextClassLoader(PigContext.getClassLoader());
 
             // mark the times that the jobs were submitted so it's reflected in job history props
@@ -300,25 +288,25 @@ public class MapReduceLauncher extends Launcher{
 
             //All the setup done, now lets launch the jobs.
             jcThread.start();
-            
+
             try {
                 // a flag whether to warn failure during the loop below, so users can notice failure earlier.
                 boolean warn_failure = true;
-                
+
                 // Now wait, till we are finished.
                 while(!jc.allFinished()){
-    
-                  try { jcThread.join(sleepTime); }
-                	catch (InterruptedException e) {}
-    
-                	List<Job> jobsAssignedIdInThisRun = new ArrayList<Job>();
-    
-                	for(Job job : jobsWithoutIds){
-                		if (job.getAssignedJobID() != null){
-    
-                			jobsAssignedIdInThisRun.add(job);
-                			log.info("HadoopJobId: "+job.getAssignedJobID());
-                			
+
+                    try { jcThread.join(sleepTime); }
+                    catch (InterruptedException e) {}
+
+                    List<Job> jobsAssignedIdInThisRun = new ArrayList<Job>();
+
+                    for(Job job : jobsWithoutIds){
+                        if (job.getAssignedJobID() != null){
+
+                            jobsAssignedIdInThisRun.add(job);
+                            log.info("HadoopJobId: "+job.getAssignedJobID());
+
                             // display the aliases being processed
                             MapReduceOper mro = jcc.getJobMroMap().get(job);
                             if (mro != null) {
@@ -327,32 +315,32 @@ public class MapReduceLauncher extends Launcher{
                                 String aliasLocation = MRScriptState.get().getAliasLocation(mro);
                                 log.info("detailed locations: " + aliasLocation);
                             }
-    
-                            
-                			if(jobTrackerLoc != null){
-                				log.info("More information at: http://"+ jobTrackerLoc+
-                						"/jobdetails.jsp?jobid="+job.getAssignedJobID());
-                			}  
-    
+
+
+                            if(jobTrackerLoc != null){
+                                log.info("More information at: http://"+ jobTrackerLoc+
+                                        "/jobdetails.jsp?jobid="+job.getAssignedJobID());
+                            }
+
                             // update statistics for this job so jobId is set
                             MRPigStatsUtil.addJobStats(job);
                             MRScriptState.get().emitJobStartedNotification(
-                                    job.getAssignedJobID().toString());                        
-                		}
-                		else{
-                			// This job is not assigned an id yet.
-                		}
-                	}
-                	jobsWithoutIds.removeAll(jobsAssignedIdInThisRun);
-    
-                	double prog = (numMRJobsCompl+calculateProgress(jc, jobClient))/totalMRJobs;
-                	if (notifyProgress(prog, lastProg)) {
+                                    job.getAssignedJobID().toString());
+                        }
+                        else{
+                            // This job is not assigned an id yet.
+                        }
+                    }
+                    jobsWithoutIds.removeAll(jobsAssignedIdInThisRun);
+
+                    double prog = (numMRJobsCompl+calculateProgress(jc, jobClient))/totalMRJobs;
+                    if (notifyProgress(prog, lastProg)) {
                         lastProg = prog;
                     }
-    
-                	// collect job stats by frequently polling of completed jobs (PIG-1829)
+
+                    // collect job stats by frequently polling of completed jobs (PIG-1829)
                     MRPigStatsUtil.accumulateStats(jc);
-                	
+
                     // if stop_on_failure is enabled, we need to stop immediately when any job has failed
                     checkStopOnFailure(stop_on_failure);
                     // otherwise, we just display a warning message if there's any failure
@@ -363,7 +351,7 @@ public class MapReduceLauncher extends Launcher{
                                 + "want Pig to stop immediately on failure.");
                     }
                 }
-                
+
                 //check for the jobControlException first
                 //if the job controller fails before launching the jobs then there are
                 //no jobs to check for failure
@@ -383,46 +371,46 @@ public class MapReduceLauncher extends Launcher{
                                 jobControlException);
                     }
                 }
-                
+
                 if (!jc.getFailedJobs().isEmpty() ) {
                     // stop if stop_on_failure is enabled
                     checkStopOnFailure(stop_on_failure);
-                    
-                    // If we only have one store and that job fail, then we sure 
+
+                    // If we only have one store and that job fail, then we sure
                     // that the job completely fail, and we shall stop dependent jobs
                     for (Job job : jc.getFailedJobs()) {
                         completeFailedJobsInThisRun.add(job);
-                        log.info("job " + job.getAssignedJobID() + " has failed! Stop running all dependent jobs"); 
+                        log.info("job " + job.getAssignedJobID() + " has failed! Stop running all dependent jobs");
                     }
                     failedJobs.addAll(jc.getFailedJobs());
                 }
-                
+
                 int removedMROp = jcc.updateMROpPlan(completeFailedJobsInThisRun);
-                
+
                 numMRJobsCompl += removedMROp;
-    
+
                 List<Job> jobs = jc.getSuccessfulJobs();
                 jcc.moveResults(jobs);
                 succJobs.addAll(jobs);
-                            
+
                 // collecting final statistics
                 MRPigStatsUtil.accumulateStats(jc);
 
-        	}
-        	catch (Exception e) {
-        		throw e;
-        	}
-        	finally {
-            jc.stop();
-        	}
+            }
+            catch (Exception e) {
+                throw e;
+            }
+            finally {
+                jc.stop();
+            }
         }
 
         MRScriptState.get().emitProgressUpdatedNotification(100);
-        
+
         log.info( "100% complete");
-             
+
         boolean failed = false;
-        
+
         if(failedNativeMR.size() > 0){
             failed = true;
         }
@@ -434,9 +422,9 @@ public class MapReduceLauncher extends Launcher{
 
         // Look to see if any jobs failed.  If so, we need to report that.
         if (failedJobs != null && failedJobs.size() > 0) {
-            
+
             Exception backendException = null;
-            for (Job fj : failedJobs) {                
+            for (Job fj : failedJobs) {
                 try {
                     getStats(fj, jobClient, true, pc);
                 } catch (Exception e) {
@@ -450,34 +438,34 @@ public class MapReduceLauncher extends Launcher{
             }
             failed = true;
         }
-        
+
         // stats collection is done, log the results
-        MRPigStatsUtil.stopCollection(true); 
-        
+        MRPigStatsUtil.stopCollection(true);
+
         // PigStatsUtil.stopCollection also computes the return code based on
         // total jobs to run, jobs successful and jobs failed
         failed = failed || !PigStats.get().isSuccessful();
-        
+
         Map<Enum, Long> warningAggMap = new HashMap<Enum, Long>();
-                
+
         if (succJobs != null) {
             for (Job job : succJobs) {
-                List<POStore> sts = jcc.getStores(job);                
+                List<POStore> sts = jcc.getStores(job);
                 for (POStore st : sts) {
                     if (pc.getExecType() == ExecType.LOCAL) {
                         HadoopShims.storeSchemaForLocal(job, st);
                     }
 
-                    if (!st.isTmpStore()) {                       
-                        // create an "_SUCCESS" file in output location if 
+                    if (!st.isTmpStore()) {
+                        // create an "_SUCCESS" file in output location if
                         // output location is a filesystem dir
-                        createSuccessFile(job, st);                   
+                        createSuccessFile(job, st);
                     } else {
                         log.debug("Successfully stored result in: \""
                                 + st.getSFile().getFileName() + "\"");
                     }
                 }
-                                
+
                 getStats(job, jobClient, false, pc);
                 if (aggregateWarning) {
                     computeWarningAggregate(job, jobClient, warningAggMap);
@@ -485,11 +473,11 @@ public class MapReduceLauncher extends Launcher{
             }
 
         }
-        
+
         if(aggregateWarning) {
             CompilationMessageCollector.logAggregate(warningAggMap, MessageType.Warning, log) ;
         }
-                                                    
+
         if (!failed) {
             log.info("Success!");
         } else {
@@ -501,33 +489,33 @@ public class MapReduceLauncher extends Launcher{
         }
         jcc.reset();
 
-        int ret = failed ? ((succJobs != null && succJobs.size() > 0) 
+        int ret = failed ? ((succJobs != null && succJobs.size() > 0)
                 ? ReturnCode.PARTIAL_FAILURE
                 : ReturnCode.FAILURE)
-                : ReturnCode.SUCCESS; 
-        
+                : ReturnCode.SUCCESS;
+
         PigStats pigStats = PigStatsUtil.getPigStats(ret);
         // run cleanup for all of the stores
-             for (OutputStats output : pigStats.getOutputStats()) {
-                 POStore store = output.getPOStore();
-                 try {
-                     if (!output.isSuccessful()) {
-                         store.getStoreFunc().cleanupOnFailure(
-                                 store.getSFile().getFileName(),
-                                 new org.apache.hadoop.mapreduce.Job(output.getConf()));
-                     } else {
-                         store.getStoreFunc().cleanupOnSuccess(
-                                 store.getSFile().getFileName(),
-                                 new org.apache.hadoop.mapreduce.Job(output.getConf()));
-                     }
-                 } catch (IOException e) {
-                     throw new ExecException(e);
-                 } catch (AbstractMethodError nsme) {
-                     // Just swallow it.  This means we're running against an
-                     // older instance of a StoreFunc that doesn't implement
-                     // this method.
-                 }
-             }
+        for (OutputStats output : pigStats.getOutputStats()) {
+            POStore store = output.getPOStore();
+            try {
+                if (!output.isSuccessful()) {
+                    store.getStoreFunc().cleanupOnFailure(
+                            store.getSFile().getFileName(),
+                            new org.apache.hadoop.mapreduce.Job(output.getConf()));
+                } else {
+                    store.getStoreFunc().cleanupOnSuccess(
+                            store.getSFile().getFileName(),
+                            new org.apache.hadoop.mapreduce.Job(output.getConf()));
+                }
+            } catch (IOException e) {
+                throw new ExecException(e);
+            } catch (AbstractMethodError nsme) {
+                // Just swallow it.  This means we're running against an
+                // older instance of a StoreFunc that doesn't implement
+                // this method.
+            }
+        }
         return pigStats;
     }
 
@@ -537,13 +525,13 @@ public class MapReduceLauncher extends Launcher{
      * @throws ExecException If stop_on_failure is enabled and any job is failed
      */
     private void checkStopOnFailure(boolean stop_on_failure) throws ExecException{
-    	if (jc.getFailedJobs().isEmpty())
+        if (jc.getFailedJobs().isEmpty())
             return;
-    	
-    	if (stop_on_failure){
+
+        if (stop_on_failure){
             int errCode = 6017;
             StringBuilder msg = new StringBuilder();
-            
+
             for (int i=0; i<jc.getFailedJobs().size(); i++) {
                 Job j = jc.getFailedJobs().get(i);
                 msg.append(j.getMessage());
@@ -551,21 +539,14 @@ public class MapReduceLauncher extends Launcher{
                     msg.append("\n");
                 }
             }
-            
+
             throw new ExecException(msg.toString(), errCode,
                     PigException.REMOTE_ENVIRONMENT);
         }
     }
-    
-    private String getStackStraceStr(Throwable e) {
-        ByteArrayOutputStream baos = new ByteArrayOutputStream();
-        PrintStream ps = new PrintStream(baos);
-        e.printStackTrace(ps);
-        return baos.toString();
-    }
 
     /**
-     * Log the progress and notify listeners if there is sufficient progress 
+     * Log the progress and notify listeners if there is sufficient progress
      * @param prog current progress
      * @param lastProg progress last time
      */
@@ -610,7 +591,7 @@ public class MapReduceLauncher extends Launcher{
             ps.println("#--------------------------------------------------");
             ps.println("# Map Reduce Plan                                  ");
             ps.println("#--------------------------------------------------");
-            
+
             DotMRPrinter printer =new DotMRPrinter(mrp, ps);
             printer.setVerbose(verbose);
             printer.dump();
@@ -626,29 +607,29 @@ public class MapReduceLauncher extends Launcher{
         comp.compile();
         comp.aggregateScalarsFiles();
         MROperPlan plan = comp.getMRPlan();
-        
+
         //display the warning message(s) from the MRCompiler
         comp.getMessageCollector().logMessages(MessageType.Warning, aggregateWarning, log);
-        
-        String lastInputChunkSize = 
+
+        String lastInputChunkSize =
             pc.getProperties().getProperty(
                     "last.input.chunksize", POJoinPackage.DEFAULT_CHUNK_SIZE);
-        
+
         String prop = pc.getProperties().getProperty(PigConfiguration.PROP_NO_COMBINER);
         if (!pc.inIllustrator && !("true".equals(prop)))  {
-            boolean doMapAgg = 
+            boolean doMapAgg =
                     Boolean.valueOf(pc.getProperties().getProperty(PigConfiguration.PROP_EXEC_MAP_PARTAGG,"false"));
             CombinerOptimizer co = new CombinerOptimizer(plan, doMapAgg);
             co.visit();
             //display the warning message(s) from the CombinerOptimizer
             co.getMessageCollector().logMessages(MessageType.Warning, aggregateWarning, log);
         }
-        
+
         // Optimize the jobs that have a load/store only first MR job followed
         // by a sample job.
         SampleOptimizer so = new SampleOptimizer(plan, pc);
         so.visit();
-        
+
         // We must ensure that there is only 1 reducer for a limit. Add a single-reducer job.
         if (!pc.inIllustrator) {
         LimitAdjuster la = new LimitAdjuster(plan, pc);
@@ -661,16 +642,16 @@ public class MapReduceLauncher extends Launcher{
             SecondaryKeyOptimizer skOptimizer = new SecondaryKeyOptimizer(plan);
             skOptimizer.visit();
         }
-        
+
         // optimize key - value handling in package
         POPackageAnnotator pkgAnnotator = new POPackageAnnotator(plan);
         pkgAnnotator.visit();
-        
+
         // optimize joins
-        LastInputStreamingOptimizer liso = 
+        LastInputStreamingOptimizer liso =
             new MRCompiler.LastInputStreamingOptimizer(plan, lastInputChunkSize);
         liso.visit();
-        
+
         // figure out the type of the key for the map plan
         // this is needed when the key is null to create
         // an appropriate NullableXXXWritable object
@@ -681,30 +662,30 @@ public class MapReduceLauncher extends Launcher{
         // splits.
         NoopFilterRemover fRem = new NoopFilterRemover(plan);
         fRem.visit();
-        
-        boolean isMultiQuery = 
+
+        boolean isMultiQuery =
             "true".equalsIgnoreCase(pc.getProperties().getProperty("opt.multiquery","true"));
-        
+
         if (isMultiQuery) {
-            // reduces the number of MROpers in the MR plan generated 
+            // reduces the number of MROpers in the MR plan generated
             // by multi-query (multi-store) script.
             MultiQueryOptimizer mqOptimizer = new MultiQueryOptimizer(plan, pc.inIllustrator);
             mqOptimizer.visit();
         }
-        
+
         // removes unnecessary stores (as can happen with splits in
         // some cases.). This has to run after the MultiQuery and
         // NoopFilterRemover.
         NoopStoreRemover sRem = new NoopStoreRemover(plan);
         sRem.visit();
-      
+
         // check whether stream operator is present
         // after MultiQueryOptimizer because it can shift streams from
         // map to reduce, etc.
         EndOfAllInputSetter checker = new EndOfAllInputSetter(plan);
         checker.visit();
-        
-        boolean isAccum = 
+
+        boolean isAccum =
             "true".equalsIgnoreCase(pc.getProperties().getProperty("opt.accumulator","true"));
         if (isAccum) {
             AccumulatorOptimizer accum = new AccumulatorOptimizer(plan);
@@ -714,12 +695,12 @@ public class MapReduceLauncher extends Launcher{
     }
 
     private boolean shouldMarkOutputDir(Job job) {
-        return job.getJobConf().getBoolean(SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, 
+        return job.getJobConf().getBoolean(SUCCESSFUL_JOB_OUTPUT_DIR_MARKER,
                                false);
     }
-    
+
     private void createSuccessFile(Job job, POStore store) throws IOException {
-        if(shouldMarkOutputDir(job)) {            
+        if(shouldMarkOutputDir(job)) {
             Path outputPath = new Path(store.getSFile().getFileName());
             FileSystem fs = outputPath.getFileSystem(job.getJobConf());
             if(fs.exists(outputPath)){
@@ -728,31 +709,10 @@ public class MapReduceLauncher extends Launcher{
                 if(!fs.exists(filePath)) {
                     fs.create(filePath).close();
                 }
-            }    
-        } 
-    }
-    
-    /**
-     * An exception handler class to handle exceptions thrown by the job controller thread
-     * Its a local class. This is the only mechanism to catch unhandled thread exceptions
-     * Unhandled exceptions in threads are handled by the VM if the handler is not registered
-     * explicitly or if the default handler is null
-     */
-    class JobControlThreadExceptionHandler implements Thread.UncaughtExceptionHandler {
-        
-        @Override
-        public void uncaughtException(Thread thread, Throwable throwable) {
-            jobControlExceptionStackTrace = getStackStraceStr(throwable);
-            try {	
-                jobControlException = getExceptionFromString(jobControlExceptionStackTrace);
-            } catch (Exception e) {
-                String errMsg = "Could not resolve error that occured when launching map reduce job: "
-                        + jobControlExceptionStackTrace;
-                jobControlException = new RuntimeException(errMsg, throwable);
             }
         }
     }
-    
+
     @SuppressWarnings("deprecation")
     void computeWarningAggregate(Job job, JobClient jobClient, Map<Enum, Long> aggMap) {
         JobID mapRedJobID = job.getAssignedJobID();
@@ -791,7 +751,50 @@ public class MapReduceLauncher extends Launcher{
         } catch (IOException ioe) {
             String msg = "Unable to retrieve job to compute warning aggregation.";
             log.warn(msg);
-        }    	
+        }
     }
 
+    private void getStats(Job job, JobClient jobClient, boolean errNotDbg,
+            PigContext pigContext) throws ExecException {
+        JobID MRJobID = job.getAssignedJobID();
+        String jobMessage = job.getMessage();
+        Exception backendException = null;
+        if (MRJobID == null) {
+            try {
+                LogUtils.writeLog(
+                        "Backend error message during job submission",
+                        jobMessage,
+                        pigContext.getProperties().getProperty("pig.logfile"),
+                        log);
+                backendException = getExceptionFromString(jobMessage);
+            } catch (Exception e) {
+                int errCode = 2997;
+                String msg = "Unable to recreate exception from backend error: "
+                        + jobMessage;
+                throw new ExecException(msg, errCode, PigException.BUG);
+            }
+            throw new ExecException(backendException);
+        }
+        try {
+            TaskReport[] mapRep = jobClient.getMapTaskReports(MRJobID);
+            getErrorMessages(mapRep, "map", errNotDbg, pigContext);
+            totalHadoopTimeSpent += computeTimeSpent(mapRep);
+            mapRep = null;
+            TaskReport[] redRep = jobClient.getReduceTaskReports(MRJobID);
+            getErrorMessages(redRep, "reduce", errNotDbg, pigContext);
+            totalHadoopTimeSpent += computeTimeSpent(redRep);
+            redRep = null;
+        } catch (IOException e) {
+            if (job.getState() == Job.SUCCESS) {
+                // if the job succeeded, let the user know that
+                // we were unable to get statistics
+                log.warn("Unable to get job related diagnostics");
+            } else {
+                throw new ExecException(e);
+            }
+        } catch (Exception e) {
+            throw new ExecException(e);
+        }
+    }
 }
+
diff --git a/src/org/apache/pig/impl/PigContext.java b/src/org/apache/pig/impl/PigContext.java
index 1b6ac61c3..bed624b57 100644
--- a/src/org/apache/pig/impl/PigContext.java
+++ b/src/org/apache/pig/impl/PigContext.java
@@ -299,24 +299,9 @@ public class PigContext implements Serializable {
     }
 
     public void connect() throws ExecException {
-                executionEngine.init();
-                dfs = executionEngine.getDataStorage();
-                lfs = new HDataStorage(URI.create("file:///"), properties);
-                Runtime.getRuntime().addShutdownHook(new ExecutionEngineKiller());
-
-    }
-
-    class ExecutionEngineKiller extends Thread {
-        public ExecutionEngineKiller() {}
-                
-                @Override
-        public void run() {
-            try {
-                executionEngine.kill();
-            } catch (Exception e) {
-                log.warn("Error in killing Execution Engine: " + e);
-            }
-        }
+        executionEngine.init();
+        dfs = executionEngine.getDataStorage();
+        lfs = new HDataStorage(URI.create("file:///"), properties);
     }
 
     public void setJobtrackerLocation(String newLocation) {
diff --git a/src/org/apache/pig/impl/util/Utils.java b/src/org/apache/pig/impl/util/Utils.java
index ca29eca32..333a5eaf9 100644
--- a/src/org/apache/pig/impl/util/Utils.java
+++ b/src/org/apache/pig/impl/util/Utils.java
@@ -17,11 +17,13 @@
  */
 package org.apache.pig.impl.util;
 
+import java.io.ByteArrayOutputStream;
 import java.io.File;
 import java.io.FileInputStream;
 import java.io.FileNotFoundException;
 import java.io.IOException;
 import java.io.InputStream;
+import java.io.PrintStream;
 import java.io.SequenceInputStream;
 import java.net.Socket;
 import java.net.SocketException;
@@ -543,4 +545,11 @@ public class Utils {
         }
     }
 
+    public static String getStackStraceStr(Throwable e) {
+        ByteArrayOutputStream baos = new ByteArrayOutputStream();
+        PrintStream ps = new PrintStream(baos);
+        e.printStackTrace(ps);
+        return baos.toString();
+    }
+
 }
