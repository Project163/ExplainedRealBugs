diff --git a/CHANGES.txt b/CHANGES.txt
index 22e83a6c2..d4bc2521e 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -105,6 +105,8 @@ OPTIMIZATIONS
  
 BUG FIXES
 
+PIG-2495: Using merge JOIN from a HBaseStorage produces an error (bridiver via daijy)
+
 PIG-4182: e2e tests Scripting_[1-12] fail on Windows (daijy)
 
 PIG-4235: Fix unit test failures on Windows (daijy)
diff --git a/src/org/apache/pig/backend/hadoop/hbase/HBaseStorage.java b/src/org/apache/pig/backend/hadoop/hbase/HBaseStorage.java
index c31ca1606..b78d23587 100644
--- a/src/org/apache/pig/backend/hadoop/hbase/HBaseStorage.java
+++ b/src/org/apache/pig/backend/hadoop/hbase/HBaseStorage.java
@@ -76,6 +76,7 @@ import org.apache.hadoop.mapreduce.OutputFormat;
 import org.apache.hadoop.mapreduce.RecordReader;
 import org.apache.hadoop.mapreduce.RecordWriter;
 import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.pig.CollectableLoadFunc;
 import org.apache.pig.LoadCaster;
 import org.apache.pig.LoadFunc;
 import org.apache.pig.LoadPushDown;
@@ -139,7 +140,8 @@ import com.google.common.collect.Lists;
  * <code>buddies</code> column family in the <code>SampleTableCopy</code> table.
  *
  */
-public class HBaseStorage extends LoadFunc implements StoreFuncInterface, LoadPushDown, OrderedLoadFunc, StoreResources {
+public class HBaseStorage extends LoadFunc implements StoreFuncInterface, LoadPushDown, OrderedLoadFunc, StoreResources,
+        CollectableLoadFunc {
 
     private static final Log LOG = LogFactory.getLog(HBaseStorage.class);
 
@@ -1130,28 +1132,24 @@ public class HBaseStorage extends LoadFunc implements StoreFuncInterface, LoadPu
         return new RequiredFieldResponse(true);
     }
 
-    @Override
-    public WritableComparable<InputSplit> getSplitComparable(InputSplit split)
-            throws IOException {
-        return new WritableComparable<InputSplit>() {
-            TableSplit tsplit = new TableSplit();
-
-            @Override
-            public void readFields(DataInput in) throws IOException {
-                tsplit.readFields(in);
-            }
-
-            @Override
-            public void write(DataOutput out) throws IOException {
-                tsplit.write(out);
-            }
+    public void ensureAllKeyInstancesInSameSplit() throws IOException {
+        /** 
+         * no-op because hbase keys are unique 
+         * This will also work with things like DelimitedKeyPrefixRegionSplitPolicy
+         * if you need a partial key match to be included in the split
+         */
+        LOG.debug("ensureAllKeyInstancesInSameSplit");
+    }
 
-            @Override
-            public int compareTo(InputSplit split) {
-                return tsplit.compareTo((TableSplit) split);
-            }
-        };
+    @Override
+    public WritableComparable<TableSplit> getSplitComparable(InputSplit split) throws IOException {
+        if (split instanceof TableSplit) {
+            return new TableSplitComparable((TableSplit) split);
+        } else {
+            throw new RuntimeException("LoadFunc expected split of type TableSplit but was " + split.getClass().getName());
+        }
     }
+ 
 
     /**
      * Class to encapsulate logic around which column names were specified in each
diff --git a/test/org/apache/pig/test/TestHBaseStorage.java b/test/org/apache/pig/test/TestHBaseStorage.java
index c1b025b25..8532f3b9a 100644
--- a/test/org/apache/pig/test/TestHBaseStorage.java
+++ b/test/org/apache/pig/test/TestHBaseStorage.java
@@ -42,6 +42,7 @@ import org.apache.pig.PigServer;
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.backend.hadoop.hbase.HBaseStorage;
+import org.apache.pig.data.DataBag;
 import org.apache.pig.data.DataByteArray;
 import org.apache.pig.data.DataType;
 import org.apache.pig.data.Tuple;
@@ -824,6 +825,114 @@ public class TestHBaseStorage {
         Assert.assertEquals(100, index);
         LOG.info("testLoadWithProjection_2 done");
     }
+    
+    /**
+     * Test merge inner join with two tables
+     *
+     * @throws IOException
+     */
+    @Test
+    public void testMergeJoin() throws IOException {
+        prepareTable(TESTTABLE_1, true, DataFormat.HBaseBinary);
+        prepareTable(TESTTABLE_2, true, DataFormat.HBaseBinary);
+        pig.registerQuery("a = load 'hbase://" + TESTTABLE_1 + "' using "
+                        + "org.apache.pig.backend.hadoop.hbase.HBaseStorage('"
+                        + TESTCOLUMN_A + " " + TESTCOLUMN_B + " " + TESTCOLUMN_C
+                        + "','-loadKey -caster HBaseBinaryConverter') as (rowKey:chararray,col_a:int, col_b:double, col_c:chararray);");
+        pig.registerQuery("b = load 'hbase://" + TESTTABLE_2 + "' using "
+                        + "org.apache.pig.backend.hadoop.hbase.HBaseStorage('"
+                        + TESTCOLUMN_A + " " + TESTCOLUMN_B + " " + TESTCOLUMN_C
+                        + "','-loadKey -caster HBaseBinaryConverter') as (rowKey:chararray,col_a:int, col_b:double, col_c:chararray);");
+        pig.registerQuery("c = join a by rowKey, b by rowKey USING 'merge';");
+        pig.registerQuery("d = ORDER c BY a::rowKey;");
+
+        Iterator<Tuple> it = pig.openIterator("d");
+        int count = 0;
+        LOG.info("MergeJoin Starting");
+        while (it.hasNext()) {
+            Tuple t = it.next();
+            // the columns for both relations should be merged into one tuple
+            // left side
+            String rowKey = (String) t.get(0);            
+            int col_a = (Integer) t.get(1);
+            double col_b = (Double) t.get(2);
+            String col_c = (String) t.get(3);
+
+            Assert.assertEquals("00".substring((count + "").length()) + count,
+                    rowKey);
+            Assert.assertEquals(count, col_a);
+            Assert.assertEquals(count + 0.0, col_b, 1e-6);
+            Assert.assertEquals("Text_" + count, col_c);
+
+            // right side
+            String rowKey2 = (String) t.get(4);
+            int col_a2 = (Integer) t.get(5);
+            double col_b2 = (Double) t.get(6);
+            String col_c2 = (String) t.get(7);
+
+            Assert.assertEquals("00".substring((count + "").length()) + count,
+                    rowKey2);
+            Assert.assertEquals(count, col_a2);
+            Assert.assertEquals(count + 0.0, col_b2, 1e-6);
+            Assert.assertEquals("Text_" + count, col_c2);
+            
+            count++;
+        }
+        Assert.assertEquals(count, TEST_ROW_COUNT);
+        LOG.info("MergeJoin done");
+    }
+
+    /**
+     * Test collected group 
+     * not much to test here since keys are unique
+     * 
+     * @throws IOException
+     */
+    @Test
+    public void testCollectedGroup() throws IOException {
+        prepareTable(TESTTABLE_1, true, DataFormat.HBaseBinary);
+        prepareTable(TESTTABLE_2, true, DataFormat.HBaseBinary);
+        pig.registerQuery("a = load 'hbase://" + TESTTABLE_1 + "' using "
+                        + "org.apache.pig.backend.hadoop.hbase.HBaseStorage('"
+                        + TESTCOLUMN_A + " " + TESTCOLUMN_B + " " + TESTCOLUMN_C
+                        + "','-loadKey -caster HBaseBinaryConverter') as (rowKey:chararray,col_a:int, col_b:double, col_c:chararray);");
+        pig.registerQuery("c = group a by rowKey USING 'collected';");
+        pig.registerQuery("d = ORDER c BY group;");
+
+        // do a merge group
+        Iterator<Tuple> it = pig.openIterator("d");
+        int count = 0;
+        LOG.info("CollectedGroup Starting");
+        while (it.hasNext()) {
+            Tuple t = it.next();
+
+            String rowKey = (String)t.get(0);
+
+            Assert.assertEquals("00".substring((count + "").length()) + count,
+                    rowKey);
+
+            int rowCount = 0;
+            DataBag rows = (DataBag)t.get(1);
+            for (Iterator<Tuple> iter = rows.iterator(); iter.hasNext();) {
+                Tuple row = iter.next();
+
+                // there should be two bags with all 3 columns
+                int col_a = (Integer) row.get(1);
+                double col_b = (Double) row.get(2);
+                String col_c = (String) row.get(3);
+                
+                Assert.assertEquals(count, col_a);
+                Assert.assertEquals(count + 0.0, col_b, 1e-6);
+                Assert.assertEquals("Text_" + count, col_c);
+                rowCount++;
+            }
+            Assert.assertEquals(1, rowCount);
+
+            count++;
+        }
+        Assert.assertEquals(TEST_ROW_COUNT, count);
+        LOG.info("CollectedGroup done");
+    }
 
     /**
      * Test Load from hbase using HBaseBinaryConverter
