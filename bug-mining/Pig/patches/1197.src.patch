diff --git a/CHANGES.txt b/CHANGES.txt
index f9c152da4..c403493b6 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -170,7 +170,9 @@ PIG-3013: BinInterSedes improve chararray sort performance (rohini)
 
 BUG FIXES
 
-PIG-3305: Infinite loop when input path contains empty partition directory (daijy)
+PIG-3097: HiveColumnarLoader doesn't correctly load partitioned Hive table (maczech via daijy)
+
+PIG-3305: Infinite loop when input path contains empty partition directory (maczech via daijy)
 
 PIG-3286: TestPigContext.testImportList fails in trunk (cheolsoo)
 
diff --git a/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/HiveColumnarLoader.java b/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/HiveColumnarLoader.java
index efc50ff1c..f5b855bf9 100644
--- a/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/HiveColumnarLoader.java
+++ b/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/HiveColumnarLoader.java
@@ -5,9 +5,9 @@
  * licenses this file to you under the Apache License, Version 2.0 (the
  * "License"); you may not use this file except in compliance with the License.
  * You may obtain a copy of the License at
- * 
+ *
  * http://www.apache.org/licenses/LICENSE-2.0
- * 
+ *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
  * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
@@ -38,7 +38,6 @@ import org.apache.hadoop.hive.serde2.columnar.BytesRefArrayWritable;
 import org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe;
 import org.apache.hadoop.hive.serde2.columnar.ColumnarStruct;
 import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
 import org.apache.hadoop.mapreduce.InputFormat;
 import org.apache.hadoop.mapreduce.Job;
 import org.apache.hadoop.mapreduce.RecordReader;
@@ -109,19 +108,19 @@ import org.apache.pig.piggybank.storage.partition.PathPartitionHelper;
  * <td>MAP</td>
  * </tr>
  * </table>
- * 
+ *
  * <p/>
  * <b>Partitions</b><br/>
  * The input paths are scanned by the loader for [partition name]=[value]
  * patterns in the subdirectories.<br/>
  * If detected these partitions are appended to the table schema.<br/>
  * For example if you have the directory structure:<br/>
- * 
+ *
  * <pre>
  * /user/hive/warehouse/mytable
  * 				/year=2010/month=02/day=01
  * </pre>
- * 
+ *
  * The mytable schema is (id int,name string).<br/>
  * The final schema returned in pig will be (id:int, name:chararray,
  * year:chararray, month:chararray, day:chararray).<br/>
@@ -135,7 +134,7 @@ import org.apache.pig.piggybank.storage.partition.PathPartitionHelper;
  * a = LOAD 'file' USING HiveColumnarLoader("uid bigint, ts long, arr array<string,string>, m map<string,string>");
  * -- to reference the fields
  * b = FOREACH GENERATE a.uid, a.ts, a.arr, a.m;
- * </pre> 
+ * </pre>
  * </code>
  * <p/>
  * Usage 2:
@@ -150,7 +149,7 @@ import org.apache.pig.piggybank.storage.partition.PathPartitionHelper;
  * a = LOAD 'file' USING HiveColumnarLoader("uid bigint, ts long, arr array<string,string>, m map<string,string>", "2009-10-01:2009-10-02");
  * -- to reference the fields
  * b = FOREACH GENERATE a.uid, a.ts, a.arr, a.m;
- * </pre> 
+ * </pre>
  * </code> <br/>
  * <b>New Usage</b/><br/>
  * <code>
@@ -175,7 +174,7 @@ import org.apache.pig.piggybank.storage.partition.PathPartitionHelper;
  * f = FILTER a BY daydate>='2009-10-01' AND daydate >='2009-10-02';
  * -- to reference the fields
  * b = FOREACH a GENERATE uid, ts, arr, m;
- * </pre> 
+ * </pre>
  * </code>
  * <p/>
  * <b>Issues</b>
@@ -193,12 +192,12 @@ import org.apache.pig.piggybank.storage.partition.PathPartitionHelper;
  * a table<br/>
  * For example:<br/>
  * The following is not valid:<br/>
- * 
+ *
  * <pre>
  *     mytable/hour=00
  *     mytable/day=01/hour=00
  * </pre>
- * 
+ *
  **/
 public class HiveColumnarLoader extends FileInputLoadFunc implements
 	LoadMetadata, LoadPushDown {
@@ -256,7 +255,7 @@ public class HiveColumnarLoader extends FileInputLoadFunc implements
      * For example uid BIGINT, pid long, means 1 column of uid type BIGINT and
      * one column of pid type LONG.<br/>
      * The types are not case sensitive.
-     * 
+     *
      * @param table_schema
      *            This property cannot be null
      */
@@ -266,13 +265,13 @@ public class HiveColumnarLoader extends FileInputLoadFunc implements
 
     /**
      * This constructor is for backward compatibility.
-     * 
+     *
      * Table schema should be a space and comma separated string describing the
      * Hive schema.<br/>
      * For example uid BIGINT, pid long, means 1 column of uid type BIGINT and
      * one column of pid type LONG.<br/>
      * The types are not case sensitive.
-     * 
+     *
      * @param table_schema
      *            This property cannot be null
      * @param dateRange
@@ -289,13 +288,13 @@ public class HiveColumnarLoader extends FileInputLoadFunc implements
 
     /**
      * This constructor is for backward compatibility.
-     * 
+     *
      * Table schema should be a space and comma separated string describing the
      * Hive schema.<br/>
      * For example uid BIGINT, pid long, means 1 column of uid type BIGINT and
      * one column of pid type LONG.<br/>
      * The types are not case sensitive.
-     * 
+     *
      * @param table_schema
      *            This property cannot be null
      * @param dateRange
@@ -359,7 +358,7 @@ public class HiveColumnarLoader extends FileInputLoadFunc implements
 	    // if any the partition keys should already exist
 	    String[] partitionKeys = getPartitionKeys(null, null);
 	    if (partitionKeys != null) {
-		fieldLen = partitionKeys.length;
+		fieldLen += partitionKeys.length;
 	    }
 
 	    requiredIndexes = new int[fieldLen];
@@ -388,7 +387,7 @@ public class HiveColumnarLoader extends FileInputLoadFunc implements
 
     /**
      * Does the configuration setup and schema parsing and setup.
-     * 
+     *
      * @param table_schema
      *            String
      * @param columnsToRead
@@ -434,7 +433,7 @@ public class HiveColumnarLoader extends FileInputLoadFunc implements
     /**
      * Uses the ColumnarSerde to deserialize the buff:BytesRefArrayWritable into
      * a ColumnarStruct instance.
-     * 
+     *
      * @param buff
      *            BytesRefArrayWritable
      * @return ColumnarStruct
@@ -454,7 +453,7 @@ public class HiveColumnarLoader extends FileInputLoadFunc implements
 
     /**
      * Only read the columns that were requested in the constructor.<br/>
-     * 
+     *
      * @param struct
      *            ColumnarStruct
      * @param path
@@ -520,7 +519,7 @@ public class HiveColumnarLoader extends FileInputLoadFunc implements
      * Will parse the required columns from the UDFContext properties if the
      * requiredColumns[] variable is null, or else just return the
      * requiredColumns.
-     * 
+     *
      * @return int[]
      */
     private int[] getRequiredColumns() {
@@ -549,7 +548,7 @@ public class HiveColumnarLoader extends FileInputLoadFunc implements
 
     /**
      * Reads the partition columns
-     * 
+     *
      * @param location
      * @param job
      * @return
@@ -724,14 +723,14 @@ public class HiveColumnarLoader extends FileInputLoadFunc implements
 
 	LOG.debug("Signature: " + signature);
 	this.signature = signature;
-	
+
 	// this provides backwards compatibility
 	// the HiveRCInputFormat will read this and if set will perform the
 	// needed partitionFiltering
 	if (dateRange != null) {
 	    getUDFContext().setProperty(DATE_RANGE, dateRange);
 	}
-	
+
     }
 
 }
diff --git a/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestHiveColumnarLoader.java b/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestHiveColumnarLoader.java
index c7a66827c..828293bed 100644
--- a/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestHiveColumnarLoader.java
+++ b/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestHiveColumnarLoader.java
@@ -5,9 +5,9 @@
  * licenses this file to you under the Apache License, Version 2.0 (the
  * "License"); you may not use this file except in compliance with the License.
  * You may obtain a copy of the License at
- * 
+ *
  * http://www.apache.org/licenses/LICENSE-2.0
- * 
+ *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
  * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
@@ -49,7 +49,7 @@ import org.junit.Assert;
 import org.junit.Test;
 
 /**
- * 
+ *
  * Tests that the HiveColumnLoader can:
  * <ul>
  * <li>Load files without partitioning</li>
@@ -57,7 +57,7 @@ import org.junit.Test;
  * filter</li>
  * <li>Load files using pig's push down loader capabilities.</li>
  * </ul>
- * 
+ *
  */
 public class TestHiveColumnarLoader extends TestCase {
 
@@ -95,418 +95,418 @@ public class TestHiveColumnarLoader extends TestCase {
     @Override
     public synchronized void setUp() throws Exception {
 
-	conf = new Configuration();
+        conf = new Configuration();
 
-	fs = LocalFileSystem.getLocal(conf);
+        fs = LocalFileSystem.getLocal(conf);
 
-	produceSimpleData();
+        produceSimpleData();
 
-	produceDatePartitionedData();
+        produceDatePartitionedData();
 
-	produceYearMonthDayHourPartitionedData();
+        produceYearMonthDayHourPartitionedData();
 
     }
 
     @Override
     public void tearDown() {
 
-	Util.deleteDirectory(datePartitionedDir);
+        Util.deleteDirectory(datePartitionedDir);
 
-	Util.deleteDirectory(yearMonthDayHourPartitionedDir);
+        Util.deleteDirectory(yearMonthDayHourPartitionedDir);
 
-	Util.deleteDirectory(simpleDataDir);
+        Util.deleteDirectory(simpleDataDir);
 
-	simpleDataFile.delete();
+        simpleDataFile.delete();
 
     }
 
     @Test
     public void testReadingSingleFileNoProjections() throws IOException {
-	String funcSpecString = "org.apache.pig.piggybank.storage.HiveColumnarLoader('f1 string,f2 string,f3 string')";
+        String funcSpecString = "org.apache.pig.piggybank.storage.HiveColumnarLoader('f1 string,f2 string,f3 string')";
 
-	String singlePartitionedFile = simpleDataFile.getAbsolutePath();
+        String singlePartitionedFile = simpleDataFile.getAbsolutePath();
 
-	PigServer server = new PigServer(ExecType.LOCAL);
-	server.setBatchOn();
-	server.registerFunction(
-		"org.apache.pig.piggybank.storage.HiveColumnarLoader",
-		new FuncSpec(funcSpecString));
+        PigServer server = new PigServer(ExecType.LOCAL);
+        server.setBatchOn();
+        server.registerFunction("org.apache.pig.piggybank.storage.HiveColumnarLoader",
+                new FuncSpec(funcSpecString));
 
-	server.registerQuery("a = LOAD '" + singlePartitionedFile + "' using "
-		+ funcSpecString + ";");
+        server.registerQuery("a = LOAD '" + singlePartitionedFile + "' using " + funcSpecString
+                + ";");
 
-	Iterator<Tuple> result = server.openIterator("a");
+        Iterator<Tuple> result = server.openIterator("a");
 
-	int count = 0;
-	Tuple t = null;
-	while ((t = result.next()) != null) {
-	    assertEquals(3, t.size());
-	    assertEquals(DataType.CHARARRAY, t.getType(0));
-	    count++;
-	}
+        int count = 0;
+        Tuple t = null;
+        while ((t = result.next()) != null) {
+            assertEquals(3, t.size());
+            assertEquals(DataType.CHARARRAY, t.getType(0));
+            count++;
+        }
 
-	Assert.assertEquals(simpleRowCount, count);
+        Assert.assertEquals(simpleRowCount, count);
     }
-    
+
     @Test
     public void testReadingMultipleNonPartitionedFiles() throws IOException {
-	String funcSpecString = "org.apache.pig.piggybank.storage.HiveColumnarLoader('f1 string,f2 string,f3 string')";
+        String funcSpecString = "org.apache.pig.piggybank.storage.HiveColumnarLoader('f1 string,f2 string,f3 string')";
 
-	String singlePartitionedDir = simpleDataDir.getAbsolutePath();
+        String singlePartitionedDir = simpleDataDir.getAbsolutePath();
 
-	PigServer server = new PigServer(ExecType.LOCAL);
-	server.setBatchOn();
-	server.registerFunction(
-		"org.apache.pig.piggybank.storage.HiveColumnarLoader",
-		new FuncSpec(funcSpecString));
+        PigServer server = new PigServer(ExecType.LOCAL);
+        server.setBatchOn();
+        server.registerFunction("org.apache.pig.piggybank.storage.HiveColumnarLoader",
+                new FuncSpec(funcSpecString));
 
-	server.registerQuery("a = LOAD '" + singlePartitionedDir + "' using "
-		+ funcSpecString + ";");
+        server.registerQuery("a = LOAD '" + singlePartitionedDir + "' using " + funcSpecString
+                + ";");
 
-	server.registerQuery("b = foreach a generate f1;");
+        server.registerQuery("b = foreach a generate f1;");
 
-	Iterator<Tuple> result = server.openIterator("b");
+        Iterator<Tuple> result = server.openIterator("b");
 
-	int count = 0;
-	Tuple t = null;
-	while ((t = result.next()) != null) {
-	    assertEquals(1, t.size());
-	    assertEquals(DataType.CHARARRAY, t.getType(0));
-	    count++;
-	}
+        int count = 0;
+        Tuple t = null;
+        while ((t = result.next()) != null) {
+            assertEquals(1, t.size());
+            assertEquals(DataType.CHARARRAY, t.getType(0));
+            count++;
+        }
 
-	Assert.assertEquals(simpleDirFileCount * simpleRowCount, count);
+        Assert.assertEquals(simpleDirFileCount * simpleRowCount, count);
     }
 
     @Test
     public void testReadingSingleFile() throws IOException {
-	String funcSpecString = "org.apache.pig.piggybank.storage.HiveColumnarLoader('f1 string,f2 string,f3 string')";
+        String funcSpecString = "org.apache.pig.piggybank.storage.HiveColumnarLoader('f1 string,f2 string,f3 string')";
 
-	String singlePartitionedFile = simpleDataFile.getAbsolutePath();
+        String singlePartitionedFile = simpleDataFile.getAbsolutePath();
 
-	PigServer server = new PigServer(ExecType.LOCAL);
-	server.setBatchOn();
-	server.registerFunction(
-		"org.apache.pig.piggybank.storage.HiveColumnarLoader",
-		new FuncSpec(funcSpecString));
+        PigServer server = new PigServer(ExecType.LOCAL);
+        server.setBatchOn();
+        server.registerFunction("org.apache.pig.piggybank.storage.HiveColumnarLoader",
+                new FuncSpec(funcSpecString));
 
-	server.registerQuery("a = LOAD '" + singlePartitionedFile + "' using "
-		+ funcSpecString + ";");
+        server.registerQuery("a = LOAD '" + singlePartitionedFile + "' using " + funcSpecString
+                + ";");
 
-	server.registerQuery("b = foreach a generate f1;");
+        server.registerQuery("b = foreach a generate f1;");
 
-	Iterator<Tuple> result = server.openIterator("b");
+        Iterator<Tuple> result = server.openIterator("b");
 
-	int count = 0;
-	Tuple t = null;
-	while ((t = result.next()) != null) {
-	    assertEquals(1, t.size());
-	    assertEquals(DataType.CHARARRAY, t.getType(0));
-	    count++;
-	}
+        int count = 0;
+        Tuple t = null;
+        while ((t = result.next()) != null) {
+            assertEquals(1, t.size());
+            assertEquals(DataType.CHARARRAY, t.getType(0));
+            count++;
+        }
 
-	Assert.assertEquals(simpleRowCount, count);
+        Assert.assertEquals(simpleRowCount, count);
     }
 
     @Test
-    public void testYearMonthDayHourPartitionedFilesWithProjection()
-	    throws IOException {
-	int count = 0;
+    public void testYearMonthDayHourPartitionedFilesWithProjection() throws IOException {
+        int count = 0;
 
-	String funcSpecString = "org.apache.pig.piggybank.storage.HiveColumnarLoader('f1 string,f2 string,f3 string')";
+        String funcSpecString = "org.apache.pig.piggybank.storage.HiveColumnarLoader('f1 string,f2 string,f3 string')";
 
-	PigServer server = new PigServer(ExecType.LOCAL);
-	server.setBatchOn();
-	server.registerFunction(
-		"org.apache.pig.piggybank.storage.HiveColumnarLoader",
-		new FuncSpec(funcSpecString));
+        PigServer server = new PigServer(ExecType.LOCAL);
+        server.setBatchOn();
+        server.registerFunction("org.apache.pig.piggybank.storage.HiveColumnarLoader",
+                new FuncSpec(funcSpecString));
 
-	server.registerQuery("a = LOAD '"
-		+ yearMonthDayHourPartitionedDir.getAbsolutePath() + "' using "
-		+ funcSpecString + ";");
-	server.registerQuery("f = FILTER a by year=='2010';");
-	server.registerQuery("b = foreach f generate f1;");
+        server.registerQuery("a = LOAD '" + yearMonthDayHourPartitionedDir.getAbsolutePath()
+                + "' using " + funcSpecString + ";");
+        server.registerQuery("f = FILTER a by year=='2010';");
+        server.registerQuery("b = foreach f generate f1,f2;");
 
-	Iterator<Tuple> result = server.openIterator("b");
+        Iterator<Tuple> result = server.openIterator("b");
 
-	Tuple t = null;
-	while ((t = result.next()) != null) {
-	    assertEquals(1, t.size());
-	    assertEquals(DataType.CHARARRAY, t.getType(0));
-	    count++;
-	}
+        Tuple t = null;
+        while ((t = result.next()) != null) {
+            assertEquals(2, t.size());
+            assertEquals(DataType.CHARARRAY, t.getType(0));
+            count++;
+        }
 
-	Assert.assertEquals(240, count);
+        Assert.assertEquals(240, count);
 
     }
 
     @Test
     public void testYearMonthDayHourPartitionedFilesWithProjectionAndPartitionColumns()
-	    throws IOException {
-	int count = 0;
-
-	String funcSpecString = "org.apache.pig.piggybank.storage.HiveColumnarLoader('f1 string,f2 string,f3 string')";
-
-	PigServer server = new PigServer(ExecType.LOCAL);
-	server.setBatchOn();
-	server.registerFunction(
-		"org.apache.pig.piggybank.storage.HiveColumnarLoader",
-		new FuncSpec(funcSpecString));
-
-	server.registerQuery("a = LOAD '"
-		+ yearMonthDayHourPartitionedDir.getAbsolutePath() + "' using "
-		+ funcSpecString + ";");
-	server.registerQuery("f = FILTER a by year=='2010';");
-	server.registerQuery("r = foreach f generate year, f2, f3, month, day, hour;");
-	server.registerQuery("b = ORDER r BY year, month, day, hour;");
-	Iterator<Tuple> result = server.openIterator("b");
-
-	Tuple t = null;
-	while ((t = result.next()) != null) {
-	    System.out.println("Tuple: " + t);
-	    assertEquals(6, t.size());
-	    count++;
-	}
-	System.out.println("Count: " + count);
-	Assert.assertEquals(240, count);
+            throws IOException {
+        int count = 0;
+
+        String funcSpecString = "org.apache.pig.piggybank.storage.HiveColumnarLoader('f1 string,f2 string,f3 string')";
+
+        PigServer server = new PigServer(ExecType.LOCAL);
+        server.setBatchOn();
+        server.registerFunction("org.apache.pig.piggybank.storage.HiveColumnarLoader",
+                new FuncSpec(funcSpecString));
+
+        server.registerQuery("a = LOAD '" + yearMonthDayHourPartitionedDir.getAbsolutePath()
+                + "' using " + funcSpecString + ";");
+        server.registerQuery("f = FILTER a by year=='2010';");
+        server.registerQuery("r = foreach f generate year, f2, f3, month, day, hour;");
+        server.registerQuery("b = ORDER r BY year, month, day, hour;");
+        Iterator<Tuple> result = server.openIterator("b");
+
+        Tuple t = null;
+        while ((t = result.next()) != null) {
+            System.out.println("Tuple: " + t);
+            assertEquals(6, t.size());
+            count++;
+        }
+        System.out.println("Count: " + count);
+        Assert.assertEquals(240, count);
     }
 
     @Test
     public void test1DayDatePartitionedFilesWithProjection() throws IOException {
-	int count = 0;
+        int count = 0;
 
-	String funcSpecString = "org.apache.pig.piggybank.storage.HiveColumnarLoader('f1 string,f2 string,f3 string'"
-		+ ", '" + startingDate + ":" + startingDate + "')";
+        String funcSpecString = "org.apache.pig.piggybank.storage.HiveColumnarLoader('f1 string,f2 string,f3 string'"
+                + ", '" + startingDate + ":" + startingDate + "')";
 
-	System.out.println(funcSpecString);
+        System.out.println(funcSpecString);
 
-	PigServer server = new PigServer(ExecType.LOCAL);
-	server.setBatchOn();
-	server.registerFunction(
-		"org.apache.pig.piggybank.storage.HiveColumnarLoader",
-		new FuncSpec(funcSpecString));
+        PigServer server = new PigServer(ExecType.LOCAL);
+        server.setBatchOn();
+        server.registerFunction("org.apache.pig.piggybank.storage.HiveColumnarLoader",
+                new FuncSpec(funcSpecString));
 
-	server.registerQuery("a = LOAD '"
-		+ datePartitionedDir.getAbsolutePath() + "' using "
-		+ funcSpecString + ";");
-	server.registerQuery("b = FOREACH a GENERATE f2 as p;");
-	Iterator<Tuple> result = server.openIterator("b");
+        server.registerQuery("a = LOAD '" + datePartitionedDir.getAbsolutePath() + "' using "
+                + funcSpecString + ";");
+        server.registerQuery("b = FOREACH a GENERATE f2 as p;");
+        Iterator<Tuple> result = server.openIterator("b");
 
-	Tuple t = null;
-	while ((t = result.next()) != null) {
-	    assertEquals(1, t.size());
-	    assertEquals(DataType.CHARARRAY, t.getType(0));
-	    count++;
-	}
+        Tuple t = null;
+        while ((t = result.next()) != null) {
+            assertEquals(1, t.size());
+            assertEquals(DataType.CHARARRAY, t.getType(0));
+            count++;
+        }
 
-	Assert.assertEquals(50, count);
+        Assert.assertEquals(50, count);
     }
 
     @Test
     public void test1DayDatePartitionedFiles() throws IOException {
-	int count = 0;
+        int count = 0;
 
-	String funcSpecString = "org.apache.pig.piggybank.storage.HiveColumnarLoader('f1 string,f2 string,f3 string'"
-		+ ", '" + startingDate + ":" + startingDate + "')";
+        String funcSpecString = "org.apache.pig.piggybank.storage.HiveColumnarLoader('f1 string,f2 string,f3 string'"
+                + ", '" + startingDate + ":" + startingDate + "')";
 
-	System.out.println(funcSpecString);
+        System.out.println(funcSpecString);
 
-	PigServer server = new PigServer(ExecType.LOCAL);
-	server.setBatchOn();
-	server.registerFunction(
-		"org.apache.pig.piggybank.storage.HiveColumnarLoader",
-		new FuncSpec(funcSpecString));
+        PigServer server = new PigServer(ExecType.LOCAL);
+        server.setBatchOn();
+        server.registerFunction("org.apache.pig.piggybank.storage.HiveColumnarLoader",
+                new FuncSpec(funcSpecString));
 
-	server.registerQuery("a = LOAD '"
-		+ datePartitionedDir.getAbsolutePath() + "' using "
-		+ funcSpecString + ";");
-	Iterator<Tuple> result = server.openIterator("a");
+        server.registerQuery("a = LOAD '" + datePartitionedDir.getAbsolutePath() + "' using "
+                + funcSpecString + ";");
+        Iterator<Tuple> result = server.openIterator("a");
 
-	while ((result.next()) != null) {
-	    count++;
-	}
+        while ((result.next()) != null) {
+            count++;
+        }
 
-	Assert.assertEquals(50, count);
+        Assert.assertEquals(50, count);
     }
 
     @Test
     public void testDatePartitionedFiles() throws IOException {
-	int count = 0;
+        int count = 0;
+
+        String funcSpecString = "org.apache.pig.piggybank.storage.HiveColumnarLoader('f1 string,f2 string,f3 string'"
+                + ", '" + startingDate + ":" + endingDate + "')";
+
+        System.out.println(funcSpecString);
+
+        PigServer server = new PigServer(ExecType.LOCAL);
+        server.setBatchOn();
+        server.registerFunction("org.apache.pig.piggybank.storage.HiveColumnarLoader",
+                new FuncSpec(funcSpecString));
 
-	String funcSpecString = "org.apache.pig.piggybank.storage.HiveColumnarLoader('f1 string,f2 string,f3 string'"
-		+ ", '" + startingDate + ":" + endingDate + "')";
+        server.registerQuery("a = LOAD '" + datePartitionedDir.getAbsolutePath() + "' using "
+                + funcSpecString + ";");
+        Iterator<Tuple> result = server.openIterator("a");
+
+        while ((result.next()) != null) {
+            count++;
+        }
+
+        Assert.assertEquals(datePartitionedRowCount, count);
+    }
+    @Test
+    public void testNumerOfColumnsWhenDatePartitionedFiles() throws IOException {
+        int count = 0;
 
-	System.out.println(funcSpecString);
+        String funcSpecString = "org.apache.pig.piggybank.storage.HiveColumnarLoader('f1 string,f2 string,f3 string'"
+                + ", '" + startingDate + ":" + endingDate + "')";
 
-	PigServer server = new PigServer(ExecType.LOCAL);
-	server.setBatchOn();
-	server.registerFunction(
-		"org.apache.pig.piggybank.storage.HiveColumnarLoader",
-		new FuncSpec(funcSpecString));
+        System.out.println(funcSpecString);
 
-	server.registerQuery("a = LOAD '"
-		+ datePartitionedDir.getAbsolutePath() + "' using "
-		+ funcSpecString + ";");
-	Iterator<Tuple> result = server.openIterator("a");
+        PigServer server = new PigServer(ExecType.LOCAL);
+        server.setBatchOn();
+        server.registerFunction("org.apache.pig.piggybank.storage.HiveColumnarLoader",
+                new FuncSpec(funcSpecString));
 
-	while ((result.next()) != null) {
-	    count++;
-	}
+        server.registerQuery("a = LOAD '" + datePartitionedDir.getAbsolutePath() + "' using "
+                + funcSpecString + ";");
+        Iterator<Tuple> result = server.openIterator("a");
+        Tuple t = null;
+        while ((t = result.next()) != null) {
+            Assert.assertEquals(4, t.size());
+            count++;
+        }
 
-	Assert.assertEquals(datePartitionedRowCount, count);
+        Assert.assertEquals(datePartitionedRowCount, count);
     }
 
     private static void produceDatePartitionedData() throws IOException {
-	datePartitionedRowCount = 0;
-	datePartitionedDir = new File("testhiveColumnarLoader-dateDir-"
-		+ System.currentTimeMillis());
-	datePartitionedDir.mkdir();
-	datePartitionedDir.deleteOnExit();
+        datePartitionedRowCount = 0;
+        datePartitionedDir = new File("testhiveColumnarLoader-dateDir-"
+                + System.currentTimeMillis());
+        datePartitionedDir.mkdir();
+        datePartitionedDir.deleteOnExit();
 
-	int dates = 4;
-	calendar = Calendar.getInstance();
+        int dates = 4;
+        calendar = Calendar.getInstance();
 
-	calendar.set(Calendar.DAY_OF_MONTH, Calendar.MONDAY);
-	calendar.set(Calendar.MONTH, Calendar.JANUARY);
+        calendar.set(Calendar.DAY_OF_MONTH, Calendar.MONDAY);
+        calendar.set(Calendar.MONTH, Calendar.JANUARY);
 
-	startingDate = dateFormat.format(calendar.getTime());
+        startingDate = dateFormat.format(calendar.getTime());
 
-	datePartitionedRCFiles = new ArrayList<String>();
-	datePartitionedDirs = new ArrayList<String>();
+        datePartitionedRCFiles = new ArrayList<String>();
+        datePartitionedDirs = new ArrayList<String>();
 
-	for (int i = 0; i < dates; i++) {
+        for (int i = 0; i < dates; i++) {
 
-	    File file = new File(datePartitionedDir, "daydate="
-		    + dateFormat.format(calendar.getTime()));
-	    calendar.add(Calendar.DAY_OF_MONTH, 1);
+            File file = new File(datePartitionedDir, "daydate="
+                    + dateFormat.format(calendar.getTime()));
+            calendar.add(Calendar.DAY_OF_MONTH, 1);
 
-	    file.mkdir();
-	    file.deleteOnExit();
+            file.mkdir();
+            file.deleteOnExit();
 
-	    // for each daydate write 5 partitions
-	    for (int pi = 0; pi < 5; pi++) {
-		Path path = new Path(new Path(file.getAbsolutePath()),
-			"parition" + pi);
+            // for each daydate write 5 partitions
+            for (int pi = 0; pi < 5; pi++) {
+                Path path = new Path(new Path(file.getAbsolutePath()), "parition" + pi);
 
-		datePartitionedRowCount += writeRCFileTest(fs, simpleRowCount,
-			path, columnCount, new DefaultCodec(), columnCount);
+                datePartitionedRowCount += writeRCFileTest(fs, simpleRowCount, path, columnCount,
+                        new DefaultCodec(), columnCount);
 
-		new File(path.toString()).deleteOnExit();
-		datePartitionedRCFiles.add(path.toString());
-		datePartitionedDirs.add(file.toString());
+                new File(path.toString()).deleteOnExit();
+                datePartitionedRCFiles.add(path.toString());
+                datePartitionedDirs.add(file.toString());
 
-	    }
+            }
 
-	}
+        }
 
-	endingDate = dateFormat.format(calendar.getTime());
+        endingDate = dateFormat.format(calendar.getTime());
     }
 
-    private static void produceYearMonthDayHourPartitionedData()
-	    throws IOException {
+    private static void produceYearMonthDayHourPartitionedData() throws IOException {
 
-	yearMonthDayHourPartitionedDir = new File(
-		"testhiveColumnarLoader-yearMonthDayHourDir-"
-			+ System.currentTimeMillis());
-	yearMonthDayHourPartitionedDir.mkdir();
-	yearMonthDayHourPartitionedDir.deleteOnExit();
+        yearMonthDayHourPartitionedDir = new File("testhiveColumnarLoader-yearMonthDayHourDir-"
+                + System.currentTimeMillis());
+        yearMonthDayHourPartitionedDir.mkdir();
+        yearMonthDayHourPartitionedDir.deleteOnExit();
 
-	int years = 1;
-	int months = 2;
-	int days = 3;
-	int hours = 4;
+        int years = 1;
+        int months = 2;
+        int days = 3;
+        int hours = 4;
 
-	yearMonthDayHourcalendar = Calendar.getInstance();
+        yearMonthDayHourcalendar = Calendar.getInstance();
 
-	yearMonthDayHourcalendar.set(Calendar.YEAR, 2010);
-	yearMonthDayHourcalendar.set(Calendar.DAY_OF_MONTH, Calendar.MONDAY);
-	yearMonthDayHourcalendar.set(Calendar.MONTH, Calendar.JANUARY);
+        yearMonthDayHourcalendar.set(Calendar.YEAR, 2010);
+        yearMonthDayHourcalendar.set(Calendar.DAY_OF_MONTH, Calendar.MONDAY);
+        yearMonthDayHourcalendar.set(Calendar.MONTH, Calendar.JANUARY);
 
-	for (int i = 0; i < years; i++) {
+        for (int i = 0; i < years; i++) {
 
-	    File file = new File(yearMonthDayHourPartitionedDir, "year="
-		    + yearMonthDayHourcalendar.get(Calendar.YEAR));
+            File file = new File(yearMonthDayHourPartitionedDir, "year="
+                    + yearMonthDayHourcalendar.get(Calendar.YEAR));
 
-	    file.mkdir();
-	    file.deleteOnExit();
+            file.mkdir();
+            file.deleteOnExit();
 
-	    for (int monthIndex = 0; monthIndex < months; monthIndex++) {
+            for (int monthIndex = 0; monthIndex < months; monthIndex++) {
 
-		File monthFile = new File(file, "month="
-			+ yearMonthDayHourcalendar.get(Calendar.MONTH));
-		monthFile.mkdir();
-		monthFile.deleteOnExit();
+                File monthFile = new File(file, "month="
+                        + yearMonthDayHourcalendar.get(Calendar.MONTH));
+                monthFile.mkdir();
+                monthFile.deleteOnExit();
 
-		for (int dayIndex = 0; dayIndex < days; dayIndex++) {
-		    File dayFile = new File(monthFile,
-			    "day="
-				    + yearMonthDayHourcalendar
-					    .get(Calendar.DAY_OF_MONTH));
-		    dayFile.mkdir();
-		    dayFile.deleteOnExit();
+                for (int dayIndex = 0; dayIndex < days; dayIndex++) {
+                    File dayFile = new File(monthFile, "day="
+                            + yearMonthDayHourcalendar.get(Calendar.DAY_OF_MONTH));
+                    dayFile.mkdir();
+                    dayFile.deleteOnExit();
 
-		    for (int hourIndex = 0; hourIndex < hours; hourIndex++) {
-			File hourFile = new File(dayFile,
-				"hour="
-					+ yearMonthDayHourcalendar
-						.get(Calendar.HOUR_OF_DAY));
-			hourFile.mkdir();
-			hourFile.deleteOnExit();
+                    for (int hourIndex = 0; hourIndex < hours; hourIndex++) {
+                        File hourFile = new File(dayFile, "hour="
+                                + yearMonthDayHourcalendar.get(Calendar.HOUR_OF_DAY));
+                        hourFile.mkdir();
+                        hourFile.deleteOnExit();
 
-			File rcFile = new File(hourFile.getAbsolutePath()
-				+ "/attempt-00000");
-			Path hourFilePath = new Path(rcFile.getAbsolutePath());
-			rcFile.deleteOnExit();
+                        File rcFile = new File(hourFile.getAbsolutePath() + "/attempt-00000");
+                        Path hourFilePath = new Path(rcFile.getAbsolutePath());
+                        rcFile.deleteOnExit();
 
-			writeRCFileTest(fs, simpleRowCount, hourFilePath,
-				columnCount, new DefaultCodec(), columnCount);
+                        writeRCFileTest(fs, simpleRowCount, hourFilePath, columnCount,
+                                new DefaultCodec(), columnCount);
 
-			yearMonthDayHourcalendar.add(Calendar.HOUR_OF_DAY, 1);
-		    }
+                        yearMonthDayHourcalendar.add(Calendar.HOUR_OF_DAY, 1);
+                    }
 
-		    yearMonthDayHourcalendar.add(Calendar.DAY_OF_MONTH, 1);
-		}
-		yearMonthDayHourcalendar.add(Calendar.MONTH, 1);
-	    }
+                    yearMonthDayHourcalendar.add(Calendar.DAY_OF_MONTH, 1);
+                }
+                yearMonthDayHourcalendar.add(Calendar.MONTH, 1);
+            }
 
-	}
+        }
 
-	endingDate = dateFormat.format(calendar.getTime());
+        endingDate = dateFormat.format(calendar.getTime());
     }
 
     /**
      * Writes out a simple temporary file with 5 columns and 100 rows.<br/>
      * Data is random numbers.
-     * 
+     *
      * @throws SerDeException
      * @throws IOException
      */
-    private static final void produceSimpleData() throws SerDeException,
-	    IOException {
-	// produce on single file
-	simpleDataFile = File.createTempFile("testhiveColumnarLoader", ".txt");
-	simpleDataFile.deleteOnExit();
+    private static final void produceSimpleData() throws SerDeException, IOException {
+        // produce on single file
+        simpleDataFile = File.createTempFile("testhiveColumnarLoader", ".txt");
+        simpleDataFile.deleteOnExit();
 
-	Path path = new Path(simpleDataFile.getPath());
+        Path path = new Path(simpleDataFile.getPath());
 
-	writeRCFileTest(fs, simpleRowCount, path, columnCount,
-		new DefaultCodec(), columnCount);
+        writeRCFileTest(fs, simpleRowCount, path, columnCount, new DefaultCodec(), columnCount);
 
-	// produce a folder of simple data
-	simpleDataDir = new File("simpleDataDir" + System.currentTimeMillis());
-	simpleDataDir.mkdir();
+        // produce a folder of simple data
+        simpleDataDir = new File("simpleDataDir" + System.currentTimeMillis());
+        simpleDataDir.mkdir();
 
-	for (int i = 0; i < simpleDirFileCount; i++) {
+        for (int i = 0; i < simpleDirFileCount; i++) {
 
-	    simpleDataFile = new File(simpleDataDir, "testhiveColumnarLoader-"
-		    + i + ".txt");
+            simpleDataFile = new File(simpleDataDir, "testhiveColumnarLoader-" + i + ".txt");
 
-	    Path filePath = new Path(simpleDataFile.getPath());
+            Path filePath = new Path(simpleDataFile.getPath());
 
-	    writeRCFileTest(fs, simpleRowCount, filePath, columnCount,
-		    new DefaultCodec(), columnCount);
+            writeRCFileTest(fs, simpleRowCount, filePath, columnCount, new DefaultCodec(),
+                    columnCount);
 
-	}
+        }
 
     }
 
@@ -515,62 +515,60 @@ public class TestHiveColumnarLoader extends TestCase {
     static Random randColLenGenerator = new Random(20);
 
     private static void resetRandomGenerators() {
-	randomCharGenerator = new Random(3);
-	randColLenGenerator = new Random(20);
+        randomCharGenerator = new Random(3);
+        randColLenGenerator = new Random(20);
     }
 
-    private static int writeRCFileTest(FileSystem fs, int rowCount, Path file,
-	    int columnNum, CompressionCodec codec, int columnCount)
-	    throws IOException {
-	fs.delete(file, true);
-	int rowsWritten = 0;
+    private static int writeRCFileTest(FileSystem fs, int rowCount, Path file, int columnNum,
+            CompressionCodec codec, int columnCount) throws IOException {
+        fs.delete(file, true);
+        int rowsWritten = 0;
 
-	resetRandomGenerators();
+        resetRandomGenerators();
 
-	RCFileOutputFormat.setColumnNumber(conf, columnNum);
-	RCFile.Writer writer = new RCFile.Writer(fs, conf, file, null, codec);
+        RCFileOutputFormat.setColumnNumber(conf, columnNum);
+        RCFile.Writer writer = new RCFile.Writer(fs, conf, file, null, codec);
 
-	byte[][] columnRandom;
+        byte[][] columnRandom;
 
-	BytesRefArrayWritable bytes = new BytesRefArrayWritable(columnNum);
-	columnRandom = new byte[columnNum][];
-	for (int i = 0; i < columnNum; i++) {
-	    BytesRefWritable cu = new BytesRefWritable();
-	    bytes.set(i, cu);
-	}
+        BytesRefArrayWritable bytes = new BytesRefArrayWritable(columnNum);
+        columnRandom = new byte[columnNum][];
+        for (int i = 0; i < columnNum; i++) {
+            BytesRefWritable cu = new BytesRefWritable();
+            bytes.set(i, cu);
+        }
 
-	for (int i = 0; i < rowCount; i++) {
-	    nextRandomRow(columnRandom, bytes, columnCount);
-	    rowsWritten++;
-	    writer.append(bytes);
-	}
-	writer.close();
+        for (int i = 0; i < rowCount; i++) {
+            nextRandomRow(columnRandom, bytes, columnCount);
+            rowsWritten++;
+            writer.append(bytes);
+        }
+        writer.close();
 
-	return rowsWritten;
+        return rowsWritten;
     }
 
-    private static void nextRandomRow(byte[][] row,
-	    BytesRefArrayWritable bytes, int columnCount) {
-	bytes.resetValid(row.length);
-	for (int i = 0; i < row.length; i++) {
+    private static void nextRandomRow(byte[][] row, BytesRefArrayWritable bytes, int columnCount) {
+        bytes.resetValid(row.length);
+        for (int i = 0; i < row.length; i++) {
 
-	    row[i] = new byte[columnCount];
-	    for (int j = 0; j < columnCount; j++)
-		row[i][j] = getRandomChar(randomCharGenerator);
-	    bytes.get(i).set(row[i], 0, columnCount);
-	}
+            row[i] = new byte[columnCount];
+            for (int j = 0; j < columnCount; j++)
+                row[i][j] = getRandomChar(randomCharGenerator);
+            bytes.get(i).set(row[i], 0, columnCount);
+        }
     }
 
     private static int CHAR_END = 122 - 7;
 
     private static byte getRandomChar(Random random) {
-	byte b = 0;
-	do {
-	    b = (byte) random.nextInt(CHAR_END);
-	} while ((b < 65));
-	if (b > 90) {
-	    b = 7;
-	}
-	return b;
+        byte b = 0;
+        do {
+            b = (byte) random.nextInt(CHAR_END);
+        } while ((b < 65));
+        if (b > 90) {
+            b = 7;
+        }
+        return b;
     }
 }
