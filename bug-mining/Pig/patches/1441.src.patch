diff --git a/CHANGES.txt b/CHANGES.txt
index 10feb965e..c162b8e8c 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -32,6 +32,8 @@ PIG-2207: Support custom counters for aggregating warnings from different udfs (
 
 IMPROVEMENTS
 
+PIG-3913: Pig should use job's jobClient wherever possible (fixes local mode counters) (aniket486)
+
 PIG-3941: Piggybank's Over UDF returns an output schema with named fields (mrflip via cheolsoo)
 
 PIG-3545: Seperate validation rules from optimizer (daijy)
diff --git a/shims/src/hadoop20/org/apache/pig/backend/hadoop/executionengine/shims/HadoopShims.java b/shims/src/hadoop20/org/apache/pig/backend/hadoop/executionengine/shims/HadoopShims.java
index 91d3dccaa..a46953267 100644
--- a/shims/src/hadoop20/org/apache/pig/backend/hadoop/executionengine/shims/HadoopShims.java
+++ b/shims/src/hadoop20/org/apache/pig/backend/hadoop/executionengine/shims/HadoopShims.java
@@ -24,6 +24,7 @@ import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.mapred.Counters;
 import org.apache.hadoop.mapred.JobClient;
+import org.apache.hadoop.mapred.RunningJob;
 import org.apache.hadoop.mapred.jobcontrol.Job;
 import org.apache.hadoop.mapred.jobcontrol.JobControl;
 import org.apache.hadoop.mapred.TaskReport;
@@ -32,6 +33,7 @@ import org.apache.hadoop.mapreduce.JobID;
 import org.apache.hadoop.mapreduce.OutputCommitter;
 import org.apache.hadoop.mapreduce.TaskAttemptContext;
 import org.apache.hadoop.mapreduce.TaskAttemptID;
+import org.apache.hadoop.mapreduce.TaskType;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
 import org.apache.pig.backend.hadoop20.PigJobControl;
@@ -53,9 +55,9 @@ public class HadoopShims {
     }
 
     static public TaskAttemptContext createTaskAttemptContext(Configuration conf,
-                                TaskAttemptID taskId) {
+            TaskAttemptID taskId) {
         TaskAttemptContext newContext = new TaskAttemptContext(conf,
-            taskId);
+                taskId);
         return newContext;
     }
 
@@ -95,7 +97,7 @@ public class HadoopShims {
     }
 
     public static JobControl newJobControl(String groupName, int timeToSleep) {
-      return new PigJobControl(groupName, timeToSleep);
+        return new PigJobControl(groupName, timeToSleep);
     }
 
     public static long getDefaultBlockSize(FileSystem fs, Path path) {
@@ -148,4 +150,37 @@ public class HadoopShims {
         return true;
     }
 
+    /**
+     * Returns the progress of a Job j which is part of a submitted JobControl
+     * object. The progress is for this Job. So it has to be scaled down by the
+     * num of jobs that are present in the JobControl.
+     *
+     * @param j The Job for which progress is required
+     * @return Returns the percentage progress of this Job
+     * @throws IOException
+     */
+    public static double progressOfRunningJob(Job j)
+            throws IOException {
+        RunningJob rj = j.getJobClient().getJob(j.getAssignedJobID());
+        if (rj == null && j.getState() == Job.SUCCESS)
+            return 1;
+        else if (rj == null)
+            return 0;
+        else {
+            return (rj.mapProgress() + rj.reduceProgress()) / 2;
+        }
+    }
+
+    public static void killJob(Job job) throws IOException {
+        RunningJob runningJob = job.getJobClient().getJob(job.getAssignedJobID());
+        if (runningJob != null)
+            runningJob.killJob();
+    }
+
+    public static TaskReport[] getTaskReports(Job job, TaskType type) throws IOException {
+        JobClient jobClient = job.getJobClient();
+        return (type == TaskType.MAP)
+                ? jobClient.getMapTaskReports(job.getAssignedJobID())
+                        : jobClient.getReduceTaskReports(job.getAssignedJobID());
+    }
 }
diff --git a/shims/src/hadoop23/org/apache/hadoop/mapred/DowngradeHelper.java b/shims/src/hadoop23/org/apache/hadoop/mapred/DowngradeHelper.java
new file mode 100644
index 000000000..e67594606
--- /dev/null
+++ b/shims/src/hadoop23/org/apache/hadoop/mapred/DowngradeHelper.java
@@ -0,0 +1,27 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.mapred;
+
+public class DowngradeHelper {
+    // This is required since hadoop 2 TaskReport allows 
+    // only package level access to this api
+    public static TaskReport[] downgradeTaskReports(
+            org.apache.hadoop.mapreduce.TaskReport[] reports) {
+        return TaskReport.downgradeArray(reports);
+    }
+}
diff --git a/shims/src/hadoop23/org/apache/pig/backend/hadoop/executionengine/shims/HadoopShims.java b/shims/src/hadoop23/org/apache/pig/backend/hadoop/executionengine/shims/HadoopShims.java
index 8cf5fe12f..93cf54432 100644
--- a/shims/src/hadoop23/org/apache/pig/backend/hadoop/executionengine/shims/HadoopShims.java
+++ b/shims/src/hadoop23/org/apache/pig/backend/hadoop/executionengine/shims/HadoopShims.java
@@ -26,6 +26,7 @@ import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.mapred.Counters;
+import org.apache.hadoop.mapred.DowngradeHelper;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.TIPStatus;
 import org.apache.hadoop.mapred.TaskReport;
@@ -55,7 +56,7 @@ public class HadoopShims {
     }
 
     static public TaskAttemptContext createTaskAttemptContext(Configuration conf,
-                                TaskAttemptID taskId) {
+            TaskAttemptID taskId) {
         if (conf instanceof JobConf) {
             return new TaskAttemptContextImpl(new JobConf(conf), taskId);
         } else {
@@ -115,8 +116,12 @@ public class HadoopShims {
         return fs.getDefaultBlockSize(path);
     }
 
-    public static Counters getCounters(Job job) throws IOException, InterruptedException {
-        return new Counters(job.getJob().getCounters());
+    public static Counters getCounters(Job job) throws IOException {
+        try {
+            return new Counters(job.getJob().getCounters());
+        } catch (InterruptedException ir) {
+            throw new IOException(ir);
+        }
     }
 
     public static boolean isJobFailed(TaskReport report) {
@@ -177,4 +182,43 @@ public class HadoopShims {
         return true;
     }
 
+    /**
+     * Returns the progress of a Job j which is part of a submitted JobControl
+     * object. The progress is for this Job. So it has to be scaled down by the
+     * num of jobs that are present in the JobControl.
+     *
+     * @param j The Job for which progress is required
+     * @return Returns the percentage progress of this Job
+     * @throws IOException
+     */
+    public static double progressOfRunningJob(Job j)
+            throws IOException {
+        org.apache.hadoop.mapreduce.Job mrJob = j.getJob();
+        try {
+            return (mrJob.mapProgress() + mrJob.reduceProgress()) / 2;
+        } catch (InterruptedException ir) {
+            return 0;
+        }
+    }
+
+    public static void killJob(Job job) throws IOException {
+        org.apache.hadoop.mapreduce.Job mrJob = job.getJob();
+        try {
+            if (mrJob != null) {
+                mrJob.killJob();
+            }
+        } catch (InterruptedException ir) {
+            throw new IOException(ir);
+        }
+    }
+
+    public static TaskReport[] getTaskReports(Job job, TaskType type) throws IOException {
+        org.apache.hadoop.mapreduce.Job mrJob = job.getJob();
+        try {
+            org.apache.hadoop.mapreduce.TaskReport[] reports = mrJob.getTaskReports(type);
+            return DowngradeHelper.downgradeTaskReports(reports);
+        } catch (InterruptedException ir) {
+            throw new IOException(ir);
+        }
+    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/Launcher.java b/src/org/apache/pig/backend/hadoop/executionengine/Launcher.java
index bb7ec9725..e2e681cd4 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/Launcher.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/Launcher.java
@@ -254,46 +254,18 @@ public abstract class Launcher {
      * @return The progress as a precentage in double format
      * @throws IOException
      */
-    protected double calculateProgress(JobControl jc, JobClient jobClient)
+    protected double calculateProgress(JobControl jc)
             throws IOException {
         double prog = 0.0;
         prog += jc.getSuccessfulJobs().size();
 
         List<Job> runnJobs = jc.getRunningJobs();
-        for (Object object : runnJobs) {
-            Job j = (Job) object;
-            prog += progressOfRunningJob(j, jobClient);
+        for (Job j : runnJobs) {
+            prog += HadoopShims.progressOfRunningJob(j);
         }
         return prog;
     }
 
-    /**
-     * Returns the progress of a Job j which is part of a submitted JobControl
-     * object. The progress is for this Job. So it has to be scaled down by the
-     * num of jobs that are present in the JobControl.
-     *
-     * @param j
-     *            - The Job for which progress is required
-     * @param jobClient
-     *            - the JobClient to which it has been submitted
-     * @return Returns the percentage progress of this Job
-     * @throws IOException
-     */
-    protected double progressOfRunningJob(Job j, JobClient jobClient)
-            throws IOException {
-        JobID mrJobID = j.getAssignedJobID();
-        RunningJob rj = jobClient.getJob(mrJobID);
-        if (rj == null && j.getState() == Job.SUCCESS)
-            return 1;
-        else if (rj == null)
-            return 0;
-        else {
-            double mapProg = rj.mapProgress();
-            double redProg = rj.reduceProgress();
-            return (mapProg + redProg) / 2;
-        }
-    }
-
     public long getTotalHadoopTimeSpent() {
         return totalHadoopTimeSpent;
     }
@@ -320,7 +292,7 @@ public abstract class Launcher {
 
     /**
      *
-     * @param stackTraceLine
+     * @param stackTrace
      *            The string representation of
      *            {@link Throwable#printStackTrace() printStackTrace} Handles
      *            internal PigException and its subclasses that override the
@@ -357,7 +329,7 @@ public abstract class Launcher {
 
     /**
      *
-     * @param stackTraceLine
+     * @param stackTraceLines
      *            An array of strings that represent
      *            {@link Throwable#printStackTrace() printStackTrace} output,
      *            split by newline
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java
index aa6c70f3f..e6a426110 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java
@@ -41,6 +41,7 @@ import org.apache.hadoop.mapred.JobID;
 import org.apache.hadoop.mapred.RunningJob;
 import org.apache.hadoop.mapred.TaskReport;
 import org.apache.hadoop.mapred.jobcontrol.Job;
+import org.apache.hadoop.mapreduce.TaskType;
 import org.apache.pig.PigConfiguration;
 import org.apache.pig.PigException;
 import org.apache.pig.PigRunner.ReturnCode;
@@ -102,9 +103,7 @@ public class MapReduceLauncher extends Launcher{
             log.debug("Receive kill signal");
             if (jc!=null) {
                 for (Job job : jc.getRunningJobs()) {
-                    RunningJob runningJob = job.getJobClient().getJob(job.getAssignedJobID());
-                    if (runningJob!=null)
-                        runningJob.killJob();
+                    HadoopShims.killJob(job);
                     log.info("Job " + job.getAssignedJobID() + " killed");
                 }
             }
@@ -342,7 +341,7 @@ public class MapReduceLauncher extends Launcher{
                     }
                     jobsWithoutIds.removeAll(jobsAssignedIdInThisRun);
 
-                    double prog = (numMRJobsCompl+calculateProgress(jc, statsJobClient))/totalMRJobs;
+                    double prog = (numMRJobsCompl+calculateProgress(jc))/totalMRJobs;
                     if (notifyProgress(prog, lastProg)) {
                         List<Job> runnJobs = jc.getRunningJobs();
                         if (runnJobs != null) {
@@ -454,7 +453,7 @@ public class MapReduceLauncher extends Launcher{
             Exception backendException = null;
             for (Job fj : failedJobs) {
                 try {
-                    getStats(fj, statsJobClient, true, pc);
+                    getStats(fj, true, pc);
                 } catch (Exception e) {
                     backendException = e;
                 }
@@ -494,9 +493,9 @@ public class MapReduceLauncher extends Launcher{
                     }
                 }
 
-                getStats(job, statsJobClient, false, pc);
+                getStats(job, false, pc);
                 if (aggregateWarning) {
-                    computeWarningAggregate(job, statsJobClient, warningAggMap);
+                    computeWarningAggregate(job, warningAggMap);
                 }
             }
 
@@ -639,8 +638,8 @@ public class MapReduceLauncher extends Launcher{
         comp.getMessageCollector().logMessages(MessageType.Warning, aggregateWarning, log);
 
         String lastInputChunkSize =
-            pc.getProperties().getProperty(
-                "last.input.chunksize", JoinPackager.DEFAULT_CHUNK_SIZE);
+                pc.getProperties().getProperty(
+                        "last.input.chunksize", JoinPackager.DEFAULT_CHUNK_SIZE);
 
         String prop = pc.getProperties().getProperty(PigConfiguration.PROP_NO_COMBINER);
         if (!pc.inIllustrator && !("true".equals(prop)))  {
@@ -746,47 +745,41 @@ public class MapReduceLauncher extends Launcher{
     }
 
     @SuppressWarnings("deprecation")
-    void computeWarningAggregate(Job job, JobClient jobClient, Map<Enum, Long> aggMap) {
-        JobID mapRedJobID = job.getAssignedJobID();
-        RunningJob runningJob = null;
+    void computeWarningAggregate(Job job, Map<Enum, Long> aggMap) {
         try {
-            runningJob = jobClient.getJob(mapRedJobID);
-            if(runningJob != null) {
-                Counters counters = runningJob.getCounters();
-                if (counters==null)
-                {
-                    long nullCounterCount = aggMap.get(PigWarning.NULL_COUNTER_COUNT)==null?0 : aggMap.get(PigWarning.NULL_COUNTER_COUNT);
-                    nullCounterCount++;
-                    aggMap.put(PigWarning.NULL_COUNTER_COUNT, nullCounterCount);
-                }
-                try {
-                    for (Enum e : PigWarning.values()) {
-                        if (e != PigWarning.NULL_COUNTER_COUNT) {
-                            Long currentCount = aggMap.get(e);
-                            currentCount = (currentCount == null ? 0 : currentCount);
-                            // This code checks if the counters is null, if it is,
-                            // we need to report to the user that the number
-                            // of warning aggregations may not be correct. In fact,
-                            // Counters should not be null, it is
-                            // a hadoop bug, once this bug is fixed in hadoop, the
-                            // null handling code should never be hit.
-                            // See Pig-943
-                            if (counters != null)
-                                currentCount += counters.getCounter(e);
-                            aggMap.put(e, currentCount);
-                        }
-                    }
-                } catch (Exception e) {
-                    log.warn("Exception getting counters.", e);
+            Counters counters = HadoopShims.getCounters(job);
+            if (counters==null)
+            {
+                long nullCounterCount =
+                        (aggMap.get(PigWarning.NULL_COUNTER_COUNT) == null)
+                          ? 0
+                          : aggMap.get(PigWarning.NULL_COUNTER_COUNT);
+                nullCounterCount++;
+                aggMap.put(PigWarning.NULL_COUNTER_COUNT, nullCounterCount);
+            }
+            for (Enum e : PigWarning.values()) {
+                if (e != PigWarning.NULL_COUNTER_COUNT) {
+                    Long currentCount = aggMap.get(e);
+                    currentCount = (currentCount == null ? 0 : currentCount);
+                    // This code checks if the counters is null, if it is,
+                    // we need to report to the user that the number
+                    // of warning aggregations may not be correct. In fact,
+                    // Counters should not be null, it is
+                    // a hadoop bug, once this bug is fixed in hadoop, the
+                    // null handling code should never be hit.
+                    // See Pig-943
+                    if (counters != null)
+                        currentCount += counters.getCounter(e);
+                    aggMap.put(e, currentCount);
                 }
             }
-        } catch (IOException ioe) {
+        } catch (Exception e) {
             String msg = "Unable to retrieve job to compute warning aggregation.";
             log.warn(msg);
         }
     }
 
-    private void getStats(Job job, JobClient jobClient, boolean errNotDbg,
+    private void getStats(Job job, boolean errNotDbg,
             PigContext pigContext) throws ExecException {
         JobID MRJobID = job.getAssignedJobID();
         String jobMessage = job.getMessage();
@@ -808,11 +801,11 @@ public class MapReduceLauncher extends Launcher{
             throw new ExecException(backendException);
         }
         try {
-            TaskReport[] mapRep = jobClient.getMapTaskReports(MRJobID);
+            TaskReport[] mapRep = HadoopShims.getTaskReports(job, TaskType.MAP);
             getErrorMessages(mapRep, "map", errNotDbg, pigContext);
             totalHadoopTimeSpent += computeTimeSpent(mapRep);
             mapRep = null;
-            TaskReport[] redRep = jobClient.getReduceTaskReports(MRJobID);
+            TaskReport[] redRep = HadoopShims.getTaskReports(job, TaskType.REDUCE);
             getErrorMessages(redRep, "reduce", errNotDbg, pigContext);
             totalHadoopTimeSpent += computeTimeSpent(redRep);
             redRep = null;
diff --git a/src/org/apache/pig/tools/pigstats/InputStats.java b/src/org/apache/pig/tools/pigstats/InputStats.java
index 9764c5acb..2b20110de 100644
--- a/src/org/apache/pig/tools/pigstats/InputStats.java
+++ b/src/org/apache/pig/tools/pigstats/InputStats.java
@@ -84,7 +84,7 @@ public final class InputStats {
         return type;
     }
 
-    public String getDisplayString(boolean local) {
+    public String getDisplayString() {
         StringBuilder sb = new StringBuilder();
         if (success) {
             sb.append("Successfully ");
@@ -96,7 +96,7 @@ public final class InputStats {
                 sb.append("read ");
             }
 
-            if (!local && records >= 0) {
+            if (records >= 0) {
                 sb.append(records).append(" records ");
             } else {
                 sb.append("records ");
diff --git a/src/org/apache/pig/tools/pigstats/JobStats.java b/src/org/apache/pig/tools/pigstats/JobStats.java
index 93c1033a6..fb87a500c 100644
--- a/src/org/apache/pig/tools/pigstats/JobStats.java
+++ b/src/org/apache/pig/tools/pigstats/JobStats.java
@@ -174,7 +174,7 @@ public abstract class JobStats extends Operator {
         exception = e;
     }
 
-    public abstract String getDisplayString(boolean isLocal);
+    public abstract String getDisplayString();
 
 
     /**
diff --git a/src/org/apache/pig/tools/pigstats/OutputStats.java b/src/org/apache/pig/tools/pigstats/OutputStats.java
index dd8ff6cc7..6eb7c0a9a 100644
--- a/src/org/apache/pig/tools/pigstats/OutputStats.java
+++ b/src/org/apache/pig/tools/pigstats/OutputStats.java
@@ -105,12 +105,12 @@ public final class OutputStats {
     public Configuration getConf() {
         return conf;
     }
-    
-    public String getDisplayString(boolean local) {
+
+    public String getDisplayString() {
         StringBuilder sb = new StringBuilder();
         if (success) {
             sb.append("Successfully stored ");
-            if (!local && records >= 0) {
+            if (records >= 0) {
                 sb.append(records).append(" records ");
             } else {
                 sb.append("records ");
diff --git a/src/org/apache/pig/tools/pigstats/PigStats.java b/src/org/apache/pig/tools/pigstats/PigStats.java
index 5c9b88c41..c3d2fff1a 100644
--- a/src/org/apache/pig/tools/pigstats/PigStats.java
+++ b/src/org/apache/pig/tools/pigstats/PigStats.java
@@ -109,6 +109,7 @@ public abstract class PigStats {
         return errorThrowable;
     }
 
+    @Deprecated
     public abstract JobClient getJobClient();
 
     public abstract boolean isEmbedded();
diff --git a/src/org/apache/pig/tools/pigstats/mapreduce/MRJobStats.java b/src/org/apache/pig/tools/pigstats/mapreduce/MRJobStats.java
index 3789515f8..b3ba80292 100644
--- a/src/org/apache/pig/tools/pigstats/mapreduce/MRJobStats.java
+++ b/src/org/apache/pig/tools/pigstats/mapreduce/MRJobStats.java
@@ -35,10 +35,13 @@ import org.apache.hadoop.mapred.JobClient;
 import org.apache.hadoop.mapred.JobID;
 import org.apache.hadoop.mapred.RunningJob;
 import org.apache.hadoop.mapred.TaskReport;
+import org.apache.hadoop.mapred.jobcontrol.Job;
+import org.apache.hadoop.mapreduce.TaskType;
 import org.apache.pig.PigCounters;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOper;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
+import org.apache.pig.backend.hadoop.executionengine.shims.HadoopShims;
 import org.apache.pig.classification.InterfaceAudience;
 import org.apache.pig.classification.InterfaceStability;
 import org.apache.pig.impl.io.FileSpec;
@@ -71,9 +74,6 @@ public final class MRJobStats extends JobStats {
 
     public static final String FAILURE_HEADER = "JobId\tAlias\tFeature\tMessage\tOutputs";
 
-    // currently counters are not working in local mode - see PIG-1286
-    public static final String SUCCESS_HEADER_LOCAL = "JobId\tAlias\tFeature\tOutputs";
-
     private static final Log LOG = LogFactory.getLog(MRJobStats.class);
 
     private List<POStore> mapStores = null;
@@ -237,11 +237,20 @@ public final class MRJobStats extends JobStats {
         medianReduceTime = median;
     }
 
+    private static void appendStat(long stat, StringBuilder sb) {
+        if(stat != -1) {
+            sb.append(stat/1000);
+        } else {
+            sb.append("n/a");
+        }
+        sb.append("\t");
+    }
+
     @Override
-    public String getDisplayString(boolean local) {
+    public String getDisplayString() {
         StringBuilder sb = new StringBuilder();
         String id = (jobId == null) ? "N/A" : jobId.toString();
-        if (state == JobState.FAILED || local) {
+        if (state == JobState.FAILED) {
             sb.append(id).append("\t")
                 .append(getAlias()).append("\t")
                 .append(getFeature()).append("\t");
@@ -252,22 +261,15 @@ public final class MRJobStats extends JobStats {
             sb.append(id).append("\t")
                 .append(numberMaps).append("\t")
                 .append(numberReduces).append("\t");
-            if (numberMaps == 0) {
-                sb.append("n/a\t").append("n/a\t").append("n/a\t").append("n/a\t");
-            } else {
-                sb.append(maxMapTime/1000).append("\t")
-                    .append(minMapTime/1000).append("\t")
-                    .append(avgMapTime/1000).append("\t")
-                    .append(medianMapTime/1000).append("\t");
-            }
-            if (numberReduces == 0) {
-                sb.append("n/a\t").append("n/a\t").append("n/a\t").append("n/a\t");
-            } else {
-                sb.append(maxReduceTime/1000).append("\t")
-                    .append(minReduceTime/1000).append("\t")
-                    .append(avgReduceTime/1000).append("\t")
-                    .append(medianReduceTime/1000).append("\t");
-            }
+            appendStat(maxMapTime, sb);
+            appendStat(minMapTime, sb);
+            appendStat(avgMapTime, sb);
+            appendStat(medianMapTime, sb);
+            appendStat(maxReduceTime, sb);
+            appendStat(minReduceTime, sb);
+            appendStat(avgReduceTime, sb);
+            appendStat(medianReduceTime, sb);
+
             sb.append(getAlias()).append("\t")
                 .append(getFeature()).append("\t");
         }
@@ -278,13 +280,11 @@ public final class MRJobStats extends JobStats {
         return sb.toString();
     }
 
-    void addCounters(RunningJob rjob) {
-        if (rjob != null) {
-            try {
-                counters = rjob.getCounters();
-            } catch (IOException e) {
-                LOG.warn("Unable to get job counters", e);
-            }
+    void addCounters(Job job) {
+        try {
+            counters = HadoopShims.getCounters(job);
+        } catch (IOException e) {
+            LOG.warn("Unable to get job counters", e);
         }
         if (counters != null) {
             Counters.Group taskgroup = counters
@@ -331,10 +331,10 @@ public final class MRJobStats extends JobStats {
         }
     }
 
-    void addMapReduceStatistics(JobClient client, Configuration conf) {
+    void addMapReduceStatistics(Job job) {
         TaskReport[] maps = null;
         try {
-            maps = client.getMapTaskReports(jobId);
+            maps = HadoopShims.getTaskReports(job, TaskType.MAP);
         } catch (IOException e) {
             LOG.warn("Failed to get map task report", e);
         }
@@ -367,7 +367,7 @@ public final class MRJobStats extends JobStats {
 
         TaskReport[] reduces = null;
         try {
-            reduces = client.getReduceTaskReports(jobId);
+            reduces = HadoopShims.getTaskReports(job, TaskType.REDUCE);
         } catch (IOException e) {
             LOG.warn("Failed to get reduce task report", e);
         }
diff --git a/src/org/apache/pig/tools/pigstats/mapreduce/MRPigStatsUtil.java b/src/org/apache/pig/tools/pigstats/mapreduce/MRPigStatsUtil.java
index a4ba4e1c2..4b5927511 100644
--- a/src/org/apache/pig/tools/pigstats/mapreduce/MRPigStatsUtil.java
+++ b/src/org/apache/pig/tools/pigstats/mapreduce/MRPigStatsUtil.java
@@ -230,21 +230,9 @@ public class MRPigStatsUtil extends PigStatsUtil {
         } else {
             js.setSuccessful(true);
 
-            js.addMapReduceStatistics(ps.getJobClient(), job.getJobConf());
+            js.addMapReduceStatistics(job);
 
-            JobClient client = ps.getJobClient();
-            RunningJob rjob = null;
-            try {
-                rjob = client.getJob(job.getAssignedJobID());
-            } catch (IOException e) {
-                LOG.warn("Failed to get running job", e);
-            }
-            if (rjob == null) {
-                LOG.warn("Failed to get RunningJob for job "
-                        + job.getAssignedJobID());
-            } else {
-                js.addCounters(rjob);
-            }
+            js.addCounters(job);
 
             js.addOutputStatistics();
 
diff --git a/src/org/apache/pig/tools/pigstats/mapreduce/SimplePigStats.java b/src/org/apache/pig/tools/pigstats/mapreduce/SimplePigStats.java
index bd3540596..46b22af37 100644
--- a/src/org/apache/pig/tools/pigstats/mapreduce/SimplePigStats.java
+++ b/src/org/apache/pig/tools/pigstats/mapreduce/SimplePigStats.java
@@ -172,6 +172,7 @@ public final class SimplePigStats extends PigStats {
         return startTime > 0;
     }
 
+    @Deprecated
     @Override
     public JobClient getJobClient() {
         return jobClient;
@@ -215,20 +216,14 @@ public final class SimplePigStats extends PigStats {
             return;
         }
 
-        // currently counters are not working in local mode - see PIG-1286
-        ExecType execType = pigContext.getExecType();
-        if (execType.isLocal()) {
-            LOG.info("Detected Local mode. Stats reported below may be incomplete");
-        }
-
         SimpleDateFormat sdf = new SimpleDateFormat(DATE_FORMAT);
         StringBuilder sb = new StringBuilder();
         sb.append("\nHadoopVersion\tPigVersion\tUserId\tStartedAt\tFinishedAt\tFeatures\n");
         sb.append(getHadoopVersion()).append("\t").append(getPigVersion()).append("\t")
-            .append(userId).append("\t")
-            .append(sdf.format(new Date(startTime))).append("\t")
-            .append(sdf.format(new Date(endTime))).append("\t")
-            .append(getFeatures()).append("\n");
+        .append(userId).append("\t")
+        .append(sdf.format(new Date(startTime))).append("\t")
+        .append(sdf.format(new Date(endTime))).append("\t")
+        .append(getFeatures()).append("\n");
         sb.append("\n");
         if (returnCode == ReturnCode.SUCCESS) {
             sb.append("Success!\n");
@@ -242,14 +237,10 @@ public final class SimplePigStats extends PigStats {
         if (returnCode == ReturnCode.SUCCESS
                 || returnCode == ReturnCode.PARTIAL_FAILURE) {
             sb.append("Job Stats (time in seconds):\n");
-            if (execType.isLocal()) {
-                sb.append(MRJobStats.SUCCESS_HEADER_LOCAL).append("\n");
-            } else {
-                sb.append(MRJobStats.SUCCESS_HEADER).append("\n");
-            }
+            sb.append(MRJobStats.SUCCESS_HEADER).append("\n");
             List<JobStats> arr = jobPlan.getSuccessfulJobs();
             for (JobStats js : arr) {
-                sb.append(js.getDisplayString(execType.isLocal()));
+                sb.append(js.getDisplayString());
             }
             sb.append("\n");
         }
@@ -259,31 +250,29 @@ public final class SimplePigStats extends PigStats {
             sb.append(MRJobStats.FAILURE_HEADER).append("\n");
             List<JobStats> arr = jobPlan.getFailedJobs();
             for (JobStats js : arr) {
-                sb.append(js.getDisplayString(execType.isLocal()));
+                sb.append(js.getDisplayString());
             }
             sb.append("\n");
         }
         sb.append("Input(s):\n");
         for (InputStats is : getInputStats()) {
-            sb.append(is.getDisplayString(execType.isLocal()));
+            sb.append(is.getDisplayString());
         }
         sb.append("\n");
         sb.append("Output(s):\n");
         for (OutputStats ds : getOutputStats()) {
-            sb.append(ds.getDisplayString(execType.isLocal()));
+            sb.append(ds.getDisplayString());
         }
 
-        if (!(execType.isLocal())) {
-            sb.append("\nCounters:\n");
-            sb.append("Total records written : " + getRecordWritten()).append("\n");
-            sb.append("Total bytes written : " + getBytesWritten()).append("\n");
-            sb.append("Spillable Memory Manager spill count : "
-                    + getSMMSpillCount()).append("\n");
-            sb.append("Total bags proactively spilled: "
-                    + getProactiveSpillCountObjects()).append("\n");
-            sb.append("Total records proactively spilled: "
-                    + getProactiveSpillCountRecords()).append("\n");
-        }
+        sb.append("\nCounters:\n");
+        sb.append("Total records written : " + getRecordWritten()).append("\n");
+        sb.append("Total bytes written : " + getBytesWritten()).append("\n");
+        sb.append("Spillable Memory Manager spill count : "
+                + getSMMSpillCount()).append("\n");
+        sb.append("Total bags proactively spilled: "
+                + getProactiveSpillCountObjects()).append("\n");
+        sb.append("Total records proactively spilled: "
+                + getProactiveSpillCountRecords()).append("\n");
 
         sb.append("\nJob DAG:\n").append(jobPlan.toString());
 
