diff --git a/CHANGES.txt b/CHANGES.txt
index f875f695c..2c968f0e4 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -108,6 +108,8 @@ PIG-1696: Performance: Use System.arraycopy() instead of manually copying the by
 
 BUG FIXES
 
+PIG-1862: Pig returns exit code 0 for the failed Pig script due to non-existing input directory (rding)
+
 PIG-1888: Fix TestLogicalPlanGenerator not use hardcoded path (daijy)
 
 PIG-1837: Error while using IsEmpty function (rding)
diff --git a/src/org/apache/pig/PigServer.java b/src/org/apache/pig/PigServer.java
index 6991f2375..cc54ec025 100644
--- a/src/org/apache/pig/PigServer.java
+++ b/src/org/apache/pig/PigServer.java
@@ -54,8 +54,10 @@ import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.backend.executionengine.ExecJob;
 import org.apache.pig.backend.executionengine.ExecJob.JOB_STATUS;
 import org.apache.pig.backend.hadoop.executionengine.HJob;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
+import org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil;
 import org.apache.pig.builtin.PigStorage;
 import org.apache.pig.classification.InterfaceAudience;
 import org.apache.pig.classification.InterfaceStability;
@@ -975,7 +977,11 @@ public class PigServer {
             currDAG.lp.explain(lps, format, verbose);
 
             pp.explain(pps, format, verbose);
-            pigContext.getExecutionEngine().explain(pp, eps, format, verbose);
+            
+            MapRedUtil.checkLeafIsStore(pp, pigContext);
+            MapReduceLauncher launcher = new MapReduceLauncher();
+            launcher.explain(pp, pigContext, eps, format, verbose);
+
             if (markAsExecute) {
                 currDAG.markAsExecuted();
             }
@@ -1216,25 +1222,40 @@ public class PigServer {
         // discover pig features used in this script
         ScriptState.get().setScriptFeatures( currDAG.lp );
         PhysicalPlan pp = compilePp();
-        // execute using appropriate engine
-        List<ExecJob> jobs = pigContext.getExecutionEngine().execute(pp, "job_pigexec_");
+       
+        MapReduceLauncher launcher = new MapReduceLauncher();
         PigStats stats = null;
-        if (jobs.size() > 0) {
-            stats = jobs.get(0).getStatistics();
-        } else {
-            stats = PigStatsUtil.getEmptyPigStats();
+        try {
+            stats = launcher.launchPig(pp, "job_pigexec_", pigContext);
+        } catch (Exception e) {
+            // There are a lot of exceptions thrown by the launcher.  If this
+            // is an ExecException, just let it through.  Else wrap it.
+            if (e instanceof ExecException){
+                throw (ExecException)e;
+            } else if (e instanceof FrontendException) {
+                throw (FrontendException)e;
+            } else {
+                int errCode = 2043;
+                String msg = "Unexpected error during execution.";
+                throw new ExecException(msg, errCode, PigException.BUG, e);
+            }
+        } finally {
+            launcher.reset();
         }
+        
         for (OutputStats output : stats.getOutputStats()) {
             if (!output.isSuccessful()) {
                 POStore store = output.getPOStore();
                 try {
-                    store.getStoreFunc().cleanupOnFailure(store.getSFile().getFileName(),
+                    store.getStoreFunc().cleanupOnFailure(
+                            store.getSFile().getFileName(),
                             new Job(output.getConf()));
                 } catch (IOException e) {
                     throw new ExecException(e);
                 }
             }
         }
+        
         return stats;
     }
 
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/HExecutionEngine.java b/src/org/apache/pig/backend/hadoop/executionengine/HExecutionEngine.java
index 372e8a6d7..e1a1c10b5 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/HExecutionEngine.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/HExecutionEngine.java
@@ -19,17 +19,14 @@
 package org.apache.pig.backend.hadoop.executionengine;
 
 import java.io.IOException;
-import java.io.PrintStream;
 import java.net.Socket;
 import java.net.SocketException;
 import java.net.SocketImplFactory;
 import java.net.URL;
-import java.util.ArrayList;
 import java.util.Enumeration;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.Iterator;
-import java.util.List;
 import java.util.Map;
 import java.util.Properties;
 
@@ -39,27 +36,18 @@ import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hdfs.DistributedFileSystem;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.pig.ExecType;
-import org.apache.pig.FuncSpec;
 import org.apache.pig.PigException;
 import org.apache.pig.backend.datastorage.DataStorage;
 import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.executionengine.ExecJob;
 import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
 import org.apache.pig.backend.hadoop.datastorage.HDataStorage;
-import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
-import org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil;
 import org.apache.pig.impl.PigContext;
-import org.apache.pig.impl.io.FileLocalizer;
-import org.apache.pig.impl.io.FileSpec;
 import org.apache.pig.impl.logicalLayer.FrontendException;
 import org.apache.pig.impl.logicalLayer.LogicalOperator;
-import org.apache.pig.impl.plan.NodeIdGenerator;
 import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.impl.util.ObjectSerializer;
-import org.apache.pig.impl.util.Utils;
 import org.apache.pig.newplan.Operator;
 import org.apache.pig.newplan.logical.optimizer.LogicalPlanOptimizer;
 import org.apache.pig.newplan.logical.optimizer.SchemaResetter;
@@ -70,8 +58,6 @@ import org.apache.pig.newplan.logical.relational.LogicalPlan;
 import org.apache.pig.newplan.logical.relational.LogicalRelationalOperator;
 import org.apache.pig.newplan.logical.rules.InputOutputFileValidator;
 import org.apache.pig.newplan.logical.visitor.SortInfoSetter;
-import org.apache.pig.tools.pigstats.OutputStats;
-import org.apache.pig.tools.pigstats.PigStats;
 import org.apache.pig.pen.POOptimizeDisabler;
 
 public class HExecutionEngine {
@@ -221,20 +207,10 @@ public class HExecutionEngine {
         jobConf = new JobConf(configuration);
     }
 
-    public Properties getConfiguration() throws ExecException {
-        return this.pigContext.getProperties();
-    }
-        
     public void updateConfiguration(Properties newConfiguration) 
             throws ExecException {
         init(newConfiguration);
     }
-        
-    public void close() throws ExecException {}
-
-    public Map<String, Object> getStatistics() throws ExecException {
-        throw new UnsupportedOperationException();
-    }
 
     @SuppressWarnings("unchecked")
     public PhysicalPlan compile(LogicalPlan plan,
@@ -338,68 +314,8 @@ public class HExecutionEngine {
     public LogicalPlan getNewPlan() {
         return newPreoptimizedPlan;
     }
-    
-    public List<ExecJob> execute(PhysicalPlan plan,
-                                 String jobName) throws ExecException, FrontendException {
-        MapReduceLauncher launcher = new MapReduceLauncher();
-        List<ExecJob> jobs = new ArrayList<ExecJob>();
-
-        Map<String, PhysicalOperator> leafMap = new HashMap<String, PhysicalOperator>();
-        for (PhysicalOperator physOp : plan.getLeaves()) {
-            log.info(physOp);
-            if (physOp instanceof POStore) {
-                FileSpec spec = ((POStore) physOp).getSFile();
-                if (spec != null)
-                    leafMap.put(spec.toString(), physOp);
-            }
-        }
-        try {
-            PigStats stats = launcher.launchPig(plan, jobName, pigContext);
-
-            for (OutputStats output : stats.getOutputStats()) {
-                POStore store = output.getPOStore();               
-                String alias = store.getAlias();
-                if (output.isSuccessful()) {
-                    jobs.add(new HJob(ExecJob.JOB_STATUS.COMPLETED, pigContext, store, alias, stats));
-                } else {
-                    HJob j = new HJob(ExecJob.JOB_STATUS.FAILED, pigContext, store, alias, stats);  
-                    j.setException(launcher.getError(store.getSFile()));
-                    jobs.add(j);
-                }
-            }
-
-            return jobs;
-        } catch (Exception e) {
-            // There are a lot of exceptions thrown by the launcher.  If this
-            // is an ExecException, just let it through.  Else wrap it.
-            if (e instanceof ExecException){
-                throw (ExecException)e;
-            } else if (e instanceof FrontendException) {
-                throw (FrontendException)e;
-            } else {
-                int errCode = 2043;
-                String msg = "Unexpected error during execution.";
-                throw new ExecException(msg, errCode, PigException.BUG, e);
-            }
-        } finally {
-            launcher.reset();
-        }
-
-    }
-
-    public void explain(PhysicalPlan plan, PrintStream stream, String format, boolean verbose) {
-        try {
-            MapRedUtil.checkLeafIsStore(plan, pigContext);
-
-            MapReduceLauncher launcher = new MapReduceLauncher();
-            launcher.explain(plan, pigContext, stream, format, verbose);
-
-        } catch (Exception ve) {
-            throw new RuntimeException(ve);
-        }
-    }
-  
-    @SuppressWarnings("unchecked")
+      
+    @SuppressWarnings({ "unchecked", "rawtypes" })
     private void setSSHFactory(){
         Properties properties = this.pigContext.getProperties();
         String g = properties.getProperty("ssh.gateway");
@@ -455,29 +371,4 @@ public class HExecutionEngine {
         }
     } 
     
-    public static FileSpec checkLeafIsStore(
-            PhysicalPlan plan,
-            PigContext pigContext) throws ExecException {
-        try {
-            PhysicalOperator leaf = plan.getLeaves().get(0);
-            FileSpec spec = null;
-            if(!(leaf instanceof POStore)){
-                String scope = leaf.getOperatorKey().getScope();
-                POStore str = new POStore(new OperatorKey(scope,
-                    NodeIdGenerator.getGenerator().getNextNodeId(scope)));
-                spec = new FileSpec(FileLocalizer.getTemporaryPath(
-                    pigContext).toString(),
-                    new FuncSpec(Utils.getTmpFileCompressorName(pigContext)));
-                str.setSFile(spec);
-                plan.addAsLeaf(str);
-            } else{
-                spec = ((POStore)leaf).getSFile();
-            }
-            return spec;
-        } catch (Exception e) {
-            int errCode = 2045;
-            String msg = "Internal error. Not able to check if the leaf node is a store operator.";
-            throw new ExecException(msg, errCode, PigException.BUG, e);
-        }
-    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java
index bb6fc138b..b2d4646d1 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java
@@ -116,9 +116,10 @@ public class MapReduceLauncher extends Launcher{
         aggregateWarning = "true".equalsIgnoreCase(pc.getProperties().getProperty("aggregate.warning"));
         MROperPlan mrp = compile(php, pc);
                 
+        ConfigurationValidator.validatePigProperties(pc.getProperties());
+        Configuration conf = ConfigurationUtil.toConfiguration(pc.getProperties());
+        
         HExecutionEngine exe = pc.getExecutionEngine();
-        ConfigurationValidator.validatePigProperties(exe.getConfiguration());
-        Configuration conf = ConfigurationUtil.toConfiguration(exe.getConfiguration());
         JobClient jobClient = new JobClient(exe.getJobConf());
         
         JobControlCompiler jcc = new JobControlCompiler(pc, conf);
diff --git a/src/org/apache/pig/pen/LocalMapReduceSimulator.java b/src/org/apache/pig/pen/LocalMapReduceSimulator.java
index b1c23defa..5c6025a7f 100644
--- a/src/org/apache/pig/pen/LocalMapReduceSimulator.java
+++ b/src/org/apache/pig/pen/LocalMapReduceSimulator.java
@@ -17,12 +17,12 @@
  */
 package org.apache.pig.pen;
 
+import java.io.IOException;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
-import java.io.IOException;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.io.Text;
@@ -31,9 +31,9 @@ import org.apache.hadoop.mapred.jobcontrol.Job;
 import org.apache.hadoop.mapred.jobcontrol.JobControl;
 import org.apache.hadoop.mapreduce.Mapper;
 import org.apache.hadoop.mapreduce.Reducer;
+import org.apache.pig.PigException;
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
-import org.apache.pig.backend.hadoop.executionengine.HExecutionEngine;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOper;
@@ -44,29 +44,28 @@ import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigSplit;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.MROperPlan;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.util.PlanHelper;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
 import org.apache.pig.data.DataBag;
 import org.apache.pig.data.Tuple;
 import org.apache.pig.impl.PigContext;
+import org.apache.pig.impl.builtin.ReadScalars;
 import org.apache.pig.impl.io.FileLocalizer;
 import org.apache.pig.impl.io.NullableTuple;
 import org.apache.pig.impl.io.PigNullableWritable;
-import org.apache.pig.newplan.logical.relational.LOLoad;
-import org.apache.pig.newplan.logical.relational.LogicalRelationalOperator;
 import org.apache.pig.impl.plan.DepthFirstWalker;
 import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.impl.plan.VisitorException;
 import org.apache.pig.impl.util.ConfigurationValidator;
 import org.apache.pig.impl.util.ObjectSerializer;
 import org.apache.pig.impl.util.Pair;
-import org.apache.pig.impl.builtin.ReadScalars;
+import org.apache.pig.newplan.logical.relational.LOLoad;
+import org.apache.pig.newplan.logical.relational.LogicalRelationalOperator;
 import org.apache.pig.pen.util.LineageTracer;
-import org.apache.pig.PigException;
 
 
 
@@ -89,9 +88,8 @@ public class LocalMapReduceSimulator {
         phyToMRMap.clear();
         MROperPlan mrp = launcher.compile(php, pc);
                 
-        HExecutionEngine exe = pc.getExecutionEngine();
-        ConfigurationValidator.validatePigProperties(exe.getConfiguration());
-        Configuration conf = ConfigurationUtil.toConfiguration(exe.getConfiguration());
+        ConfigurationValidator.validatePigProperties(pc.getProperties());
+        Configuration conf = ConfigurationUtil.toConfiguration(pc.getProperties());
         
         JobControlCompiler jcc = new JobControlCompiler(pc, conf);
         
diff --git a/src/org/apache/pig/tools/grunt/GruntParser.java b/src/org/apache/pig/tools/grunt/GruntParser.java
index baa133032..bce7ab72a 100644
--- a/src/org/apache/pig/tools/grunt/GruntParser.java
+++ b/src/org/apache/pig/tools/grunt/GruntParser.java
@@ -69,6 +69,9 @@ import org.apache.pig.tools.parameters.ParameterSubstitutionPreprocessor;
 import org.apache.pig.tools.pigscript.parser.ParseException;
 import org.apache.pig.tools.pigscript.parser.PigScriptParser;
 import org.apache.pig.tools.pigscript.parser.PigScriptParserTokenManager;
+import org.apache.pig.tools.pigstats.JobStats;
+import org.apache.pig.tools.pigstats.PigStats;
+import org.apache.pig.tools.pigstats.PigStats.JobGraph;
 
 @SuppressWarnings("deprecation")
 public class GruntParser extends PigScriptParser {
@@ -113,11 +116,15 @@ public class GruntParser extends PigScriptParser {
             }
 
             if (!mLoadOnly) {
-                List<ExecJob> jobs = mPigServer.executeBatch();
-                for(ExecJob job: jobs) {
-                    if (job.getStatus() == ExecJob.JOB_STATUS.FAILED) {
+                mPigServer.executeBatch();
+                PigStats stats = PigStats.get();
+                JobGraph jg = stats.getJobGraph();
+                Iterator<JobStats> iter = jg.iterator();
+                while (iter.hasNext()) {
+                    JobStats js = iter.next();
+                    if (!js.isSuccessful()) {
                         mNumFailedJobs++;
-                        Exception exp = (job.getException() != null) ? job.getException()
+                        Exception exp = (js.getException() != null) ? js.getException()
                                 : new ExecException(
                                         "Job failed, hadoop does not return any error message",
                                         2244);                        
diff --git a/src/org/apache/pig/tools/pigstats/JobStats.java b/src/org/apache/pig/tools/pigstats/JobStats.java
index fdd7d1e4a..a1e5b60fc 100644
--- a/src/org/apache/pig/tools/pigstats/JobStats.java
+++ b/src/org/apache/pig/tools/pigstats/JobStats.java
@@ -431,7 +431,7 @@ public final class JobStats extends Operator {
         if (mapStores.size() + reduceStores.size() == 1) {
             POStore sto = (mapStores.size() > 0) ? mapStores.get(0)
                     : reduceStores.get(0);
-            if (!sto.isTmpStore() || state != JobState.SUCCESS) {
+            if (!sto.isTmpStore()) {
                 long records = (mapStores.size() > 0) ? mapOutputRecords
                         : reduceOutputRecords;           
                 OutputStats ds = new OutputStats(sto.getSFile().getFileName(),
diff --git a/test/org/apache/pig/test/TestGroupConstParallel.java b/test/org/apache/pig/test/TestGroupConstParallel.java
index 276c40e80..2cf2c0776 100644
--- a/test/org/apache/pig/test/TestGroupConstParallel.java
+++ b/test/org/apache/pig/test/TestGroupConstParallel.java
@@ -31,7 +31,6 @@ import org.apache.hadoop.mapred.jobcontrol.JobControl;
 import org.apache.pig.ExecType;
 import org.apache.pig.PigServer;
 import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
-import org.apache.pig.backend.hadoop.executionengine.HExecutionEngine;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.MROperPlan;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
@@ -126,9 +125,8 @@ public class TestGroupConstParallel {
         pp.addAsLeaf(store);
         MROperPlan mrPlan = Util.buildMRPlan(pp, pc);
 
-        HExecutionEngine exe = pc.getExecutionEngine();
-        ConfigurationValidator.validatePigProperties(exe.getConfiguration());
-        Configuration conf = ConfigurationUtil.toConfiguration(exe.getConfiguration());
+        ConfigurationValidator.validatePigProperties(pc.getProperties());
+        Configuration conf = ConfigurationUtil.toConfiguration(pc.getProperties());
         JobControlCompiler jcc = new JobControlCompiler(pc, conf);
         
         JobControl jobControl = jcc.compile(mrPlan, "Test");
@@ -157,9 +155,8 @@ public class TestGroupConstParallel {
         pp.addAsLeaf(store);
         MROperPlan mrPlan = Util.buildMRPlan(pp, pc);
 
-        HExecutionEngine exe = pc.getExecutionEngine();
-        ConfigurationValidator.validatePigProperties(exe.getConfiguration());
-        Configuration conf = ConfigurationUtil.toConfiguration(exe.getConfiguration());
+        ConfigurationValidator.validatePigProperties(pc.getProperties());
+        Configuration conf = ConfigurationUtil.toConfiguration(pc.getProperties());
         JobControlCompiler jcc = new JobControlCompiler(pc, conf);
         
         JobControl jobControl = jcc.compile(mrPlan, "Test");
diff --git a/test/org/apache/pig/test/TestJobSubmission.java b/test/org/apache/pig/test/TestJobSubmission.java
index 590bfdc93..73c63cb02 100644
--- a/test/org/apache/pig/test/TestJobSubmission.java
+++ b/test/org/apache/pig/test/TestJobSubmission.java
@@ -18,7 +18,6 @@
 package org.apache.pig.test;
 
 import java.io.File;
-import java.io.IOException;
 import java.util.Iterator;
 import java.util.Random;
 
@@ -32,7 +31,6 @@ import org.apache.hadoop.mapred.jobcontrol.JobControl;
 import org.apache.pig.ExecType;
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
-import org.apache.pig.backend.hadoop.executionengine.HExecutionEngine;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobCreationException;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher;
@@ -465,9 +463,8 @@ public class TestJobSubmission extends junit.framework.TestCase{
     		}
     	}
     	
-        HExecutionEngine exe = pc.getExecutionEngine();
-        ConfigurationValidator.validatePigProperties(exe.getConfiguration());
-        Configuration conf = ConfigurationUtil.toConfiguration(exe.getConfiguration());
+        ConfigurationValidator.validatePigProperties(pc.getProperties());
+        Configuration conf = ConfigurationUtil.toConfiguration(pc.getProperties());
         JobControlCompiler jcc = new JobControlCompiler(pc, conf);
         try {
         	jcc.compile(mrPlan, "Test");
@@ -488,9 +485,8 @@ public class TestJobSubmission extends junit.framework.TestCase{
         pp.addAsLeaf(store);
         MROperPlan mrPlan = Util.buildMRPlan(pp, pc);
 
-        HExecutionEngine exe = pc.getExecutionEngine();
-        ConfigurationValidator.validatePigProperties(exe.getConfiguration());
-        Configuration conf = ConfigurationUtil.toConfiguration(exe.getConfiguration());
+        ConfigurationValidator.validatePigProperties(pc.getProperties());
+        Configuration conf = ConfigurationUtil.toConfiguration(pc.getProperties());
         JobControlCompiler jcc = new JobControlCompiler(pc, conf);
         
         JobControl jobControl = jcc.compile(mrPlan, "Test");
@@ -514,11 +510,6 @@ public class TestJobSubmission extends junit.framework.TestCase{
         pp.addAsLeaf(store);
         MROperPlan mrPlan = Util.buildMRPlan(pp, pc);
 
-        HExecutionEngine exe = pc.getExecutionEngine();
-        ConfigurationValidator.validatePigProperties(exe.getConfiguration());
-        Configuration conf = ConfigurationUtil.toConfiguration(exe.getConfiguration());
-        JobControlCompiler jcc = new JobControlCompiler(pc, conf);
-        
         // Get the sort job
         Iterator<MapReduceOper> iter = mrPlan.getKeys().values().iterator();
         int counter = 0;
@@ -546,11 +537,6 @@ public class TestJobSubmission extends junit.framework.TestCase{
         POStore store = GenPhyOp.dummyPigStorageOp();
         pp.addAsLeaf(store);
         MROperPlan mrPlan = Util.buildMRPlan(pp, pc);
-
-        HExecutionEngine exe = pc.getExecutionEngine();
-        ConfigurationValidator.validatePigProperties(exe.getConfiguration());
-        Configuration conf = ConfigurationUtil.toConfiguration(exe.getConfiguration());
-        JobControlCompiler jcc = new JobControlCompiler(pc, conf);
         
         // Get the skew join job
         Iterator<MapReduceOper> iter = mrPlan.getKeys().values().iterator();
@@ -580,9 +566,8 @@ public class TestJobSubmission extends junit.framework.TestCase{
                
         pc.getConf().setProperty("pig.exec.reducers.bytes.per.reducer", "100");
         pc.getConf().setProperty("pig.exec.reducers.max", "10");
-        HExecutionEngine exe = pc.getExecutionEngine();
-        ConfigurationValidator.validatePigProperties(exe.getConfiguration());
-        Configuration conf = ConfigurationUtil.toConfiguration(exe.getConfiguration());
+        ConfigurationValidator.validatePigProperties(pc.getProperties());
+        Configuration conf = ConfigurationUtil.toConfiguration(pc.getProperties());
         JobControlCompiler jcc = new JobControlCompiler(pc, conf);
         JobControl jc=jcc.compile(mrPlan, "Test");
         Job job = jc.getWaitingJobs().get(0);
@@ -600,9 +585,8 @@ public class TestJobSubmission extends junit.framework.TestCase{
                
         pc.getConf().setProperty("pig.exec.reducers.bytes.per.reducer", "100");
         pc.getConf().setProperty("pig.exec.reducers.max", "10");
-        exe = pc.getExecutionEngine();
-        ConfigurationValidator.validatePigProperties(exe.getConfiguration());
-        conf = ConfigurationUtil.toConfiguration(exe.getConfiguration());
+        ConfigurationValidator.validatePigProperties(pc.getProperties());
+        conf = ConfigurationUtil.toConfiguration(pc.getProperties());
         jcc = new JobControlCompiler(pc, conf);
         jc=jcc.compile(mrPlan, "Test");
         job = jc.getWaitingJobs().get(0);
@@ -623,9 +607,9 @@ public class TestJobSubmission extends junit.framework.TestCase{
                 
         pc.getConf().setProperty("pig.exec.reducers.bytes.per.reducer", "100");
         pc.getConf().setProperty("pig.exec.reducers.max", "10");
-        exe = pc.getExecutionEngine();
-        ConfigurationValidator.validatePigProperties(exe.getConfiguration());
-        conf = ConfigurationUtil.toConfiguration(exe.getConfiguration());
+
+        ConfigurationValidator.validatePigProperties(pc.getProperties());
+        conf = ConfigurationUtil.toConfiguration(pc.getProperties());
         jcc = new JobControlCompiler(pc, conf);
         jc=jcc.compile(mrPlan, "Test");
         job = jc.getWaitingJobs().get(0);
diff --git a/test/org/apache/pig/test/TestMultiQueryLocal.java b/test/org/apache/pig/test/TestMultiQueryLocal.java
index dfb0a389b..873bd7923 100644
--- a/test/org/apache/pig/test/TestMultiQueryLocal.java
+++ b/test/org/apache/pig/test/TestMultiQueryLocal.java
@@ -20,6 +20,7 @@ package org.apache.pig.test;
 import java.io.StringReader;
 import java.io.IOException;
 import java.io.File;
+import java.util.Iterator;
 import java.util.List;
 import java.util.ArrayList;
 import java.util.Collections;
@@ -32,6 +33,7 @@ import org.apache.pig.ExecType;
 import org.apache.pig.PigException;
 import org.apache.pig.PigServer;
 import org.apache.pig.backend.executionengine.ExecJob;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
 import org.apache.pig.impl.io.FileLocalizer;
 import org.apache.pig.impl.plan.Operator;
@@ -41,6 +43,8 @@ import org.apache.pig.tools.grunt.GruntParser;
 import org.apache.pig.impl.util.LogUtils;
 import org.apache.pig.newplan.logical.relational.LogicalPlan;
 import org.apache.pig.tools.pigscript.parser.ParseException;
+import org.apache.pig.tools.pigstats.JobStats;
+import org.apache.pig.tools.pigstats.PigStats;
 import org.junit.After;
 import org.junit.Before;
 import org.junit.Test;
@@ -544,9 +548,18 @@ public class TestMultiQueryLocal {
 
     private boolean executePlan(PhysicalPlan pp) throws IOException {
         boolean failed = true;
-        List<ExecJob> jobs = myPig.getPigContext().getExecutionEngine().execute(pp, "execute");
-        for (ExecJob job: jobs) {
-            failed = (job.getStatus() == ExecJob.JOB_STATUS.FAILED);
+        MapReduceLauncher launcher = new MapReduceLauncher();
+        PigStats stats = null;
+        try {
+            stats = launcher.launchPig(pp, "execute", myPig.getPigContext());
+        } catch (Exception e) {
+            e.printStackTrace(System.out);
+            throw new IOException(e);
+        }
+        Iterator<JobStats> iter = stats.getJobGraph().iterator();
+        while (iter.hasNext()) {
+            JobStats js = iter.next();
+            failed = !js.isSuccessful();
             if (failed) {
                 break;
             }
diff --git a/test/org/apache/pig/test/TestPigRunner.java b/test/org/apache/pig/test/TestPigRunner.java
index ca1d794db..ce13c8ab3 100644
--- a/test/org/apache/pig/test/TestPigRunner.java
+++ b/test/org/apache/pig/test/TestPigRunner.java
@@ -567,6 +567,53 @@ public class TestPigRunner {
         }
     }
     
+    @Test
+    public void returnCodeTest() throws Exception {
+        PrintWriter w = new PrintWriter(new FileWriter(PIG_FILE));
+        w.println("A = load 'non-existine.file' as (a0:int, a1:int, a2:int);");
+        w.println("B = filter A by a0 > 0;;");
+        w.println("C = group B by $0;");
+        w.println("D = join C by $0, B by $0;");
+        w.println("store D into '" + OUTPUT_FILE + "';");
+        w.close();
+        
+        try {
+            String[] args = { PIG_FILE };
+            PigStats stats = PigRunner.run(args, null);
+            
+            assertTrue(!stats.isSuccessful());
+            assertTrue(stats.getReturnCode() != 0);
+            assertTrue(stats.getOutputStats().size() == 0);
+            
+        } finally {
+            new File(PIG_FILE).delete();
+            Util.deleteFile(cluster, OUTPUT_FILE);
+        }
+    }
+    
+    @Test
+    public void returnCodeTest2() throws Exception {
+        PrintWriter w = new PrintWriter(new FileWriter(PIG_FILE));
+        w.println("A = load 'non-existine.file' as (a0, a1);");
+        w.println("B = load 'data' as (b0, b1);");
+        w.println("C = join B by b0, A by a0 using 'repl';");
+        w.println("store C into '" + OUTPUT_FILE + "';");
+        w.close();
+        
+        try {
+            String[] args = { PIG_FILE };
+            PigStats stats = PigRunner.run(args, null);
+            
+            assertTrue(!stats.isSuccessful());
+            assertTrue(stats.getReturnCode() != 0);
+            assertTrue(stats.getOutputStats().size() == 0);
+            
+        } finally {
+            new File(PIG_FILE).delete();
+            Util.deleteFile(cluster, OUTPUT_FILE);
+        }
+    }
+    
     public static class TestNotificationListener implements PigProgressNotificationListener {
         
         private Map<String, int[]> numMap = new HashMap<String, int[]>();
