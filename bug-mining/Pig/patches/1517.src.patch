diff --git a/CHANGES.txt b/CHANGES.txt
index abf54055a..ece6d8b69 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -50,6 +50,8 @@ OPTIMIZATIONS
  
 BUG FIXES
 
+PIG-4074: mapreduce.client.submit.file.replication is not honored in cached files (cheolsoo)
+
 PIG-4052: TestJobControlSleep, TestInvokerSpeed are unreliable (daijy)
 
 PIG-4053: TestMRCompiler succeeded with sun jdk 1.6 while failed with sun jdk 1.7 (daijy)
diff --git a/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/HadoopJobHistoryLoader.java b/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/HadoopJobHistoryLoader.java
index 19045fee2..9c937f9a5 100644
--- a/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/HadoopJobHistoryLoader.java
+++ b/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/HadoopJobHistoryLoader.java
@@ -52,6 +52,7 @@ import org.apache.hadoop.mapreduce.TaskAttemptContext;
 import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
 import org.apache.hadoop.mapreduce.lib.input.FileSplit;
 import org.apache.pig.LoadFunc;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigSplit;
 import org.apache.pig.data.DefaultTupleFactory;
 import org.apache.pig.data.Tuple;
@@ -496,11 +497,11 @@ public class HadoopJobHistoryLoader extends LoadFunc {
     private static final Map<String, String> XML_KEYS;
     
     static {
-        XML_KEYS = new HashMap<String, String> ();       
+        XML_KEYS = new HashMap<String, String> ();
+        XML_KEYS.put(MRConfiguration.JOB_QUEUE_NAME, "QUEUE_NAME");
         XML_KEYS.put("group.name", "USER_GROUP");
         XML_KEYS.put("user.name", "USER");
         XML_KEYS.put("user.dir", "HOST_DIR");
-        XML_KEYS.put("mapred.job.queue.name", "QUEUE_NAME");
         XML_KEYS.put("cluster", "CLUSTER");
         XML_KEYS.put("jobName", "JOB_NAME");
         XML_KEYS.put("pig.script.id", "PIG_SCRIPT_ID");
diff --git a/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/IndexedStorage.java b/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/IndexedStorage.java
index b2096fb08..47c6105eb 100644
--- a/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/IndexedStorage.java
+++ b/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/IndexedStorage.java
@@ -6,9 +6,9 @@
  * to you under the Apache License, Version 2.0 (the
  * "License"); you may not use this file except in compliance
  * with the License.  You may obtain a copy of the License at
- * 
+ *
  *   http://www.apache.org/licenses/LICENSE-2.0
- * 
+ *
  * Unless required by applicable law or agreed to in writing,
  * software distributed under the License is distributed on an
  * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
@@ -47,6 +47,7 @@ import org.apache.hadoop.mapreduce.TaskAttemptContext;
 import org.apache.hadoop.mapreduce.TaskAttemptID;
 import org.apache.hadoop.mapreduce.lib.input.FileSplit;
 import org.apache.pig.IndexableLoadFunc;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigTextInputFormat;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigTextOutputFormat;
 import org.apache.pig.backend.hadoop.executionengine.shims.HadoopShims;
@@ -71,7 +72,7 @@ import org.apache.pig.backend.hadoop.executionengine.shims.HadoopShims;
  * | Footer     |
  * </pre>
  * The Header contains the list of record indices (field numbers) that represent index keys.
- * The Index Body contains a <code>Tuple</code> for each record in the data.  
+ * The Index Body contains a <code>Tuple</code> for each record in the data.
  * The fields of the <code>Tuple</code> are:
  * <ul>
  * <li> The index key(s) <code>Tuple</code> </li>
@@ -84,7 +85,7 @@ import org.apache.pig.backend.hadoop.executionengine.shims.HadoopShims;
  * <li> The largest key(s) <code>Tuple</code> in the index. </li>
  * <li> The offset in bytes to the start of the footer </li>
  * </ul>
- * 
+ *
  * <code>IndexStorage</code> implements <code>IndexableLoadFunc</code> and
  * can be used as the 'right table' in a PIG 'merge' or 'merge-sparse' join.
  *
@@ -95,966 +96,962 @@ import org.apache.pig.backend.hadoop.executionengine.shims.HadoopShims;
  */
 public class IndexedStorage extends PigStorage implements IndexableLoadFunc {
 
-	/**
-	 * Constructs a Pig Storer that uses specified regex as a field delimiter.
-	 * @param delimiter - field delimiter to use
-	 * @param offsetsToIndexKeys - list of offset into Tuple for index keys (comma separated)
-	 */
-	public IndexedStorage(String delimiter, String offsetsToIndexKeys) {
-		super(delimiter);
-
-		this.fieldDelimiter = StorageUtil.parseFieldDel(delimiter);
-
-		String[] stroffsetsToIndexKeys = offsetsToIndexKeys.split(",");
-		this.offsetsToIndexKeys = new int[stroffsetsToIndexKeys.length];
-		for (int i = 0; i < stroffsetsToIndexKeys.length; ++i) {
-			this.offsetsToIndexKeys[i] = Integer.parseInt(stroffsetsToIndexKeys[i]);
-		}
-	}
-
-	@Override
-	public OutputFormat getOutputFormat() {
-		return new IndexedStorageOutputFormat(fieldDelimiter, offsetsToIndexKeys);
-	}
-
-	/**
-	 * Assumes this list of readers is already sorted except for the provided element. 
-         * This element is bubbled up the array to its appropriate sort location 
+    /**
+     * Constructs a Pig Storer that uses specified regex as a field delimiter.
+     * @param delimiter - field delimiter to use
+     * @param offsetsToIndexKeys - list of offset into Tuple for index keys (comma separated)
+     */
+    public IndexedStorage(String delimiter, String offsetsToIndexKeys) {
+        super(delimiter);
+
+        this.fieldDelimiter = StorageUtil.parseFieldDel(delimiter);
+
+        String[] stroffsetsToIndexKeys = offsetsToIndexKeys.split(",");
+        this.offsetsToIndexKeys = new int[stroffsetsToIndexKeys.length];
+        for (int i = 0; i < stroffsetsToIndexKeys.length; ++i) {
+            this.offsetsToIndexKeys[i] = Integer.parseInt(stroffsetsToIndexKeys[i]);
+        }
+    }
+
+    @Override
+    public OutputFormat getOutputFormat() {
+        return new IndexedStorageOutputFormat(fieldDelimiter, offsetsToIndexKeys);
+    }
+
+    /**
+     * Assumes this list of readers is already sorted except for the provided element.
+         * This element is bubbled up the array to its appropriate sort location
          * (faster than doing a Utils sort).
-	 */
-	private void sortReader(int startIndex) {
-		int idx = startIndex;
-		while (idx < this.readers.length - 1) {
-			IndexedStorageRecordReader reader1 = this.readers[idx];
-			IndexedStorageRecordReader reader2 = this.readers[idx+1];
-			if (this.readerComparator.compare(reader1, reader2) <= 0) {
-				return;
-			}
-			this.readers[idx] = reader2;
-			this.readers[idx+1] = reader1;
-			idx++;
-		}
-	}
-
-	/**
-	 * Internal OutputFormat class
-	 */
-	public static class IndexedStorageOutputFormat extends PigTextOutputFormat {
-
-		public IndexedStorageOutputFormat(byte delimiter, int[] offsetsToIndexKeys) {
-			/* Call the base class constructor */
-			super(delimiter);
-
-			this.fieldDelimiter = delimiter;
-			this.offsetsToIndexKeys = offsetsToIndexKeys;
-		}
-
-		@Override
-		public RecordWriter<WritableComparable, Tuple> getRecordWriter(
-				TaskAttemptContext context) throws IOException,
-				InterruptedException {
-
-			Configuration conf = context.getConfiguration();
-
-			FileSystem fs = FileSystem.get(conf);
-			Path file = this.getDefaultWorkFile(context, "");    
-			FSDataOutputStream fileOut = fs.create(file, false);
-
-			IndexManager indexManager = new IndexManager(offsetsToIndexKeys);
-			indexManager.createIndexFile(fs, file);
-			return new IndexedStorageRecordWriter(fileOut, this.fieldDelimiter, indexManager);
-		}
-
-		/**
-		 * Internal class to do the actual record writing and index generation
-		 * 
-		 */
-		public static class IndexedStorageRecordWriter extends PigLineRecordWriter {
-
-			public IndexedStorageRecordWriter(FSDataOutputStream fileOut, byte fieldDel, IndexManager indexManager) throws IOException {
-				super(fileOut, fieldDel);
-
-				this.fileOut = fileOut;
-				this.indexManager = indexManager;
-
-				/* Write the index header first */
-				this.indexManager.WriteIndexHeader();
-			}
-
-			@Override
-			public void write(WritableComparable key, Tuple value) throws IOException {
-				/* Write the data */
-				long offset = this.fileOut.getPos();
-				super.write(key, value);
-
-				/* Build index */
-				this.indexManager.BuildIndex(value, offset);	
-			}
-
-			@Override
-			public void close(TaskAttemptContext context)
-			throws IOException {
-				this.indexManager.WriterIndexFooter();
-				this.indexManager.Close();
-				super.close(context);
-			}
-
-			/**
-			 * Output stream for data
-			 */
-			private FSDataOutputStream fileOut;
-
-			/**
-			 * Index builder
-			 */
-			private IndexManager indexManager = null;
-		}
-
-		/**
-		 * Delimiter to use between fields
-		 */
-		final private byte fieldDelimiter;
-
-		/**
-		 * Offsets to index keys in given tuple
-		 */
-		final protected int[] offsetsToIndexKeys;
-	}
-
-
-	@Override
-	public InputFormat getInputFormat() {
-		return new IndexedStorageInputFormat();
-	}
-	
-	@Override
-	public Tuple getNext() throws IOException {
-		if (this.readers == null) {
-			return super.getNext();
-		}
-
-		while (currentReaderIndexStart < this.readers.length) {
-			IndexedStorageRecordReader r = this.readers[currentReaderIndexStart];
-
-			this.prepareToRead(r, null);
-			Tuple tuple = super.getNext();
-			if (tuple == null) {
-				currentReaderIndexStart++;
-				r.close();
-				continue; //next Reader
-			} 
-		
-			//if we haven't yet initialized the indexManager (by reading the first index key)	
-			if (r.indexManager.lastIndexKeyTuple == null) {
-
-				//initialize the indexManager
-				if (r.indexManager.ReadIndex() == null) {
-					//There should never be a case where there is a non-null record - but no corresponding index.
-					throw new IOException("Missing Index for Tuple: " + tuple);
-				}
-			}
-
-			r.indexManager.numberOfTuples--;
-
-			if (r.indexManager.numberOfTuples == 0) {
-				if (r.indexManager.ReadIndex() == null) {
-					r.close();
-					currentReaderIndexStart++;
-				} else {
-					//Since the index of the current reader was increased, we may need to push the
-					//current reader back in the sorted list of readers.
-					sortReader(currentReaderIndexStart);
-				}
-			}
-			return tuple;
-		}
-
-		return null;
-	}
-
-	/**
-	 * IndexableLoadFunc interface implementation
-	 */
-	@Override
-	public void initialize(Configuration conf) throws IOException {
-		try {
-			InputFormat inputFormat = this.getInputFormat();
-			TaskAttemptID id = HadoopShims.getNewTaskAttemptID();
-			
-			if (System.getenv("HADOOP_TOKEN_FILE_LOCATION") != null) {
-		                conf.set("mapreduce.job.credentials.binary", System.getenv("HADOOP_TOKEN_FILE_LOCATION"));
-			}
-			List<FileSplit> fileSplits = inputFormat.getSplits(HadoopShims.createJobContext(conf, null));
-			this.readers = new IndexedStorageRecordReader[fileSplits.size()];
-			
-			int idx = 0;
-			Iterator<FileSplit> it = fileSplits.iterator();
-			while (it.hasNext()) {
-				FileSplit fileSplit = it.next();
-				TaskAttemptContext context = HadoopShims.createTaskAttemptContext(conf, id);
-				IndexedStorageRecordReader r = (IndexedStorageRecordReader) inputFormat.createRecordReader(fileSplit, context);
-				r.initialize(fileSplit, context);
-				this.readers[idx] = r;
-				idx++;
-			}
-
-			Arrays.sort(this.readers, this.readerComparator);
-		} catch (InterruptedException e) {
-			throw new IOException(e);
-		}		
-	}
-
-	@Override
-	/* The list of readers is always sorted before and after this call. */
-	public void seekNear(Tuple keys) throws IOException {
-
-		/* Keeps track of the last (if any) reader where seekNear was called */
-		int lastIndexModified = -1;
-
-		int idx = currentReaderIndexStart;
-		while (idx < this.readers.length) {
-			IndexedStorageRecordReader r = this.readers[idx];
-
-			/* The key falls within the range of the reader index */
-			if (keys.compareTo(r.indexManager.maxIndexKeyTuple) <= 0 && keys.compareTo(r.indexManager.minIndexKeyTuple) >= 0) {
-				r.seekNear(keys);
-				lastIndexModified = idx;
-
-			/* The key is greater than the current range of the reader index */
-			} else if (keys.compareTo(r.indexManager.maxIndexKeyTuple) > 0) {
-				currentReaderIndexStart++;
-			/* DO NOTHING - The key is less than the current range of the reader index */
-			} else {
-				break;
-			}
-			idx++;
-		}
-
-		/* 
-		 * There is something to sort.  
-		 * We can rely on the following invariants that make the following check accurate:
- 		 *  - currentReaderIndexStart is always >= 0.
-		 *  - lastIndexModified is only positive if seekNear was called.
-		 *  - lastIndexModified >= currentReaderIndexStart if lastIndexModifed >= 0.  This is true because the list
- 		 * is already sorted.
-		 */
-		if (lastIndexModified - currentReaderIndexStart >= 0) {
-
-			/*
-			 * The following logic is optimized for the (common) case where there are a tiny number of readers that
-		         * need to be repositioned relative to the other readers in the much larger sorted list.
-			 */
-
-			/* First, just sort the readers that were updated relative to one another. */
-			Arrays.sort(this.readers, currentReaderIndexStart, lastIndexModified+1, this.readerComparator);
-
-			/* In descending order, push the updated readers back in the the sorted list. */
-			for (idx = lastIndexModified; idx >= currentReaderIndexStart; idx--) {
-				sortReader(idx);
-			}
-		}
-	}
-
-
-	@Override
-	public void close() throws IOException {
-		for (IndexedStorageRecordReader reader : this.readers) {
-			reader.close();
-		}
-	}
-
-	/**
-	 * <code>IndexManager</code> manages the index file (both writing and reading)
-	 * It keeps track of the last index read during reading.
-	 */
-	public static class IndexManager {
-
-		/**
-		 * Constructor (called during reading)
-		 * @param ifile index file to read
-		 */
-		public IndexManager(FileStatus ifile) {
-			this.indexFile = ifile;
-			this.offsetToFooter = -1;
-		}
-
-		/**
-		 * Constructor (called during writing)
-		 * @param offsetsToIndexKeys
-		 */
-		public IndexManager(int[] offsetsToIndexKeys) {
-			this.offsetsToIndexKeys = offsetsToIndexKeys;
-			this.offsetToFooter = -1;
-		}
-
-		/**
-		 * Construct index file path for a given a data file
-		 * @param file - Data file
-		 * @return - Index file path for given data file
-		 */
-		private static Path getIndexFileName(Path file) {
-			return new Path(file.getParent(), "." + file.getName() + ".index");
-		}
-
-		/**
-		 * Open the index file for writing for given data file
-		 * @param fs
-		 * @param file
-		 * @throws IOException
-		 */
-		public void createIndexFile(FileSystem fs, Path file) throws IOException {
-			this.indexOut = fs.create(IndexManager.getIndexFileName(file), false);	
-		}
-
-		/**
-	 	 * Opens the index file.
-		 */
-		public void openIndexFile(FileSystem fs) throws IOException {
-			this.indexIn = fs.open(this.indexFile.getPath());	
-		}
-
-		/**
-		 * Close the index file
-		 * @throws IOException
-		 */
-		public void Close() throws IOException {
-			this.indexOut.close();
-		}
-
-		/**
-		 * Build index tuple
-		 * 
-		 * @throws IOException
-		 */
-		private void BuildIndex(Tuple t, long offset) throws IOException {
-			/* Build index key tuple */
-			Tuple indexKeyTuple = tupleFactory.newTuple(this.offsetsToIndexKeys.length);
-			for (int i = 0; i < this.offsetsToIndexKeys.length; ++i) {
-				indexKeyTuple.set(i, t.get(this.offsetsToIndexKeys[i]));
-			}
-
-			/* Check if we have already seen Tuple(s) with same index keys */
-			if (indexKeyTuple.compareTo(this.lastIndexKeyTuple) == 0) {
-				/* We have seen Tuple(s) with given index keys, update the tuple count */
-				this.numberOfTuples += 1;
-			}
-			else {
-				if (this.lastIndexKeyTuple != null)
-					this.WriteIndex();
-
-				this.lastIndexKeyTuple = indexKeyTuple;
-				this.minIndexKeyTuple = ((this.minIndexKeyTuple == null) || (indexKeyTuple.compareTo(this.minIndexKeyTuple) < 0)) ? indexKeyTuple : this.minIndexKeyTuple;
-				this.maxIndexKeyTuple = ((this.maxIndexKeyTuple == null) || (indexKeyTuple.compareTo(this.maxIndexKeyTuple) > 0)) ? indexKeyTuple : this.maxIndexKeyTuple;
-
-				/* New index tuple for newly seen index key */
-				this.indexTuple = tupleFactory.newTuple(3);
-
-				/* Add index keys to index Tuple */
-				this.indexTuple.set(0, indexKeyTuple);
-
-				/* Reset Tuple count for index key */
-				this.numberOfTuples = 1;
-
-				/* Remember offset to Tuple with new index keys */
-				this.indexTuple.set(2, offset);
-			}
-		}
-
-
-		/**
-		 * Write index header
-		 * @param indexOut - Stream to write to
-		 * @param ih - Index header to write
-		 * @throws IOException
-		 */
-		public void WriteIndexHeader() throws IOException {
-			/* Number of index keys */
-			indexOut.writeInt(this.offsetsToIndexKeys.length);
-
-			/* Offset to index keys */
-			for (int i = 0; i < this.offsetsToIndexKeys.length; ++i) {
-				indexOut.writeInt(this.offsetsToIndexKeys[i]);
-			}
-		}
-
-		/**
-		 * Read index header
-		 * @param indexIn - Stream to read from
-		 * @return Index header
-		 * @throws IOException
-		 */
-		public void ReadIndexHeader() throws IOException {
-			/* Number of index keys */
-			int nkeys = this.indexIn.readInt();
-
-			/* Offset to index keys */
-			this.offsetsToIndexKeys = new int[nkeys];
-			for (int i = 0; i < nkeys; ++i) {
-				offsetsToIndexKeys[i] = this.indexIn.readInt();
-			}
-		}
-
-		/**
-		 * Writes the index footer
-		 */
-		public void WriterIndexFooter() throws IOException {
-			/* Flush indexes for remaining records */
-			this.WriteIndex();
-
-			/* record the offset to footer */
-			this.offsetToFooter = this.indexOut.getPos();
-
-			/* Write index footer */
-			DataReaderWriter.writeDatum(indexOut, this.minIndexKeyTuple);
-			DataReaderWriter.writeDatum(indexOut, this.maxIndexKeyTuple);
-
-			/* Offset to footer */
-			indexOut.writeLong(this.offsetToFooter);
-		}
-
-		/**
-		 * Reads the index footer
-		 */
-		public void ReadIndexFooter() throws IOException {
-			long currentOffset = this.indexIn.getPos();
-
-			this.SeekToIndexFooter();
-			this.minIndexKeyTuple = (Tuple)DataReaderWriter.readDatum(this.indexIn);
-			this.maxIndexKeyTuple = (Tuple)DataReaderWriter.readDatum(this.indexIn);
-
-			this.indexIn.seek(currentOffset);
-		}
-
-		/**
-		 * Seeks to the index footer
-		 */
-		public void SeekToIndexFooter() throws IOException {
-			if (this.offsetToFooter < 0) {
-				/* offset to footer is at last long (8 bytes) in the file */
-				this.indexIn.seek(this.indexFile.getLen()-8);
-				this.offsetToFooter = this.indexIn.readLong();
-			}
-			this.indexIn.seek(this.offsetToFooter);
-		}
-
-		/**
-		 * Writes the current index.
-		 */
-		public void WriteIndex() throws IOException {
-			this.indexTuple.set(1, this.numberOfTuples);
-			DataReaderWriter.writeDatum(this.indexOut, this.indexTuple);
-		}
-
-		/**
-		 * Extracts the index key from the index tuple
-		 */
-		public Tuple getIndexKeyTuple(Tuple indexTuple) throws IOException {
-			if (indexTuple.size() == 3) 
-				return (Tuple)indexTuple.get(0);
-			else
-				throw new IOException("Invalid index record with size " + indexTuple.size());
-		}
-
-		/**
-		 * Extracts the number of records that share the current key from the index tuple.
-		 */
-		public long getIndexKeyTupleCount(Tuple indexTuple) throws IOException {
-			if (indexTuple.size() == 3) 
-				return (Long)indexTuple.get(1);
-			else
-				throw new IOException("Invalid index record with size " + indexTuple.size());
-		}
-
-		/**
-		 * Extracts the offset into the data file from the index tuple.
-		 */
-		public long getOffset(Tuple indexTuple) throws IOException {
-			if (indexTuple.size() == 3) 
-				return (Long)indexTuple.get(2);
-			else
-				throw new IOException("Invalid index record with size " + indexTuple.size());
-		}
-	
-		/**
-	 	 * Reads the next index from the index file (or null if EOF) and extracts
-		 * the index fields.
-		 */	
-		public Tuple ReadIndex() throws IOException {
-			if (this.indexIn.getPos() < this.offsetToFooter) {
-				indexTuple = (Tuple)DataReaderWriter.readDatum(this.indexIn);
-				if (indexTuple != null) {
-					this.lastIndexKeyTuple = this.getIndexKeyTuple(indexTuple);
-					this.numberOfTuples = this.getIndexKeyTupleCount(indexTuple);
-				}
-				return indexTuple;
-			}
-			return null;
-		}
-
-		/**
-		 * Scans the index looking for a given key.
-		 * @return the matching index tuple OR the last index tuple
-		 * greater than the requested key if no match is found.
-		 */
-		public Tuple ScanIndex(Tuple keys) throws IOException {
-			if (lastIndexKeyTuple != null && keys.compareTo(this.lastIndexKeyTuple) <= 0) {
-				return indexTuple;
-			}
-
-			/* Scan the index looking for given key */
-			while ((indexTuple = this.ReadIndex()) != null) {
-				if (keys.compareTo(this.lastIndexKeyTuple) > 0)
-					continue;
-				else
-					break;
-			}
-			
-			return indexTuple;
-		}
-
-		/**
-		 * stores the list of record indices that identify keys.
-		 */
-		private int[] offsetsToIndexKeys = null;
-
-		/**
-		 * offset in bytes to the start of the footer of the index.
-		 */
-		private long offsetToFooter = -1;
-
-		/**
-		 * output stream when writing the index.
-		 */
-		FSDataOutputStream indexOut;
-
-		/**
-		 * input stream when reading the index.
-		 */
-		FSDataInputStream indexIn;
-
-		/**
-		 * Tuple factory to create index tuples
-		 */
-		private TupleFactory tupleFactory = TupleFactory.getInstance();
-
-		/**
-		 * Index key tuple of the form
-		 * ((Tuple of index keys), count of tuples with index keys, offset to first tuple with index keys)
-		 */
-		private Tuple indexTuple = tupleFactory.newTuple(3);
-
-		/**
-		 * "Smallest" index key tuple seen
-		 */
-		private Tuple minIndexKeyTuple = null;
-
-		/**
-		 * "Biggest" index key tuple seen
-		 */
-		private Tuple maxIndexKeyTuple = null;
-
-		/**
-		 * Last seen index key tuple 
-		 */
-		private Tuple lastIndexKeyTuple = null;
-
-		/**
-		 * Number of tuples seen for a index key
-		 */
-		private long numberOfTuples = 0;
-
-		/**
-	 	 * The index file.
-		 */
-		private FileStatus indexFile;
-	}
-
-	/** 
-	 * Internal InputFormat class
-	 */
-	public static class IndexedStorageInputFormat extends PigTextInputFormat {
-
-		@Override
-		public RecordReader<LongWritable, Text> createRecordReader(InputSplit split, TaskAttemptContext context) {
-			IndexManager im = null;
-			try {
-				FileSystem fs = FileSystem.get(context.getConfiguration());
-				Path indexFile = IndexManager.getIndexFileName(((FileSplit)split).getPath());
-				im = new IndexManager(fs.getFileStatus(indexFile));
-				im.openIndexFile(fs);
-				im.ReadIndexHeader();
-				im.ReadIndexFooter();
-			} catch (IOException e) {
-				// TODO Auto-generated catch block
-				e.printStackTrace();
-			}
-
-			return new IndexedStorageRecordReader(im);
-		}
-		
-		@Override
-		public boolean isSplitable(JobContext context, Path filename) {
-			return false;
-		}
-
-		/**
-		 * Internal RecordReader class
-	 	 */
-		public static class IndexedStorageRecordReader extends RecordReader<LongWritable, Text> {
-			private long start;
-			private long pos;
-			private long end;
-			private IndexedStorageLineReader in;
-			private int maxLineLength;
-			private LongWritable key = null;
-			private Text value = null;
-			private IndexManager indexManager = null;
-
-			@Override
-			public String toString() {
-				return indexManager.minIndexKeyTuple + "|" + indexManager.lastIndexKeyTuple + "|" + indexManager.maxIndexKeyTuple;
-			}
-
-			public IndexedStorageRecordReader(IndexManager im) {
-				this.indexManager = im;
-			}
-
-			/**
-			 * Class to compare record readers using underlying indexes
-			 *
-			 */
-			public static class IndexedStorageRecordReaderComparator implements Comparator<IndexedStorageRecordReader> {
-				@Override
-				public int compare(IndexedStorageRecordReader o1, IndexedStorageRecordReader o2) {
-					Tuple t1 = (o1.indexManager.lastIndexKeyTuple == null) ?  o1.indexManager.minIndexKeyTuple : o1.indexManager.lastIndexKeyTuple;
-					Tuple t2 = (o2.indexManager.lastIndexKeyTuple == null) ?  o2.indexManager.minIndexKeyTuple : o2.indexManager.lastIndexKeyTuple;
-					return t1.compareTo(t2);
-				}
-			}
-
-			public static class IndexedStorageLineReader {
-				private static final int DEFAULT_BUFFER_SIZE = 64 * 1024;
-				private int bufferSize = DEFAULT_BUFFER_SIZE;
-				private InputStream in;
-				private byte[] buffer;
-				// the number of bytes of real data in the buffer
-				private int bufferLength = 0;
-				// the current position in the buffer
-				private int bufferPosn = 0;
-				private long bufferOffset = 0;
-
-				private static final byte CR = '\r';
-				private static final byte LF = '\n';
-
-				/**
-				 * Create a line reader that reads from the given stream using the
-				 * default buffer-size (64k).
-				 * @param in The input stream
-				 * @throws IOException
-				 */
-				public IndexedStorageLineReader(InputStream in) {
-					this(in, DEFAULT_BUFFER_SIZE);
-				}
-
-				/**
-				 * Create a line reader that reads from the given stream using the 
-				 * given buffer-size.
-				 * @param in The input stream
-				 * @param bufferSize Size of the read buffer
-				 * @throws IOException
-				 */
-				public IndexedStorageLineReader(InputStream in, int bufferSize) {
-					if( !(in instanceof Seekable) || !(in instanceof PositionedReadable) ) {
-					      throw new IllegalArgumentException(
-					          "In is not an instance of Seekable or PositionedReadable");
-					}
-					
-					this.in = in;
-					this.bufferSize = bufferSize;
-					this.buffer = new byte[this.bufferSize];
-				}
-
-				/**
-				 * Create a line reader that reads from the given stream using the
-				 * <code>io.file.buffer.size</code> specified in the given
-				 * <code>Configuration</code>.
-				 * @param in input stream
-				 * @param conf configuration
-				 * @throws IOException
-				 */
-				public IndexedStorageLineReader(InputStream in, Configuration conf) throws IOException {
-					this(in, conf.getInt("io.file.buffer.size", DEFAULT_BUFFER_SIZE));
-				}
-
-				/**
-				 * Close the underlying stream.
-				 * @throws IOException
-				 */
-				public void close() throws IOException {
-					in.close();
-				}
-
-				/**
-				 * Read one line from the InputStream into the given Text.  A line
-				 * can be terminated by one of the following: '\n' (LF) , '\r' (CR),
-				 * or '\r\n' (CR+LF).  EOF also terminates an otherwise unterminated
-				 * line.
-				 *
-				 * @param str the object to store the given line (without newline)
-				 * @param maxLineLength the maximum number of bytes to store into str;
-				 *  the rest of the line is silently discarded.
-				 * @param maxBytesToConsume the maximum number of bytes to consume
-				 *  in this call.  This is only a hint, because if the line cross
-				 *  this threshold, we allow it to happen.  It can overshoot
-				 *  potentially by as much as one buffer length.
-				 *
-				 * @return the number of bytes read including the (longest) newline
-				 * found.
-				 *
-				 * @throws IOException if the underlying stream throws
-				 */
-				public int readLine(Text str, int maxLineLength,
-						int maxBytesToConsume) throws IOException {
-					/* We're reading data from in, but the head of the stream may be
-					 * already buffered in buffer, so we have several cases:
-					 * 1. No newline characters are in the buffer, so we need to copy
-					 *    everything and read another buffer from the stream.
-					 * 2. An unambiguously terminated line is in buffer, so we just
-					 *    copy to str.
-					 * 3. Ambiguously terminated line is in buffer, i.e. buffer ends
-					 *    in CR.  In this case we copy everything up to CR to str, but
-					 *    we also need to see what follows CR: if it's LF, then we
-					 *    need consume LF as well, so next call to readLine will read
-					 *    from after that.
-					 * We use a flag prevCharCR to signal if previous character was CR
-					 * and, if it happens to be at the end of the buffer, delay
-					 * consuming it until we have a chance to look at the char that
-					 * follows.
-					 */
-					str.clear();
-					int txtLength = 0; //tracks str.getLength(), as an optimization
-					int newlineLength = 0; //length of terminating newline
-					boolean prevCharCR = false; //true of prev char was CR
-					long bytesConsumed = 0;
-					do {
-						int startPosn = bufferPosn; //starting from where we left off the last time
-						if (bufferPosn >= bufferLength) {
-							startPosn = bufferPosn = 0;
-							if (prevCharCR)
-								++bytesConsumed; //account for CR from previous read
-							
-							bufferOffset = ((Seekable)in).getPos();
-							bufferLength = in.read(buffer);
-							
-							if (bufferLength <= 0)
-								break; // EOF
-						}
-						for (; bufferPosn < bufferLength; ++bufferPosn) { //search for newline
-							if (buffer[bufferPosn] == LF) {
-								newlineLength = (prevCharCR) ? 2 : 1;
-								++bufferPosn; // at next invocation proceed from following byte
-								break;
-							}
-							if (prevCharCR) { //CR + notLF, we are at notLF
-								newlineLength = 1;
-								break;
-							}
-							prevCharCR = (buffer[bufferPosn] == CR);
-						}
-						int readLength = bufferPosn - startPosn;
-						if (prevCharCR && newlineLength == 0)
-							--readLength; //CR at the end of the buffer
-						bytesConsumed += readLength;
-						int appendLength = readLength - newlineLength;
-						if (appendLength > maxLineLength - txtLength) {
-							appendLength = maxLineLength - txtLength;
-						}
-						if (appendLength > 0) {
-							str.append(buffer, startPosn, appendLength);
-							txtLength += appendLength;
-						}
-					} while (newlineLength == 0 && bytesConsumed < maxBytesToConsume);
-
-					if (bytesConsumed > (long)Integer.MAX_VALUE)
-						throw new IOException("Too many bytes before newline: " + bytesConsumed);    
-					return (int)bytesConsumed;
-				}
-
-				/**
-				 * Read from the InputStream into the given Text.
-				 * @param str the object to store the given line
-				 * @param maxLineLength the maximum number of bytes to store into str.
-				 * @return the number of bytes read including the newline
-				 * @throws IOException if the underlying stream throws
-				 */
-				public int readLine(Text str, int maxLineLength) throws IOException {
-					return readLine(str, maxLineLength, Integer.MAX_VALUE);
-				}
-
-				/**
-				 * Read from the InputStream into the given Text.
-				 * @param str the object to store the given line
-				 * @return the number of bytes read including the newline
-				 * @throws IOException if the underlying stream throws
-				 */
-				public int readLine(Text str) throws IOException {
-					return readLine(str, Integer.MAX_VALUE, Integer.MAX_VALUE);
-				}
-				
-				/**
-				 * If given offset is within the buffer, adjust the buffer position to read from
-				 * otherwise seek to the given offset from start of the file.
-				 * @param offset
-				 * @throws IOException
-				 */
-				public void seek(long offset) throws IOException {
-					if ((offset >= bufferOffset) && (offset < (bufferOffset + bufferLength)))
-						bufferPosn = (int) (offset - bufferOffset);
-					else {
-						bufferPosn = bufferLength;
-						((Seekable)in).seek(offset);
-					}
-				}
-			}
-
-			@Override
-			public void initialize(InputSplit genericSplit, TaskAttemptContext context)
-			throws IOException, InterruptedException {
-
-				FileSplit split = (FileSplit) genericSplit;
-				Configuration job = context.getConfiguration();
-				this.maxLineLength = job.getInt("mapred.linerecordreader.maxlength", Integer.MAX_VALUE);
-				start = split.getStart();
-				end = start + split.getLength();
-				final Path file = split.getPath();
-
-				FileSystem fs = file.getFileSystem(job);
-				FSDataInputStream fileIn = fs.open(split.getPath());
-				boolean skipFirstLine = false;
-				if (start != 0) {
-					skipFirstLine = true;
-					--start;
-					fileIn.seek(start);
-				}
-				in = new IndexedStorageLineReader(fileIn, job);
-				if (skipFirstLine) {
-					start += in.readLine(new Text(), 0, (int)Math.min((long)Integer.MAX_VALUE, end - start));
-				}
-				this.pos = start;
-			}
-
-			public void seek(long offset) throws IOException {
-				in.seek(offset);
-				pos = offset;
-			}
-
-			/**
-			 * Scan the index for given key and seek to appropriate offset in the data 
-			 * @param keys to look for
-			 * @return true if the given key was found, false otherwise
-			 * @throws IOException
-			 */
-			public boolean seekNear(Tuple keys) throws IOException {
-				boolean ret = false;
-				Tuple indexTuple = this.indexManager.ScanIndex(keys);
-				if (indexTuple != null) {
-					long offset = this.indexManager.getOffset(indexTuple) ;
-					in.seek(offset);
-				
-					if (keys.compareTo(this.indexManager.getIndexKeyTuple(indexTuple)) == 0) {
-						ret = true;
-					}
-				}
-				
-				return ret;
-			}
-
-			@Override
-			public boolean nextKeyValue() throws IOException,
-			InterruptedException {
-				if (key == null) {
-					key = new LongWritable();
-				}
-				key.set(pos);
-				if (value == null) {
-					value = new Text();
-				}
-				int newSize = 0;
-				while (pos < end) {
-					newSize = in.readLine(value, maxLineLength,
-							Math.max((int)Math.min(Integer.MAX_VALUE, end-pos),
-									maxLineLength));
-					if (newSize == 0) {
-						break;
-					}
-					pos += newSize;
-					if (newSize < maxLineLength) {
-						break;
-					}
-				}
-				if (newSize == 0) {
-					key = null;
-					value = null;
-					return false;
-				} else {
-					return true;
-				}
-			}
-
-			@Override
-			public LongWritable getCurrentKey() throws IOException,
-			InterruptedException {
-				return key;
-			}
-
-			@Override
-			public Text getCurrentValue() throws IOException,
-			InterruptedException {
-				return value;
-			}
-
-			@Override
-			public float getProgress() throws IOException, InterruptedException {
-				if (start == end) {
-					return 0.0f;
-				} else {
-					return Math.min(1.0f, (pos - start) / (float)(end - start));
-				}
-			}
-
-			@Override
-			public void close() throws IOException {
-				if (in != null) {
-					in.close(); 
-				}
-			}
-		}
-	}
-
-
-	/**
-	 * List of record readers.
-	 */
-	protected IndexedStorageRecordReader[] readers = null;	
-
-	/**
-	 * Index into the the list of readers to the current reader.  
- 	 * Readers before this index have been fully scanned for keys.
-	 */
-	protected int currentReaderIndexStart = 0;
-
-	/**
-	 * Delimiter to use between fields
-	 */
-	protected byte fieldDelimiter = '\t';
-
-	/**
-	 * Offsets to index keys in tuple
-	 */
-	final protected int[] offsetsToIndexKeys;
-
-	/**
-	 * Comparator used to compare key tuples.
-	 */
-	protected Comparator<IndexedStorageRecordReader> readerComparator = new IndexedStorageInputFormat.IndexedStorageRecordReader.IndexedStorageRecordReaderComparator();
+     */
+    private void sortReader(int startIndex) {
+        int idx = startIndex;
+        while (idx < this.readers.length - 1) {
+            IndexedStorageRecordReader reader1 = this.readers[idx];
+            IndexedStorageRecordReader reader2 = this.readers[idx+1];
+            if (this.readerComparator.compare(reader1, reader2) <= 0) {
+                return;
+            }
+            this.readers[idx] = reader2;
+            this.readers[idx+1] = reader1;
+            idx++;
+        }
+    }
+
+    /**
+     * Internal OutputFormat class
+     */
+    public static class IndexedStorageOutputFormat extends PigTextOutputFormat {
+
+        public IndexedStorageOutputFormat(byte delimiter, int[] offsetsToIndexKeys) {
+            /* Call the base class constructor */
+            super(delimiter);
+
+            this.fieldDelimiter = delimiter;
+            this.offsetsToIndexKeys = offsetsToIndexKeys;
+        }
+
+        @Override
+        public RecordWriter<WritableComparable, Tuple> getRecordWriter(
+                TaskAttemptContext context) throws IOException,
+                InterruptedException {
+
+            Configuration conf = context.getConfiguration();
+
+            FileSystem fs = FileSystem.get(conf);
+            Path file = this.getDefaultWorkFile(context, "");
+            FSDataOutputStream fileOut = fs.create(file, false);
+
+            IndexManager indexManager = new IndexManager(offsetsToIndexKeys);
+            indexManager.createIndexFile(fs, file);
+            return new IndexedStorageRecordWriter(fileOut, this.fieldDelimiter, indexManager);
+        }
+
+        /**
+         * Internal class to do the actual record writing and index generation
+         *
+         */
+        public static class IndexedStorageRecordWriter extends PigLineRecordWriter {
+
+            public IndexedStorageRecordWriter(FSDataOutputStream fileOut, byte fieldDel, IndexManager indexManager) throws IOException {
+                super(fileOut, fieldDel);
+
+                this.fileOut = fileOut;
+                this.indexManager = indexManager;
+
+                /* Write the index header first */
+                this.indexManager.WriteIndexHeader();
+            }
+
+            @Override
+            public void write(WritableComparable key, Tuple value) throws IOException {
+                /* Write the data */
+                long offset = this.fileOut.getPos();
+                super.write(key, value);
+
+                /* Build index */
+                this.indexManager.BuildIndex(value, offset);
+            }
+
+            @Override
+            public void close(TaskAttemptContext context)
+            throws IOException {
+                this.indexManager.WriterIndexFooter();
+                this.indexManager.Close();
+                super.close(context);
+            }
+
+            /**
+             * Output stream for data
+             */
+            private FSDataOutputStream fileOut;
+
+            /**
+             * Index builder
+             */
+            private IndexManager indexManager = null;
+        }
+
+        /**
+         * Delimiter to use between fields
+         */
+        final private byte fieldDelimiter;
+
+        /**
+         * Offsets to index keys in given tuple
+         */
+        final protected int[] offsetsToIndexKeys;
+    }
+
+    @Override
+    public InputFormat getInputFormat() {
+        return new IndexedStorageInputFormat();
+    }
+
+    @Override
+    public Tuple getNext() throws IOException {
+        if (this.readers == null) {
+            return super.getNext();
+        }
+
+        while (currentReaderIndexStart < this.readers.length) {
+            IndexedStorageRecordReader r = this.readers[currentReaderIndexStart];
+
+            this.prepareToRead(r, null);
+            Tuple tuple = super.getNext();
+            if (tuple == null) {
+                currentReaderIndexStart++;
+                r.close();
+                continue; //next Reader
+            }
+
+            //if we haven't yet initialized the indexManager (by reading the first index key)
+            if (r.indexManager.lastIndexKeyTuple == null) {
+
+                //initialize the indexManager
+                if (r.indexManager.ReadIndex() == null) {
+                    //There should never be a case where there is a non-null record - but no corresponding index.
+                    throw new IOException("Missing Index for Tuple: " + tuple);
+                }
+            }
+
+            r.indexManager.numberOfTuples--;
+
+            if (r.indexManager.numberOfTuples == 0) {
+                if (r.indexManager.ReadIndex() == null) {
+                    r.close();
+                    currentReaderIndexStart++;
+                } else {
+                    //Since the index of the current reader was increased, we may need to push the
+                    //current reader back in the sorted list of readers.
+                    sortReader(currentReaderIndexStart);
+                }
+            }
+            return tuple;
+        }
+
+        return null;
+    }
+
+    /**
+     * IndexableLoadFunc interface implementation
+     */
+    @Override
+    public void initialize(Configuration conf) throws IOException {
+        try {
+            InputFormat inputFormat = this.getInputFormat();
+            TaskAttemptID id = HadoopShims.getNewTaskAttemptID();
+
+            if (System.getenv("HADOOP_TOKEN_FILE_LOCATION") != null) {
+                        conf.set(MRConfiguration.JOB_CREDENTIALS_BINARY, System.getenv("HADOOP_TOKEN_FILE_LOCATION"));
+            }
+            List<FileSplit> fileSplits = inputFormat.getSplits(HadoopShims.createJobContext(conf, null));
+            this.readers = new IndexedStorageRecordReader[fileSplits.size()];
+
+            int idx = 0;
+            Iterator<FileSplit> it = fileSplits.iterator();
+            while (it.hasNext()) {
+                FileSplit fileSplit = it.next();
+                TaskAttemptContext context = HadoopShims.createTaskAttemptContext(conf, id);
+                IndexedStorageRecordReader r = (IndexedStorageRecordReader) inputFormat.createRecordReader(fileSplit, context);
+                r.initialize(fileSplit, context);
+                this.readers[idx] = r;
+                idx++;
+            }
+
+            Arrays.sort(this.readers, this.readerComparator);
+        } catch (InterruptedException e) {
+            throw new IOException(e);
+        }
+    }
+
+    @Override
+    /* The list of readers is always sorted before and after this call. */
+    public void seekNear(Tuple keys) throws IOException {
+
+        /* Keeps track of the last (if any) reader where seekNear was called */
+        int lastIndexModified = -1;
+
+        int idx = currentReaderIndexStart;
+        while (idx < this.readers.length) {
+            IndexedStorageRecordReader r = this.readers[idx];
+
+            /* The key falls within the range of the reader index */
+            if (keys.compareTo(r.indexManager.maxIndexKeyTuple) <= 0 && keys.compareTo(r.indexManager.minIndexKeyTuple) >= 0) {
+                r.seekNear(keys);
+                lastIndexModified = idx;
+
+            /* The key is greater than the current range of the reader index */
+            } else if (keys.compareTo(r.indexManager.maxIndexKeyTuple) > 0) {
+                currentReaderIndexStart++;
+            /* DO NOTHING - The key is less than the current range of the reader index */
+            } else {
+                break;
+            }
+            idx++;
+        }
+
+        /*
+         * There is something to sort.
+         * We can rely on the following invariants that make the following check accurate:
+          *  - currentReaderIndexStart is always >= 0.
+         *  - lastIndexModified is only positive if seekNear was called.
+         *  - lastIndexModified >= currentReaderIndexStart if lastIndexModifed >= 0.  This is true because the list
+          * is already sorted.
+         */
+        if (lastIndexModified - currentReaderIndexStart >= 0) {
+
+            /*
+             * The following logic is optimized for the (common) case where there are a tiny number of readers that
+                 * need to be repositioned relative to the other readers in the much larger sorted list.
+             */
+
+            /* First, just sort the readers that were updated relative to one another. */
+            Arrays.sort(this.readers, currentReaderIndexStart, lastIndexModified+1, this.readerComparator);
+
+            /* In descending order, push the updated readers back in the the sorted list. */
+            for (idx = lastIndexModified; idx >= currentReaderIndexStart; idx--) {
+                sortReader(idx);
+            }
+        }
+    }
+
+    @Override
+    public void close() throws IOException {
+        for (IndexedStorageRecordReader reader : this.readers) {
+            reader.close();
+        }
+    }
+
+    /**
+     * <code>IndexManager</code> manages the index file (both writing and reading)
+     * It keeps track of the last index read during reading.
+     */
+    public static class IndexManager {
+
+        /**
+         * Constructor (called during reading)
+         * @param ifile index file to read
+         */
+        public IndexManager(FileStatus ifile) {
+            this.indexFile = ifile;
+            this.offsetToFooter = -1;
+        }
+
+        /**
+         * Constructor (called during writing)
+         * @param offsetsToIndexKeys
+         */
+        public IndexManager(int[] offsetsToIndexKeys) {
+            this.offsetsToIndexKeys = offsetsToIndexKeys;
+            this.offsetToFooter = -1;
+        }
+
+        /**
+         * Construct index file path for a given a data file
+         * @param file - Data file
+         * @return - Index file path for given data file
+         */
+        private static Path getIndexFileName(Path file) {
+            return new Path(file.getParent(), "." + file.getName() + ".index");
+        }
+
+        /**
+         * Open the index file for writing for given data file
+         * @param fs
+         * @param file
+         * @throws IOException
+         */
+        public void createIndexFile(FileSystem fs, Path file) throws IOException {
+            this.indexOut = fs.create(IndexManager.getIndexFileName(file), false);
+        }
+
+        /**
+          * Opens the index file.
+         */
+        public void openIndexFile(FileSystem fs) throws IOException {
+            this.indexIn = fs.open(this.indexFile.getPath());
+        }
+
+        /**
+         * Close the index file
+         * @throws IOException
+         */
+        public void Close() throws IOException {
+            this.indexOut.close();
+        }
+
+        /**
+         * Build index tuple
+         *
+         * @throws IOException
+         */
+        private void BuildIndex(Tuple t, long offset) throws IOException {
+            /* Build index key tuple */
+            Tuple indexKeyTuple = tupleFactory.newTuple(this.offsetsToIndexKeys.length);
+            for (int i = 0; i < this.offsetsToIndexKeys.length; ++i) {
+                indexKeyTuple.set(i, t.get(this.offsetsToIndexKeys[i]));
+            }
+
+            /* Check if we have already seen Tuple(s) with same index keys */
+            if (indexKeyTuple.compareTo(this.lastIndexKeyTuple) == 0) {
+                /* We have seen Tuple(s) with given index keys, update the tuple count */
+                this.numberOfTuples += 1;
+            }
+            else {
+                if (this.lastIndexKeyTuple != null)
+                    this.WriteIndex();
+
+                this.lastIndexKeyTuple = indexKeyTuple;
+                this.minIndexKeyTuple = ((this.minIndexKeyTuple == null) || (indexKeyTuple.compareTo(this.minIndexKeyTuple) < 0)) ? indexKeyTuple : this.minIndexKeyTuple;
+                this.maxIndexKeyTuple = ((this.maxIndexKeyTuple == null) || (indexKeyTuple.compareTo(this.maxIndexKeyTuple) > 0)) ? indexKeyTuple : this.maxIndexKeyTuple;
+
+                /* New index tuple for newly seen index key */
+                this.indexTuple = tupleFactory.newTuple(3);
+
+                /* Add index keys to index Tuple */
+                this.indexTuple.set(0, indexKeyTuple);
+
+                /* Reset Tuple count for index key */
+                this.numberOfTuples = 1;
+
+                /* Remember offset to Tuple with new index keys */
+                this.indexTuple.set(2, offset);
+            }
+        }
+
+        /**
+         * Write index header
+         * @param indexOut - Stream to write to
+         * @param ih - Index header to write
+         * @throws IOException
+         */
+        public void WriteIndexHeader() throws IOException {
+            /* Number of index keys */
+            indexOut.writeInt(this.offsetsToIndexKeys.length);
+
+            /* Offset to index keys */
+            for (int i = 0; i < this.offsetsToIndexKeys.length; ++i) {
+                indexOut.writeInt(this.offsetsToIndexKeys[i]);
+            }
+        }
+
+        /**
+         * Read index header
+         * @param indexIn - Stream to read from
+         * @return Index header
+         * @throws IOException
+         */
+        public void ReadIndexHeader() throws IOException {
+            /* Number of index keys */
+            int nkeys = this.indexIn.readInt();
+
+            /* Offset to index keys */
+            this.offsetsToIndexKeys = new int[nkeys];
+            for (int i = 0; i < nkeys; ++i) {
+                offsetsToIndexKeys[i] = this.indexIn.readInt();
+            }
+        }
+
+        /**
+         * Writes the index footer
+         */
+        public void WriterIndexFooter() throws IOException {
+            /* Flush indexes for remaining records */
+            this.WriteIndex();
+
+            /* record the offset to footer */
+            this.offsetToFooter = this.indexOut.getPos();
+
+            /* Write index footer */
+            DataReaderWriter.writeDatum(indexOut, this.minIndexKeyTuple);
+            DataReaderWriter.writeDatum(indexOut, this.maxIndexKeyTuple);
+
+            /* Offset to footer */
+            indexOut.writeLong(this.offsetToFooter);
+        }
+
+        /**
+         * Reads the index footer
+         */
+        public void ReadIndexFooter() throws IOException {
+            long currentOffset = this.indexIn.getPos();
+
+            this.SeekToIndexFooter();
+            this.minIndexKeyTuple = (Tuple)DataReaderWriter.readDatum(this.indexIn);
+            this.maxIndexKeyTuple = (Tuple)DataReaderWriter.readDatum(this.indexIn);
+
+            this.indexIn.seek(currentOffset);
+        }
+
+        /**
+         * Seeks to the index footer
+         */
+        public void SeekToIndexFooter() throws IOException {
+            if (this.offsetToFooter < 0) {
+                /* offset to footer is at last long (8 bytes) in the file */
+                this.indexIn.seek(this.indexFile.getLen()-8);
+                this.offsetToFooter = this.indexIn.readLong();
+            }
+            this.indexIn.seek(this.offsetToFooter);
+        }
+
+        /**
+         * Writes the current index.
+         */
+        public void WriteIndex() throws IOException {
+            this.indexTuple.set(1, this.numberOfTuples);
+            DataReaderWriter.writeDatum(this.indexOut, this.indexTuple);
+        }
+
+        /**
+         * Extracts the index key from the index tuple
+         */
+        public Tuple getIndexKeyTuple(Tuple indexTuple) throws IOException {
+            if (indexTuple.size() == 3)
+                return (Tuple)indexTuple.get(0);
+            else
+                throw new IOException("Invalid index record with size " + indexTuple.size());
+        }
+
+        /**
+         * Extracts the number of records that share the current key from the index tuple.
+         */
+        public long getIndexKeyTupleCount(Tuple indexTuple) throws IOException {
+            if (indexTuple.size() == 3)
+                return (Long)indexTuple.get(1);
+            else
+                throw new IOException("Invalid index record with size " + indexTuple.size());
+        }
+
+        /**
+         * Extracts the offset into the data file from the index tuple.
+         */
+        public long getOffset(Tuple indexTuple) throws IOException {
+            if (indexTuple.size() == 3)
+                return (Long)indexTuple.get(2);
+            else
+                throw new IOException("Invalid index record with size " + indexTuple.size());
+        }
+
+        /**
+          * Reads the next index from the index file (or null if EOF) and extracts
+         * the index fields.
+         */
+        public Tuple ReadIndex() throws IOException {
+            if (this.indexIn.getPos() < this.offsetToFooter) {
+                indexTuple = (Tuple)DataReaderWriter.readDatum(this.indexIn);
+                if (indexTuple != null) {
+                    this.lastIndexKeyTuple = this.getIndexKeyTuple(indexTuple);
+                    this.numberOfTuples = this.getIndexKeyTupleCount(indexTuple);
+                }
+                return indexTuple;
+            }
+            return null;
+        }
+
+        /**
+         * Scans the index looking for a given key.
+         * @return the matching index tuple OR the last index tuple
+         * greater than the requested key if no match is found.
+         */
+        public Tuple ScanIndex(Tuple keys) throws IOException {
+            if (lastIndexKeyTuple != null && keys.compareTo(this.lastIndexKeyTuple) <= 0) {
+                return indexTuple;
+            }
+
+            /* Scan the index looking for given key */
+            while ((indexTuple = this.ReadIndex()) != null) {
+                if (keys.compareTo(this.lastIndexKeyTuple) > 0)
+                    continue;
+                else
+                    break;
+            }
+
+            return indexTuple;
+        }
+
+        /**
+         * stores the list of record indices that identify keys.
+         */
+        private int[] offsetsToIndexKeys = null;
+
+        /**
+         * offset in bytes to the start of the footer of the index.
+         */
+        private long offsetToFooter = -1;
+
+        /**
+         * output stream when writing the index.
+         */
+        FSDataOutputStream indexOut;
+
+        /**
+         * input stream when reading the index.
+         */
+        FSDataInputStream indexIn;
+
+        /**
+         * Tuple factory to create index tuples
+         */
+        private TupleFactory tupleFactory = TupleFactory.getInstance();
+
+        /**
+         * Index key tuple of the form
+         * ((Tuple of index keys), count of tuples with index keys, offset to first tuple with index keys)
+         */
+        private Tuple indexTuple = tupleFactory.newTuple(3);
+
+        /**
+         * "Smallest" index key tuple seen
+         */
+        private Tuple minIndexKeyTuple = null;
+
+        /**
+         * "Biggest" index key tuple seen
+         */
+        private Tuple maxIndexKeyTuple = null;
+
+        /**
+         * Last seen index key tuple
+         */
+        private Tuple lastIndexKeyTuple = null;
+
+        /**
+         * Number of tuples seen for a index key
+         */
+        private long numberOfTuples = 0;
+
+        /**
+          * The index file.
+         */
+        private FileStatus indexFile;
+    }
+
+    /**
+     * Internal InputFormat class
+     */
+    public static class IndexedStorageInputFormat extends PigTextInputFormat {
+
+        @Override
+        public RecordReader<LongWritable, Text> createRecordReader(InputSplit split, TaskAttemptContext context) {
+            IndexManager im = null;
+            try {
+                FileSystem fs = FileSystem.get(context.getConfiguration());
+                Path indexFile = IndexManager.getIndexFileName(((FileSplit)split).getPath());
+                im = new IndexManager(fs.getFileStatus(indexFile));
+                im.openIndexFile(fs);
+                im.ReadIndexHeader();
+                im.ReadIndexFooter();
+            } catch (IOException e) {
+                // TODO Auto-generated catch block
+                e.printStackTrace();
+            }
+
+            return new IndexedStorageRecordReader(im);
+        }
+
+        @Override
+        public boolean isSplitable(JobContext context, Path filename) {
+            return false;
+        }
+
+        /**
+         * Internal RecordReader class
+          */
+        public static class IndexedStorageRecordReader extends RecordReader<LongWritable, Text> {
+            private long start;
+            private long pos;
+            private long end;
+            private IndexedStorageLineReader in;
+            private int maxLineLength;
+            private LongWritable key = null;
+            private Text value = null;
+            private IndexManager indexManager = null;
+
+            @Override
+            public String toString() {
+                return indexManager.minIndexKeyTuple + "|" + indexManager.lastIndexKeyTuple + "|" + indexManager.maxIndexKeyTuple;
+            }
+
+            public IndexedStorageRecordReader(IndexManager im) {
+                this.indexManager = im;
+            }
+
+            /**
+             * Class to compare record readers using underlying indexes
+             *
+             */
+            public static class IndexedStorageRecordReaderComparator implements Comparator<IndexedStorageRecordReader> {
+                @Override
+                public int compare(IndexedStorageRecordReader o1, IndexedStorageRecordReader o2) {
+                    Tuple t1 = (o1.indexManager.lastIndexKeyTuple == null) ?  o1.indexManager.minIndexKeyTuple : o1.indexManager.lastIndexKeyTuple;
+                    Tuple t2 = (o2.indexManager.lastIndexKeyTuple == null) ?  o2.indexManager.minIndexKeyTuple : o2.indexManager.lastIndexKeyTuple;
+                    return t1.compareTo(t2);
+                }
+            }
+
+            public static class IndexedStorageLineReader {
+                private static final int DEFAULT_BUFFER_SIZE = 64 * 1024;
+                private int bufferSize = DEFAULT_BUFFER_SIZE;
+                private InputStream in;
+                private byte[] buffer;
+                // the number of bytes of real data in the buffer
+                private int bufferLength = 0;
+                // the current position in the buffer
+                private int bufferPosn = 0;
+                private long bufferOffset = 0;
+
+                private static final byte CR = '\r';
+                private static final byte LF = '\n';
+
+                /**
+                 * Create a line reader that reads from the given stream using the
+                 * default buffer-size (64k).
+                 * @param in The input stream
+                 * @throws IOException
+                 */
+                public IndexedStorageLineReader(InputStream in) {
+                    this(in, DEFAULT_BUFFER_SIZE);
+                }
+
+                /**
+                 * Create a line reader that reads from the given stream using the
+                 * given buffer-size.
+                 * @param in The input stream
+                 * @param bufferSize Size of the read buffer
+                 * @throws IOException
+                 */
+                public IndexedStorageLineReader(InputStream in, int bufferSize) {
+                    if( !(in instanceof Seekable) || !(in instanceof PositionedReadable) ) {
+                          throw new IllegalArgumentException(
+                              "In is not an instance of Seekable or PositionedReadable");
+                    }
+
+                    this.in = in;
+                    this.bufferSize = bufferSize;
+                    this.buffer = new byte[this.bufferSize];
+                }
+
+                /**
+                 * Create a line reader that reads from the given stream using the
+                 * <code>io.file.buffer.size</code> specified in the given
+                 * <code>Configuration</code>.
+                 * @param in input stream
+                 * @param conf configuration
+                 * @throws IOException
+                 */
+                public IndexedStorageLineReader(InputStream in, Configuration conf) throws IOException {
+                    this(in, conf.getInt("io.file.buffer.size", DEFAULT_BUFFER_SIZE));
+                }
+
+                /**
+                 * Close the underlying stream.
+                 * @throws IOException
+                 */
+                public void close() throws IOException {
+                    in.close();
+                }
+
+                /**
+                 * Read one line from the InputStream into the given Text.  A line
+                 * can be terminated by one of the following: '\n' (LF) , '\r' (CR),
+                 * or '\r\n' (CR+LF).  EOF also terminates an otherwise unterminated
+                 * line.
+                 *
+                 * @param str the object to store the given line (without newline)
+                 * @param maxLineLength the maximum number of bytes to store into str;
+                 *  the rest of the line is silently discarded.
+                 * @param maxBytesToConsume the maximum number of bytes to consume
+                 *  in this call.  This is only a hint, because if the line cross
+                 *  this threshold, we allow it to happen.  It can overshoot
+                 *  potentially by as much as one buffer length.
+                 *
+                 * @return the number of bytes read including the (longest) newline
+                 * found.
+                 *
+                 * @throws IOException if the underlying stream throws
+                 */
+                public int readLine(Text str, int maxLineLength,
+                        int maxBytesToConsume) throws IOException {
+                    /* We're reading data from in, but the head of the stream may be
+                     * already buffered in buffer, so we have several cases:
+                     * 1. No newline characters are in the buffer, so we need to copy
+                     *    everything and read another buffer from the stream.
+                     * 2. An unambiguously terminated line is in buffer, so we just
+                     *    copy to str.
+                     * 3. Ambiguously terminated line is in buffer, i.e. buffer ends
+                     *    in CR.  In this case we copy everything up to CR to str, but
+                     *    we also need to see what follows CR: if it's LF, then we
+                     *    need consume LF as well, so next call to readLine will read
+                     *    from after that.
+                     * We use a flag prevCharCR to signal if previous character was CR
+                     * and, if it happens to be at the end of the buffer, delay
+                     * consuming it until we have a chance to look at the char that
+                     * follows.
+                     */
+                    str.clear();
+                    int txtLength = 0; //tracks str.getLength(), as an optimization
+                    int newlineLength = 0; //length of terminating newline
+                    boolean prevCharCR = false; //true of prev char was CR
+                    long bytesConsumed = 0;
+                    do {
+                        int startPosn = bufferPosn; //starting from where we left off the last time
+                        if (bufferPosn >= bufferLength) {
+                            startPosn = bufferPosn = 0;
+                            if (prevCharCR)
+                                ++bytesConsumed; //account for CR from previous read
+
+                            bufferOffset = ((Seekable)in).getPos();
+                            bufferLength = in.read(buffer);
+
+                            if (bufferLength <= 0)
+                                break; // EOF
+                        }
+                        for (; bufferPosn < bufferLength; ++bufferPosn) { //search for newline
+                            if (buffer[bufferPosn] == LF) {
+                                newlineLength = (prevCharCR) ? 2 : 1;
+                                ++bufferPosn; // at next invocation proceed from following byte
+                                break;
+                            }
+                            if (prevCharCR) { //CR + notLF, we are at notLF
+                                newlineLength = 1;
+                                break;
+                            }
+                            prevCharCR = (buffer[bufferPosn] == CR);
+                        }
+                        int readLength = bufferPosn - startPosn;
+                        if (prevCharCR && newlineLength == 0)
+                            --readLength; //CR at the end of the buffer
+                        bytesConsumed += readLength;
+                        int appendLength = readLength - newlineLength;
+                        if (appendLength > maxLineLength - txtLength) {
+                            appendLength = maxLineLength - txtLength;
+                        }
+                        if (appendLength > 0) {
+                            str.append(buffer, startPosn, appendLength);
+                            txtLength += appendLength;
+                        }
+                    } while (newlineLength == 0 && bytesConsumed < maxBytesToConsume);
+
+                    if (bytesConsumed > (long)Integer.MAX_VALUE)
+                        throw new IOException("Too many bytes before newline: " + bytesConsumed);
+                    return (int)bytesConsumed;
+                }
+
+                /**
+                 * Read from the InputStream into the given Text.
+                 * @param str the object to store the given line
+                 * @param maxLineLength the maximum number of bytes to store into str.
+                 * @return the number of bytes read including the newline
+                 * @throws IOException if the underlying stream throws
+                 */
+                public int readLine(Text str, int maxLineLength) throws IOException {
+                    return readLine(str, maxLineLength, Integer.MAX_VALUE);
+                }
+
+                /**
+                 * Read from the InputStream into the given Text.
+                 * @param str the object to store the given line
+                 * @return the number of bytes read including the newline
+                 * @throws IOException if the underlying stream throws
+                 */
+                public int readLine(Text str) throws IOException {
+                    return readLine(str, Integer.MAX_VALUE, Integer.MAX_VALUE);
+                }
+
+                /**
+                 * If given offset is within the buffer, adjust the buffer position to read from
+                 * otherwise seek to the given offset from start of the file.
+                 * @param offset
+                 * @throws IOException
+                 */
+                public void seek(long offset) throws IOException {
+                    if ((offset >= bufferOffset) && (offset < (bufferOffset + bufferLength)))
+                        bufferPosn = (int) (offset - bufferOffset);
+                    else {
+                        bufferPosn = bufferLength;
+                        ((Seekable)in).seek(offset);
+                    }
+                }
+            }
+
+            @Override
+            public void initialize(InputSplit genericSplit, TaskAttemptContext context)
+            throws IOException, InterruptedException {
+
+                FileSplit split = (FileSplit) genericSplit;
+                Configuration job = context.getConfiguration();
+                this.maxLineLength = job.getInt(MRConfiguration.LINERECORDREADER_MAXLENGTH, Integer.MAX_VALUE);
+                start = split.getStart();
+                end = start + split.getLength();
+                final Path file = split.getPath();
+
+                FileSystem fs = file.getFileSystem(job);
+                FSDataInputStream fileIn = fs.open(split.getPath());
+                boolean skipFirstLine = false;
+                if (start != 0) {
+                    skipFirstLine = true;
+                    --start;
+                    fileIn.seek(start);
+                }
+                in = new IndexedStorageLineReader(fileIn, job);
+                if (skipFirstLine) {
+                    start += in.readLine(new Text(), 0, (int)Math.min((long)Integer.MAX_VALUE, end - start));
+                }
+                this.pos = start;
+            }
+
+            public void seek(long offset) throws IOException {
+                in.seek(offset);
+                pos = offset;
+            }
+
+            /**
+             * Scan the index for given key and seek to appropriate offset in the data
+             * @param keys to look for
+             * @return true if the given key was found, false otherwise
+             * @throws IOException
+             */
+            public boolean seekNear(Tuple keys) throws IOException {
+                boolean ret = false;
+                Tuple indexTuple = this.indexManager.ScanIndex(keys);
+                if (indexTuple != null) {
+                    long offset = this.indexManager.getOffset(indexTuple) ;
+                    in.seek(offset);
+
+                    if (keys.compareTo(this.indexManager.getIndexKeyTuple(indexTuple)) == 0) {
+                        ret = true;
+                    }
+                }
+
+                return ret;
+            }
+
+            @Override
+            public boolean nextKeyValue() throws IOException,
+            InterruptedException {
+                if (key == null) {
+                    key = new LongWritable();
+                }
+                key.set(pos);
+                if (value == null) {
+                    value = new Text();
+                }
+                int newSize = 0;
+                while (pos < end) {
+                    newSize = in.readLine(value, maxLineLength,
+                            Math.max((int)Math.min(Integer.MAX_VALUE, end-pos),
+                                    maxLineLength));
+                    if (newSize == 0) {
+                        break;
+                    }
+                    pos += newSize;
+                    if (newSize < maxLineLength) {
+                        break;
+                    }
+                }
+                if (newSize == 0) {
+                    key = null;
+                    value = null;
+                    return false;
+                } else {
+                    return true;
+                }
+            }
+
+            @Override
+            public LongWritable getCurrentKey() throws IOException,
+            InterruptedException {
+                return key;
+            }
+
+            @Override
+            public Text getCurrentValue() throws IOException,
+            InterruptedException {
+                return value;
+            }
+
+            @Override
+            public float getProgress() throws IOException, InterruptedException {
+                if (start == end) {
+                    return 0.0f;
+                } else {
+                    return Math.min(1.0f, (pos - start) / (float)(end - start));
+                }
+            }
+
+            @Override
+            public void close() throws IOException {
+                if (in != null) {
+                    in.close();
+                }
+            }
+        }
+    }
+
+    /**
+     * List of record readers.
+     */
+    protected IndexedStorageRecordReader[] readers = null;
+
+    /**
+     * Index into the the list of readers to the current reader.
+      * Readers before this index have been fully scanned for keys.
+     */
+    protected int currentReaderIndexStart = 0;
+
+    /**
+     * Delimiter to use between fields
+     */
+    protected byte fieldDelimiter = '\t';
+
+    /**
+     * Offsets to index keys in tuple
+     */
+    final protected int[] offsetsToIndexKeys;
+
+    /**
+     * Comparator used to compare key tuples.
+     */
+    protected Comparator<IndexedStorageRecordReader> readerComparator = new IndexedStorageInputFormat.IndexedStorageRecordReader.IndexedStorageRecordReaderComparator();
 }
diff --git a/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/MultiStorage.java b/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/MultiStorage.java
index e65cc3749..90bbb4467 100644
--- a/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/MultiStorage.java
+++ b/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/MultiStorage.java
@@ -39,6 +39,7 @@ import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
 import org.apache.hadoop.util.ReflectionUtils;
 import org.apache.pig.StoreFunc;
 import org.apache.pig.backend.executionengine.ExecException;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.data.Tuple;
 import org.apache.pig.impl.util.StorageUtil;
 
@@ -161,7 +162,7 @@ public class MultiStorage extends StoreFunc {
     
   @Override
   public void setStoreLocation(String location, Job job) throws IOException {
-    job.getConfiguration().set("mapred.textoutputformat.separator", "");
+    job.getConfiguration().set(MRConfiguration.TEXTOUTPUTFORMAT_SEPARATOR, "");
     FileOutputFormat.setOutputPath(job, new Path(location));
     if (comp == Compression.bz2 || comp == Compression.bz) {
       FileOutputFormat.setCompressOutput(job, true);
diff --git a/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/hiverc/HiveRCOutputFormat.java b/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/hiverc/HiveRCOutputFormat.java
index cd378d4c7..6f8f58e77 100644
--- a/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/hiverc/HiveRCOutputFormat.java
+++ b/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/hiverc/HiveRCOutputFormat.java
@@ -33,6 +33,7 @@ import org.apache.hadoop.mapreduce.RecordWriter;
 import org.apache.hadoop.mapreduce.TaskAttemptContext;
 import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
 import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -77,8 +78,8 @@ public class HiveRCOutputFormat extends FileOutputFormat<NullWritable, Writable>
           // override compression codec if set.
           String codecOverride = conf.get(COMPRESSION_CODEC_CONF);
           if (codecOverride != null) {
-            conf.setBoolean("mapred.output.compress", true);
-            conf.set("mapred.output.compression.codec", codecOverride);
+            conf.setBoolean(MRConfiguration.OUTPUT_COMPRESS, true);
+            conf.set(MRConfiguration.OUTPUT_COMPRESSION_CODEC, codecOverride);
           }
 
           CompressionCodec codec = null;
diff --git a/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestCSVExcelStorage.java b/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestCSVExcelStorage.java
index f0bad12e5..e74f3be0e 100644
--- a/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestCSVExcelStorage.java
+++ b/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestCSVExcelStorage.java
@@ -30,6 +30,7 @@ import org.apache.commons.lang.StringUtils;
 
 import org.apache.pig.ExecType;
 import org.apache.pig.PigServer;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.data.DataByteArray;
 import org.apache.pig.data.Tuple;
 import org.apache.pig.tools.parameters.ParseException;
@@ -139,11 +140,11 @@ public class TestCSVExcelStorage  {
     public void setup() throws IOException {
         pig = new PigServer(ExecType.LOCAL);
         pig.getPigContext().getProperties()
-                 .setProperty("mapred.map.max.attempts", "1");
+                 .setProperty(MRConfiguration.MAP_MAX_ATTEMPTS, "1");
         pig.getPigContext().getProperties()
-                 .setProperty("mapred.reduce.max.attempts", "1");
+                 .setProperty(MRConfiguration.REDUCE_MAX_ATTEMPTS, "1");
         pig.getPigContext().getProperties()
-                 .setProperty("mapreduce.job.end-notification.retry.interval", "100");
+                 .setProperty(MRConfiguration.JOB_END_NOTIFICATION_RETRY_INTERVAL, "100");
 
         Util.deleteDirectory(new File(dataDir));
 
diff --git a/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestCSVStorage.java b/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestCSVStorage.java
index 6824a7218..de2accd52 100644
--- a/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestCSVStorage.java
+++ b/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestCSVStorage.java
@@ -28,6 +28,7 @@ import org.apache.commons.logging.LogFactory;
 import org.apache.pig.ExecType;
 import org.apache.pig.PigServer;
 import org.apache.pig.backend.executionengine.ExecException;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.data.Tuple;
 import org.apache.pig.test.MiniCluster;
 import org.apache.pig.test.Util;
@@ -43,9 +44,9 @@ public class TestCSVStorage {
         cluster = MiniCluster.buildCluster();
         pigServer = new PigServer(ExecType.LOCAL, new Properties());
         pigServer.getPigContext().getProperties()
-                .setProperty("mapred.map.max.attempts", "1");
+                .setProperty(MRConfiguration.MAP_MAX_ATTEMPTS, "1");
         pigServer.getPigContext().getProperties()
-                .setProperty("mapred.reduce.max.attempts", "1");
+                .setProperty(MRConfiguration.REDUCE_MAX_ATTEMPTS, "1");
     }
 
     @Test
diff --git a/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestDBStorage.java b/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestDBStorage.java
index 01bb29c31..89d126c46 100644
--- a/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestDBStorage.java
+++ b/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestDBStorage.java
@@ -36,6 +36,7 @@ import org.apache.pig.ExecType;
 import org.apache.pig.PigServer;
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.backend.executionengine.ExecJob;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.test.MiniCluster;
 import org.apache.pig.test.Util;
 import org.hsqldb.Server;
@@ -65,9 +66,9 @@ public class TestDBStorage extends TestCase {
         cluster = MiniCluster.buildCluster();
         pigServer = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
         pigServer.getPigContext().getProperties()
-                .setProperty("mapred.map.max.attempts", "1");
+                .setProperty(MRConfiguration.MAP_MAX_ATTEMPTS, "1");
         pigServer.getPigContext().getProperties()
-                .setProperty("mapred.reduce.max.attempts", "1");
+                .setProperty(MRConfiguration.REDUCE_MAX_ATTEMPTS, "1");
         System.out.println("Pig server initialized successfully");
         TMP_DIR = System.getProperty("user.dir") + "/build/test/";
         dblocation = TMP_DIR + "batchtest";
diff --git a/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestIndexedStorage.java b/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestIndexedStorage.java
index 51f368b3d..46385e980 100644
--- a/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestIndexedStorage.java
+++ b/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestIndexedStorage.java
@@ -43,6 +43,7 @@ import org.apache.hadoop.mapreduce.TaskAttemptContext;
 import org.apache.hadoop.mapreduce.TaskAttemptID;
 import org.apache.pig.ExecType;
 import org.apache.pig.PigServer;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.backend.hadoop.executionengine.shims.HadoopShims;
 import org.apache.pig.data.DataByteArray;
 import org.apache.pig.data.Tuple;
@@ -122,9 +123,9 @@ public class TestIndexedStorage {
         LocalFileSystem fs = FileSystem.getLocal(conf);
 
         TaskAttemptID taskId = HadoopShims.createTaskAttemptID("jt", 1, true, 1, 1);
-        conf.set("mapred.task.id", taskId.toString());
+        conf.set(MRConfiguration.TASK_ID, taskId.toString());
 
-        conf.set("mapred.input.dir", Util.encodeEscape(outputDir.getAbsolutePath()));
+        conf.set(MRConfiguration.INPUT_DIR, Util.encodeEscape(outputDir.getAbsolutePath()));
         storage.initialize(conf);
 
         Integer key;
@@ -154,9 +155,9 @@ public class TestIndexedStorage {
         LocalFileSystem fs = FileSystem.getLocal(conf);
 
         TaskAttemptID taskId =  HadoopShims.createTaskAttemptID("jt", 2, true, 2, 2);
-        conf.set("mapred.task.id", taskId.toString());
+        conf.set(MRConfiguration.TASK_ID, taskId.toString());
 
-        conf.set("mapred.input.dir", Util.encodeEscape(outputDir.getAbsolutePath()));
+        conf.set(MRConfiguration.INPUT_DIR, Util.encodeEscape(outputDir.getAbsolutePath()));
         storage.initialize(conf);
 
         TupleFactory tupleFactory = TupleFactory.getInstance();
diff --git a/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestXMLLoader.java b/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestXMLLoader.java
index a3409199e..0592b8131 100644
--- a/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestXMLLoader.java
+++ b/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestXMLLoader.java
@@ -3,9 +3,9 @@
  * NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF
  * licenses this file to you under the Apache License, Version 2.0 (the "License"); you may not use this file
  * except in compliance with the License. You may obtain a copy of the License at
- * 
+ *
  * http://www.apache.org/licenses/LICENSE-2.0
- * 
+ *
  * Unless required by applicable law or agreed to in writing, software distributed under the License is
  * distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  * See the License for the specific language governing permissions and limitations under the License.
@@ -35,6 +35,7 @@ import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.pig.ExecType;
 import org.apache.pig.PigServer;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.data.Tuple;
 import org.apache.pig.test.Util;
 import org.w3c.dom.Document;
@@ -55,31 +56,31 @@ public class TestXMLLoader extends TestCase {
     data.add(new String[] { "</property>"});
     data.add(new String[] { "</configuration>"});
   }
-  
+
   public static ArrayList<String[]> nestedTags = new ArrayList<String[]>();
   static {
      nestedTags.add(new String[] { "<events>"});
      nestedTags.add(new String[] { "<event id='116913365'>"});
      nestedTags.add(new String[] { "<eventRank>1.000000000000</eventRank>"});
-     nestedTags.add(new String[] { "<name>XY</name>"});   
+     nestedTags.add(new String[] { "<name>XY</name>"});
      nestedTags.add(new String[] { "<relatedEvents>"});
      nestedTags.add(new String[] { "<event id='116913365'>x</event>"});
      nestedTags.add(new String[] { "<event id='116913365'>y</event>"});
      nestedTags.add(new String[] { "</relatedEvents>"});
      nestedTags.add(new String[] { "</event>"});
-    
+
      nestedTags.add(new String[] { "<event id='116913365'>"});
      nestedTags.add(new String[] { "<eventRank>3.0000</eventRank>"});
-     nestedTags.add(new String[] { "<name>AB</name>"});   
+     nestedTags.add(new String[] { "<name>AB</name>"});
      nestedTags.add(new String[] { "<relatedEvents>"});
      nestedTags.add(new String[] { "<event id='116913365'>a</event>"});
      nestedTags.add(new String[] { "<event id='116913365'>b</event>"});
      nestedTags.add(new String[] { "</relatedEvents>"});
      nestedTags.add(new String[] { "</event>"});
-     
+
      nestedTags.add(new String[] { "<event>"});
      nestedTags.add(new String[] { "<eventRank>4.0000</eventRank>"});
-     nestedTags.add(new String[] { "<name>CD</name>"});   
+     nestedTags.add(new String[] { "<name>CD</name>"});
      nestedTags.add(new String[] { "<relatedEvents>"});
      nestedTags.add(new String[] { "<event>c</event>"});
      nestedTags.add(new String[] { "<event>d</event>"});
@@ -87,7 +88,7 @@ public class TestXMLLoader extends TestCase {
      nestedTags.add(new String[] { "</event>"});
      nestedTags.add(new String[] { "</events>"});
   }
-  
+
   public static ArrayList<String[]> inlineClosedTags = new ArrayList<String[]>();
   static {
     inlineClosedTags.add(new String[] { "<events>"});
@@ -97,8 +98,8 @@ public class TestXMLLoader extends TestCase {
     inlineClosedTags.add(new String[] { "<event id='33'><tag k='a' v='b'/></event>"});
     inlineClosedTags.add(new String[] { "</events>"});
   }
-  public void testShouldReturn0TupleCountIfSearchTagIsNotFound () throws Exception
-  {
+
+  public void testShouldReturn0TupleCountIfSearchTagIsNotFound () throws Exception {
     String filename = TestHelper.createTempFile(data, "");
     PigServer pig = new PigServer(LOCAL);
     filename = filename.replace("\\", "\\\\");
@@ -116,9 +117,9 @@ public class TestXMLLoader extends TestCase {
         }
       }
     }
-    assertEquals(0, tupleCount); 
+    assertEquals(0, tupleCount);
   }
-  
+
   public void testLoadXMLLoader() throws Exception {
     //ArrayList<DataByteArray[]> expected = TestHelper.getExpected(data, pattern);
     String filename = TestHelper.createTempFile(data, "");
@@ -138,90 +139,82 @@ public class TestXMLLoader extends TestCase {
         }
       }
     }
-    assertEquals(2, tupleCount);  
+    assertEquals(2, tupleCount);
   }
-  
+
   public void testXMLLoaderShouldLoadBasicBzip2Files() throws Exception {
     String filename = TestHelper.createTempFile(data, "");
     Process bzipProc = Runtime.getRuntime().exec("bzip2 "+filename);
     int waitFor = bzipProc.waitFor();
-  
-    if(waitFor != 0)
-    {
+
+    if(waitFor != 0) {
         fail ("Failed to create the class");
     }
 
     filename = filename + ".bz2";
 
-    try
-    {
+    try {
         PigServer pigServer = new PigServer (ExecType.LOCAL);
         String loadQuery = "A = LOAD '" + Util.encodeEscape(filename) + "' USING org.apache.pig.piggybank.storage.XMLLoader('property') as (doc:chararray);";
         pigServer.registerQuery(loadQuery);
+
         Iterator<Tuple> it = pigServer.openIterator("A");
         int tupleCount = 0;
         while (it.hasNext()) {
-        Tuple tuple = (Tuple) it.next();
-        if (tuple == null)
-        break;
-        else {
-        //TestHelper.examineTuple(expected, tuple, tupleCount);
-        if (tuple.size() > 0) {
-            tupleCount++;
-	        }
-	     }
+            Tuple tuple = (Tuple) it.next();
+            if (tuple == null)
+                break;
+            else {
+                //TestHelper.examineTuple(expected, tuple, tupleCount);
+                if (tuple.size() > 0) {
+                    tupleCount++;
+                }
+            }
         }
-	    
         assertEquals(2, tupleCount);
-    
-    }finally
-    {
+
+    } finally {
         new File(filename).delete();
     }
-	    
    }
+
    public void testLoaderShouldLoadBasicGzFile() throws Exception {
     String filename = TestHelper.createTempFile(data, "");
-	  
+
     Process bzipProc = Runtime.getRuntime().exec("gzip "+filename);
     int waitFor = bzipProc.waitFor();
-	  
-    if(waitFor != 0)
-    {
+
+    if(waitFor != 0) {
         fail ("Failed to create the class");
     }
-	  
+
     filename = filename + ".gz";
-    
-    try
-    {
 
-    PigServer pigServer = new PigServer (ExecType.LOCAL);
-    String loadQuery = "A = LOAD '" + Util.encodeEscape(filename) + "' USING org.apache.pig.piggybank.storage.XMLLoader('property') as (doc:chararray);";
-    pigServer.registerQuery(loadQuery);
+    try {
+        PigServer pigServer = new PigServer (ExecType.LOCAL);
+        String loadQuery = "A = LOAD '" + Util.encodeEscape(filename) + "' USING org.apache.pig.piggybank.storage.XMLLoader('property') as (doc:chararray);";
+        pigServer.registerQuery(loadQuery);
 
-    Iterator<Tuple> it = pigServer.openIterator("A");
-    int tupleCount = 0;
-    while (it.hasNext()) {
-    Tuple tuple = (Tuple) it.next();
-    if (tuple == null)
-        break;
-    else {
-        if (tuple.size() > 0) {
-        tupleCount++;
-             }
+        Iterator<Tuple> it = pigServer.openIterator("A");
+        int tupleCount = 0;
+        while (it.hasNext()) {
+            Tuple tuple = (Tuple) it.next();
+            if (tuple == null)
+                break;
+            else {
+                if (tuple.size() > 0) {
+                    tupleCount++;
+                }
+            }
         }
-    }	
-    assertEquals(2, tupleCount); 
-   
-    }finally
-    {
+        assertEquals(2, tupleCount);
+
+    } finally {
         new File(filename).delete();
     }
  }
-   
-   public void testXMLLoaderShouldNotConfusedWithTagsHavingSimilarPrefix () throws Exception
-   {
+
+   public void testXMLLoaderShouldNotConfusedWithTagsHavingSimilarPrefix () throws Exception {
       ArrayList<String[]> testData = new ArrayList<String[]>();
       testData.add(new String[] { "<namethisalso> foobar9 </namethisalso>"});
       testData.addAll(data);
@@ -233,21 +226,19 @@ public class TestXMLLoader extends TestCase {
       Iterator<?> it = pig.openIterator("A");
       int tupleCount = 0;
       while (it.hasNext()) {
-        Tuple tuple = (Tuple) it.next();
-        if (tuple == null) 
-          break;
-        else {
-          if (tuple.size() > 0) {
-              tupleCount++;
+          Tuple tuple = (Tuple) it.next();
+          if (tuple == null)
+              break;
+          else {
+              if (tuple.size() > 0) {
+                  tupleCount++;
+              }
           }
-        }
       }
       assertEquals(3, tupleCount);
-      
    }
-   
-   public void testShouldReturn1ForIntermediateTagData () throws Exception
-   {
+
+   public void testShouldReturn1ForIntermediateTagData () throws Exception {
       String filename = TestHelper.createTempFile(data, "");
       PigServer pig = new PigServer(LOCAL);
       filename = filename.replace("\\", "\\\\");
@@ -256,29 +247,27 @@ public class TestXMLLoader extends TestCase {
       Iterator<?> it = pig.openIterator("A");
       int tupleCount = 0;
       while (it.hasNext()) {
-        Tuple tuple = (Tuple) it.next();
-        if (tuple == null) 
-          break;
-        else {
-          if (tuple.size() > 0) {
-              tupleCount++;
+          Tuple tuple = (Tuple) it.next();
+          if (tuple == null)
+              break;
+          else {
+              if (tuple.size() > 0) {
+                  tupleCount++;
+              }
           }
-        }
       }
-      assertEquals(1, tupleCount);  
+      assertEquals(1, tupleCount);
    }
-   public void testShouldReturn0TupleCountIfNoEndTagIsFound() throws Exception
-   {
+
+   public void testShouldReturn0TupleCountIfNoEndTagIsFound() throws Exception {
       // modify the data content to avoid end tag for </ignoreProperty>
       ArrayList<String[]> testData = new ArrayList<String[]>();
       for (String content[] : data) {
-         
-         if(!content[0].equals("</ignoreProperty>"))
-         {
+         if(!content[0].equals("</ignoreProperty>")) {
             testData.add(content);
          }
       }
-      
+
       String filename = TestHelper.createTempFile(testData, "");
       PigServer pig = new PigServer(LOCAL);
       filename = filename.replace("\\", "\\\\");
@@ -287,23 +276,22 @@ public class TestXMLLoader extends TestCase {
       Iterator<?> it = pig.openIterator("A");
       int tupleCount = 0;
       while (it.hasNext()) {
-        Tuple tuple = (Tuple) it.next();
-        if (tuple == null)
-          break;
-        else {
-          if (tuple.size() > 0) {
-              tupleCount++;
+          Tuple tuple = (Tuple) it.next();
+          if (tuple == null)
+              break;
+          else {
+              if (tuple.size() > 0) {
+                  tupleCount++;
+              }
           }
-        }
       }
-      assertEquals(0, tupleCount);  
+      assertEquals(0, tupleCount);
    }
-   
-   public void testShouldReturn0TupleCountIfEmptyFileIsPassed() throws Exception
-   {
+
+   public void testShouldReturn0TupleCountIfEmptyFileIsPassed() throws Exception {
       // modify the data content to avoid end tag for </ignoreProperty>
       ArrayList<String[]> testData = new ArrayList<String[]>();
-      
+
       String filename = TestHelper.createTempFile(testData, "");
       PigServer pig = new PigServer(LOCAL);
       filename = filename.replace("\\", "\\\\");
@@ -312,20 +300,19 @@ public class TestXMLLoader extends TestCase {
       Iterator<?> it = pig.openIterator("A");
       int tupleCount = 0;
       while (it.hasNext()) {
-        Tuple tuple = (Tuple) it.next();
-        if (tuple == null)
-          break;
-        else {
-          if (tuple.size() > 0) {
-              tupleCount++;
+          Tuple tuple = (Tuple) it.next();
+          if (tuple == null)
+              break;
+          else {
+              if (tuple.size() > 0) {
+                  tupleCount++;
+              }
           }
-        }
       }
-      assertEquals(0, tupleCount);  
+      assertEquals(0, tupleCount);
    }
-   
+
    public void testXMLLoaderShouldSupportNestedTagWithSameName() throws Exception {
-      
       String filename = TestHelper.createTempFile(nestedTags, "");
       PigServer pig = new PigServer(LOCAL);
       filename = filename.replace("\\", "\\\\");
@@ -334,18 +321,17 @@ public class TestXMLLoader extends TestCase {
       Iterator<?> it = pig.openIterator("A");
       int tupleCount = 0;
       while (it.hasNext()) {
-        Tuple tuple = (Tuple) it.next();
-        if (tuple == null)
-          break;
-        else {
-          if (tuple.size() > 0) {
-              tupleCount++;
+          Tuple tuple = (Tuple) it.next();
+          if (tuple == null)
+              break;
+          else {
+              if (tuple.size() > 0) {
+                  tupleCount++;
+              }
           }
-        }
       }
-      assertEquals(3, tupleCount);  
+      assertEquals(3, tupleCount);
    }
-   
 
    public void testXMLLoaderShouldWorkWithInlineClosedTags() throws Exception {
      String filename = TestHelper.createTempFile(inlineClosedTags, "");
@@ -356,16 +342,16 @@ public class TestXMLLoader extends TestCase {
      Iterator<?> it = pig.openIterator("A");
      int tupleCount = 0;
      while (it.hasNext()) {
-       Tuple tuple = (Tuple) it.next();
-       if (tuple == null)
-         break;
-       else {
-         if (tuple.size() > 0) {
-             tupleCount++;
+         Tuple tuple = (Tuple) it.next();
+         if (tuple == null)
+             break;
+         else {
+             if (tuple.size() > 0) {
+                 tupleCount++;
+             }
          }
-       }
      }
-     assertEquals(4, tupleCount);  
+     assertEquals(4, tupleCount);
    }
 
    public void testXMLLoaderShouldReturnValidXML() throws Exception {
@@ -376,15 +362,15 @@ public class TestXMLLoader extends TestCase {
      pig.registerQuery(query);
      Iterator<?> it = pig.openIterator("A");
      while (it.hasNext()) {
-       Tuple tuple = (Tuple) it.next();
-       if (tuple == null)
-         break;
-       else {
-         // Test it returns a valid XML
-         DocumentBuilder docBuilder =
-           DocumentBuilderFactory.newInstance().newDocumentBuilder();
-         docBuilder.parse(new ByteArrayInputStream(((String)tuple.get(0)).getBytes()));
-       }
+         Tuple tuple = (Tuple) it.next();
+         if (tuple == null)
+             break;
+         else {
+             // Test it returns a valid XML
+             DocumentBuilder docBuilder =
+                     DocumentBuilderFactory.newInstance().newDocumentBuilder();
+             docBuilder.parse(new ByteArrayInputStream(((String)tuple.get(0)).getBytes()));
+         }
      }
    }
 
@@ -394,7 +380,7 @@ public class TestXMLLoader extends TestCase {
     * the first split is a prefix of the matching tag.
     * In other words, till the end of the first split, it looks like the tag is
     * matching but it is not actually matching.
-    * 
+    *
     * @throws Exception
     */
    public void testXMLLoaderShouldNotReturnLastNonMatchedTag() throws Exception {
@@ -435,7 +421,7 @@ public class TestXMLLoader extends TestCase {
      Configuration conf = new Configuration();
      long blockSize = 512;
      conf.setLong("fs.local.block.size", blockSize);
-     conf.setLong("mapred.max.split.size", blockSize);
+     conf.setLong(MRConfiguration.MAX_SPLIT_SIZE, blockSize);
 
      String tagName = "event";
      File tempFile = File.createTempFile("long-file", ".xml");
@@ -465,14 +451,13 @@ public class TestXMLLoader extends TestCase {
        matchingCount++;
      }
      ps.close();
-     
-     
+
      PigServer pig = new PigServer(LOCAL, conf);
      String tempFileName = tempFile.getAbsolutePath().replace("\\", "\\\\");
      String query = "A = LOAD '" + tempFileName + "' USING org.apache.pig.piggybank.storage.XMLLoader('event') as (doc:chararray);";
      pig.registerQuery(query);
      Iterator<?> it = pig.openIterator("A");
-     
+
      int count = 0;
      while (it.hasNext()) {
        Tuple tuple = (Tuple) it.next();
diff --git a/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/avro/TestAvroStorage.java b/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/avro/TestAvroStorage.java
index 7d40ba0bb..21c144c6c 100644
--- a/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/avro/TestAvroStorage.java
+++ b/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/avro/TestAvroStorage.java
@@ -39,6 +39,7 @@ import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.FileUtil;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.PathFilter;
+import org.apache.hadoop.io.compress.SnappyCodec;
 import org.apache.pig.ExecType;
 import org.apache.pig.LoadFunc;
 import org.apache.pig.PigConfiguration;
@@ -47,6 +48,7 @@ import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.backend.executionengine.ExecJob;
 import org.apache.pig.backend.executionengine.ExecJob.JOB_STATUS;
 import org.apache.pig.backend.hadoop.executionengine.JobCreationException;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.builtin.mock.Storage.Data;
 import org.apache.pig.data.Tuple;
 import org.apache.pig.impl.io.FileLocalizer;
@@ -971,8 +973,8 @@ public class TestAvroStorage {
         deleteDirectory(new File(output));
 
         Properties properties = new Properties();
-        properties.setProperty("mapred.output.compress", "true");
-        properties.setProperty("mapred.output.compression.codec", "org.apache.hadoop.io.compress.SnappyCodec");
+        properties.setProperty(MRConfiguration.OUTPUT_COMPRESS, "true");
+        properties.setProperty(MRConfiguration.OUTPUT_COMPRESSION_CODEC, "org.apache.hadoop.io.compress.SnappyCodec");
         properties.setProperty("avro.output.codec", "snappy");
         PigServer pigServer = new PigServer(ExecType.LOCAL, properties);
         pigServer.setBatchOn();
diff --git a/shims/src/hadoop20/org/apache/pig/backend/hadoop/executionengine/shims/HadoopShims.java b/shims/src/hadoop20/org/apache/pig/backend/hadoop/executionengine/shims/HadoopShims.java
index 9230480ff..6ccc736da 100644
--- a/shims/src/hadoop20/org/apache/pig/backend/hadoop/executionengine/shims/HadoopShims.java
+++ b/shims/src/hadoop20/org/apache/pig/backend/hadoop/executionengine/shims/HadoopShims.java
@@ -37,6 +37,7 @@ import org.apache.hadoop.mapreduce.TaskAttemptContext;
 import org.apache.hadoop.mapreduce.TaskAttemptID;
 import org.apache.hadoop.mapreduce.TaskType;
 import org.apache.pig.PigConfiguration;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
 import org.apache.pig.backend.hadoop20.PigJobControl;
@@ -134,7 +135,7 @@ public class HadoopShims {
      * @param taskAttemptID
      */
     public static void setTaskAttemptId(Configuration conf, TaskAttemptID taskAttemptID) {
-        conf.set("mapred.task.id", taskAttemptID.toString());
+        conf.set(MRConfiguration.TASK_ID, taskAttemptID.toString());
     }
 
     /**
diff --git a/shims/src/hadoop23/org/apache/pig/backend/hadoop/executionengine/shims/HadoopShims.java b/shims/src/hadoop23/org/apache/pig/backend/hadoop/executionengine/shims/HadoopShims.java
index 1beee62b3..dcacf476b 100644
--- a/shims/src/hadoop23/org/apache/pig/backend/hadoop/executionengine/shims/HadoopShims.java
+++ b/shims/src/hadoop23/org/apache/pig/backend/hadoop/executionengine/shims/HadoopShims.java
@@ -42,6 +42,7 @@ import org.apache.hadoop.mapreduce.TaskType;
 import org.apache.hadoop.mapreduce.task.JobContextImpl;
 import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;
 import org.apache.pig.PigConfiguration;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
 import org.apache.pig.backend.hadoop23.PigJobControl;
 
@@ -139,7 +140,7 @@ public class HadoopShims {
      * @param taskAttemptID
      */
     public static void setTaskAttemptId(Configuration conf, TaskAttemptID taskAttemptID) {
-        conf.setInt("mapreduce.job.application.attempt.id", taskAttemptID.getId());
+        conf.setInt(MRConfiguration.JOB_APPLICATION_ATTEMPT_ID, taskAttemptID.getId());
     }
 
     /**
diff --git a/shims/test/hadoop20/org/apache/pig/test/MiniCluster.java b/shims/test/hadoop20/org/apache/pig/test/MiniCluster.java
index 2c4559dd8..07b014c5b 100644
--- a/shims/test/hadoop20/org/apache/pig/test/MiniCluster.java
+++ b/shims/test/hadoop20/org/apache/pig/test/MiniCluster.java
@@ -25,6 +25,7 @@ import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 import org.apache.hadoop.mapred.MiniMRCluster;
 import org.apache.pig.ExecType;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 
 public class MiniCluster extends MiniGenericCluster {
     private static final File CONF_DIR = new File("build/classes");
@@ -68,16 +69,16 @@ public class MiniCluster extends MiniGenericCluster {
 
             // Write the necessary config info to hadoop-site.xml
             m_conf = m_mr.createJobConf();
-            m_conf.setInt("mapred.submit.replication", 2);
+            m_conf.setInt(MRConfiguration.SUMIT_REPLICATION, 2);
+            m_conf.setInt(MRConfiguration.MAP_MAX_ATTEMPTS, 2);
+            m_conf.setInt(MRConfiguration.REDUCE_MAX_ATTEMPTS, 2);
             m_conf.set("dfs.datanode.address", "0.0.0.0:0");
             m_conf.set("dfs.datanode.http.address", "0.0.0.0:0");
-            m_conf.set("mapred.map.max.attempts", "2");
-            m_conf.set("mapred.reduce.max.attempts", "2");
             m_conf.set("pig.jobcontrol.sleep", "100");
             m_conf.writeXml(new FileOutputStream(CONF_FILE));
 
             // Set the system properties needed by Pig
-            System.setProperty("cluster", m_conf.get("mapred.job.tracker"));
+            System.setProperty("cluster", m_conf.get(MRConfiguration.JOB_TRACKER));
             System.setProperty("namenode", m_conf.get("fs.default.name"));
             System.setProperty("junit.hadoop.conf", CONF_DIR.getPath());
         } catch (IOException e) {
diff --git a/shims/test/hadoop23/org/apache/pig/test/MiniCluster.java b/shims/test/hadoop23/org/apache/pig/test/MiniCluster.java
index ba97577ec..9ff0b869e 100644
--- a/shims/test/hadoop23/org/apache/pig/test/MiniCluster.java
+++ b/shims/test/hadoop23/org/apache/pig/test/MiniCluster.java
@@ -27,6 +27,7 @@ import org.apache.hadoop.hdfs.MiniDFSCluster;
 import org.apache.hadoop.mapreduce.filecache.DistributedCache;
 import org.apache.hadoop.mapreduce.v2.MiniMRYarnCluster;
 import org.apache.pig.ExecType;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 
 /**
  * This class builds a single instance of itself with the Singleton
@@ -91,16 +92,16 @@ public class MiniCluster extends MiniGenericCluster {
 
             m_conf = m_mr_conf;
             m_conf.set("fs.default.name", m_dfs_conf.get("fs.default.name"));
-            m_conf.unset("mapreduce.job.cache.files");
+            m_conf.unset(MRConfiguration.JOB_CACHE_FILES);
 
-            m_conf.setInt("mapred.io.sort.mb", 200);
-            m_conf.set("mapred.child.java.opts", "-Xmx512m");
+            m_conf.setInt(MRConfiguration.IO_SORT_MB, 200);
+            m_conf.set(MRConfiguration.CHILD_JAVA_OPTS, "-Xmx512m");
 
-            m_conf.setInt("mapred.submit.replication", 2);
+            m_conf.setInt(MRConfiguration.SUMIT_REPLICATION, 2);
+            m_conf.setInt(MRConfiguration.MAP_MAX_ATTEMPTS, 2);
+            m_conf.setInt(MRConfiguration.REDUCE_MAX_ATTEMPTS, 2);
             m_conf.set("dfs.datanode.address", "0.0.0.0:0");
             m_conf.set("dfs.datanode.http.address", "0.0.0.0:0");
-            m_conf.set("mapred.map.max.attempts", "2");
-            m_conf.set("mapred.reduce.max.attempts", "2");
             m_conf.set("pig.jobcontrol.sleep", "100");
             m_conf.writeXml(new FileOutputStream(CONF_FILE));
             m_fileSys.copyFromLocalFile(new Path(CONF_FILE.getAbsoluteFile().toString()),
@@ -109,7 +110,7 @@ public class MiniCluster extends MiniGenericCluster {
 
             System.err.println("XXX: Setting fs.default.name to: " + m_dfs_conf.get("fs.default.name"));
             // Set the system properties needed by Pig
-            System.setProperty("cluster", m_conf.get("mapred.job.tracker"));
+            System.setProperty("cluster", m_conf.get(MRConfiguration.JOB_TRACKER));
             //System.setProperty("namenode", m_dfs_conf.get("fs.default.name"));
             System.setProperty("namenode", m_conf.get("fs.default.name"));
             System.setProperty("junit.hadoop.conf", CONF_DIR.getPath());
diff --git a/shims/test/hadoop23/org/apache/pig/test/TezMiniCluster.java b/shims/test/hadoop23/org/apache/pig/test/TezMiniCluster.java
index 6b3615b49..b58f35960 100644
--- a/shims/test/hadoop23/org/apache/pig/test/TezMiniCluster.java
+++ b/shims/test/hadoop23/org/apache/pig/test/TezMiniCluster.java
@@ -31,6 +31,7 @@ import org.apache.hadoop.mapreduce.v2.MiniMRYarnCluster;
 import org.apache.hadoop.yarn.conf.YarnConfiguration;
 import org.apache.pig.ExecType;
 import org.apache.pig.PigConfiguration;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.backend.hadoop.executionengine.tez.TezExecType;
 import org.apache.pig.backend.hadoop.executionengine.tez.TezSessionManager;
 import org.apache.tez.runtime.library.api.TezRuntimeConfiguration;
@@ -85,7 +86,7 @@ public class TezMiniCluster extends MiniGenericCluster {
             m_mr.init(m_dfs_conf);
             m_mr.start();
             m_mr_conf = m_mr.getConfig();
-            m_mr_conf.set("mapreduce.framework.name", "yarn-tez");
+            m_mr_conf.set(MRConfiguration.FRAMEWORK_NAME, "yarn-tez");
             m_mr_conf.set(YarnConfiguration.YARN_APPLICATION_CLASSPATH,
                     System.getProperty("java.class.path"));
 
diff --git a/src/org/apache/pig/backend/hadoop/datastorage/ConfigurationUtil.java b/src/org/apache/pig/backend/hadoop/datastorage/ConfigurationUtil.java
index 73e018a6c..639a54b32 100644
--- a/src/org/apache/pig/backend/hadoop/datastorage/ConfigurationUtil.java
+++ b/src/org/apache/pig/backend/hadoop/datastorage/ConfigurationUtil.java
@@ -27,6 +27,7 @@ import java.util.Properties;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.pig.ExecType;
 import org.apache.pig.PigConstants;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce;
 import org.apache.pig.backend.hadoop.executionengine.shims.HadoopShims;
 import org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil;
@@ -84,7 +85,7 @@ public class ConfigurationUtil {
             // so build/classes/hadoop-site.xml contains such entry. This prevents some tests from
             // successful (They expect those files in hdfs), so we need to unset it in hadoop 23.
             // This should go away once MiniMRCluster fix the distributed cache issue.
-            HadoopShims.unsetConf(localConf, "mapreduce.job.cache.files");
+            HadoopShims.unsetConf(localConf, MRConfiguration.JOB_CACHE_FILES);
         }
         localConf.set(MapRedUtil.FILE_SYSTEM_NAME, "file:///");
         Properties props = ConfigurationUtil.toProperties(localConf);
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/HExecutionEngine.java b/src/org/apache/pig/backend/hadoop/executionengine/HExecutionEngine.java
index 41c2b7c41..2f8da97ed 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/HExecutionEngine.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/HExecutionEngine.java
@@ -40,6 +40,7 @@ import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
 import org.apache.pig.backend.hadoop.datastorage.HDataStorage;
 import org.apache.pig.backend.hadoop.executionengine.fetch.FetchLauncher;
 import org.apache.pig.backend.hadoop.executionengine.fetch.FetchOptimizer;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
 import org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil;
@@ -71,10 +72,8 @@ public abstract class HExecutionEngine implements ExecutionEngine {
     public static final String MAPRED_DEFAULT_SITE = "mapred-default.xml";
     public static final String YARN_DEFAULT_SITE = "yarn-default.xml";
 
-    public static final String JOB_TRACKER_LOCATION = "mapred.job.tracker";
     public static final String FILE_SYSTEM_LOCATION = "fs.default.name";
     public static final String ALTERNATIVE_FILE_SYSTEM_LOCATION = "fs.defaultFS";
-    public static final String MAPREDUCE_FRAMEWORK_NAME = "mapreduce.framework.name";
     public static final String LOCAL = "local";
 
     protected PigContext pigContext;
@@ -175,10 +174,10 @@ public abstract class HExecutionEngine implements ExecutionEngine {
             new DistributedFileSystem();
         } else {
             // If we are running in local mode we dont read the hadoop conf file
-            if (properties.getProperty(MAPREDUCE_FRAMEWORK_NAME) == null) {
-                properties.setProperty(MAPREDUCE_FRAMEWORK_NAME, LOCAL);
+            if (properties.getProperty(MRConfiguration.FRAMEWORK_NAME) == null) {
+                properties.setProperty(MRConfiguration.FRAMEWORK_NAME, LOCAL);
             }
-            properties.setProperty(JOB_TRACKER_LOCATION, LOCAL);
+            properties.setProperty(MRConfiguration.JOB_TRACKER, LOCAL);
             properties.setProperty(FILE_SYSTEM_LOCATION, "file:///");
             properties.setProperty(ALTERNATIVE_FILE_SYSTEM_LOCATION, "file:///");
 
@@ -190,7 +189,7 @@ public abstract class HExecutionEngine implements ExecutionEngine {
         // the properties
         Utils.recomputeProperties(jc, properties);
 
-        cluster = jc.get(JOB_TRACKER_LOCATION);
+        cluster = jc.get(MRConfiguration.JOB_TRACKER);
         nameNode = jc.get(FILE_SYSTEM_LOCATION);
         if (nameNode == null) {
             nameNode = (String) pigContext.getProperties().get(ALTERNATIVE_FILE_SYSTEM_LOCATION);
@@ -200,7 +199,7 @@ public abstract class HExecutionEngine implements ExecutionEngine {
             if (!cluster.contains(":") && !cluster.equalsIgnoreCase(LOCAL)) {
                 cluster = cluster + ":50020";
             }
-            properties.setProperty(JOB_TRACKER_LOCATION, cluster);
+            properties.setProperty(MRConfiguration.JOB_TRACKER, cluster);
         }
 
         if (nameNode != null && nameNode.length() > 0) {
@@ -217,7 +216,7 @@ public abstract class HExecutionEngine implements ExecutionEngine {
 
         if (cluster != null && !cluster.equalsIgnoreCase(LOCAL)) {
             LOG.info("Connecting to map-reduce job tracker at: "
-                    + jc.get(JOB_TRACKER_LOCATION));
+                    + jc.get(MRConfiguration.JOB_TRACKER));
         }
     }
 
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java
index 3852f119f..fc20cf08d 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java
@@ -452,7 +452,7 @@ public class JobControlCompiler{
             return false;
         }
 
-        int reducers = conf.getInt("mapred.reduce.tasks", 1);
+        int reducers = conf.getInt(MRConfiguration.REDUCE_TASKS, 1);
         log.info("No of reducers: " + reducers);
         if (reducers > 1) {
             return false;
@@ -509,15 +509,16 @@ public class JobControlCompiler{
             ss.addSettingsToConf(mro, conf);
         }
 
-        conf.set("mapred.mapper.new-api", "true");
-        conf.set("mapred.reducer.new-api", "true");
+        conf.set(MRConfiguration.MAPPER_NEW_API, "true");
+        conf.set(MRConfiguration.REDUCER_NEW_API, "true");
 
-        String buffPercent = conf.get("mapred.job.reduce.markreset.buffer.percent");
+        String buffPercent = conf.get(MRConfiguration.JOB_REDUCE_MARKRESET_BUFFER_PERCENT);
         if (buffPercent == null || Double.parseDouble(buffPercent) <= 0) {
-            log.info("mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3");
-            conf.set("mapred.job.reduce.markreset.buffer.percent", "0.3");
+            log.info(MRConfiguration.JOB_REDUCE_MARKRESET_BUFFER_PERCENT + " is not set, set to default 0.3");
+            conf.set(MRConfiguration.JOB_REDUCE_MARKRESET_BUFFER_PERCENT, "0.3");
         }else{
-            log.info("mapred.job.reduce.markreset.buffer.percent is set to " + conf.get("mapred.job.reduce.markreset.buffer.percent"));
+            log.info(MRConfiguration.JOB_REDUCE_MARKRESET_BUFFER_PERCENT + " is set to " +
+                     conf.get(MRConfiguration.JOB_REDUCE_MARKRESET_BUFFER_PERCENT));
         }
 
         configureCompression(conf);
@@ -570,7 +571,7 @@ public class JobControlCompiler{
                     // override with the default conf to run in local mode
                     for (Entry<String, String> entry : defaultConf) {
                         String key = entry.getKey();
-                        if (key.equals("mapred.reduce.tasks") || key.equals("mapreduce.job.reduces")) {
+                        if (key.equals(MRConfiguration.REDUCE_TASKS) || key.equals(MRConfiguration.JOB_REDUCES)) {
                             // this must not be set back to the default in case it has been set to 0 for example.
                             continue;
                         }
@@ -646,7 +647,7 @@ public class JobControlCompiler{
             // this is for unit tests since some don't create PigServer
 
             // if user specified the job name using -D switch, Pig won't reset the name then.
-            if (System.getProperty("mapred.job.name") == null &&
+            if (System.getProperty(MRConfiguration.JOB_NAME) == null &&
                     pigContext.getProperties().getProperty(PigContext.JOB_NAME) != null){
                 nwJob.setJobName(pigContext.getProperties().getProperty(PigContext.JOB_NAME));
             }
@@ -657,7 +658,7 @@ public class JobControlCompiler{
                 String jobPriority = pigContext.getProperties().getProperty(PigContext.JOB_PRIORITY).toUpperCase();
                 try {
                     // Allow arbitrary case; the Hadoop job priorities are all upper case.
-                    conf.set("mapred.job.priority", JobPriority.valueOf(jobPriority).toString());
+                    conf.set(MRConfiguration.JOB_PRIORITY, JobPriority.valueOf(jobPriority).toString());
 
                 } catch (IllegalArgumentException e) {
                     StringBuffer sb = new StringBuffer("The job priority must be one of [");
@@ -936,8 +937,8 @@ public class JobControlCompiler{
             if (pigContext.getExecType() == ExecType.MAPREDUCE) {
                 String newfiles = conf.get("alternative.mapreduce.job.cache.files");
                 if (newfiles!=null) {
-                    String files = conf.get("mapreduce.job.cache.files");
-                    conf.set("mapreduce.job.cache.files",
+                    String files = conf.get(MRConfiguration.JOB_CACHE_FILES);
+                    conf.set(MRConfiguration.JOB_CACHE_FILES,
                             files == null ? newfiles.toString() : files + "," + newfiles);
                 }
             }
@@ -958,11 +959,13 @@ public class JobControlCompiler{
 
     public static void configureCompression(Configuration conf) {
         // Convert mapred.output.* to output.compression.*, See PIG-1791
-        if( "true".equals( conf.get( "mapred.output.compress" ) ) ) {
+        if( "true".equals( conf.get(MRConfiguration.OUTPUT_COMPRESS) ) ) {
             conf.set( "output.compression.enabled",  "true" );
-            String codec = conf.get( "mapred.output.compression.codec" );
+            String codec = conf.get(MRConfiguration.OUTPUT_COMPRESSION_CODEC);
             if( codec == null ) {
-                throw new IllegalArgumentException("'mapred.output.compress' is set but no value is specified for 'mapred.output.compression.codec'." );
+                throw new IllegalArgumentException("'" + MRConfiguration.OUTPUT_COMPRESS +
+                        "' is set but no value is specified for '" +
+                        MRConfiguration.OUTPUT_COMPRESSION_CODEC + "'." );
             } else {
                 conf.set( "output.compression.codec", codec );
             }
@@ -1009,7 +1012,7 @@ public class JobControlCompiler{
         mro.requestedParallelism = jobParallelism;
 
         // finally set the number of reducers
-        conf.setInt("mapred.reduce.tasks", jobParallelism);
+        conf.setInt(MRConfiguration.REDUCE_TASKS, jobParallelism);
     }
 
     /**
@@ -1506,6 +1509,7 @@ public class JobControlCompiler{
                             new Path(FileLocalizer.getTemporaryPath(pigContext).toString());
                     FileSystem fs = dst.getFileSystem(conf);
                     fs.copyFromLocalFile(src, dst);
+                    fs.setReplication(dst, (short)conf.getInt(MRConfiguration.SUMIT_REPLICATION, 3));
 
                     // Construct the dst#srcName uri for DistributedCache
                     URI dstURI = null;
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRConfiguration.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRConfiguration.java
new file mode 100644
index 000000000..7d87bd9e4
--- /dev/null
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRConfiguration.java
@@ -0,0 +1,64 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.pig.backend.hadoop.executionengine.mapReduceLayer;
+
+public class MRConfiguration {
+
+    public static final String CHILD_JAVA_OPTS = "mapred.child.java.opts";
+    public static final String FILEOUTPUTCOMMITTER_MARKSUCCESSFULJOBS = "mapreduce.fileoutputcommitter.marksuccessfuljobs";
+    public static final String FRAMEWORK_NAME = "mapreduce.framework.name";
+    public static final String INPUT_DIR = "mapred.input.dir";
+    public static final String INPUT_DIR_RECURSIVE = "mapred.input.dir.recursive";
+    public static final String INPUTFORMAT_CLASS = "mapreduce.inputformat.class";
+    public static final String IO_SORT_MB = "mapred.io.sort.mb";
+    public static final String JAR = "mapred.jar";
+    public static final String JOB_APPLICATION_ATTEMPT_ID = "mapreduce.job.application.attempt.id";
+    public static final String JOB_CACHE_FILES = "mapreduce.job.cache.files";
+    public static final String JOB_CREDENTIALS_BINARY = "mapreduce.job.credentials.binary";
+    public static final String JOB_CREDENTIALS_JSON = "mapreduce.job.credentials.json";
+    public static final String JOB_END_NOTIFICATION_RETRY_INTERVAL = "mapreduce.job.end-notification.retry.interval";
+    public static final String JOB_HDFS_SERVERS = "mapreduce.job.hdfs-servers";
+    public static final String JOB_ID = "mapred.job.id";
+    public static final String JOB_NAME = "mapred.job.name";
+    public static final String JOB_PRIORITY = "mapred.job.priority";
+    public static final String JOB_QUEUE_NAME = "mapred.job.queue.name";
+    public static final String JOB_REDUCE_MARKRESET_BUFFER_PERCENT = "mapred.job.reduce.markreset.buffer.percent";
+    public static final String JOB_TRACKER = "mapred.job.tracker";
+    public static final String JOB_TRACKER_HTTP_ADDRESS = "mapred.job.tracker.http.address";
+    public static final String JOB_REDUCES = "mapreduce.job.reduces";
+    public static final String LINERECORDREADER_MAXLENGTH = "mapred.linerecordreader.maxlength";
+    public static final String MAP_MAX_ATTEMPTS = "mapred.map.max.attempts";
+    public static final String MAP_TASKS = "mapred.map.tasks";
+    public static final String MAPPER_NEW_API = "mapred.mapper.new-api";
+    public static final String MAX_SPLIT_SIZE = "mapred.max.split.size";
+    public static final String OUTPUT_BASENAME = "mapreduce.output.basename";
+    public static final String OUTPUT_COMPRESS = "mapred.output.compress";
+    public static final String OUTPUT_COMPRESSION_CODEC = "mapred.output.compression.codec";
+    public static final String OUTPUT_DIR = "mapred.output.dir";
+    public static final String REDUCE_MAX_ATTEMPTS = "mapred.reduce.max.attempts";
+    public static final String REDUCE_TASKS = "mapred.reduce.tasks";
+    public static final String REDUCER_NEW_API = "mapred.reducer.new-api";
+    public static final String SUMIT_REPLICATION = "mapred.submit.replication";
+    public static final String TASK_ID = "mapred.task.id";
+    public static final String TASK_IS_MAP = "mapred.task.is.map";
+    public static final String TASK_PARTITION = "mapred.task.partition";
+    public static final String TASK_TIMEOUT = "mapred.task.timeout";
+    public static final String TEXTOUTPUTFORMAT_SEPARATOR = "mapred.textoutputformat.separator";
+    public static final String WORK_OUPUT_DIR = "mapred.work.output.dir";
+
+}
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java
index 65a8c3b02..d0dea2fd6 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java
@@ -49,7 +49,6 @@ import org.apache.pig.PigWarning;
 import org.apache.pig.backend.BackendException;
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
-import org.apache.pig.backend.hadoop.executionengine.HExecutionEngine;
 import org.apache.pig.backend.hadoop.executionengine.JobCreationException;
 import org.apache.pig.backend.hadoop.executionengine.Launcher;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.LastInputStreamingOptimizer;
@@ -90,9 +89,6 @@ public class MapReduceLauncher extends Launcher{
 
     public static final String SUCCEEDED_FILE_NAME = "_SUCCESS";
 
-    public static final String SUCCESSFUL_JOB_OUTPUT_DIR_MARKER =
-            "mapreduce.fileoutputcommitter.marksuccessfuljobs";
-
     private static final Log log = LogFactory.getLog(MapReduceLauncher.class);
 
     private boolean aggregateWarning = false;
@@ -254,8 +250,8 @@ public class MapReduceLauncher extends Launcher{
             String jobTrackerLoc;
             JobConf jobConf = jobsWithoutIds.get(0).getJobConf();
             try {
-                String port = jobConf.get("mapred.job.tracker.http.address");
-                String jobTrackerAdd = jobConf.get(HExecutionEngine.JOB_TRACKER_LOCATION);
+                String port = jobConf.get(MRConfiguration.JOB_TRACKER_HTTP_ADDRESS);
+                String jobTrackerAdd = jobConf.get(MRConfiguration.JOB_TRACKER);
 
                 jobTrackerLoc = jobTrackerAdd.substring(0,jobTrackerAdd.indexOf(":"))
                         + port.substring(port.indexOf(":"));
@@ -721,7 +717,7 @@ public class MapReduceLauncher extends Launcher{
     }
 
     private boolean shouldMarkOutputDir(Job job) {
-        return job.getJobConf().getBoolean(SUCCESSFUL_JOB_OUTPUT_DIR_MARKER,
+        return job.getJobConf().getBoolean(MRConfiguration.FILEOUTPUTCOMMITTER_MARKSUCCESSFULJOBS,
                 false);
     }
 
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigOutputFormat.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigOutputFormat.java
index 854e59823..d8cb704c5 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigOutputFormat.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigOutputFormat.java
@@ -52,11 +52,6 @@ import org.apache.pig.impl.util.ObjectSerializer;
 public class PigOutputFormat extends OutputFormat<WritableComparable, Tuple> {
     
     private enum Mode { SINGLE_STORE, MULTI_STORE};
-
-    /** hadoop job output directory */
-    public static final String MAPRED_OUTPUT_DIR = "mapred.output.dir";
-    /** hadoop partition number */ 
-    public static final String MAPRED_TASK_PARTITION = "mapred.task.partition";
     
     /** the temporary directory for the multi store */
     public static final String PIG_MAPRED_OUTPUT_DIR = "pig.mapred.output.dir";
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/tez/PigProcessor.java b/src/org/apache/pig/backend/hadoop/executionengine/tez/PigProcessor.java
index 17f3d899b..ddbe186f4 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/tez/PigProcessor.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/tez/PigProcessor.java
@@ -31,6 +31,7 @@ import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.pig.PigException;
 import org.apache.pig.backend.executionengine.ExecException;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigHadoopLogger;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.UDFFinishVisitor;
@@ -120,7 +121,7 @@ public class PigProcessor implements LogicalIOProcessor {
         PigContext pc = (PigContext) ObjectSerializer.deserialize(conf.get("pig.pigContext"));
 
         // To determine front-end in UDFContext
-        conf.set("mapreduce.job.application.attempt.id", processorContext.getUniqueIdentifier());
+        conf.set(MRConfiguration.JOB_APPLICATION_ATTEMPT_ID, processorContext.getUniqueIdentifier());
         UDFContext.getUDFContext().addJobConf(conf);
         UDFContext.getUDFContext().deserialize();
 
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/tez/TezDagBuilder.java b/src/org/apache/pig/backend/hadoop/executionengine/tez/TezDagBuilder.java
index 7e86dd151..fb86aa874 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/tez/TezDagBuilder.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/tez/TezDagBuilder.java
@@ -62,6 +62,7 @@ import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCo
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.PigGroupingPartitionWritableComparator;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.PigGroupingTupleWritableComparator;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.PigSecondaryKeyGroupComparator;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PhyPlanSetter;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigBigDecimalRawComparator;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigBigIntegerRawComparator;
@@ -310,7 +311,7 @@ public class TezDagBuilder extends TezOpPlanVisitor {
                     edge.getIntermediateOutputKeyComparatorClass());
         }
 
-        conf.setBoolean("mapred.mapper.new-api", true);
+        conf.setBoolean(MRConfiguration.MAPPER_NEW_API, true);
         conf.set("pig.pigContext", ObjectSerializer.serialize(pc));
         conf.set("udf.import.list",
                 ObjectSerializer.serialize(PigContext.getPackageImportList()));
@@ -375,7 +376,7 @@ public class TezDagBuilder extends TezOpPlanVisitor {
                 MRCombiner.class.getName());
         conf.set(MRJobConfig.COMBINE_CLASS_ATTR,
                 PigCombiner.Combine.class.getName());
-        conf.setBoolean("mapred.mapper.new-api", true);
+        conf.setBoolean(MRConfiguration.MAPPER_NEW_API, true);
         conf.set("pig.pigContext", ObjectSerializer.serialize(pc));
         conf.set("udf.import.list",
                 ObjectSerializer.serialize(PigContext.getPackageImportList()));
@@ -431,8 +432,8 @@ public class TezDagBuilder extends TezOpPlanVisitor {
         payloadConf.set("udf.import.list",
                 ObjectSerializer.serialize(PigContext.getPackageImportList()));
         payloadConf.set("exectype", "TEZ");
-        payloadConf.setBoolean("mapred.mapper.new-api", true);
-        payloadConf.setClass("mapreduce.inputformat.class",
+        payloadConf.setBoolean(MRConfiguration.MAPPER_NEW_API, true);
+        payloadConf.setClass(MRConfiguration.INPUTFORMAT_CLASS,
                 PigInputFormat.class, InputFormat.class);
 
         // Set parent plan for all operators in the Tez plan.
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/tez/TezResourceManager.java b/src/org/apache/pig/backend/hadoop/executionengine/tez/TezResourceManager.java
index e42c6d937..464a6668e 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/tez/TezResourceManager.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/tez/TezResourceManager.java
@@ -28,6 +28,7 @@ import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.mapreduce.Job;
 import org.apache.hadoop.yarn.api.records.LocalResource;
 import org.apache.hadoop.yarn.api.records.LocalResourceType;
 import org.apache.hadoop.yarn.api.records.LocalResourceVisibility;
@@ -41,6 +42,7 @@ public class TezResourceManager {
     private boolean inited = false;
     private Path stagingDir;
     private FileSystem remoteFs;
+    private Configuration conf;
     public Map<String, Path> resources = new HashMap<String, Path>();
 
     static public TezResourceManager getInstance() {
@@ -53,8 +55,9 @@ public class TezResourceManager {
     public void init(PigContext pigContext, Configuration conf) throws IOException {
         if (!inited) {
             this.stagingDir = ((HPath)FileLocalizer.getTemporaryResourcePath(pigContext)).getPath();
-            remoteFs = FileSystem.get(conf);
-            inited = true;
+            this.remoteFs = FileSystem.get(conf);
+            this.conf = conf;
+            this.inited = true;
         }
     }
 
@@ -77,6 +80,7 @@ public class TezResourceManager {
             if (uri.toString().startsWith("file:")) {
                 Path remoteFsPath = remoteFs.makeQualified(new Path(stagingDir, resourceName));
                 remoteFs.copyFromLocalFile(resourcePath, remoteFsPath);
+                remoteFs.setReplication(remoteFsPath, (short)conf.getInt(Job.SUBMIT_REPLICATION, 3));
                 resources.put(resourceName, remoteFsPath);
                 return remoteFsPath;
             }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/tez/util/MRToTezHelper.java b/src/org/apache/pig/backend/hadoop/executionengine/tez/util/MRToTezHelper.java
index 208ff9900..586688968 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/tez/util/MRToTezHelper.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/tez/util/MRToTezHelper.java
@@ -31,6 +31,7 @@ import org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter;
 import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
 import org.apache.hadoop.yarn.conf.YarnConfiguration;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.classification.InterfaceAudience;
 import org.apache.tez.dag.api.TezConfiguration;
 import org.apache.tez.mapreduce.hadoop.DeprecatedKeys;
@@ -61,7 +62,7 @@ public class MRToTezHelper {
         mrSettingsToRetain.add(FileInputFormat.INPUT_DIR_RECURSIVE);
 
         // FileOutputFormat
-        mrSettingsToRetain.add("mapreduce.output.basename");
+        mrSettingsToRetain.add(MRConfiguration.OUTPUT_BASENAME);
         mrSettingsToRetain.add(FileOutputFormat.COMPRESS);
         mrSettingsToRetain.add(FileOutputFormat.COMPRESS_CODEC);
         mrSettingsToRetain.add(FileOutputFormat.COMPRESS_TYPE);
@@ -120,9 +121,9 @@ public class MRToTezHelper {
                 + dagAMConf.getInt(MRJobConfig.MR_AM_MAX_ATTEMPTS,
                         MRJobConfig.DEFAULT_MR_AM_MAX_ATTEMPTS));
 
-        if (tezConf.get("mapreduce.job.credentials.binary") != null) {
+        if (tezConf.get(MRConfiguration.JOB_CREDENTIALS_BINARY) != null) {
             dagAMConf.setIfUnset(TezConfiguration.TEZ_CREDENTIALS_PATH,
-                    tezConf.get("mapreduce.job.credentials.binary"));
+                    tezConf.get(MRConfiguration.JOB_CREDENTIALS_BINARY));
         }
 
         //TODO: Strip out all MR settings
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/tez/util/SecurityHelper.java b/src/org/apache/pig/backend/hadoop/executionengine/tez/util/SecurityHelper.java
index 4737b91c3..1be260193 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/tez/util/SecurityHelper.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/tez/util/SecurityHelper.java
@@ -30,6 +30,7 @@ import org.apache.hadoop.io.Text;
 import org.apache.hadoop.mapreduce.MRJobConfig;
 import org.apache.hadoop.mapreduce.security.TokenCache;
 import org.apache.hadoop.security.Credentials;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.classification.InterfaceAudience;
 import org.codehaus.jackson.JsonParseException;
 import org.codehaus.jackson.map.JsonMappingException;
@@ -54,14 +55,14 @@ public class SecurityHelper {
             Credentials credentials) throws IOException {
         // add tokens and secrets coming from a token storage file
         String binaryTokenFilename = conf
-                .get("mapreduce.job.credentials.binary");
+                .get(MRConfiguration.JOB_CREDENTIALS_BINARY);
         if (binaryTokenFilename != null) {
             Credentials binary = Credentials.readTokenStorageFile(new Path(
                     "file:///" + binaryTokenFilename), conf);
             credentials.addAll(binary);
         }
         // add secret keys coming from a json file
-        String tokensFileName = conf.get("mapreduce.job.credentials.json");
+        String tokensFileName = conf.get(MRConfiguration.JOB_CREDENTIALS_JSON);
         if (tokensFileName != null) {
             LOG.info("loading user's secret keys from " + tokensFileName);
             String localFileName = new Path(tokensFileName).toUri().getPath();
diff --git a/src/org/apache/pig/backend/hadoop/streaming/HadoopExecutableManager.java b/src/org/apache/pig/backend/hadoop/streaming/HadoopExecutableManager.java
index c369b264b..951146f6d 100644
--- a/src/org/apache/pig/backend/hadoop/streaming/HadoopExecutableManager.java
+++ b/src/org/apache/pig/backend/hadoop/streaming/HadoopExecutableManager.java
@@ -34,6 +34,7 @@ import org.apache.hadoop.mapreduce.InputSplit;
 import org.apache.hadoop.mapreduce.lib.input.FileSplit;
 import org.apache.pig.PigException;
 import org.apache.pig.backend.executionengine.ExecException;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigSplit;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStream;
@@ -98,7 +99,7 @@ public class HadoopExecutableManager extends ExecutableManager {
         
         // Save the taskid
         // TODO Get an equivalent property in Tez mode (currently this returns null)
-        taskId = job.get("mapred.task.id");
+        taskId = job.get(MRConfiguration.TASK_ID);
     }
     
     protected void exec() throws IOException {
@@ -140,16 +141,13 @@ public class HadoopExecutableManager extends ExecutableManager {
                 for (int i=1; i < outputSpecs.size(); ++i) {
                     String fileName = outputSpecs.get(i).getName();
                     try {
-                        int partition = job.getInt("mapred.task.partition", -1);
-                        fs.copyFromLocalFile(false, true, new Path(fileName), 
-                                             new Path(
-                                                     new Path(scriptOutputDir, 
-                                                              fileName), 
-                                                     getOutputName(partition))
-                                            );
+                        int partition = job.getInt(MRConfiguration.TASK_PARTITION, -1);
+                        Path dst = new Path(new Path(scriptOutputDir, fileName), getOutputName(partition));
+                        fs.copyFromLocalFile(false, true, new Path(fileName), dst);
+                        fs.setReplication(dst, (short)job.getInt(MRConfiguration.SUMIT_REPLICATION, 3));
                     } catch (IOException ioe) {
                         int errCode = 6014; 
-                        String msg = "Failed to save secondary output '" + 
+                        String msg = "Failed to save secondary output '" +
                         fileName + "' of task: " + taskId;
                         throw new ExecException(msg, errCode, PigException.REMOTE_ENVIRONMENT, ioe);
                     }
@@ -200,7 +198,7 @@ public class HadoopExecutableManager extends ExecutableManager {
 
         processError("\nCommand: " + command);
         processError("\nStart time: " + new Date(System.currentTimeMillis()));
-        if (job.getBoolean("mapred.task.is.map", false)) {
+        if (job.getBoolean(MRConfiguration.TASK_IS_MAP, false)) {
             MapContext context = (MapContext)PigMapReduce.sJobContext;
             PigSplit pigSplit = (PigSplit)context.getInputSplit();
             int numPaths = pigSplit.getNumPaths();
diff --git a/src/org/apache/pig/builtin/PigStorage.java b/src/org/apache/pig/builtin/PigStorage.java
index 675f138fc..9d9d7c46d 100644
--- a/src/org/apache/pig/builtin/PigStorage.java
+++ b/src/org/apache/pig/builtin/PigStorage.java
@@ -62,6 +62,7 @@ import org.apache.pig.StoreFunc;
 import org.apache.pig.StoreFuncInterface;
 import org.apache.pig.StoreMetadata;
 import org.apache.pig.backend.executionengine.ExecException;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigSplit;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigTextInputFormat;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigTextOutputFormat;
@@ -452,7 +453,7 @@ LoadPushDown, LoadMetadata, StoreMetadata, OverwritableStoreFunc {
 
     @Override
     public void setStoreLocation(String location, Job job) throws IOException {
-        job.getConfiguration().set("mapred.textoutputformat.separator", "");
+        job.getConfiguration().set(MRConfiguration.TEXTOUTPUTFORMAT_SEPARATOR, "");
         FileOutputFormat.setOutputPath(job, new Path(location));
 
         if( "true".equals( job.getConfiguration().get( "output.compression.enabled" ) ) ) {
@@ -596,7 +597,7 @@ LoadPushDown, LoadMetadata, StoreMetadata, OverwritableStoreFunc {
     @Override
     public void cleanupOutput(POStore store, Job job) throws IOException {
         Configuration conf = job.getConfiguration();
-        String output = conf.get("mapred.output.dir");
+        String output = conf.get(MRConfiguration.OUTPUT_DIR);
         Path outputPath = null;
         if (output != null)
             outputPath = new Path(output);
diff --git a/src/org/apache/pig/builtin/TrevniStorage.java b/src/org/apache/pig/builtin/TrevniStorage.java
index b6ae63c19..c468be2e4 100644
--- a/src/org/apache/pig/builtin/TrevniStorage.java
+++ b/src/org/apache/pig/builtin/TrevniStorage.java
@@ -43,6 +43,7 @@ import org.apache.hadoop.mapreduce.TaskAttemptContext;
 import org.apache.hadoop.mapreduce.lib.input.FileSplit;
 import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
 import org.apache.pig.LoadPushDown;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigFileInputFormat;
 import org.apache.pig.data.Tuple;
 import org.apache.pig.impl.util.Utils;
@@ -108,7 +109,7 @@ public class TrevniStorage extends AvroStorage implements LoadPushDown{
       @Override protected  List<FileStatus> listStatus(final JobContext job)
           throws IOException {
         List<FileStatus> results = Lists.newArrayList();
-        job.getConfiguration().setBoolean("mapred.input.dir.recursive", true);
+        job.getConfiguration().setBoolean(MRConfiguration.INPUT_DIR_RECURSIVE, true);
         for (FileStatus file : super.listStatus(job)) {
           if (Utils.VISIBLE_FILES.accept(file.getPath())) {
             results.add(file);
diff --git a/src/org/apache/pig/data/SchemaTupleFrontend.java b/src/org/apache/pig/data/SchemaTupleFrontend.java
index 9544d6caa..6a414837c 100644
--- a/src/org/apache/pig/data/SchemaTupleFrontend.java
+++ b/src/org/apache/pig/data/SchemaTupleFrontend.java
@@ -36,7 +36,7 @@ import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.filecache.DistributedCache;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
-import org.apache.pig.ExecType;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.data.SchemaTupleClassGenerator.GenContext;
 import org.apache.pig.data.utils.StructuresHelper.Pair;
 import org.apache.pig.data.utils.StructuresHelper.SchemaKey;
@@ -149,6 +149,7 @@ public class SchemaTupleFrontend {
                 }
                 try {
                     fs.copyFromLocalFile(src, dst);
+                    fs.setReplication(dst, (short)conf.getInt(MRConfiguration.SUMIT_REPLICATION, 3));
                 } catch (IOException e) {
                     throw new RuntimeException("Unable to copy from local filesystem to HDFS, src = "
                             + src + ", dst = " + dst, e);
diff --git a/src/org/apache/pig/impl/PigContext.java b/src/org/apache/pig/impl/PigContext.java
index 46b83de30..f304f70b9 100644
--- a/src/org/apache/pig/impl/PigContext.java
+++ b/src/org/apache/pig/impl/PigContext.java
@@ -62,6 +62,7 @@ import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.backend.executionengine.ExecutionEngine;
 import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
 import org.apache.pig.backend.hadoop.datastorage.HDataStorage;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.impl.streaming.ExecutableManager;
 import org.apache.pig.impl.streaming.StreamingCommand;
 import org.apache.pig.impl.util.JarManager;
@@ -314,7 +315,7 @@ public class PigContext implements Serializable {
     }
 
     public void setJobtrackerLocation(String newLocation) {
-        executionEngine.setProperty("mapred.job.tracker", newLocation);
+        executionEngine.setProperty(MRConfiguration.JOB_TRACKER, newLocation);
     }
 
     /**
diff --git a/src/org/apache/pig/impl/builtin/DefaultIndexableLoader.java b/src/org/apache/pig/impl/builtin/DefaultIndexableLoader.java
index 22370f98d..dcb76d90b 100644
--- a/src/org/apache/pig/impl/builtin/DefaultIndexableLoader.java
+++ b/src/org/apache/pig/impl/builtin/DefaultIndexableLoader.java
@@ -37,6 +37,7 @@ import org.apache.pig.LoadFunc;
 import org.apache.pig.PigException;
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigSplit;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
@@ -200,7 +201,7 @@ public class DefaultIndexableLoader extends LoadFunc implements IndexableLoadFun
         
         // Hadoop security need this property to be set
         if (System.getenv("HADOOP_TOKEN_FILE_LOCATION") != null) {
-            conf.set("mapreduce.job.credentials.binary", 
+            conf.set(MRConfiguration.JOB_CREDENTIALS_BINARY, 
                     System.getenv("HADOOP_TOKEN_FILE_LOCATION"));
         }
         
diff --git a/src/org/apache/pig/impl/builtin/GFCross.java b/src/org/apache/pig/impl/builtin/GFCross.java
index fbb988395..8963fd490 100644
--- a/src/org/apache/pig/impl/builtin/GFCross.java
+++ b/src/org/apache/pig/impl/builtin/GFCross.java
@@ -24,6 +24,7 @@ import org.apache.hadoop.conf.Configuration;
 
 import org.apache.pig.EvalFunc;
 import org.apache.pig.backend.executionengine.ExecException;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.data.BagFactory;
 import org.apache.pig.data.DataBag;
 import org.apache.pig.data.Tuple;
@@ -47,7 +48,7 @@ public class GFCross extends EvalFunc<DataBag> {
             parallelism = DEFAULT_PARALLELISM;
             Configuration cfg = UDFContext.getUDFContext().getJobConf();
             if (cfg != null) {
-                String s = cfg.get("mapred.reduce.tasks");
+                String s = cfg.get(MRConfiguration.REDUCE_TASKS);
                 if (s == null) {
                     throw new IOException("Unable to determine parallelism from job conf");
                 }
diff --git a/src/org/apache/pig/impl/builtin/ReadScalars.java b/src/org/apache/pig/impl/builtin/ReadScalars.java
index ced40659e..8f591495d 100644
--- a/src/org/apache/pig/impl/builtin/ReadScalars.java
+++ b/src/org/apache/pig/impl/builtin/ReadScalars.java
@@ -23,6 +23,7 @@ import java.util.Map;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.pig.EvalFunc;
 import org.apache.pig.backend.executionengine.ExecException;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.data.DataBag;
 import org.apache.pig.data.DataType;
 import org.apache.pig.data.Tuple;
@@ -90,7 +91,7 @@ public class ReadScalars extends EvalFunc<Object> {
                 // Hadoop security need this property to be set
                 Configuration conf = UDFContext.getUDFContext().getJobConf();
                 if (System.getenv("HADOOP_TOKEN_FILE_LOCATION") != null) {
-                    conf.set("mapreduce.job.credentials.binary",
+                    conf.set(MRConfiguration.JOB_CREDENTIALS_BINARY,
                             System.getenv("HADOOP_TOKEN_FILE_LOCATION"));
                 }
                 loader = new ReadToEndLoader(
diff --git a/src/org/apache/pig/impl/builtin/StreamingUDF.java b/src/org/apache/pig/impl/builtin/StreamingUDF.java
index 6cc967d2f..954eec255 100644
--- a/src/org/apache/pig/impl/builtin/StreamingUDF.java
+++ b/src/org/apache/pig/impl/builtin/StreamingUDF.java
@@ -39,6 +39,7 @@ import org.apache.pig.EvalFunc;
 import org.apache.pig.ExecType;
 import org.apache.pig.ExecTypeProvider;
 import org.apache.pig.backend.executionengine.ExecException;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.data.Tuple;
 import org.apache.pig.data.TupleFactory;
 import org.apache.pig.impl.io.BufferedPositionedInputStream;
@@ -77,14 +78,14 @@ public class StreamingUDF extends EvalFunc<Object> {
     private static final int STD_ERR_OUTPUT_PATH = 7; //File for output from when user writes to standard error.
     private static final int CONTROLLER_LOG_FILE_PATH = 8; //Controller log file logs progress through the controller script not user code.
     private static final int IS_ILLUSTRATE = 9; //Controller captures output differently in illustrate vs running.
-    
+
     private String language;
     private String filePath;
     private String funcName;
     private Schema schema;
     private ExecType execType;
     private String isIllustrate;
-    
+
     private boolean initialized = false;
     private ScriptingOutputCapturer soc;
 
@@ -92,7 +93,7 @@ public class StreamingUDF extends EvalFunc<Object> {
     private ProcessErrorThread stderrThread; // thread to get process stderr
     private ProcessInputThread stdinThread; // thread to send input to process
     private ProcessOutputThread stdoutThread; //thread to read output from process
-    
+
     private InputHandler inputHandler;
     private OutputHandler outputHandler;
 
@@ -105,13 +106,13 @@ public class StreamingUDF extends EvalFunc<Object> {
 
     private static final Object ERROR_OUTPUT = new Object();
     private static final Object NULL_OBJECT = new Object(); //BlockingQueue can't have null.  Use place holder object instead.
-    
+
     private volatile StreamingUDFException outerrThreadsError;
-    
+
     public static final String TURN_ON_OUTPUT_CAPTURING = "TURN_ON_OUTPUT_CAPTURING";
 
-    public StreamingUDF(String language, 
-                        String filePath, String funcName, 
+    public StreamingUDF(String language,
+                        String filePath, String funcName,
                         String outputSchemaString, String schemaLineNumber,
                         String execType, String isIllustrate)
                                 throws StreamingUDFOutputSchemaException, ExecException {
@@ -120,7 +121,7 @@ public class StreamingUDF extends EvalFunc<Object> {
         this.funcName = funcName;
         try {
             this.schema = Utils.getSchemaFromString(outputSchemaString);
-            //ExecTypeProvider.fromString doesn't seem to load the ExecTypes in 
+            //ExecTypeProvider.fromString doesn't seem to load the ExecTypes in
             //mapreduce mode so we'll try to figure out the exec type ourselves.
             if (execType.equals("local")) {
                 this.execType = ExecType.LOCAL;
@@ -139,7 +140,7 @@ public class StreamingUDF extends EvalFunc<Object> {
         }
         this.isIllustrate = isIllustrate;
     }
-    
+
     @Override
     public Object exec(Tuple input) throws IOException {
         if (!initialized) {
@@ -174,7 +175,7 @@ public class StreamingUDF extends EvalFunc<Object> {
 
         String jarPath = conf.get("mapreduce.job.jar");
         if (jarPath == null) {
-            jarPath = conf.get("mapred.jar");
+            jarPath = conf.get(MRConfiguration.JAR);
         }
         String jobDir;
         if (jarPath != null) {
@@ -182,7 +183,7 @@ public class StreamingUDF extends EvalFunc<Object> {
         } else {
             jobDir = "";
         }
-        
+
         String standardOutputRootWriteLocation = soc.getStandardOutputRootWriteLocation();
         String controllerLogFileName, outFileName, errOutFileName;
 
@@ -202,8 +203,8 @@ public class StreamingUDF extends EvalFunc<Object> {
         command[PATH_TO_CONTROLLER_FILE] = getControllerPath(jobDir);
         int lastSeparator = filePath.lastIndexOf(File.separator) + 1;
         command[UDF_FILE_NAME] = filePath.substring(lastSeparator);
-        command[UDF_FILE_PATH] = lastSeparator <= 0 ? 
-                "." : 
+        command[UDF_FILE_PATH] = lastSeparator <= 0 ?
+                "." :
                 filePath.substring(0, lastSeparator - 1);
         command[UDF_NAME] = funcName;
         String fileCachePath = jobDir + filePath.substring(0, lastSeparator);
@@ -212,9 +213,9 @@ public class StreamingUDF extends EvalFunc<Object> {
         command[STD_ERR_OUTPUT_PATH] = errOutFileName;
         command[CONTROLLER_LOG_FILE_PATH] = controllerLogFileName;
         command[IS_ILLUSTRATE] = isIllustrate;
-        
+
         ensureUserFileAvailable(command, fileCachePath);
-        
+
         return command;
     }
 
@@ -265,7 +266,7 @@ public class StreamingUDF extends EvalFunc<Object> {
             throw new ExecException("Unrecognized streamingUDF language: " + language);
         }
     }
-    
+
     private void createInputHandlers() throws ExecException, FrontendException {
         PigStreamingUDF serializer = new PigStreamingUDF();
         this.inputHandler = new StreamingUDFInputHandler(serializer);
@@ -273,16 +274,16 @@ public class StreamingUDF extends EvalFunc<Object> {
         this.outputHandler = new StreamingUDFOutputHandler(deserializer);
     }
 
-    private void setStreams() throws IOException { 
+    private void setStreams() throws IOException {
         stdout = new DataInputStream(new BufferedInputStream(process
                 .getInputStream()));
         outputHandler.bindTo("", new BufferedPositionedInputStream(stdout),
                 0, Long.MAX_VALUE);
-        
+
         stdin = new DataOutputStream(new BufferedOutputStream(process
                 .getOutputStream()));
-        inputHandler.bindTo(stdin); 
-        
+        inputHandler.bindTo(stdin);
+
         stderr = new DataInputStream(new BufferedInputStream(process
                 .getErrorStream()));
     }
@@ -290,10 +291,10 @@ public class StreamingUDF extends EvalFunc<Object> {
     private void startThreads() {
         stdinThread = new ProcessInputThread();
         stdinThread.start();
-        
+
         stdoutThread = new ProcessOutputThread();
         stdoutThread.start();
-        
+
         stderrThread = new ProcessErrorThread();
         stderrThread.start();
     }
@@ -316,7 +317,8 @@ public class StreamingUDF extends EvalFunc<Object> {
             File controller = new File(controllerPath);
             if (!controller.exists()) {
                 File controllerFile = File.createTempFile("controller", ".py");
-                InputStream pythonControllerStream = this.getClass().getResourceAsStream(PYTHON_CONTROLLER_JAR_PATH);                try {
+                InputStream pythonControllerStream = this.getClass().getResourceAsStream(PYTHON_CONTROLLER_JAR_PATH);
+                try {
                     FileUtils.copyInputStreamToFile(pythonControllerStream, controllerFile);
                 } finally {
                     pythonControllerStream.close();
@@ -341,7 +343,7 @@ public class StreamingUDF extends EvalFunc<Object> {
     private boolean isPython() {
         return language.toLowerCase().equals("python");
     }
-    
+
     /**
      * Returns a list of file names (relative to root of pig jar) of files that need to be
      * included in the jar shipped to the cluster.
@@ -362,7 +364,7 @@ public class StreamingUDF extends EvalFunc<Object> {
             throw new ExecException("Process has already been shut down.  No way to retrieve output for input: " + input);
         }
 
-        if (ScriptingOutputCapturer.isClassCapturingOutput() && 
+        if (ScriptingOutputCapturer.isClassCapturingOutput() &&
                 !soc.isInstanceCapturingOutput()) {
             Tuple t = TupleFactory.getInstance().newTuple(TURN_ON_OUTPUT_CAPTURING);
             try {
@@ -375,7 +377,7 @@ public class StreamingUDF extends EvalFunc<Object> {
 
         try {
             if (this.getInputSchema() == null || this.getInputSchema().size() == 0) {
-                //When nothing is passed into the UDF the tuple 
+                //When nothing is passed into the UDF the tuple
                 //being sent is the full tuple for the relation.
                 //We want it to be nothing (since that's what the user wrote).
                 input = TupleFactory.getInstance().newTuple(0);
@@ -437,10 +439,10 @@ public class StreamingUDF extends EvalFunc<Object> {
             }
         }
     }
-    
+
     private static final int WAIT_FOR_ERROR_LENGTH = 500;
     private static final int MAX_WAIT_FOR_ERROR_ATTEMPTS = 5;
-    
+
     /**
      * The thread which consumes output from process
      */
@@ -475,8 +477,9 @@ public class StreamingUDF extends EvalFunc<Object> {
                         //Only write this if no other error.  Don't want to overwrite
                         //an error from the error thread.
                         if (outerrThreadsError == null) {
-                            outerrThreadsError = new StreamingUDFException(language, "Error deserializing output.  Please check that the declared outputSchema for function " +
-                                                                        funcName + " matches the data type being returned.", e);
+                            outerrThreadsError = new StreamingUDFException(
+                                    language, "Error deserializing output.  Please check that the declared outputSchema for function " +
+                                    funcName + " matches the data type being returned.", e);
                         }
                         outputQueue.put(ERROR_OUTPUT); //Need to wake main thread.
                     } catch(InterruptedException ie) {
@@ -528,7 +531,7 @@ public class StreamingUDF extends EvalFunc<Object> {
             }
         }
     }
-    
+
     public class ProcessKiller implements Runnable {
         public void run() {
             process.destroy();
diff --git a/src/org/apache/pig/impl/util/UDFContext.java b/src/org/apache/pig/impl/util/UDFContext.java
index 0c639e9ef..22ccdab02 100644
--- a/src/org/apache/pig/impl/util/UDFContext.java
+++ b/src/org/apache/pig/impl/util/UDFContext.java
@@ -24,6 +24,7 @@ import java.util.HashMap;
 import java.util.Properties;
 
 import org.apache.hadoop.conf.Configuration;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 
 public class UDFContext {
 
@@ -226,8 +227,8 @@ public class UDFContext {
         // mapred.task.id is for MR1
         // mapreduce.job.application.attempt.id is for MR2
         return (this.jconf == null
-                || (jconf.get("mapred.task.id") == null &&
-                    jconf.get("mapreduce.job.application.attempt.id") == null));
+                || (jconf.get(MRConfiguration.TASK_ID) == null &&
+                    jconf.get(MRConfiguration.JOB_APPLICATION_ATTEMPT_ID) == null));
     }
 
     /**
diff --git a/src/org/apache/pig/impl/util/Utils.java b/src/org/apache/pig/impl/util/Utils.java
index 03ff1535a..c95f9caee 100644
--- a/src/org/apache/pig/impl/util/Utils.java
+++ b/src/org/apache/pig/impl/util/Utils.java
@@ -61,6 +61,7 @@ import org.apache.pig.ResourceSchema;
 import org.apache.pig.ResourceSchema.ResourceFieldSchema;
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.data.DataType;
 import org.apache.pig.data.Tuple;
 import org.apache.pig.impl.PigContext;
@@ -390,11 +391,12 @@ public class Utils {
     public static void setMapredCompressionCodecProps(Configuration conf) {
         String codec = conf.get(
                 PigConfiguration.PIG_TEMP_FILE_COMPRESSION_CODEC, "");
-        if ("".equals(codec) && conf.get("mapred.output.compression.codec") != null) {
-            conf.setBoolean("mapred.output.compress", true);
-        } else if(TEMPFILE_STORAGE.SEQFILE.ensureCodecSupported(codec)) {
-            conf.setBoolean("mapred.output.compress", true);
-            conf.set("mapred.output.compression.codec", TEMPFILE_CODEC.valueOf(codec.toUpperCase()).getHadoopCodecClassName());
+        if ("".equals(codec) && conf.get(MRConfiguration.OUTPUT_COMPRESSION_CODEC) != null) {
+            conf.setBoolean(MRConfiguration.OUTPUT_COMPRESS, true);
+        } else if (TEMPFILE_STORAGE.SEQFILE.ensureCodecSupported(codec)) {
+            conf.setBoolean(MRConfiguration.OUTPUT_COMPRESS, true);
+            conf.set(MRConfiguration.OUTPUT_COMPRESSION_CODEC,
+                    TEMPFILE_CODEC.valueOf(codec.toUpperCase()).getHadoopCodecClassName());
         }
         // no codec specified
     }
@@ -412,17 +414,19 @@ public class Utils {
             break;
         case SEQFILE:
             conf.set(PigConfiguration.PIG_TEMP_FILE_COMPRESSION_STORAGE, "seqfile");
-            if("".equals(codec)) {
+            if ("".equals(codec)) {
                 // codec is not specified, ensure  is set
-                log.warn("Temporary file compression codec is not specified. Using mapred.output.compression.codec property.");
-                if(conf.get("mapred.output.compression.codec") == null) {
-                    throw new IOException("mapred.output.compression.codec is not set");
+                log.warn("Temporary file compression codec is not specified. Using " +
+                         MRConfiguration.OUTPUT_COMPRESSION_CODEC + " property.");
+                if(conf.get(MRConfiguration.OUTPUT_COMPRESSION_CODEC) == null) {
+                    throw new IOException(MRConfiguration.OUTPUT_COMPRESSION_CODEC + " is not set");
                 }
             } else if(storage.ensureCodecSupported(codec)) {
                 // do nothing
             } else {
                 throw new IOException("Invalid temporary file compression codec [" + codec + "]. " +
-                        "Expected compression codecs for " + storage.getStorageClass().getName() + " are " + storage.supportedCodecsToString() + ".");
+                        "Expected compression codecs for " + storage.getStorageClass().getName() +
+                        " are " + storage.supportedCodecsToString() + ".");
             }
             break;
         case TFILE:
@@ -430,7 +434,8 @@ public class Utils {
                 conf.set(PigConfiguration.PIG_TEMP_FILE_COMPRESSION_CODEC, codec.toLowerCase());
             } else {
                 throw new IOException("Invalid temporary file compression codec [" + codec + "]. " +
-                        "Expected compression codecs for " + storage.getStorageClass().getName() + " are " + storage.supportedCodecsToString() + ".");
+                        "Expected compression codecs for " + storage.getStorageClass().getName() +
+                        " are " + storage.supportedCodecsToString() + ".");
             }
             break;
         }
diff --git a/src/org/apache/pig/parser/QueryParserUtils.java b/src/org/apache/pig/parser/QueryParserUtils.java
index 68fce96fd..908a7b1f4 100644
--- a/src/org/apache/pig/parser/QueryParserUtils.java
+++ b/src/org/apache/pig/parser/QueryParserUtils.java
@@ -40,6 +40,7 @@ import org.apache.pig.backend.datastorage.ContainerDescriptor;
 import org.apache.pig.backend.datastorage.DataStorage;
 import org.apache.pig.backend.datastorage.ElementDescriptor;
 import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.backend.hadoop.executionengine.shims.HadoopShims;
 import org.apache.pig.builtin.PigStorage;
 import org.apache.pig.impl.PigContext;
@@ -103,7 +104,7 @@ public class QueryParserUtils {
         ConfigurationUtil.mergeConf(conf, ConfigurationUtil.toConfiguration(pigContext.getProperties()));
         Set<String> remoteHosts = getRemoteHosts(absolutePath, defaultFSURI, conf);
 
-        String hdfsServersString = (String)pigContext.getProperties().get("mapreduce.job.hdfs-servers");
+        String hdfsServersString = (String)pigContext.getProperties().get(MRConfiguration.JOB_HDFS_SERVERS);
         if (hdfsServersString == null) hdfsServersString = "";
         String hdfsServers[] = hdfsServersString.split(",");
 
@@ -123,7 +124,7 @@ public class QueryParserUtils {
         }
 
         if (!hdfsServersString.isEmpty()) {
-            pigContext.getProperties().setProperty("mapreduce.job.hdfs-servers", hdfsServersString);
+            pigContext.getProperties().setProperty(MRConfiguration.JOB_HDFS_SERVERS, hdfsServersString);
         }
     }
 
diff --git a/src/org/apache/pig/scripting/ScriptingOutputCapturer.java b/src/org/apache/pig/scripting/ScriptingOutputCapturer.java
index 9fe73751b..774a34c72 100644
--- a/src/org/apache/pig/scripting/ScriptingOutputCapturer.java
+++ b/src/org/apache/pig/scripting/ScriptingOutputCapturer.java
@@ -31,6 +31,7 @@ import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.pig.ExecType;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.impl.util.UDFContext;
 
 import com.google.common.base.Charsets;
@@ -63,8 +64,8 @@ public class ScriptingOutputCapturer {
     public String getStandardOutputRootWriteLocation() throws IOException {
         Configuration conf = UDFContext.getUDFContext().getJobConf();
         
-        String jobId = conf.get("mapred.job.id");
-        String taskId = conf.get("mapred.task.id");
+        String jobId = conf.get(MRConfiguration.JOB_ID);
+        String taskId = conf.get(MRConfiguration.TASK_ID);
         String hadoopLogDir = System.getProperty("hadoop.log.dir");
         if (hadoopLogDir == null) {
             hadoopLogDir = conf.get("hadoop.log.dir");
diff --git a/src/org/apache/pig/tools/pigstats/mapreduce/MRJobStats.java b/src/org/apache/pig/tools/pigstats/mapreduce/MRJobStats.java
index 7b5835832..2d0f81d32 100644
--- a/src/org/apache/pig/tools/pigstats/mapreduce/MRJobStats.java
+++ b/src/org/apache/pig/tools/pigstats/mapreduce/MRJobStats.java
@@ -39,6 +39,7 @@ import org.apache.hadoop.mapred.jobcontrol.Job;
 import org.apache.hadoop.mapreduce.TaskType;
 import org.apache.pig.PigCounters;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOper;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
 import org.apache.pig.backend.hadoop.executionengine.shims.HadoopShims;
@@ -391,7 +392,7 @@ public final class MRJobStats extends JobStats {
             TaskStat st = getTaskStat(maps);
             setMapStat(st.size, st.max, st.min, st.avg, st.median);
         } else {
-            int m = conf.getInt("mapred.map.tasks", 1);
+            int m = conf.getInt(MRConfiguration.MAP_TASKS, 1);
             if (m > 0) {
                 setMapStat(m, -1, -1, -1, -1);
             }
@@ -401,7 +402,7 @@ public final class MRJobStats extends JobStats {
             TaskStat st = getTaskStat(reduces);
             setReduceStat(st.size, st.max, st.min, st.avg, st.median);
         } else {
-            int m = conf.getInt("mapred.reduce.tasks", 1);
+            int m = conf.getInt(MRConfiguration.REDUCE_TASKS, 1);
             if (m > 0) {
                 setReduceStat(m, -1, -1, -1, -1);
             }
diff --git a/test/org/apache/pig/data/TestSchemaTuple.java b/test/org/apache/pig/data/TestSchemaTuple.java
index baf76e91c..62af7c9e0 100644
--- a/test/org/apache/pig/data/TestSchemaTuple.java
+++ b/test/org/apache/pig/data/TestSchemaTuple.java
@@ -52,6 +52,7 @@ import org.apache.pig.PigConfiguration;
 import org.apache.pig.PigServer;
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.backend.hadoop.executionengine.shims.HadoopShims;
 import org.apache.pig.builtin.mock.Storage.Data;
 import org.apache.pig.data.SchemaTupleClassGenerator.GenContext;
@@ -536,7 +537,7 @@ public class TestSchemaTuple {
         conf.set("fs.default.name", "file:///");
 
         TaskAttemptID taskId = HadoopShims.createTaskAttemptID("jt", 1, true, 1, 1);
-        conf.set("mapred.task.id", taskId.toString());
+        conf.set(MRConfiguration.TASK_ID, taskId.toString());
 
         InputSplit is = new FileSplit(new Path(temp.getAbsolutePath()), 0, temp.length(), null);
 
diff --git a/test/org/apache/pig/parser/TestQueryParserUtils.java b/test/org/apache/pig/parser/TestQueryParserUtils.java
index 4bd74f02e..37827460d 100644
--- a/test/org/apache/pig/parser/TestQueryParserUtils.java
+++ b/test/org/apache/pig/parser/TestQueryParserUtils.java
@@ -22,6 +22,7 @@ import static org.junit.Assert.assertEquals;
 import java.util.Properties;
 
 import org.apache.pig.ExecType;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.impl.PigContext;
 import org.apache.pig.test.Util;
 import org.junit.Test;
@@ -36,73 +37,73 @@ public class TestQueryParserUtils {
 
         //No scheme/host
         QueryParserUtils.setHdfsServers("hdfs:///tmp", pc);
-        assertEquals(null, props.getProperty("mapreduce.job.hdfs-servers"));
+        assertEquals(null, props.getProperty(MRConfiguration.JOB_HDFS_SERVERS));
         QueryParserUtils.setHdfsServers("/tmp", pc);
-        assertEquals(null, props.getProperty("mapreduce.job.hdfs-servers"));
+        assertEquals(null, props.getProperty(MRConfiguration.JOB_HDFS_SERVERS));
         QueryParserUtils.setHdfsServers("tmp", pc);
-        assertEquals(null, props.getProperty("mapreduce.job.hdfs-servers"));
+        assertEquals(null, props.getProperty(MRConfiguration.JOB_HDFS_SERVERS));
 
         // Same as default host and scheme
         QueryParserUtils.setHdfsServers("hdfs://nn1/tmp", pc);
-        assertEquals(null, props.getProperty("mapreduce.job.hdfs-servers"));
+        assertEquals(null, props.getProperty(MRConfiguration.JOB_HDFS_SERVERS));
         QueryParserUtils.setHdfsServers("hdfs://nn1:8020/tmp", pc);
-        assertEquals(null, props.getProperty("mapreduce.job.hdfs-servers"));
+        assertEquals(null, props.getProperty(MRConfiguration.JOB_HDFS_SERVERS));
 
         // Same host different scheme
         QueryParserUtils.setHdfsServers("hftp://nn1/tmp", pc);
-        assertEquals("hftp://nn1", props.getProperty("mapreduce.job.hdfs-servers"));
+        assertEquals("hftp://nn1", props.getProperty(MRConfiguration.JOB_HDFS_SERVERS));
         QueryParserUtils.setHdfsServers("hftp://nn1:50070/tmp", pc);
-        assertEquals("hftp://nn1,hftp://nn1:50070", props.getProperty("mapreduce.job.hdfs-servers"));
+        assertEquals("hftp://nn1,hftp://nn1:50070", props.getProperty(MRConfiguration.JOB_HDFS_SERVERS));
         // There should be no duplicates
         QueryParserUtils.setHdfsServers("hftp://nn1:50070/tmp", pc);
-        assertEquals("hftp://nn1,hftp://nn1:50070", props.getProperty("mapreduce.job.hdfs-servers"));
+        assertEquals("hftp://nn1,hftp://nn1:50070", props.getProperty(MRConfiguration.JOB_HDFS_SERVERS));
 
         // har
-        props.remove("mapreduce.job.hdfs-servers");
+        props.remove(MRConfiguration.JOB_HDFS_SERVERS);
         QueryParserUtils.setHdfsServers("har:///tmp", pc);
-        assertEquals(null, props.getProperty("mapreduce.job.hdfs-servers"));
+        assertEquals(null, props.getProperty(MRConfiguration.JOB_HDFS_SERVERS));
         QueryParserUtils.setHdfsServers("har://hdfs-nn1:8020/tmp", pc);
-        assertEquals(null, props.getProperty("mapreduce.job.hdfs-servers"));
+        assertEquals(null, props.getProperty(MRConfiguration.JOB_HDFS_SERVERS));
         QueryParserUtils.setHdfsServers("har://hdfs-nn1/tmp", pc);
-        assertEquals("hdfs://nn1", props.getProperty("mapreduce.job.hdfs-servers"));
+        assertEquals("hdfs://nn1", props.getProperty(MRConfiguration.JOB_HDFS_SERVERS));
 
         // Non existing filesystem scheme
-        props.remove("mapreduce.job.hdfs-servers");
+        props.remove(MRConfiguration.JOB_HDFS_SERVERS);
         QueryParserUtils.setHdfsServers("hello://nn1/tmp", pc);
-        assertEquals(null, props.getProperty("mapreduce.job.hdfs-servers"));
+        assertEquals(null, props.getProperty(MRConfiguration.JOB_HDFS_SERVERS));
 
         if(Util.isHadoop23() || Util.isHadoop2_0()) {
             // webhdfs
-            props.remove("mapreduce.job.hdfs-servers");
+            props.remove(MRConfiguration.JOB_HDFS_SERVERS);
             QueryParserUtils.setHdfsServers("webhdfs://nn1/tmp", pc);
-            assertEquals("webhdfs://nn1", props.getProperty("mapreduce.job.hdfs-servers"));
+            assertEquals("webhdfs://nn1", props.getProperty(MRConfiguration.JOB_HDFS_SERVERS));
             QueryParserUtils.setHdfsServers("webhdfs://nn1:50070/tmp", pc);
-            assertEquals("webhdfs://nn1,webhdfs://nn1:50070", props.getProperty("mapreduce.job.hdfs-servers"));
+            assertEquals("webhdfs://nn1,webhdfs://nn1:50070", props.getProperty(MRConfiguration.JOB_HDFS_SERVERS));
 
             // har with webhfs
             QueryParserUtils.setHdfsServers("har://webhdfs-nn1:50070/tmp", pc);
-            assertEquals("webhdfs://nn1,webhdfs://nn1:50070", props.getProperty("mapreduce.job.hdfs-servers"));
+            assertEquals("webhdfs://nn1,webhdfs://nn1:50070", props.getProperty(MRConfiguration.JOB_HDFS_SERVERS));
             QueryParserUtils.setHdfsServers("har://webhdfs-nn2:50070/tmp", pc);
-            assertEquals("webhdfs://nn1,webhdfs://nn1:50070,webhdfs://nn2:50070", props.getProperty("mapreduce.job.hdfs-servers"));
-            props.remove("mapreduce.job.hdfs-servers");
+            assertEquals("webhdfs://nn1,webhdfs://nn1:50070,webhdfs://nn2:50070", props.getProperty(MRConfiguration.JOB_HDFS_SERVERS));
+            props.remove(MRConfiguration.JOB_HDFS_SERVERS);
             QueryParserUtils.setHdfsServers("har://webhdfs-nn1/tmp", pc);
-            assertEquals("webhdfs://nn1", props.getProperty("mapreduce.job.hdfs-servers"));
+            assertEquals("webhdfs://nn1", props.getProperty(MRConfiguration.JOB_HDFS_SERVERS));
 
             //viewfs
-            props.remove("mapreduce.job.hdfs-servers");
+            props.remove(MRConfiguration.JOB_HDFS_SERVERS);
             QueryParserUtils.setHdfsServers("viewfs:/tmp", pc);
-            assertEquals("viewfs://", props.getProperty("mapreduce.job.hdfs-servers"));
+            assertEquals("viewfs://", props.getProperty(MRConfiguration.JOB_HDFS_SERVERS));
             QueryParserUtils.setHdfsServers("viewfs:///tmp", pc);
-            assertEquals("viewfs://", props.getProperty("mapreduce.job.hdfs-servers"));
+            assertEquals("viewfs://", props.getProperty(MRConfiguration.JOB_HDFS_SERVERS));
             QueryParserUtils.setHdfsServers("viewfs://cluster1/tmp", pc);
-            assertEquals("viewfs://,viewfs://cluster1", props.getProperty("mapreduce.job.hdfs-servers"));
+            assertEquals("viewfs://,viewfs://cluster1", props.getProperty(MRConfiguration.JOB_HDFS_SERVERS));
 
             //har with viewfs
-            props.remove("mapreduce.job.hdfs-servers");
+            props.remove(MRConfiguration.JOB_HDFS_SERVERS);
             QueryParserUtils.setHdfsServers("har://viewfs/tmp", pc);
-            assertEquals("viewfs://", props.getProperty("mapreduce.job.hdfs-servers"));
+            assertEquals("viewfs://", props.getProperty(MRConfiguration.JOB_HDFS_SERVERS));
             QueryParserUtils.setHdfsServers("har://viewfs-cluster1/tmp", pc);
-            assertEquals("viewfs://,viewfs://cluster1", props.getProperty("mapreduce.job.hdfs-servers"));
+            assertEquals("viewfs://,viewfs://cluster1", props.getProperty(MRConfiguration.JOB_HDFS_SERVERS));
 
 
         }
diff --git a/test/org/apache/pig/test/TestAccumulator.java b/test/org/apache/pig/test/TestAccumulator.java
index 198ec15a4..373a25e4c 100644
--- a/test/org/apache/pig/test/TestAccumulator.java
+++ b/test/org/apache/pig/test/TestAccumulator.java
@@ -35,6 +35,7 @@ import java.util.Properties;
 
 import org.apache.pig.PigConfiguration;
 import org.apache.pig.PigServer;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.data.Tuple;
 import org.apache.pig.parser.ParserException;
 import org.junit.After;
@@ -61,8 +62,8 @@ public class TestAccumulator {
         properties.setProperty("pig.accumulative.batchsize", "2");
         properties.setProperty("pig.exec.nocombiner", "true");
         // Reducing the number of retry attempts to speed up test completion
-        properties.setProperty("mapred.map.max.attempts","1");
-        properties.setProperty("mapred.reduce.max.attempts","1");
+        properties.setProperty(MRConfiguration.MAP_MAX_ATTEMPTS,"1");
+        properties.setProperty(MRConfiguration.REDUCE_MAX_ATTEMPTS,"1");
         createFiles();
     }
 
diff --git a/test/org/apache/pig/test/TestBZip.java b/test/org/apache/pig/test/TestBZip.java
index 82eac24da..615396d50 100644
--- a/test/org/apache/pig/test/TestBZip.java
+++ b/test/org/apache/pig/test/TestBZip.java
@@ -41,6 +41,7 @@ import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.mapreduce.InputSplit;
 import org.apache.pig.PigServer;
 import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.data.DataType;
 import org.apache.pig.data.Tuple;
 import org.apache.pig.impl.PigContext;
@@ -449,7 +450,7 @@ public class TestBZip {
         for (Entry<Object, Object> entry : properties.entrySet()) {
             props.put(entry.getKey(), entry.getValue());
         }
-        props.setProperty("mapred.max.split.size", Integer.toString(splitSize));
+        props.setProperty(MRConfiguration.MAX_SPLIT_SIZE, Integer.toString(splitSize));
         PigServer pig = new PigServer(cluster.getExecType(), props);
         FileSystem fs = FileSystem.get(ConfigurationUtil.toConfiguration(props));
         fs.delete(new Path(outputFile), true);
diff --git a/test/org/apache/pig/test/TestCombiner.java b/test/org/apache/pig/test/TestCombiner.java
index 36ed4842a..df44293fa 100644
--- a/test/org/apache/pig/test/TestCombiner.java
+++ b/test/org/apache/pig/test/TestCombiner.java
@@ -32,6 +32,7 @@ import java.util.Properties;
 
 import org.apache.pig.EvalFunc;
 import org.apache.pig.PigServer;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.builtin.PigStorage;
 import org.apache.pig.data.DataBag;
 import org.apache.pig.data.DefaultDataBag;
@@ -173,7 +174,7 @@ public class TestCombiner {
         properties.setProperty("io.sort.mb", "1");
 
         PigServer pigServer = new PigServer(cluster.getExecType(), properties);
-        pigServer.getPigContext().getProperties().setProperty("mapred.child.java.opts", "-Xmx1024m");
+        pigServer.getPigContext().getProperties().setProperty(MRConfiguration.CHILD_JAVA_OPTS, "-Xmx1024m");
         pigServer.registerQuery("a = load 'MultiCombinerUseInput.txt' as (x:int);");
         pigServer.registerQuery("b = group a all;");
         pigServer.registerQuery("c = foreach b generate COUNT(a), SUM(a.$0), " +
diff --git a/test/org/apache/pig/test/TestGFCross.java b/test/org/apache/pig/test/TestGFCross.java
index 76448cd6d..b148a4809 100644
--- a/test/org/apache/pig/test/TestGFCross.java
+++ b/test/org/apache/pig/test/TestGFCross.java
@@ -18,19 +18,15 @@
 package org.apache.pig.test;
 
 import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertTrue;
 
 import org.apache.hadoop.conf.Configuration;
 
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.data.DataBag;
 import org.apache.pig.data.Tuple;
 import org.apache.pig.data.TupleFactory;
-import org.apache.pig.impl.PigContext;
 import org.apache.pig.impl.builtin.GFCross;
 import org.apache.pig.impl.util.UDFContext;
-import org.junit.AfterClass;
-import org.junit.Assert;
-import org.junit.Before;
 import org.junit.Test;
 
 public class TestGFCross {
@@ -54,7 +50,7 @@ public class TestGFCross {
     @Test
     public void testSerial() throws Exception {
         Configuration cfg = new Configuration();
-        cfg.set("mapred.reduce.tasks", "1");
+        cfg.set(MRConfiguration.REDUCE_TASKS, "1");
         UDFContext.getUDFContext().addJobConf(cfg);
         Tuple t = TupleFactory.getInstance().newTuple(2);
 
@@ -70,7 +66,7 @@ public class TestGFCross {
     @Test
     public void testParallelSet() throws Exception {
         Configuration cfg = new Configuration();
-        cfg.set("mapred.reduce.tasks", "10");
+        cfg.set(MRConfiguration.REDUCE_TASKS, "10");
         UDFContext.getUDFContext().addJobConf(cfg);
         Tuple t = TupleFactory.getInstance().newTuple(2);
 
diff --git a/test/org/apache/pig/test/TestHBaseStorage.java b/test/org/apache/pig/test/TestHBaseStorage.java
index 2957f9c80..b89b6062d 100644
--- a/test/org/apache/pig/test/TestHBaseStorage.java
+++ b/test/org/apache/pig/test/TestHBaseStorage.java
@@ -39,7 +39,7 @@ import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.pig.ExecType;
 import org.apache.pig.PigServer;
 import org.apache.pig.backend.executionengine.ExecException;
-import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.backend.hadoop.hbase.HBaseStorage;
 import org.apache.pig.data.DataByteArray;
 import org.apache.pig.data.DataType;
@@ -864,7 +864,7 @@ public class TestHBaseStorage {
         prepareTable(TESTTABLE_2, false, DataFormat.HBaseBinary);
 
         pig.getPigContext().getProperties()
-                .setProperty(MapReduceLauncher.SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, "true");
+                .setProperty(MRConfiguration.FILEOUTPUTCOMMITTER_MARKSUCCESSFULJOBS, "true");
 
         scanTable1(pig, DataFormat.HBaseBinary);
         pig.store("a", "hbase://" +  TESTTABLE_2,
@@ -891,7 +891,7 @@ public class TestHBaseStorage {
         Assert.assertEquals(100, i);
 
         pig.getPigContext().getProperties()
-                .setProperty(MapReduceLauncher.SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, "false");
+                .setProperty(MRConfiguration.FILEOUTPUTCOMMITTER_MARKSUCCESSFULJOBS, "false");
     }
 
     /**
diff --git a/test/org/apache/pig/test/TestJobControlCompiler.java b/test/org/apache/pig/test/TestJobControlCompiler.java
index 7a53e4863..0f54e4245 100644
--- a/test/org/apache/pig/test/TestJobControlCompiler.java
+++ b/test/org/apache/pig/test/TestJobControlCompiler.java
@@ -56,6 +56,7 @@ import org.apache.pig.FuncSpec;
 import org.apache.pig.LoadFunc;
 import org.apache.pig.PigServer;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOper;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.MROperPlan;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad;
diff --git a/test/org/apache/pig/test/TestMRExecutionEngine.java b/test/org/apache/pig/test/TestMRExecutionEngine.java
index 2f5ccdcb5..742ed64e2 100644
--- a/test/org/apache/pig/test/TestMRExecutionEngine.java
+++ b/test/org/apache/pig/test/TestMRExecutionEngine.java
@@ -17,12 +17,13 @@
  */
 package org.apache.pig.test;
 
-import junit.framework.Assert;
+import static org.junit.Assert.assertEquals;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.pig.ExecType;
 import org.apache.pig.backend.executionengine.ExecException;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRExecutionEngine;
 import org.apache.pig.impl.PigContext;
 import org.junit.Test;
@@ -46,12 +47,12 @@ public class TestMRExecutionEngine {
         // This property allows Pig to depend on user Configuration 
         // and not the classpath
         conf.set("pig.use.overriden.hadoop.configs", "true");
-        conf.set("mapred.job.tracker", "host:12345");
+        conf.set(MRConfiguration.JOB_TRACKER, "host:12345");
         conf.set("apache", "pig");
         PigContext pigContext = new PigContext(ExecType.MAPREDUCE, conf);
         pigContext.connect();
         JobConf jc = ((MRExecutionEngine)pigContext.getExecutionEngine()).getJobConf();
-        Assert.assertEquals(jc.get("mapred.job.tracker"), "host:12345");
-        Assert.assertEquals(jc.get("apache"), "pig");
+        assertEquals(jc.get(MRConfiguration.JOB_TRACKER), "host:12345");
+        assertEquals(jc.get("apache"), "pig");
     }
 }
diff --git a/test/org/apache/pig/test/TestMapReduce.java b/test/org/apache/pig/test/TestMapReduce.java
index 0b0e8335d..7d699ac7a 100644
--- a/test/org/apache/pig/test/TestMapReduce.java
+++ b/test/org/apache/pig/test/TestMapReduce.java
@@ -39,6 +39,7 @@ import org.apache.pig.FuncSpec;
 import org.apache.pig.PigServer;
 import org.apache.pig.backend.datastorage.ElementDescriptor;
 import org.apache.pig.backend.executionengine.ExecException;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.builtin.COUNT;
 import org.apache.pig.builtin.PigStorage;
 import org.apache.pig.data.BagFactory;
@@ -94,12 +95,12 @@ public class TestMapReduce {
         PrintStream ps = new PrintStream(new FileOutputStream(tmpFile));
         long nonNullCnt = 0;
         for(int i = 0; i < LOOP_COUNT; i++) {
-	    if ( i % 10 == 0 ){
+        if ( i % 10 == 0 ){
                ps.println("");
-	    } else {
+        } else {
                ps.println(i);
                nonNullCnt ++;
-	    }
+        }
         }
         ps.close();
 
@@ -122,7 +123,7 @@ public class TestMapReduce {
             for (Entry<Object, Object> entry : cluster.getProperties().entrySet()) {
                 props.put(entry.getKey(), entry.getValue());
             }
-            props.setProperty("mapred.max.split.size", Integer.toString(offsets[i]));
+            props.setProperty(MRConfiguration.MAX_SPLIT_SIZE, Integer.toString(offsets[i]));
             PigContext pigContext = new PigContext(cluster.getExecType(), props);
             PigServer pig = new PigServer(pigContext);
             pig.registerQuery("a = load '"
@@ -293,7 +294,7 @@ public class TestMapReduce {
 
         File tmpFile=TestHelper.createTempFile(data) ;
 
-	//Load, Execute and Store query
+    //Load, Execute and Store query
         String query = "foreach (load '"
                 + Util.generateURI(tmpFile.toString(), pig.getPigContext())
                 + "') generate $0,$1;";
@@ -309,7 +310,7 @@ public class TestMapReduce {
         BufferedReader br = new BufferedReader(new InputStreamReader(is));
         String line;
 
-	//verify query
+    //verify query
         int i= 0;
         while((line = br.readLine()) != null) {
 
@@ -369,11 +370,11 @@ public class TestMapReduce {
         File tmpFile = File.createTempFile("test", ".txt");
         PrintStream ps = new PrintStream(new FileOutputStream(tmpFile));
         for(int i = 0; i < 1; i++) {
-	    if ( i % 10 == 0 ){
+        if ( i % 10 == 0 ){
                ps.println("");
-	    } else {
+        } else {
                ps.println(i);
-	    }
+        }
         }
         ps.close();
 
@@ -508,14 +509,14 @@ public class TestMapReduce {
      * For generating a sample dataset as
      *
      * no nulls:
-     *   	$0 $1
+     *       $0 $1
      *           0  9
      *           1  1
      *           ....
      *           9  9
      *
      * has nulls:
-     *   	$0 $1
+     *       $0 $1
      *           0  9
      *           1  1
      *              2
@@ -529,48 +530,32 @@ public class TestMapReduce {
      *
      */
     private String[][] genDataSetFile1( int dataLength, boolean hasNulls ) throws IOException {
-
-
         String[][] data= new String[dataLength][];
-
         if ( hasNulls == true ) {
-
-        	for (int i = 0; i < dataLength; i++) {
-
-            	     data[i] = new String[2] ;
-                     if ( i == 2 ) {
-            		data[i][0] = "";
-            		data[i][1] = new Integer(i).toString();
-
-		     } else if ( i == 6 ) {
-
-            		data[i][0] = new Integer(i).toString();
-            		data[i][1] = "";
-
-		     } else if ( i == 8 ) {
-
-            		data[i][0] = "";
-            		data[i][1] = "";
-
-		     } else {
-            		data[i][0] = new Integer(i).toString();
-            		data[i][1] = new Integer(i).toString();
-           	     }
-	     }
-
-	} else {
-
-        	for (int i = 0; i < dataLength; i++) {
-            		data[i] = new String[2] ;
-            		data[i][0] = new Integer(i).toString();
-            		data[i][1] = new Integer(i).toString();
-        	}
-
-	}
-
-         return  data;
-
+            for (int i = 0; i < dataLength; i++) {
+                data[i] = new String[2] ;
+                if ( i == 2 ) {
+                    data[i][0] = "";
+                    data[i][1] = new Integer(i).toString();
+                } else if ( i == 6 ) {
+                    data[i][0] = new Integer(i).toString();
+                    data[i][1] = "";
+                } else if ( i == 8 ) {
+                    data[i][0] = "";
+                    data[i][1] = "";
+                } else {
+                    data[i][0] = new Integer(i).toString();
+                    data[i][1] = new Integer(i).toString();
+                }
+            }
+        } else {
+            for (int i = 0; i < dataLength; i++) {
+                data[i] = new String[2] ;
+                data[i][0] = new Integer(i).toString();
+                data[i][1] = new Integer(i).toString();
+            }
+        }
+        return  data;
     }
 
-
 }
diff --git a/test/org/apache/pig/test/TestMergeJoin.java b/test/org/apache/pig/test/TestMergeJoin.java
index 37a69b6a2..ea16c23d1 100644
--- a/test/org/apache/pig/test/TestMergeJoin.java
+++ b/test/org/apache/pig/test/TestMergeJoin.java
@@ -32,6 +32,7 @@ import org.apache.pig.LoadFunc;
 import org.apache.pig.PigException;
 import org.apache.pig.PigServer;
 import org.apache.pig.backend.executionengine.ExecException;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigSplit;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.MROperPlan;
 import org.apache.pig.data.BagFactory;
@@ -57,8 +58,8 @@ public class TestMergeJoin {
     public TestMergeJoin() throws ExecException{
 
         Properties props = cluster.getProperties();
-        props.setProperty("mapred.map.max.attempts", "1");
-        props.setProperty("mapred.reduce.max.attempts", "1");
+        props.setProperty(MRConfiguration.MAP_MAX_ATTEMPTS, "1");
+        props.setProperty(MRConfiguration.REDUCE_MAX_ATTEMPTS, "1");
         pigServer = new PigServer(ExecType.MAPREDUCE, props);
     }
     /**
diff --git a/test/org/apache/pig/test/TestMergeJoinOuter.java b/test/org/apache/pig/test/TestMergeJoinOuter.java
index 942f615e1..0ab67fe81 100644
--- a/test/org/apache/pig/test/TestMergeJoinOuter.java
+++ b/test/org/apache/pig/test/TestMergeJoinOuter.java
@@ -33,6 +33,7 @@ import java.util.Properties;
 import org.apache.pig.ExecType;
 import org.apache.pig.PigServer;
 import org.apache.pig.backend.executionengine.ExecException;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOper;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.MROperPlan;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
@@ -64,8 +65,8 @@ public class TestMergeJoinOuter {
     public TestMergeJoinOuter() throws ExecException{
         
         Properties props = cluster.getProperties();
-        props.setProperty("mapred.map.max.attempts", "1");
-        props.setProperty("mapred.reduce.max.attempts", "1");
+        props.setProperty(MRConfiguration.MAP_MAX_ATTEMPTS, "1");
+        props.setProperty(MRConfiguration.REDUCE_MAX_ATTEMPTS, "1");
         pigServer = new PigServer(ExecType.MAPREDUCE, props);
     }
     
diff --git a/test/org/apache/pig/test/TestMultiQueryBasic.java b/test/org/apache/pig/test/TestMultiQueryBasic.java
index d5cf074a3..1a467a340 100644
--- a/test/org/apache/pig/test/TestMultiQueryBasic.java
+++ b/test/org/apache/pig/test/TestMultiQueryBasic.java
@@ -45,6 +45,7 @@ import org.apache.pig.PigServer;
 import org.apache.pig.ResourceSchema;
 import org.apache.pig.StoreFunc;
 import org.apache.pig.backend.executionengine.ExecJob;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.data.DataBag;
 import org.apache.pig.data.Tuple;
 import org.apache.pig.impl.PigContext;
@@ -656,7 +657,7 @@ public class TestMultiQueryBasic {
         public void setStoreLocation(String location, Job job)
                 throws IOException {
             Configuration conf = job.getConfiguration();
-            conf.set("mapred.output.dir", location);
+            conf.set(MRConfiguration.OUTPUT_DIR, location);
 
         }
 
@@ -679,7 +680,7 @@ public class TestMultiQueryBasic {
             Configuration conf = context.getConfiguration();
             FileSystem fs = FileSystem.get(conf);
             // create a file to test that this method got called
-            fs.create(new Path(conf.get("mapred.output.dir") + "_checkOutputSpec_test"));
+            fs.create(new Path(conf.get(MRConfiguration.OUTPUT_DIR) + "_checkOutputSpec_test"));
         }
 
         @Override
diff --git a/test/org/apache/pig/test/TestMultiQueryLocal.java b/test/org/apache/pig/test/TestMultiQueryLocal.java
index 20fd979d7..ae0b59f57 100644
--- a/test/org/apache/pig/test/TestMultiQueryLocal.java
+++ b/test/org/apache/pig/test/TestMultiQueryLocal.java
@@ -37,6 +37,7 @@ import org.apache.pig.ExecType;
 import org.apache.pig.PigConfiguration;
 import org.apache.pig.PigException;
 import org.apache.pig.PigServer;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRExecutionEngine;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigTextOutputFormat;
@@ -392,7 +393,7 @@ public class TestMultiQueryLocal {
         @Override
         public synchronized OutputCommitter getOutputCommitter(TaskAttemptContext context)
                 throws IOException {
-            context.getConfiguration().set(PigStorageWithConfig.key, "mapred.work.output.dir");
+            context.getConfiguration().set(PigStorageWithConfig.key, MRConfiguration.WORK_OUPUT_DIR);
             return super.getOutputCommitter(context);
         }
     }
diff --git a/test/org/apache/pig/test/TestParser.java b/test/org/apache/pig/test/TestParser.java
index 679b6acda..068f73d3b 100644
--- a/test/org/apache/pig/test/TestParser.java
+++ b/test/org/apache/pig/test/TestParser.java
@@ -34,6 +34,7 @@ import org.apache.pig.ExecType;
 import org.apache.pig.PigServer;
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.builtin.mock.Storage;
 import org.apache.pig.builtin.mock.Storage.Data;
 import org.apache.pig.data.Tuple;
@@ -94,36 +95,36 @@ public class TestParser {
 
             pigServer.registerQuery("a = load '/user/pig/1.txt' using mock.Storage;");
             conf = ConfigurationUtil.toConfiguration(pigProperties);
-            assertTrue(conf.get("mapreduce.job.hdfs-servers") == null ||
-                    conf.get("mapreduce.job.hdfs-servers").equals(pigProperties.get("fs.default.name"))||
-                    conf.get("mapreduce.job.hdfs-servers").equals(pigProperties.get("fs.defaultFS")));
+            assertTrue(conf.get(MRConfiguration.JOB_HDFS_SERVERS) == null ||
+                    conf.get(MRConfiguration.JOB_HDFS_SERVERS).equals(pigProperties.get("fs.default.name"))||
+                    conf.get(MRConfiguration.JOB_HDFS_SERVERS).equals(pigProperties.get("fs.defaultFS")));
 
             pigServer.registerQuery("a = load 'hdfs://a.com/user/pig/1.txt' using mock.Storage;");
             conf = ConfigurationUtil.toConfiguration(pigProperties);
-            assertTrue(pigProperties.getProperty("mapreduce.job.hdfs-servers") == null ||
-                    conf.get("mapreduce.job.hdfs-servers").equals(pigProperties.get("fs.default.name"))||
-                    conf.get("mapreduce.job.hdfs-servers").equals(pigProperties.get("fs.defaultFS")));
+            assertTrue(pigProperties.getProperty(MRConfiguration.JOB_HDFS_SERVERS) == null ||
+                    conf.get(MRConfiguration.JOB_HDFS_SERVERS).equals(pigProperties.get("fs.default.name"))||
+                    conf.get(MRConfiguration.JOB_HDFS_SERVERS).equals(pigProperties.get("fs.defaultFS")));
 
             pigServer.registerQuery("a = load 'har:///1.txt' using mock.Storage;");
             conf = ConfigurationUtil.toConfiguration(pigProperties);
-            assertTrue(pigProperties.getProperty("mapreduce.job.hdfs-servers") == null ||
-                    conf.get("mapreduce.job.hdfs-servers").equals(pigProperties.get("fs.default.name"))||
-                    conf.get("mapreduce.job.hdfs-servers").equals(pigProperties.get("fs.defaultFS")));
+            assertTrue(pigProperties.getProperty(MRConfiguration.JOB_HDFS_SERVERS) == null ||
+                    conf.get(MRConfiguration.JOB_HDFS_SERVERS).equals(pigProperties.get("fs.default.name"))||
+                    conf.get(MRConfiguration.JOB_HDFS_SERVERS).equals(pigProperties.get("fs.defaultFS")));
 
             pigServer.registerQuery("a = load 'hdfs://b.com/user/pig/1.txt' using mock.Storage;");
             conf = ConfigurationUtil.toConfiguration(pigProperties);
-            assertTrue(conf.get("mapreduce.job.hdfs-servers") != null &&
-                    conf.get("mapreduce.job.hdfs-servers").contains("hdfs://b.com"));
+            assertTrue(conf.get(MRConfiguration.JOB_HDFS_SERVERS) != null &&
+                    conf.get(MRConfiguration.JOB_HDFS_SERVERS).contains("hdfs://b.com"));
 
             pigServer.registerQuery("a = load 'har://hdfs-c.com/user/pig/1.txt' using mock.Storage;");
             conf = ConfigurationUtil.toConfiguration(pigProperties);
-            assertTrue(conf.get("mapreduce.job.hdfs-servers") != null &&
-                    conf.get("mapreduce.job.hdfs-servers").contains("hdfs://c.com"));
+            assertTrue(conf.get(MRConfiguration.JOB_HDFS_SERVERS) != null &&
+                    conf.get(MRConfiguration.JOB_HDFS_SERVERS).contains("hdfs://c.com"));
 
             pigServer.registerQuery("a = load 'hdfs://d.com:8020/user/pig/1.txt' using mock.Storage;");
             conf = ConfigurationUtil.toConfiguration(pigProperties);
-            assertTrue(conf.get("mapreduce.job.hdfs-servers") != null &&
-                    conf.get("mapreduce.job.hdfs-servers").contains("hdfs://d.com:8020"));
+            assertTrue(conf.get(MRConfiguration.JOB_HDFS_SERVERS) != null &&
+                    conf.get(MRConfiguration.JOB_HDFS_SERVERS).contains("hdfs://d.com:8020"));
         }
     }
 
@@ -143,32 +144,32 @@ public class TestParser {
         pigServer.registerQuery("store a into '/user/pig/1.txt';");
 
         System.out.println("hdfs-servers: "
-                + pigProperties.getProperty("mapreduce.job.hdfs-servers"));
+                + pigProperties.getProperty(MRConfiguration.JOB_HDFS_SERVERS));
         conf = ConfigurationUtil.toConfiguration(pigProperties);
-        assertTrue(conf.get("mapreduce.job.hdfs-servers") == null ||
-                conf.get("mapreduce.job.hdfs-servers").equals(pigProperties.get("fs.default.name"))||
-                conf.get("mapreduce.job.hdfs-servers").equals(pigProperties.get("fs.defaultFS")));
+        assertTrue(conf.get(MRConfiguration.JOB_HDFS_SERVERS) == null ||
+                conf.get(MRConfiguration.JOB_HDFS_SERVERS).equals(pigProperties.get("fs.default.name"))||
+                conf.get(MRConfiguration.JOB_HDFS_SERVERS).equals(pigProperties.get("fs.defaultFS")));
 
         pigServer.registerQuery("store a into 'hdfs://b.com/user/pig/1.txt' using mock.Storage;");
         System.out.println("hdfs-servers: "
-                + pigProperties.getProperty("mapreduce.job.hdfs-servers"));
+                + pigProperties.getProperty(MRConfiguration.JOB_HDFS_SERVERS));
         conf = ConfigurationUtil.toConfiguration(pigProperties);
-        assertTrue(conf.get("mapreduce.job.hdfs-servers") != null &&
-                conf.get("mapreduce.job.hdfs-servers").contains("hdfs://b.com"));
+        assertTrue(conf.get(MRConfiguration.JOB_HDFS_SERVERS) != null &&
+                conf.get(MRConfiguration.JOB_HDFS_SERVERS).contains("hdfs://b.com"));
 
         pigServer.registerQuery("store a into 'har://hdfs-c.com:8020/user/pig/1.txt' using mock.Storage;");
         System.out.println("hdfs-servers: "
-                + pigProperties.getProperty("mapreduce.job.hdfs-servers"));
+                + pigProperties.getProperty(MRConfiguration.JOB_HDFS_SERVERS));
         conf = ConfigurationUtil.toConfiguration(pigProperties);
-        assertTrue(conf.get("mapreduce.job.hdfs-servers") != null &&
-                conf.get("mapreduce.job.hdfs-servers").contains("hdfs://c.com:8020"));
+        assertTrue(conf.get(MRConfiguration.JOB_HDFS_SERVERS) != null &&
+                conf.get(MRConfiguration.JOB_HDFS_SERVERS).contains("hdfs://c.com:8020"));
 
         pigServer.registerQuery("store a into 'hdfs://d.com:8020/user/pig/1.txt' using mock.Storage;");
         System.out.println("hdfs-servers: "
-                + pigProperties.getProperty("mapreduce.job.hdfs-servers"));
+                + pigProperties.getProperty(MRConfiguration.JOB_HDFS_SERVERS));
         conf = ConfigurationUtil.toConfiguration(pigProperties);
-        assertTrue(conf.get("mapreduce.job.hdfs-servers") != null &&
-                conf.get("mapreduce.job.hdfs-servers").contains("hdfs://d.com:8020"));
+        assertTrue(conf.get(MRConfiguration.JOB_HDFS_SERVERS) != null &&
+                conf.get(MRConfiguration.JOB_HDFS_SERVERS).contains("hdfs://d.com:8020"));
 
     }
 
diff --git a/test/org/apache/pig/test/TestPigContext.java b/test/org/apache/pig/test/TestPigContext.java
index fe28e1316..b5d7aad94 100644
--- a/test/org/apache/pig/test/TestPigContext.java
+++ b/test/org/apache/pig/test/TestPigContext.java
@@ -30,6 +30,7 @@ import java.util.Random;
 import org.apache.hadoop.mapred.FileAlreadyExistsException;
 import org.apache.pig.ExecType;
 import org.apache.pig.PigServer;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.data.Tuple;
 import org.apache.pig.impl.PigContext;
 import org.apache.pig.impl.io.FileLocalizer;
@@ -226,7 +227,7 @@ public class TestPigContext {
 
     private static Properties getProperties() {
         Properties props = new Properties();
-        props.put("mapred.job.tracker", JOB_TRACKER);
+        props.put(MRConfiguration.JOB_TRACKER, JOB_TRACKER);
         props.put("fs.default.name", FS_NAME);
         props.put("hadoop.tmp.dir", TMP_DIR_PROP);
         return props;
@@ -255,7 +256,7 @@ public class TestPigContext {
 
     private void check_asserts(PigServer pigServer) {
         assertEquals(JOB_TRACKER,
-                pigServer.getPigContext().getProperties().getProperty("mapred.job.tracker"));
+                pigServer.getPigContext().getProperties().getProperty(MRConfiguration.JOB_TRACKER));
         assertEquals(FS_NAME,
                 pigServer.getPigContext().getProperties().getProperty("fs.default.name"));
         assertEquals(TMP_DIR_PROP,
diff --git a/test/org/apache/pig/test/TestPigProgressReporting.java b/test/org/apache/pig/test/TestPigProgressReporting.java
index 4922e9904..78697ad60 100644
--- a/test/org/apache/pig/test/TestPigProgressReporting.java
+++ b/test/org/apache/pig/test/TestPigProgressReporting.java
@@ -26,6 +26,7 @@ import java.io.PrintStream;
 
 import org.apache.pig.ExecType;
 import org.apache.pig.PigServer;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.junit.AfterClass;
 import org.junit.Test;
 
@@ -37,11 +38,11 @@ public class TestPigProgressReporting {
     public void testProgressReportingWithStatusMessage() throws Exception {
 
         // Get the default time out value
-        String taskTimeout = cluster.getProperties().getProperty("mapred.task.timeout");
+        String taskTimeout = cluster.getProperties().getProperty(MRConfiguration.TASK_TIMEOUT);
 
         try{
             // Override the timeout as 10 secs
-            cluster.setProperty("mapred.task.timeout", "10000");
+            cluster.setProperty(MRConfiguration.TASK_TIMEOUT, "10000");
 
             Util.createInputFile(cluster, "a.txt", new String[] { "dummy"});
 
@@ -58,7 +59,7 @@ public class TestPigProgressReporting {
         }finally{
             // Set back the orginal value
             if (taskTimeout!=null)
-                cluster.setProperty("mapred.task.timeout", taskTimeout);
+                cluster.setProperty(MRConfiguration.TASK_TIMEOUT, taskTimeout);
         }
 
     }
diff --git a/test/org/apache/pig/test/TestPigRunner.java b/test/org/apache/pig/test/TestPigRunner.java
index 566ef4e32..a8757ccf7 100644
--- a/test/org/apache/pig/test/TestPigRunner.java
+++ b/test/org/apache/pig/test/TestPigRunner.java
@@ -40,6 +40,7 @@ import org.apache.pig.PigConfiguration;
 import org.apache.pig.PigRunner;
 import org.apache.pig.PigRunner.ReturnCode;
 import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.impl.PigContext;
 import org.apache.pig.impl.io.FileLocalizer;
 import org.apache.pig.impl.plan.OperatorPlan;
@@ -622,8 +623,10 @@ public class TestPigRunner {
         assertNotNull(ctx);
 
         assertTrue(ctx.extraJars.contains(ClassLoader.getSystemResource(jarName)));
-        assertTrue("default", ctx.getProperties().getProperty("mapred.job.queue.name")!=null && ctx.getProperties().getProperty("mapred.job.queue.name").equals("default")||
-                ctx.getProperties().getProperty("mapreduce.job.queuename")!=null && ctx.getProperties().getProperty("mapreduce.job.queuename").equals("default"));
+        assertTrue("default", ctx.getProperties().getProperty(MRConfiguration.JOB_QUEUE_NAME) != null
+                && ctx.getProperties().getProperty(MRConfiguration.JOB_QUEUE_NAME).equals("default")
+                || ctx.getProperties().getProperty("mapreduce.job.queuename") != null
+                && ctx.getProperties().getProperty("mapreduce.job.queuename").equals("default"));
 
     }
 
diff --git a/test/org/apache/pig/test/TestPigStorage.java b/test/org/apache/pig/test/TestPigStorage.java
index f839869c2..d35047c68 100644
--- a/test/org/apache/pig/test/TestPigStorage.java
+++ b/test/org/apache/pig/test/TestPigStorage.java
@@ -42,6 +42,7 @@ import org.apache.pig.PigServer;
 import org.apache.pig.ResourceSchema;
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject;
 import org.apache.pig.builtin.PigStorage;
 import org.apache.pig.data.DataType;
@@ -127,7 +128,7 @@ public class TestPigStorage  {
         for (Entry<Object, Object> entry : properties.entrySet()) {
             props.put(entry.getKey(), entry.getValue());
         }
-        props.setProperty("mapred.max.split.size", "20");
+        props.setProperty(MRConfiguration.MAX_SPLIT_SIZE, "20");
         Util.resetStateForExecModeSwitch();
         PigServer pigServer = new PigServer(cluster.getExecType(), props);
         String[] inputs = {
diff --git a/test/org/apache/pig/test/TestPoissonSampleLoader.java b/test/org/apache/pig/test/TestPoissonSampleLoader.java
index be325943b..880488316 100644
--- a/test/org/apache/pig/test/TestPoissonSampleLoader.java
+++ b/test/org/apache/pig/test/TestPoissonSampleLoader.java
@@ -27,6 +27,7 @@ import java.util.Iterator;
 import org.apache.pig.ExecType;
 import org.apache.pig.PigServer;
 import org.apache.pig.backend.executionengine.ExecException;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.data.Tuple;
 import org.junit.After;
 import org.junit.Before;
@@ -43,7 +44,7 @@ public class TestPoissonSampleLoader {
                 .setProperty("pig.skewedjoin.reduce.maxtuple", "5");
         pigServer.getPigContext().getProperties()
                 .setProperty("pig.skewedjoin.reduce.memusage", "0.0001");
-        pigServer.getPigContext().getProperties().setProperty("mapred.child.java.opts", "-Xmx512m");
+        pigServer.getPigContext().getProperties().setProperty(MRConfiguration.CHILD_JAVA_OPTS, "-Xmx512m");
 
         pigServer.getPigContext().getProperties().setProperty("pig.mapsplits.count", "5");
     }
diff --git a/test/org/apache/pig/test/TestStore.java b/test/org/apache/pig/test/TestStore.java
index 47a860676..c91190aaa 100644
--- a/test/org/apache/pig/test/TestStore.java
+++ b/test/org/apache/pig/test/TestStore.java
@@ -56,6 +56,7 @@ import org.apache.pig.StoreMetadata;
 import org.apache.pig.backend.datastorage.DataStorage;
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigTextOutputFormat;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject;
@@ -104,7 +105,7 @@ public class TestStore {
 
     private static final String FAIL_UDF_NAME
     = "org.apache.pig.test.TestStore\\$FailUDF";
-    private static final String MAP_MAX_ATTEMPTS = "mapred.map.max.attempts";
+    private static final String MAP_MAX_ATTEMPTS = MRConfiguration.MAP_MAX_ATTEMPTS;
     private static final String TESTDIR = "/tmp/" + TestStore.class.getSimpleName();
     private static ExecType[] modes = new ExecType[] { ExecType.LOCAL, cluster.getExecType() };
 
@@ -616,7 +617,7 @@ public class TestStore {
                             ps = new PigServer(ExecType.LOCAL, props);
                         }
                         ps.getPigContext().getProperties().setProperty(
-                                MapReduceLauncher.SUCCESSFUL_JOB_OUTPUT_DIR_MARKER,
+                                MRConfiguration.FILEOUTPUTCOMMITTER_MARKSUCCESSFULJOBS,
                                 Boolean.toString(isPropertySet));
                         Util.deleteFile(ps.getPigContext(), TESTDIR);
                         ps.setBatchOn();
@@ -684,7 +685,7 @@ public class TestStore {
                             ps = new PigServer(ExecType.LOCAL, props);
                         }
                         ps.getPigContext().getProperties().setProperty(
-                                MapReduceLauncher.SUCCESSFUL_JOB_OUTPUT_DIR_MARKER,
+                                MRConfiguration.FILEOUTPUTCOMMITTER_MARKSUCCESSFULJOBS,
                                 Boolean.toString(isPropertySet));
                         Util.deleteFile(ps.getPigContext(), TESTDIR);
                         ps.setBatchOn();
diff --git a/test/org/apache/pig/test/TestStoreInstances.java b/test/org/apache/pig/test/TestStoreInstances.java
index d689c4963..3716a5765 100644
--- a/test/org/apache/pig/test/TestStoreInstances.java
+++ b/test/org/apache/pig/test/TestStoreInstances.java
@@ -38,6 +38,7 @@ import org.apache.pig.PigServer;
 import org.apache.pig.StoreFunc;
 import org.apache.pig.backend.executionengine.ExecJob;
 import org.apache.pig.backend.executionengine.ExecJob.JOB_STATUS;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.data.Tuple;
 import org.apache.pig.impl.io.FileLocalizer;
 import org.apache.pig.parser.ParserException;
@@ -159,7 +160,7 @@ public class TestStoreInstances  {
         public void setStoreLocation(String location, Job job)
                 throws IOException {
             Configuration conf = job.getConfiguration();
-            conf.set("mapred.output.dir", location);
+            conf.set(MRConfiguration.OUTPUT_DIR, location);
 
         }
 
@@ -196,7 +197,7 @@ public class TestStoreInstances  {
 
         public OutputCommitterTestInstances(ArrayList<Tuple> outRows,
                 TaskAttemptContext taskAttemptCtx) throws IOException {
-            super(new Path(taskAttemptCtx.getConfiguration().get("mapred.output.dir")), taskAttemptCtx);
+            super(new Path(taskAttemptCtx.getConfiguration().get(MRConfiguration.OUTPUT_DIR)), taskAttemptCtx);
             this.outRows = outRows;
         }
 
diff --git a/test/org/apache/pig/test/Util.java b/test/org/apache/pig/test/Util.java
index 379849b70..65a6e9662 100644
--- a/test/org/apache/pig/test/Util.java
+++ b/test/org/apache/pig/test/Util.java
@@ -66,6 +66,7 @@ import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
 import org.apache.pig.backend.hadoop.executionengine.HExecutionEngine;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.MROperPlan;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
@@ -1249,7 +1250,7 @@ public class Util {
         assertConfLong(conf, "pig.info.reducers.default.parallel", defaultParallel);
         assertConfLong(conf, "pig.info.reducers.requested.parallel", requestedParallel);
         assertConfLong(conf, "pig.info.reducers.estimated.parallel", estimatedParallel);
-        assertConfLong(conf, "mapred.reduce.tasks", runtimeParallel);
+        assertConfLong(conf, MRConfiguration.REDUCE_TASKS, runtimeParallel);
     }
 
     private static void assertConfLong(Configuration conf, String param, long expected) {
diff --git a/test/org/apache/pig/tez/TestTezAutoParallelism.java b/test/org/apache/pig/tez/TestTezAutoParallelism.java
index 963bddf81..751aecfcf 100644
--- a/test/org/apache/pig/tez/TestTezAutoParallelism.java
+++ b/test/org/apache/pig/tez/TestTezAutoParallelism.java
@@ -32,6 +32,7 @@ import org.apache.hadoop.fs.PathFilter;
 import org.apache.pig.PigConfiguration;
 import org.apache.pig.PigServer;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.test.MiniGenericCluster;
 import org.apache.pig.test.Util;
 import org.junit.After;
@@ -117,7 +118,7 @@ public class TestTezAutoParallelism {
     public void testGroupBy() throws IOException{
         // parallelism is 3 originally, reduce to 1
         pigServer.getPigContext().getProperties().setProperty(PigConfiguration.PIG_NO_SPLIT_COMBINATION, "true");
-        pigServer.getPigContext().getProperties().setProperty("mapred.max.split.size", "3000");
+        pigServer.getPigContext().getProperties().setProperty(MRConfiguration.MAX_SPLIT_SIZE, "3000");
         pigServer.getPigContext().getProperties().setProperty(InputSizeReducerEstimator.BYTES_PER_REDUCER_PARAM, 
                 Long.toString(InputSizeReducerEstimator.DEFAULT_BYTES_PER_REDUCER));
         pigServer.registerQuery("A = load '" + INPUT_FILE1 + "' as (name:chararray, age:int);");
@@ -139,7 +140,7 @@ public class TestTezAutoParallelism {
     public void testOrderbyDecreaseParallelism() throws IOException{
         // order by parallelism is 3 originally, reduce to 1
         pigServer.getPigContext().getProperties().setProperty(PigConfiguration.PIG_NO_SPLIT_COMBINATION, "true");
-        pigServer.getPigContext().getProperties().setProperty("mapred.max.split.size", "3000");
+        pigServer.getPigContext().getProperties().setProperty(MRConfiguration.MAX_SPLIT_SIZE, "3000");
         pigServer.getPigContext().getProperties().setProperty(InputSizeReducerEstimator.BYTES_PER_REDUCER_PARAM, 
                 Long.toString(InputSizeReducerEstimator.DEFAULT_BYTES_PER_REDUCER));
         pigServer.registerQuery("A = load '" + INPUT_FILE1 + "' as (name:chararray, age:int);");
@@ -163,7 +164,7 @@ public class TestTezAutoParallelism {
     public void testOrderbyIncreaseParallelism() throws IOException{
         // order by parallelism is 3 originally, increase to 4
         pigServer.getPigContext().getProperties().setProperty(PigConfiguration.PIG_NO_SPLIT_COMBINATION, "true");
-        pigServer.getPigContext().getProperties().setProperty("mapred.max.split.size", "3000");
+        pigServer.getPigContext().getProperties().setProperty(MRConfiguration.MAX_SPLIT_SIZE, "3000");
         pigServer.getPigContext().getProperties().setProperty(InputSizeReducerEstimator.BYTES_PER_REDUCER_PARAM, "1000");
         pigServer.registerQuery("A = load '" + INPUT_FILE1 + "' as (name:chararray, age:int);");
         pigServer.registerQuery("B = group A by name parallel 3;");
@@ -186,7 +187,7 @@ public class TestTezAutoParallelism {
     public void testSkewedJoinDecreaseParallelism() throws IOException{
         // skewed join parallelism is 4 originally, reduce to 1
         pigServer.getPigContext().getProperties().setProperty(PigConfiguration.PIG_NO_SPLIT_COMBINATION, "true");
-        pigServer.getPigContext().getProperties().setProperty("mapred.max.split.size", "3000");
+        pigServer.getPigContext().getProperties().setProperty(MRConfiguration.MAX_SPLIT_SIZE, "3000");
         pigServer.getPigContext().getProperties().setProperty(InputSizeReducerEstimator.BYTES_PER_REDUCER_PARAM, 
                 Long.toString(InputSizeReducerEstimator.DEFAULT_BYTES_PER_REDUCER));
         pigServer.registerQuery("A = load '" + INPUT_FILE1 + "' as (name:chararray, age:int);");
@@ -209,7 +210,7 @@ public class TestTezAutoParallelism {
     public void testSkewedJoinIncreaseParallelism() throws IOException{
         // skewed join parallelism is 3 originally, increase to 5
         pigServer.getPigContext().getProperties().setProperty(PigConfiguration.PIG_NO_SPLIT_COMBINATION, "true");
-        pigServer.getPigContext().getProperties().setProperty("mapred.max.split.size", "3000");
+        pigServer.getPigContext().getProperties().setProperty(MRConfiguration.MAX_SPLIT_SIZE, "3000");
         pigServer.getPigContext().getProperties().setProperty(InputSizeReducerEstimator.BYTES_PER_REDUCER_PARAM, "80000");
         pigServer.registerQuery("A = load '" + INPUT_FILE1 + "' as (name:chararray, age:int);");
         pigServer.registerQuery("B = load '" + INPUT_FILE2 + "' as (name:chararray, gender:chararray);");
diff --git a/test/perf/pigmix/src/java/org/apache/pig/test/pigmix/datagen/DataGenerator.java b/test/perf/pigmix/src/java/org/apache/pig/test/pigmix/datagen/DataGenerator.java
index 8741d6834..5953f8f54 100644
--- a/test/perf/pigmix/src/java/org/apache/pig/test/pigmix/datagen/DataGenerator.java
+++ b/test/perf/pigmix/src/java/org/apache/pig/test/pigmix/datagen/DataGenerator.java
@@ -24,6 +24,7 @@ import java.util.*;
 
 import sdsu.algorithms.data.Zipf;
 
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRConfiguration;
 import org.apache.pig.tools.cmdline.CmdLineParser;
 
 import org.apache.hadoop.fs.FileSystem;
@@ -50,30 +51,30 @@ public class DataGenerator extends Configured implements Tool {
         "j", "k", "l", "m", "n", "o", "p", "q", "r", "s", "t", "u", "v", "w",
         "x", "y", "z"};
 
-    public static void main(String[] args) throws Exception {    	
-    	DataGenerator dg = new DataGenerator();    	 
-    	try {
-    		ToolRunner.run(new Configuration(), dg, args);    			
-   		}catch(Exception e) {
-    		throw new IOException (e);
-    	}    	
+    public static void main(String[] args) throws Exception {
+        DataGenerator dg = new DataGenerator();
+        try {
+            ToolRunner.run(new Configuration(), dg, args);
+           }catch(Exception e) {
+            throw new IOException (e);
+        }
         dg.go();
     }
-    
-    protected DataGenerator(long seed) {    
-    	System.out.println("Using seed " + seed);
+
+    protected DataGenerator(long seed) {
+        System.out.println("Using seed " + seed);
         rand = new Random(seed);
     }
 
     protected DataGenerator() {
-    	
+
     }
-    
+
     protected DataGenerator(String[] args) {
-    	
+
     }
-    
-    public int run(String[] args) throws Exception {    	
+
+    public int run(String[] args) throws Exception {
         CmdLineParser opts = new CmdLineParser(args);
         opts.registerOpt('e', "seed", CmdLineParser.ValueExpected.REQUIRED);
         opts.registerOpt('f', "file", CmdLineParser.ValueExpected.REQUIRED);
@@ -91,24 +92,24 @@ public class DataGenerator extends Configured implements Tool {
                     break;
 
                 case 'f':
-                    outputFile = opts.getValStr();                   
+                    outputFile = opts.getValStr();
                     break;
 
                 case 'i':
-                    inFile = opts.getValStr();                   
+                    inFile = opts.getValStr();
                     break;
 
                 case 'r':
                     numRows = Long.valueOf(opts.getValStr());
                     break;
-    
+
                 case 's':
                     separator = opts.getValStr().charAt(0);
                     break;
-                    
+
                 case 'm':
-                	numMappers = Integer.valueOf(opts.getValStr());
-                	break;
+                    numMappers = Integer.valueOf(opts.getValStr());
+                    break;
 
                 default:
                     usage();
@@ -123,44 +124,44 @@ public class DataGenerator extends Configured implements Tool {
 
         if (numRows < 1 && inFile == null) usage();
 
-        if (numRows > 0 && inFile != null) usage();              
-        
+        if (numRows > 0 && inFile != null) usage();
+
         if (numMappers > 0 && seed != -1) usage();
-        
+
         if (seed == -1){
-        	seed = System.currentTimeMillis();
+            seed = System.currentTimeMillis();
         }
-        
+
         String remainders[] = opts.getRemainingArgs();
         colSpecs = new ColSpec[remainders.length];
         for (int i = 0; i < remainders.length; i++) {
             colSpecs[i] = new ColSpec(remainders[i]);
         }
         System.err.println("Using seed " + seed);
-        rand = new Random(seed); 
-        
+        rand = new Random(seed);
+
         return 0;
     }
 
-    private void go() throws IOException {    	    
-    	long t1 = System.currentTimeMillis();
-    	if (numMappers <= 0) {
-    		System.out.println("Generate data in local mode.");
-        	goLocal();
+    private void go() throws IOException {
+        long t1 = System.currentTimeMillis();
+        if (numMappers <= 0) {
+            System.out.println("Generate data in local mode.");
+            goLocal();
         }else{
-        	System.out.println("Generate data in hadoop mode.");        
-        	HadoopRunner runner = new HadoopRunner();
-        	runner.goHadoop();
+            System.out.println("Generate data in hadoop mode.");
+            HadoopRunner runner = new HadoopRunner();
+            runner.goHadoop();
         }
-    	   	
-    	long t2 = System.currentTimeMillis();
-    	System.out.println("Job is successful! It took " + (t2-t1)/1000 + " seconds.");
+
+        long t2 = System.currentTimeMillis();
+        System.out.println("Job is successful! It took " + (t2-t1)/1000 + " seconds.");
     }
-    
+
     public void goLocal() throws IOException {
-    	
-    	PrintWriter out = null;
-		try {
+
+        PrintWriter out = null;
+        try {
             out = new PrintWriter(outputFile);
         } catch (FileNotFoundException fnfe) {
             System.err.println("Could not find file " + outputFile +
@@ -171,39 +172,39 @@ public class DataGenerator extends Configured implements Tool {
                 ", " + se.getMessage());
             return;
         }
-        
+
         BufferedReader in = null;
         if (inFile != null) {
-        	try {
-      	  		in = new BufferedReader(new FileReader(inFile));
-        	} catch (FileNotFoundException fnfe) {
-        		System.err.println("Unable to find input file " + inFile);
-        		return;
-        	}
-        }
-        
+            try {
+                    in = new BufferedReader(new FileReader(inFile));
+            } catch (FileNotFoundException fnfe) {
+                System.err.println("Unable to find input file " + inFile);
+                return;
+            }
+        }
+
         if (numRows > 0) {
-        	for (int i = 0; i < numRows; i++) {
-        		writeLine(out);
-        		out.println();
-        	}
+            for (int i = 0; i < numRows; i++) {
+                writeLine(out);
+                out.println();
+            }
         } else if (in != null) {
-        	String line;
-        	while ((line = in.readLine()) != null) {
-        		out.print(line);
-        		writeLine(out);
-        		out.println();
-        	}
+            String line;
+            while ((line = in.readLine()) != null) {
+                out.print(line);
+                writeLine(out);
+                out.println();
+            }
         }
         out.close();
-    }      
-    
+    }
+
     protected void writeLine(PrintWriter out) {
         for (int j = 0; j < colSpecs.length; j++) {
             if (j != 0) out.print(separator);
-            // First, decide if it's going to be null            
+            // First, decide if it's going to be null
             if (rand.nextInt(100) < colSpecs[j].pctNull) {
-            	continue;
+                continue;
             }
             writeCol(colSpecs[j], out);
         }
@@ -270,7 +271,7 @@ public class DataGenerator extends Configured implements Tool {
         System.err.println("\t-r -rows number of rows to output");
         System.err.println("\t-s -separator character, default is ^A");
         System.err.println("\t-m -number of mappers to run concurrently to generate data. " +
-        		"If not specified, DataGenerator runs locally. This option can NOT be used with -e.");
+                "If not specified, DataGenerator runs locally. This option can NOT be used with -e.");
         System.err.println();
         System.err.print("\tcolspec: columntype:average_size:cardinality:");
         System.err.println("distribution_type:percent_null");
@@ -290,11 +291,11 @@ public class DataGenerator extends Configured implements Tool {
     }
 
 
- 
+
     static enum Datatype { INT, LONG, FLOAT, DOUBLE, STRING, MAP, BAG };
     static enum DistributionType { UNIFORM, ZIPF };
     protected class ColSpec {
-    	String arg;
+        String arg;
         Datatype datatype;
         DistributionType distype;
         int avgsz;
@@ -306,11 +307,11 @@ public class DataGenerator extends Configured implements Tool {
         Map<Integer, Object> map;
 
         public ColSpec(String arg) {
-        	this.arg = arg;
-        	
+            this.arg = arg;
+
             String[] parts = arg.split(":");
             if (parts.length != 5 && parts.length != 6) {
-                System.err.println("Colspec [" + arg + "] format incorrect"); 
+                System.err.println("Colspec [" + arg + "] format incorrect");
                 usage();
             }
 
@@ -325,7 +326,7 @@ public class DataGenerator extends Configured implements Tool {
                     datatype = Datatype.BAG;
                     contained = new ColSpec(arg.substring(1));
                     return;
-                default: 
+                default:
                     System.err.println("Don't know column type " +
                         parts[0].charAt(0));
                     usage();
@@ -334,12 +335,12 @@ public class DataGenerator extends Configured implements Tool {
             avgsz = Integer.valueOf(parts[1]);
             card = Integer.valueOf(parts[2]);
             switch (parts[3].charAt(0)) {
-                case 'u': 
+                case 'u':
                     gen = new UniformRandomGenerator(avgsz, card);
                     distype = DistributionType.UNIFORM;
                     break;
 
-                case 'z': 
+                case 'z':
                     gen = new ZipfRandomGenerator(avgsz, card);
                     distype = DistributionType.ZIPF;
                     break;
@@ -359,41 +360,41 @@ public class DataGenerator extends Configured implements Tool {
             }
             contained = null;
 
-            // if config has 6 columns, the last col is the file name 
+            // if config has 6 columns, the last col is the file name
             // of the mapping file from random number to field value
             if (parts.length == 6) {
-            	mapfile = parts[5];
-            	gen.hasMapFile = true;
+                mapfile = parts[5];
+                gen.hasMapFile = true;
             }
-            
+
             map = new HashMap<Integer, Object>();
         }
-        
+
         public int nextInt() {
-        	return gen.nextInt(map);
+            return gen.nextInt(map);
         }
-        
+
         public long nextLong() {
-        	return gen.nextInt(map);
+            return gen.nextInt(map);
         }
-        
+
         public double nextDouble() {
-        	return gen.nextDouble(map);
+            return gen.nextDouble(map);
         }
-        
+
         public float nextFloat() {
-        	return gen.nextFloat(map);
+            return gen.nextFloat(map);
         }
-        
+
         public String nextString() {
-        	return gen.nextString(map);
+            return gen.nextString(map);
         }
     }
 
     abstract class RandomGenerator {
 
         protected int avgsz;
-        protected boolean hasMapFile; // indicating whether a map file from 
+        protected boolean hasMapFile; // indicating whether a map file from
                                       // random number to the field value is pre-defined
 
         abstract public int nextInt(Map<Integer, Object> map);
@@ -413,22 +414,22 @@ public class DataGenerator extends Configured implements Tool {
             }
             return sb.toString();
         }
-        
+
         public float randomFloat() {
-        	return rand.nextFloat() * rand.nextInt();
+            return rand.nextFloat() * rand.nextInt();
         }
-        
+
         public double randomDouble() {
-        	return rand.nextDouble() * rand.nextInt();
+            return rand.nextDouble() * rand.nextInt();
         }
     }
 
     class UniformRandomGenerator extends RandomGenerator {
-        int card;       
-        
+        int card;
+
         public UniformRandomGenerator(int a, int c) {
             avgsz = a;
-            card = c;            
+            card = c;
         }
 
         public int nextInt(Map<Integer, Object> map) {
@@ -443,26 +444,26 @@ public class DataGenerator extends Configured implements Tool {
             int seed = rand.nextInt(card);
             Float f = (Float)map.get(seed);
             if (f == null) {
-            	if (!hasMapFile) {
-            		f = randomFloat();
-            		map.put(seed, f);
-            	}else{
-            		throw new IllegalStateException("Number " + seed + " is not found in map file");
-            	}
+                if (!hasMapFile) {
+                    f = randomFloat();
+                    map.put(seed, f);
+                }else{
+                    throw new IllegalStateException("Number " + seed + " is not found in map file");
+                }
             }
             return f;
         }
-        
+
         public double nextDouble(Map<Integer, Object> map) {
             int seed = rand.nextInt(card);
             Double d = (Double)map.get(seed);
             if (d == null) {
-            	if (!hasMapFile) {
-            		d = randomDouble();
-            		map.put(seed, d);
-            	}else{
-            		throw new IllegalStateException("Number " + seed + " is not found in map file");
-            	}
+                if (!hasMapFile) {
+                    d = randomDouble();
+                    map.put(seed, d);
+                }else{
+                    throw new IllegalStateException("Number " + seed + " is not found in map file");
+                }
             }
             return d;
         }
@@ -471,16 +472,16 @@ public class DataGenerator extends Configured implements Tool {
             int seed = rand.nextInt(card);
             String s = (String)map.get(seed);
             if (s == null) {
-            	if (!hasMapFile) {
-            		s = randomString();
-            		map.put(seed, s);
-            	}else{
-            		throw new IllegalStateException("Number " + seed + " is not found in map file");
-            	}
+                if (!hasMapFile) {
+                    s = randomString();
+                    map.put(seed, s);
+                }else{
+                    throw new IllegalStateException("Number " + seed + " is not found in map file");
+                }
             }
             return s;
         }
-        
+
     }
 
     class ZipfRandomGenerator extends RandomGenerator {
@@ -488,15 +489,15 @@ public class DataGenerator extends Configured implements Tool {
 
         public ZipfRandomGenerator(int a, int c) {
             avgsz = a;
-            z = new Zipf(c);         
+            z = new Zipf(c);
         }
-        
-        
+
+
         // the Zipf library returns a random number [1..cardinality], so we substract by 1
         // to get [0..cardinality)
         // the randome number returned by zipf library is an integer, but converted into double
         private double next() {
-        	return z.nextElement()-1;
+            return z.nextElement()-1;
         }
 
         public int nextInt(Map<Integer, Object> map) {
@@ -508,324 +509,321 @@ public class DataGenerator extends Configured implements Tool {
         }
 
         public float nextFloat(Map<Integer, Object> map) {
-        	int seed = (int)next();
+            int seed = (int)next();
             Float d = (Float)map.get(seed);
             if (d == null) {
-            	if (!hasMapFile) {
-            		d = randomFloat();
-            		map.put(seed, d);
-            	}else{
-            		throw new IllegalStateException("Number " + seed + " is not found in map file");
-            	}
+                if (!hasMapFile) {
+                    d = randomFloat();
+                    map.put(seed, d);
+                }else{
+                    throw new IllegalStateException("Number " + seed + " is not found in map file");
+                }
             }
             return d;
         }
 
         public double nextDouble(Map<Integer, Object> map) {
-        	 int seed = (int)next();
+             int seed = (int)next();
              Double d = (Double)map.get(seed);
              if (d == null) {
-             	if (!hasMapFile) {
-             		d = randomDouble();
-             		map.put(seed, d);
-             	}else{
-             		throw new IllegalStateException("Number " + seed + " is not found in map file");
-             	}
+                 if (!hasMapFile) {
+                     d = randomDouble();
+                     map.put(seed, d);
+                 }else{
+                     throw new IllegalStateException("Number " + seed + " is not found in map file");
+                 }
              }
              return d;
         }
-        
+
         public String nextString(Map<Integer, Object> map) {
             int seed = (int)next();
             String s = (String)map.get(seed);
             if (s == null) {
-            	if (!hasMapFile) {
-            		s = randomString();
-            		map.put(seed, s);
-            	}else{
-            		throw new IllegalStateException("Number " + seed + " is not found in map file");
-            	}
+                if (!hasMapFile) {
+                    s = randomString();
+                    map.put(seed, s);
+                }else{
+                    throw new IllegalStateException("Number " + seed + " is not found in map file");
+                }
             }
             return s;
         }
     }
-    
-//  launch hadoop job
+
+    //  launch hadoop job
     class HadoopRunner {
-    	Random r;
-    	FileSystem fs;
-    	Path tmpHome;
-    	
-    	public HadoopRunner() {
-    		r = new Random();
-    	}
-    	
-    	public void goHadoop() throws IOException {
+        Random r;
+        FileSystem fs;
+        Path tmpHome;
+
+        public HadoopRunner() {
+            r = new Random();
+        }
+
+        public void goHadoop() throws IOException {
             // Configuration processed by ToolRunner
             Configuration conf = getConf();
-            
-            // Create a JobConf using the processed conf            
-            JobConf job = new JobConf(conf);     
-            fs = FileSystem.get(job);           
-            
-            tmpHome = createTempDir(null);                          
-            
+
+            // Create a JobConf using the processed conf
+            JobConf job = new JobConf(conf);
+            fs = FileSystem.get(job);
+
+            tmpHome = createTempDir(null);
+
             String config = genMapFiles().toUri().getRawPath();
             // set config properties into job conf
-            job.set("fieldconfig", config);      
+            job.set("fieldconfig", config);
             job.set("separator", String.valueOf((int)separator));
-            
-            
+
+
             job.setJobName("data-gen");
             job.setNumMapTasks(numMappers);
-            job.setNumReduceTasks(0);          
-            job.setMapperClass(DataGenMapper.class);   
+            job.setNumReduceTasks(0);
+            job.setMapperClass(DataGenMapper.class);
             job.setJarByClass(DataGenMapper.class);
-            
+
             // if inFile is specified, use it as input
             if (inFile != null) {
-           	 FileInputFormat.setInputPaths(job, inFile);
-           	 job.set("hasinput", "true");
+                FileInputFormat.setInputPaths(job, inFile);
+                job.set("hasinput", "true");
            } else {
-        	   job.set("hasinput", "false");
-        	   Path input = genInputFiles();        
-        	   FileInputFormat.setInputPaths(job, input);
+               job.set("hasinput", "false");
+               Path input = genInputFiles();
+               FileInputFormat.setInputPaths(job, input);
            }
            FileOutputFormat.setOutputPath(job, new Path(outputFile));
-                               
+
             // Submit the job, then poll for progress until the job is complete
             System.out.println("Submit hadoop job...");
             RunningJob j = JobClient.runJob(job);
             if (!j.isSuccessful()) {
-            	throw new IOException("Job failed");
+                throw new IOException("Job failed");
             }
-                        
-            if (fs.exists(tmpHome)) {            	
-            	fs.delete(tmpHome, true);
+
+            if (fs.exists(tmpHome)) {
+                fs.delete(tmpHome, true);
             }
          }
-    	
-    	 private Path genInputFiles() throws IOException {
-    		 long avgRows = numRows/numMappers;
-    	        
-    		 // create a temp directory as mappers input
-    	     Path input = createTempDir(tmpHome);
-    	     System.out.println("Generating input files into " + input.toString());
-    	     
-    	     long rowsLeft = numRows;
-    	     
-    	     // create one input file per mapper, which contains
-    	     // the number of rows 
-    	     for(int i=0; i<numMappers; i++) {
-    	    	 Object[] tmp = createTempFile(input, false);
-    	    	 PrintWriter pw = new PrintWriter((OutputStream)tmp[1]);
-    	    	 
-    	    	 if (i < numMappers-1) {
-    	    		 pw.println(avgRows);
-    	    	 }else{
-    	    		 // last mapper takes all the rows left
-    	    		 pw.println(rowsLeft);
-    	    	 }
-    	    
-    	    	 pw.close();
-    	    	 rowsLeft -= avgRows;
-    	     }
-    	     
-    	     return input;
-    	 }
-    	
-    	// generate map files for all the fields that need to pre-generate map files
-    	// return a config file which contains config info for each field, including 
-    	// the path to their map file
-    	 private Path genMapFiles() throws IOException {
-    		 Object[] tmp = createTempFile(tmpHome, false);
-    		 
-    		 System.out.println("Generating column config file in " + tmp[0].toString());
-    		 PrintWriter pw = new PrintWriter((OutputStream)tmp[1]);
-    		 for(int i=0; i<colSpecs.length; i++) {
-    			 DataGenerator.Datatype datatype = colSpecs[i].datatype;
-    			 pw.print(colSpecs[i].arg);
-    			 
-    			 if ( datatype == DataGenerator.Datatype.FLOAT || datatype == DataGenerator.Datatype.DOUBLE ||
-    					 datatype == DataGenerator.Datatype.STRING) 	 {
-    				 Path p = genMapFile(colSpecs[i]);
-    				 pw.print(':');    				
-    				 pw.print(p.toUri().getRawPath());				 
-    			 } 
-    			 
-    			 pw.println();
-    		 }
-    		 
-    		 pw.close();
-    		 
-    		 return (Path)tmp[0];
-    	 }
-    	 
-    	 // genereate a map file between random number to field value
-    	 // return the path of the map file
-    	 private Path genMapFile(DataGenerator.ColSpec col) throws IOException {
-    		 int card = col.card;
-    		 Object[] tmp = createTempFile(tmpHome, false);
-    		 
-    		 System.out.println("Generating mapping file for column " + col.arg + " into " + tmp[0].toString());
-    		 PrintWriter pw = new PrintWriter((OutputStream)tmp[1]);
-    		 HashSet<Object> hash = new HashSet<Object>(card);
-    		 for(int i=0; i<card; i++) {
-    			 pw.print(i);
-    			 pw.print("\t");
-    			 Object next = null;
-    			 do {
-    				 if (col.datatype == DataGenerator.Datatype.DOUBLE) {
-        				 next = col.gen.randomDouble();
-        			 }else if (col.datatype == DataGenerator.Datatype.FLOAT) {
-        				 next = col.gen.randomFloat();
-        			 }else if (col.datatype == DataGenerator.Datatype.STRING) {
-        				 next = col.gen.randomString();
-        			 }
-    			 }while(hash.contains(next));
-    			 
-    			 hash.add(next);
-    			 
-    			 pw.println(next);
-    			 
-    			 if ( (i>0 && i%300000 == 0) || i == card-1 ) {
-    				 System.out.println("processed " + i*100/card + "%." );
-    				 pw.flush();
-    			 }    			 
-    		 }    		 
-    		 
-    		 pw.close();
-    		 
-    		 return (Path)tmp[0];
-        	 
-    	 }
-    	 
-    	 private Path createTempDir(Path parentDir) throws IOException {
-    		 Object[] obj = createTempFile(parentDir, true);
-    		 return (Path)obj[0];
-    	 }
-    	 
-    	 private Object[] createTempFile(Path parentDir, boolean isDir) throws IOException {		
-    		 Path tmp_home = parentDir;
-    		 
-    		 if (tmp_home == null) {
-    			 tmp_home = new Path(fs.getHomeDirectory(), "tmp");
-    		 }
-    		 
-    		 if (!fs.exists(tmp_home)) {
-    			 fs.mkdirs(tmp_home);
-    		 }
-    		 
-    		 int id = r.nextInt();
-    		 Path f = new Path(tmp_home, "tmp" + id);
-    		 while (fs.exists(f)) {
-    			 id = r.nextInt();
-    			 f = new Path(tmp_home, "tmp" + id);
-    		 }
-    		 
-    		 // return a 2-element array. first element is PATH,
-    		 // second element is OutputStream
-    		 Object[] r = new Object[2];
-    		 r[0] = f;
-    		 if (!isDir) {
-    			 r[1] = fs.create(f);
-    		 }else{
-    			 fs.mkdirs(f);
-    		 }
-    		 
-    		 return r;
-    	 }    	
+
+         private Path genInputFiles() throws IOException {
+             long avgRows = numRows/numMappers;
+
+             // create a temp directory as mappers input
+             Path input = createTempDir(tmpHome);
+             System.out.println("Generating input files into " + input.toString());
+
+             long rowsLeft = numRows;
+
+             // create one input file per mapper, which contains
+             // the number of rows
+             for(int i=0; i<numMappers; i++) {
+                 Object[] tmp = createTempFile(input, false);
+                 PrintWriter pw = new PrintWriter((OutputStream)tmp[1]);
+
+                 if (i < numMappers-1) {
+                     pw.println(avgRows);
+                 }else{
+                     // last mapper takes all the rows left
+                     pw.println(rowsLeft);
+                 }
+
+                 pw.close();
+                 rowsLeft -= avgRows;
+             }
+
+             return input;
+         }
+
+        // generate map files for all the fields that need to pre-generate map files
+        // return a config file which contains config info for each field, including
+        // the path to their map file
+         private Path genMapFiles() throws IOException {
+             Object[] tmp = createTempFile(tmpHome, false);
+
+             System.out.println("Generating column config file in " + tmp[0].toString());
+             PrintWriter pw = new PrintWriter((OutputStream)tmp[1]);
+             for(int i=0; i<colSpecs.length; i++) {
+                 DataGenerator.Datatype datatype = colSpecs[i].datatype;
+                 pw.print(colSpecs[i].arg);
+
+                 if ( datatype == DataGenerator.Datatype.FLOAT || datatype == DataGenerator.Datatype.DOUBLE ||
+                         datatype == DataGenerator.Datatype.STRING)      {
+                     Path p = genMapFile(colSpecs[i]);
+                     pw.print(':');
+                     pw.print(p.toUri().getRawPath());
+                 }
+
+                 pw.println();
+             }
+
+             pw.close();
+
+             return (Path)tmp[0];
+         }
+
+         // genereate a map file between random number to field value
+         // return the path of the map file
+         private Path genMapFile(DataGenerator.ColSpec col) throws IOException {
+             int card = col.card;
+             Object[] tmp = createTempFile(tmpHome, false);
+
+             System.out.println("Generating mapping file for column " + col.arg + " into " + tmp[0].toString());
+             PrintWriter pw = new PrintWriter((OutputStream)tmp[1]);
+             HashSet<Object> hash = new HashSet<Object>(card);
+             for(int i=0; i<card; i++) {
+                 pw.print(i);
+                 pw.print("\t");
+                 Object next = null;
+                 do {
+                     if (col.datatype == DataGenerator.Datatype.DOUBLE) {
+                         next = col.gen.randomDouble();
+                     }else if (col.datatype == DataGenerator.Datatype.FLOAT) {
+                         next = col.gen.randomFloat();
+                     }else if (col.datatype == DataGenerator.Datatype.STRING) {
+                         next = col.gen.randomString();
+                     }
+                 }while(hash.contains(next));
+
+                 hash.add(next);
+
+                 pw.println(next);
+
+                 if ( (i>0 && i%300000 == 0) || i == card-1 ) {
+                     System.out.println("processed " + i*100/card + "%." );
+                     pw.flush();
+                 }
+             }
+
+             pw.close();
+
+             return (Path)tmp[0];
+
+         }
+
+         private Path createTempDir(Path parentDir) throws IOException {
+             Object[] obj = createTempFile(parentDir, true);
+             return (Path)obj[0];
+         }
+
+         private Object[] createTempFile(Path parentDir, boolean isDir) throws IOException {
+             Path tmp_home = parentDir;
+
+             if (tmp_home == null) {
+                 tmp_home = new Path(fs.getHomeDirectory(), "tmp");
+             }
+
+             if (!fs.exists(tmp_home)) {
+                 fs.mkdirs(tmp_home);
+             }
+
+             int id = r.nextInt();
+             Path f = new Path(tmp_home, "tmp" + id);
+             while (fs.exists(f)) {
+                 id = r.nextInt();
+                 f = new Path(tmp_home, "tmp" + id);
+             }
+
+             // return a 2-element array. first element is PATH,
+             // second element is OutputStream
+             Object[] r = new Object[2];
+             r[0] = f;
+             if (!isDir) {
+                 r[1] = fs.create(f);
+             }else{
+                 fs.mkdirs(f);
+             }
+
+             return r;
+         }
     }
 
-	 public static class DataGenMapper extends MapReduceBase implements Mapper<LongWritable, Text, String, String> {
- 		private JobConf jobConf;
- 		private DataGenerator dg; 		
- 		private boolean hasInput;
- 	   
- 		public void configure(JobConf jobconf) {
- 			this.jobConf = jobconf;
- 						 			
- 			int id = Integer.parseInt(jobconf.get("mapred.task.partition"));
- 			long time = System.currentTimeMillis() - id*3600*24*1000;
- 			 			
- 			dg = new DataGenerator( ((time-id*3600*24*1000) | (id << 48)));
- 			
- 		    dg.separator = (char)Integer.parseInt(jobConf.get("separator"));
- 		     		    
- 		    if (jobConf.get("hasinput").equals("true")) {
- 		    	hasInput = true;
- 		    }
- 		    
- 		    String config = jobConf.get("fieldconfig");
- 		    		    
- 		    try {			    
- 			    FileSystem fs = FileSystem.get(jobconf);
- 			    
- 			    // load in config file for each column
- 			    BufferedReader reader = new BufferedReader(new InputStreamReader(fs.open(new Path(config))));
- 			    String line = null;
- 			    List<DataGenerator.ColSpec> cols = new ArrayList<DataGenerator.ColSpec>();
- 			    while((line = reader.readLine()) != null) { 			    	
- 			    	cols.add(dg.new ColSpec(line));		    	
- 			    }
- 			    reader.close();
- 			    dg.colSpecs = cols.toArray(new DataGenerator.ColSpec[0]); 			    
- 			    
- 			    // load in mapping files
- 			    for(int i=0; i<dg.colSpecs.length; i++) {
- 			    	DataGenerator.ColSpec col = dg.colSpecs[i];
- 			    	if (col.mapfile != null) { 			    		
- 			    		reader = new BufferedReader(new InputStreamReader(fs.open(new Path(col.mapfile))));
- 			    		Map<Integer, Object> map = dg.colSpecs[i].map;
- 			    		while((line = reader.readLine()) != null) {
- 					    	String[] fields = line.split("\t");
- 					    	int key = Integer.parseInt(fields[0]);
- 					    	if (col.datatype == DataGenerator.Datatype.DOUBLE) {
- 					    		map.put(key, Double.parseDouble(fields[1]));
- 					    	}else if (col.datatype == DataGenerator.Datatype.FLOAT) {
- 					    		map.put(key, Float.parseFloat(fields[1]));
- 					    	}else {
- 					    		map.put(key, fields[1]);
- 					    	}
- 					    }
- 			    		
- 			    		reader.close();
- 			    	}
- 			    }
- 		    }catch(IOException e) {
- 		    	throw new RuntimeException("Failed to load config file. " + e);
- 		    }
-  		}
- 		
- 		public void map(LongWritable key, Text value, OutputCollector<String, String> output, Reporter reporter) throws IOException {
- 			int intialsz = dg.colSpecs.length * 50;
- 			
- 			if (!hasInput) {
- 				long numRows = Long.parseLong(value.toString().trim()); 		
- 	 	        dg.numRows = numRows; 	 	         	       
- 	 	        
- 	 	        for (int i = 0; i < numRows; i++) {
- 	 	        	StringWriter str = new StringWriter(intialsz);
- 	 	        	PrintWriter pw = new PrintWriter(str);
- 	 	        	dg.writeLine(pw); 	    
- 	 	        	output.collect(null, str.toString()); 	        	 	        
- 	 	        	
- 	 	        	if ((i+1) % 10000 == 0) {
- 	 	        		reporter.progress();
- 	 	        		reporter.setStatus("" + (i+1) + " tuples generated.");  	        	
- 	 	        	}
- 	 	        }	       
- 			} else {
- 				StringWriter str = new StringWriter(intialsz);
-	 	        PrintWriter pw = new PrintWriter(str);
-	 	        pw.write(value.toString());
-	 	        dg.writeLine(pw); 	    
-	 	        output.collect(null, str.toString()); 	        	 	        	 	        	
- 			}
- 		}
- 	}
-}
+    public static class DataGenMapper extends MapReduceBase implements Mapper<LongWritable, Text, String, String> {
+         private JobConf jobConf;
+         private DataGenerator dg;
+         private boolean hasInput;
+
+         public void configure(JobConf jobconf) {
+             this.jobConf = jobconf;
 
+             int id = Integer.parseInt(jobconf.get(MRConfiguration.TASK_PARTITION));
+             long time = System.currentTimeMillis() - id*3600*24*1000;
 
+             dg = new DataGenerator( ((time-id*3600*24*1000) | (id << 48)));
+
+             dg.separator = (char)Integer.parseInt(jobConf.get("separator"));
+
+             if (jobConf.get("hasinput").equals("true")) {
+                 hasInput = true;
+             }
 
+             String config = jobConf.get("fieldconfig");
+
+             try {
+                 FileSystem fs = FileSystem.get(jobconf);
+
+                 // load in config file for each column
+                 BufferedReader reader = new BufferedReader(new InputStreamReader(fs.open(new Path(config))));
+                 String line = null;
+                 List<DataGenerator.ColSpec> cols = new ArrayList<DataGenerator.ColSpec>();
+                 while((line = reader.readLine()) != null) {
+                     cols.add(dg.new ColSpec(line));
+                 }
+                 reader.close();
+                 dg.colSpecs = cols.toArray(new DataGenerator.ColSpec[0]);
+
+                 // load in mapping files
+                 for(int i=0; i<dg.colSpecs.length; i++) {
+                     DataGenerator.ColSpec col = dg.colSpecs[i];
+                     if (col.mapfile != null) {
+                         reader = new BufferedReader(new InputStreamReader(fs.open(new Path(col.mapfile))));
+                         Map<Integer, Object> map = dg.colSpecs[i].map;
+                         while((line = reader.readLine()) != null) {
+                             String[] fields = line.split("\t");
+                             int key = Integer.parseInt(fields[0]);
+                             if (col.datatype == DataGenerator.Datatype.DOUBLE) {
+                                 map.put(key, Double.parseDouble(fields[1]));
+                             }else if (col.datatype == DataGenerator.Datatype.FLOAT) {
+                                 map.put(key, Float.parseFloat(fields[1]));
+                             }else {
+                                 map.put(key, fields[1]);
+                             }
+                         }
+
+                         reader.close();
+                     }
+                 }
+             }catch(IOException e) {
+                 throw new RuntimeException("Failed to load config file. " + e);
+             }
+         }
+
+         public void map(LongWritable key, Text value, OutputCollector<String, String> output, Reporter reporter) throws IOException {
+             int intialsz = dg.colSpecs.length * 50;
+
+             if (!hasInput) {
+                 long numRows = Long.parseLong(value.toString().trim());
+                  dg.numRows = numRows;
+
+                  for (int i = 0; i < numRows; i++) {
+                      StringWriter str = new StringWriter(intialsz);
+                      PrintWriter pw = new PrintWriter(str);
+                      dg.writeLine(pw);
+                      output.collect(null, str.toString());
+
+                      if ((i+1) % 10000 == 0) {
+                          reporter.progress();
+                          reporter.setStatus("" + (i+1) + " tuples generated.");
+                      }
+                  }
+             } else {
+                 StringWriter str = new StringWriter(intialsz);
+                 PrintWriter pw = new PrintWriter(str);
+                 pw.write(value.toString());
+                 dg.writeLine(pw);
+                 output.collect(null, str.toString());
+             }
+         }
+     }
+}
 
