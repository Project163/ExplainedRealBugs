diff --git a/CHANGES.txt b/CHANGES.txt
index 6c29c5984..00585610b 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -62,6 +62,8 @@ PIG-3174: Remove rpm and deb artifacts from build.xml (gates)
 
 IMPROVEMENTS
 
+PIG-3480: TFile-based tmpfile compression crashes in some cases (dvryaboy via aniket486)
+
 PIG-3503: More document for Pig 0.12 new features (daijy)
 
 PIG-3445: Make Parquet format available out of the box in Pig (lbendig via aniket486)
diff --git a/conf/pig.properties b/conf/pig.properties
index 10cfd44dc..b4bab9752 100644
--- a/conf/pig.properties
+++ b/conf/pig.properties
@@ -59,13 +59,17 @@
 
 #Performance tuning properties
 #pig.cachedbag.memusage=0.2
-#pig.skewedjoin.reduce.memusagea=0.3
+#pig.skewedjoin.reduce.memusage=0.3
 #pig.exec.nocombiner=false
 #opt.multiquery=true
+
+#Following parameters are for configuring intermediate storage format
+#Supported storage types are seqfile and tfile
+#Supported codec types: tfile supports gz(gzip) and lzo, seqfile support gz(gzip), lzo, snappy, bzip2
 #pig.tmpfilecompression=false
+#pig.tmpfilecompression.storage=seqfile
+#pig.tmpfilecompression.codec=gz
 
-#value can be lzo or gzip
-#pig.tmpfilecompression.codec=gzip
 #pig.noSplitCombination=true
 
 #pig.exec.mapPartAgg=false
diff --git a/src/org/apache/pig/PigConfiguration.java b/src/org/apache/pig/PigConfiguration.java
index 6f08bdbc0..c3a75900f 100644
--- a/src/org/apache/pig/PigConfiguration.java
+++ b/src/org/apache/pig/PigConfiguration.java
@@ -109,4 +109,23 @@ public class PigConfiguration {
      * Controls the size of Pig script stored in job xml.
      */
     public static final String MAX_SCRIPT_SIZE = "pig.script.max.size";
+
+   /**
+     * This key is used to define whether to have intermediate file compressed
+     */
+    public static final String PIG_ENABLE_TEMP_FILE_COMPRESSION = "pig.tmpfilecompression";
+
+    /**
+     * This key is used to set the storage type used by intermediate file storage
+     * If pig.tmpfilecompression, default storage used is TFileStorage.
+     * This can be overriden to use SequenceFileInterStorage by setting following property to "seqfile".
+     */
+    public static final String PIG_TEMP_FILE_COMPRESSION_STORAGE = "pig.tmpfilecompression.storage";
+
+    /**
+     * Compression codec used by intermediate storage
+     * TFileStorage only support gzip and lzo.
+     */
+    public static final String PIG_TEMP_FILE_COMPRESSION_CODEC = "pig.tmpfilecompression.codec";
 }
+
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java
index d54214834..a6393dc24 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java
@@ -810,10 +810,7 @@ public class JobControlCompiler{
             }
 
             // tmp file compression setups
-            if (Utils.tmpFileCompression(pigContext)) {
-                conf.setBoolean("pig.tmpfilecompression", true);
-                conf.set("pig.tmpfilecompression.codec", Utils.tmpFileCompressionCodec(pigContext));
-            }
+            Utils.setTmpFileCompressionOnConf(pigContext, conf);
 
             String tmp;
             long maxCombinedSplitSize = 0;
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/partitioners/WeightedRangePartitioner.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/partitioners/WeightedRangePartitioner.java
index 3bb5b00a8..526566563 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/partitioners/WeightedRangePartitioner.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/partitioners/WeightedRangePartitioner.java
@@ -31,7 +31,6 @@ import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.io.RawComparator;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.mapreduce.Partitioner;
-import org.apache.pig.ExecType;
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.backend.hadoop.HDataType;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce;
@@ -122,12 +121,9 @@ public class WeightedRangePartitioner extends Partitioner<PigNullableWritable, W
             if (configuration.get("fs.hdfs.impl") != null) {
                 conf.set("fs.hdfs.impl", configuration.get("fs.hdfs.impl"));
             }
-            if (configuration.getBoolean("pig.tmpfilecompression", false)) {
-                conf.setBoolean("pig.tmpfilecompression", true);
-                if (configuration.get("pig.tmpfilecompression.codec") != null) {
-                    conf.set("pig.tmpfilecompression.codec", configuration.get("pig.tmpfilecompression.codec"));
-            }
-            }
+
+            MapRedUtil.copyTmpFileConfigurationValues(configuration, conf);
+
             conf.set(MapRedUtil.FILE_SYSTEM_NAME, "file:///");
 
             ReadToEndLoader loader = new ReadToEndLoader(Utils.getTmpFileStorageObject(conf),
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPartitionRearrange.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPartitionRearrange.java
index bbb137b48..4ef55b81b 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPartitionRearrange.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POPartitionRearrange.java
@@ -17,6 +17,7 @@
  */
 package org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators;
 
+import java.io.IOException;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.List;
@@ -87,14 +88,10 @@ public class POPartitionRearrange extends POLocalRearrange {
             "Internal error: missing key distribution file property.");
         }
 
-        boolean tmpFileCompression = Utils.tmpFileCompression(pigContext);
-        if (tmpFileCompression) {
-            PigMapReduce.sJobConfInternal.get().setBoolean("pig.tmpfilecompression", true);
-            try {
-                PigMapReduce.sJobConfInternal.get().set("pig.tmpfilecompression.codec", Utils.tmpFileCompressionCodec(pigContext));
-            } catch (Exception e) {
-                throw new RuntimeException(e);
-            }
+        try {
+            Utils.setTmpFileCompressionOnConf(pigContext, PigMapReduce.sJobConfInternal.get());
+        } catch (IOException ie) {
+            throw new RuntimeException(ie);
         }
         try {
 
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/util/MapRedUtil.java b/src/org/apache/pig/backend/hadoop/executionengine/util/MapRedUtil.java
index 0502917a2..9769c31d7 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/util/MapRedUtil.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/util/MapRedUtil.java
@@ -20,14 +20,14 @@ package org.apache.pig.backend.hadoop.executionengine.util;
 
 import java.io.IOException;
 import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.Comparator;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
-import java.util.Comparator;
-import java.util.Collections;
-import java.util.Arrays;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
@@ -38,13 +38,13 @@ import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.PathFilter;
 import org.apache.hadoop.mapreduce.InputSplit;
 import org.apache.pig.FuncSpec;
+import org.apache.pig.PigConfiguration;
 import org.apache.pig.PigException;
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
-import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigSplit;
 import org.apache.pig.data.DataBag;
 import org.apache.pig.data.DataType;
 import org.apache.pig.data.Tuple;
@@ -89,20 +89,17 @@ public class MapRedUtil {
         if (mapConf.get("yarn.resourcemanager.principal")!=null) {
             conf.set("yarn.resourcemanager.principal", mapConf.get("yarn.resourcemanager.principal"));
         }
-        
+
         if (PigMapReduce.sJobConfInternal.get().get("fs.file.impl")!=null)
             conf.set("fs.file.impl", PigMapReduce.sJobConfInternal.get().get("fs.file.impl"));
         if (PigMapReduce.sJobConfInternal.get().get("fs.hdfs.impl")!=null)
             conf.set("fs.hdfs.impl", PigMapReduce.sJobConfInternal.get().get("fs.hdfs.impl"));
-        if (PigMapReduce.sJobConfInternal.get().getBoolean("pig.tmpfilecompression", false))
-        {
-            conf.setBoolean("pig.tmpfilecompression", true);
-            if (PigMapReduce.sJobConfInternal.get().get("pig.tmpfilecompression.codec")!=null)
-                conf.set("pig.tmpfilecompression.codec", PigMapReduce.sJobConfInternal.get().get("pig.tmpfilecompression.codec"));
-        }
+
+        copyTmpFileConfigurationValues(PigMapReduce.sJobConfInternal.get(), conf);
+
         conf.set(MapRedUtil.FILE_SYSTEM_NAME, "file:///");
 
-        ReadToEndLoader loader = new ReadToEndLoader(Utils.getTmpFileStorageObject(PigMapReduce.sJobConfInternal.get()), conf, 
+        ReadToEndLoader loader = new ReadToEndLoader(Utils.getTmpFileStorageObject(PigMapReduce.sJobConfInternal.get()), conf,
                 keyDistFile, 0);
         DataBag partitionList;
         Tuple t = loader.getNext();
@@ -150,7 +147,24 @@ public class MapRedUtil {
         }
         return reducerMap;
     }
-    
+
+    public static void copyTmpFileConfigurationValues(Configuration fromConf, Configuration toConf) {
+        // Currently these are used only by loaders (and not storers), so we do not need to copy
+        // mapred properties that are required by @{Link SequenceFileInterStorage}
+
+        if (fromConf.getBoolean(PigConfiguration.PIG_ENABLE_TEMP_FILE_COMPRESSION, false)) {
+            toConf.setBoolean(PigConfiguration.PIG_ENABLE_TEMP_FILE_COMPRESSION, true);
+            if (fromConf.get(PigConfiguration.PIG_TEMP_FILE_COMPRESSION_CODEC) != null) {
+                toConf.set(PigConfiguration.PIG_TEMP_FILE_COMPRESSION_CODEC,
+                        fromConf.get(PigConfiguration.PIG_TEMP_FILE_COMPRESSION_CODEC));
+            }
+            if (fromConf.get(PigConfiguration.PIG_TEMP_FILE_COMPRESSION_STORAGE) != null) {
+                toConf.set(PigConfiguration.PIG_TEMP_FILE_COMPRESSION_STORAGE,
+                        fromConf.get(PigConfiguration.PIG_TEMP_FILE_COMPRESSION_STORAGE));
+            }
+        }
+    }
+
     public static void setupUDFContext(Configuration job) throws IOException {
         UDFContext udfc = UDFContext.getUDFContext();
         udfc.addJobConf(job);
diff --git a/src/org/apache/pig/impl/io/InterStorage.java b/src/org/apache/pig/impl/io/InterStorage.java
index 4795f252b..4a28f9570 100644
--- a/src/org/apache/pig/impl/io/InterStorage.java
+++ b/src/org/apache/pig/impl/io/InterStorage.java
@@ -61,6 +61,7 @@ public class InterStorage extends FileInputLoadFunc
 implements StoreFuncInterface, LoadMetadata {
 
     private static final Log mLog = LogFactory.getLog(InterStorage.class);
+    public static final String useLog = "Pig Internal storage in use";
     
     private InterRecordReader recReader = null;
     private InterRecordWriter recWriter = null;
@@ -69,7 +70,7 @@ implements StoreFuncInterface, LoadMetadata {
      * Simple binary nested reader format
      */
     public InterStorage() {
-        mLog.debug("Pig Internal storage in use");
+        mLog.debug(useLog);
     }
 
     @Override
diff --git a/src/org/apache/pig/impl/io/SequenceFileInterStorage.java b/src/org/apache/pig/impl/io/SequenceFileInterStorage.java
new file mode 100644
index 000000000..dd3fe42fe
--- /dev/null
+++ b/src/org/apache/pig/impl/io/SequenceFileInterStorage.java
@@ -0,0 +1,238 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements. See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership. The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License. You may obtain a copy of the License at
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.pig.impl.io;
+
+import java.io.IOException;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.classification.InterfaceAudience;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.NullWritable;
+import org.apache.hadoop.io.SequenceFile;
+import org.apache.hadoop.io.SequenceFile.CompressionType;
+import org.apache.hadoop.io.compress.CompressionCodec;
+import org.apache.hadoop.io.compress.DefaultCodec;
+import org.apache.hadoop.mapreduce.InputFormat;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.OutputFormat;
+import org.apache.hadoop.mapreduce.RecordReader;
+import org.apache.hadoop.mapreduce.RecordWriter;
+import org.apache.hadoop.mapreduce.TaskAttemptContext;
+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
+import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
+import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
+import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.pig.Expression;
+import org.apache.pig.FileInputLoadFunc;
+import org.apache.pig.LoadFunc;
+import org.apache.pig.LoadMetadata;
+import org.apache.pig.ResourceSchema;
+import org.apache.pig.ResourceStatistics;
+import org.apache.pig.StoreFunc;
+import org.apache.pig.StoreFuncInterface;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigSplit;
+import org.apache.pig.data.BinSedesTuple;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.impl.util.Utils;
+
+/**
+ * Store tuples (BinSedesTuples, specifically) using sequence files to leverage
+ * sequence file's compression features. Replacement for previous use of {@code TFileStorage}, which
+ * had some edge cases that were not properly handled there.
+ */
+@InterfaceAudience.Private
+public class SequenceFileInterStorage extends FileInputLoadFunc implements
+StoreFuncInterface, LoadMetadata {
+    private static final Log mLog = LogFactory.getLog(SequenceFileInterStorage.class);
+    public static final String useLog = "SequenceFile storage in use";
+
+    final private NullWritable KEY0 = NullWritable.get();
+
+    RecordReader<NullWritable, BinSedesTuple> recReader;
+    RecordWriter<NullWritable, BinSedesTuple> recWriter;
+
+    /**
+     * Simple binary nested reader format
+     */
+    public SequenceFileInterStorage() throws IOException {
+        mLog.debug(useLog);
+    }
+
+    @Override
+    public Tuple getNext() throws IOException {
+        try {
+            if (recReader.nextKeyValue()) {
+                return recReader.getCurrentValue();
+            }
+            else {
+                return null;
+            }
+        } catch (InterruptedException e) {
+            throw new IOException(e);
+        }
+    }
+
+    @Override
+    public void putNext(Tuple t) throws IOException {
+        try {
+            recWriter.write(KEY0, (BinSedesTuple) t);
+        }
+        catch (InterruptedException e) {
+            throw new IOException(e);
+        }
+    }
+
+    @SuppressWarnings("rawtypes")
+    @Override
+    public InputFormat getInputFormat() {
+        return new SequenceFileInputFormat<Object, Tuple>();
+    }
+
+    @Override
+    public void setLocation(String location, Job job) throws IOException {
+        FileInputFormat.setInputPaths(job, location);
+    }
+
+    @Override
+    public ResourceSchema getSchema(String location, Job job)
+                    throws IOException {
+        return Utils.getSchema(this, location, true, job);
+    }
+
+    @SuppressWarnings({ "unchecked", "rawtypes" })
+    @Override
+    public void prepareToRead(RecordReader reader, PigSplit split)
+            throws IOException {
+        recReader = reader;
+    }
+
+    @Override
+    public ResourceStatistics getStatistics(String location, Job job)
+            throws IOException {
+        return null;
+    }
+
+    @Override
+    public String[] getPartitionKeys(String location, Job job)
+            throws IOException {
+        return null;
+    }
+
+    @Override
+    public void setPartitionFilter(Expression partitionFilter)
+            throws IOException {
+        throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public String relToAbsPathForStoreLocation(String location, Path curDir)
+                    throws IOException {
+        return LoadFunc.getAbsolutePath(location, curDir);
+    }
+
+    @SuppressWarnings("rawtypes")
+    @Override
+    public OutputFormat getOutputFormat() throws IOException {
+        return new SequenceFileTupleOutputFormat();
+    }
+
+    public static class SequenceFileTupleOutputFormat extends SequenceFileOutputFormat<NullWritable, BinSedesTuple> {
+        protected SequenceFile.Writer getSequenceWriter(TaskAttemptContext context)
+                throws IOException {
+              Configuration conf = context.getConfiguration();
+
+              CompressionCodec codec = null;
+              CompressionType compressionType = CompressionType.NONE;
+              if (getCompressOutput(context)) {
+                // find the kind of compression to do
+                compressionType = getOutputCompressionType(context);
+                // find the right codec
+                Class<?> codecClass = getOutputCompressorClass(context,
+                                                               DefaultCodec.class);
+                codec = (CompressionCodec)
+                  ReflectionUtils.newInstance(codecClass, conf);
+              }
+              // get the path of the temporary output file
+              Path file = getDefaultWorkFile(context, "");
+              FileSystem fs = file.getFileSystem(conf);
+              return SequenceFile.createWriter(fs, conf, file,
+                       NullWritable.class,
+                       BinSedesTuple.class,
+                       compressionType,
+                       codec,
+                       context);
+            }
+
+            @Override
+            public RecordWriter<NullWritable, BinSedesTuple>
+                   getRecordWriter(TaskAttemptContext context
+                                   ) throws IOException, InterruptedException {
+              final SequenceFile.Writer out = getSequenceWriter(context);
+
+              return new RecordWriter<NullWritable, BinSedesTuple>() {
+
+                  @Override
+                public void write(NullWritable key, BinSedesTuple value)
+                    throws IOException {
+
+                    out.append(key, value);
+                  }
+
+                  @Override
+                public void close(TaskAttemptContext context) throws IOException {
+                    out.close();
+                  }
+                };
+            }
+    }
+
+    @Override
+    public void setStoreLocation(String location, Job job) throws IOException {
+        FileOutputFormat.setOutputPath(job, new Path(location));
+    }
+
+    @Override
+    public void checkSchema(ResourceSchema s) throws IOException {
+
+    }
+
+    @SuppressWarnings({ "unchecked", "rawtypes" })
+    @Override
+    public void prepareToWrite(RecordWriter writer) throws IOException {
+        recWriter = writer;
+    }
+
+    @Override
+    public void setStoreFuncUDFContextSignature(String signature) {
+    }
+
+    @Override
+    public void cleanupOnFailure(String location, Job job) throws IOException {
+        StoreFunc.cleanupOnFailureImpl(location, job);
+
+    }
+
+    @Override
+    public void cleanupOnSuccess(String location, Job job) throws IOException {
+        // DEFAULT : do nothing
+    }
+
+
+}
diff --git a/src/org/apache/pig/impl/io/TFileStorage.java b/src/org/apache/pig/impl/io/TFileStorage.java
index 3bd6c4990..e5e525e4e 100644
--- a/src/org/apache/pig/impl/io/TFileStorage.java
+++ b/src/org/apache/pig/impl/io/TFileStorage.java
@@ -39,6 +39,7 @@ import org.apache.pig.Expression;
 import org.apache.pig.FileInputLoadFunc;
 import org.apache.pig.LoadFunc;
 import org.apache.pig.LoadMetadata;
+import org.apache.pig.PigConfiguration;
 import org.apache.pig.ResourceSchema;
 import org.apache.pig.ResourceStatistics;
 import org.apache.pig.StoreFunc;
@@ -60,6 +61,7 @@ public class TFileStorage extends FileInputLoadFunc implements
                 StoreFuncInterface, LoadMetadata {
 
     private static final Log mLog = LogFactory.getLog(TFileStorage.class);
+    public static final String useLog = "TFile storage in use";
 
     private TFileRecordReader recReader = null;
     private TFileRecordWriter recWriter = null;
@@ -68,7 +70,7 @@ public class TFileStorage extends FileInputLoadFunc implements
      * Simple binary nested reader format
      */
     public TFileStorage() throws IOException {
-        mLog.debug("TFile storage in use");
+        mLog.debug(useLog);
     }
 
     @Override
@@ -138,10 +140,13 @@ public class TFileStorage extends FileInputLoadFunc implements
                         TaskAttemptContext job) throws IOException,
                         InterruptedException {
             Configuration conf = job.getConfiguration();
-            String codec = conf.get("pig.tmpfilecompression.codec", "");
-            if (!codec.equals("lzo") && !codec.equals("gz"))
+            String codec = conf.get(PigConfiguration.PIG_TEMP_FILE_COMPRESSION_CODEC, "");
+            if (!codec.equals("lzo") && !codec.equals("gz") && !codec.equals("gzip"))
                 throw new IOException(
-                                "Invalid temporary file compression codec [" + codec + "]. Expected compression codecs are gz and lzo");
+                                "Invalid temporary file compression codec [" + codec + "]. Expected compression codecs are gz(gzip) and lzo");
+            if (codec.equals("gzip")) {
+                codec = "gz";
+            }
             mLog.info(codec + " compression codec in use");
             Path file = getDefaultWorkFile(job, "");
             return new TFileRecordWriter(file, codec, conf);
diff --git a/src/org/apache/pig/impl/util/Utils.java b/src/org/apache/pig/impl/util/Utils.java
index b5cffcf5c..ca29eca32 100644
--- a/src/org/apache/pig/impl/util/Utils.java
+++ b/src/org/apache/pig/impl/util/Utils.java
@@ -26,6 +26,7 @@ import java.io.SequenceInputStream;
 import java.net.Socket;
 import java.net.SocketException;
 import java.net.SocketImplFactory;
+import java.util.Arrays;
 import java.util.Collection;
 import java.util.Enumeration;
 import java.util.HashMap;
@@ -40,26 +41,32 @@ import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.compress.BZip2Codec;
+import org.apache.hadoop.io.compress.GzipCodec;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapreduce.Job;
 import org.apache.pig.FileInputLoadFunc;
 import org.apache.pig.FuncSpec;
 import org.apache.pig.LoadFunc;
+import org.apache.pig.PigConfiguration;
 import org.apache.pig.PigException;
 import org.apache.pig.ResourceSchema;
 import org.apache.pig.ResourceSchema.ResourceFieldSchema;
 import org.apache.pig.backend.executionengine.ExecException;
+import org.apache.pig.backend.hadoop.datastorage.ConfigurationUtil;
 import org.apache.pig.data.DataType;
 import org.apache.pig.data.Tuple;
 import org.apache.pig.impl.PigContext;
 import org.apache.pig.impl.io.InterStorage;
 import org.apache.pig.impl.io.ReadToEndLoader;
+import org.apache.pig.impl.io.SequenceFileInterStorage;
 import org.apache.pig.impl.io.TFileStorage;
 import org.apache.pig.impl.logicalLayer.schema.Schema;
 import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;
 import org.apache.pig.newplan.logical.relational.LogicalSchema;
 import org.apache.pig.parser.ParserException;
 import org.apache.pig.parser.QueryParserDriver;
+import org.xerial.snappy.SnappyCodec;
 
 import com.google.common.collect.Lists;
 
@@ -139,11 +146,11 @@ public class Utils {
     }
 
     public static String getScriptSchemaKey(String loadFuncSignature) {
-      return loadFuncSignature + ".scriptSchema";
+        return loadFuncSignature + ".scriptSchema";
     }
 
     public static ResourceSchema getSchema(LoadFunc wrappedLoadFunc, String location, boolean checkExistence, Job job)
-    throws IOException {
+            throws IOException {
         Configuration conf = job.getConfiguration();
         if (checkExistence) {
             Path path = new Path(location);
@@ -237,38 +244,160 @@ public class Utils {
         return schema.setFields(fieldSchemasWithSourceTag);
     }
 
+    private static enum TEMPFILE_CODEC {
+        GZ (GzipCodec.class.getName()),
+        GZIP (GzipCodec.class.getName()),
+        LZO ("com.hadoop.compression.lzo.LzoCodec"),
+        SNAPPY (SnappyCodec.class.getName()),
+        BZIP2 (BZip2Codec.class.getName());
+
+        private String hadoopCodecClassName;
+
+        TEMPFILE_CODEC(String codecClassName) {
+            this.hadoopCodecClassName = codecClassName;
+        }
+
+        public String lowerName() {
+            return this.name().toLowerCase();
+        }
+
+        public String getHadoopCodecClassName() {
+            return this.hadoopCodecClassName;
+        }
+    }
+
+    private static enum TEMPFILE_STORAGE {
+        INTER(InterStorage.class,
+                null),
+        TFILE(TFileStorage.class,
+                Arrays.asList(TEMPFILE_CODEC.GZ,
+                        TEMPFILE_CODEC.GZIP,
+                        TEMPFILE_CODEC.LZO)),
+        SEQFILE(SequenceFileInterStorage.class,
+                Arrays.asList(TEMPFILE_CODEC.GZ,
+                        TEMPFILE_CODEC.GZIP,
+                        TEMPFILE_CODEC.LZO,
+                        TEMPFILE_CODEC.SNAPPY,
+                        TEMPFILE_CODEC.BZIP2));
+
+        private Class<? extends FileInputLoadFunc> storageClass;
+        private List<TEMPFILE_CODEC> supportedCodecs;
+
+        TEMPFILE_STORAGE(
+                Class<? extends FileInputLoadFunc> storageClass,
+                List<TEMPFILE_CODEC> supportedCodecs) {
+            this.storageClass = storageClass;
+            this.supportedCodecs = supportedCodecs;
+        }
+
+        public String lowerName() {
+            return this.name().toLowerCase();
+        }
+
+        public Class<? extends FileInputLoadFunc> getStorageClass() {
+            return storageClass;
+        }
+
+        public boolean ensureCodecSupported(String codec) {
+            try {
+                return this.supportedCodecs.contains(TEMPFILE_CODEC.valueOf(codec.toUpperCase()));
+            } catch (IllegalArgumentException e) {
+                return false;
+            }
+        }
+        
+        public String supportedCodecsToString() {
+            StringBuffer sb = new StringBuffer();
+            boolean first = true;
+            for (TEMPFILE_CODEC codec : supportedCodecs) {
+                if(first) {
+                    first = false;
+                } else {
+                    sb.append(",");    
+                }
+                sb.append(codec.name());
+            }
+            return sb.toString();
+        }
+    }
+
     public static String getTmpFileCompressorName(PigContext pigContext) {
         if (pigContext == null)
             return InterStorage.class.getName();
-        boolean tmpFileCompression = pigContext.getProperties().getProperty("pig.tmpfilecompression", "false").equals("true");
-        String codec = pigContext.getProperties().getProperty("pig.tmpfilecompression.codec", "");
-        if (tmpFileCompression) {
-            if (codec.equals("lzo"))
-                pigContext.getProperties().setProperty("io.compression.codec.lzo.class", "com.hadoop.compression.lzo.LzoCodec");
-            return TFileStorage.class.getName();
-        } else
-            return InterStorage.class.getName();
+
+        String codec = pigContext.getProperties().getProperty(PigConfiguration.PIG_TEMP_FILE_COMPRESSION_CODEC, "");
+        if (codec.equals(TEMPFILE_CODEC.LZO.lowerName())) {
+            pigContext.getProperties().setProperty("io.compression.codec.lzo.class", "com.hadoop.compression.lzo.LzoCodec");
+        }
+
+        return getTmpFileStorage(pigContext.getProperties()).getStorageClass().getName();
     }
 
     public static FileInputLoadFunc getTmpFileStorageObject(Configuration conf) throws IOException {
-        boolean tmpFileCompression = conf.getBoolean("pig.tmpfilecompression", false);
-        return tmpFileCompression ? new TFileStorage() : new InterStorage();
+        Class<? extends FileInputLoadFunc> storageClass = getTmpFileStorage(ConfigurationUtil.toProperties(conf)).getStorageClass();
+        try {
+            return storageClass.newInstance();
+        } catch (InstantiationException e) {
+            throw new IOException(e);
+        } catch (IllegalAccessException e) {
+            throw new IOException(e);
+        }
     }
 
-    public static boolean tmpFileCompression(PigContext pigContext) {
-        if (pigContext == null)
-            return false;
-        return pigContext.getProperties().getProperty("pig.tmpfilecompression", "false").equals("true");
+    private static TEMPFILE_STORAGE getTmpFileStorage(Properties properties) {
+        boolean tmpFileCompression = properties.getProperty(
+                PigConfiguration.PIG_ENABLE_TEMP_FILE_COMPRESSION, "false").equals("true");
+        String tmpFileCompressionStorage =
+                properties.getProperty(PigConfiguration.PIG_TEMP_FILE_COMPRESSION_STORAGE,
+                        TEMPFILE_STORAGE.TFILE.lowerName());
+
+        if (!tmpFileCompression) {
+            return TEMPFILE_STORAGE.INTER;
+        } else if (TEMPFILE_STORAGE.SEQFILE.lowerName().equals(tmpFileCompressionStorage)) {
+            return TEMPFILE_STORAGE.SEQFILE;
+        } else if (TEMPFILE_STORAGE.TFILE.lowerName().equals(tmpFileCompressionStorage)) {
+            return TEMPFILE_STORAGE.TFILE;
+        } else {
+            throw new IllegalArgumentException("Unsupported storage format " + tmpFileCompressionStorage + 
+                    ". Should be one of " + Arrays.toString(TEMPFILE_STORAGE.values()));
+        }
     }
 
-    public static String tmpFileCompressionCodec(PigContext pigContext) throws IOException {
-        if (pigContext == null)
-            return "";
-        String codec = pigContext.getProperties().getProperty("pig.tmpfilecompression.codec", "");
-        if (codec.equals("gz") || codec.equals("lzo"))
-            return codec;
-        else
-            throw new IOException("Invalid temporary file compression codec ["+codec+"]. Expected compression codecs are gz and lzo");
+    public static void setTmpFileCompressionOnConf(PigContext pigContext, Configuration conf) throws IOException{
+        if (pigContext == null) {
+            return;
+        }
+        TEMPFILE_STORAGE storage = getTmpFileStorage(pigContext.getProperties());
+        String codec = pigContext.getProperties().getProperty(
+                PigConfiguration.PIG_TEMP_FILE_COMPRESSION_CODEC, "");
+        switch (storage) {
+        case INTER:
+            break;
+        case SEQFILE:
+            conf.setBoolean("mapred.output.compress", true);
+            conf.set(PigConfiguration.PIG_TEMP_FILE_COMPRESSION_STORAGE, "seqfile");
+            if("".equals(codec)) {
+                // codec is not specified, ensure  is set
+                log.warn("Temporary file compression codec is not specified. Using mapred.output.compression.codec property.");
+                if(conf.get("mapred.output.compression.codec") == null) {
+                    throw new IOException("mapred.output.compression.codec is not set");
+                }
+            } else if(storage.ensureCodecSupported(codec)) {
+                conf.set("mapred.output.compression.codec", TEMPFILE_CODEC.valueOf(codec.toUpperCase()).getHadoopCodecClassName());
+            } else {
+                throw new IOException("Invalid temporary file compression codec [" + codec + "]. " +
+                        "Expected compression codecs for " + storage.getStorageClass().getName() + " are " + storage.supportedCodecsToString() + ".");
+            }
+            break;
+        case TFILE:
+            if(storage.ensureCodecSupported(codec)) {
+                conf.set(PigConfiguration.PIG_TEMP_FILE_COMPRESSION_CODEC, codec.toLowerCase());
+            } else {
+                throw new IOException("Invalid temporary file compression codec [" + codec + "]. " +
+                        "Expected compression codecs for " + storage.getStorageClass().getName() + " are " + storage.supportedCodecsToString() + ".");
+            }
+            break;
+        }
     }
 
     public static String getStringFromArray(String[] arr) {
@@ -298,7 +427,7 @@ public class Utils {
     public static String slashisize(String str) {
         return str.replace("\\\\", "\\");
     }
-    
+
     @SuppressWarnings("unchecked")
     public static <O> Collection<O> mergeCollection(Collection<O> a, Collection<O> b) {
         if (a==null && b==null)
@@ -326,16 +455,16 @@ public class Utils {
                 }
             }
         }
-        
+
         return result;
     }
-    
-   public static InputStream getCompositeStream(InputStream in, Properties properties) {
-       //Load default ~/.pigbootup if not specified by user
+
+    public static InputStream getCompositeStream(InputStream in, Properties properties) {
+        //Load default ~/.pigbootup if not specified by user
         final String bootupFile = properties.getProperty("pig.load.default.statements", System.getProperty("user.home") + "/.pigbootup");
         try {
-        final InputStream inputSteam = new FileInputStream(new File(bootupFile));
-        return new SequenceInputStream(inputSteam, in);
+            final InputStream inputSteam = new FileInputStream(new File(bootupFile));
+            return new SequenceInputStream(inputSteam, in);
         } catch(FileNotFoundException fe) {
             log.info("Default bootup file " +bootupFile+ " not found");
             return in;
@@ -373,7 +502,7 @@ public class Utils {
         try {
             Class clazz = Class.forName("org.apache.pig.shock.SSHSocketImplFactory");
             SocketImplFactory f = (SocketImplFactory) clazz.getMethod(
-                        "getFactory", new Class[0]).invoke(0, new Object[0]);
+                    "getFactory", new Class[0]).invoke(0, new Object[0]);
             Socket.setSocketImplFactory(f);
         } catch (SocketException e) {
         } catch (Exception e) {
diff --git a/test/org/apache/pig/test/TestTmpFileCompression.java b/test/org/apache/pig/test/TestTmpFileCompression.java
index 24a766a46..9c58f2c9d 100644
--- a/test/org/apache/pig/test/TestTmpFileCompression.java
+++ b/test/org/apache/pig/test/TestTmpFileCompression.java
@@ -17,36 +17,38 @@
  */
 package org.apache.pig.test;
 
-import static org.junit.Assert.*;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
 
+import java.io.BufferedReader;
 import java.io.File;
+import java.io.FileReader;
 import java.io.FileWriter;
 import java.io.IOException;
 import java.io.PrintWriter;
 import java.util.HashMap;
 import java.util.Iterator;
-import java.util.Properties;
-
-import java.io.BufferedReader;
-import java.io.FileReader;
-
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.mapreduce.TaskAttemptContext;
-import org.apache.hadoop.mapreduce.TaskAttemptID;
 import org.apache.hadoop.mapreduce.lib.input.FileSplit;
+import org.apache.log4j.FileAppender;
+import org.apache.log4j.Level;
+import org.apache.log4j.Logger;
+import org.apache.log4j.SimpleLayout;
 import org.apache.pig.ExecType;
+import org.apache.pig.PigConfiguration;
 import org.apache.pig.PigRunner;
 import org.apache.pig.PigServer;
 import org.apache.pig.backend.hadoop.executionengine.shims.HadoopShims;
-import org.apache.pig.data.DataBag;
 import org.apache.pig.data.Tuple;
 import org.apache.pig.data.TupleFactory;
+import org.apache.pig.impl.io.InterStorage;
+import org.apache.pig.impl.io.SequenceFileInterStorage;
 import org.apache.pig.impl.io.TFileRecordReader;
 import org.apache.pig.impl.io.TFileRecordWriter;
 import org.apache.pig.impl.io.TFileStorage;
-import org.apache.pig.impl.io.InterStorage;
 import org.apache.pig.tools.pigstats.OutputStats;
 import org.apache.pig.tools.pigstats.PigStats;
 import org.junit.After;
@@ -54,11 +56,6 @@ import org.junit.AfterClass;
 import org.junit.Before;
 import org.junit.Test;
 
-import org.apache.log4j.FileAppender;
-import org.apache.log4j.Level;
-import org.apache.log4j.Logger;
-import org.apache.log4j.SimpleLayout;
-
 public class TestTmpFileCompression {
     private PigServer pigServer;
     static MiniCluster cluster = MiniCluster.buildCluster();
@@ -190,13 +187,34 @@ public class TestTmpFileCompression {
         Util.deleteFile(cluster, input1);
         Util.deleteFile(cluster, input2);
         assertTrue(checkLogFileMessage(new String[] {
-            "Pig Internal storage in use"
+            InterStorage.useLog
         }));
     }
 
     @Test
-    public void testImplicitSplit() throws Exception {
+    public void testImplicitSplitTFile() throws Exception {
         resetLog(TFileStorage.class);
+        pigServer.getPigContext().getProperties().setProperty(
+                        PigConfiguration.PIG_ENABLE_TEMP_FILE_COMPRESSION, "true");
+        pigServer.getPigContext().getProperties().setProperty(
+                        PigConfiguration.PIG_TEMP_FILE_COMPRESSION_CODEC, "gz");
+        testImplicitSplit(TFileStorage.useLog);
+    }
+
+    @Test
+    public void testImplicitSplitSeqFile() throws Exception {
+        resetLog(SequenceFileInterStorage.class);
+        pigServer.getPigContext().getProperties().setProperty(
+                        PigConfiguration.PIG_ENABLE_TEMP_FILE_COMPRESSION, "true");
+        pigServer.getPigContext().getProperties().setProperty(
+                PigConfiguration.PIG_TEMP_FILE_COMPRESSION_STORAGE, "seqfile");
+        // bzip2 does not need native-hadoop
+        pigServer.getPigContext().getProperties().setProperty(
+                PigConfiguration.PIG_TEMP_FILE_COMPRESSION_CODEC, "bzip2");
+           testImplicitSplit(SequenceFileInterStorage.useLog);
+    }
+
+    private void testImplicitSplit(String assertLog) throws Exception {
         int LOOP_SIZE = 20;
         String[] input = new String[LOOP_SIZE];
         for (int i = 1; i <= LOOP_SIZE; i++) {
@@ -204,10 +222,6 @@ public class TestTmpFileCompression {
         }
         String inputFileName = "testImplicitSplit-input.txt";
         Util.createInputFile(cluster, inputFileName, input);
-        pigServer.getPigContext().getProperties().setProperty(
-                        "pig.tmpfilecompression", "true");
-        pigServer.getPigContext().getProperties().setProperty(
-                        "pig.tmpfilecompression.codec", "gz");
         pigServer.registerQuery("A = LOAD '" + inputFileName + "';");
         pigServer.registerQuery("B = filter A by $0<=10;");
         pigServer.registerQuery("C = filter A by $0>10;");
@@ -222,15 +236,36 @@ public class TestTmpFileCompression {
         assertEquals(20, cnt);
         Util.deleteFile(cluster, inputFileName);
         assertTrue(checkLogFileMessage(new String[] {
-            "TFile storage in use"
+            assertLog
         }));
     }
 
     @Test
-    public void testImplicitSplitInCoGroup() throws Exception {
+    public void testImplicitSplitInCoGroupTFile() throws Exception {
+        resetLog(TFileStorage.class);
+        pigServer.getPigContext().getProperties().setProperty(
+                        PigConfiguration.PIG_ENABLE_TEMP_FILE_COMPRESSION, "true");
+        pigServer.getPigContext().getProperties().setProperty(
+                        PigConfiguration.PIG_TEMP_FILE_COMPRESSION_CODEC, "gz");
+        testImplicitSplitInCoGroup(TFileStorage.useLog);
+    }
+
+    @Test
+    public void testImplicitSplitInCoGroupSeqFile() throws Exception {
+        resetLog(SequenceFileInterStorage.class);
+        pigServer.getPigContext().getProperties().setProperty(
+                        PigConfiguration.PIG_ENABLE_TEMP_FILE_COMPRESSION, "true");
+        pigServer.getPigContext().getProperties().setProperty(
+                PigConfiguration.PIG_TEMP_FILE_COMPRESSION_STORAGE, "seqfile");
+        pigServer.getPigContext().getProperties().setProperty(
+                PigConfiguration.PIG_TEMP_FILE_COMPRESSION_CODEC, "bzip2");
+        testImplicitSplitInCoGroup(SequenceFileInterStorage.useLog);
+    }
+
+    private void testImplicitSplitInCoGroup(String assertLog) throws Exception {
         // this query is similar to the one reported in JIRA - PIG-537
         // Create input file
-        resetLog(TFileStorage.class);
+
         String input1 = "testImplicitSplitInCoGroup-input1.txt";
         String input2 = "testImplicitSplitInCoGroup-input2.txt";
         Util.createInputFile(cluster, input1, new String[] {
@@ -239,10 +274,7 @@ public class TestTmpFileCompression {
         Util.createInputFile(cluster, input2, new String[] {
                         "a:first", "b:second", "c:third"
         });
-        pigServer.getPigContext().getProperties().setProperty(
-                        "pig.tmpfilecompression", "true");
-        pigServer.getPigContext().getProperties().setProperty(
-                        "pig.tmpfilecompression.codec", "gz");
+
         pigServer.registerQuery("a = load '" + input1 + "' using PigStorage(':') as (name:chararray, marks:int);");
         pigServer.registerQuery("b = load '" + input2 + "' using PigStorage(':') as (name:chararray, rank:chararray);");
         pigServer.registerQuery("c = cogroup a by name, b by name;");
@@ -279,11 +311,11 @@ public class TestTmpFileCompression {
         Util.deleteFile(cluster, input1);
         Util.deleteFile(cluster, input2);
         assertTrue(checkLogFileMessage(new String[] {
-            "TFile storage in use"
+            assertLog
         }));
     }
 
-    
+
     // PIG-1977
     @Test
     public void testTFileRecordReader() throws Exception {
