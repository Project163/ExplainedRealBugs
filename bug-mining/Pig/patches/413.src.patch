diff --git a/CHANGES.txt b/CHANGES.txt
index d563dd515..c402f1b0c 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -33,13 +33,15 @@ subsequent calls to LOLoad.getSchema() or LOLoad.determineSchema()
 
 OPTIMIZATIONS
 
+PIG-1309: Map-side Cogroup (ashutoshc)
+
 BUG FIXES
 
 Release 0.7.0 - Unreleased
 
 INCOMPATIBLE CHANGES
 
-PIG-1292: Interface Refinements (hashutosh)
+PIG-1292: Interface Refinements (ashutoshc)
 
 PIG-1259: ResourceFieldSchema.setSchema should not allow a bag field without a
 Tuple as its only sub field (the tuple itself can have a schema with > 1
@@ -759,7 +761,7 @@ in the split plans have different key types (tuple and non tuple key type)
 PIG-839: incorrect return codes on failure when using -f or -e flags (hagleitn
 via sms)
 
-PIG-796: support conversion from numeric types to chararray (Ashutosh Chauhan
+PIG-796: support conversion from numeric types to chararray (ashutoshc
 via pradeepkth)
 
 PIG-564: problem with parameter substitution and special charachters (olgan)
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java
index bcd6e1e0b..7769c5ac8 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/JobControlCompiler.java
@@ -59,6 +59,7 @@ import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlan
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFRJoin;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeCogroup;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
@@ -808,7 +809,7 @@ public class JobControlCompiler{
         // global sort set (because in that case it's the sampling job) or if
         // it's a limit after a sort. 
         boolean hasOrderBy = false;
-        if (mro.isGlobalSort() || mro.isLimitAfterSort()) {
+        if (mro.isGlobalSort() || mro.isLimitAfterSort() || mro.usingTypedComparator()) {
             hasOrderBy = true;
         } else {
             List<MapReduceOper> succs = plan.getSuccessors(mro);
@@ -1102,6 +1103,29 @@ public class JobControlCompiler{
                 throw new VisitorException(msg, e);
             }
          }
+         
+         @Override
+        public void visitMergeCoGroup(POMergeCogroup mergeCoGrp)
+                throws VisitorException {
+          
+             // XXX Hadoop currently doesn't support distributed cache in local mode.
+             // This line will be removed after the support is added
+             if (pigContext.getExecType() == ExecType.LOCAL) return;
+             
+             String indexFile = mergeCoGrp.getIndexFileName();
+             
+             if (indexFile == null) throw new VisitorException("No index file");
+             
+             try {
+                String symlink = addSingleFileToDistributedCache(pigContext,
+                        conf, indexFile, "indexfile_mergecogrp_");
+                mergeCoGrp.setIndexFileName(symlink);
+            } catch (IOException e) {
+                String msg = "Internal error. Distributed cache could not " +
+                        "be set up for merge cogrp index file";
+                throw new VisitorException(msg, e);
+            }
+        }
      }
     
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRCompiler.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRCompiler.java
index 4e01e95a8..f24068c2a 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRCompiler.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MRCompiler.java
@@ -61,6 +61,7 @@ import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOpe
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLimit;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeCogroup;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackageLite;
@@ -766,7 +767,7 @@ public class MRCompiler extends PhyPlanVisitor {
         }
     }
     
-    public void connectMapToReduceLimitedSort(MapReduceOper mro, MapReduceOper sortMROp) throws PlanException, VisitorException
+    private void connectMapToReduceLimitedSort(MapReduceOper mro, MapReduceOper sortMROp) throws PlanException, VisitorException
     {
         POLocalRearrange slr = (POLocalRearrange)sortMROp.mapPlan.getLeaves().get(0);
         
@@ -795,7 +796,7 @@ public class MRCompiler extends PhyPlanVisitor {
         mro.reducePlan.addAsLeaf(getPlainForEachOP());
     }
     
-    public void simpleConnectMapToReduce(MapReduceOper mro) throws PlanException
+    private void simpleConnectMapToReduce(MapReduceOper mro) throws PlanException
     {
         PhysicalPlan ep = new PhysicalPlan();
         POProject prjStar = new POProject(new OperatorKey(scope,nig.getNextNodeId(scope)));
@@ -830,7 +831,7 @@ public class MRCompiler extends PhyPlanVisitor {
         mro.reducePlan.addAsLeaf(getPlainForEachOP());
     }
     
-    public POForEach getPlainForEachOP()
+    private POForEach getPlainForEachOP()
     {
         List<PhysicalPlan> eps1 = new ArrayList<PhysicalPlan>();
         List<Boolean> flat1 = new ArrayList<Boolean>();
@@ -925,11 +926,12 @@ public class MRCompiler extends PhyPlanVisitor {
             }
             
             POLoad loader = (POLoad)phyOp;
-            LoadFunc loadFunc = (LoadFunc) PigContext.instantiateFuncFromSpec(loader.getLFile().getFuncSpec());
+            Object loadFunc = PigContext.instantiateFuncFromSpec(loader.getLFile().getFuncSpec());
             try {
-                if(!(loadFunc instanceof CollectableLoadFunc)){
+                if(!(CollectableLoadFunc.class.isAssignableFrom(loadFunc.getClass()))){
                     throw new MRCompilerException("While using 'collected' on group; data must be loaded via loader implementing CollectableLoadFunc.");
                 }
+                ((LoadFunc)loadFunc).setUDFContextSignature(loader.getSignature());
                 ((CollectableLoadFunc)loadFunc).ensureAllKeyInstancesInSameSplit();
             } catch (MRCompilerException e){
                 throw (e);
@@ -1081,6 +1083,184 @@ public class MRCompiler extends PhyPlanVisitor {
         }
     }
 
+    /** Leftmost relation is referred as base relation (this is the one fed into mappers.) 
+     *  First, close all MROpers except for first one (referred as baseMROPer)
+     *  Then, create a MROper which will do indexing job (idxMROper)
+     *  Connect idxMROper before the mappedMROper in the MRPlan.
+     */
+
+    @Override
+    public void visitMergeCoGroup(POMergeCogroup poCoGrp) throws VisitorException {
+
+        if(compiledInputs.length < 2){
+            String errMsg = "Merge Cogroup work on two or more relations." +
+            		"To use map-side group-by on single relation, use 'collected' qualifier.";
+            throw new MRCompilerException(errMsg);
+        }
+            
+        List<FuncSpec> funcSpecs = new ArrayList<FuncSpec>(compiledInputs.length-1);
+        List<String> fileSpecs = new ArrayList<String>(compiledInputs.length-1);
+        List<String> loaderSigns = new ArrayList<String>(compiledInputs.length-1);
+        
+        try{
+            // Iterate through all the MROpers, disconnect side MROPers from 
+            // MROPerPlan and collect all the information needed in different lists.
+            
+            for(int i=0 ; i < compiledInputs.length; i++){
+                
+                MapReduceOper mrOper = compiledInputs[i];
+                PhysicalPlan mapPlan = mrOper.mapPlan;
+                if(mapPlan.getRoots().size() != 1){
+                    int errCode = 2171;
+                    String errMsg = "Expected one but found more then one root physical operator in physical plan.";
+                    throw new MRCompilerException(errMsg,errCode,PigException.BUG);
+                }
+
+                PhysicalOperator rootPOOp = mapPlan.getRoots().get(0);
+                if(! (rootPOOp instanceof POLoad)){
+                    int errCode = 2172;
+                    String errMsg = "Expected physical operator at root to be POLoad. Found : "+rootPOOp.getClass().getCanonicalName();
+                    throw new MRCompilerException(errMsg,errCode);
+                }
+                
+                POLoad sideLoader = (POLoad)rootPOOp;
+                FileSpec loadFileSpec = sideLoader.getLFile();
+                FuncSpec funcSpec = loadFileSpec.getFuncSpec();
+                Object loadfunc = PigContext.instantiateFuncFromSpec(funcSpec);
+                if(i == 0){
+                    
+                    if(!(CollectableLoadFunc.class.isAssignableFrom(loadfunc.getClass())))
+                        throw new MRCompilerException("Base loader in Cogroup must implement CollectableLoadFunc.");
+                    
+                    ((LoadFunc)loadfunc).setUDFContextSignature(sideLoader.getSignature());
+                    ((CollectableLoadFunc)loadfunc).ensureAllKeyInstancesInSameSplit();
+                    continue;
+                }
+                if(!(IndexableLoadFunc.class.isAssignableFrom(loadfunc.getClass())))
+                    throw new MRCompilerException("Side loaders in cogroup must implement IndexableLoadFunc.");
+                
+                funcSpecs.add(funcSpec);
+                fileSpecs.add(loadFileSpec.getFileName());
+                loaderSigns.add(sideLoader.getSignature());
+                MRPlan.remove(mrOper);
+            }
+            
+            poCoGrp.setSideLoadFuncs(funcSpecs);
+            poCoGrp.setSideFileSpecs(fileSpecs);
+            poCoGrp.setLoaderSignatures(loaderSigns);
+            
+            // Use map-reduce operator of base relation for the cogroup operation.
+            MapReduceOper baseMROp = phyToMROpMap.get(poCoGrp.getInputs().get(0));
+            if(baseMROp.mapDone || !baseMROp.reducePlan.isEmpty())
+                throw new MRCompilerException("Currently merged cogroup is not supported after blocking operators.");
+            
+            // Create new map-reduce operator for indexing job and then configure it.
+            MapReduceOper indexerMROp = getMROp();
+            FileSpec idxFileSpec = getIndexingJob(indexerMROp, baseMROp, poCoGrp.getLRInnerPlansOf(0));
+            poCoGrp.setIdxFuncSpec(idxFileSpec.getFuncSpec());
+            poCoGrp.setIndexFileName(idxFileSpec.getFileName());
+            
+            baseMROp.mapPlan.addAsLeaf(poCoGrp);
+            MRPlan.add(indexerMROp);
+            MRPlan.connect(indexerMROp, baseMROp);
+
+            phyToMROpMap.put(poCoGrp,baseMROp);
+            // Going forward, new operators should be added in baseMRop. To make
+            // sure, reset curMROp.
+            curMROp = baseMROp;
+        }
+        catch (ExecException e){
+           throw new MRCompilerException(e.getDetailedMessage(),e.getErrorCode(),e.getErrorSource(),e);
+        }
+        catch (MRCompilerException mrce){
+            throw(mrce);
+        }
+        catch (CloneNotSupportedException e) {
+            throw new MRCompilerException(e);
+        }
+        catch(PlanException e){
+            int errCode = 2034;
+            String msg = "Error compiling operator " + poCoGrp.getClass().getCanonicalName();
+            throw new MRCompilerException(msg, errCode, PigException.BUG, e);
+        }
+        catch (IOException e){
+            int errCode = 3000;
+            String errMsg = "IOException caught while compiling POMergeCoGroup";
+            throw new MRCompilerException(errMsg, errCode,e);
+        }
+    }
+    
+    // Sets up the indexing job for map-side cogroups.
+    private FileSpec getIndexingJob(MapReduceOper indexerMROp, 
+            final MapReduceOper baseMROp, final List<PhysicalPlan> mapperLRInnerPlans)
+        throws MRCompilerException, PlanException, ExecException, IOException, CloneNotSupportedException {
+        
+        // First replace loader with  MergeJoinIndexer.
+        PhysicalPlan baseMapPlan = baseMROp.mapPlan;
+        POLoad baseLoader = (POLoad)baseMapPlan.getRoots().get(0);                            
+        FileSpec origLoaderFileSpec = baseLoader.getLFile();
+        FuncSpec funcSpec = origLoaderFileSpec.getFuncSpec();
+        Object loadFunc = PigContext.instantiateFuncFromSpec(funcSpec);
+        
+        if (! (OrderedLoadFunc.class.isAssignableFrom(loadFunc.getClass()))){
+            int errCode = 1104;
+            String errMsg = "Base relation of merge-coGroup must implement " +
+            "OrderedLoadFunc interface. The specified loader " 
+            + funcSpec + " doesn't implement it";
+            throw new MRCompilerException(errMsg,errCode);
+        }
+        
+        String[] indexerArgs = new String[6];
+        indexerArgs[0] = funcSpec.toString();
+        indexerArgs[1] = ObjectSerializer.serialize((Serializable)mapperLRInnerPlans);
+        indexerArgs[3] = baseLoader.getSignature();
+        indexerArgs[4] = baseLoader.getOperatorKey().scope;
+        indexerArgs[5] = Boolean.toString(false); // we care for nulls. 
+            
+        PhysicalPlan phyPlan;
+        if (baseMapPlan.getSuccessors(baseLoader) == null 
+                || baseMapPlan.getSuccessors(baseLoader).isEmpty()){
+         // Load-Load-Cogroup case.
+            phyPlan = null; 
+        }
+            
+        else{ // We got something. Yank it and set it as inner plan.
+            phyPlan = baseMapPlan.clone();
+            PhysicalOperator root = phyPlan.getRoots().get(0);
+            phyPlan.disconnect(root, phyPlan.getSuccessors(root).get(0));
+            phyPlan.remove(root);
+
+        }
+        indexerArgs[2] = ObjectSerializer.serialize(phyPlan);
+
+        POLoad idxJobLoader = getLoad();
+        idxJobLoader.setLFile(new FileSpec(origLoaderFileSpec.getFileName(),
+                new FuncSpec(MergeJoinIndexer.class.getName(), indexerArgs)));
+        indexerMROp.mapPlan.add(idxJobLoader);
+        
+        // Loader of mro will return a tuple of form - 
+        // (key1, key2, .. , WritableComparable, splitIndex). See MergeJoinIndexer for details.
+        
+        // After getting an index entry in each mapper, send all of them to one 
+        // reducer where they will be sorted on the way by Hadoop.
+        simpleConnectMapToReduce(indexerMROp);
+        
+        indexerMROp.requestedParallelism = 1; // we need exactly one reducer for indexing job.
+        
+        // We want to use typed tuple comparator for this job, instead of default 
+        // raw binary comparator used by Pig, to make sure index entries are 
+        // sorted correctly by Hadoop.
+        indexerMROp.useTypedComparator(true); 
+
+        POStore st = getStore();
+        FileSpec strFile = getTempFileSpec();
+        st.setSFile(strFile);
+        indexerMROp.reducePlan.addAsLeaf(st);
+        indexerMROp.setReduceDone(true);
+
+        return strFile;
+    }
+    
     /** Since merge-join works on two inputs there are exactly two MROper predecessors identified  as left and right.
      *  Instead of merging two operators, both are used to generate a MR job each. First MR oper is run to generate on-the-fly index on right side.
      *  Second is used to actually do the join. First MR oper is identified as rightMROper and second as curMROper.
@@ -1193,7 +1373,7 @@ public class MRCompiler extends PhyPlanVisitor {
             }
             
             joinOp.setupRightPipeline(rightPipelinePlan);
-	    rightMROpr.requestedParallelism = 1; // we need exactly one reducer for indexing job.        
+            rightMROpr.requestedParallelism = 1; // we need exactly one reducer for indexing job.        
             
             // At this point, we must be operating on map plan of right input and it would contain nothing else other then a POLoad.
             POLoad rightLoader = (POLoad)rightMROpr.mapPlan.getRoots().get(0);            
@@ -1235,8 +1415,9 @@ public class MRCompiler extends PhyPlanVisitor {
                     }
                 }
             } else {
+                
                 // Replace POLoad with  indexer.
-                String[] indexerArgs = new String[3];
+                String[] indexerArgs = new String[6];
                 FileSpec origRightLoaderFileSpec = rightLoader.getLFile();
                 indexerArgs[0] = origRightLoaderFileSpec.getFuncSpec().toString();
                 if (! (PigContext.instantiateFuncFromSpec(indexerArgs[0]) instanceof OrderedLoadFunc)){
@@ -1249,73 +1430,19 @@ public class MRCompiler extends PhyPlanVisitor {
                 List<PhysicalPlan> rightInpPlans = joinOp.getInnerPlansOf(1);
                 indexerArgs[1] = ObjectSerializer.serialize((Serializable)rightInpPlans);
                 indexerArgs[2] = ObjectSerializer.serialize(rightPipelinePlan);
+                indexerArgs[3] = rightLoader.getSignature();
+                indexerArgs[4] = rightLoader.getOperatorKey().scope;
+                indexerArgs[5] = Boolean.toString(true);
+                
                 FileSpec lFile = new FileSpec(rightLoader.getLFile().getFileName(),new FuncSpec(MergeJoinIndexer.class.getName(), indexerArgs));
                 rightLoader.setLFile(lFile);
     
                 // Loader of mro will return a tuple of form - 
                 // (keyFirst1, keyFirst2, .. , position, splitIndex) See MergeJoinIndexer
-                // Now set up a POLocalRearrange which has "all" as the key and tuple fetched
-                // by loader as the "value" of POLocalRearrange
-                // Sorting of index can possibly be achieved by using Hadoop sorting 
-                // between map and reduce instead of Pig doing sort. If that is so, 
-                // it will simplify lot of the code below.
+
+                simpleConnectMapToReduce(rightMROpr);
+                rightMROpr.useTypedComparator(true);
                 
-                PhysicalPlan lrPP = new PhysicalPlan();
-                ConstantExpression ce = new ConstantExpression(new OperatorKey(scope,nig.getNextNodeId(scope)));
-                ce.setValue("all");
-                ce.setResultType(DataType.CHARARRAY);
-                lrPP.add(ce);
-    
-                List<PhysicalPlan> lrInnerPlans = new ArrayList<PhysicalPlan>();
-                lrInnerPlans.add(lrPP);
-    
-                POLocalRearrange lr = new POLocalRearrange(new OperatorKey(scope,nig.getNextNodeId(scope)));
-                lr.setIndex(0);
-                lr.setKeyType(DataType.CHARARRAY);
-                lr.setPlans(lrInnerPlans);
-                lr.setResultType(DataType.TUPLE);
-                rightMROpr.mapPlan.addAsLeaf(lr);
-    
-                rightMROpr.setMapDone(true);
-    
-                // On the reduce side of this indexing job, there will be a global rearrange followed by POSort.
-                // Output of POSort will be index file dumped on the DFS.
-    
-                // First add POPackage.
-                POPackage pkg = new POPackage(new OperatorKey(scope,nig.getNextNodeId(scope)));
-                pkg.setKeyType(DataType.CHARARRAY);
-                pkg.setNumInps(1); 
-                pkg.setInner(new boolean[]{false});
-                rightMROpr.reducePlan.add(pkg);
-    
-                // Next project tuples from the bag created by POPackage.
-                POProject topPrj = new POProject(new OperatorKey(scope,nig.getNextNodeId(scope)));
-                topPrj.setColumn(1);
-                topPrj.setResultType(DataType.TUPLE);
-                topPrj.setOverloaded(true);
-                rightMROpr.reducePlan.add(topPrj);
-                rightMROpr.reducePlan.connect(pkg, topPrj);
-    
-                // Now create and add POSort. Sort plan is project *.
-                List<PhysicalPlan> sortPlans = new ArrayList<PhysicalPlan>(1);
-                PhysicalPlan innerSortPlan = new PhysicalPlan();
-                POProject prj = new POProject(new OperatorKey(scope,nig.getNextNodeId(scope)));
-                prj.setStar(true);
-                prj.setOverloaded(false);
-                prj.setResultType(DataType.TUPLE);
-                innerSortPlan.add(prj);
-                sortPlans.add(innerSortPlan);
-    
-                // Currently we assume all columns are in asc order.
-                // Add two because filename and offset are added by Indexer in addition to keys.
-                List<Boolean>  mAscCols = new ArrayList<Boolean>(rightInpPlans.size()+2);
-                for(int i=0; i< rightInpPlans.size()+2; i++)
-                    mAscCols.add(true);
-    
-                POSort sortOp = new POSort(new OperatorKey(scope,nig.getNextNodeId(scope)),1, null, sortPlans, mAscCols, null);
-                rightMROpr.reducePlan.add(sortOp);
-                rightMROpr.reducePlan.connect(topPrj, sortOp);
-    
                 POStore st = getStore();
                 FileSpec strFile = getTempFileSpec();
                 st.setSFile(strFile);
@@ -1333,12 +1460,8 @@ public class MRCompiler extends PhyPlanVisitor {
                 joinOp.setRightInputFileName(origRightLoaderFileSpec.getFileName());  
                 
                 joinOp.setIndexFile(strFile.getFileName());
-                 
             }
             
-   
-//            joinOp.setIndexFile(strFile);
-            
             // We are done with right side. Lets work on left now.
             // Join will be materialized in leftMROper.
             if(!curMROp.mapDone) // Life is easy 
@@ -1656,7 +1779,7 @@ public class MRCompiler extends PhyPlanVisitor {
         throw new PlanException(msg, errCode, PigException.BUG);
     }
     
-    public MapReduceOper getSortJob(
+    private MapReduceOper getSortJob(
             POSort sort,
             MapReduceOper quantJob,
             FileSpec lFile,
@@ -1817,7 +1940,7 @@ public class MRCompiler extends PhyPlanVisitor {
         return mro;
     }
 
-    public Pair<MapReduceOper,Integer> getQuantileJob(
+    private Pair<MapReduceOper,Integer> getQuantileJob(
             POSort inpSort,
             MapReduceOper prevJob,
             FileSpec lFile,
@@ -1853,7 +1976,7 @@ public class MRCompiler extends PhyPlanVisitor {
     /**
      * Create Sampling job for skewed join.
      */
-    public Pair<MapReduceOper, Integer> getSkewedJoinSampleJob(POSkewedJoin op, MapReduceOper prevJob, 
+    private Pair<MapReduceOper, Integer> getSkewedJoinSampleJob(POSkewedJoin op, MapReduceOper prevJob, 
     		FileSpec lFile, FileSpec sampleFile, int rp ) throws PlanException, VisitorException {
     	    	
     	MultiMap<PhysicalOperator, PhysicalPlan> joinPlans = op.getJoinPlans();
@@ -1931,7 +2054,7 @@ public class MRCompiler extends PhyPlanVisitor {
      * @throws VisitorException
      */
   	@SuppressWarnings("deprecation")
-    protected Pair<MapReduceOper,Integer> getSamplingJob(POSort sort, MapReduceOper prevJob, List<PhysicalPlan> transformPlans,
+    private Pair<MapReduceOper,Integer> getSamplingJob(POSort sort, MapReduceOper prevJob, List<PhysicalPlan> transformPlans,
   			FileSpec lFile, FileSpec sampleFile, int rp, List<PhysicalPlan> sortKeyPlans, 
   			String udfClassName, String[] udfArgs, String sampleLdrClassName ) throws PlanException, VisitorException {
   		
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceOper.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceOper.java
index 68a9a15a1..61127451b 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceOper.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceOper.java
@@ -18,18 +18,12 @@
 package org.apache.pig.backend.hadoop.executionengine.mapReduceLayer;
 
 import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.util.ArrayList;
 import java.util.HashSet;
-import java.util.List;
-import java.util.Properties;
 import java.util.Set;
 
-import org.apache.pig.impl.io.FileSpec;
 import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.impl.plan.NodeIdGenerator;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.MROpPlanVisitor;
-import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POUnion;
 import org.apache.pig.impl.plan.Operator;
@@ -41,7 +35,7 @@ import org.apache.pig.impl.plan.VisitorException;
  * Acts as a host to the plans that will
  * execute in map, reduce and optionally combine
  * phases. These will be embedded in the MROperPlan
- * in order to capture the dependecies amongst jobs.
+ * in order to capture the dependencies amongst jobs.
  */
 public class MapReduceOper extends Operator<MROpPlanVisitor> {
     private static final long serialVersionUID = 1L;
@@ -133,6 +127,12 @@ public class MapReduceOper extends Operator<MROpPlanVisitor> {
     // Used by Skewed Join
 	private String skewedJoinPartitionFile;
 	
+	// Flag to communicate from MRCompiler to JobControlCompiler what kind of
+	// comparator is used by Hadoop for sorting for this MROper. 
+	// By default, set to false which will make Pig provide raw comparators. 
+	// Set to true in indexing job generated in map-side cogroup, merge join.
+	private boolean usingTypedComparator = false;
+	
     public MapReduceOper(OperatorKey k) {
         super(k);
         mapPlan = new PhysicalPlan();
@@ -383,4 +383,12 @@ public class MapReduceOper extends Operator<MROpPlanVisitor> {
     public void setUseSecondaryKey(boolean useSecondaryKey) {
         this.useSecondaryKey = useSecondaryKey;
     }
+
+    protected boolean usingTypedComparator() {
+        return usingTypedComparator;
+    }
+
+    protected void useTypedComparator(boolean useTypedComparator) {
+        this.usingTypedComparator = useTypedComparator;
+    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MergeJoinIndexer.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MergeJoinIndexer.java
index a28cbced8..d5aff3dac 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MergeJoinIndexer.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MergeJoinIndexer.java
@@ -58,6 +58,7 @@ public class MergeJoinIndexer  extends LoadFunc{
     private Tuple dummyTuple = null;
     private LoadFunc loader;
     private PigSplit pigSplit = null;
+    private boolean ignoreNullKeys;
     
     /** @param funcSpec : Loader specification.
      *  @param innerPlan : This is serialized version of LR plan. We 
@@ -67,12 +68,16 @@ public class MergeJoinIndexer  extends LoadFunc{
      * @throws ExecException 
      */
     @SuppressWarnings("unchecked")
-    public MergeJoinIndexer(String funcSpec, String innerPlan, String serializedPhyPlan) throws ExecException{
+    public MergeJoinIndexer(String funcSpec, String innerPlan, String serializedPhyPlan, 
+            String udfCntxtSignature, String scope, String ignoreNulls) throws ExecException{
         
         loader = (LoadFunc)PigContext.instantiateFuncFromSpec(funcSpec);
+        loader.setUDFContextSignature(udfCntxtSignature);
+        this.ignoreNullKeys = Boolean.parseBoolean(ignoreNulls);
+        
         try {
             List<PhysicalPlan> innerPlans = (List<PhysicalPlan>)ObjectSerializer.deserialize(innerPlan);
-            lr = new POLocalRearrange(new OperatorKey("MergeJoin Indexer",NodeIdGenerator.getGenerator().getNextNodeId("MergeJoin Indexer")));
+            lr = new POLocalRearrange(new OperatorKey(scope,NodeIdGenerator.getGenerator().getNextNodeId(scope)));
             lr.setPlans(innerPlans);
             keysCnt = innerPlans.size();
             precedingPhyPlan = (PhysicalPlan)ObjectSerializer.deserialize(serializedPhyPlan);
@@ -121,7 +126,7 @@ public class MergeJoinIndexer  extends LoadFunc{
                 lr.attachInput(readTuple);
                 key = ((Tuple)lr.getNext(dummyTuple).result).get(1);
                 lr.detachInput();
-                if ( null == key) // Tuple with null key. Drop it.
+                if ( null == key && ignoreNullKeys) // Tuple with null key. Drop it.
                     continue;
                 break;      
             }
@@ -141,7 +146,7 @@ public class MergeJoinIndexer  extends LoadFunc{
                     lr.attachInput((Tuple)res.result);
                     key = ((Tuple)lr.getNext(dummyTuple).result).get(1);
                     lr.detachInput();
-                    if ( null == key) // Tuple with null key. Drop it.
+                    if ( null == key && ignoreNullKeys) // Tuple with null key. Drop it.
                         continue;
                      fetchNewTup = false;
                     break;
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PhyPlanSetter.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PhyPlanSetter.java
index f644e9087..4e31b49fd 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PhyPlanSetter.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PhyPlanSetter.java
@@ -296,4 +296,10 @@ public class PhyPlanSetter extends PhyPlanVisitor {
         preCombinerLocalRearrange.setParentPlan(parent);
     }
 
+
+    @Override
+    public void visitMergeCoGroup(POMergeCogroup mergeCoGrp)
+            throws VisitorException {
+        mergeCoGrp.setParentPlan(parent);
+    }
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigSplit.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigSplit.java
index ad99bfb3e..c1e9ca872 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigSplit.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/PigSplit.java
@@ -125,6 +125,7 @@ public class PigSplit extends InputSplit implements Writable, Configurable {
     
     @SuppressWarnings("unchecked")
     public void readFields(DataInput is) throws IOException {
+        totalSplits = is.readInt();
         splitIndex = is.readInt();
         inputIndex = is.readInt();
         targetOps = (ArrayList<OperatorKey>) readObject(is);
@@ -145,6 +146,7 @@ public class PigSplit extends InputSplit implements Writable, Configurable {
 
     @SuppressWarnings("unchecked")
     public void write(DataOutput os) throws IOException {
+        os.writeInt(totalSplits);
         os.writeInt(splitIndex);
         os.writeInt(inputIndex);
         writeObject(targetOps, os);
@@ -185,7 +187,7 @@ public class PigSplit extends InputSplit implements Writable, Configurable {
     // package level access because we don't want LoadFunc implementations
     // to get this information - this is to be used only from
     // MergeJoinIndexer
-    int getSplitIndex() {
+    public int getSplitIndex() {
         return splitIndex;
     }
 
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/plans/EndOfAllInputSetter.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/plans/EndOfAllInputSetter.java
index d208a43cf..680b722d8 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/plans/EndOfAllInputSetter.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/plans/EndOfAllInputSetter.java
@@ -21,6 +21,7 @@ import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOpe
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeCogroup;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStream;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCollectedGroup;
@@ -88,6 +89,11 @@ public class EndOfAllInputSetter extends MROpPlanVisitor {
             endOfAllInputFlag = true;
         }
 
+        @Override
+        public void visitMergeCoGroup(POMergeCogroup mergeCoGrp)
+                throws VisitorException {
+            endOfAllInputFlag = true;
+        }
         /**
          * @return if end of all input is present
          */
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/LogToPhyTranslationVisitor.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/LogToPhyTranslationVisitor.java
index 6e8fa73a2..6a909be30 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/LogToPhyTranslationVisitor.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/LogToPhyTranslationVisitor.java
@@ -23,20 +23,13 @@ import java.util.Arrays;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import java.util.Random;
 import java.util.Stack;
 
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
 import org.apache.pig.ComparisonFunc;
 import org.apache.pig.EvalFunc;
 import org.apache.pig.FuncSpec;
-import org.apache.pig.LoadFunc;
 import org.apache.pig.PigException;
-import org.apache.pig.SortInfo;
-import org.apache.pig.data.DataBag;
 import org.apache.pig.data.DataType;
-import org.apache.pig.data.NonSpillableDataBag;
 import org.apache.pig.data.Tuple;
 import org.apache.pig.data.TupleFactory;
 import org.apache.pig.impl.PigContext;
@@ -48,7 +41,6 @@ import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOpe
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.ExpressionOperator;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.BinaryExpressionOperator;
 import org.apache.pig.builtin.BinStorage;
-import org.apache.pig.builtin.IsEmpty;
 import org.apache.pig.impl.builtin.GFCross;
 import org.apache.pig.impl.io.FileLocalizer;
 import org.apache.pig.impl.io.FileSpec;
@@ -666,14 +658,94 @@ public class LogToPhyTranslationVisitor extends LOVisitor {
     @Override
     public void visit(LOCogroup cg) throws VisitorException {
             
-        if (cg.getGroupType() == LOCogroup.GROUPTYPE.COLLECTED) {
-
+        switch (cg.getGroupType()) {
+        
+        case COLLECTED:
             translateCollectedCogroup(cg);
+            break;
 
-        } else {
-            
+        case REGULAR:
             translateRegularCogroup(cg);
+            break;
+            
+        case MERGE:
+            translateMergeCogroup(cg);
+            break;
+            
+        default:
+            throw new LogicalToPhysicalTranslatorException("Unknown CoGroup Modifier",PigException.BUG);
+        }
+    }
+    
+    private void translateMergeCogroup(LOCogroup loCogrp) throws VisitorException{
+        
+        String scope = loCogrp.getOperatorKey().scope;
+        List<LogicalOperator> inputs = loCogrp.getInputs();
+        
+        // LocalRearrange corresponding to each of input 
+        // LR is needed to extract keys out of the tuples.
+        
+        POLocalRearrange[] innerLRs = new POLocalRearrange[inputs.size()];
+        int count = 0;
+        List<PhysicalOperator> inpPOs = new ArrayList<PhysicalOperator>(inputs.size());
+        
+        for (LogicalOperator op : inputs) {
+            PhysicalOperator physOp = logToPhyMap.get(op);
+            inpPOs.add(physOp);
+            
+            List<LogicalPlan> plans = (List<LogicalPlan>)loCogrp.getGroupByPlans().get(op);
+            POLocalRearrange poInnerLR = new POLocalRearrange(new OperatorKey(scope, nodeGen.getNextNodeId(scope)));
+            poInnerLR.setAlias(loCogrp.getAlias());
+            // LR will contain list of physical plans, because there could be
+            // multiple keys and each key can be an expression.
+            List<PhysicalPlan> exprPlans = new ArrayList<PhysicalPlan>();
+            currentPlans.push(currentPlan);
+            for (LogicalPlan lp : plans) {
+                currentPlan = new PhysicalPlan();
+                PlanWalker<LogicalOperator, LogicalPlan> childWalker = mCurrentWalker
+                        .spawnChildWalker(lp);
+                pushWalker(childWalker);
+                mCurrentWalker.walk(this);
+                exprPlans.add(currentPlan);
+                popWalker();
+            }
+            currentPlan = currentPlans.pop();
+            try {
+                poInnerLR.setPlans(exprPlans);
+            } catch (PlanException pe) {
+                int errCode = 2071;
+                String msg = "Problem with setting up local rearrange's plans.";
+                throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, pe);
+            }
+            innerLRs[count] = poInnerLR;
+            try {
+                poInnerLR.setIndex(count++);
+            } catch (ExecException e1) {
+                int errCode = 2058;
+                String msg = "Unable to set index on newly create POLocalRearrange.";
+                throw new VisitorException(msg, errCode, PigException.BUG, e1);
+            }
+            poInnerLR.setKeyType(plans.size() > 1 ? DataType.TUPLE : 
+                        exprPlans.get(0).getLeaves().get(0).getResultType());
+            poInnerLR.setResultType(DataType.TUPLE);
+        }
+        
+        POMergeCogroup poCogrp = new POMergeCogroup(new OperatorKey(
+                scope, nodeGen.getNextNodeId(scope)),inpPOs,innerLRs,
+                loCogrp.getRequestedParallelism());
+        poCogrp.setAlias(loCogrp.getAlias());
+        poCogrp.setResultType(DataType.TUPLE);
+        currentPlan.add(poCogrp);
+        for (LogicalOperator op : inputs) {
+            try {
+                currentPlan.connect(logToPhyMap.get(op), poCogrp);
+            } catch (PlanException e) {
+                int errCode = 2015;
+                String msg = "Invalid physical operators in the physical plan" ;
+                throw new LogicalToPhysicalTranslatorException(msg, errCode, PigException.BUG, e);
+            }
         }
+        logToPhyMap.put(loCogrp, poCogrp);
     }
     
     private void translateRegularCogroup(LOCogroup cg) throws VisitorException {
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/plans/PhyPlanVisitor.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/plans/PhyPlanVisitor.java
index daa475278..465de5d4e 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/plans/PhyPlanVisitor.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/plans/PhyPlanVisitor.java
@@ -266,6 +266,10 @@ public class PhyPlanVisitor extends PlanVisitor<PhysicalOperator,PhysicalPlan> {
     public void visitMergeJoin(POMergeJoin join) throws VisitorException {
         //do nothing
     }
+    
+    public void visitMergeCoGroup(POMergeCogroup mergeCoGrp) throws VisitorException{
+        
+    }
     /**
      * @param stream
      * @throws VisitorException 
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POCollectedGroup.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POCollectedGroup.java
index 478f0ac59..0b330b74d 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POCollectedGroup.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POCollectedGroup.java
@@ -20,7 +20,6 @@ package org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOp
 import java.util.ArrayList;
 import java.util.List;
 
-import org.apache.hadoop.io.WritableComparable;
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
@@ -35,7 +34,6 @@ import org.apache.pig.data.DataType;
 import org.apache.pig.data.InternalCachedBag;
 import org.apache.pig.data.Tuple;
 import org.apache.pig.data.TupleFactory;
-import org.apache.pig.impl.io.PigNullableWritable;
 import org.apache.pig.impl.plan.OperatorKey;
 import org.apache.pig.impl.plan.PlanException;
 import org.apache.pig.impl.plan.VisitorException;
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POLocalRearrange.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POLocalRearrange.java
index 51526e925..56953042d 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POLocalRearrange.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POLocalRearrange.java
@@ -122,6 +122,9 @@ public class POLocalRearrange extends PhysicalOperator {
     
     private boolean useSecondaryKey = false;
     
+    // By default, we strip keys from the value.
+    private boolean stripKeyFromValue = true;
+    
     public POLocalRearrange(OperatorKey k) {
         this(k, -1, null);
     }
@@ -412,7 +415,6 @@ public class POLocalRearrange extends PhysicalOperator {
         Object key;
         Object secondaryKey=null;
         
-        
         if (secondaryResLst!=null && secondaryResLst.size()>0)
         {
             key = getKeyFromResult(resLst, mainKeyType);
@@ -420,6 +422,13 @@ public class POLocalRearrange extends PhysicalOperator {
         } else
             key = getKeyFromResult(resLst, keyType);
         
+
+        if(!stripKeyFromValue){
+            lrOutput.set(1, key);
+            lrOutput.set(2, value);
+            return lrOutput;
+        }
+        
         if (mIsDistinct) {
 
             //Put the key and the indexed tuple
@@ -781,4 +790,8 @@ public class POLocalRearrange extends PhysicalOperator {
         
     }
 
+    protected void setStripKeyFromValue(boolean stripKeyFromValue) {
+        this.stripKeyFromValue = stripKeyFromValue;
+    }
+
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POMergeCogroup.java b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POMergeCogroup.java
new file mode 100644
index 000000000..f894361fb
--- /dev/null
+++ b/src/org/apache/pig/backend/hadoop/executionengine/physicalLayer/relationalOperators/POMergeCogroup.java
@@ -0,0 +1,566 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators;
+
+import java.io.IOException;
+import java.io.ObjectInputStream;
+import java.util.ArrayList;
+import java.util.Comparator;
+import java.util.List;
+import java.util.PriorityQueue;
+import java.util.Properties;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.hadoop.mapreduce.Mapper.Context;
+import org.apache.pig.ExecType;
+import org.apache.pig.FuncSpec;
+import org.apache.pig.IndexableLoadFunc;
+import org.apache.pig.LoadFunc;
+import org.apache.pig.PigException;
+import org.apache.pig.backend.executionengine.ExecException;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigSplit;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.POStatus;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.Result;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhyPlanVisitor;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
+import org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil;
+import org.apache.pig.data.DataBag;
+import org.apache.pig.data.DataType;
+import org.apache.pig.data.InternalCachedBag;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.data.TupleFactory;
+import org.apache.pig.impl.PigContext;
+import org.apache.pig.impl.io.FileSpec;
+import org.apache.pig.impl.plan.NodeIdGenerator;
+import org.apache.pig.impl.plan.OperatorKey;
+import org.apache.pig.impl.plan.VisitorException;
+import org.apache.pig.impl.util.Pair;
+
+public class POMergeCogroup extends PhysicalOperator {
+
+    private static final long serialVersionUID = 1L;
+
+    private transient List<LoadFunc> sideLoaders;
+
+    private List<FuncSpec> sidFuncSpecs;
+
+    private List<String> sideFileSpecs;
+
+    // Local Rearranges corresponding to side Loader to extract relevant bits
+    // out of the tuple we get from side loaders.
+    private POLocalRearrange LRs[];
+
+    private transient boolean firstTime;
+
+    private transient Comparable<Object> firstKeyOfNextSplit;
+
+    // This is the count of all the relations involved in Cogroup. The Mapped
+    // relation is also included in the count.
+    private transient int relationCnt;
+
+    private transient TupleFactory mTupleFactory;
+
+    private String indexFileName;
+
+    private FuncSpec idxFuncSpec; 
+
+    private transient DataBag[] outBags;
+
+    private transient Tuple prevTopOfHeap;
+
+    private List<String> loaderSignatures;
+
+    private transient boolean createNewBags;
+
+    // Heap contains tuples with exactly three fields, which is same as output
+    // of LR except for third field which contains full-fledged input tuple,
+    // as oppose to key-stripped tuple generated by LR.
+    private transient PriorityQueue<Tuple> heap;
+
+    private transient boolean lastTime;
+
+    // Need to know in each getNext() whether we generated valid output in last
+    // call or not.
+    private transient boolean workingOnNewKey;
+
+    public POMergeCogroup(OperatorKey k,List<PhysicalOperator> inpPOs, 
+            POLocalRearrange[] lrs, int parallel) {
+
+        super(k,parallel,inpPOs);
+        this.LRs = lrs;
+        for(int i=0; i < lrs.length; i++)
+            LRs[i].setStripKeyFromValue(false);
+    }
+
+    @Override
+    public Result getNext(Tuple t) throws ExecException {
+
+        try{
+            if(createNewBags){      
+                // This implies we just generated output tuple in last call.
+                // So, its time to create new bags.
+                for (int i=0; i < relationCnt; i++)
+                    outBags[i] = new InternalCachedBag(relationCnt);
+                createNewBags = false;
+            }
+
+            // Get the tuple from predecessor.
+            Result baseInp = processInput();
+            Tuple rearranged;
+
+            switch (baseInp.returnStatus) {
+
+            case POStatus.STATUS_OK:
+                rearranged = applyLRon((Tuple)baseInp.result, 0);
+                break;
+
+            case POStatus.STATUS_EOP:
+                if(!this.parentPlan.endOfAllInput)
+                    return baseInp;
+
+                if(lastTime)
+                    return baseInp; 
+
+                // We got all the records from mapper but have not yet generated
+                // all the outputs that we need to, so we continue filling 
+                // and draining the heap from side loaders.
+                while(!heap.isEmpty()){
+
+                    Tuple topOfHeap = heap.peek();
+
+                    if(needToBreak(topOfHeap, prevTopOfHeap))
+                        break;
+
+                    workingOnNewKey = false;
+                    topOfHeap = heap.poll();
+                    byte relIdx = (Byte)topOfHeap.get(0);
+                    prevTopOfHeap = topOfHeap;
+                    outBags[relIdx].add((Tuple)topOfHeap.get(2));
+
+                    if(relIdx == 0) // We got all the mapper has to offer.
+                        continue;
+
+                    Tuple nxtTuple = sideLoaders.get(relIdx-1).getNext();
+                    if(nxtTuple == null)
+                        continue;
+
+                    Tuple rearrangedTup = applyLRon(nxtTuple, relIdx);
+                    Object key = rearrangedTup.get(1);
+                    if(null == firstKeyOfNextSplit || null == key || firstKeyOfNextSplit.compareTo(key) > 0){
+                        heap.offer(rearrangedTup);
+                    }
+                }
+
+                // This is the last output we will produce, if heap is empty
+                // or we got ahead of the first key of the next split.
+                if(heap.isEmpty() || (firstKeyOfNextSplit != null && firstKeyOfNextSplit.compareTo(heap.peek().get(1)) <= 0))
+                    lastTime = true;
+
+                return getOutputTuple();
+
+            default: // In case of errors, we return the tuple as it is.
+                return baseInp;
+            }
+
+            // Every time we read something valid from mapper, we add it to heap.
+            heap.offer(rearranged);
+
+            if(firstTime){
+                setup(rearranged);
+                firstTime = false;
+                // Heap is initialized with first record from all the relations.
+            }
+
+            // Algorithm is as following:
+
+            /* while (there are records in heap) {
+                peek at top record from heap
+                if we see a key change from previous run, break to return output.
+                else, 
+                pull tuple from top of heap and place it in bag based on which input it came from
+                pull record from input that last record from top of heap came from
+                if (key pulled < first key in next split) insert into heap
+                }
+                output final record
+             */
+
+            while(!heap.isEmpty()){
+
+                Tuple topOfHeap = heap.peek();
+
+                if(needToBreak(topOfHeap, prevTopOfHeap))
+                    break;
+
+                workingOnNewKey = false;
+                topOfHeap = heap.poll();
+                byte relIdx = (Byte)topOfHeap.get(0);
+                prevTopOfHeap = topOfHeap;
+                outBags[relIdx].add((Tuple)topOfHeap.get(2));
+
+                // Pull tuple from the corresponding loader.
+                if(relIdx == 0)
+                    return new Result(POStatus.STATUS_EOP,null);
+
+                Tuple nxtTuple = sideLoaders.get(relIdx-1).getNext();
+                if(nxtTuple == null) // EOF for this relation
+                    continue;
+
+                Tuple rearrangedTup = applyLRon(nxtTuple, relIdx);
+                Object key = rearrangedTup.get(1);
+
+                if(firstKeyOfNextSplit == null || null == key || firstKeyOfNextSplit.compareTo(key) > 0){
+                    heap.offer(rearrangedTup);
+                }
+            }
+
+            return getOutputTuple();
+        }
+
+        catch(IOException ioe){
+            throw new ExecException(ioe);
+        }
+    }
+
+    private Result getOutputTuple() throws ExecException{
+
+        workingOnNewKey = true;
+        createNewBags = true;
+
+        Tuple out = mTupleFactory.newTuple(relationCnt+1);
+
+        out.set(0, prevTopOfHeap.get(1));
+        for(int i=0; i < relationCnt; i++)
+            out.set(i+1,(outBags[i]));
+
+        return new Result(POStatus.STATUS_OK, out);        
+    }
+
+
+    private boolean needToBreak(final Tuple curTopOfHeap, final Tuple prevTopOfHeap) throws ExecException{
+
+        if(workingOnNewKey)
+            // This implies we just returned an output tuple in previous call
+            // so we are now working on a fresh key.
+            return false;
+
+        Object curKey = curTopOfHeap.get(1);
+        Object prevKey = prevTopOfHeap.get(1);
+
+        if (curKey == null && null == prevKey)
+            // If both keys are nulls, check if they are coming from same relation.
+            // because nulls from different relations are not considered equal.
+            return ! ((Byte)curTopOfHeap.get(0)).equals((Byte)prevTopOfHeap.get(0));
+
+
+        if(curKey == null || null == prevKey)
+            // If only one of them is null, then key has changed from null to non-null.
+            return true;
+
+        // Both the keys are non-null.
+        return ! curKey.equals(prevKey);
+    }
+
+
+    @SuppressWarnings( "unchecked")
+    private void setup(Tuple firstRearrangedTup) throws IOException{
+
+        // Read our own split Index.
+        int  curSplitIdx = ((PigSplit)((Context)PigMapReduce.sJobContext).getInputSplit()).getSplitIndex();
+        Object firstBaseKey = firstRearrangedTup.get(1);
+        List<Pair<Integer,Tuple>> index = readIndex();
+
+        // If we are in last split, firstKeyOfNextSplit is marked as null.
+        // Null value of firstKeyOfNextSplit is used to indicate collect all 
+        // tuples from both base loaders as well as side loaders 
+        // and process them in this map.
+        // Note that nulls are smaller then anything else. So, if there are 
+        // nulls in data, they will be in very first split. So, null value of 
+        // this variable can be used to determine whether we are working in last
+        // split or not.
+
+        firstKeyOfNextSplit = getFirstKeyOfNextSplit(curSplitIdx, index); 
+
+        // Open all other streams. 
+        // If this is first split, start from very first record. 
+        // For all other splits, bind to the first key which is greater
+        // then or equal to the first key of the map.
+
+        for(int i=0; i < relationCnt-1; i ++){
+
+            LoadFunc loadfunc = (LoadFunc)PigContext.instantiateFuncFromSpec(sidFuncSpecs.get(i));
+            loadfunc.setUDFContextSignature(loaderSignatures.get(i));
+            Job dummyJob = new Job(new Configuration(PigMapReduce.sJobConf));
+            loadfunc.setLocation(sideFileSpecs.get(i), dummyJob);
+            ((IndexableLoadFunc)loadfunc).initialize(dummyJob.getConfiguration());
+            sideLoaders.add(loadfunc);
+            Tuple rearranged;
+
+            if ( index.get(0).first.equals(curSplitIdx)){ 
+                // This is a first split, bind at very first record in all side relations.
+                Tuple t = loadfunc.getNext();
+                if(null == t)   // This side relation is entirely empty.
+                    continue;
+                rearranged = applyLRon(t, i+1);
+                heap.offer(rearranged);
+                continue;
+            }
+            else{
+                // This is not a first split, we need to bind to the key equal 
+                // to the firstBaseKey or next key thereafter.
+
+                // First seek close to base key.  
+                ((IndexableLoadFunc)loadfunc).seekNear(firstBaseKey instanceof 
+                        Tuple ? (Tuple) firstBaseKey : mTupleFactory.newTuple(firstBaseKey));
+
+                // Since contract of IndexableLoadFunc is not clear where we 
+                // will land up after seekNear() call,
+                // we start reading from side loader to get to the point where key 
+                // is actually greater or equal to base key.
+                while(true){
+                    Tuple t = loadfunc.getNext();
+                    if(t==null) // This relation has ended.
+                        break;
+                    rearranged = applyLRon(t, i+1);
+                    if(rearranged.get(1) == null) // If we got a null key here
+                        continue;             // it implies we are still behind.
+
+                    int cmpVal = ((Comparable<Object>)rearranged.get(1)).compareTo(firstBaseKey);
+                    if(cmpVal >= 0){  // Break away as soon as we get ahead.
+
+                        // Add this tuple in heap only if it needs to be processed in
+                        // this map. That is it needs to be smaller then next split's
+                        // first key, unless this is the last split, in which case it
+                        // will be processed in this map.
+                        if(firstKeyOfNextSplit == null || firstKeyOfNextSplit.compareTo(rearranged.get(1)) > 0 ){
+                            heap.offer(rearranged);                
+                        }                        
+                        break;
+                    }
+                }
+            }
+        }
+    }
+
+    private List<Pair<Integer,Tuple>> readIndex() throws ExecException{
+
+        // Assertions on index we are about to read:
+        // We are reading index from a file through POLoad which will return tuples.
+        // These tuples looks as follows:
+        // (key1, key2, key3,..,WritableComparable(wc),splitIdx(si))
+        // Index is sorted on key1, then on key2, key3.. , wc, si.
+        // Index contains exactly one entry per split.
+        // Since this is loaded through CollectableLoadFunc, keys can't repeat.
+        // Thus, each index entry contains unique key.
+        // Since nulls are smaller then anything else, if its in data, key with null value 
+        // should be very first entry of index.
+
+        // First create a loader to read index.
+        POLoad ld = new POLoad(new OperatorKey(this.mKey.scope,NodeIdGenerator.getGenerator().getNextNodeId(this.mKey.scope)), 
+                new FileSpec(indexFileName, idxFuncSpec));
+
+        // Index file is distributed through Distributed Cache to all mappers. So, read it locally.
+        Properties props = new Properties();                                          
+        props.setProperty(MapRedUtil.FILE_SYSTEM_NAME, "file:///");
+        ld.setPc(new PigContext(ExecType.LOCAL, props));
+
+        // Each index entry is read as a pair of split index and a tuple consisting of key.
+        List<Pair<Integer,Tuple>> index = new ArrayList<Pair<Integer,Tuple>>();
+
+        for(Result res=ld.getNext(dummyTuple);res.returnStatus!=POStatus.STATUS_EOP;res=ld.getNext(dummyTuple)){
+
+            Tuple  idxTuple = (Tuple)res.result;
+            int colCnt = idxTuple.size()-2;
+            Tuple keyTuple = mTupleFactory.newTuple(colCnt);
+
+            for (int i=0; i< colCnt; i++)
+                keyTuple.set(i, idxTuple.get(i));
+
+            index.add(new Pair<Integer, Tuple>((Integer)idxTuple.get(colCnt+1), keyTuple));
+        }
+
+        return index;
+    }
+
+    @SuppressWarnings("unchecked")
+    private Comparable<Object> getFirstKeyOfNextSplit(final int curSplitIdx, final List<Pair<Integer,Tuple>> index) throws IOException{
+
+        // First find out the index entry corresponding to our current split.
+        int i;
+        for(i=0; i < index.size(); i++){
+            if(index.get(i).first.equals(curSplitIdx))
+                break;
+        }
+
+        // Now read key of the very next index entry.
+        if(i < index.size()-1){
+            Tuple keyTuple =  index.get(i+1).second;
+            return keyTuple.size() == 1 ? (Comparable<Object>)keyTuple.get(0) : keyTuple;
+        }
+
+        // If we are here it implies, current split is the last split.
+        return null;
+
+
+    }
+
+    private Tuple applyLRon(final Tuple inp, final int lrIdx) throws ExecException{
+
+        //Separate Key & Value of input using corresponding LR operator
+        POLocalRearrange lr = LRs[lrIdx];
+        lr.attachInput(inp);
+        Result lrOut = lr.getNext(dummyTuple);
+
+        if(lrOut.returnStatus!=POStatus.STATUS_OK){
+            int errCode = 2167;
+            String errMsg = "LocalRearrange used to extract keys from tuple isn't configured correctly";
+            throw new ExecException(errMsg,errCode,PigException.BUG);
+        } 
+
+        return mTupleFactory.newTuple(((Tuple)lrOut.result).getAll());
+    }
+
+    @Override
+    public void visit(PhyPlanVisitor v) throws VisitorException {
+        v.visitMergeCoGroup(this);
+    }
+
+    @Override
+    public String name() {
+        return "MergeCogroup[" + DataType.findTypeName(resultType) + "]" +" - " + mKey.toString();
+    }
+
+    @Override
+    public boolean supportsMultipleInputs() {
+        return true;
+    }
+
+    @Override
+    public boolean supportsMultipleOutputs() {
+        return false;
+    }
+
+    public List<PhysicalPlan> getLRInnerPlansOf(int i) {
+        return this.LRs[i].getPlans();
+    }
+
+    public void setSideLoadFuncs(List<FuncSpec> sideLoadFuncs) {
+        this.sidFuncSpecs = sideLoadFuncs;
+    }
+
+    public void setSideFileSpecs(List<String> sideFileSpecs) {
+        this.sideFileSpecs = sideFileSpecs;
+    }
+
+    public String getIndexFileName() {
+        return indexFileName;
+    }
+
+    public void setIndexFileName(String indexFileName) {
+        this.indexFileName = indexFileName;
+    }
+
+    public FuncSpec getIdxFuncSpec() {
+        return idxFuncSpec;
+    }
+
+    public void setIdxFuncSpec(FuncSpec idxFileSpec) {
+        this.idxFuncSpec = idxFileSpec;
+    }
+
+    public void setLoaderSignatures(List<String> loaderSignatures) {
+        this.loaderSignatures = loaderSignatures;
+    }
+
+    private void readObject(final ObjectInputStream is) throws IOException,
+    ClassNotFoundException, ExecException {
+
+        is.defaultReadObject();
+        mTupleFactory = TupleFactory.getInstance();
+        this.heap = new PriorityQueue<Tuple>(11, new Comparator<Tuple>() {
+
+            @SuppressWarnings("unchecked")
+            @Override
+            public int compare(final Tuple resLHS, final Tuple resRHS) {
+                try {
+                    // First, about null keys.
+                    // Java's priority queue doesn't handle null, so we handle it ourselves.
+
+                    // If both keys are null, then keys of side loader is considered smaller.
+                    // That is so, because when we poll the heap, we want to get the keys
+                    // of side loader first then the keys of base loader.
+
+                    // If null is compared against non-null, null is smaller. 
+
+                    Object leftKey = resLHS.get(1);
+                    Object rightKey = resRHS.get(1);
+
+                    if (null == leftKey && null == rightKey){
+                        return ((Byte)resRHS.get(0)).compareTo((Byte)resLHS.get(0));
+                    }
+                    if(null == leftKey)
+                        return -1;
+                    if(null == rightKey)
+                        return 1;
+
+                    // Now, about non-null keys.
+                    // Compare the keys which are at index 1 of tuples being
+                    // put in heap.                    
+
+                    int cmpval = ((Comparable<Object>)leftKey).compareTo(rightKey);
+
+                    // If keys are equal, tuple from side relations 
+                    // are considered smaller. This is so because we want 
+                    // to get back tuple of the mapper last when polling tuples
+                    // from heap. And index of mapped relation is 0.
+
+                    return cmpval == 0 ? ((Byte)resRHS.get(0)).compareTo((Byte)resLHS.get(0)) : cmpval;
+                } catch (ExecException e) {
+
+                    // Alas, no choice but to throw Runtime exception.
+                    String errMsg = "Exception occured in compare() of heap in POMergeCogroup.";
+                    throw new RuntimeException(errMsg,e);
+                }
+            } 
+        });
+
+        this.createNewBags = true;
+        this.lastTime = false;
+        this.relationCnt = LRs.length;
+        this.outBags = new DataBag[relationCnt];
+        this.firstTime = true;
+        this.workingOnNewKey = true;
+        this.sideLoaders = new ArrayList<LoadFunc>();
+    }
+
+    // This function is only for debugging. Call it whenever you want to print
+    // the current state of heap.
+    int counter = 0;
+    private void printHeap(){
+        System.out.println("Printing heap :"+ ++counter);
+        PriorityQueue<Tuple> copy = new PriorityQueue<Tuple>(heap);
+        System.out.println("Heap size: "+heap.size());
+        int i =0;
+        while(!copy.isEmpty()){
+            System.out.println(i+++"th item in heap: "+ copy.poll());
+        }
+    }
+}
diff --git a/src/org/apache/pig/data/DefaultTuple.java b/src/org/apache/pig/data/DefaultTuple.java
index 1231644c4..a384d7069 100644
--- a/src/org/apache/pig/data/DefaultTuple.java
+++ b/src/org/apache/pig/data/DefaultTuple.java
@@ -25,11 +25,6 @@ import java.util.ArrayList;
 import java.util.List;
 import java.util.Iterator;
 import java.util.Map;
-import java.util.Map.Entry;
-
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.io.WritableComparable;
 
 import org.apache.pig.PigException;
 import org.apache.pig.backend.executionengine.ExecException;
diff --git a/src/org/apache/pig/impl/logicalLayer/LOCogroup.java b/src/org/apache/pig/impl/logicalLayer/LOCogroup.java
index 09f533424..abe4c37a6 100644
--- a/src/org/apache/pig/impl/logicalLayer/LOCogroup.java
+++ b/src/org/apache/pig/impl/logicalLayer/LOCogroup.java
@@ -50,7 +50,8 @@ public class LOCogroup extends RelationalOperator {
      */
     public static enum GROUPTYPE {
         REGULAR,    // Regular (co)group
-        COLLECTED   // Collected group
+        COLLECTED,   // Collected group
+        MERGE       // Map-side CoGroup on sorted data
     };
 
     /**
diff --git a/src/org/apache/pig/impl/logicalLayer/parser/QueryParser.jjt b/src/org/apache/pig/impl/logicalLayer/parser/QueryParser.jjt
index b5118f064..27fe4b035 100644
--- a/src/org/apache/pig/impl/logicalLayer/parser/QueryParser.jjt
+++ b/src/org/apache/pig/impl/logicalLayer/parser/QueryParser.jjt
@@ -268,8 +268,14 @@ public class QueryParser {
             return cogroup;
         }
 
+        else if (modifier.equalsIgnoreCase("merge")){
+            LogicalOperator cogroup = parseCogroup(gis, lp, LOCogroup.GROUPTYPE.MERGE);
+            cogroup.pinOption(LOCogroup.OPTION_GROUPTYPE);
+            return cogroup;
+        }
+        
         else{
-            throw new ParseException("Only COLLECTED or REGULAR are valid GROUP modifiers.");
+            throw new ParseException("Only COLLECTED, REGULAR or MERGE are valid GROUP modifiers.");
         }
     }
     
@@ -1744,6 +1750,10 @@ LogicalOperator CogroupClause(LogicalPlan lp) :
             log.info("[WARN] Use of double-quoted string to specify hint is deprecated. Please specify hint in single quotes."); 
             cogroup = parseUsingForGroupBy("regular", gis, lp);
             }
+         |("\"merge\"") {
+            log.info("[WARN] Use of double-quoted string to specify hint is deprecated. Please specify hint in single quotes."); 
+            cogroup = parseUsingForGroupBy("merge", gis, lp);
+            }
         )])
     )
 
diff --git a/test/org/apache/pig/test/TestMapSideCogroup.java b/test/org/apache/pig/test/TestMapSideCogroup.java
new file mode 100644
index 000000000..a8e3f3641
--- /dev/null
+++ b/test/org/apache/pig/test/TestMapSideCogroup.java
@@ -0,0 +1,425 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.pig.test;
+
+
+import java.io.IOException;
+import java.util.Iterator;
+
+import junit.framework.TestCase;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.mapreduce.Job;
+import org.apache.pig.CollectableLoadFunc;
+import org.apache.pig.ExecType;
+import org.apache.pig.IndexableLoadFunc;
+import org.apache.pig.PigServer;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOper;
+import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.plans.MROperPlan;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
+import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeCogroup;
+import org.apache.pig.builtin.PigStorage;
+import org.apache.pig.data.BagFactory;
+import org.apache.pig.data.DataBag;
+import org.apache.pig.data.DefaultTuple;
+import org.apache.pig.data.Tuple;
+import org.apache.pig.impl.PigContext;
+import org.apache.pig.impl.logicalLayer.LOCogroup;
+import org.apache.pig.impl.logicalLayer.LogicalPlan;
+import org.apache.pig.test.utils.LogicalPlanTester;
+import org.junit.After;
+import org.junit.Before;
+
+public class TestMapSideCogroup extends TestCase{
+
+    private static final String INPUT_FILE1 = "testCogrpInput1.txt";
+    private static final String INPUT_FILE2 = "testCogrpInput2.txt";
+    private static final String INPUT_FILE3 = "testCogrpInput3.txt";
+    private static final String INPUT_FILE4 = "testCogrpInput4.txt";
+    private static final String INPUT_FILE5 = "testCogrpInput5.txt";
+    private static final String EMPTY_FILE = "empty.txt";
+    private static final String DATA_WITH_NULL_KEYS = "null.txt";
+
+    private MiniCluster cluster = MiniCluster.buildCluster();
+
+    @Before
+    public void setUp() throws Exception {
+
+        Util.deleteFile(cluster, INPUT_FILE1);
+        Util.deleteFile(cluster, INPUT_FILE2);
+        Util.deleteFile(cluster, INPUT_FILE3);
+        Util.deleteFile(cluster, INPUT_FILE4);
+        Util.deleteFile(cluster, INPUT_FILE5);
+        Util.deleteFile(cluster, EMPTY_FILE);
+        int LOOP_SIZE = 3;
+        String[] input = new String[LOOP_SIZE*LOOP_SIZE];
+        int k = 0;
+        for(int i = 1; i <= LOOP_SIZE; i++) {
+            String si = i + "";
+            for(int j=1;j<=LOOP_SIZE;j++)
+                input[k++] = si + "\t" + j;
+        }
+
+        String[] input2 = new String[LOOP_SIZE*2*LOOP_SIZE];
+        k = 0;
+        for(int i = LOOP_SIZE + 1; i <= 3*LOOP_SIZE ; i++) {
+            String si = i + "";
+            for(int j=1;j<=LOOP_SIZE;j++)
+                input2[k++] = si + "\t" + j;
+        }
+
+        String[] input3 = new String[LOOP_SIZE*LOOP_SIZE];
+        k = 0;
+        for(int i = LOOP_SIZE ; i < 2*LOOP_SIZE ; i++) {
+            String si = i + "";
+            for(int j=1;j<=LOOP_SIZE;j++)
+                input3[k++] = si + "\t" + j;
+        }
+
+        String[] dataWithNullKeys = new String[LOOP_SIZE*LOOP_SIZE];
+        k = 0;
+        for(int i = 1; i <= LOOP_SIZE ; i++) {
+            String si;
+            if(i == 1)
+                si = "";
+            else
+                si = i + "";
+            for(int j=1;j<=LOOP_SIZE;j++){
+                dataWithNullKeys[k++] = si + "\t" + j;                
+            }
+
+        }
+
+        Util.createInputFile(cluster, INPUT_FILE1, input);
+        Util.createInputFile(cluster, INPUT_FILE2, input);
+        Util.createInputFile(cluster, INPUT_FILE3, input);
+        Util.createInputFile(cluster, INPUT_FILE4, input2);
+        Util.createInputFile(cluster, INPUT_FILE5, input3);
+        Util.createInputFile(cluster, EMPTY_FILE, new String[]{});
+        Util.createInputFile(cluster, DATA_WITH_NULL_KEYS, dataWithNullKeys);
+
+
+    }
+
+    @After
+    public void tearDown() throws Exception {
+        Util.deleteFile(cluster, INPUT_FILE1);
+        Util.deleteFile(cluster, INPUT_FILE2);
+        Util.deleteFile(cluster, INPUT_FILE3);
+        Util.deleteFile(cluster, INPUT_FILE4);
+        Util.deleteFile(cluster, INPUT_FILE5);
+        Util.deleteFile(cluster, EMPTY_FILE);
+        Util.deleteFile(cluster, DATA_WITH_NULL_KEYS);
+    }
+
+    public void testCompilation(){
+        try{
+            LogicalPlanTester lpt = new LogicalPlanTester();
+            lpt.buildPlan("A = LOAD 'data1' using "+ DummyCollectableLoader.class.getName() +"() as (id, name, grade);");
+            lpt.buildPlan("B = LOAD 'data2' using "+ DummyIndexableLoader.class.getName() +"() as (id, name, grade);");
+            lpt.buildPlan("D = LOAD 'data2' using "+ DummyIndexableLoader.class.getName() +"() as (id, name, grade);");
+            LogicalPlan lp = lpt.buildPlan("C = cogroup A by id, B by id, D by id using 'merge';");
+            assertEquals(LOCogroup.GROUPTYPE.MERGE, ((LOCogroup)lp.getLeaves().get(0)).getGroupType());
+
+            PigContext pc = new PigContext(ExecType.MAPREDUCE,cluster.getProperties());
+            pc.connect();
+            PhysicalPlan phyP = Util.buildPhysicalPlan(lp, pc);
+            PhysicalOperator phyOp = phyP.getLeaves().get(0);
+            assertTrue(phyOp instanceof POMergeCogroup);
+
+            lp = lpt.buildPlan("store C into 'out';");
+            MROperPlan mrPlan = Util.buildMRPlan(Util.buildPhysicalPlan(lp, pc),pc);            
+            assertEquals(2,mrPlan.size());
+
+            Iterator<MapReduceOper> itr = mrPlan.iterator();
+            MapReduceOper oper = itr.next();
+            assertTrue(oper.reducePlan.isEmpty());
+            assertFalse(oper.mapPlan.isEmpty());
+
+            oper = itr.next();
+            assertFalse(oper.reducePlan.isEmpty());
+            assertFalse(oper.mapPlan.isEmpty());
+
+
+        } catch(Exception e){
+            e.printStackTrace();
+            fail("Compilation of merged cogroup failed.");
+        }
+
+    }
+
+    public void testSimple() throws Exception{
+
+        PigServer pigServer = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
+        pigServer.registerQuery("A = LOAD '" + INPUT_FILE1 + "' using "+ DummyCollectableLoader.class.getName() +"() as (c1:chararray,c2:int);");
+        pigServer.registerQuery("B = LOAD '" + INPUT_FILE2 + "' using "+ DummyIndexableLoader.class.getName()   +"() as (c1:chararray,c2:int);");
+
+        DataBag dbMergeCogrp = BagFactory.getInstance().newDefaultBag();
+        pigServer.registerQuery("C = cogroup A by c1, B by c1 using 'merge';");
+        Iterator<Tuple> iter = pigServer.openIterator("C");
+
+        while(iter.hasNext()) {
+            Tuple t = iter.next();
+            dbMergeCogrp.add(t);
+        }
+
+        String[] results = new String[]{
+                "(1,{(1,1),(1,2),(1,3)},{(1,1),(1,2),(1,3)})",
+                "(2,{(2,2),(2,1),(2,3)},{(2,1),(2,2),(2,3)})",
+                "(3,{(3,3),(3,2),(3,1)},{(3,1),(3,2),(3,3)})"
+        };
+
+        assertEquals(3, dbMergeCogrp.size());
+        Iterator<Tuple> itr = dbMergeCogrp.iterator();
+        for(int i=0; i<3; i++){
+            assertEquals(itr.next().toString(), results[i]);   
+        }
+        assertFalse(itr.hasNext());
+    }
+
+    public void test3Way() throws Exception{
+
+        PigServer pigServer = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
+        pigServer.registerQuery("A = LOAD '" + INPUT_FILE1 + "' using "+ DummyCollectableLoader.class.getName() +"() as (c1:chararray,c2:int);");
+        pigServer.registerQuery("B = LOAD '" + INPUT_FILE2 + "' using "+ DummyIndexableLoader.class.getName()   +"() as (c1:chararray,c2:int);");
+        pigServer.registerQuery("E = LOAD '" + INPUT_FILE3 + "' using "+ DummyIndexableLoader.class.getName()   +"() as (c1:chararray,c2:int);");
+
+        DataBag dbMergeCogrp = BagFactory.getInstance().newDefaultBag();
+
+        pigServer.registerQuery("C = cogroup A by c1, B by c1, E by c1 using 'merge';");
+        Iterator<Tuple> iter = pigServer.openIterator("C");
+
+        while(iter.hasNext()) {
+            Tuple t = iter.next();
+            dbMergeCogrp.add(t);
+        }
+
+
+        String[] results = new String[]{
+                "(1,{(1,1),(1,2),(1,3)},{(1,1),(1,2),(1,3)},{(1,1),(1,2),(1,3)})",
+                "(2,{(2,2),(2,1),(2,3)},{(2,1),(2,2),(2,3)},{(2,1),(2,2),(2,3)})",
+                "(3,{(3,2),(3,3),(3,1)},{(3,1),(3,2),(3,3)},{(3,1),(3,2),(3,3)})"
+        };
+
+        assertEquals(3, dbMergeCogrp.size());
+        Iterator<Tuple> itr = dbMergeCogrp.iterator();
+        for(int i=0; i<3; i++){
+            assertEquals(itr.next().toString(), results[i]);   
+        }
+        assertFalse(itr.hasNext());
+
+    }
+
+    public void testMultiSplits() throws Exception{
+
+        PigServer pigServer = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
+        pigServer.registerQuery("A = LOAD '" + INPUT_FILE1 + "," + INPUT_FILE4 + "' using "+ DummyCollectableLoader.class.getName() +"() as (c1:chararray,c2:int);");
+        pigServer.registerQuery("B = LOAD '" + INPUT_FILE5 + "' using "+ DummyIndexableLoader.class.getName()   +"() as (c1:chararray,c2:int);");
+
+        DataBag dbMergeCogrp = BagFactory.getInstance().newDefaultBag();
+
+        pigServer.registerQuery("C = cogroup A by c1, B by c1 using 'merge';");
+        Iterator<Tuple> iter = pigServer.openIterator("C");
+
+
+        while(iter.hasNext()) {
+            Tuple t = iter.next();
+            dbMergeCogrp.add(t);
+        }
+
+
+        String[] results = new String[]{
+                "(4,{(4,1),(4,2),(4,3)},{(4,1),(4,2),(4,3)})",
+                "(5,{(5,2),(5,1),(5,3)},{(5,1),(5,2),(5,3)})",
+                "(6,{(6,1),(6,3),(6,2)},{})",
+                "(7,{(7,2),(7,1),(7,3)},{})",
+                "(8,{(8,3),(8,1),(8,2)},{})",
+                "(9,{(9,1),(9,3),(9,2)},{})",
+                "(1,{(1,1),(1,2),(1,3)},{})",
+                "(2,{(2,1),(2,2),(2,3)},{})",
+                "(3,{(3,3),(3,2),(3,1)},{(3,1),(3,2),(3,3)})"
+        };
+
+        assertEquals(9, dbMergeCogrp.size());
+        Iterator<Tuple> itr = dbMergeCogrp.iterator();
+        for(int i=0; i<9; i++){
+            assertEquals(itr.next().toString(), results[i]);   
+        }
+        assertFalse(itr.hasNext());
+    }
+
+    public void testCogrpOnMultiKeys() throws Exception{
+
+        PigServer pigServer = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
+        pigServer.registerQuery("A = LOAD '" + INPUT_FILE1 + "' using "+ DummyCollectableLoader.class.getName() +"() as (c1:chararray,c2:chararray);");
+        pigServer.registerQuery("B = LOAD '" + INPUT_FILE2 + "' using "+ DummyIndexableLoader.class.getName()   +"() as (c1:chararray,c2:chararray);");
+
+        DataBag dbMergeCogrp = BagFactory.getInstance().newDefaultBag();
+        pigServer.registerQuery("C = cogroup A by (c1,c2) , B by (c1,c2) using 'merge' ;");
+        Iterator<Tuple> iter = pigServer.openIterator("C");
+
+        while(iter.hasNext()) {
+            Tuple t = iter.next();
+            dbMergeCogrp.add(t);
+        }
+
+        String[] results = new String[]{
+                "((1,1),{(1,1)},{(1,1)})",
+                "((1,2),{(1,2)},{(1,2)})",
+                "((1,3),{(1,3)},{(1,3)})",
+                "((2,1),{(2,1)},{(2,1)})",
+                "((2,2),{(2,2)},{(2,2)})",
+                "((2,3),{(2,3)},{(2,3)})",
+                "((3,1),{(3,1)},{(3,1)})",
+                "((3,2),{(3,2)},{(3,2)})",
+                "((3,3),{(3,3)},{(3,3)})"
+        };
+
+        assertEquals(9, dbMergeCogrp.size());
+        Iterator<Tuple> itr = dbMergeCogrp.iterator();
+        for(int i=0; i<9; i++){
+            assertEquals(itr.next().toString(), results[i]);   
+        }
+        assertFalse(itr.hasNext());
+    }
+    
+    public void testEmptyDeltaFile() throws Exception{
+
+        PigServer pigServer = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
+        pigServer.registerQuery("A = LOAD '" + INPUT_FILE1 + "' using "+ DummyCollectableLoader.class.getName() +"() as (c1:chararray,c2:int);");
+        pigServer.registerQuery("B = LOAD '" + EMPTY_FILE + "' using "+ DummyIndexableLoader.class.getName()   +"() as (c1:chararray,c2:int);");
+
+        DataBag dbMergeCogrp = BagFactory.getInstance().newDefaultBag();
+
+        pigServer.registerQuery("C = cogroup A by c1, B by c1 using 'merge';");
+        Iterator<Tuple> iter = pigServer.openIterator("C");
+
+        while(iter.hasNext()) {
+            Tuple t = iter.next();
+            dbMergeCogrp.add(t);
+        }
+
+        String[] results = new String[]{
+                "(1,{(1,1),(1,2),(1,3)},{})",
+                "(2,{(2,1),(2,2),(2,3)},{})",
+                "(3,{(3,1),(3,3),(3,2)},{})"
+        };
+
+        assertEquals(3, dbMergeCogrp.size());
+        Iterator<Tuple> itr = dbMergeCogrp.iterator();
+        for(int i=0; i<3; i++){
+            assertEquals(itr.next().toString(), results[i]);   
+        }
+        assertFalse(itr.hasNext());
+    }
+
+ 
+    public void testDataWithNullKeys() throws Exception{
+
+        PigServer pigServer = new PigServer(ExecType.MAPREDUCE, cluster.getProperties());
+        pigServer.registerQuery("A = LOAD '" + DATA_WITH_NULL_KEYS + "' using "+ DummyCollectableLoader.class.getName() +"() as (c1:chararray,c2:int);");
+        pigServer.registerQuery("B = LOAD '" + DATA_WITH_NULL_KEYS + "' using "+ DummyIndexableLoader.class.getName()   +"() as (c1:chararray,c2:int);");
+
+        String[] results = new String[]{
+                "(,{(,1),(,2),(,3)},{})",
+                "(,{},{(,1),(,2),(,3)})",
+                "(2,{(2,3),(2,1),(2,2)},{(2,1),(2,2),(2,3)})",
+                "(3,{(3,3),(3,1),(3,2)},{(3,1),(3,2),(3,3)})"
+        };   
+
+        DataBag dbMergeCogrp = BagFactory.getInstance().newDefaultBag();
+
+        pigServer.registerQuery("C = cogroup A by c1, B by c1 using 'merge';");
+        Iterator<Tuple> iter = pigServer.openIterator("C");
+
+        while(iter.hasNext()) {
+            Tuple t = iter.next();
+            dbMergeCogrp.add(t);
+        }
+
+        assertEquals(4, dbMergeCogrp.size());
+        Iterator<Tuple> itr = dbMergeCogrp.iterator();
+        for(int i=0; i<4; i++){
+            assertEquals(itr.next().toString(), results[i]);   
+        }
+        assertFalse(itr.hasNext());
+
+    }
+
+    /**
+     * A dummy loader which implements {@link CollectableLoadFunc}
+     */
+    public static class DummyCollectableLoader extends PigStorage implements CollectableLoadFunc{
+
+        public DummyCollectableLoader() {
+        }
+
+        @Override
+        public void ensureAllKeyInstancesInSameSplit() throws IOException {
+        }
+    }
+
+    /**
+     * A dummy loader which implements {@link IndexableLoadFunc} 
+     */
+    public static class DummyIndexableLoader extends PigStorage implements IndexableLoadFunc {
+
+        private String loc;
+        private FSDataInputStream is;
+
+        public DummyIndexableLoader() {
+        }
+
+        @Override
+        public void close() throws IOException {
+            is.close();
+        }
+
+        @Override
+        public void seekNear(Tuple keys) throws IOException {
+
+        }
+
+        @Override
+        public void initialize(Configuration conf) throws IOException {
+            is = FileSystem.get(conf).open(new Path(loc));
+        }
+
+        @Override
+        public void setLocation(String location, Job job) throws IOException {
+            super.setLocation(location, job);
+            loc = location;
+        }
+
+        @Override
+        public Tuple getNext() throws IOException {
+            String line = is.readLine();
+            if(line == null)
+                return null;
+            String[] members = line.split("\t");
+            DefaultTuple tuple = new DefaultTuple();
+            for(String member : members)
+                tuple.append(member);
+            return tuple;
+        }
+    }
+}
diff --git a/test/org/apache/pig/test/data/GoldenFiles/MRC18.gld b/test/org/apache/pig/test/data/GoldenFiles/MRC18.gld
index 158ca76a0..b20474810 100644
--- a/test/org/apache/pig/test/data/GoldenFiles/MRC18.gld
+++ b/test/org/apache/pig/test/data/GoldenFiles/MRC18.gld
@@ -7,17 +7,15 @@ Reduce Plan Empty
 |       |---Load(file:///tmp/input1:org.apache.pig.builtin.PigStorage) - scope-117
 |
 |---MapReduce(1,PigStorage) - scope-126:
-    |   Store(file:/tmp/temp-1456742965/tmp-1456742965:org.apache.pig.builtin.BinStorage) - scope-133
+    |   Store(file:/tmp/temp-1456742965/tmp-1456742965:org.apache.pig.builtin.BinStorage) - scope-132
     |   |
-    |   |---POSort[tuple]() - scope-132
+    |   |---New For Each(true)[bag] - scope-131
     |       |   |
-    |       |   Project[tuple][*] - scope-131
+    |       |   Project[tuple][1] - scope-130
     |       |
-    |       |---Project[tuple][1] - scope-130
-    |           |
-    |           |---Package[tuple]{chararray} - scope-129
-    |   Local Rearrange[tuple]{chararray}(false) - scope-128
+    |       |---Package[tuple]{tuple} - scope-129
+    |   Local Rearrange[tuple]{tuple}(false) - scope-128
     |   |   |
-    |   |   Constant(all) - scope-127
+    |   |   Project[tuple][*] - scope-127
     |   |
-    |   |---Load(file:///tmp/input2:org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MergeJoinIndexer('org.apache.pig.builtin.PigStorage','kmonaaafhdhcaabdgkgbhggbcohfhegjgmcoebhchcgbhjemgjhdhehiibncbnjjmhgbjnadaaabejaaaehdgjhkgfhihaaaaaaaabhhaeaaaaaaabhdhcaaeogphcghcogbhagbgdgigfcohagjghcogcgbgdglgfgogecogigbgegpgphacogfhigfgdhfhegjgpgogfgoghgjgogfcohagihjhdgjgdgbgmemgbhjgfhccohagmgbgohdcofagihjhdgjgdgbgmfagmgbgoaaaaaaaaaaaaaaabacaaabfkaaangfgogeepggebgmgmejgohahfhehihcaacfgphcghcogbhagbgdgigfcohagjghcogjgnhagmcohagmgbgocoephagfhcgbhegphcfagmgbgogpkikkjgaddcgofpacaaagemaaakgneghcgpgnefgeghgfhdheaacdemgphcghcpgbhagbgdgigfcphagjghcpgjgnhagmcphfhegjgmcpenhfgmhegjengbhadlemaaafgnelgfhjhdheaaapemgkgbhggbcphfhegjgmcpengbhadlemaaahgnemgfgbhggfhdheaabaemgkgbhggbcphfhegjgmcpemgjhdhedlemaaaegnephahdhbaahoaaafemaaaggnfcgpgphehdhbaahoaaagemaaaignfegpefgeghgfhdhbaahoaaaehihahdhcaacbgphcghcogbhagbgdgigfcohagjghcogjgnhagmcohfhegjgmcoenhfgmhegjengbhaaaaaaaaaaaaaaaacacaaabemaaaegnengbhahbaahoaaafhihahdhcaabbgkgbhggbcohfhegjgmcoeigbhdgiengbhaafahnkmbmdbgganbadaaacegaaakgmgpgbgeeggbgdhegphcejaaajhegihcgfhdgigpgmgehihadpeaaaaaaaaaaaamhhaiaaaaaabaaaaaaaaahihdhbaahoaaakdpeaaaaaaaaaaaamhhaiaaaaaabaaaaaaaabhdhcaacegphcghcogbhagbgdgigfcohagjghcogjgnhagmcohagmgbgocoephagfhcgbhegphcelgfhjaaaaaaaaaaaaaaabacaaacekaaacgjgeemaaafhdgdgphagfheaabcemgkgbhggbcpgmgbgoghcpfdhehcgjgoghdlhihaaaaaaaaaaaaaaahiheaaafhdgdgphagfhdhcaafjgphcghcogbhagbgdgigfcohagjghcogcgbgdglgfgogecogigbgegpgphacogfhigfgdhfhegjgpgogfgoghgjgogfcohagihjhdgjgdgbgmemgbhjgfhccogfhihahcgfhdhdgjgpgoephagfhcgbhegphchdcofaepfahcgpgkgfgdheaaaaaaaaaaaaaaabacaaaffkaaakgphggfhcgmgpgbgegfgefkaabfhahcgpgdgfhdhdgjgoghecgbghepggfehfhagmgfhdfkaabehcgfhdhfgmhefdgjgoghgmgffehfhagmgfecgbghfkaaaehdhegbhcemaaahgdgpgmhfgngohdheaabfemgkgbhggbcphfhegjgmcpebhchcgbhjemgjhdhedlhihcaagcgphcghcogbhagbgdgigfcohagjghcogcgbgdglgfgogecogigbgegpgphacogfhigfgdhfhegjgpgogfgoghgjgogfcohagihjhdgjgdgbgmemgbhjgfhccogfhihahcgfhdhdgjgpgoephagfhcgbhegphchdcoefhihahcgfhdhdgjgpgoephagfhcgbhegphcaaaaaaaaaaaaaaabacaaabemaaadgmgpghheaacaemgphcghcpgbhagbgdgigfcpgdgpgngngpgohdcpgmgpghghgjgoghcpemgpghdlhihcaaemgphcghcogbhagbgdgigfcohagjghcogcgbgdglgfgogecogigbgegpgphacogfhigfgdhfhegjgpgogfgoghgjgogfcohagihjhdgjgdgbgmemgbhjgfhccofagihjhdgjgdgbgmephagfhcgbhegphcaaaaaaaaaaaaaaabacaaamfkaaafgbgdgdhfgnfkaaangjgohahfheebhehegbgdgigfgeejaabehcgfhbhfgfhdhegfgefagbhcgbgmgmgfgmgjhdgnecaaakhcgfhdhfgmhefehjhagfemaaafgbgmgjgbhdhbaahoaaaoemaaafgjgohahfheheaablemgphcghcpgbhagbgdgigfcphagjghcpgegbhegbcpfehfhagmgfdlemaaaggjgohahfhehdhbaahoaaagemaaangmgjgogfgbghgffehcgbgdgfhcheaachemgphcghcpgbhagbgdgigfcphagjghcphagfgocphfhegjgmcpemgjgogfgbghgffehcgbgdgfhcdlemaaadgmgpghhbaahoaabeemaaahgphfhehahfhehdhbaahoaaagemaaakhagbhcgfgohefagmgbgoheaafaemgphcghcpgbhagbgdgigfcphagjghcpgcgbgdglgfgogecpgigbgegpgphacpgfhigfgdhfhegjgpgogfgoghgjgogfcphagihjhdgjgdgbgmemgbhjgfhccphagmgbgohdcpfagihjhdgjgdgbgmfagmgbgodlemaaadhcgfhdheaaeeemgphcghcpgbhagbgdgigfcphagjghcpgcgbgdglgfgogecpgigbgegpgphacpgfhigfgdhfhegjgpgogfgoghgjgogfcphagihjhdgjgdgbgmemgbhjgfhccpfcgfhdhfgmhedlhihcaacbgphcghcogbhagbgdgigfcohagjghcogjgnhagmcohagmgbgocoephagfhcgbhegphcaaaaaaaaaaaaaaabacaaabemaaaegnelgfhjheaacgemgphcghcpgbhagbgdgigfcphagjghcpgjgnhagmcphagmgbgocpephagfhcgbhegphcelgfhjdlhihahbaahoaaapaaaappppppppdchahahahahdhcaaclgphcghcogbhagbgdgigfcogdgpgngngpgohdcogmgpghghgjgoghcogjgnhagmcoemgpghdeekemgpghghgfhccikmpnoicknfncdiacaaabemaaaegogbgngfhbaahoaaaohihaheaafjgphcghcogbhagbgdgigfcohagjghcogcgbgdglgfgogecogigbgegpgphacogfhigfgdhfhegjgpgogfgoghgjgogfcohagihjhdgjgdgbgmemgbhjgfhccogfhihahcgfhdhdgjgpgoephagfhcgbhegphchdcofaepfahcgpgkgfgdhehahahdhcaaecgphcghcogbhagbgdgigfcohagjghcogcgbgdglgfgogecogigbgegpgphacogfhigfgdhfhegjgpgogfgoghgjgogfcohagihjhdgjgdgbgmemgbhjgfhccofcgfhdhfgmheaaaaaaaaaaaaaaabacaaacecaaamhcgfhehfhcgofdhegbhehfhdemaaaghcgfhdhfgmheheaabcemgkgbhggbcpgmgbgoghcpepgcgkgfgdhedlhihaachahbaahoaaboaaaaaaaahdhbaahoaaaaaaaaaaabhhaeaaaaaaabhdhcaabbgkgbhggbcogmgbgoghcoejgohegfghgfhcbcockakephibihdiacaaabejaaafhggbgmhfgfhihcaabagkgbhggbcogmgbgoghcoeohfgngcgfhcigkmjfbnaljeoailacaaaahihaaaaaaaaahihihdhbaahoaaaaaaaaaaabhhaeaaaaaaakhbaahoaabmhihdhbaahoaaakdpeaaaaaaaaaaaamhhaiaaaaaabaaaaaaaabhbaahoaabmhbaahoaaaphihdhbaahoaaaaaaaaaaaahhaeaaaaaaakhihdhbaahoaaaihdhbaahoaaakdpeaaaaaaaaaaaamhhaiaaaaaabaaaaaaaaahiaahi','')) - scope-118
\ No newline at end of file
+    |   |---Load(file:///tmp/input2:org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MergeJoinIndexer('org.apache.pig.builtin.PigStorage','kmonaaafhdhcaabdgkgbhggbcohfhegjgmcoebhchcgbhjemgjhdhehiibncbnjjmhgbjnadaaabejaaaehdgjhkgfhihaaaaaaaabhhaeaaaaaaabhdhcaaeogphcghcogbhagbgdgigfcohagjghcogcgbgdglgfgogecogigbgegpgphacogfhigfgdhfhegjgpgogfgoghgjgogfcohagihjhdgjgdgbgmemgbhjgfhccohagmgbgohdcofagihjhdgjgdgbgmfagmgbgoaaaaaaaaaaaaaaabacaaabfkaaangfgogeepggebgmgmejgohahfhehihcaacfgphcghcogbhagbgdgigfcohagjghcogjgnhagmcohagmgbgocoephagfhcgbhegphcfagmgbgogpkikkjgaddcgofpacaaagemaaakgneghcgpgnefgeghgfhdheaacdemgphcghcpgbhagbgdgigfcphagjghcpgjgnhagmcphfhegjgmcpenhfgmhegjengbhadlemaaafgnelgfhjhdheaaapemgkgbhggbcphfhegjgmcpengbhadlemaaahgnemgfgbhggfhdheaabaemgkgbhggbcphfhegjgmcpemgjhdhedlemaaaegnephahdhbaahoaaafemaaaggnfcgpgphehdhbaahoaaagemaaaignfegpefgeghgfhdhbaahoaaaehihahdhcaacbgphcghcogbhagbgdgigfcohagjghcogjgnhagmcohfhegjgmcoenhfgmhegjengbhaaaaaaaaaaaaaaaacacaaabemaaaegnengbhahbaahoaaafhihahdhcaabbgkgbhggbcohfhegjgmcoeigbhdgiengbhaafahnkmbmdbgganbadaaacegaaakgmgpgbgeeggbgdhegphcejaaajhegihcgfhdgigpgmgehihadpeaaaaaaaaaaaamhhaiaaaaaabaaaaaaaaahihdhbaahoaaakdpeaaaaaaaaaaaamhhaiaaaaaabaaaaaaaabhdhcaacegphcghcogbhagbgdgigfcohagjghcogjgnhagmcohagmgbgocoephagfhcgbhegphcelgfhjaaaaaaaaaaaaaaabacaaacekaaacgjgeemaaafhdgdgphagfheaabcemgkgbhggbcpgmgbgoghcpfdhehcgjgoghdlhihaaaaaaaaaaaaaaahiheaaafhdgdgphagfhdhcaafjgphcghcogbhagbgdgigfcohagjghcogcgbgdglgfgogecogigbgegpgphacogfhigfgdhfhegjgpgogfgoghgjgogfcohagihjhdgjgdgbgmemgbhjgfhccogfhihahcgfhdhdgjgpgoephagfhcgbhegphchdcofaepfahcgpgkgfgdheaaaaaaaaaaaaaaabacaaaffkaaakgphggfhcgmgpgbgegfgefkaabfhahcgpgdgfhdhdgjgoghecgbghepggfehfhagmgfhdfkaabehcgfhdhfgmhefdgjgoghgmgffehfhagmgfecgbghfkaaaehdhegbhcemaaahgdgpgmhfgngohdheaabfemgkgbhggbcphfhegjgmcpebhchcgbhjemgjhdhedlhihcaagcgphcghcogbhagbgdgigfcohagjghcogcgbgdglgfgogecogigbgegpgphacogfhigfgdhfhegjgpgogfgoghgjgogfcohagihjhdgjgdgbgmemgbhjgfhccogfhihahcgfhdhdgjgpgoephagfhcgbhegphchdcoefhihahcgfhdhdgjgpgoephagfhcgbhegphcaaaaaaaaaaaaaaabacaaabemaaadgmgpghheaacaemgphcghcpgbhagbgdgigfcpgdgpgngngpgohdcpgmgpghghgjgoghcpemgpghdlhihcaaemgphcghcogbhagbgdgigfcohagjghcogcgbgdglgfgogecogigbgegpgphacogfhigfgdhfhegjgpgogfgoghgjgogfcohagihjhdgjgdgbgmemgbhjgfhccofagihjhdgjgdgbgmephagfhcgbhegphcaaaaaaaaaaaaaaabacaaamfkaaafgbgdgdhfgnfkaaangjgohahfheebhehegbgdgigfgeejaabehcgfhbhfgfhdhegfgefagbhcgbgmgmgfgmgjhdgnecaaakhcgfhdhfgmhefehjhagfemaaafgbgmgjgbhdhbaahoaaaoemaaafgjgohahfheheaablemgphcghcpgbhagbgdgigfcphagjghcpgegbhegbcpfehfhagmgfdlemaaaggjgohahfhehdhbaahoaaagemaaangmgjgogfgbghgffehcgbgdgfhcheaachemgphcghcpgbhagbgdgigfcphagjghcphagfgocphfhegjgmcpemgjgogfgbghgffehcgbgdgfhcdlemaaadgmgpghhbaahoaabeemaaahgphfhehahfhehdhbaahoaaagemaaakhagbhcgfgohefagmgbgoheaafaemgphcghcpgbhagbgdgigfcphagjghcpgcgbgdglgfgogecpgigbgegpgphacpgfhigfgdhfhegjgpgogfgoghgjgogfcphagihjhdgjgdgbgmemgbhjgfhccphagmgbgohdcpfagihjhdgjgdgbgmfagmgbgodlemaaadhcgfhdheaaeeemgphcghcpgbhagbgdgigfcphagjghcpgcgbgdglgfgogecpgigbgegpgphacpgfhigfgdhfhegjgpgogfgoghgjgogfcphagihjhdgjgdgbgmemgbhjgfhccpfcgfhdhfgmhedlhihcaacbgphcghcogbhagbgdgigfcohagjghcogjgnhagmcohagmgbgocoephagfhcgbhegphcaaaaaaaaaaaaaaabacaaabemaaaegnelgfhjheaacgemgphcghcpgbhagbgdgigfcphagjghcpgjgnhagmcphagmgbgocpephagfhcgbhegphcelgfhjdlhihahbaahoaaapaaaappppppppdchahahahahdhcaaclgphcghcogbhagbgdgigfcogdgpgngngpgohdcogmgpghghgjgoghcogjgnhagmcoemgpghdeekemgpghghgfhccikmpnoicknfncdiacaaabemaaaegogbgngfhbaahoaaaohihaheaafjgphcghcogbhagbgdgigfcohagjghcogcgbgdglgfgogecogigbgegpgphacogfhigfgdhfhegjgpgogfgoghgjgogfcohagihjhdgjgdgbgmemgbhjgfhccogfhihahcgfhdhdgjgpgoephagfhcgbhegphchdcofaepfahcgpgkgfgdhehahahdhcaaecgphcghcogbhagbgdgigfcohagjghcogcgbgdglgfgogecogigbgegpgphacogfhigfgdhfhegjgpgogfgoghgjgogfcohagihjhdgjgdgbgmemgbhjgfhccofcgfhdhfgmheaaaaaaaaaaaaaaabacaaacecaaamhcgfhehfhcgofdhegbhehfhdemaaaghcgfhdhfgmheheaabcemgkgbhggbcpgmgbgoghcpepgcgkgfgdhedlhihaachahbaahoaaboaaaaaaaahdhbaahoaaaaaaaaaaabhhaeaaaaaaabhdhcaabbgkgbhggbcogmgbgoghcoejgohegfghgfhcbcockakephibihdiacaaabejaaafhggbgmhfgfhihcaabagkgbhggbcogmgbgoghcoeohfgngcgfhcigkmjfbnaljeoailacaaaahihaaaaaaaaahihihdhbaahoaaaaaaaaaaabhhaeaaaaaaakhbaahoaabmhihdhbaahoaaakdpeaaaaaaaaaaaamhhaiaaaaaabaaaaaaaabhbaahoaabmhbaahoaaaphihdhbaahoaaaaaaaaaaaahhaeaaaaaaakhihdhbaahoaaaihdhbaahoaaakdpeaaaaaaaaaaaamhhaiaaaaaabaaaaaaaaahiaahi','','b','scope','true')) - scope-118
\ No newline at end of file
diff --git a/test/org/apache/pig/test/utils/TestHelper.java b/test/org/apache/pig/test/utils/TestHelper.java
index 2e7b7a406..70beeb08c 100644
--- a/test/org/apache/pig/test/utils/TestHelper.java
+++ b/test/org/apache/pig/test/utils/TestHelper.java
@@ -18,9 +18,12 @@
 package org.apache.pig.test.utils;
 
 import java.io.*;
+import java.util.HashMap;
 import java.util.Iterator;
 import java.util.Map;
 import java.util.Random;
+import java.util.Set;
+import java.util.Map.Entry;
 
 import org.apache.pig.backend.executionengine.ExecException;
 import org.apache.pig.data.DataBag;
@@ -56,7 +59,6 @@ public class TestHelper {
         if (db1.size() != db2.size())
             return false;
         
-        int i=-1;
         boolean equal = true;
         for (Tuple tuple : db2) {
             boolean contains = false;
@@ -70,12 +72,37 @@ public class TestHelper {
                 equal = false;
                 break;
             }
-            /*if(++i%dispAfterNumTuples==0)
-                System.out.println(i/dispAfterNumTuples);*/
         }
         return equal;
     }
     
+    public static boolean compareCogroupOutputs(DataBag db1, DataBag db2) throws ExecException{
+        
+        if (db1.size() != db2.size())
+            return false;
+        
+        Map<Object,DataBag> first  = new HashMap<Object, DataBag>();
+        for (Tuple t : db1)
+           first.put(t.get(0), (DataBag)t.get(1)); 
+        
+        Map<Object,DataBag> second = new HashMap<Object, DataBag>();
+        for (Tuple t : db2)
+            second.put(t.get(0), (DataBag)t.get(1)); 
+        
+        Set<Entry<Object,DataBag>> entrySet =  first.entrySet();
+        
+        for (Entry<Object,DataBag> entry : entrySet){
+            Object key = entry.getKey();
+            DataBag bagOfSecond = second.get(key);
+            if(bagOfSecond == null)
+                return false;
+            boolean cmpVal = compareBags(bagOfSecond, entry.getValue());
+            if(cmpVal == false)
+                return false;
+        }
+        return true;
+    }
+    
     public static DataBag projectBag(DataBag db2, int i) throws ExecException {
         DataBag ret = DefaultBagFactory.getInstance().newDefaultBag();
         for (Tuple tuple : db2) {
