diff --git a/CHANGES.txt b/CHANGES.txt
index eaa892484..6637017e4 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -42,6 +42,8 @@ OPTIMIZATIONS
  
 BUG FIXES
 
+PIG-4043: JobClient.getMap/ReduceTaskReports() causes OOM for jobs with a large number of tasks (cheolsoo)
+
 PIG-4036: Fix e2e failures - JobManagement_3, CmdErrors_3 and BigData_4 (daijy)
 
 PIG-4041: org.apache.pig.backend.hadoop.executionengine.tez.util.MRToTezHelper compiling error (jeagles via cheolsoo)
diff --git a/conf/pig.properties b/conf/pig.properties
index 1456c308b..3ac7491b8 100644
--- a/conf/pig.properties
+++ b/conf/pig.properties
@@ -456,6 +456,11 @@
 # pig.stats.output.size.reader=<fully qualified class name of a PigStatsOutputSizeReader implementation>
 # pig.stats.output.size.reader.unsupported=<comma separated list of StoreFuncs that are not supported by this reader>
 
+# By default, Pig retrieves TaskReports for every launched task to compute
+# various job statistics. But this can cause OOM if the number of tasks is
+# large. In such case, you can disable it by setting this property to true.
+# pig.stats.notaskreport=false
+
 #
 # Override hadoop configs programatically
 #
diff --git a/shims/src/hadoop20/org/apache/pig/backend/hadoop/executionengine/shims/HadoopShims.java b/shims/src/hadoop20/org/apache/pig/backend/hadoop/executionengine/shims/HadoopShims.java
index 9eae1a7bc..9230480ff 100644
--- a/shims/src/hadoop20/org/apache/pig/backend/hadoop/executionengine/shims/HadoopShims.java
+++ b/shims/src/hadoop20/org/apache/pig/backend/hadoop/executionengine/shims/HadoopShims.java
@@ -19,6 +19,8 @@ package org.apache.pig.backend.hadoop.executionengine.shims;
 
 import java.io.IOException;
 
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
@@ -34,6 +36,7 @@ import org.apache.hadoop.mapreduce.OutputCommitter;
 import org.apache.hadoop.mapreduce.TaskAttemptContext;
 import org.apache.hadoop.mapreduce.TaskAttemptID;
 import org.apache.hadoop.mapreduce.TaskType;
+import org.apache.pig.PigConfiguration;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
 import org.apache.pig.backend.hadoop20.PigJobControl;
@@ -49,6 +52,9 @@ import org.apache.pig.backend.hadoop20.PigJobControl;
  * version dependant implementation of PigGenericMapReduce, PigGenericMapBase and MiniGenericCluster.
  **/
 public class HadoopShims {
+
+    private static Log LOG = LogFactory.getLog(HadoopShims.class);
+
     static public JobContext cloneJobContext(JobContext original) throws IOException, InterruptedException {
         JobContext newContext = new JobContext(original.getConfiguration(), original.getJobID());
         return newContext;
@@ -178,6 +184,10 @@ public class HadoopShims {
     }
 
     public static TaskReport[] getTaskReports(Job job, TaskType type) throws IOException {
+        if (job.getJobConf().getBoolean(PigConfiguration.PIG_NO_TASK_REPORT, false)) {
+            LOG.info("TaskReports are disabled for job: " + job.getAssignedJobID());
+            return null;
+        }
         JobClient jobClient = job.getJobClient();
         return (type == TaskType.MAP)
                 ? jobClient.getMapTaskReports(job.getAssignedJobID())
diff --git a/shims/src/hadoop23/org/apache/pig/backend/hadoop/executionengine/shims/HadoopShims.java b/shims/src/hadoop23/org/apache/pig/backend/hadoop/executionengine/shims/HadoopShims.java
index 8c3846041..1beee62b3 100644
--- a/shims/src/hadoop23/org/apache/pig/backend/hadoop/executionengine/shims/HadoopShims.java
+++ b/shims/src/hadoop23/org/apache/pig/backend/hadoop/executionengine/shims/HadoopShims.java
@@ -41,6 +41,7 @@ import org.apache.hadoop.mapreduce.TaskAttemptID;
 import org.apache.hadoop.mapreduce.TaskType;
 import org.apache.hadoop.mapreduce.task.JobContextImpl;
 import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;
+import org.apache.pig.PigConfiguration;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
 import org.apache.pig.backend.hadoop23.PigJobControl;
 
@@ -213,6 +214,10 @@ public class HadoopShims {
     }
 
     public static TaskReport[] getTaskReports(Job job, TaskType type) throws IOException {
+        if (job.getJobConf().getBoolean(PigConfiguration.PIG_NO_TASK_REPORT, false)) {
+            LOG.info("TaskReports are disabled for job: " + job.getAssignedJobID());
+            return null;
+        }
         org.apache.hadoop.mapreduce.Job mrJob = job.getJob();
         try {
             org.apache.hadoop.mapreduce.TaskReport[] reports = mrJob.getTaskReports(type);
diff --git a/src/org/apache/pig/PigConfiguration.java b/src/org/apache/pig/PigConfiguration.java
index fc6d85dea..087f30257 100644
--- a/src/org/apache/pig/PigConfiguration.java
+++ b/src/org/apache/pig/PigConfiguration.java
@@ -259,4 +259,9 @@ public class PigConfiguration {
      * which case, the entry would be "pig.whitelist=load,store,filter,group"
      */
     public static final String PIG_WHITELIST = "pig.whitelist";
+
+    /**
+     * This key is used to turns off use of task reports in job statistics.
+     */
+    public static final String PIG_NO_TASK_REPORT = "pig.stats.notaskreport";
 }
diff --git a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java
index afdcb2c20..65a8c3b02 100644
--- a/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java
+++ b/src/org/apache/pig/backend/hadoop/executionengine/mapReduceLayer/MapReduceLauncher.java
@@ -802,13 +802,17 @@ public class MapReduceLauncher extends Launcher{
         }
         try {
             TaskReport[] mapRep = HadoopShims.getTaskReports(job, TaskType.MAP);
-            getErrorMessages(mapRep, "map", errNotDbg, pigContext);
-            totalHadoopTimeSpent += computeTimeSpent(mapRep);
-            mapRep = null;
+            if (mapRep != null) {
+                getErrorMessages(mapRep, "map", errNotDbg, pigContext);
+                totalHadoopTimeSpent += computeTimeSpent(mapRep);
+                mapRep = null;
+            }
             TaskReport[] redRep = HadoopShims.getTaskReports(job, TaskType.REDUCE);
-            getErrorMessages(redRep, "reduce", errNotDbg, pigContext);
-            totalHadoopTimeSpent += computeTimeSpent(redRep);
-            redRep = null;
+            if (redRep != null) {
+                getErrorMessages(redRep, "reduce", errNotDbg, pigContext);
+                totalHadoopTimeSpent += computeTimeSpent(redRep);
+                redRep = null;
+            }
         } catch (IOException e) {
             if (job.getState() == Job.SUCCESS) {
                 // if the job succeeded, let the user know that
diff --git a/test/org/apache/pig/test/TestMRJobStats.java b/test/org/apache/pig/test/TestMRJobStats.java
index a598b380f..96970ebcf 100644
--- a/test/org/apache/pig/test/TestMRJobStats.java
+++ b/test/org/apache/pig/test/TestMRJobStats.java
@@ -23,9 +23,11 @@ import static org.junit.Assert.assertTrue;
 
 import java.io.File;
 import java.io.IOException;
+import java.io.PrintWriter;
 import java.io.RandomAccessFile;
 import java.lang.reflect.Constructor;
 import java.lang.reflect.Method;
+import java.util.List;
 import java.util.Properties;
 
 import org.apache.hadoop.conf.Configuration;
@@ -34,7 +36,10 @@ import org.apache.hadoop.mapred.JobID;
 import org.apache.hadoop.mapred.TaskReport;
 import org.apache.hadoop.mapreduce.Job;
 import org.apache.pig.FuncSpec;
+import org.apache.pig.PigConfiguration;
+import org.apache.pig.PigServer;
 import org.apache.pig.StoreFuncInterface;
+import org.apache.pig.backend.executionengine.ExecJob;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.FileBasedOutputSizeReader;
 import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigStatsOutputSizeReader;
 import org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore;
@@ -324,4 +329,57 @@ public class TestMRJobStats {
         assertEquals("The default output size reader returns -1 for unsupported store funcs",
                 -1, outputSize);
     }
+
+    // See PIG-4043
+    @Test
+    public void testNoTaskReportProperty() throws IOException{
+        MiniGenericCluster cluster = MiniGenericCluster.buildCluster(MiniGenericCluster.EXECTYPE_MR);
+        Properties properties = cluster.getProperties();
+
+        String inputFile = "input";
+        PrintWriter pw = new PrintWriter(Util.createInputFile(cluster, inputFile));
+        pw.println("100\tapple");
+        pw.println("200\torange");
+        pw.close();
+
+        // Enable task reports in job statistics
+        properties.setProperty(PigConfiguration.PIG_NO_TASK_REPORT, "false");
+        PigServer pigServer = new PigServer(cluster.getExecType(), properties);
+        pigServer.setBatchOn();
+
+        // Launch a map-only job
+        pigServer.registerQuery("A = load '" + inputFile + "' as (id:int, fruit:chararray);");
+        pigServer.registerQuery("store A into 'task_reports';");
+        List<ExecJob> jobs = pigServer.executeBatch();
+        PigStats pigStats = jobs.get(0).getStatistics();
+        MRJobStats jobStats = (MRJobStats) pigStats.getJobGraph().getJobList().get(0);
+
+        // Make sure JobStats includes TaskReports information
+        long minMapTime = jobStats.getMinMapTime();
+        long maxMapTime = jobStats.getMaxMapTime();
+        long avgMapTime = jobStats.getAvgMapTime();
+        assertTrue("TaskReports are enabled, so minMapTime shouldn't be -1", minMapTime != -1l);
+        assertTrue("TaskReports are enabled, so maxMapTime shouldn't be -1", maxMapTime != -1l);
+        assertTrue("TaskReports are enabled, so avgMapTime shouldn't be -1", avgMapTime != -1l);
+
+        // Disable task reports in job statistics
+        properties.setProperty(PigConfiguration.PIG_NO_TASK_REPORT, "true");
+
+        // Launch another map-only job
+        pigServer.registerQuery("B = load '" + inputFile + "' as (id:int, fruit:chararray);");
+        pigServer.registerQuery("store B into 'no_task_reports';");
+        jobs = pigServer.executeBatch();
+        pigStats = jobs.get(0).getStatistics();
+        jobStats = (MRJobStats) pigStats.getJobGraph().getJobList().get(0);
+
+        // Make sure JobStats doesn't include any TaskReports information
+        minMapTime = jobStats.getMinMapTime();
+        maxMapTime = jobStats.getMaxMapTime();
+        avgMapTime = jobStats.getAvgMapTime();
+        assertEquals("TaskReports are disabled, so minMapTime should be -1", -1l, minMapTime);
+        assertEquals("TaskReports are disabled, so maxMapTime should be -1", -1l, maxMapTime);
+        assertEquals("TaskReports are disabled, so avgMapTime should be -1", -1l, avgMapTime);
+
+        cluster.shutDown();
+    }
 }
