diff --git a/CHANGES.txt b/CHANGES.txt
index e2a7dd224..c06d8980d 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -80,6 +80,8 @@ OPTIMIZATIONS
 
 BUG FIXES
 
+PIG-2326: Pig minicluster tests can not be run from eclipse (julienledem via daijy)
+
 PIG-2432: Eclipse .classpath file is out of date (gates)
 
 PIG-2427: getSchemaFromString throws away the name of the tuple that is in a bag (jcoveney via dvryaboy)
diff --git a/build.xml b/build.xml
index 8c1538b16..425740004 100644
--- a/build.xml
+++ b/build.xml
@@ -97,7 +97,6 @@
     <property name="ssh.gateway" value="" />
     <property name="hod.server" value="" />
     <property name="test.log.dir" value="${basedir}/test/logs"/>
-    <property name="junit.hadoop.conf" value="${user.home}/pigtest/conf/"/>
     <property name="test.output" value="no"/>
 
     <!-- e2e test properties -->
@@ -789,7 +788,6 @@
         <mkdir dir="${test.log.dir}"/>
         <tempfile property="junit.tmp.dir" prefix="pig_junit_tmp" destDir="${java.io.tmpdir}" />
         <mkdir dir="${junit.tmp.dir}/"/>
-        <mkdir dir="${junit.hadoop.conf}"/>
         <junit showoutput="${test.output}" printsummary="yes" haltonfailure="no" fork="yes" maxmemory="1024m" dir="${basedir}" timeout="${test.timeout}" errorProperty="tests.failed" failureProperty="tests.failed">
             <sysproperty key="ssh.gateway" value="${ssh.gateway}" />
             <sysproperty key="hod.server" value="${hod.server}" />
@@ -800,7 +798,6 @@
             <classpath>
                 <pathelement location="${output.jarfile.withouthadoop}" />
                 <pathelement location="${test.build.classes}" />
-                <pathelement location="${junit.hadoop.conf}" />
                 <pathelement location="." />
 		<pathelement path="${clover.jar}"/>
                 <path refid="test.classpath"/>
diff --git a/contrib/penny/java/build.xml b/contrib/penny/java/build.xml
index 8ab2fd01c..c81e82600 100644
--- a/contrib/penny/java/build.xml
+++ b/contrib/penny/java/build.xml
@@ -48,7 +48,6 @@
     <property name="test.timeout" value="900000" />
     <property name="test.junit.output.format" value="plain" />
     <property name="test.src.dir" value="src/test/java" />
-    <property name="junit.hadoop.conf" value="${user.home}/pigtest/conf/"/>
 
     <path id="penny.classpath">
         <pathelement location="${build.classes}"/>
@@ -66,7 +65,6 @@
         <pathelement location="${pigjar-withouthadoop}"/>
         <pathelement location="${test.classes}"/>
         <pathelement location="${test.src.dir}"/>
-        <pathelement location="${junit.hadoop.conf}" />
         <pathelement location="${pigtest}"/>
         <fileset dir="../../../build/ivy/lib">
             <include name="**/*.jar"/>
diff --git a/contrib/piggybank/java/build.xml b/contrib/piggybank/java/build.xml
index ff2053e0c..dde727d92 100755
--- a/contrib/piggybank/java/build.xml
+++ b/contrib/piggybank/java/build.xml
@@ -56,7 +56,6 @@
     <property name="test.timeout" value="900000" />
     <property name="test.junit.output.format" value="plain" />
     <property name="test.src.dir" value="src/test/java" />
-    <property name="junit.hadoop.conf" value="${user.home}/pigtest/conf/"/>
 
     <path id="pigudf.classpath">
         <pathelement location="${build.classes}"/>
@@ -72,7 +71,6 @@
         <pathelement location="${build.classes}"/>
         <pathelement location="${test.classes}"/>
         <pathelement location="${test.src.dir}"/>
-        <pathelement location="${junit.hadoop.conf}" />
         <path refid="pigudf.classpath"/>
     </path>
 
diff --git a/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestPathPartitionHelper.java b/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestPathPartitionHelper.java
index b8048a152..631b637cc 100644
--- a/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestPathPartitionHelper.java
+++ b/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestPathPartitionHelper.java
@@ -146,7 +146,7 @@ public class TestPathPartitionHelper extends TestCase {
 
     @Override
     protected void setUp() throws Exception {
-    File oldConf = new File(System.getProperty("user.home")+"/pigtest/conf/hadoop-site.xml");
+    File oldConf = new File("build/classes/hadoop-site.xml");
     oldConf.delete();
 	conf = new Configuration();
 
diff --git a/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestPathPartitioner.java b/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestPathPartitioner.java
index 2e900f40d..2259bb64c 100644
--- a/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestPathPartitioner.java
+++ b/contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/storage/TestPathPartitioner.java
@@ -54,7 +54,7 @@ public class TestPathPartitioner extends TestCase {
 
     @Override
     protected void setUp() throws Exception {
-    File oldConf = new File(System.getProperty("user.home")+"/pigtest/conf/hadoop-site.xml");
+    File oldConf = new File("build/classes/hadoop-site.xml");
     oldConf.delete();
 	conf = new Configuration();
 
diff --git a/shims/test/hadoop20/org/apache/pig/test/MiniCluster.java b/shims/test/hadoop20/org/apache/pig/test/MiniCluster.java
index 4ca408945..72670460d 100644
--- a/shims/test/hadoop20/org/apache/pig/test/MiniCluster.java
+++ b/shims/test/hadoop20/org/apache/pig/test/MiniCluster.java
@@ -34,31 +34,32 @@ public class MiniCluster extends MiniGenericCluster {
     @Override
     protected void setupMiniDfsAndMrClusters() {
         try {
+            System.setProperty("hadoop.log.dir", "build/test/logs");
             final int dataNodes = 4;     // There will be 4 data nodes
             final int taskTrackers = 4;  // There will be 4 task tracker nodes
-            
+
             // Create the configuration hadoop-site.xml file
-            File conf_dir = new File(System.getProperty("user.home"), "pigtest/conf/");
+            File conf_dir = new File("build/classes/");
             conf_dir.mkdirs();
             File conf_file = new File(conf_dir, "hadoop-site.xml");
-            
+
             conf_file.delete();
-            
+
             // Builds and starts the mini dfs and mapreduce clusters
             Configuration config = new Configuration();
             m_dfs = new MiniDFSCluster(config, dataNodes, true, null);
             m_fileSys = m_dfs.getFileSystem();
             m_mr = new MiniMRCluster(taskTrackers, m_fileSys.getUri().toString(), 1);
-            
+
             // Write the necessary config info to hadoop-site.xml
-            m_conf = m_mr.createJobConf();      
+            m_conf = m_mr.createJobConf();
             m_conf.setInt("mapred.submit.replication", 2);
             m_conf.set("dfs.datanode.address", "0.0.0.0:0");
             m_conf.set("dfs.datanode.http.address", "0.0.0.0:0");
             m_conf.set("mapred.map.max.attempts", "2");
             m_conf.set("mapred.reduce.max.attempts", "2");
             m_conf.writeXml(new FileOutputStream(conf_file));
-            
+
             // Set the system properties needed by Pig
             System.setProperty("cluster", m_conf.get("mapred.job.tracker"));
             System.setProperty("namenode", m_conf.get("fs.default.name"));
@@ -71,6 +72,6 @@ public class MiniCluster extends MiniGenericCluster {
     @Override
     protected void shutdownMiniMrClusters() {
         if (m_mr != null) { m_mr.shutdown(); }
-            m_mr = null;        
+            m_mr = null;
     }
 }
diff --git a/shims/test/hadoop23/org/apache/pig/test/MiniCluster.java b/shims/test/hadoop23/org/apache/pig/test/MiniCluster.java
index cc60ec0b2..fc31f6d31 100644
--- a/shims/test/hadoop23/org/apache/pig/test/MiniCluster.java
+++ b/shims/test/hadoop23/org/apache/pig/test/MiniCluster.java
@@ -52,12 +52,13 @@ public class MiniCluster extends MiniGenericCluster {
     
     @Override
     protected void setupMiniDfsAndMrClusters() {
-		try {
+	try {
             final int dataNodes = 4;     // There will be 4 data nodes
             final int taskTrackers = 4;  // There will be 4 task tracker nodes
             
-		    // Create the configuration hadoop-site.xml file
-            File conf_dir = new File(System.getProperty("user.home"), "pigtest/conf/");
+	    // Create the configuration hadoop-site.xml file
+            System.setProperty("hadoop.log.dir", "build/test/logs");
+            File conf_dir = new File("build/classes/");
             conf_dir.mkdirs();
             File conf_file = new File(conf_dir, "hadoop-site.xml");
             
diff --git a/src/docs/src/documentation/content/xdocs/pigunit.xml b/src/docs/src/documentation/content/xdocs/pigunit.xml
index 402a6388d..e7882e447 100644
--- a/src/docs/src/documentation/content/xdocs/pigunit.xml
+++ b/src/docs/src/documentation/content/xdocs/pigunit.xml
@@ -188,7 +188,7 @@ $pig_trunk ant pigunit-jar
         <p>When using PigUnit in mapreduce mode, be sure to include the $HADOOP_CONF_DIR of the
           cluster in your CLASSPATH.</p>
         <p>
-          The default value is ~/pigtest/conf.
+          MiniCluster generates one in build/classes.
         </p>
         <source>
 org.apache.pig.backend.executionengine.ExecException: ERROR 4010: Cannot find hadoop configurations in classpath (neither hadoop-site.xml nor core-site.xml was found in the classpath).If you plan to use local mode, please put -x local option in command line
diff --git a/src/docs/src/documentation/content/xdocs/test.xml b/src/docs/src/documentation/content/xdocs/test.xml
index 7e8357ebe..53166df01 100644
--- a/src/docs/src/documentation/content/xdocs/test.xml
+++ b/src/docs/src/documentation/content/xdocs/test.xml
@@ -933,7 +933,7 @@ junit.framework.ComparisonFailure: null expected:&lt;...ahoo,3)
         <p>When using PigUnit in mapreduce mode, be sure to include the $HADOOP_CONF_DIR of the
           cluster in your CLASSPATH.</p>
         <p>
-          The default value is ~/pigtest/conf.
+          MiniCluster generates one in build/classes.
         </p>
         <source>
 org.apache.pig.backend.executionengine.ExecException: 
diff --git a/src/org/apache/pig/backend/hadoop/datastorage/ConfigurationUtil.java b/src/org/apache/pig/backend/hadoop/datastorage/ConfigurationUtil.java
index f59bb9aba..d1fa6e10d 100644
--- a/src/org/apache/pig/backend/hadoop/datastorage/ConfigurationUtil.java
+++ b/src/org/apache/pig/backend/hadoop/datastorage/ConfigurationUtil.java
@@ -65,9 +65,9 @@ public class ConfigurationUtil {
         for (Entry<String, String> entry : replaceConf) {
             origConf.set(entry.getKey(), entry.getValue());
         }
-        
+
     }
-    
+
     public static Properties getLocalFSProperties() {
         Configuration localConf;
         if (PigMapReduce.sJobContext!=null && PigMapReduce.sJobContext.getConfiguration().get("exectype").equals(ExecType.LOCAL.toString())) {
@@ -76,8 +76,8 @@ public class ConfigurationUtil {
         } else {
             localConf = new Configuration(true);
             // It's really hacky, try to get unit test working under hadoop 23.
-            // Hadoop23 MiniMRCluster currently need setup Distributed cache before start, 
-            // so pigtest/conf/hadoop-site.xml contains such entry. This prevents some tests from 
+            // Hadoop23 MiniMRCluster currently need setup Distributed cache before start,
+            // so build/classes/hadoop-site.xml contains such entry. This prevents some tests from
             // successful (They expect those files in hdfs), so we need to unset it in hadoop 23.
             // This should go away once MiniMRCluster fix the distributed cache issue.
             Method unsetMethod = null;
diff --git a/test/org/apache/pig/test/TestNativeMapReduce.java b/test/org/apache/pig/test/TestNativeMapReduce.java
index aa8028de8..fa51b3e6c 100644
--- a/test/org/apache/pig/test/TestNativeMapReduce.java
+++ b/test/org/apache/pig/test/TestNativeMapReduce.java
@@ -39,15 +39,15 @@ import org.junit.BeforeClass;
 import org.junit.Test;
 
 public class TestNativeMapReduce  {
-    
+
     //NOTE:
     // Testing NativeMapReduce in LOCAL mode from unit test setup is not easy.
     // (ie current WordCount.jar does not work as-is).
-    // the presence of ~/pigtest/conf/hadoop-site.xml created by MiniCluster
+    // the presence of build/classes/hadoop-site.xml created by MiniCluster
     // in the class path makes the MR job try to contact the MiniCluster.
     // if the MiniCluster shutdown is changed to delete the file, the other
-    // test cases fail because the file in classpath does not exist 
-    
+    // test cases fail because the file in classpath does not exist
+
     // the jar has been created using the source at
     // http://svn.apache.org/repos/asf/hadoop/mapreduce/trunk/src/examples/org/apache/hadoop/examples/WordCount.java:816822
     private String jarFileName = "test//org/apache/pig/test/data/TestWordCount.jar";
@@ -60,12 +60,12 @@ public class TestNativeMapReduce  {
     final static String STOPWORD_FILE = "TestNMapReduceStopwFile";
     static MiniCluster cluster = MiniCluster.buildCluster();
     private PigServer pigServer = null;
-    
+
     /**
      * TODO - Move to runtime jar creation approach
     private void createWordCountJar() {
     }*/
-    
+
     @BeforeClass
     public static void oneTimeSetup() throws Exception{
         String[] input = {
@@ -80,19 +80,19 @@ public class TestNativeMapReduce  {
         String[] stopw = {
                 "one"
         };
-   
+
         Util.createInputFile(cluster, INPUT_FILE, input);
         Util.createLocalInputFile(STOPWORD_FILE, stopw);
     }
 
     //  createWordCountJar(){
-    //  // its a manual process 
-    //  javac -cp build/ivy/lib/Pig/hadoop-core-0.20.2.jar:build/ivy/lib/Pig/commons-cli-1.2.jar test/org/apache/pig/test/utils/WordCount.java 
+    //  // its a manual process
+    //  javac -cp build/ivy/lib/Pig/hadoop-core-0.20.2.jar:build/ivy/lib/Pig/commons-cli-1.2.jar test/org/apache/pig/test/utils/WordCount.java
     //  cd test/
     //  jar -cf WordCount.jar org/apache/pig/test/utils/WordCount*class
     //  mv WordCount.jar org/apache/pig/test/data/TestWordCount.jar
     //
-    //  
+    //
     //}
 
     @Before
@@ -101,7 +101,7 @@ public class TestNativeMapReduce  {
 
         //createWordCountJar();
     }
-    
+
 
 
     @AfterClass
@@ -110,7 +110,7 @@ public class TestNativeMapReduce  {
         new File(STOPWORD_FILE).delete();
         cluster.shutDown();
     }
-      
+
 
     // See PIG-506
     @Test
@@ -145,7 +145,7 @@ public class TestNativeMapReduce  {
                 }
             }
             assertTrue("foundNativeFeature", foundNativeFeature);
-            
+
             // Check the output
             pigServer.registerQuery("C = load 'table_testNativeMRJobSimpleDir';");
 
@@ -187,13 +187,13 @@ public class TestNativeMapReduce  {
         }
     }
 
-    
+
     @Test
     public void testNativeMRJobSimpleFailure() throws Exception{
         try{
             //test if correct return code is obtained when query fails
             // the native MR is writing to an exisiting and should fail
-            
+
             Collection<String> results = new HashSet<String>();
             results.add("(one,1)");
             results.add("(two,2)");
@@ -209,7 +209,7 @@ public class TestNativeMapReduce  {
             pigServer.executeBatch();
 
             assertTrue("job failed", PigStats.get().getReturnCode() != 0);
-   
+
         }
         finally{
             // We have to manually delete intermediate mapreduce files
@@ -218,7 +218,7 @@ public class TestNativeMapReduce  {
         }
     }
 
-    
+
     // See PIG-506
     @Test
     public void testNativeMRJobMultiStoreOnPred() throws Exception{
@@ -354,7 +354,7 @@ public class TestNativeMapReduce  {
             results.add("(2)");
             results.add("(3)");
             results.add("(4)");
-            
+
             pigServer.registerQuery("A = load '" + INPUT_FILE + "';");
             pigServer.registerQuery("B = mapreduce '" + jarFileName + "' " +
                     "Store A into 'table_testNativeMRJobTypeCastInserter_input' "+
@@ -378,7 +378,7 @@ public class TestNativeMapReduce  {
             assertTrue(exp_msg_prefix + t, results.contains(t.toString()));
 
             assertFalse(iter.hasNext());
-        }finally{     
+        }finally{
             Util.deleteFile(cluster,"table_testNativeMRJobTypeCastInserter_input");
             Util.deleteFile(cluster,"table_testNativeMRJobTypeCastInserter_output");
         }
