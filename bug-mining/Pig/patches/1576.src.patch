diff --git a/CHANGES.txt b/CHANGES.txt
index 0c5651b52..4f82ce816 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -84,6 +84,8 @@ OPTIMIZATIONS
  
 BUG FIXES
 
+PIG-4164: After Pig job finish, Pig client spend too much time retry to connect to AM (daijy)
+
 PIG-4195: Support loading char/varchar data in OrcStorage (daijy)
 
 PIG-4201: Native e2e tests fail when run against old version of pig (rohini)
diff --git a/shims/src/hadoop23/org/apache/pig/backend/hadoop/executionengine/shims/HadoopShims.java b/shims/src/hadoop23/org/apache/pig/backend/hadoop/executionengine/shims/HadoopShims.java
index dcacf476b..8b99e69bb 100644
--- a/shims/src/hadoop23/org/apache/pig/backend/hadoop/executionengine/shims/HadoopShims.java
+++ b/shims/src/hadoop23/org/apache/pig/backend/hadoop/executionengine/shims/HadoopShims.java
@@ -32,6 +32,7 @@ import org.apache.hadoop.mapred.TIPStatus;
 import org.apache.hadoop.mapred.TaskReport;
 import org.apache.hadoop.mapred.jobcontrol.Job;
 import org.apache.hadoop.mapred.jobcontrol.JobControl;
+import org.apache.hadoop.mapreduce.Cluster;
 import org.apache.hadoop.mapreduce.ContextFactory;
 import org.apache.hadoop.mapreduce.JobContext;
 import org.apache.hadoop.mapreduce.JobID;
@@ -120,7 +121,8 @@ public class HadoopShims {
 
     public static Counters getCounters(Job job) throws IOException {
         try {
-            return new Counters(job.getJob().getCounters());
+            Cluster cluster = new Cluster(job.getJobConf());
+            return new Counters(cluster.getJob(job.getAssignedJobID()).getCounters());
         } catch (Exception ir) {
             throw new IOException(ir);
         }
@@ -219,8 +221,9 @@ public class HadoopShims {
             LOG.info("TaskReports are disabled for job: " + job.getAssignedJobID());
             return null;
         }
-        org.apache.hadoop.mapreduce.Job mrJob = job.getJob();
+        Cluster cluster = new Cluster(job.getJobConf());
         try {
+            org.apache.hadoop.mapreduce.Job mrJob = cluster.getJob(job.getAssignedJobID());
             org.apache.hadoop.mapreduce.TaskReport[] reports = mrJob.getTaskReports(type);
             return DowngradeHelper.downgradeTaskReports(reports);
         } catch (InterruptedException ir) {
