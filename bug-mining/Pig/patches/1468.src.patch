diff --git a/CHANGES.txt b/CHANGES.txt
index e41eb2ed4..d499bc38b 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -181,6 +181,8 @@ PIG-3882: Multiquery off mode execution is not done in batch and very inefficien
  
 BUG FIXES
 
+PIG-3998: Documentation fix: invalid page links, wrong Groovy udf example (lbendig via cheolsoo)
+
 PIG-4000: Minor documentation fix for PIG-3642 (lbendig via cheolsoo)
 
 PIG-3991: TestErrorHandling.tesNegative7 is broken in trunk/branch-0.13 (cheolsoo)
diff --git a/src/docs/src/documentation/content/xdocs/basic.xml b/src/docs/src/documentation/content/xdocs/basic.xml
index ad5f3a5ff..e969baf0c 100644
--- a/src/docs/src/documentation/content/xdocs/basic.xml
+++ b/src/docs/src/documentation/content/xdocs/basic.xml
@@ -311,7 +311,7 @@ A!B
 <!-- RELATIONS, BAGS, TUPLES, FIELDS-->
    <section id="relations">
    <title>Relations, Bags, Tuples, Fields</title>
-      <p><a href="start.html#pl-Statements">Pig Latin statements</a> work with relations. A relation can be defined as follows:</p>
+      <p><a href="start.html#pl-statements">Pig Latin statements</a> work with relations. A relation can be defined as follows:</p>
    <ul>
       <li>
          <p>A relation is a bag (more specifically, an outer bag).</p>
@@ -1633,7 +1633,7 @@ A = load ‘input’ as (x, y, z);
 B = foreach A generate x+y;
 </source>
 
- <p>If you do <a href="test.html#DESCRIBE">DESCRIBE</a> on B, you will see a single column of type double. This is because Pig makes the safest choice and uses the largest numeric type when the schema is not know. In practice, the input data could contain integer values; however, Pig will cast the data to double and make sure that a double result is returned.</p>
+ <p>If you do <a href="test.html#describe">DESCRIBE</a> on B, you will see a single column of type double. This is because Pig makes the safest choice and uses the largest numeric type when the schema is not know. In practice, the input data could contain integer values; however, Pig will cast the data to double and make sure that a double result is returned.</p>
 
  <p>If the schema of a relation can’t be inferred, Pig will just use the runtime data as is and propagate it through the pipeline.</p>
 
@@ -5767,7 +5767,7 @@ ASSERT A by a0 > 0, 'a0 should be greater than 0';
              <p>Use this feature to specify the Hadoop Partitioner. The partitioner controls the partitioning of the keys of the intermediate map-outputs. </p>
              <ul>
              <li>
-             <p>For more details, see http://hadoop.apache.org/common/docs/r0.20.2/api/org/apache/hadoop/mapred/Partitioner.html</p>
+             <p>For more details, see <a href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/Partitioner.html">http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/Partitioner.html</a></p>
              </li>
              <li>
              <p>For usage, see <a href="#partitionby">Example: PARTITION BY</a></p>
@@ -5781,7 +5781,7 @@ ASSERT A by a0 > 0, 'a0 should be greater than 0';
             </td>
             <td>
                <p>Increase the parallelism of a job by specifying the number of reduce tasks, n. </p>
-               <p>For more information, see <a href="perf.html#Parallel">Use the Parallel Features</a>.</p>
+               <p>For more information, see <a href="perf.html#parallel">Use the Parallel Features</a>.</p>
             </td>
          </tr> 
    </table></section>
@@ -5906,7 +5906,7 @@ DUMP X;
                </td>
                <td>
                   <p>Increase the parallelism of a job by specifying the number of reduce tasks, n.</p>
-                  <p>For more information, see <a href="perf.html#Parallel">Use the Parallel Features</a>.</p>
+                  <p>For more information, see <a href="perf.html#parallel">Use the Parallel Features</a>.</p>
                </td>
             </tr>
          </table>
@@ -6058,7 +6058,7 @@ state: chararray,city: chararray,sales: long)}}</source>
              <p>Use this feature to specify the Hadoop Partitioner. The partitioner controls the partitioning of the keys of the intermediate map-outputs. </p>
              <ul>
              <li>
-             <p>For more details, see http://hadoop.apache.org/common/docs/r0.20.2/api/org/apache/hadoop/mapred/Partitioner.html</p>
+             <p>For more details, see <a href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/Partitioner.html">http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/Partitioner.html</a></p>
              </li>
              <li>
               <p>For usage, see <a href="#partitionby">Example: PARTITION BY</a>.</p>
@@ -6073,7 +6073,7 @@ state: chararray,city: chararray,sales: long)}}</source>
             </td>
             <td>
                <p>Increase the parallelism of a job by specifying the number of reduce tasks, n.</p>
-               <p>For more information, see <a href="perf.html#Parallel">Use the Parallel Features</a>.</p>
+               <p>For more information, see <a href="perf.html#parallel">Use the Parallel Features</a>.</p>
             </td>
          </tr> 
    </table>
@@ -6700,7 +6700,7 @@ DUMP X;
              <p>Use this feature to specify the Hadoop Partitioner. The partitioner controls the partitioning of the keys of the intermediate map-outputs. </p>
              <ul>
              <li>
-             <p>For more details, see http://hadoop.apache.org/common/docs/r0.20.2/api/org/apache/hadoop/mapred/Partitioner.html</p>
+             <p>For more details, see <a href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/Partitioner.html">http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/Partitioner.html</a></p>
              </li>
              <li>
              <p>For usage, see <a href="#partitionby">Example: PARTITION BY</a></p>
@@ -6968,7 +6968,7 @@ public class SimpleCustomPartitioner extends Partitioner &lt;PigNullableWritable
                <p>'replicated'</p>
             </td>
             <td>
-               <p>Use to perform replicated joins (see <a href="perf.html#Replicated-Joins">Replicated Joins</a>).</p>
+               <p>Use to perform replicated joins (see <a href="perf.html#replicated-joins">Replicated Joins</a>).</p>
             </td>
          </tr>
          
@@ -6977,7 +6977,7 @@ public class SimpleCustomPartitioner extends Partitioner &lt;PigNullableWritable
                <p>'skewed'</p>
             </td>
             <td>
-               <p>Use to perform skewed joins (see <a href="perf.html#Skewed-Joins">Skewed Joins</a>).</p>
+               <p>Use to perform skewed joins (see <a href="perf.html#skewed-joins">Skewed Joins</a>).</p>
             </td>
          </tr>
          
@@ -6986,7 +6986,7 @@ public class SimpleCustomPartitioner extends Partitioner &lt;PigNullableWritable
                <p>'merge'</p>
             </td>
             <td>
-               <p>Use to perform merge joins (see <a href="perf.html#Merge-Joins">Merge Joins</a>).</p>
+               <p>Use to perform merge joins (see <a href="perf.html#merge-joins">Merge Joins</a>).</p>
             </td>
          </tr>
          
@@ -6995,7 +6995,7 @@ public class SimpleCustomPartitioner extends Partitioner &lt;PigNullableWritable
                <p>'merge-sparse'</p>
             </td>
             <td>
-               <p>Use to perform merge-sparse joins (see <a href="perf.html#Merge-sparse-Joins">Merge-Sparse Joins</a>).</p>
+               <p>Use to perform merge-sparse joins (see <a href="perf.html#merge-sparse-joins">Merge-Sparse Joins</a>).</p>
             </td>
          </tr>         
          
@@ -7007,8 +7007,7 @@ public class SimpleCustomPartitioner extends Partitioner &lt;PigNullableWritable
              <p>Use this feature to specify the Hadoop Partitioner. The partitioner controls the partitioning of the keys of the intermediate map-outputs. </p>
              <ul>
              <li>
-             <p>For more details, see http://hadoop.apache.org/common/docs/r0.20.2/api/org/apache/hadoop/mapred/Partitioner.html</p>
-             </li>
+             <p>For more details, see <a href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/Partitioner.html">http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/Partitioner.html</a></p>             </li>
              <li>
               <p>For usage, see <a href="#partitionby">Example: PARTITION BY</a></p>
              </li>
@@ -7024,7 +7023,7 @@ public class SimpleCustomPartitioner extends Partitioner &lt;PigNullableWritable
             </td>
             <td>
                <p>Increase the parallelism of a job by specifying the number of reduce tasks, n. </p>
-               <p>For more information, see <a href="perf.html#Parallel">Use the Parallel Features</a>.</p>
+               <p>For more information, see <a href="perf.html#parallel">Use the Parallel Features</a>.</p>
             </td>
          </tr> 
    </table></section>
@@ -7194,7 +7193,7 @@ DUMP X;
                <p>'replicated'</p>
             </td>
             <td>
-               <p>Use to perform replicated joins (see <a href="perf.html#Replicated-Joins">Replicated Joins</a>).</p>
+               <p>Use to perform replicated joins (see <a href="perf.html#replicated-joins">Replicated Joins</a>).</p>
                <p>Only left outer join is supported for replicated joins.</p>
             </td>
          </tr>
@@ -7204,7 +7203,7 @@ DUMP X;
                <p>'skewed'</p>
             </td>
             <td>
-               <p>Use to perform skewed joins (see <a href="perf.html#Skewed-Joins">Skewed Joins</a>).</p>
+               <p>Use to perform skewed joins (see <a href="perf.html#skewed-joins">Skewed Joins</a>).</p>
             </td>
          </tr>
 
@@ -7213,7 +7212,7 @@ DUMP X;
                <p>'merge'</p>
             </td>
             <td>
-               <p>Use to perform merge joins (see <a href="perf.html#Merge-Joins">Merge Joins</a>).</p>
+               <p>Use to perform merge joins (see <a href="perf.html#merge-joins">Merge Joins</a>).</p>
             </td>
          </tr>
          
@@ -7226,7 +7225,7 @@ DUMP X;
              <p>Use this feature to specify the Hadoop Partitioner. The partitioner controls the partitioning of the keys of the intermediate map-outputs. </p>
              <ul>
              <li>
-             <p>For more details, see http://hadoop.apache.org/common/docs/r0.20.2/api/org/apache/hadoop/mapred/Partitioner.html</p>
+             <p>For more details, see <a href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/Partitioner.html">http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/Partitioner.html</a></p>
              </li>
              <li>
               <p>For usage, see <a href="#partitionby">Example: PARTITION BY</a></p>
@@ -7243,7 +7242,7 @@ DUMP X;
             </td>
             <td>
                <p>Increase the parallelism of a job by specifying the number of reduce tasks, n. </p>
-               <p>For more information, see <a href="perf.html#Parallel">Use the Parallel Features</a>.</p>
+               <p>For more information, see <a href="perf.html#parallel">Use the Parallel Features</a>.</p>
             </td>
          </tr>
     
@@ -7463,7 +7462,7 @@ DUMP X;
                   <li>
                   
                   
-                     <p>You can use a built in function (see <a href="func.html#Load-Store-Functions">Load/Store Functions</a>). PigStorage is the default load function and does not need to be specified (simply omit the USING clause).</p>
+                     <p>You can use a built in function (see <a href="func.html#load-store-functions">Load/Store Functions</a>). PigStorage is the default load function and does not need to be specified (simply omit the USING clause).</p>
                   </li>
                   <li>
                      <p>You can write your own load function  
@@ -8954,7 +8953,7 @@ B = FOREACH A GENERATE myFunc($0);
    <title>Usage</title>
    <p><strong>Pig Scripts</strong></p>
    
-   <p>Use the REGISTER statement inside a Pig script to specify a JAR file or a Python/JavaScript module. Pig supports JAR files and modules stored in local file systems as well as remote, distributed file systems such as HDFS and Amazon S3 (see <a href="start.html#Pig-Scripts">Pig Scripts</a>).</p>
+   <p>Use the REGISTER statement inside a Pig script to specify a JAR file or a Python/JavaScript module. Pig supports JAR files and modules stored in local file systems as well as remote, distributed file systems such as HDFS and Amazon S3 (see <a href="start.html#pig-scripts">Pig Scripts</a>).</p>
    
    <p id="register-glob">Additionally, JAR files stored in local file systems can be specified as a glob pattern using “*”. Pig will search for matching jars in the local file system, either the relative path (relative to your working directory) or the absolute path. Pig will pick up all JARs that match the glob.</p>
    
diff --git a/src/docs/src/documentation/content/xdocs/cmds.xml b/src/docs/src/documentation/content/xdocs/cmds.xml
index 5f109fc70..b6ec954ef 100644
--- a/src/docs/src/documentation/content/xdocs/cmds.xml
+++ b/src/docs/src/documentation/content/xdocs/cmds.xml
@@ -72,7 +72,7 @@
    The fs command greatly extends the set of supported file system commands and the capabilities
    supported for existing commands such as ls that will now support globing. For a complete list of
    FsShell commands, see 
-   <a href="http://hadoop.apache.org/common/docs/current/file_system_shell.html">File System Shell Guide</a></p>
+   <a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html">File System Shell Guide</a></p>
    </section>
    
    <section>
@@ -652,7 +652,7 @@ grunt&gt; run –param out=myoutput myscript.pig
             </td>
             <td>
                <p>Sets the number of reducers for all MapReduce jobs generated by Pig 
-              (see  <a href="perf.html#Parallel">Use the Parallel Features</a>).</p>
+              (see  <a href="perf.html#parallel">Use the Parallel Features</a>).</p>
             </td>
          </tr>
          <tr>
@@ -698,7 +698,7 @@ grunt&gt; run –param out=myoutput myscript.pig
                <p>String that contains the path.</p>
             </td>
             <td>
-               <p>For streaming, sets the path from which not to ship data (see <a href="basic.html#DEFINE-udfs">DEFINE (UDFs, streaming)</a> and <a href="basic.html#autoship"> About Auto-Ship</a>).</p>
+               <p>For streaming, sets the path from which not to ship data (see <a href="basic.html#define-udfs">DEFINE (UDFs, streaming)</a> and <a href="basic.html#autoship"> About Auto-Ship</a>).</p>
             </td>
             </tr>
    </table>
diff --git a/src/docs/src/documentation/content/xdocs/cont.xml b/src/docs/src/documentation/content/xdocs/cont.xml
index 3da22db34..7680e395d 100644
--- a/src/docs/src/documentation/content/xdocs/cont.xml
+++ b/src/docs/src/documentation/content/xdocs/cont.xml
@@ -28,7 +28,7 @@
    
 <p>To enable control flow, you can embed Pig Latin statements and Pig commands in the Python, JavaScript and Groovy scripting languages using a JDBC-like compile, bind, run model. For Python, make sure the Jython jar is included in your class path. For JavaScript, make sure the Rhino jar is included in your classpath. For Groovy, make sure the groovy-all jar is included in your classpath.</p>
 
-<p>Note that host languages and the languages of UDFs (included as part of the embedded Pig) are completely orthogonal. For example, a Pig Latin statement that registers a Python UDF may be embedded in Python, JavaScript, or Java. The exception to this rule is "combined" scripts – here the languages must match (see the <a href="udf.html#python-advanced">Advanced Topics for Python</a>, <a href="udf.html#js-advanced">Advanced Topics for JavaScript</a> and <a href="udf.html#groovy-advanced">Advanced Topics for Groovy</a>). </p>
+<p>Note that host languages and the languages of UDFs (included as part of the embedded Pig) are completely orthogonal. For example, a Pig Latin statement that registers a Python UDF may be embedded in Python, JavaScript, or Java. The exception to this rule is "combined" scripts – here the languages must match (see the <a href="udf.html#jython-advanced">Advanced Topics for Python</a>, <a href="udf.html#js-advanced">Advanced Topics for JavaScript</a> and <a href="udf.html#groovy-advanced">Advanced Topics for Groovy</a>). </p>
 
 
 <!-- ============================================== -->
@@ -818,11 +818,11 @@ public interface PigProgressNotificationListener extends java.util.EventListener
 
 <p>To enable control flow, you can embed Pig Latin statements and Pig commands in the Java programming language. </p>
 
-<p>Note that host languages and the languages of UDFs (included as part of the embedded Pig) are completely orthogonal. For example, a Pig Latin statement that registers a Java UDF may be embedded in Python, JavaScript, Groovy, or Java. The exception to this rule is "combined" scripts – here the languages must match (see the <a href="udf.html#python-advanced">Advanced Topics for Python</a>, <a href="udf.html#js-advanced">Advanced Topics for JavaScript</a> and <a href="udf.html#groovy-advanced">Advanced Topics for Groovy</a>). </p>
+<p>Note that host languages and the languages of UDFs (included as part of the embedded Pig) are completely orthogonal. For example, a Pig Latin statement that registers a Java UDF may be embedded in Python, JavaScript, Groovy, or Java. The exception to this rule is "combined" scripts – here the languages must match (see the <a href="udf.html#jython-advanced">Advanced Topics for Python</a>, <a href="udf.html#js-advanced">Advanced Topics for JavaScript</a> and <a href="udf.html#groovy-advanced">Advanced Topics for Groovy</a>). </p>
 
 <section id="pigserver">
 <title>PigServer Interface</title>
-<p>Currently, <a href="http://pig.apache.org/docs/r0.10.0/api/org/apache/pig/PigServer.html">PigServer</a> is the main interface for embedding Pig in Java. PigServer can now be instantiated from multiple threads. (In the past, PigServer contained references to static data that prevented multiple instances of the object to be created from different threads within your application.) Please note that PigServer is NOT thread safe; the same object can't be shared across multiple threads. </p>
+<p>Currently, <a href="http://pig.apache.org/docs/r0.13.0/api/org/apache/pig/PigServer.html">PigServer</a> is the main interface for embedding Pig in Java. PigServer can now be instantiated from multiple threads. (In the past, PigServer contained references to static data that prevented multiple instances of the object to be created from different threads within your application.) Please note that PigServer is NOT thread safe; the same object can't be shared across multiple threads. </p>
 </section>
 
 <section>
diff --git a/src/docs/src/documentation/content/xdocs/func.xml b/src/docs/src/documentation/content/xdocs/func.xml
index 65269477d..4cb35656c 100644
--- a/src/docs/src/documentation/content/xdocs/func.xml
+++ b/src/docs/src/documentation/content/xdocs/func.xml
@@ -102,7 +102,7 @@ decoded_strings = FOREACH encoded_strings GENERATE UrlDecode(encoded, 'UTF-8');
    
    <section>
    <title>Example</title>
-   <p>In this example the average GPA for each student is computed (see the <a href="basic.html#GROUP">GROUP</a> operator for information about the field names in relation B).</p>
+   <p>In this example the average GPA for each student is computed (see the <a href="basic.html#group">GROUP</a> operator for information about the field names in relation B).</p>
 <source>
 A = LOAD 'student.txt' AS (name:chararray, term:chararray, gpa:float);
 
@@ -401,7 +401,7 @@ DUMP X;
    
    <section>
    <title>Example</title>
-   <p>In this example the tuples in the bag are counted (see the <a href="basic.html#GROUP">GROUP</a> operator for information about the field names in relation B).</p>
+   <p>In this example the tuples in the bag are counted (see the <a href="basic.html#group">GROUP</a> operator for information about the field names in relation B).</p>
 <source>
 A = LOAD 'data' AS (f1:int,f2:int,f3:int);
 
diff --git a/src/docs/src/documentation/content/xdocs/perf.xml b/src/docs/src/documentation/content/xdocs/perf.xml
index 523b06d39..4df012d0b 100644
--- a/src/docs/src/documentation/content/xdocs/perf.xml
+++ b/src/docs/src/documentation/content/xdocs/perf.xml
@@ -61,7 +61,7 @@ explain C;
 </ul>
 <p></p>
 
-<p>You can check if the combiner is used for your query by running <a href="test.html#EXPLAIN">EXPLAIN</a> on the FOREACH alias as shown above. You should see the combine section in the MapReduce part of the plan:</p>
+<p>You can check if the combiner is used for your query by running <a href="test.html#explain">EXPLAIN</a> on the FOREACH alias as shown above. You should see the combine section in the MapReduce part of the plan:</p>
 
 
 
@@ -89,7 +89,7 @@ B: Local Rearrange[tuple]{bytearray}(false) - scope-42
 </source>
 
 <p>The combiner is also used with a nested foreach as long as the only nested operation used is DISTINCT
-(see <a href="basic.html#FOREACH">FOREACH</a> and <a href="basic.html#nestedblock">Example: Nested Block</a>).
+(see <a href="basic.html#foreach">FOREACH</a> and <a href="basic.html#nestedblock">Example: Nested Block</a>).
 </p>
 
 <source>
@@ -226,7 +226,7 @@ $ pig -no_multiquery myscript.pig
 <li>
 <p>For batch mode execution, the entire script is first parsed to determine if intermediate tasks 
 can be combined to reduce the overall amount of work that needs to be done; execution starts only after the parsing is completed 
-(see the <a href="test.html#EXPLAIN">EXPLAIN</a> operator and the <a href="cmds.html#run">run</a> and <a href="cmds.html#exec">exec</a> commands). </p>
+(see the <a href="test.html#explain">EXPLAIN</a> operator and the <a href="cmds.html#run">run</a> and <a href="cmds.html#exec">exec</a> commands). </p>
 
 </li>
 <li>
@@ -294,8 +294,8 @@ With multi-query execution, the script will process A and dump A' as a side-effe
 <!-- ++++++++++++++++++++++++++++++++++++++++++ -->
 <section id="store-dump">
 	<title>Store vs. Dump</title>
-	<p>With multi-query exection, you want to use <a href="basic.html#STORE">STORE</a> to save (persist) your results. 
-	You do not want to use <a href="test.html#DUMP">DUMP</a> as it will disable multi-query execution and is likely to slow down execution. (If you have included DUMP statements in your scripts for debugging purposes, you should remove them.) </p>
+	<p>With multi-query exection, you want to use <a href="basic.html#store">STORE</a> to save (persist) your results. 
+	You do not want to use <a href="test.html#dump">DUMP</a> as it will disable multi-query execution and is likely to slow down execution. (If you have included DUMP statements in your scripts for debugging purposes, you should remove them.) </p>
 	
 	<p>DUMP Example: In this script, because the DUMP command is interactive, the multi-query execution will be disabled and two separate jobs will be created to execute this script. The first job will execute A > B > DUMP while the second job will execute A > B > C > STORE.</p>
 	
@@ -699,7 +699,7 @@ B = GROUP A all PARALLEL 10;
 
 <section>
 <title>Use Optimization</title>
-<p>Pig supports various <a href="perf.html#Optimization-Rules">optimization rules</a> which are turned on by default. 
+<p>Pig supports various <a href="perf.html#optimization-rules">optimization rules</a> which are turned on by default. 
 Become familiar with these rules.</p>
 </section>
 
@@ -833,7 +833,7 @@ C = foreach B generate group, MyUDF(A);
 <section id="accumulator-interface">
 <title>Use the Accumulator Interface</title>
 <p>
-If your UDF can't be made Algebraic but is able to deal with getting input in chunks rather than all at once, consider implementing the Accumulator  interface to reduce the amount of memory used by your script. If your function <em>is</em> Algebraic and can be used on conjunction with Accumulator functions, you will need to implement the Accumulator interface as well as the Algebraic interface. For more information, see <a href="udf.html#Accumulator-Interface">Accumulator Interface</a>.</p>
+If your UDF can't be made Algebraic but is able to deal with getting input in chunks rather than all at once, consider implementing the Accumulator  interface to reduce the amount of memory used by your script. If your function <em>is</em> Algebraic and can be used on conjunction with Accumulator functions, you will need to implement the Accumulator interface as well as the Algebraic interface. For more information, see <a href="udf.html#accumulator-interface">Accumulator Interface</a>.</p>
 
 <p><strong>Note:</strong> Pig automatically chooses the interface that it expects to provide the best performance: Algebraic &gt; Accumulator &gt; Default. </p>
 
@@ -889,7 +889,7 @@ C = join small by t, large by x;
 
 <p><strong>Specialized Join Optimizations</strong></p>
 <p>Optimization can also be achieved using fragment replicate joins, skewed joins, and merge joins. 
-For more information see <a href="perf.html#Specialized-Joins">Specialized Joins</a>.</p>
+For more information see <a href="perf.html#specialized-joins">Specialized Joins</a>.</p>
 
 </section>
 
@@ -906,13 +906,13 @@ For more information see <a href="perf.html#Specialized-Joins">Specialized Joins
 <p>Alternatively, use the PARALLEL clause to set the number of reducers at the operator level. 
 (In a script, the value set via the PARALLEL clause will override any value set via "set default parallel.")
 You can include the PARALLEL clause with any operator that starts a reduce phase:  
-<a href="basic.html#COGROUP">COGROUP</a>, 
-<a href="basic.html#CROSS">CROSS</a>, 
-<a href="basic.html#DISTINCT">DISTINCT</a>, 
-<a href="basic.html#GROUP">GROUP</a>, 
-<a href="basic.html#JOIN-inner">JOIN (inner)</a>, 
-<a href="basic.html#JOIN-outer">JOIN (outer)</a>, and
-<a href="basic.html#ORDER-BY">ORDER BY</a>.
+<a href="basic.html#cogroup">COGROUP</a>, 
+<a href="basic.html#cross">CROSS</a>, 
+<a href="basic.html#distinct">DISTINCT</a>, 
+<a href="basic.html#group">GROUP</a>, 
+<a href="basic.html#join-inner">JOIN (inner)</a>, 
+<a href="basic.html#join-outer">JOIN (outer)</a>, and
+<a href="basic.html#order-by">ORDER BY</a>.
 </p>
 
 <p>The number of reducers you need for a particular construct in Pig that forms a MapReduce boundary depends entirely on (1) your data and the number of intermediate keys you are generating in your mappers and (2) the partitioner and distribution of map (combiner) output keys. In the best cases we have seen that a reducer processing about 1 GB of data behaves efficiently.</p>
@@ -1038,7 +1038,7 @@ java -cp $PIG_HOME/pig.jar
 </ul>
 <p></p>
 
-<p>This feature works with <a href="func.html#PigStorage">PigStorage</a>. However, if you are using a custom loader, please note the following:</p>
+<p>This feature works with <a href="func.html#pigstorage">PigStorage</a>. However, if you are using a custom loader, please note the following:</p>
 
 <ul>
 <li>If your loader implementation makes use of the PigSplit object passed through the prepareToRead method, then you may need to rebuild the loader since the definition of PigSplit has been modified. </li>
@@ -1130,7 +1130,7 @@ don't, the process fails and an error is generated.</p>
  
 <section>
 <title>Usage</title>
-<p>Perform a replicated join with the USING clause (see <a href="basic.html#JOIN-inner">JOIN (inner)</a> and <a href="basic.html#JOIN-outer">JOIN (outer)</a>).
+<p>Perform a replicated join with the USING clause (see <a href="basic.html#join-inner">JOIN (inner)</a> and <a href="basic.html#join-outer">JOIN (outer)</a>).
 In this example, a large relation is joined with two smaller relations. Note that the large relation comes first followed by the smaller relations; 
 and, all small relations together must fit into main memory, otherwise an error is generated. </p>
 <source>
@@ -1176,7 +1176,7 @@ associated with a given key is too large to fit in memory.
 
 <section>
 <title>Usage</title>
-<p>Perform a skewed join with the USING clause (see <a href="basic.html#JOIN-inner">JOIN (inner)</a> and <a href="basic.html#JOIN-outer">JOIN (outer)</a>). </p>
+<p>Perform a skewed join with the USING clause (see <a href="basic.html#join-inner">JOIN (inner)</a> and <a href="basic.html#join-outer">JOIN (outer)</a>). </p>
 <source>
 A = LOAD 'skewed_data' AS (a1,a2,a3);
 B = LOAD 'data' AS (b1,b2,b3);
@@ -1233,7 +1233,7 @@ and the right input of the join to be the side file. It then samples records fro
 
 <section>
 <title>Usage</title>
-<p>Perform a merge join with the USING clause (see <a href="basic.html#JOIN-inner">JOIN (inner)</a> and <a href="basic.html#JOIN-outer">JOIN (outer)</a>). </p>
+<p>Perform a merge join with the USING clause (see <a href="basic.html#join-inner">JOIN (inner)</a> and <a href="basic.html#join-outer">JOIN (outer)</a>). </p>
 <source>
 C = JOIN A BY a1, B BY b1, C BY c1 USING 'merge';
 </source>
@@ -1286,7 +1286,7 @@ C = JOIN A BY a1, B BY b1, C BY c1 USING 'merge';
 
 <section>
 <title>Usage</title>
-<p>Perform a merge-sparse join with the USING clause (see <a href="basic.html#JOIN-inner">JOIN (inner)</a>). </p>
+<p>Perform a merge-sparse join with the USING clause (see <a href="basic.html#join-inner">JOIN (inner)</a>). </p>
 <source>
 a = load 'sorted_input1' using org.apache.pig.piggybank.storage.IndexedStorage('\t', '0');
 b = load 'sorted_input2' using org.apache.pig.piggybank.storage.IndexedStorage('\t', '0');
diff --git a/src/docs/src/documentation/content/xdocs/start.xml b/src/docs/src/documentation/content/xdocs/start.xml
index 804d526d7..3e711eda5 100644
--- a/src/docs/src/documentation/content/xdocs/start.xml
+++ b/src/docs/src/documentation/content/xdocs/start.xml
@@ -270,7 +270,7 @@ DUMP B;  -- retrieving results
 
 <p id="dfs"><strong>Scripts and Distributed File Systems</strong></p>
 
-<p>Pig supports running scripts (and Jar files) that are stored in HDFS, Amazon S3, and other distributed file systems. The script's full location URI is required (see <a href="basic.html#REGISTER">REGISTER</a> for information about Jar files). For example, to run a Pig script on HDFS, do the following:</p>
+<p>Pig supports running scripts (and Jar files) that are stored in HDFS, Amazon S3, and other distributed file systems. The script's full location URI is required (see <a href="basic.html#register">REGISTER</a> for information about Jar files). For example, to run a Pig script on HDFS, do the following:</p>
 <source>
 $ pig hdfs://nn.mydomain.com:9020/myscripts/script.pig
 </source> 
@@ -286,7 +286,7 @@ $ pig hdfs://nn.mydomain.com:9020/myscripts/script.pig
    <p>Pig Latin statements are the basic constructs you use to process data using Pig. 
    A Pig Latin statement is an operator that takes a <a href="basic.html#relations">relation</a> as input and produces another relation as output. 
    (This definition applies to all Pig Latin operators except LOAD and STORE which read data from and write data to the file system.) 
-   Pig Latin statements may include <a href="basic.html#Expressions">expressions</a> and <a href="basic.html#Schemas">schemas</a>. 
+   Pig Latin statements may include <a href="basic.html#expressions">expressions</a> and <a href="basic.html#schemas">schemas</a>. 
    Pig Latin statements can span multiple lines and must end with a semi-colon ( ; ).  
    By default, Pig Latin statements are processed using <a href="perf.html#multi-query-execution">multi-query execution</a>.  
  </p>
@@ -330,7 +330,7 @@ DUMP B;
    <!-- ++++++++++++++++++++++++++++++++++ -->   
    <section id="data-load">
    <title>Loading Data</title>
-   <p>Use the  <a href="basic.html#LOAD">LOAD</a> operator and the <a href="udf.html#load-store-functions">load/store functions</a> to read data into Pig (PigStorage is the default load function).</p>
+   <p>Use the  <a href="basic.html#load">LOAD</a> operator and the <a href="udf.html#load-store-functions">load/store functions</a> to read data into Pig (PigStorage is the default load function).</p>
    </section>
   
    <!-- ++++++++++++++++++++++++++++++++++ -->   
@@ -339,19 +339,19 @@ DUMP B;
    <p>Pig allows you to transform data in many ways. As a starting point, become familiar with these operators:</p>
    <ul>
       <li>
-         <p>Use the <a href="basic.html#FILTER">FILTER</a> operator to work with tuples or rows of data. 
-         Use the <a href="basic.html#FOREACH">FOREACH</a> operator to work with columns of data.</p>
+         <p>Use the <a href="basic.html#filter">FILTER</a> operator to work with tuples or rows of data. 
+         Use the <a href="basic.html#foreach">FOREACH</a> operator to work with columns of data.</p>
       </li>
       <li>
-         <p>Use the <a href="basic.html#GROUP ">GROUP</a> operator to group data in a single relation. 
-         Use the <a href="basic.html#COGROUP ">COGROUP</a>,
+         <p>Use the <a href="basic.html#group ">GROUP</a> operator to group data in a single relation. 
+         Use the <a href="basic.html#cogroup ">COGROUP</a>,
          <a href="basic.html#join-inner">inner JOIN</a>, and
          <a href="basic.html#join-outer">outer JOIN</a>
          operators  to group or join data in two or more relations.</p>
       </li>
       <li>
-         <p>Use the <a href="basic.html#UNION">UNION</a> operator to merge the contents of two or more relations. 
-         Use the <a href="basic.html#SPLIT">SPLIT</a> operator to partition the contents of a relation into multiple relations.</p>
+         <p>Use the <a href="basic.html#union">UNION</a> operator to merge the contents of two or more relations. 
+         Use the <a href="basic.html#split">SPLIT</a> operator to partition the contents of a relation into multiple relations.</p>
       </li>
    </ul>
    </section>
@@ -368,10 +368,10 @@ DUMP B;
    
     <section id="data-results">
    <title>Storing Final Results</title>
-   <p>Use the  <a href="basic.html#STORE">STORE</a> operator and the <a href="udf.html#load-store-functions">load/store functions</a> 
+   <p>Use the  <a href="basic.html#store">STORE</a> operator and the <a href="udf.html#load-store-functions">load/store functions</a> 
    to write results to the file system (PigStorage is the default store function). </p>
 <p><strong>Note:</strong> During the testing/debugging phase of your implementation, you can use DUMP to display results to your terminal screen. 
-However, in a production environment you always want to use the STORE operator to save your results (see <a href="perf.html#Store-Dump">Store vs. Dump</a>).</p>   
+However, in a production environment you always want to use the STORE operator to save your results (see <a href="perf.html#store-dump">Store vs. Dump</a>).</p>   
    </section> 
 
  <!-- ++++++++++++++++++++++++++++++++++ -->     
diff --git a/src/docs/src/documentation/content/xdocs/test.xml b/src/docs/src/documentation/content/xdocs/test.xml
index 9e3ee2aa4..af0d63d87 100644
--- a/src/docs/src/documentation/content/xdocs/test.xml
+++ b/src/docs/src/documentation/content/xdocs/test.xml
@@ -159,7 +159,7 @@ D: {age: bytearray}
    
    <p>
    Note that production scripts SHOULD NOT use DUMP as it will disable multi-query optimizations and is likely to slow down execution 
-   (see <a href="perf.html#Store-Dump">Store vs. Dump</a>).
+   (see <a href="perf.html#store-dump">Store vs. Dump</a>).
    </p>
    </section>
    
@@ -846,7 +846,7 @@ $pig_trunk ant pigunit-jar
       
       <p>The example included here computes the top N of the most common queries. 
         The Pig script, top_queries.pig, is similar to the 
-        <a href="start.html#Pig-Script-1">Query Phrase Popularity</a> 
+        <a href="start.html#pig-script-1">Query Phrase Popularity</a> 
         in the Pig tutorial. It expects an input a file of queries and a parameter n (n is 2 in our case in order to do a top 2). 
       </p>
       
diff --git a/src/docs/src/documentation/content/xdocs/udf.xml b/src/docs/src/documentation/content/xdocs/udf.xml
index 9f1056125..97070b8cb 100644
--- a/src/docs/src/documentation/content/xdocs/udf.xml
+++ b/src/docs/src/documentation/content/xdocs/udf.xml
@@ -1817,9 +1817,9 @@ outputSchema "t:(m:[], t:(name:chararray, age:int, gpa:double), b:{t:(name:chara
 </source>
 <p>@OutputSchemaFunction annotation - Defines the name of a function which will return the schema at runtime according to the input schema.</p>
 <source>
-import org.apache.pig.scripting.groovy.OutputSchemaFunction;",
+import org.apache.pig.scripting.groovy.OutputSchemaFunction;
 
-class GroovyUDFs {",
+class GroovyUDFs {
   @OutputSchemaFunction('squareSchema')
   public static square(x) {
     return x * x;
