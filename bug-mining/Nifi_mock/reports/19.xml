<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Sat Nov 08 20:46:07 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[NIFI-2865] Address issues of PublishKafka blocking when having trouble communicating with Kafka broker and improve performance</title>
                <link>https://issues.apache.org/jira/browse/NIFI-2865</link>
                <project id="12316020" key="NIFI">Apache NiFi</project>
                    <description>&lt;p&gt;When NiFi is unable to communicate properly with the Kafka broker, we see the NiFi threads occasionally block. This should be resolvable by calling the wakeup() method of the client. Additionally, if Kafka takes too long to respond, we should be able to route the FlowFile to failure and move on.&lt;/p&gt;

&lt;p&gt;PublishKafka has a nice feature that allows a demarcated stream to be sent as separate messages, so that a large number of messages can be sent as a single FlowFile. However, in the case of individual messages per FlowFile, the performance could be improved by batching together multiple FlowFiles per session&lt;/p&gt;</description>
                <environment></environment>
        <key id="13009663">NIFI-2865</key>
            <summary>Address issues of PublishKafka blocking when having trouble communicating with Kafka broker and improve performance</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="markap14">Mark Payne</assignee>
                                    <reporter username="markap14">Mark Payne</reporter>
                        <labels>
                    </labels>
                <created>Tue, 4 Oct 2016 19:22:22 +0000</created>
                <updated>Thu, 13 Oct 2016 18:54:31 +0000</updated>
                            <resolved>Thu, 6 Oct 2016 20:12:44 +0000</resolved>
                                                    <fixVersion>1.1.0</fixVersion>
                                    <component>Extensions</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="15546630" author="githubbot" created="Tue, 4 Oct 2016 20:53:29 +0000"  >&lt;p&gt;GitHub user markap14 opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/nifi/pull/1097&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/nifi/pull/1097&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/NIFI-2865&quot; title=&quot;Address issues of PublishKafka blocking when having trouble communicating with Kafka broker and improve performance&quot; class=&quot;issue-link&quot; data-issue-key=&quot;NIFI-2865&quot;&gt;&lt;del&gt;NIFI-2865&lt;/del&gt;&lt;/a&gt;: Refactored PublishKafka and PublishKafka_0_10 to allow bat&#8230;&lt;/p&gt;

&lt;p&gt;    &#8230;ching of FlowFiles within a single publish and to let messages timeout if not acknowledged&lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/markap14/nifi&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/markap14/nifi&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/NIFI-2865&quot; title=&quot;Address issues of PublishKafka blocking when having trouble communicating with Kafka broker and improve performance&quot; class=&quot;issue-link&quot; data-issue-key=&quot;NIFI-2865&quot;&gt;&lt;del&gt;NIFI-2865&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/nifi/pull/1097.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/nifi/pull/1097.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #1097&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 5b10a50ff34150c2642b544e4ee7a855c080b285&lt;br/&gt;
Author: Mark Payne &amp;lt;markap14@hotmail.com&amp;gt;&lt;br/&gt;
Date:   2016-09-08T23:37:35Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/NIFI-2865&quot; title=&quot;Address issues of PublishKafka blocking when having trouble communicating with Kafka broker and improve performance&quot; class=&quot;issue-link&quot; data-issue-key=&quot;NIFI-2865&quot;&gt;&lt;del&gt;NIFI-2865&lt;/del&gt;&lt;/a&gt;: Refactored PublishKafka and PublishKafka_0_10 to allow batching of FlowFiles within a single publish and to let messages timeout if not acknowledged&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="15549545" author="githubbot" created="Wed, 5 Oct 2016 18:16:51 +0000"  >&lt;p&gt;Github user bbende commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/nifi/pull/1097&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/nifi/pull/1097&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Reviewing...&lt;/p&gt;</comment>
                            <comment id="15549944" author="githubbot" created="Wed, 5 Oct 2016 21:05:10 +0000"  >&lt;p&gt;Github user bbende commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/nifi/pull/1097#discussion_r82070115&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/nifi/pull/1097#discussion_r82070115&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: nifi-nar-bundles/nifi-kafka-bundle/nifi-kafka-0-10-processors/src/main/java/org/apache/nifi/processors/kafka/pubsub/PublishKafka_0_10.java &amp;#8212;&lt;br/&gt;
    @@ -249,242 +241,141 @@&lt;/p&gt;

&lt;p&gt;         @Override&lt;br/&gt;
         protected List&amp;lt;PropertyDescriptor&amp;gt; getSupportedPropertyDescriptors() &lt;/p&gt;
{
    -        return DESCRIPTORS;
    +        return PROPERTIES;
         }

&lt;p&gt;         @Override&lt;br/&gt;
         protected PropertyDescriptor getSupportedDynamicPropertyDescriptor(final String propertyDescriptorName) &lt;/p&gt;
{
             return new PropertyDescriptor.Builder()
    -                .description(&quot;Specifies the value for &apos;&quot; + propertyDescriptorName + &quot;&apos; Kafka Configuration.&quot;)
    -                .name(propertyDescriptorName).addValidator(new KafkaProcessorUtils.KafkaConfigValidator(ProducerConfig.class)).dynamic(true)
    -                .build();
    +            .description(&quot;Specifies the value for &apos;&quot; + propertyDescriptorName + &quot;&apos; Kafka Configuration.&quot;)
    +            .name(propertyDescriptorName)
    +            .addValidator(new KafkaProcessorUtils.KafkaConfigValidator(ProducerConfig.class))
    +            .dynamic(true)
    +            .build();
         }

&lt;p&gt;         @Override&lt;br/&gt;
         protected Collection&amp;lt;ValidationResult&amp;gt; customValidate(final ValidationContext validationContext) &lt;/p&gt;
{
             return KafkaProcessorUtils.validateCommonProperties(validationContext);
         }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;volatile KafkaPublisher kafkaPublisher;&lt;br/&gt;
    -&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* This thread-safe operation will delegate to&lt;/li&gt;
	&lt;li&gt;* 
{@link #rendezvousWithKafka(ProcessContext, ProcessSession)} after first&lt;br/&gt;
    -     * checking and creating (if necessary) Kafka resource which could be either&lt;br/&gt;
    -     * {@link KafkaPublisher} or {@link KafkaConsumer}. It will also close and&lt;br/&gt;
    -     * destroy the underlying Kafka resource upon catching an {@link Exception}&lt;br/&gt;
    -     * raised by {@link #rendezvousWithKafka(ProcessContext, ProcessSession)}
&lt;p&gt;.&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;* After Kafka resource is destroyed it will be re-created upon the next&lt;/li&gt;
	&lt;li&gt;* invocation of this operation essentially providing a self healing&lt;/li&gt;
	&lt;li&gt;* mechanism to deal with potentially corrupted resource.&lt;/li&gt;
	&lt;li&gt;* &amp;lt;p&amp;gt;&lt;/li&gt;
	&lt;li&gt;* Keep in mind that upon catching an exception the state of this processor&lt;/li&gt;
	&lt;li&gt;* will be set to no longer accept any more tasks, until Kafka resource is&lt;/li&gt;
	&lt;li&gt;* reset. This means that in a multi-threaded situation currently executing&lt;/li&gt;
	&lt;li&gt;* tasks will be given a chance to complete while no new tasks will be&lt;/li&gt;
	&lt;li&gt;* accepted.&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* @param context context&lt;/li&gt;
	&lt;li&gt;* @param sessionFactory factory&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public final void onTrigger(final ProcessContext context, final ProcessSessionFactory sessionFactory) throws ProcessException {&lt;/li&gt;
	&lt;li&gt;if (this.acceptTask) { // acts as a circuit breaker to allow existing tasks to wind down so &apos;kafkaPublisher&apos; can be reset before new tasks are accepted.&lt;/li&gt;
	&lt;li&gt;this.taskCounter.incrementAndGet();&lt;/li&gt;
	&lt;li&gt;final ProcessSession session = sessionFactory.createSession();&lt;/li&gt;
	&lt;li&gt;try {&lt;/li&gt;
	&lt;li&gt;/*&lt;/li&gt;
	&lt;li&gt;* We can&apos;t be doing double null check here since as a pattern&lt;/li&gt;
	&lt;li&gt;* it only works for lazy init but not reset, which is what we&lt;/li&gt;
	&lt;li&gt;* are doing here. In fact the first null check is dangerous&lt;/li&gt;
	&lt;li&gt;* since &apos;kafkaPublisher&apos; can become null right after its null&lt;/li&gt;
	&lt;li&gt;* check passed causing subsequent NPE.&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;synchronized (this) {&lt;/li&gt;
	&lt;li&gt;if (this.kafkaPublisher == null) 
{
    -                        this.kafkaPublisher = this.buildKafkaResource(context, session);
    -                    }&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
    -&lt;/li&gt;
	&lt;li&gt;/*&lt;/li&gt;
	&lt;li&gt;* The &apos;processed&apos; boolean flag does not imply any failure or success. It simply states that:&lt;/li&gt;
	&lt;li&gt;* - ConsumeKafka - some messages were received form Kafka and 1_ FlowFile were generated&lt;/li&gt;
	&lt;li&gt;* - PublishKafka0_10 - some messages were sent to Kafka based on existence of the input FlowFile&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;boolean processed = this.rendezvousWithKafka(context, session);&lt;/li&gt;
	&lt;li&gt;session.commit();&lt;/li&gt;
	&lt;li&gt;if (!processed) 
{
    -                    context.yield();
    -                }&lt;/li&gt;
	&lt;li&gt;} catch (Throwable e) {&lt;/li&gt;
	&lt;li&gt;this.acceptTask = false;&lt;/li&gt;
	&lt;li&gt;session.rollback(true);&lt;/li&gt;
	&lt;li&gt;this.getLogger().error(&quot;{} failed to process due to {}; rolling back session&quot;, new Object[]
{this, e}
&lt;p&gt;);&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;} finally {&lt;/li&gt;
	&lt;li&gt;synchronized (this) {&lt;/li&gt;
	&lt;li&gt;if (this.taskCounter.decrementAndGet() == 0 &amp;amp;&amp;amp; !this.acceptTask) 
{
    -                        this.close();
    -                        this.acceptTask = true;
    -                    }&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
	&lt;li&gt;} else {&lt;/li&gt;
	&lt;li&gt;this.logger.debug(&quot;Task was not accepted due to the processor being in &apos;reset&apos; state. It will be re-submitted upon completion of the reset.&quot;);&lt;/li&gt;
	&lt;li&gt;this.getLogger().debug(&quot;Task was not accepted due to the processor being in &apos;reset&apos; state. It will be re-submitted upon completion of the reset.&quot;);&lt;/li&gt;
	&lt;li&gt;context.yield();&lt;br/&gt;
    +    private synchronized PublisherPool getPublisherPool(final ProcessContext context) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {    +        PublisherPool pool = publisherPool;    +        if (pool != null) {
    +            return pool;
             }    +    +        return publisherPool = createPublisherPool(context);    +    }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;    +&lt;br/&gt;
    +    protected PublisherPool createPublisherPool(final ProcessContext context) &lt;/p&gt;
{
    +        final int maxMessageSize = context.getProperty(MAX_REQUEST_SIZE).asDataSize(DataUnit.B).intValue();
    +        final long maxAckWaitMillis = context.getProperty(ACK_WAIT_TIME).asTimePeriod(TimeUnit.MILLISECONDS).longValue();
    +
    +        final Map&amp;lt;String, Object&amp;gt; kafkaProperties = new HashMap&amp;lt;&amp;gt;();
    +        KafkaProcessorUtils.buildCommonKafkaProperties(context, ProducerConfig.class, kafkaProperties);
    +        kafkaProperties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());
    +        kafkaProperties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());
    +        kafkaProperties.put(&quot;max.request.size&quot;, String.valueOf(maxMessageSize));
    +
    +        return new PublisherPool(kafkaProperties, getLogger(), maxMessageSize, maxAckWaitMillis);
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Will call 
{@link Closeable#close()}
&lt;p&gt; on the target resource after which&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;* the target resource will be set to null. Should only be called when there&lt;/li&gt;
	&lt;li&gt;* are no more threads being executed on this processor or when it has been&lt;/li&gt;
	&lt;li&gt;* verified that only a single thread remains.&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* @see KafkaPublisher&lt;/li&gt;
	&lt;li&gt;* @see KafkaConsumer&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
         @OnStopped&lt;/li&gt;
	&lt;li&gt;public void close() {&lt;/li&gt;
	&lt;li&gt;try {&lt;/li&gt;
	&lt;li&gt;if (this.kafkaPublisher != null) {&lt;/li&gt;
	&lt;li&gt;try 
{
    -                    this.kafkaPublisher.close();
    -                }
&lt;p&gt; catch (Exception e) &lt;/p&gt;
{
    -                    this.getLogger().warn(&quot;Failed while closing &quot; + this.kafkaPublisher, e);
    -                }&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
	&lt;li&gt;} finally {&lt;/li&gt;
	&lt;li&gt;this.kafkaPublisher = null;&lt;br/&gt;
    +    public void closePool() 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {    +        if (publisherPool != null) {
    +            publisherPool.close();
             }    +    +        publisherPool = null;         }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Will rendezvous with Kafka if 
{@link ProcessSession}
&lt;p&gt; contains&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;* 
{@link FlowFile} producing a result {@link FlowFile}
&lt;p&gt;.&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;* &amp;lt;br&amp;gt;&lt;/li&gt;
	&lt;li&gt;* The result 
{@link FlowFile} that is successful is then transfered to&lt;br/&gt;
    -     * {@link #REL_SUCCESS}&lt;br/&gt;
    -     * &amp;lt;br&amp;gt;&lt;br/&gt;
    -     * The result {@link FlowFile}
&lt;p&gt; that is failed is then transfered to&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;* 
{@link #REL_FAILURE}&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;protected boolean rendezvousWithKafka(ProcessContext context, ProcessSession session) {&lt;/li&gt;
	&lt;li&gt;FlowFile flowFile = session.get();&lt;/li&gt;
	&lt;li&gt;if (flowFile != null) {&lt;/li&gt;
	&lt;li&gt;long start = System.nanoTime();&lt;/li&gt;
	&lt;li&gt;flowFile = this.doRendezvousWithKafka(flowFile, context, session);&lt;/li&gt;
	&lt;li&gt;Relationship relationship = REL_SUCCESS;&lt;/li&gt;
	&lt;li&gt;if (!this.isFailedFlowFile(flowFile)) {&lt;/li&gt;
	&lt;li&gt;String topic = context.getProperty(TOPIC).evaluateAttributeExpressions(flowFile).getValue();&lt;/li&gt;
	&lt;li&gt;long executionDuration = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start);&lt;/li&gt;
	&lt;li&gt;String transitUri = KafkaProcessorUtils.buildTransitURI(context.getProperty(KafkaProcessorUtils.SECURITY_PROTOCOL).getValue(), this.brokers, topic);&lt;/li&gt;
	&lt;li&gt;session.getProvenanceReporter().send(flowFile, transitUri, &quot;Sent &quot; + flowFile.getAttribute(MSG_COUNT) + &quot; Kafka messages&quot;, executionDuration);&lt;/li&gt;
	&lt;li&gt;this.getLogger().debug(&quot;Successfully sent {} to Kafka as {} message(s) in {} millis&quot;,&lt;/li&gt;
	&lt;li&gt;new Object[]
{flowFile, flowFile.getAttribute(MSG_COUNT), executionDuration}
&lt;p&gt;);&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;} else 
{
    -                relationship = REL_FAILURE;
    -                flowFile = session.penalize(flowFile);
    -            }&lt;/li&gt;
	&lt;li&gt;session.transfer(flowFile, relationship);&lt;br/&gt;
    +    @Override&lt;br/&gt;
    +    public void onTrigger(final ProcessContext context, final ProcessSession session) throws ProcessException {&lt;br/&gt;
    +        final boolean useDemarcator = context.getProperty(MESSAGE_DEMARCATOR).isSet();&lt;br/&gt;
    +&lt;br/&gt;
    +        final List&amp;lt;FlowFile&amp;gt; flowFiles = session.get(FlowFileFilters.newSizeBasedFilter(250, DataUnit.KB, 500));&lt;br/&gt;
    +        if (flowFiles.isEmpty()) 
{
    +            return;
             }&lt;/li&gt;
	&lt;li&gt;return flowFile != null;&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Builds and instance of 
{@link KafkaPublisher}.&lt;br/&gt;
    -     */&lt;br/&gt;
    -    protected KafkaPublisher buildKafkaResource(ProcessContext context, ProcessSession session) {
    -        final Map&amp;lt;String, String&amp;gt; kafkaProps = new HashMap&amp;lt;&amp;gt;();
    -        KafkaProcessorUtils.buildCommonKafkaProperties(context, ProducerConfig.class, kafkaProps);
    -        kafkaProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());
    -        kafkaProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());
    -        kafkaProps.put(&quot;max.request.size&quot;, String.valueOf(context.getProperty(MAX_REQUEST_SIZE).asDataSize(DataUnit.B).intValue()));
    -        this.brokers = context.getProperty(KafkaProcessorUtils.BOOTSTRAP_SERVERS).evaluateAttributeExpressions().getValue();
    -        final Properties props = new Properties();
    -        props.putAll(kafkaProps);
    -        KafkaPublisher publisher = new KafkaPublisher(props, this.getLogger());
    -        return publisher;
    -    }&lt;br/&gt;
    +        final PublisherPool pool = getPublisherPool(context);&lt;br/&gt;
    +        if (pool == null) {
    +            context.yield();
    +            return;
    +        }&lt;br/&gt;
    +&lt;br/&gt;
    +        final String securityProtocol = context.getProperty(KafkaProcessorUtils.SECURITY_PROTOCOL).getValue();&lt;br/&gt;
    +        final String bootstrapServers = context.getProperty(KafkaProcessorUtils.BOOTSTRAP_SERVERS).evaluateAttributeExpressions().getValue();&lt;br/&gt;
    +&lt;br/&gt;
    +        final long startTime = System.nanoTime();&lt;br/&gt;
    +        try (final PublisherLease lease = pool.obtainPublisher()) {&lt;br/&gt;
    +            // Send each FlowFile to Kafka asynchronously.&lt;br/&gt;
    +            for (final FlowFile flowFile : flowFiles) {&lt;br/&gt;
    +                if (!isScheduled()) {
    +                    // If stopped, re-queue FlowFile instead of sending it
    +                    session.transfer(flowFile);
    +                    continue;
    +                }&lt;br/&gt;
     &lt;br/&gt;
    -    /**&lt;br/&gt;
    -     * Will rendezvous with {@link KafkaPublisher}
&lt;p&gt; after building&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;* 
{@link PublishingContext} and will produce the resulting&lt;br/&gt;
    -     * {@link FlowFile}. The resulting FlowFile contains all required&lt;br/&gt;
    -     * information to determine if message publishing originated from the&lt;br/&gt;
    -     * provided FlowFile has actually succeeded fully, partially or failed&lt;br/&gt;
    -     * completely (see {@link #isFailedFlowFile(FlowFile)}.&lt;br/&gt;
    -     */&lt;br/&gt;
    -    private FlowFile doRendezvousWithKafka(final FlowFile flowFile, final ProcessContext context, final ProcessSession session) {&lt;br/&gt;
    -        final AtomicReference&amp;lt;KafkaPublisher.KafkaPublisherResult&amp;gt; publishResultRef = new AtomicReference&amp;lt;&amp;gt;();&lt;br/&gt;
    -        session.read(flowFile, new InputStreamCallback() {&lt;br/&gt;
    -            @Override&lt;br/&gt;
    -            public void process(InputStream contentStream) throws IOException {&lt;br/&gt;
    -                PublishingContext publishingContext = PublishKafka_0_10.this.buildPublishingContext(flowFile, context, contentStream);&lt;br/&gt;
    -                KafkaPublisher.KafkaPublisherResult result = PublishKafka_0_10.this.kafkaPublisher.publish(publishingContext);&lt;br/&gt;
    -                publishResultRef.set(result);&lt;br/&gt;
    +                final byte[] messageKey = getMessageKey(flowFile, context);&lt;br/&gt;
    +                final String topic = context.getProperty(TOPIC).evaluateAttributeExpressions(flowFile).getValue();&lt;br/&gt;
    +                final byte[] demarcatorBytes;&lt;br/&gt;
    +                if (useDemarcator) {
    +                    demarcatorBytes = context.getProperty(MESSAGE_DEMARCATOR).evaluateAttributeExpressions(flowFile).getValue().getBytes(StandardCharsets.UTF_8);
    +                } else {
    +                    demarcatorBytes = null;
    +                }&lt;br/&gt;
    +&lt;br/&gt;
    +                session.read(flowFile, new InputStreamCallback() {&lt;br/&gt;
    +                    @Override&lt;br/&gt;
    +                    public void process(final InputStream rawIn) throws IOException {&lt;br/&gt;
    +                        try (final InputStream in = new BufferedInputStream(rawIn)) {
    +                            lease.publish(flowFile, in, messageKey, demarcatorBytes, topic);
    +                        }&lt;br/&gt;
    +                    }&lt;br/&gt;
    +                });&lt;br/&gt;
                 }&lt;br/&gt;
    -        });&lt;br/&gt;
     &lt;br/&gt;
    -        FlowFile resultFile = publishResultRef.get().isAllAcked()&lt;br/&gt;
    -                ? this.cleanUpFlowFileIfNecessary(flowFile, session)&lt;br/&gt;
    -                : session.putAllAttributes(flowFile, this.buildFailedFlowFileAttributes(publishResultRef.get().getLastMessageAcked(), flowFile, context));&lt;br/&gt;
    +            // Complete the send&lt;br/&gt;
    +            final PublishResult publishResult = lease.complete();&lt;br/&gt;
     &lt;br/&gt;
    -        if (!this.isFailedFlowFile(resultFile)) {
    -            resultFile = session.putAttribute(resultFile, MSG_COUNT, String.valueOf(publishResultRef.get().getMessagesSent()));
    -        }&lt;br/&gt;
    -        return resultFile;&lt;br/&gt;
    -    }&lt;br/&gt;
    +            // Transfer any successful FlowFiles.&lt;br/&gt;
    +            final long transmissionMillis = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - startTime);&lt;br/&gt;
    +            for (FlowFile success : publishResult.getSuccessfulFlowFiles()) {&lt;br/&gt;
    +                final String topic = context.getProperty(TOPIC).evaluateAttributeExpressions(success).getValue();&lt;br/&gt;
     &lt;br/&gt;
    -    /**&lt;br/&gt;
    -     * Builds {@link PublishingContext}
&lt;p&gt; for message(s) to be sent to Kafka.&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;* 
{@link PublishingContext} contains all contextual information required by&lt;br/&gt;
    -     * {@link KafkaPublisher} to publish to Kafka. Such information contains&lt;br/&gt;
    -     * things like topic name, content stream, delimiter, key and last ACKed&lt;br/&gt;
    -     * message for cases where provided FlowFile is being retried (failed in the&lt;br/&gt;
    -     * past).&lt;br/&gt;
    -     * &amp;lt;br&amp;gt;&lt;br/&gt;
    -     * For the clean FlowFile (file that has been sent for the first time),&lt;br/&gt;
    -     * PublishingContext will be built form {@link ProcessContext} associated&lt;br/&gt;
    -     * with this invocation.&lt;br/&gt;
    -     * &amp;lt;br&amp;gt;&lt;br/&gt;
    -     * For the failed FlowFile, {@link PublishingContext}
&lt;p&gt; will be built from&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;* attributes of that FlowFile which by then will already contain required&lt;/li&gt;
	&lt;li&gt;* information (e.g., topic, key, delimiter etc.). This is required to&lt;/li&gt;
	&lt;li&gt;* ensure the affinity of the retry in the even where processor&lt;/li&gt;
	&lt;li&gt;* configuration has changed. However keep in mind that failed FlowFile is&lt;/li&gt;
	&lt;li&gt;* only considered a failed FlowFile if it is being re-processed by the same&lt;/li&gt;
	&lt;li&gt;* processor (determined via 
{@link #FAILED_PROC_ID_ATTR}
&lt;p&gt;, see&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;* 
{@link #isFailedFlowFile(FlowFile)}
&lt;p&gt;). If failed FlowFile is being sent to&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;* another PublishKafka0_10 processor it is treated as a fresh FlowFile&lt;/li&gt;
	&lt;li&gt;* regardless if it has #FAILED* attributes set.&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;private PublishingContext buildPublishingContext(FlowFile flowFile, ProcessContext context, InputStream contentStream) {&lt;/li&gt;
	&lt;li&gt;final byte[] keyBytes = getMessageKey(flowFile, context);&lt;br/&gt;
    -&lt;/li&gt;
	&lt;li&gt;final String topicName;&lt;/li&gt;
	&lt;li&gt;final byte[] delimiterBytes;&lt;/li&gt;
	&lt;li&gt;int lastAckedMessageIndex = -1;&lt;/li&gt;
	&lt;li&gt;if (this.isFailedFlowFile(flowFile)) 
{
    -            lastAckedMessageIndex = Integer.valueOf(flowFile.getAttribute(FAILED_LAST_ACK_IDX));
    -            topicName = flowFile.getAttribute(FAILED_TOPIC_ATTR);
    -            delimiterBytes = flowFile.getAttribute(FAILED_DELIMITER_ATTR) != null
    -                    ? flowFile.getAttribute(FAILED_DELIMITER_ATTR).getBytes(StandardCharsets.UTF_8) : null;
    -        }
&lt;p&gt; else &lt;/p&gt;
{
    -            topicName = context.getProperty(TOPIC).evaluateAttributeExpressions(flowFile).getValue();
    -            delimiterBytes = context.getProperty(MESSAGE_DEMARCATOR).isSet() ? context.getProperty(MESSAGE_DEMARCATOR)
    -                    .evaluateAttributeExpressions(flowFile).getValue().getBytes(StandardCharsets.UTF_8) : null;
    -        }
&lt;p&gt;    +                final int msgCount = publishResult.getSuccessfulMessageCount(success);&lt;br/&gt;
    +                success = session.putAttribute(success, MSG_COUNT, String.valueOf(msgCount));&lt;br/&gt;
    +                session.adjustCounter(&quot;Messages Sent&quot;, msgCount, true);&lt;br/&gt;
    +&lt;br/&gt;
    +                final String transitUri = KafkaProcessorUtils.buildTransitURI(securityProtocol, bootstrapServers, topic);&lt;br/&gt;
    +                session.getProvenanceReporter().send(success, transitUri, &quot;Sent &quot; + msgCount + &quot; messages&quot;, transmissionMillis);&lt;br/&gt;
    +                session.transfer(success, REL_SUCCESS);&lt;br/&gt;
    +            }&lt;br/&gt;
    +&lt;br/&gt;
    +            // Transfer any failures.&lt;br/&gt;
    +            for (final FlowFile failure : publishResult.getFailedFlowFiles()) {&lt;br/&gt;
    +                final int successCount = publishResult.getSuccessfulMessageCount(failure);&lt;br/&gt;
    +                if (successCount &amp;gt; 0) {&lt;br/&gt;
    +                    getLogger().error(&quot;Failed to send all message for {} to Kafka, but {} messages were acknowledged by Kafka. Routing to failure due to {}&quot;,&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    This logging statement looks like it was meant to include successCount since it has 3 parameters, but only 2 passed in. It might also read better if it said &quot;Failed to send some messages for {} to Kafka, {} messages were acknowledged&quot; or something like that.&lt;/p&gt;</comment>
                            <comment id="15549945" author="githubbot" created="Wed, 5 Oct 2016 21:05:10 +0000"  >&lt;p&gt;Github user bbende commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/nifi/pull/1097#discussion_r82070354&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/nifi/pull/1097#discussion_r82070354&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: nifi-nar-bundles/nifi-kafka-bundle/nifi-kafka-0-9-processors/src/main/java/org/apache/nifi/processors/kafka/pubsub/PublishKafka.java &amp;#8212;&lt;br/&gt;
    @@ -250,242 +241,141 @@&lt;/p&gt;

&lt;p&gt;         @Override&lt;br/&gt;
         protected List&amp;lt;PropertyDescriptor&amp;gt; getSupportedPropertyDescriptors() &lt;/p&gt;
{
    -        return DESCRIPTORS;
    +        return PROPERTIES;
         }

&lt;p&gt;         @Override&lt;br/&gt;
         protected PropertyDescriptor getSupportedDynamicPropertyDescriptor(final String propertyDescriptorName) &lt;/p&gt;
{
             return new PropertyDescriptor.Builder()
    -                .description(&quot;Specifies the value for &apos;&quot; + propertyDescriptorName + &quot;&apos; Kafka Configuration.&quot;)
    -                .name(propertyDescriptorName).addValidator(new KafkaProcessorUtils.KafkaConfigValidator(ProducerConfig.class)).dynamic(true)
    -                .build();
    +            .description(&quot;Specifies the value for &apos;&quot; + propertyDescriptorName + &quot;&apos; Kafka Configuration.&quot;)
    +            .name(propertyDescriptorName)
    +            .addValidator(new KafkaProcessorUtils.KafkaConfigValidator(ProducerConfig.class))
    +            .dynamic(true)
    +            .build();
         }

&lt;p&gt;         @Override&lt;br/&gt;
         protected Collection&amp;lt;ValidationResult&amp;gt; customValidate(final ValidationContext validationContext) &lt;/p&gt;
{
             return KafkaProcessorUtils.validateCommonProperties(validationContext);
         }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;volatile KafkaPublisher kafkaPublisher;&lt;br/&gt;
    -&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* This thread-safe operation will delegate to&lt;/li&gt;
	&lt;li&gt;* 
{@link #rendezvousWithKafka(ProcessContext, ProcessSession)} after first&lt;br/&gt;
    -     * checking and creating (if necessary) Kafka resource which could be either&lt;br/&gt;
    -     * {@link KafkaPublisher} or {@link KafkaConsumer}. It will also close and&lt;br/&gt;
    -     * destroy the underlying Kafka resource upon catching an {@link Exception}&lt;br/&gt;
    -     * raised by {@link #rendezvousWithKafka(ProcessContext, ProcessSession)}
&lt;p&gt;.&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;* After Kafka resource is destroyed it will be re-created upon the next&lt;/li&gt;
	&lt;li&gt;* invocation of this operation essentially providing a self healing&lt;/li&gt;
	&lt;li&gt;* mechanism to deal with potentially corrupted resource.&lt;/li&gt;
	&lt;li&gt;* &amp;lt;p&amp;gt;&lt;/li&gt;
	&lt;li&gt;* Keep in mind that upon catching an exception the state of this processor&lt;/li&gt;
	&lt;li&gt;* will be set to no longer accept any more tasks, until Kafka resource is&lt;/li&gt;
	&lt;li&gt;* reset. This means that in a multi-threaded situation currently executing&lt;/li&gt;
	&lt;li&gt;* tasks will be given a chance to complete while no new tasks will be&lt;/li&gt;
	&lt;li&gt;* accepted.&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* @param context context&lt;/li&gt;
	&lt;li&gt;* @param sessionFactory factory&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public final void onTrigger(final ProcessContext context, final ProcessSessionFactory sessionFactory) throws ProcessException {&lt;/li&gt;
	&lt;li&gt;if (this.acceptTask) { // acts as a circuit breaker to allow existing tasks to wind down so &apos;kafkaPublisher&apos; can be reset before new tasks are accepted.&lt;/li&gt;
	&lt;li&gt;this.taskCounter.incrementAndGet();&lt;/li&gt;
	&lt;li&gt;final ProcessSession session = sessionFactory.createSession();&lt;/li&gt;
	&lt;li&gt;try {&lt;/li&gt;
	&lt;li&gt;/*&lt;/li&gt;
	&lt;li&gt;* We can&apos;t be doing double null check here since as a pattern&lt;/li&gt;
	&lt;li&gt;* it only works for lazy init but not reset, which is what we&lt;/li&gt;
	&lt;li&gt;* are doing here. In fact the first null check is dangerous&lt;/li&gt;
	&lt;li&gt;* since &apos;kafkaPublisher&apos; can become null right after its null&lt;/li&gt;
	&lt;li&gt;* check passed causing subsequent NPE.&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;synchronized (this) {&lt;/li&gt;
	&lt;li&gt;if (this.kafkaPublisher == null) 
{
    -                        this.kafkaPublisher = this.buildKafkaResource(context, session);
    -                    }&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
    -&lt;/li&gt;
	&lt;li&gt;/*&lt;/li&gt;
	&lt;li&gt;* The &apos;processed&apos; boolean flag does not imply any failure or success. It simply states that:&lt;/li&gt;
	&lt;li&gt;* - ConsumeKafka - some messages were received form Kafka and 1_ FlowFile were generated&lt;/li&gt;
	&lt;li&gt;* - PublishKafka0_10 - some messages were sent to Kafka based on existence of the input FlowFile&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;boolean processed = this.rendezvousWithKafka(context, session);&lt;/li&gt;
	&lt;li&gt;session.commit();&lt;/li&gt;
	&lt;li&gt;if (!processed) 
{
    -                    context.yield();
    -                }&lt;/li&gt;
	&lt;li&gt;} catch (Throwable e) {&lt;/li&gt;
	&lt;li&gt;this.acceptTask = false;&lt;/li&gt;
	&lt;li&gt;session.rollback(true);&lt;/li&gt;
	&lt;li&gt;this.getLogger().error(&quot;{} failed to process due to {}; rolling back session&quot;, new Object[]
{this, e}
&lt;p&gt;);&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;} finally {&lt;/li&gt;
	&lt;li&gt;synchronized (this) {&lt;/li&gt;
	&lt;li&gt;if (this.taskCounter.decrementAndGet() == 0 &amp;amp;&amp;amp; !this.acceptTask) 
{
    -                        this.close();
    -                        this.acceptTask = true;
    -                    }&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
	&lt;li&gt;} else {&lt;/li&gt;
	&lt;li&gt;this.logger.debug(&quot;Task was not accepted due to the processor being in &apos;reset&apos; state. It will be re-submitted upon completion of the reset.&quot;);&lt;/li&gt;
	&lt;li&gt;this.getLogger().debug(&quot;Task was not accepted due to the processor being in &apos;reset&apos; state. It will be re-submitted upon completion of the reset.&quot;);&lt;/li&gt;
	&lt;li&gt;context.yield();&lt;br/&gt;
    +    private synchronized PublisherPool getPublisherPool(final ProcessContext context) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {    +        PublisherPool pool = publisherPool;    +        if (pool != null) {
    +            return pool;
             }    +    +        return publisherPool = createPublisherPool(context);    +    }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;    +&lt;br/&gt;
    +    protected PublisherPool createPublisherPool(final ProcessContext context) &lt;/p&gt;
{
    +        final int maxMessageSize = context.getProperty(MAX_REQUEST_SIZE).asDataSize(DataUnit.B).intValue();
    +        final long maxAckWaitMillis = context.getProperty(ACK_WAIT_TIME).asTimePeriod(TimeUnit.MILLISECONDS).longValue();
    +
    +        final Map&amp;lt;String, Object&amp;gt; kafkaProperties = new HashMap&amp;lt;&amp;gt;();
    +        KafkaProcessorUtils.buildCommonKafkaProperties(context, ProducerConfig.class, kafkaProperties);
    +        kafkaProperties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());
    +        kafkaProperties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());
    +        kafkaProperties.put(&quot;max.request.size&quot;, String.valueOf(maxMessageSize));
    +
    +        return new PublisherPool(kafkaProperties, getLogger(), maxMessageSize, maxAckWaitMillis);
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Will call 
{@link Closeable#close()}
&lt;p&gt; on the target resource after which&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;* the target resource will be set to null. Should only be called when there&lt;/li&gt;
	&lt;li&gt;* are no more threads being executed on this processor or when it has been&lt;/li&gt;
	&lt;li&gt;* verified that only a single thread remains.&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* @see KafkaPublisher&lt;/li&gt;
	&lt;li&gt;* @see KafkaConsumer&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
         @OnStopped&lt;/li&gt;
	&lt;li&gt;public void close() {&lt;/li&gt;
	&lt;li&gt;try {&lt;/li&gt;
	&lt;li&gt;if (this.kafkaPublisher != null) {&lt;/li&gt;
	&lt;li&gt;try 
{
    -                    this.kafkaPublisher.close();
    -                }
&lt;p&gt; catch (Exception e) &lt;/p&gt;
{
    -                    this.getLogger().warn(&quot;Failed while closing &quot; + this.kafkaPublisher, e);
    -                }&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
	&lt;li&gt;} finally {&lt;/li&gt;
	&lt;li&gt;this.kafkaPublisher = null;&lt;br/&gt;
    +    public void closePool() 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {    +        if (publisherPool != null) {
    +            publisherPool.close();
             }    +    +        publisherPool = null;         }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Will rendezvous with Kafka if 
{@link ProcessSession}
&lt;p&gt; contains&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;* 
{@link FlowFile} producing a result {@link FlowFile}
&lt;p&gt;.&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;* &amp;lt;br&amp;gt;&lt;/li&gt;
	&lt;li&gt;* The result 
{@link FlowFile} that is successful is then transfered to&lt;br/&gt;
    -     * {@link #REL_SUCCESS}&lt;br/&gt;
    -     * &amp;lt;br&amp;gt;&lt;br/&gt;
    -     * The result {@link FlowFile}
&lt;p&gt; that is failed is then transfered to&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;* 
{@link #REL_FAILURE}&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;protected boolean rendezvousWithKafka(ProcessContext context, ProcessSession session) {&lt;/li&gt;
	&lt;li&gt;FlowFile flowFile = session.get();&lt;/li&gt;
	&lt;li&gt;if (flowFile != null) {&lt;/li&gt;
	&lt;li&gt;long start = System.nanoTime();&lt;/li&gt;
	&lt;li&gt;flowFile = this.doRendezvousWithKafka(flowFile, context, session);&lt;/li&gt;
	&lt;li&gt;Relationship relationship = REL_SUCCESS;&lt;/li&gt;
	&lt;li&gt;if (!this.isFailedFlowFile(flowFile)) {&lt;/li&gt;
	&lt;li&gt;String topic = context.getProperty(TOPIC).evaluateAttributeExpressions(flowFile).getValue();&lt;/li&gt;
	&lt;li&gt;long executionDuration = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start);&lt;/li&gt;
	&lt;li&gt;String transitUri = KafkaProcessorUtils.buildTransitURI(context.getProperty(KafkaProcessorUtils.SECURITY_PROTOCOL).getValue(), this.brokers, topic);&lt;/li&gt;
	&lt;li&gt;session.getProvenanceReporter().send(flowFile, transitUri, &quot;Sent &quot; + flowFile.getAttribute(MSG_COUNT) + &quot; Kafka messages&quot;, executionDuration);&lt;/li&gt;
	&lt;li&gt;this.getLogger().debug(&quot;Successfully sent {} to Kafka as {} message(s) in {} millis&quot;,&lt;/li&gt;
	&lt;li&gt;new Object[]
{flowFile, flowFile.getAttribute(MSG_COUNT), executionDuration}
&lt;p&gt;);&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;} else 
{
    -                relationship = REL_FAILURE;
    -                flowFile = session.penalize(flowFile);
    -            }&lt;/li&gt;
	&lt;li&gt;session.transfer(flowFile, relationship);&lt;br/&gt;
    +    @Override&lt;br/&gt;
    +    public void onTrigger(final ProcessContext context, final ProcessSession session) throws ProcessException {&lt;br/&gt;
    +        final boolean useDemarcator = context.getProperty(MESSAGE_DEMARCATOR).isSet();&lt;br/&gt;
    +&lt;br/&gt;
    +        final List&amp;lt;FlowFile&amp;gt; flowFiles = session.get(FlowFileFilters.newSizeBasedFilter(250, DataUnit.KB, 500));&lt;br/&gt;
    +        if (flowFiles.isEmpty()) 
{
    +            return;
             }&lt;/li&gt;
	&lt;li&gt;return flowFile != null;&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Builds and instance of 
{@link KafkaPublisher}.&lt;br/&gt;
    -     */&lt;br/&gt;
    -    protected KafkaPublisher buildKafkaResource(ProcessContext context, ProcessSession session) {
    -        final Map&amp;lt;String, String&amp;gt; kafkaProps = new HashMap&amp;lt;&amp;gt;();
    -        KafkaProcessorUtils.buildCommonKafkaProperties(context, ProducerConfig.class, kafkaProps);
    -        kafkaProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());
    -        kafkaProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());
    -        kafkaProps.put(&quot;max.request.size&quot;, String.valueOf(context.getProperty(MAX_REQUEST_SIZE).asDataSize(DataUnit.B).intValue()));
    -        this.brokers = context.getProperty(KafkaProcessorUtils.BOOTSTRAP_SERVERS).evaluateAttributeExpressions().getValue();
    -        final Properties props = new Properties();
    -        props.putAll(kafkaProps);
    -        KafkaPublisher publisher = new KafkaPublisher(props, this.getLogger());
    -        return publisher;
    -    }&lt;br/&gt;
    +        final PublisherPool pool = getPublisherPool(context);&lt;br/&gt;
    +        if (pool == null) {
    +            context.yield();
    +            return;
    +        }&lt;br/&gt;
     &lt;br/&gt;
    -    /**&lt;br/&gt;
    -     * Will rendezvous with {@link KafkaPublisher}
&lt;p&gt; after building&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;* 
{@link PublishingContext} and will produce the resulting&lt;br/&gt;
    -     * {@link FlowFile}. The resulting FlowFile contains all required&lt;br/&gt;
    -     * information to determine if message publishing originated from the&lt;br/&gt;
    -     * provided FlowFile has actually succeeded fully, partially or failed&lt;br/&gt;
    -     * completely (see {@link #isFailedFlowFile(FlowFile)}.&lt;br/&gt;
    -     */&lt;br/&gt;
    -    private FlowFile doRendezvousWithKafka(final FlowFile flowFile, final ProcessContext context, final ProcessSession session) {&lt;br/&gt;
    -        final AtomicReference&amp;lt;KafkaPublisher.KafkaPublisherResult&amp;gt; publishResultRef = new AtomicReference&amp;lt;&amp;gt;();&lt;br/&gt;
    -        session.read(flowFile, new InputStreamCallback() {&lt;br/&gt;
    -            @Override&lt;br/&gt;
    -            public void process(InputStream contentStream) throws IOException {&lt;br/&gt;
    -                PublishingContext publishingContext = PublishKafka.this.buildPublishingContext(flowFile, context, contentStream);&lt;br/&gt;
    -                KafkaPublisher.KafkaPublisherResult result = PublishKafka.this.kafkaPublisher.publish(publishingContext);&lt;br/&gt;
    -                publishResultRef.set(result);&lt;br/&gt;
    +        final String securityProtocol = context.getProperty(KafkaProcessorUtils.SECURITY_PROTOCOL).getValue();&lt;br/&gt;
    +        final String bootstrapServers = context.getProperty(KafkaProcessorUtils.BOOTSTRAP_SERVERS).evaluateAttributeExpressions().getValue();&lt;br/&gt;
    +&lt;br/&gt;
    +        final long startTime = System.nanoTime();&lt;br/&gt;
    +        try (final PublisherLease lease = pool.obtainPublisher()) {&lt;br/&gt;
    +            // Send each FlowFile to Kafka asynchronously.&lt;br/&gt;
    +            for (final FlowFile flowFile : flowFiles) {&lt;br/&gt;
    +                if (!isScheduled()) {
    +                    // If stopped, re-queue FlowFile instead of sending it
    +                    session.transfer(flowFile);
    +                    continue;
    +                }&lt;br/&gt;
    +&lt;br/&gt;
    +                final byte[] messageKey = getMessageKey(flowFile, context);&lt;br/&gt;
    +                final String topic = context.getProperty(TOPIC).evaluateAttributeExpressions(flowFile).getValue();&lt;br/&gt;
    +                final byte[] demarcatorBytes;&lt;br/&gt;
    +                if (useDemarcator) {
    +                    demarcatorBytes = context.getProperty(MESSAGE_DEMARCATOR).evaluateAttributeExpressions(flowFile).getValue().getBytes(StandardCharsets.UTF_8);
    +                } else {
    +                    demarcatorBytes = null;
    +                }&lt;br/&gt;
    +&lt;br/&gt;
    +                session.read(flowFile, new InputStreamCallback() {&lt;br/&gt;
    +                    @Override&lt;br/&gt;
    +                    public void process(final InputStream rawIn) throws IOException {&lt;br/&gt;
    +                        try (final InputStream in = new BufferedInputStream(rawIn)) {
    +                            lease.publish(flowFile, in, messageKey, demarcatorBytes, topic);
    +                        }&lt;br/&gt;
    +                    }&lt;br/&gt;
    +                });&lt;br/&gt;
                 }&lt;br/&gt;
    -        });&lt;br/&gt;
     &lt;br/&gt;
    -        FlowFile resultFile = publishResultRef.get().isAllAcked()&lt;br/&gt;
    -                ? this.cleanUpFlowFileIfNecessary(flowFile, session)&lt;br/&gt;
    -                : session.putAllAttributes(flowFile, this.buildFailedFlowFileAttributes(publishResultRef.get().getLastMessageAcked(), flowFile, context));&lt;br/&gt;
    +            // Complete the send&lt;br/&gt;
    +            final PublishResult publishResult = lease.complete();&lt;br/&gt;
     &lt;br/&gt;
    -        if (!this.isFailedFlowFile(resultFile)) {
    -            resultFile = session.putAttribute(resultFile, MSG_COUNT, String.valueOf(publishResultRef.get().getMessagesSent()));
    -        }&lt;br/&gt;
    -        return resultFile;&lt;br/&gt;
    -    }&lt;br/&gt;
    +            // Transfer any successful FlowFiles.&lt;br/&gt;
    +            final long transmissionMillis = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - startTime);&lt;br/&gt;
    +            for (FlowFile success : publishResult.getSuccessfulFlowFiles()) {&lt;br/&gt;
    +                final String topic = context.getProperty(TOPIC).evaluateAttributeExpressions(success).getValue();&lt;br/&gt;
     &lt;br/&gt;
    -    /**&lt;br/&gt;
    -     * Builds {@link PublishingContext}
&lt;p&gt; for message(s) to be sent to Kafka.&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;* 
{@link PublishingContext} contains all contextual information required by&lt;br/&gt;
    -     * {@link KafkaPublisher} to publish to Kafka. Such information contains&lt;br/&gt;
    -     * things like topic name, content stream, delimiter, key and last ACKed&lt;br/&gt;
    -     * message for cases where provided FlowFile is being retried (failed in the&lt;br/&gt;
    -     * past).&lt;br/&gt;
    -     * &amp;lt;br&amp;gt;&lt;br/&gt;
    -     * For the clean FlowFile (file that has been sent for the first time),&lt;br/&gt;
    -     * PublishingContext will be built form {@link ProcessContext} associated&lt;br/&gt;
    -     * with this invocation.&lt;br/&gt;
    -     * &amp;lt;br&amp;gt;&lt;br/&gt;
    -     * For the failed FlowFile, {@link PublishingContext}
&lt;p&gt; will be built from&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;* attributes of that FlowFile which by then will already contain required&lt;/li&gt;
	&lt;li&gt;* information (e.g., topic, key, delimiter etc.). This is required to&lt;/li&gt;
	&lt;li&gt;* ensure the affinity of the retry in the even where processor&lt;/li&gt;
	&lt;li&gt;* configuration has changed. However keep in mind that failed FlowFile is&lt;/li&gt;
	&lt;li&gt;* only considered a failed FlowFile if it is being re-processed by the same&lt;/li&gt;
	&lt;li&gt;* processor (determined via 
{@link #FAILED_PROC_ID_ATTR}
&lt;p&gt;, see&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;* 
{@link #isFailedFlowFile(FlowFile)}
&lt;p&gt;). If failed FlowFile is being sent to&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;* another PublishKafka0_10 processor it is treated as a fresh FlowFile&lt;/li&gt;
	&lt;li&gt;* regardless if it has #FAILED* attributes set.&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;private PublishingContext buildPublishingContext(FlowFile flowFile, ProcessContext context, InputStream contentStream) {&lt;/li&gt;
	&lt;li&gt;final byte[] keyBytes = getMessageKey(flowFile, context);&lt;br/&gt;
    -&lt;/li&gt;
	&lt;li&gt;final String topicName;&lt;/li&gt;
	&lt;li&gt;final byte[] delimiterBytes;&lt;/li&gt;
	&lt;li&gt;int lastAckedMessageIndex = -1;&lt;/li&gt;
	&lt;li&gt;if (this.isFailedFlowFile(flowFile)) 
{
    -            lastAckedMessageIndex = Integer.valueOf(flowFile.getAttribute(FAILED_LAST_ACK_IDX));
    -            topicName = flowFile.getAttribute(FAILED_TOPIC_ATTR);
    -            delimiterBytes = flowFile.getAttribute(FAILED_DELIMITER_ATTR) != null
    -                    ? flowFile.getAttribute(FAILED_DELIMITER_ATTR).getBytes(StandardCharsets.UTF_8) : null;
    -        }
&lt;p&gt; else &lt;/p&gt;
{
    -            topicName = context.getProperty(TOPIC).evaluateAttributeExpressions(flowFile).getValue();
    -            delimiterBytes = context.getProperty(MESSAGE_DEMARCATOR).isSet() ? context.getProperty(MESSAGE_DEMARCATOR)
    -                    .evaluateAttributeExpressions(flowFile).getValue().getBytes(StandardCharsets.UTF_8) : null;
    -        }
&lt;p&gt;    +                final int msgCount = publishResult.getSuccessfulMessageCount(success);&lt;br/&gt;
    +                success = session.putAttribute(success, MSG_COUNT, String.valueOf(msgCount));&lt;br/&gt;
    +                session.adjustCounter(&quot;Messages Sent&quot;, msgCount, true);&lt;br/&gt;
    +&lt;br/&gt;
    +                final String transitUri = KafkaProcessorUtils.buildTransitURI(securityProtocol, bootstrapServers, topic);&lt;br/&gt;
    +                session.getProvenanceReporter().send(success, transitUri, &quot;Sent &quot; + msgCount + &quot; messages&quot;, transmissionMillis);&lt;br/&gt;
    +                session.transfer(success, REL_SUCCESS);&lt;br/&gt;
    +            }&lt;br/&gt;
    +&lt;br/&gt;
    +            // Transfer any failures.&lt;br/&gt;
    +            for (final FlowFile failure : publishResult.getFailedFlowFiles()) {&lt;br/&gt;
    +                final int successCount = publishResult.getSuccessfulMessageCount(failure);&lt;br/&gt;
    +                if (successCount &amp;gt; 0) {&lt;br/&gt;
    +                    getLogger().error(&quot;Failed to send all message for {} to Kafka, but {} messages were acknowledged by Kafka. Routing to failure due to {}&quot;,&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Same as other comment in the PublishKafka_0_10&lt;/p&gt;</comment>
                            <comment id="15550068" author="githubbot" created="Wed, 5 Oct 2016 22:03:35 +0000"  >&lt;p&gt;Github user bbende commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/nifi/pull/1097#discussion_r82078303&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/nifi/pull/1097#discussion_r82078303&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: nifi-nar-bundles/nifi-kafka-bundle/nifi-kafka-0-10-processors/src/main/java/org/apache/nifi/processors/kafka/pubsub/PublisherLease.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,132 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.nifi.processors.kafka.pubsub;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.io.Closeable;&lt;br/&gt;
    +import java.io.IOException;&lt;br/&gt;
    +import java.io.InputStream;&lt;br/&gt;
    +import java.util.concurrent.TimeUnit;&lt;br/&gt;
    +import java.util.concurrent.TimeoutException;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.kafka.clients.producer.Callback;&lt;br/&gt;
    +import org.apache.kafka.clients.producer.Producer;&lt;br/&gt;
    +import org.apache.kafka.clients.producer.ProducerRecord;&lt;br/&gt;
    +import org.apache.kafka.clients.producer.RecordMetadata;&lt;br/&gt;
    +import org.apache.nifi.flowfile.FlowFile;&lt;br/&gt;
    +import org.apache.nifi.logging.ComponentLog;&lt;br/&gt;
    +import org.apache.nifi.stream.io.exception.TokenTooLargeException;&lt;br/&gt;
    +import org.apache.nifi.stream.io.util.StreamDemarcator;&lt;br/&gt;
    +&lt;br/&gt;
    +public class PublisherLease implements Closeable {&lt;br/&gt;
    +    private final ComponentLog logger;&lt;br/&gt;
    +    private final Producer&amp;lt;byte[], byte[]&amp;gt; producer;&lt;br/&gt;
    +    private final int maxMessageSize;&lt;br/&gt;
    +    private final long maxAckWaitMillis;&lt;br/&gt;
    +    private volatile boolean poisoned = false;&lt;br/&gt;
    +&lt;br/&gt;
    +    private InFlightMessageTracker tracker;&lt;br/&gt;
    +&lt;br/&gt;
    +    public PublisherLease(final Producer&amp;lt;byte[], byte[]&amp;gt; producer, final int maxMessageSize, final long maxAckWaitMillis, final ComponentLog logger) &lt;/p&gt;
{
    +        this.producer = producer;
    +        this.maxMessageSize = maxMessageSize;
    +        this.logger = logger;
    +        this.maxAckWaitMillis = maxAckWaitMillis;
    +    }
&lt;p&gt;    +&lt;br/&gt;
    +    protected void poison() &lt;/p&gt;
{
    +        this.poisoned = true;
    +    }
&lt;p&gt;    +&lt;br/&gt;
    +    public boolean isPoisoned() &lt;/p&gt;
{
    +        return poisoned;
    +    }
&lt;p&gt;    +&lt;br/&gt;
    +    void publish(final FlowFile flowFile, final InputStream flowFileContent, final byte[] messageKey, final byte[] demarcatorBytes, final String topic) throws IOException {&lt;br/&gt;
    +        if (tracker == null) {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Not sure if this is necessary, but would we want to check if poisoned here and throw an exception if publish is being called after being poisoned?&lt;/p&gt;</comment>
                            <comment id="15552008" author="githubbot" created="Thu, 6 Oct 2016 14:00:51 +0000"  >&lt;p&gt;Github user markap14 commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/nifi/pull/1097#discussion_r82194639&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/nifi/pull/1097#discussion_r82194639&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: nifi-nar-bundles/nifi-kafka-bundle/nifi-kafka-0-10-processors/src/main/java/org/apache/nifi/processors/kafka/pubsub/PublisherLease.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,132 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
    + * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
    + * this work for additional information regarding copyright ownership.&lt;br/&gt;
    + * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
    + * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
    + * the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.nifi.processors.kafka.pubsub;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.io.Closeable;&lt;br/&gt;
    +import java.io.IOException;&lt;br/&gt;
    +import java.io.InputStream;&lt;br/&gt;
    +import java.util.concurrent.TimeUnit;&lt;br/&gt;
    +import java.util.concurrent.TimeoutException;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.kafka.clients.producer.Callback;&lt;br/&gt;
    +import org.apache.kafka.clients.producer.Producer;&lt;br/&gt;
    +import org.apache.kafka.clients.producer.ProducerRecord;&lt;br/&gt;
    +import org.apache.kafka.clients.producer.RecordMetadata;&lt;br/&gt;
    +import org.apache.nifi.flowfile.FlowFile;&lt;br/&gt;
    +import org.apache.nifi.logging.ComponentLog;&lt;br/&gt;
    +import org.apache.nifi.stream.io.exception.TokenTooLargeException;&lt;br/&gt;
    +import org.apache.nifi.stream.io.util.StreamDemarcator;&lt;br/&gt;
    +&lt;br/&gt;
    +public class PublisherLease implements Closeable {&lt;br/&gt;
    +    private final ComponentLog logger;&lt;br/&gt;
    +    private final Producer&amp;lt;byte[], byte[]&amp;gt; producer;&lt;br/&gt;
    +    private final int maxMessageSize;&lt;br/&gt;
    +    private final long maxAckWaitMillis;&lt;br/&gt;
    +    private volatile boolean poisoned = false;&lt;br/&gt;
    +&lt;br/&gt;
    +    private InFlightMessageTracker tracker;&lt;br/&gt;
    +&lt;br/&gt;
    +    public PublisherLease(final Producer&amp;lt;byte[], byte[]&amp;gt; producer, final int maxMessageSize, final long maxAckWaitMillis, final ComponentLog logger) &lt;/p&gt;
{
    +        this.producer = producer;
    +        this.maxMessageSize = maxMessageSize;
    +        this.logger = logger;
    +        this.maxAckWaitMillis = maxAckWaitMillis;
    +    }
&lt;p&gt;    +&lt;br/&gt;
    +    protected void poison() &lt;/p&gt;
{
    +        this.poisoned = true;
    +    }
&lt;p&gt;    +&lt;br/&gt;
    +    public boolean isPoisoned() &lt;/p&gt;
{
    +        return poisoned;
    +    }
&lt;p&gt;    +&lt;br/&gt;
    +    void publish(final FlowFile flowFile, final InputStream flowFileContent, final byte[] messageKey, final byte[] demarcatorBytes, final String topic) throws IOException {&lt;br/&gt;
    +        if (tracker == null) {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    We probably could... but I held off on doing that, because it may get poisoned in a background thread, and the caller may not know when calling publish() that it was poisoned. If that happens, the new FlowFile is likely to fail anyway. If it doesn&apos;t then it&apos;s okay - there&apos;s no reason it can&apos;t get published to Kafka at that point. We simply poison the lease to ensure that a new one gets created, in case there&apos;s an issue with the connection.&lt;/p&gt;</comment>
                            <comment id="15552019" author="jira-bot" created="Thu, 6 Oct 2016 14:06:16 +0000"  >&lt;p&gt;Commit 92cca96d49042f9898f93b3a2d2210b924708e52 in nifi&apos;s branch refs/heads/master from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markap14&quot; class=&quot;user-hover&quot; rel=&quot;markap14&quot;&gt;markap14&lt;/a&gt;&lt;br/&gt;
[ &lt;a href=&quot;https://git-wip-us.apache.org/repos/asf?p=nifi.git;h=92cca96&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://git-wip-us.apache.org/repos/asf?p=nifi.git;h=92cca96&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/NIFI-2865&quot; title=&quot;Address issues of PublishKafka blocking when having trouble communicating with Kafka broker and improve performance&quot; class=&quot;issue-link&quot; data-issue-key=&quot;NIFI-2865&quot;&gt;&lt;del&gt;NIFI-2865&lt;/del&gt;&lt;/a&gt;: Refactored PublishKafka and PublishKafka_0_10 to allow batching of FlowFiles within a single publish and to let messages timeout if not acknowledged&lt;/p&gt;

&lt;p&gt;This closes #1097.&lt;/p&gt;

&lt;p&gt;Signed-off-by: Bryan Bende &amp;lt;bbende@apache.org&amp;gt;&lt;/p&gt;</comment>
                            <comment id="15552020" author="githubbot" created="Thu, 6 Oct 2016 14:06:35 +0000"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/nifi/pull/1097&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/nifi/pull/1097&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15552904" author="jira-bot" created="Thu, 6 Oct 2016 19:19:29 +0000"  >&lt;p&gt;Commit a4ed622152187155463af2b748c9bf492621bbc7 in nifi&apos;s branch refs/heads/master from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bbende&quot; class=&quot;user-hover&quot; rel=&quot;bbende&quot;&gt;bbende&lt;/a&gt;&lt;br/&gt;
[ &lt;a href=&quot;https://git-wip-us.apache.org/repos/asf?p=nifi.git;h=a4ed622&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://git-wip-us.apache.org/repos/asf?p=nifi.git;h=a4ed622&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;Revert &quot;&lt;a href=&quot;https://issues.apache.org/jira/browse/NIFI-2865&quot; title=&quot;Address issues of PublishKafka blocking when having trouble communicating with Kafka broker and improve performance&quot; class=&quot;issue-link&quot; data-issue-key=&quot;NIFI-2865&quot;&gt;&lt;del&gt;NIFI-2865&lt;/del&gt;&lt;/a&gt;: Refactored PublishKafka and PublishKafka_0_10 to allow batching of FlowFiles within a single publish and to let messages timeout if not acknowledged&quot;&lt;/p&gt;

&lt;p&gt;This reverts commit 92cca96d49042f9898f93b3a2d2210b924708e52.&lt;/p&gt;</comment>
                            <comment id="15552908" author="bende" created="Thu, 6 Oct 2016 19:21:06 +0000"  >&lt;p&gt;Reverted this commit on master due to a failing test with stream demarcator that exposed a bug that needs to be resolved.&lt;/p&gt;</comment>
                            <comment id="15553053" author="jira-bot" created="Thu, 6 Oct 2016 20:11:59 +0000"  >&lt;p&gt;Commit b9cb6b1b475eb4688b7cd32f6d343c5dffb20567 in nifi&apos;s branch refs/heads/master from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markap14&quot; class=&quot;user-hover&quot; rel=&quot;markap14&quot;&gt;markap14&lt;/a&gt;&lt;br/&gt;
[ &lt;a href=&quot;https://git-wip-us.apache.org/repos/asf?p=nifi.git;h=b9cb6b1&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://git-wip-us.apache.org/repos/asf?p=nifi.git;h=b9cb6b1&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/NIFI-2865&quot; title=&quot;Address issues of PublishKafka blocking when having trouble communicating with Kafka broker and improve performance&quot; class=&quot;issue-link&quot; data-issue-key=&quot;NIFI-2865&quot;&gt;&lt;del&gt;NIFI-2865&lt;/del&gt;&lt;/a&gt;: Refactored PublishKafka and PublishKafka_0_10 to allow batching of FlowFiles within a single publish and to let messages timeout if not acknowledged&lt;/p&gt;

&lt;p&gt;Signed-off-by: Bryan Bende &amp;lt;bbende@apache.org&amp;gt;&lt;/p&gt;</comment>
                            <comment id="15553054" author="jira-bot" created="Thu, 6 Oct 2016 20:12:01 +0000"  >&lt;p&gt;Commit 9304df4de060335526d29a77aa093db4004c8b2e in nifi&apos;s branch refs/heads/master from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markap14&quot; class=&quot;user-hover&quot; rel=&quot;markap14&quot;&gt;markap14&lt;/a&gt;&lt;br/&gt;
[ &lt;a href=&quot;https://git-wip-us.apache.org/repos/asf?p=nifi.git;h=9304df4&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://git-wip-us.apache.org/repos/asf?p=nifi.git;h=9304df4&lt;/a&gt; ]&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/NIFI-2865&quot; title=&quot;Address issues of PublishKafka blocking when having trouble communicating with Kafka broker and improve performance&quot; class=&quot;issue-link&quot; data-issue-key=&quot;NIFI-2865&quot;&gt;&lt;del&gt;NIFI-2865&lt;/del&gt;&lt;/a&gt;: Fixed bug in StreamDemarcator that is exposed when the final bit of data in a stream is smaller than the previous and the previous demarcation ended on a buffer length boundary&lt;/p&gt;

&lt;p&gt;This closes #1110.&lt;/p&gt;

&lt;p&gt;Signed-off-by: Bryan Bende &amp;lt;bbende@apache.org&amp;gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12994437">NIFI-2463</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 6 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i34fq7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            </customfields>
    </item>
</channel>
</rss>