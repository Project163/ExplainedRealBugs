diff --git a/python/pyspark/sql/context.py b/python/pyspark/sql/context.py
index f203e1cb555..e9f6a62feb6 100644
--- a/python/pyspark/sql/context.py
+++ b/python/pyspark/sql/context.py
@@ -87,7 +87,8 @@ class SQLContext(object):
         self._jsqlContext = jsqlContext
         _monkey_patch_RDD(self.sparkSession)
         install_exception_handler()
-        if SQLContext._instantiatedContext is None:
+        if (SQLContext._instantiatedContext is None
+                or SQLContext._instantiatedContext._sc._jsc is None):
             SQLContext._instantiatedContext = self
 
     @property
@@ -118,7 +119,8 @@ class SQLContext(object):
             "Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.",
             DeprecationWarning)
 
-        if cls._instantiatedContext is None:
+        if (cls._instantiatedContext is None
+                or SQLContext._instantiatedContext._sc._jsc is None):
             jsqlContext = sc._jvm.SparkSession.builder().sparkContext(
                 sc._jsc.sc()).getOrCreate().sqlContext()
             sparkSession = SparkSession(sc, jsqlContext.sparkSession())
diff --git a/python/pyspark/sql/session.py b/python/pyspark/sql/session.py
index 233f4927389..be4fa20a043 100644
--- a/python/pyspark/sql/session.py
+++ b/python/pyspark/sql/session.py
@@ -699,12 +699,14 @@ class SparkSession(SparkConversionMixin):
     def stop(self):
         """Stop the underlying :class:`SparkContext`.
         """
+        from pyspark.sql.context import SQLContext
         self._sc.stop()
         # We should clean the default session up. See SPARK-23228.
         self._jvm.SparkSession.clearDefaultSession()
         self._jvm.SparkSession.clearActiveSession()
         SparkSession._instantiatedSession = None
         SparkSession._activeSession = None
+        SQLContext._instantiatedContext = None
 
     @since(2.0)
     def __enter__(self):
diff --git a/python/pyspark/sql/tests/test_context.py b/python/pyspark/sql/tests/test_context.py
index 92e54340f81..d4a476dd363 100644
--- a/python/pyspark/sql/tests/test_context.py
+++ b/python/pyspark/sql/tests/test_context.py
@@ -270,7 +270,6 @@ class SQLContextTests(unittest.TestCase):
             sql_context = SQLContext.getOrCreate(sc)
             assert(isinstance(sql_context, SQLContext))
         finally:
-            SQLContext._instantiatedContext = None
             if sql_context is not None:
                 sql_context.sparkSession.stop()
             if sc is not None:
diff --git a/python/pyspark/sql/tests/test_session.py b/python/pyspark/sql/tests/test_session.py
index f7b6585db7d..5e4166e6f8e 100644
--- a/python/pyspark/sql/tests/test_session.py
+++ b/python/pyspark/sql/tests/test_session.py
@@ -225,6 +225,52 @@ class SparkSessionTests4(ReusedSQLTestCase):
                 session2.stop()
 
 
+class SparkSessionTests5(unittest.TestCase):
+
+    def setUp(self):
+        # These tests require restarting the Spark context so we set up a new one for each test
+        # rather than at the class level.
+        self.sc = SparkContext('local[4]', self.__class__.__name__, conf=SparkConf())
+        self.spark = SparkSession(self.sc)
+
+    def tearDown(self):
+        self.sc.stop()
+        self.spark.stop()
+
+    def test_sqlcontext_with_stopped_sparksession(self):
+        # SPARK-30856: test that SQLContext.getOrCreate() returns a usable instance after
+        # the SparkSession is restarted.
+        sql_context = self.spark._wrapped
+        self.spark.stop()
+        sc = SparkContext('local[4]', self.sc.appName)
+        spark = SparkSession(sc)  # Instantiate the underlying SQLContext
+        new_sql_context = spark._wrapped
+
+        self.assertIsNot(new_sql_context, sql_context)
+        self.assertIs(SQLContext.getOrCreate(sc).sparkSession, spark)
+        try:
+            df = spark.createDataFrame([(1, 2)], ['c', 'c'])
+            df.collect()
+        finally:
+            spark.stop()
+            self.assertIsNone(SQLContext._instantiatedContext)
+            sc.stop()
+
+    def test_sqlcontext_with_stopped_sparkcontext(self):
+        # SPARK-30856: test initialization via SparkSession when only the SparkContext is stopped
+        self.sc.stop()
+        self.sc = SparkContext('local[4]', self.sc.appName)
+        self.spark = SparkSession(self.sc)
+        self.assertIs(SQLContext.getOrCreate(self.sc).sparkSession, self.spark)
+
+    def test_get_sqlcontext_with_stopped_sparkcontext(self):
+        # SPARK-30856: test initialization via SQLContext.getOrCreate() when only the SparkContext
+        # is stopped
+        self.sc.stop()
+        self.sc = SparkContext('local[4]', self.sc.appName)
+        self.assertIs(SQLContext.getOrCreate(self.sc)._sc, self.sc)
+
+
 class SparkSessionBuilderTests(unittest.TestCase):
 
     def test_create_spark_context_first_then_spark_session(self):
