diff --git a/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/connect/client/ArtifactManager.scala b/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/connect/client/ArtifactManager.scala
index a0158170a5b..136a31fca3c 100644
--- a/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/connect/client/ArtifactManager.scala
+++ b/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/connect/client/ArtifactManager.scala
@@ -42,8 +42,8 @@ import org.apache.spark.util.{SparkFileUtils, SparkThreadUtils}
 /**
  * The Artifact Manager is responsible for handling and transferring artifacts from the local
  * client to the server (local/remote).
- * @param userContext
- *   The user context the artifact manager operates in.
+ * @param clientConfig
+ *   The configuration of the client that the artifact manager operates in.
  * @param sessionId
  *   An unique identifier of the session which the artifact manager belongs to.
  * @param bstub
@@ -52,7 +52,7 @@ import org.apache.spark.util.{SparkFileUtils, SparkThreadUtils}
  *   An async stub to the server.
  */
 class ArtifactManager(
-    userContext: proto.UserContext,
+    clientConfig: SparkConnectClient.Configuration,
     sessionId: String,
     bstub: CustomSparkConnectBlockingStub,
     stub: CustomSparkConnectStub) {
@@ -114,7 +114,8 @@ class ArtifactManager(
     val artifactName = CACHE_PREFIX + "/" + hash
     val request = proto.ArtifactStatusesRequest
       .newBuilder()
-      .setUserContext(userContext)
+      .setUserContext(clientConfig.userContext)
+      .setClientType(clientConfig.userAgent)
       .setSessionId(sessionId)
       .addAllNames(Arrays.asList(artifactName))
       .build()
@@ -216,7 +217,8 @@ class ArtifactManager(
       stream: StreamObserver[proto.AddArtifactsRequest]): Unit = {
     val builder = proto.AddArtifactsRequest
       .newBuilder()
-      .setUserContext(userContext)
+      .setUserContext(clientConfig.userContext)
+      .setClientType(clientConfig.userAgent)
       .setSessionId(sessionId)
     artifacts.foreach { artifact =>
       val in = new CheckedInputStream(artifact.storage.stream, new CRC32)
@@ -271,7 +273,8 @@ class ArtifactManager(
       stream: StreamObserver[proto.AddArtifactsRequest]): Unit = {
     val builder = proto.AddArtifactsRequest
       .newBuilder()
-      .setUserContext(userContext)
+      .setUserContext(clientConfig.userContext)
+      .setClientType(clientConfig.userAgent)
       .setSessionId(sessionId)
 
     val in = new CheckedInputStream(artifact.storage.stream, new CRC32)
diff --git a/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/connect/client/SparkConnectClient.scala b/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/connect/client/SparkConnectClient.scala
index d03d27a6f53..1a7fbcefc1d 100644
--- a/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/connect/client/SparkConnectClient.scala
+++ b/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/connect/client/SparkConnectClient.scala
@@ -59,7 +59,7 @@ private[sql] class SparkConnectClient(
   private[sql] val sessionId: String = UUID.randomUUID.toString
 
   private[client] val artifactManager: ArtifactManager = {
-    new ArtifactManager(userContext, sessionId, bstub, stub)
+    new ArtifactManager(configuration, sessionId, bstub, stub)
   }
 
   /**
diff --git a/connector/connect/client/jvm/src/test/scala/org/apache/spark/sql/connect/client/ArtifactSuite.scala b/connector/connect/client/jvm/src/test/scala/org/apache/spark/sql/connect/client/ArtifactSuite.scala
index a15d7562f19..7901008bc12 100644
--- a/connector/connect/client/jvm/src/test/scala/org/apache/spark/sql/connect/client/ArtifactSuite.scala
+++ b/connector/connect/client/jvm/src/test/scala/org/apache/spark/sql/connect/client/ArtifactSuite.scala
@@ -28,8 +28,8 @@ import io.grpc.inprocess.{InProcessChannelBuilder, InProcessServerBuilder}
 import org.apache.commons.codec.digest.DigestUtils.sha256Hex
 import org.scalatest.BeforeAndAfterEach
 
-import org.apache.spark.connect.proto
 import org.apache.spark.connect.proto.AddArtifactsRequest
+import org.apache.spark.sql.connect.client.SparkConnectClient.Configuration
 import org.apache.spark.sql.connect.client.util.ConnectFunSuite
 
 class ArtifactSuite extends ConnectFunSuite with BeforeAndAfterEach {
@@ -57,7 +57,7 @@ class ArtifactSuite extends ConnectFunSuite with BeforeAndAfterEach {
     retryPolicy = GrpcRetryHandler.RetryPolicy()
     bstub = new CustomSparkConnectBlockingStub(channel, retryPolicy)
     stub = new CustomSparkConnectStub(channel, retryPolicy)
-    artifactManager = new ArtifactManager(proto.UserContext.newBuilder().build(), "", bstub, stub)
+    artifactManager = new ArtifactManager(Configuration(), "", bstub, stub)
   }
 
   override def beforeEach(): Unit = {
