diff --git a/sql/api/src/main/scala/org/apache/spark/sql/types/Metadata.scala b/sql/api/src/main/scala/org/apache/spark/sql/types/Metadata.scala
index 8d7bcbac8af..4c86bfba3a1 100644
--- a/sql/api/src/main/scala/org/apache/spark/sql/types/Metadata.scala
+++ b/sql/api/src/main/scala/org/apache/spark/sql/types/Metadata.scala
@@ -305,7 +305,12 @@ class MetadataBuilder {
 
   /** Builds the [[Metadata]] instance. */
   def build(): Metadata = {
-    new Metadata(map.toMap, runtimeMap.toMap)
+    if (map.isEmpty && runtimeMap.isEmpty) {
+      // Save some memory when the metadata is empty
+      Metadata.empty
+    } else {
+      new Metadata(map.toMap, runtimeMap.toMap)
+    }
   }
 
   private def put(key: String, value: Any): this.type = {
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/TableOutputResolver.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/TableOutputResolver.scala
index adf74c489ce..5243e17afe5 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/TableOutputResolver.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/TableOutputResolver.scala
@@ -132,7 +132,11 @@ object TableOutputResolver extends SQLConfHelper with Logging {
       case (valueType, colType) if DataType.equalsIgnoreCompatibleNullability(valueType, colType) =>
         val canWriteExpr = canWrite(
           tableName, valueType, colType, byName = true, conf, addError, colPath)
-        if (canWriteExpr) checkNullability(value, col, conf, colPath) else value
+        if (canWriteExpr) {
+          applyColumnMetadata(checkNullability(value, col, conf, colPath), col)
+        } else {
+          value
+        }
       case (valueType: StructType, colType: StructType) =>
         val resolvedValue = resolveStructType(
           tableName, value, valueType, col, colType,
@@ -180,12 +184,67 @@ object TableOutputResolver extends SQLConfHelper with Logging {
       } else {
         CharVarcharUtils.stringLengthCheck(casted, attr.dataType)
       }
-      Alias(exprWithStrLenCheck, attr.name)(explicitMetadata = Some(attr.metadata))
+      applyColumnMetadata(exprWithStrLenCheck, attr)
     } else {
       value
     }
   }
 
+
+  /**
+   * Add an [[Alias]] with the name and metadata from the given target table attribute.
+   *
+   * The metadata may be used by writers to get certain table properties.
+   * For example [[org.apache.spark.sql.catalyst.json.JacksonGenerator]]
+   * looks for default value metadata to control some behavior.
+   * This is not the best design, but it is the way at this time.
+   * We should change all the writers to pick up table configuration
+   * from the table directly. However, there are many third-party
+   * connectors that may rely on this behavior.
+   *
+   * We also must remove any [[CharVarcharUtils.CHAR_VARCHAR_TYPE_STRING_METADATA_KEY]]
+   * metadata from flowing out the top of the query.
+   * If we don't do this, the write operation will remain unresolved, or worse
+   * it may flip from resolved to unresolved.  We assume that the read-side
+   * handling is performed lower in the query.
+   *
+   * Moreover, we cannot propagate other source metadata, like source table
+   * default value definitions without confusing writers with reader metadata.
+   * So we need to be sure we block the source metadata from propagating.
+   *
+   * See SPARK-52772 for a discussion on rewrites that caused trouble with
+   * going from resolved to unresolved.
+   */
+  private def applyColumnMetadata(expr: Expression, column: Attribute): NamedExpression = {
+    // We have dealt with the required write-side char/varchar processing.
+    // We do not want to transfer that information to the read-side.
+    // If we do, the write operation will fail to resolve.
+    val requiredMetadata = CharVarcharUtils.cleanMetadata(column.metadata)
+
+    // Make sure that the result has the requiredMetadata and only that.
+    // If the expr is an Attribute or NamedLambdaVariable with the proper name and metadata,
+    // it should remain stable, but we do not trust that other NamedAttributes will
+    // remain stable (namely Alias).
+    expr match {
+      case a: Attribute if a.name == column.name && a.metadata == requiredMetadata =>
+        a
+      case v: NamedLambdaVariable if v.name == column.name && v.metadata == requiredMetadata =>
+        v
+      case _ =>
+        // We cannot keep an Alias with the correct name and metadata because the
+        // metadata might be derived, and derived metadata is not stable upon rewrites.
+        // eg:
+        //   Alias(cast(attr, attr.dataType), n).metadata is empty =>
+        //   Alias(attr, n).metadata == attr.metadata.
+        val stripAlias = expr match {
+          case a: Alias => a.child
+          case _ => expr
+        }
+        Alias(stripAlias, column.name)(explicitMetadata = Some(requiredMetadata))
+    }
+  }
+
+
   private def canWrite(
       tableName: String,
       valueType: DataType,
@@ -227,20 +286,14 @@ object TableOutputResolver extends SQLConfHelper with Logging {
             tableName, newColPath.quoted
           )
         }
-        defaultExpr
+        Some(applyColumnMetadata(defaultExpr.get, expectedCol))
       } else if (matched.length > 1) {
         throw QueryCompilationErrors.incompatibleDataToTableAmbiguousColumnNameError(
           tableName, newColPath.quoted
         )
       } else {
         matchedCols += matched.head.name
-        val expectedName = expectedCol.name
-        val matchedCol = matched.head match {
-          // Save an Alias if we can change the name directly.
-          case a: Attribute => a.withName(expectedName)
-          case a: Alias => a.withName(expectedName)
-          case other => other
-        }
+        val matchedCol = matched.head
         val actualExpectedCol = expectedCol.withDataType {
           CharVarcharUtils.getRawType(expectedCol.metadata).getOrElse(expectedCol.dataType)
         }
@@ -386,7 +439,7 @@ object TableOutputResolver extends SQLConfHelper with Logging {
       } else {
         struct
       }
-      Some(Alias(res, expected.name)())
+      Some(applyColumnMetadata(res, expected))
     } else {
       None
     }
@@ -412,16 +465,15 @@ object TableOutputResolver extends SQLConfHelper with Logging {
       resolveColumnsByPosition(tableName, Seq(param), Seq(fakeAttr), conf, addError, colPath)
     }
     if (res.length == 1) {
-      if (res.head == param) {
-        // If the element type is the same, we can reuse the input array directly.
-        Some(
-          Alias(nullCheckedInput, expected.name)(
-            nonInheritableMetadataKeys =
-              Seq(CharVarcharUtils.CHAR_VARCHAR_TYPE_STRING_METADATA_KEY)))
-      } else {
-        val func = LambdaFunction(res.head, Seq(param))
-        Some(Alias(ArrayTransform(nullCheckedInput, func), expected.name)())
-      }
+      val castedArray =
+        if (res.head == param) {
+          // If the element type is the same, we can reuse the input array directly.
+          nullCheckedInput
+        } else {
+          val func = LambdaFunction(res.head, Seq(param))
+          ArrayTransform(nullCheckedInput, func)
+        }
+      Some(applyColumnMetadata(castedArray, expected))
     } else {
       None
     }
@@ -461,26 +513,25 @@ object TableOutputResolver extends SQLConfHelper with Logging {
     if (resKey.length == 1 && resValue.length == 1) {
       // If the key and value expressions have not changed, we just check original map field.
       // Otherwise, we construct a new map by adding transformations to the keys and values.
-      if (resKey.head == keyParam && resValue.head == valueParam) {
-        Some(
-          Alias(nullCheckedInput, expected.name)(
-            nonInheritableMetadataKeys =
-              Seq(CharVarcharUtils.CHAR_VARCHAR_TYPE_STRING_METADATA_KEY)))
-      } else {
-        val newKeys = if (resKey.head != keyParam) {
-          val keyFunc = LambdaFunction(resKey.head, Seq(keyParam))
-          ArrayTransform(MapKeys(nullCheckedInput), keyFunc)
-        } else {
-          MapKeys(nullCheckedInput)
-        }
-        val newValues = if (resValue.head != valueParam) {
-          val valueFunc = LambdaFunction(resValue.head, Seq(valueParam))
-          ArrayTransform(MapValues(nullCheckedInput), valueFunc)
+      val casted =
+        if (resKey.head == keyParam && resValue.head == valueParam) {
+          nullCheckedInput
         } else {
-          MapValues(nullCheckedInput)
+          val newKeys = if (resKey.head != keyParam) {
+            val keyFunc = LambdaFunction(resKey.head, Seq(keyParam))
+            ArrayTransform(MapKeys(nullCheckedInput), keyFunc)
+          } else {
+            MapKeys(nullCheckedInput)
+          }
+          val newValues = if (resValue.head != valueParam) {
+            val valueFunc = LambdaFunction(resValue.head, Seq(valueParam))
+            ArrayTransform(MapValues(nullCheckedInput), valueFunc)
+          } else {
+            MapValues(nullCheckedInput)
+          }
+          MapFromArrays(newKeys, newValues)
         }
-        Some(Alias(MapFromArrays(newKeys, newValues), expected.name)())
-      }
+      Some(applyColumnMetadata(casted, expected))
     } else {
       None
     }
@@ -525,12 +576,6 @@ object TableOutputResolver extends SQLConfHelper with Logging {
       !Cast.canUpCast(cast.child.dataType, cast.dataType)
   }
 
-  private def isCompatible(tableAttr: Attribute, queryExpr: NamedExpression): Boolean = {
-    DataTypeUtils.sameType(tableAttr.dataType, queryExpr.dataType) &&
-      tableAttr.name == queryExpr.name &&
-      tableAttr.metadata == queryExpr.metadata
-  }
-
   private def checkField(
       tableName: String,
       tableAttr: Attribute,
@@ -546,33 +591,32 @@ object TableOutputResolver extends SQLConfHelper with Logging {
     } else {
       tableAttr.dataType
     }
-    lazy val outputField = if (isCompatible(tableAttr, queryExpr)) {
-      if (requiresNullChecks(queryExpr, tableAttr, conf)) {
-        val assert = AssertNotNull(queryExpr, colPath)
-        Some(Alias(assert, tableAttr.name)(explicitMetadata = Some(tableAttr.metadata)))
-      } else {
-        Some(queryExpr)
-      }
-    } else {
-      val nullCheckedQueryExpr = checkNullability(queryExpr, tableAttr, conf, colPath)
-      val udtUnwrapped = unwrapUDT(nullCheckedQueryExpr)
-      val casted = cast(udtUnwrapped, attrTypeWithoutCharVarchar, conf, colPath.quoted)
-      val exprWithStrLenCheck = if (conf.charVarcharAsString || !attrTypeHasCharVarchar) {
-        casted
-      } else {
-        CharVarcharUtils.stringLengthCheck(casted, tableAttr.dataType)
-      }
-      // Renaming is needed for handling the following cases like
-      // 1) Column names/types do not match, e.g., INSERT INTO TABLE tab1 SELECT 1, 2
-      // 2) Target tables have column metadata
-      Some(Alias(exprWithStrLenCheck, tableAttr.name)(explicitMetadata = Some(tableAttr.metadata)))
-    }
 
     val canWriteExpr = canWrite(
       tableName, queryExpr.dataType, attrTypeWithoutCharVarchar,
       byName, conf, addError, colPath)
 
-    if (canWriteExpr) outputField else None
+    if (canWriteExpr) {
+      val prepared =
+        if (DataTypeUtils.sameType(tableAttr.dataType, queryExpr.dataType)) {
+          // If the types are an exact match, we can leave UDTs alone,
+          // we obviously do not need a cast, and the constraints of the target
+          // table char/varchar types must be met.
+          queryExpr
+        } else {
+          val udtUnwrapped = unwrapUDT(queryExpr)
+          val casted = cast(udtUnwrapped, attrTypeWithoutCharVarchar, conf, colPath.quoted)
+          if (conf.charVarcharAsString || !attrTypeHasCharVarchar) {
+            casted
+          } else {
+            CharVarcharUtils.stringLengthCheck(casted, tableAttr.dataType)
+          }
+        }
+      val nullChecked = checkNullability(prepared, tableAttr, conf, colPath)
+      Some(applyColumnMetadata(nullChecked, tableAttr))
+    } else {
+      None
+    }
   }
 
   private def unwrapUDT(expr: Expression): Expression = expr.dataType match {
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/AliasHelper.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/AliasHelper.scala
index 2cd313c873c..6ea79f73632 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/AliasHelper.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/AliasHelper.scala
@@ -99,16 +99,12 @@ trait AliasHelper {
     val res = CurrentOrigin.withOrigin(e.origin) {
       e match {
         case a: Alias =>
-          val metadata = if (a.metadata == Metadata.empty) {
-            None
-          } else {
-            Some(a.metadata)
-          }
+          // Preserve the _effective_ metadata.
           a.copy(child = trimAliases(a.child))(
             exprId = a.exprId,
             qualifier = a.qualifier,
-            explicitMetadata = metadata,
-            nonInheritableMetadataKeys = a.nonInheritableMetadataKeys)
+            explicitMetadata = Some(a.metadata),
+            nonInheritableMetadataKeys = Nil)
         case a: MultiAlias =>
           a.copy(child = trimAliases(a.child))
         case other => trimAliases(other)
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala
index a69e579b8a8..4a828375f9d 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala
@@ -597,7 +597,7 @@ object RemoveRedundantAliases extends Rule[LogicalPlan] {
     // If the alias name is different from attribute name, we can't strip it either, or we
     // may accidentally change the output schema name of the root plan.
     case a @ Alias(attr: Attribute, name)
-      if (a.metadata == Metadata.empty || a.metadata == attr.metadata) &&
+      if (a.metadata == attr.metadata) &&
         name == attr.name &&
         !excludeList.contains(attr) &&
         !excludeList.contains(a) =>
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala
index 01e49a3eda8..515424db635 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala
@@ -97,8 +97,9 @@ case class Project(projectList: Seq[NamedExpression], child: LogicalPlan)
     val noSemanticChange = projectList.length == child.output.length &&
       projectList.zip(child.output).forall {
         case (alias: Alias, attr) =>
-          alias.child.semanticEquals(attr) && alias.explicitMetadata.isEmpty &&
-            alias.qualifier.isEmpty && alias.nonInheritableMetadataKeys.isEmpty
+          alias.qualifier.isEmpty &&
+            alias.metadata == attr.metadata &&
+            alias.child.semanticEquals(attr)
         case (attr1: Attribute, attr2) => attr1.semanticEquals(attr2)
         case _ => false
       }
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/CharVarcharUtils.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/CharVarcharUtils.scala
index 6ba7e528ea2..467c04f1ed5 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/CharVarcharUtils.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/CharVarcharUtils.scala
@@ -90,9 +90,18 @@ object CharVarcharUtils extends Logging with SparkCharVarcharUtils {
    * the given attribute's metadata.
    */
   def cleanAttrMetadata(attr: AttributeReference): AttributeReference = {
-    val cleaned = new MetadataBuilder().withMetadata(attr.metadata)
-      .remove(CHAR_VARCHAR_TYPE_STRING_METADATA_KEY).build()
-    attr.withMetadata(cleaned)
+    attr.withMetadata(cleanMetadata(attr.metadata))
+  }
+
+  /**
+   * Removes the metadata entry that contains the original type string of CharType/VarcharType from
+   * the given metadata.
+   */
+  def cleanMetadata(metadata: Metadata): Metadata = {
+    new MetadataBuilder()
+      .withMetadata(metadata)
+      .remove(CHAR_VARCHAR_TYPE_STRING_METADATA_KEY)
+      .build()
   }
 
   def getRawTypeString(metadata: Metadata): Option[String] = {
diff --git a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/V2WriteAnalysisSuite.scala b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/V2WriteAnalysisSuite.scala
index 0b872d61eca..2ed7c612d3f 100644
--- a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/V2WriteAnalysisSuite.scala
+++ b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/V2WriteAnalysisSuite.scala
@@ -331,6 +331,31 @@ abstract class V2WriteAnalysisSuiteBase extends AnalysisTest {
       ArrayType(new StructType().add("x", "int").add("y", "int")),
       ArrayType(new StructType().add("y", "int").add("x", "byte")),
       hasTransform = true)
+
+    withSQLConf("spark.sql.preserveCharVarcharTypeInfo" -> "true") {
+      // exact match on VARCHAR does not need transform
+      assertArrayField(ArrayType(VarcharType(7)), ArrayType(VarcharType(7)), hasTransform = false)
+      // VARCHAR length increase could avoid transform
+      assertArrayField(ArrayType(VarcharType(7)), ArrayType(VarcharType(8)), hasTransform = true)
+      // VARCHAR length decrease requires length check
+      assertArrayField(ArrayType(VarcharType(8)), ArrayType(VarcharType(7)), hasTransform = true)
+      // Widening doesn't really require transform, but does require type change.
+      assertArrayField(ArrayType(VarcharType(7)), ArrayType(StringType), hasTransform = true)
+      // CHAR length increase needs transform to add padding
+      assertArrayField(ArrayType(CharType(7)), ArrayType(CharType(8)), hasTransform = true)
+      // VARCHAR to STRING widening doesn't really require transform, but does require type change.
+      assertArrayField(ArrayType(VarcharType(7)), ArrayType(StringType), hasTransform = true)
+      // Exact match could avoid transform, but structs always transform today...
+      assertArrayField(
+        ArrayType(new StructType().add("x", VarcharType(7)).add("y", CharType(2))),
+        ArrayType(new StructType().add("x", VarcharType(7)).add("y", CharType(2))),
+        hasTransform = true)
+      // struct needs to be reordered
+      assertArrayField(
+        ArrayType(new StructType().add("x", VarcharType(7)).add("y", CharType(2))),
+        ArrayType(new StructType().add("y", CharType(2)).add("x", CharType(7))),
+        hasTransform = true)
+    }
   }
 
   test("SPARK-48922: Avoid redundant array transform of identical expression for map type") {
@@ -487,7 +512,7 @@ abstract class V2WriteAnalysisSuiteBase extends AnalysisTest {
     val y = query.output.last
 
     val parsedPlan = byName(table, query)
-    val expectedPlan = byName(table, Project(Seq(X.withName("x"), y), query))
+    val expectedPlan = byName(table, Project(Seq(Alias(X, "x")(), y), query))
 
     assertNotResolved(parsedPlan)
     checkAnalysis(parsedPlan, expectedPlan, caseSensitive = false)
@@ -620,8 +645,8 @@ abstract class V2WriteAnalysisSuiteBase extends AnalysisTest {
     val parsedPlan = byPosition(table, query)
     val expectedPlan = byPosition(table,
       Project(Seq(
-        Alias(Cast(a, FloatType, Some(conf.sessionLocalTimeZone)), "x")(),
-        Alias(Cast(b, FloatType, Some(conf.sessionLocalTimeZone)), "y")()),
+        Alias(a, "x")(),
+        Alias(b, "y")()),
         query))
 
     assertNotResolved(parsedPlan)
@@ -641,8 +666,8 @@ abstract class V2WriteAnalysisSuiteBase extends AnalysisTest {
     val parsedPlan = byPosition(table, query)
     val expectedPlan = byPosition(table,
       Project(Seq(
-        Alias(Cast(y, FloatType, Some(conf.sessionLocalTimeZone)), "x")(),
-        Alias(Cast(x, FloatType, Some(conf.sessionLocalTimeZone)), "y")()),
+        Alias(y, "x")(),
+        Alias(x, "y")()),
         query))
 
     assertNotResolved(parsedPlan)
@@ -796,14 +821,11 @@ abstract class V2WriteAnalysisSuiteBase extends AnalysisTest {
           IsNull(queryCol),
           Literal(null, expectedColType),
           CreateNamedStruct(Seq(
-            Literal("a"), Cast(
+            Literal("a"),
               GetStructField(queryCol, 0, name = Some("x")),
-              IntegerType,
-              Some(conf.sessionLocalTimeZone)),
-            Literal("b"), Cast(
-              GetStructField(queryCol, 1, name = Some("y")),
-              IntegerType,
-              Some(conf.sessionLocalTimeZone))))),
+            Literal("b"),
+              GetStructField(queryCol, 1, name = Some("y"))
+          ))),
         "col")()),
         query)
       checkAnalysis(parsedPlan, byPosition(tableWithStructCol, expectedQuery))
@@ -1376,8 +1398,8 @@ abstract class V2WriteAnalysisSuiteBase extends AnalysisTest {
 
     val expectedPlan = OverwriteByExpression.byPosition(table,
       Project(Seq(
-        Alias(Cast(a, DoubleType, Some(conf.sessionLocalTimeZone)), "x")(),
-        Alias(Cast(b, DoubleType, Some(conf.sessionLocalTimeZone)), "y")()),
+        Alias(a, "x")(),
+        Alias(b, "y")()),
         query),
       LessThanOrEqual(x, Literal(15.0d)))
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/charvarchar.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/charvarchar.sql.out
index 524797015a2..4e864523368 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/charvarchar.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/charvarchar.sql.out
@@ -376,7 +376,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`char_tbl4`, false
 insert into char_tbl4 select c, c, v, c from str_view
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/char_tbl4, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/char_tbl4], Append, `spark_catalog`.`default`.`char_tbl4`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/char_tbl4), [c7, c8, v, s]
-+- Project [static_invoke(CharVarcharCodegenUtils.charTypeWriteSideCheck(cast(c#x as string), 7)) AS c7#x, static_invoke(CharVarcharCodegenUtils.charTypeWriteSideCheck(cast(c#x as string), 8)) AS c8#x, static_invoke(CharVarcharCodegenUtils.varcharTypeWriteSideCheck(cast(v#x as string), 6)) AS v#x, cast(c#x as string) AS s#x]
++- Project [static_invoke(CharVarcharCodegenUtils.charTypeWriteSideCheck(cast(c#x as string), 7)) AS c7#x, static_invoke(CharVarcharCodegenUtils.charTypeWriteSideCheck(cast(c#x as string), 8)) AS c8#x, static_invoke(CharVarcharCodegenUtils.varcharTypeWriteSideCheck(cast(v#x as string), 6)) AS v#x, c#x AS s#x]
    +- Project [c#x, c#x, v#x, c#x]
       +- SubqueryAlias str_view
          +- View (`str_view`, [c#x, v#x])
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/collations.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/collations.sql.out
index 957bcabb078..2e80c46dfe4 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/collations.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/collations.sql.out
@@ -9,7 +9,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t1`, false
 insert into t1 values('aaa', 'aaa')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [utf8_binary, utf8_lcase]
-+- Project [cast(col1#x as string) AS utf8_binary#x, cast(col2#x as string collate UTF8_LCASE) AS utf8_lcase#x]
++- Project [col1#x AS utf8_binary#x, cast(col2#x as string collate UTF8_LCASE) AS utf8_lcase#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -17,7 +17,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 insert into t1 values('AAA', 'AAA')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [utf8_binary, utf8_lcase]
-+- Project [cast(col1#x as string) AS utf8_binary#x, cast(col2#x as string collate UTF8_LCASE) AS utf8_lcase#x]
++- Project [col1#x AS utf8_binary#x, cast(col2#x as string collate UTF8_LCASE) AS utf8_lcase#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -25,7 +25,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 insert into t1 values('bbb', 'bbb')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [utf8_binary, utf8_lcase]
-+- Project [cast(col1#x as string) AS utf8_binary#x, cast(col2#x as string collate UTF8_LCASE) AS utf8_lcase#x]
++- Project [col1#x AS utf8_binary#x, cast(col2#x as string collate UTF8_LCASE) AS utf8_lcase#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -33,7 +33,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 insert into t1 values('BBB', 'BBB')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [utf8_binary, utf8_lcase]
-+- Project [cast(col1#x as string) AS utf8_binary#x, cast(col2#x as string collate UTF8_LCASE) AS utf8_lcase#x]
++- Project [col1#x AS utf8_binary#x, cast(col2#x as string collate UTF8_LCASE) AS utf8_lcase#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -118,7 +118,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t2`, false
 insert into t2 values('aaa', 'aaa')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t2, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t2], Append, `spark_catalog`.`default`.`t2`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t2), [utf8_binary, utf8_lcase]
-+- Project [cast(col1#x as string) AS utf8_binary#x, cast(col2#x as string collate UTF8_LCASE) AS utf8_lcase#x]
++- Project [col1#x AS utf8_binary#x, cast(col2#x as string collate UTF8_LCASE) AS utf8_lcase#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -126,7 +126,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 insert into t2 values('bbb', 'bbb')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t2, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t2], Append, `spark_catalog`.`default`.`t2`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t2), [utf8_binary, utf8_lcase]
-+- Project [cast(col1#x as string) AS utf8_binary#x, cast(col2#x as string collate UTF8_LCASE) AS utf8_lcase#x]
++- Project [col1#x AS utf8_binary#x, cast(col2#x as string collate UTF8_LCASE) AS utf8_lcase#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -526,7 +526,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t4`, false
 insert into t4 values('a:1,b:2,c:3', ',', ':')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t4, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t4], Append, `spark_catalog`.`default`.`t4`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t4), [text, pairDelim, keyValueDelim]
-+- Project [cast(col1#x as string) AS text#x, cast(col2#x as string collate UTF8_LCASE) AS pairDelim#x, cast(col3#x as string) AS keyValueDelim#x]
++- Project [col1#x AS text#x, cast(col2#x as string collate UTF8_LCASE) AS pairDelim#x, col3#x AS keyValueDelim#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -600,7 +600,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t5`, false
 insert into t5 values ('Spark', 'Spark', 'SQL')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t5, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t5], Append, `spark_catalog`.`default`.`t5`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t5), [s, utf8_binary, utf8_lcase]
-+- Project [cast(col1#x as string) AS s#x, cast(col2#x as string) AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
++- Project [col1#x AS s#x, col2#x AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -608,7 +608,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 insert into t5 values ('aaAaAAaA', 'aaAaAAaA', 'aaAaAAaA')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t5, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t5], Append, `spark_catalog`.`default`.`t5`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t5), [s, utf8_binary, utf8_lcase]
-+- Project [cast(col1#x as string) AS s#x, cast(col2#x as string) AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
++- Project [col1#x AS s#x, col2#x AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -616,7 +616,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 insert into t5 values ('aaAaAAaA', 'aaAaAAaA', 'aaAaaAaA')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t5, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t5], Append, `spark_catalog`.`default`.`t5`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t5), [s, utf8_binary, utf8_lcase]
-+- Project [cast(col1#x as string) AS s#x, cast(col2#x as string) AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
++- Project [col1#x AS s#x, col2#x AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -624,7 +624,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 insert into t5 values ('aaAaAAaA', 'aaAaAAaA', 'aaAaaAaAaaAaaAaAaaAaaAaA')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t5, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t5], Append, `spark_catalog`.`default`.`t5`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t5), [s, utf8_binary, utf8_lcase]
-+- Project [cast(col1#x as string) AS s#x, cast(col2#x as string) AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
++- Project [col1#x AS s#x, col2#x AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -632,7 +632,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 insert into t5 values ('bbAbaAbA', 'bbAbAAbA', 'a')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t5, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t5], Append, `spark_catalog`.`default`.`t5`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t5), [s, utf8_binary, utf8_lcase]
-+- Project [cast(col1#x as string) AS s#x, cast(col2#x as string) AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
++- Project [col1#x AS s#x, col2#x AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -640,7 +640,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 insert into t5 values ('İo', 'İo', 'İo')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t5, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t5], Append, `spark_catalog`.`default`.`t5`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t5), [s, utf8_binary, utf8_lcase]
-+- Project [cast(col1#x as string) AS s#x, cast(col2#x as string) AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
++- Project [col1#x AS s#x, col2#x AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -648,7 +648,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 insert into t5 values ('İo', 'İo', 'İo ')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t5, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t5], Append, `spark_catalog`.`default`.`t5`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t5), [s, utf8_binary, utf8_lcase]
-+- Project [cast(col1#x as string) AS s#x, cast(col2#x as string) AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
++- Project [col1#x AS s#x, col2#x AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -656,7 +656,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 insert into t5 values ('İo', 'İo ', 'İo')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t5, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t5], Append, `spark_catalog`.`default`.`t5`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t5), [s, utf8_binary, utf8_lcase]
-+- Project [cast(col1#x as string) AS s#x, cast(col2#x as string) AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
++- Project [col1#x AS s#x, col2#x AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -664,7 +664,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 insert into t5 values ('İo', 'İo', 'i̇o')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t5, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t5], Append, `spark_catalog`.`default`.`t5`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t5), [s, utf8_binary, utf8_lcase]
-+- Project [cast(col1#x as string) AS s#x, cast(col2#x as string) AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
++- Project [col1#x AS s#x, col2#x AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -672,7 +672,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 insert into t5 values ('efd2', 'efd2', 'efd2')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t5, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t5], Append, `spark_catalog`.`default`.`t5`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t5), [s, utf8_binary, utf8_lcase]
-+- Project [cast(col1#x as string) AS s#x, cast(col2#x as string) AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
++- Project [col1#x AS s#x, col2#x AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -680,7 +680,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 insert into t5 values ('Hello, world! Nice day.', 'Hello, world! Nice day.', 'Hello, world! Nice day.')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t5, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t5], Append, `spark_catalog`.`default`.`t5`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t5), [s, utf8_binary, utf8_lcase]
-+- Project [cast(col1#x as string) AS s#x, cast(col2#x as string) AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
++- Project [col1#x AS s#x, col2#x AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -688,7 +688,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 insert into t5 values ('Something else. Nothing here.', 'Something else. Nothing here.', 'Something else. Nothing here.')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t5, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t5], Append, `spark_catalog`.`default`.`t5`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t5), [s, utf8_binary, utf8_lcase]
-+- Project [cast(col1#x as string) AS s#x, cast(col2#x as string) AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
++- Project [col1#x AS s#x, col2#x AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -696,7 +696,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 insert into t5 values ('kitten', 'kitten', 'sitTing')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t5, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t5], Append, `spark_catalog`.`default`.`t5`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t5), [s, utf8_binary, utf8_lcase]
-+- Project [cast(col1#x as string) AS s#x, cast(col2#x as string) AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
++- Project [col1#x AS s#x, col2#x AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -704,7 +704,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 insert into t5 values ('abc', 'abc', 'abc')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t5, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t5], Append, `spark_catalog`.`default`.`t5`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t5), [s, utf8_binary, utf8_lcase]
-+- Project [cast(col1#x as string) AS s#x, cast(col2#x as string) AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
++- Project [col1#x AS s#x, col2#x AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -712,7 +712,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 insert into t5 values ('abcdcba', 'abcdcba', 'aBcDCbA')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t5, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t5], Append, `spark_catalog`.`default`.`t5`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t5), [s, utf8_binary, utf8_lcase]
-+- Project [cast(col1#x as string) AS s#x, cast(col2#x as string) AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
++- Project [col1#x AS s#x, col2#x AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -770,7 +770,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t8`, false
 insert into t8 values ('%s%s', 'abCdE', 'abCdE')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t8, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t8], Append, `spark_catalog`.`default`.`t8`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t8), [format, utf8_binary, utf8_lcase]
-+- Project [cast(col1#x as string) AS format#x, cast(col2#x as string) AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
++- Project [col1#x AS format#x, col2#x AS utf8_binary#x, cast(col3#x as string collate UTF8_LCASE) AS utf8_lcase#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -806,7 +806,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t10`, false
 insert into t10 values ('aaAaAAaA', 'aaAaaAaA')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t10, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t10], Append, `spark_catalog`.`default`.`t10`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t10), [utf8_binary, utf8_lcase]
-+- Project [cast(col1#x as string) AS utf8_binary#x, cast(col2#x as string collate UTF8_LCASE) AS utf8_lcase#x]
++- Project [col1#x AS utf8_binary#x, cast(col2#x as string collate UTF8_LCASE) AS utf8_lcase#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -814,7 +814,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 insert into t10 values ('efd2', 'efd2')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t10, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t10], Append, `spark_catalog`.`default`.`t10`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t10), [utf8_binary, utf8_lcase]
-+- Project [cast(col1#x as string) AS utf8_binary#x, cast(col2#x as string collate UTF8_LCASE) AS utf8_lcase#x]
++- Project [col1#x AS utf8_binary#x, cast(col2#x as string collate UTF8_LCASE) AS utf8_lcase#x]
    +- LocalRelation [col1#x, col2#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/columnresolution.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/columnresolution.sql.out
index e0ef6c1248a..5a2447d4400 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/columnresolution.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/columnresolution.sql.out
@@ -337,7 +337,7 @@ CreateDataSourceTableCommand `spark_catalog`.`mydb1`.`t5`, false
 INSERT INTO t5 VALUES(1, (2, 3))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/mydb1.db/t5, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/mydb1.db/t5], Append, `spark_catalog`.`mydb1`.`t5`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/mydb1.db/t5), [i1, t5]
-+- Project [cast(col1#x as int) AS i1#x, named_struct(i1, cast(col2#x.col1 as int), i2, cast(col2#x.col2 as int)) AS t5#x]
++- Project [col1#x AS i1#x, named_struct(i1, col2#x.col1, i2, col2#x.col2) AS t5#x]
    +- LocalRelation [col1#x, col2#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/cte-recursion.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/cte-recursion.sql.out
index 66d01bc838a..32169cb51b9 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/cte-recursion.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/cte-recursion.sql.out
@@ -1366,7 +1366,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`tb`, false
 INSERT INTO tb VALUES (0), (1)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/tb, false, JSON, [path=file:[not included in comparison]/{warehouse_dir}/tb], Append, `spark_catalog`.`default`.`tb`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/tb), [next]
-+- Project [cast(col1#x as int) AS next#x]
++- Project [col1#x AS next#x]
    +- LocalRelation [col1#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/decimalArithmeticOperations.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/decimalArithmeticOperations.sql.out
index d75f4d41bd4..5c136f6fe17 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/decimalArithmeticOperations.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/decimalArithmeticOperations.sql.out
@@ -51,7 +51,7 @@ insert into decimals_test values(1, 100.0, 999.0), (2, 12345.123, 12345.123),
   (3, 0.1234567891011, 1234.1), (4, 123456789123456789.0, 1.123456789123456789)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/decimals_test, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/decimals_test], Append, `spark_catalog`.`default`.`decimals_test`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/decimals_test), [id, a, b]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,18)) AS a#x, cast(col3#x as decimal(38,18)) AS b#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,18)) AS a#x, cast(col3#x as decimal(38,18)) AS b#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/describe-part-after-analyze.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/describe-part-after-analyze.sql.out
index 8bb042e9979..5da91fd7401 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/describe-part-after-analyze.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/describe-part-after-analyze.sql.out
@@ -12,7 +12,7 @@ VALUES ('k1', 100), ('k2', 200), ('k3', 300)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, [ds=2017-08-01, hr=10], false, [ds#x, hr#x], Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.CatalogFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [key, value, ds, hr]
 +- Project [key#x, value#x, cast(2017-08-01 as string) AS ds#x, cast(10 as int) AS hr#x]
-   +- Project [cast(col1#x as string) AS key#x, cast(col2#x as string) AS value#x]
+   +- Project [col1#x AS key#x, cast(col2#x as string) AS value#x]
       +- LocalRelation [col1#x, col2#x]
 
 
@@ -22,7 +22,7 @@ VALUES ('k1', 101), ('k2', 201), ('k3', 301), ('k4', 401)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, [ds=2017-08-01, hr=11], false, [ds#x, hr#x], Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.CatalogFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [key, value, ds, hr]
 +- Project [key#x, value#x, cast(2017-08-01 as string) AS ds#x, cast(11 as int) AS hr#x]
-   +- Project [cast(col1#x as string) AS key#x, cast(col2#x as string) AS value#x]
+   +- Project [col1#x AS key#x, cast(col2#x as string) AS value#x]
       +- LocalRelation [col1#x, col2#x]
 
 
@@ -32,7 +32,7 @@ VALUES ('k1', 102), ('k2', 202)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, [ds=2017-09-01, hr=5], false, [ds#x, hr#x], Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.CatalogFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [key, value, ds, hr]
 +- Project [key#x, value#x, cast(2017-09-01 as string) AS ds#x, cast(5 as int) AS hr#x]
-   +- Project [cast(col1#x as string) AS key#x, cast(col2#x as string) AS value#x]
+   +- Project [col1#x AS key#x, cast(col2#x as string) AS value#x]
       +- LocalRelation [col1#x, col2#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/execute-immediate.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/execute-immediate.sql.out
index 78bf1ccb167..d575cac56d2 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/execute-immediate.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/execute-immediate.sql.out
@@ -204,7 +204,7 @@ Project [id#x, name#x, data#x]
 EXECUTE IMMEDIATE 'INSERT INTO x VALUES(?)' USING 1
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/x, false, CSV, [path=file:[not included in comparison]/{warehouse_dir}/x], Append, `spark_catalog`.`default`.`x`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/x), [id]
-+- Project [cast(col1#x as int) AS id#x]
++- Project [col1#x AS id#x]
    +- LocalRelation [col1#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/identifier-clause.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/identifier-clause.sql.out
index deb2526e346..38b28f2d3b2 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/identifier-clause.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/identifier-clause.sql.out
@@ -84,7 +84,7 @@ SetNamespaceCommand [s]
 INSERT INTO IDENTIFIER('ta' || 'b') VALUES(1)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/s.db/tab, false, CSV, [path=file:[not included in comparison]/{warehouse_dir}/s.db/tab], Append, `spark_catalog`.`s`.`tab`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/s.db/tab), [c1]
-+- Project [cast(col1#x as int) AS c1#x]
++- Project [col1#x AS c1#x]
    +- LocalRelation [col1#x]
 
 
@@ -308,7 +308,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`tab`, false
 INSERT INTO tab VALUES (1)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/tab, false, CSV, [path=file:[not included in comparison]/{warehouse_dir}/tab], Append, `spark_catalog`.`default`.`tab`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/tab), [c1]
-+- Project [cast(col1#x as int) AS c1#x]
++- Project [col1#x AS c1#x]
    +- LocalRelation [col1#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/nonansi/decimalArithmeticOperations.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/nonansi/decimalArithmeticOperations.sql.out
index d75f4d41bd4..5c136f6fe17 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/nonansi/decimalArithmeticOperations.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/nonansi/decimalArithmeticOperations.sql.out
@@ -51,7 +51,7 @@ insert into decimals_test values(1, 100.0, 999.0), (2, 12345.123, 12345.123),
   (3, 0.1234567891011, 1234.1), (4, 123456789123456789.0, 1.123456789123456789)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/decimals_test, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/decimals_test], Append, `spark_catalog`.`default`.`decimals_test`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/decimals_test), [id, a, b]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,18)) AS a#x, cast(col3#x as decimal(38,18)) AS b#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,18)) AS a#x, cast(col3#x as decimal(38,18)) AS b#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/null-handling.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/null-handling.sql.out
index 37d84f6c5fc..86ee26bd1cf 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/null-handling.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/null-handling.sql.out
@@ -9,7 +9,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t1`, false
 insert into t1 values(1,0,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [a, b, c]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as int) AS c#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -17,7 +17,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 insert into t1 values(2,0,1)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [a, b, c]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as int) AS c#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -25,7 +25,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 insert into t1 values(3,1,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [a, b, c]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as int) AS c#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -33,7 +33,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 insert into t1 values(4,1,1)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [a, b, c]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as int) AS c#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -41,7 +41,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 insert into t1 values(5,null,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [a, b, c]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as int) AS c#x]
++- Project [col1#x AS a#x, cast(col2#x as int) AS b#x, col3#x AS c#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -49,7 +49,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 insert into t1 values(6,null,1)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [a, b, c]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as int) AS c#x]
++- Project [col1#x AS a#x, cast(col2#x as int) AS b#x, col3#x AS c#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -57,7 +57,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 insert into t1 values(7,null,null)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [a, b, c]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as int) AS c#x]
++- Project [col1#x AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as int) AS c#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/pipe-operators.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/pipe-operators.sql.out
index 3de5ec38a2b..dda0722e21d 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/pipe-operators.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/pipe-operators.sql.out
@@ -16,7 +16,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t`, false
 insert into t values (0, 'abc'), (1, 'def')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, false, CSV, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [x, y]
-+- Project [cast(col1#x as int) AS x#x, cast(col2#x as string) AS y#x]
++- Project [col1#x AS x#x, col2#x AS y#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -37,7 +37,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`other`, false
 insert into other values (1, 1), (1, 2), (2, 4)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/other, false, JSON, [path=file:[not included in comparison]/{warehouse_dir}/other], Append, `spark_catalog`.`default`.`other`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/other), [a, b]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x]
++- Project [col1#x AS a#x, col2#x AS b#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -58,7 +58,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`st`, false
 insert into st values (1, (2, 3))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/st, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/st], Append, `spark_catalog`.`default`.`st`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/st), [x, col]
-+- Project [cast(col1#x as int) AS x#x, named_struct(i1, cast(col2#x.col1 as int), i2, cast(col2#x.col2 as int)) AS col#x]
++- Project [col1#x AS x#x, named_struct(i1, col2#x.col1, i2, col2#x.col2) AS col#x]
    +- LocalRelation [col1#x, col2#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/boolean.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/boolean.sql.out
index 277ab866f97..9e5733212c2 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/boolean.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/boolean.sql.out
@@ -290,7 +290,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`BOOLTBL1`, false
 INSERT INTO BOOLTBL1 VALUES (cast('t' as boolean))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/booltbl1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/booltbl1], Append, `spark_catalog`.`default`.`booltbl1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/booltbl1), [f1]
-+- Project [cast(col1#x as boolean) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -298,7 +298,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO BOOLTBL1 VALUES (cast('True' as boolean))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/booltbl1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/booltbl1], Append, `spark_catalog`.`default`.`booltbl1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/booltbl1), [f1]
-+- Project [cast(col1#x as boolean) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -306,7 +306,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO BOOLTBL1 VALUES (cast('true' as boolean))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/booltbl1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/booltbl1], Append, `spark_catalog`.`default`.`booltbl1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/booltbl1), [f1]
-+- Project [cast(col1#x as boolean) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -355,7 +355,7 @@ Project [ AS zero#x, f1#x]
 INSERT INTO BOOLTBL1 VALUES (boolean('f'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/booltbl1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/booltbl1], Append, `spark_catalog`.`default`.`booltbl1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/booltbl1), [f1]
-+- Project [cast(col1#x as boolean) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -380,7 +380,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`BOOLTBL2`, false
 INSERT INTO BOOLTBL2 VALUES (boolean('f'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/booltbl2, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/booltbl2], Append, `spark_catalog`.`default`.`booltbl2`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/booltbl2), [f1]
-+- Project [cast(col1#x as boolean) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -388,7 +388,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO BOOLTBL2 VALUES (boolean('false'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/booltbl2, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/booltbl2], Append, `spark_catalog`.`default`.`booltbl2`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/booltbl2), [f1]
-+- Project [cast(col1#x as boolean) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -396,7 +396,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO BOOLTBL2 VALUES (boolean('False'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/booltbl2, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/booltbl2], Append, `spark_catalog`.`default`.`booltbl2`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/booltbl2), [f1]
-+- Project [cast(col1#x as boolean) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -404,7 +404,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO BOOLTBL2 VALUES (boolean('FALSE'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/booltbl2, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/booltbl2], Append, `spark_catalog`.`default`.`booltbl2`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/booltbl2), [f1]
-+- Project [cast(col1#x as boolean) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -593,7 +593,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`BOOLTBL3`, false
 INSERT INTO BOOLTBL3 VALUES ('true', true, 1)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/booltbl3, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/booltbl3], Append, `spark_catalog`.`default`.`booltbl3`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/booltbl3), [d, b, o]
-+- Project [cast(col1#x as string) AS d#x, cast(col2#x as boolean) AS b#x, cast(col3#x as int) AS o#x]
++- Project [col1#x AS d#x, col2#x AS b#x, col3#x AS o#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -601,7 +601,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO BOOLTBL3 VALUES ('false', false, 2)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/booltbl3, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/booltbl3], Append, `spark_catalog`.`default`.`booltbl3`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/booltbl3), [d, b, o]
-+- Project [cast(col1#x as string) AS d#x, cast(col2#x as boolean) AS b#x, cast(col3#x as int) AS o#x]
++- Project [col1#x AS d#x, col2#x AS b#x, col3#x AS o#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -609,7 +609,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO BOOLTBL3 VALUES ('null', null, 3)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/booltbl3, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/booltbl3], Append, `spark_catalog`.`default`.`booltbl3`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/booltbl3), [d, b, o]
-+- Project [cast(col1#x as string) AS d#x, cast(col2#x as boolean) AS b#x, cast(col3#x as int) AS o#x]
++- Project [col1#x AS d#x, cast(col2#x as boolean) AS b#x, col3#x AS o#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -641,7 +641,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`booltbl4`, false
 INSERT INTO booltbl4 VALUES (false, true, null)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/booltbl4, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/booltbl4], Append, `spark_catalog`.`default`.`booltbl4`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/booltbl4), [isfalse, istrue, isnul]
-+- Project [cast(col1#x as boolean) AS isfalse#x, cast(col2#x as boolean) AS istrue#x, cast(col3#x as boolean) AS isnul#x]
++- Project [col1#x AS isfalse#x, col2#x AS istrue#x, cast(col3#x as boolean) AS isnul#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/case.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/case.sql.out
index 31d199e4754..465da4d2e73 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/case.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/case.sql.out
@@ -21,7 +21,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`CASE2_TBL`, false
 INSERT INTO CASE_TBL VALUES (1, 10.1)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case_tbl], Append, `spark_catalog`.`default`.`case_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case_tbl), [i, f]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as double) AS f#x]
++- Project [col1#x AS i#x, cast(col2#x as double) AS f#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -29,7 +29,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO CASE_TBL VALUES (2, 20.2)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case_tbl], Append, `spark_catalog`.`default`.`case_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case_tbl), [i, f]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as double) AS f#x]
++- Project [col1#x AS i#x, cast(col2#x as double) AS f#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -37,7 +37,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO CASE_TBL VALUES (3, -30.3)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case_tbl], Append, `spark_catalog`.`default`.`case_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case_tbl), [i, f]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as double) AS f#x]
++- Project [col1#x AS i#x, cast(col2#x as double) AS f#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -45,7 +45,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO CASE_TBL VALUES (4, NULL)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case_tbl], Append, `spark_catalog`.`default`.`case_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case_tbl), [i, f]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as double) AS f#x]
++- Project [col1#x AS i#x, cast(col2#x as double) AS f#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -53,7 +53,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO CASE2_TBL VALUES (1, -1)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case2_tbl], Append, `spark_catalog`.`default`.`case2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case2_tbl), [i, j]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x]
++- Project [col1#x AS i#x, col2#x AS j#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -61,7 +61,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO CASE2_TBL VALUES (2, -2)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case2_tbl], Append, `spark_catalog`.`default`.`case2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case2_tbl), [i, j]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x]
++- Project [col1#x AS i#x, col2#x AS j#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -69,7 +69,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO CASE2_TBL VALUES (3, -3)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case2_tbl], Append, `spark_catalog`.`default`.`case2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case2_tbl), [i, j]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x]
++- Project [col1#x AS i#x, col2#x AS j#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -77,7 +77,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO CASE2_TBL VALUES (2, -4)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case2_tbl], Append, `spark_catalog`.`default`.`case2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case2_tbl), [i, j]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x]
++- Project [col1#x AS i#x, col2#x AS j#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -85,7 +85,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO CASE2_TBL VALUES (1, NULL)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case2_tbl], Append, `spark_catalog`.`default`.`case2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case2_tbl), [i, j]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x]
++- Project [col1#x AS i#x, cast(col2#x as int) AS j#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -93,7 +93,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO CASE2_TBL VALUES (NULL, -6)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case2_tbl], Append, `spark_catalog`.`default`.`case2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case2_tbl), [i, j]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x]
++- Project [cast(col1#x as int) AS i#x, col2#x AS j#x]
    +- LocalRelation [col1#x, col2#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/create_view.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/create_view.sql.out
index e4e4be8fee7..e39f479f6ea 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/create_view.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/create_view.sql.out
@@ -94,7 +94,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`viewtest_tbl`, false
 INSERT INTO viewtest_tbl VALUES (5, 10), (10, 15), (15, 20), (20, 25)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/viewtest_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/viewtest_tbl], Append, `spark_catalog`.`default`.`viewtest_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/viewtest_tbl), [a, b]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x]
++- Project [col1#x AS a#x, col2#x AS b#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -833,7 +833,7 @@ CreateDataSourceTableCommand `spark_catalog`.`testviewschm2`.`tmptbl`, false
 INSERT INTO tmptbl VALUES (1, 1)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/testviewschm2.db/tmptbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/testviewschm2.db/tmptbl], Append, `spark_catalog`.`testviewschm2`.`tmptbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/testviewschm2.db/tmptbl), [i, j]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x]
++- Project [col1#x AS i#x, col2#x AS j#x]
    +- LocalRelation [col1#x, col2#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/date.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/date.sql.out
index d3ac6a3eb2b..16d1ffffd7d 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/date.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/date.sql.out
@@ -9,7 +9,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`DATE_TBL`, false
 INSERT INTO DATE_TBL VALUES (date('1957-04-09'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/date_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/date_tbl], Append, `spark_catalog`.`default`.`date_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/date_tbl), [f1]
-+- Project [cast(col1#x as date) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -17,7 +17,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO DATE_TBL VALUES (date('1957-06-13'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/date_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/date_tbl], Append, `spark_catalog`.`default`.`date_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/date_tbl), [f1]
-+- Project [cast(col1#x as date) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -25,7 +25,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO DATE_TBL VALUES (date('1996-02-28'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/date_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/date_tbl], Append, `spark_catalog`.`default`.`date_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/date_tbl), [f1]
-+- Project [cast(col1#x as date) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -33,7 +33,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO DATE_TBL VALUES (date('1996-02-29'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/date_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/date_tbl], Append, `spark_catalog`.`default`.`date_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/date_tbl), [f1]
-+- Project [cast(col1#x as date) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -41,7 +41,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO DATE_TBL VALUES (date('1996-03-01'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/date_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/date_tbl], Append, `spark_catalog`.`default`.`date_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/date_tbl), [f1]
-+- Project [cast(col1#x as date) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -49,7 +49,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO DATE_TBL VALUES (date('1996-03-02'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/date_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/date_tbl], Append, `spark_catalog`.`default`.`date_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/date_tbl), [f1]
-+- Project [cast(col1#x as date) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -57,7 +57,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO DATE_TBL VALUES (date('1997-02-28'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/date_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/date_tbl], Append, `spark_catalog`.`default`.`date_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/date_tbl), [f1]
-+- Project [cast(col1#x as date) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -65,7 +65,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO DATE_TBL VALUES (date('1997-03-01'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/date_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/date_tbl], Append, `spark_catalog`.`default`.`date_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/date_tbl), [f1]
-+- Project [cast(col1#x as date) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -73,7 +73,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO DATE_TBL VALUES (date('1997-03-02'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/date_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/date_tbl], Append, `spark_catalog`.`default`.`date_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/date_tbl), [f1]
-+- Project [cast(col1#x as date) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -81,7 +81,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO DATE_TBL VALUES (date('2000-04-01'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/date_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/date_tbl], Append, `spark_catalog`.`default`.`date_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/date_tbl), [f1]
-+- Project [cast(col1#x as date) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -89,7 +89,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO DATE_TBL VALUES (date('2000-04-02'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/date_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/date_tbl], Append, `spark_catalog`.`default`.`date_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/date_tbl), [f1]
-+- Project [cast(col1#x as date) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -97,7 +97,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO DATE_TBL VALUES (date('2000-04-03'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/date_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/date_tbl], Append, `spark_catalog`.`default`.`date_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/date_tbl), [f1]
-+- Project [cast(col1#x as date) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -105,7 +105,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO DATE_TBL VALUES (date('2038-04-08'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/date_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/date_tbl], Append, `spark_catalog`.`default`.`date_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/date_tbl), [f1]
-+- Project [cast(col1#x as date) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -113,7 +113,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO DATE_TBL VALUES (date('2039-04-09'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/date_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/date_tbl], Append, `spark_catalog`.`default`.`date_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/date_tbl), [f1]
-+- Project [cast(col1#x as date) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -121,7 +121,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO DATE_TBL VALUES (date('2040-04-10'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/date_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/date_tbl], Append, `spark_catalog`.`default`.`date_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/date_tbl), [f1]
-+- Project [cast(col1#x as date) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/float4.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/float4.sql.out
index 0618a1d8432..445edd15f47 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/float4.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/float4.sql.out
@@ -9,7 +9,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`FLOAT4_TBL`, false
 INSERT INTO FLOAT4_TBL VALUES (float('    0.0'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/float4_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/float4_tbl], Append, `spark_catalog`.`default`.`float4_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/float4_tbl), [f1]
-+- Project [cast(col1#x as float) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -17,7 +17,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO FLOAT4_TBL VALUES (float('1004.30   '))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/float4_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/float4_tbl], Append, `spark_catalog`.`default`.`float4_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/float4_tbl), [f1]
-+- Project [cast(col1#x as float) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -25,7 +25,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO FLOAT4_TBL VALUES (float('     -34.84    '))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/float4_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/float4_tbl], Append, `spark_catalog`.`default`.`float4_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/float4_tbl), [f1]
-+- Project [cast(col1#x as float) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -33,7 +33,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO FLOAT4_TBL VALUES (float('1.2345678901234e+20'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/float4_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/float4_tbl], Append, `spark_catalog`.`default`.`float4_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/float4_tbl), [f1]
-+- Project [cast(col1#x as float) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -41,7 +41,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO FLOAT4_TBL VALUES (float('1.2345678901234e-20'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/float4_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/float4_tbl], Append, `spark_catalog`.`default`.`float4_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/float4_tbl), [f1]
-+- Project [cast(col1#x as float) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/float8.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/float8.sql.out
index 2f2beda4f1c..31f315d61bc 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/float8.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/float8.sql.out
@@ -9,7 +9,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`FLOAT8_TBL`, false
 INSERT INTO FLOAT8_TBL VALUES (double('    0.0   '))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/float8_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/float8_tbl], Append, `spark_catalog`.`default`.`float8_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/float8_tbl), [f1]
-+- Project [cast(col1#x as double) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -17,7 +17,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO FLOAT8_TBL VALUES (double('1004.30  '))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/float8_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/float8_tbl], Append, `spark_catalog`.`default`.`float8_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/float8_tbl), [f1]
-+- Project [cast(col1#x as double) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -25,7 +25,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO FLOAT8_TBL VALUES (double('   -34.84'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/float8_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/float8_tbl], Append, `spark_catalog`.`default`.`float8_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/float8_tbl), [f1]
-+- Project [cast(col1#x as double) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -33,7 +33,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO FLOAT8_TBL VALUES (double('1.2345678901234e+200'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/float8_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/float8_tbl], Append, `spark_catalog`.`default`.`float8_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/float8_tbl), [f1]
-+- Project [cast(col1#x as double) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -41,7 +41,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO FLOAT8_TBL VALUES (double('1.2345678901234e-200'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/float8_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/float8_tbl], Append, `spark_catalog`.`default`.`float8_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/float8_tbl), [f1]
-+- Project [cast(col1#x as double) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -612,7 +612,7 @@ TruncateTableCommand `spark_catalog`.`default`.`float8_tbl`
 INSERT INTO FLOAT8_TBL VALUES (double('0.0'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/float8_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/float8_tbl], Append, `spark_catalog`.`default`.`float8_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/float8_tbl), [f1]
-+- Project [cast(col1#x as double) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -620,7 +620,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO FLOAT8_TBL VALUES (double('-34.84'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/float8_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/float8_tbl], Append, `spark_catalog`.`default`.`float8_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/float8_tbl), [f1]
-+- Project [cast(col1#x as double) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -628,7 +628,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO FLOAT8_TBL VALUES (double('-1004.30'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/float8_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/float8_tbl], Append, `spark_catalog`.`default`.`float8_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/float8_tbl), [f1]
-+- Project [cast(col1#x as double) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -636,7 +636,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO FLOAT8_TBL VALUES (double('-1.2345678901234e+200'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/float8_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/float8_tbl], Append, `spark_catalog`.`default`.`float8_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/float8_tbl), [f1]
-+- Project [cast(col1#x as double) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -644,7 +644,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO FLOAT8_TBL VALUES (double('-1.2345678901234e-200'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/float8_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/float8_tbl], Append, `spark_catalog`.`default`.`float8_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/float8_tbl), [f1]
-+- Project [cast(col1#x as double) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/groupingsets.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/groupingsets.sql.out
index 7cef2a51d13..84b280b5e9e 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/groupingsets.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/groupingsets.sql.out
@@ -33,7 +33,7 @@ insert into gstest2 values
   (2, 2, 2, 2, 2, 2, 2, 2)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/gstest2, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/gstest2], Append, `spark_catalog`.`default`.`gstest2`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/gstest2), [a, b, c, d, e, f, g, h]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as int) AS c#x, cast(col4#x as int) AS d#x, cast(col5#x as int) AS e#x, cast(col6#x as int) AS f#x, cast(col7#x as int) AS g#x, cast(col8#x as int) AS h#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x, col5#x AS e#x, col6#x AS f#x, col7#x AS g#x, col8#x AS h#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x, col5#x, col6#x, col7#x, col8#x]
 
 
@@ -49,7 +49,7 @@ insert into gstest3 values
   (2, 2, 2, 2)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/gstest3, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/gstest3], Append, `spark_catalog`.`default`.`gstest3`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/gstest3), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as int) AS c#x, cast(col4#x as int) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -68,7 +68,7 @@ values (1,1,tinyint('0'),1), (2,2,tinyint('1'),1),
        (7,64,tinyint('2'),1), (8,128,tinyint('3'),1)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/gstest4, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/gstest4], Append, `spark_catalog`.`default`.`gstest4`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/gstest4), [id, v, unhashable_col, unsortable_col]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as int) AS v#x, cast(col3#x as tinyint) AS unhashable_col#x, cast(col4#x as int) AS unsortable_col#x]
++- Project [col1#x AS id#x, col2#x AS v#x, col3#x AS unhashable_col#x, col4#x AS unsortable_col#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/int2.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/int2.sql.out
index 9dda3c0dc42..6720132a09a 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/int2.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/int2.sql.out
@@ -9,7 +9,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`INT2_TBL`, false
 INSERT INTO INT2_TBL VALUES (smallint(trim('0   ')))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/int2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/int2_tbl], Append, `spark_catalog`.`default`.`int2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/int2_tbl), [f1]
-+- Project [cast(col1#x as smallint) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -17,7 +17,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO INT2_TBL VALUES (smallint(trim('  1234 ')))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/int2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/int2_tbl], Append, `spark_catalog`.`default`.`int2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/int2_tbl), [f1]
-+- Project [cast(col1#x as smallint) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -25,7 +25,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO INT2_TBL VALUES (smallint(trim('    -1234')))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/int2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/int2_tbl], Append, `spark_catalog`.`default`.`int2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/int2_tbl), [f1]
-+- Project [cast(col1#x as smallint) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -33,7 +33,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO INT2_TBL VALUES (smallint('32767'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/int2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/int2_tbl], Append, `spark_catalog`.`default`.`int2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/int2_tbl), [f1]
-+- Project [cast(col1#x as smallint) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -41,7 +41,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO INT2_TBL VALUES (smallint('-32767'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/int2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/int2_tbl], Append, `spark_catalog`.`default`.`int2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/int2_tbl), [f1]
-+- Project [cast(col1#x as smallint) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/int4.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/int4.sql.out
index d261b59a4c5..e19adeb5f0b 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/int4.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/int4.sql.out
@@ -9,7 +9,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`INT4_TBL`, false
 INSERT INTO INT4_TBL VALUES (int(trim('   0  ')))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/int4_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/int4_tbl], Append, `spark_catalog`.`default`.`int4_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/int4_tbl), [f1]
-+- Project [cast(col1#x as int) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -17,7 +17,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO INT4_TBL VALUES (int(trim('123456     ')))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/int4_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/int4_tbl], Append, `spark_catalog`.`default`.`int4_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/int4_tbl), [f1]
-+- Project [cast(col1#x as int) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -25,7 +25,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO INT4_TBL VALUES (int(trim('    -123456')))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/int4_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/int4_tbl], Append, `spark_catalog`.`default`.`int4_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/int4_tbl), [f1]
-+- Project [cast(col1#x as int) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -33,7 +33,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO INT4_TBL VALUES (int('2147483647'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/int4_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/int4_tbl], Append, `spark_catalog`.`default`.`int4_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/int4_tbl), [f1]
-+- Project [cast(col1#x as int) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -41,7 +41,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO INT4_TBL VALUES (int('-2147483647'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/int4_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/int4_tbl], Append, `spark_catalog`.`default`.`int4_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/int4_tbl), [f1]
-+- Project [cast(col1#x as int) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/int8.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/int8.sql.out
index 72972469fa6..9546f1dae5e 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/int8.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/int8.sql.out
@@ -9,7 +9,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`INT8_TBL`, false
 INSERT INTO INT8_TBL VALUES(bigint(trim('  123   ')),bigint(trim('  456')))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/int8_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/int8_tbl], Append, `spark_catalog`.`default`.`int8_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/int8_tbl), [q1, q2]
-+- Project [cast(col1#xL as bigint) AS q1#xL, cast(col2#xL as bigint) AS q2#xL]
++- Project [col1#xL AS q1#xL, col2#xL AS q2#xL]
    +- LocalRelation [col1#xL, col2#xL]
 
 
@@ -17,7 +17,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO INT8_TBL VALUES(bigint(trim('123   ')),bigint('4567890123456789'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/int8_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/int8_tbl], Append, `spark_catalog`.`default`.`int8_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/int8_tbl), [q1, q2]
-+- Project [cast(col1#xL as bigint) AS q1#xL, cast(col2#xL as bigint) AS q2#xL]
++- Project [col1#xL AS q1#xL, col2#xL AS q2#xL]
    +- LocalRelation [col1#xL, col2#xL]
 
 
@@ -25,7 +25,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO INT8_TBL VALUES(bigint('4567890123456789'),bigint('123'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/int8_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/int8_tbl], Append, `spark_catalog`.`default`.`int8_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/int8_tbl), [q1, q2]
-+- Project [cast(col1#xL as bigint) AS q1#xL, cast(col2#xL as bigint) AS q2#xL]
++- Project [col1#xL AS q1#xL, col2#xL AS q2#xL]
    +- LocalRelation [col1#xL, col2#xL]
 
 
@@ -33,7 +33,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO INT8_TBL VALUES(+4567890123456789,bigint('4567890123456789'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/int8_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/int8_tbl], Append, `spark_catalog`.`default`.`int8_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/int8_tbl), [q1, q2]
-+- Project [cast(col1#xL as bigint) AS q1#xL, cast(col2#xL as bigint) AS q2#xL]
++- Project [col1#xL AS q1#xL, col2#xL AS q2#xL]
    +- LocalRelation [col1#xL, col2#xL]
 
 
@@ -41,7 +41,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO INT8_TBL VALUES(bigint('+4567890123456789'),bigint('-4567890123456789'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/int8_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/int8_tbl], Append, `spark_catalog`.`default`.`int8_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/int8_tbl), [q1, q2]
-+- Project [cast(col1#xL as bigint) AS q1#xL, cast(col2#xL as bigint) AS q2#xL]
++- Project [col1#xL AS q1#xL, col2#xL AS q2#xL]
    +- LocalRelation [col1#xL, col2#xL]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/join.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/join.sql.out
index de6b6879f7a..439094c1128 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/join.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/join.sql.out
@@ -116,7 +116,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`J2_TBL`, false
 INSERT INTO J1_TBL VALUES (1, 4, 'one')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j1_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j1_tbl], Append, `spark_catalog`.`default`.`j1_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j1_tbl), [i, j, t]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x, cast(col3#x as string) AS t#x]
++- Project [col1#x AS i#x, col2#x AS j#x, col3#x AS t#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -124,7 +124,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J1_TBL VALUES (2, 3, 'two')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j1_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j1_tbl], Append, `spark_catalog`.`default`.`j1_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j1_tbl), [i, j, t]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x, cast(col3#x as string) AS t#x]
++- Project [col1#x AS i#x, col2#x AS j#x, col3#x AS t#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -132,7 +132,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J1_TBL VALUES (3, 2, 'three')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j1_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j1_tbl], Append, `spark_catalog`.`default`.`j1_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j1_tbl), [i, j, t]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x, cast(col3#x as string) AS t#x]
++- Project [col1#x AS i#x, col2#x AS j#x, col3#x AS t#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -140,7 +140,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J1_TBL VALUES (4, 1, 'four')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j1_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j1_tbl], Append, `spark_catalog`.`default`.`j1_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j1_tbl), [i, j, t]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x, cast(col3#x as string) AS t#x]
++- Project [col1#x AS i#x, col2#x AS j#x, col3#x AS t#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -148,7 +148,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J1_TBL VALUES (5, 0, 'five')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j1_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j1_tbl], Append, `spark_catalog`.`default`.`j1_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j1_tbl), [i, j, t]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x, cast(col3#x as string) AS t#x]
++- Project [col1#x AS i#x, col2#x AS j#x, col3#x AS t#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -156,7 +156,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J1_TBL VALUES (6, 6, 'six')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j1_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j1_tbl], Append, `spark_catalog`.`default`.`j1_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j1_tbl), [i, j, t]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x, cast(col3#x as string) AS t#x]
++- Project [col1#x AS i#x, col2#x AS j#x, col3#x AS t#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -164,7 +164,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J1_TBL VALUES (7, 7, 'seven')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j1_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j1_tbl], Append, `spark_catalog`.`default`.`j1_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j1_tbl), [i, j, t]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x, cast(col3#x as string) AS t#x]
++- Project [col1#x AS i#x, col2#x AS j#x, col3#x AS t#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -172,7 +172,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J1_TBL VALUES (8, 8, 'eight')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j1_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j1_tbl], Append, `spark_catalog`.`default`.`j1_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j1_tbl), [i, j, t]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x, cast(col3#x as string) AS t#x]
++- Project [col1#x AS i#x, col2#x AS j#x, col3#x AS t#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -180,7 +180,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J1_TBL VALUES (0, NULL, 'zero')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j1_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j1_tbl], Append, `spark_catalog`.`default`.`j1_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j1_tbl), [i, j, t]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x, cast(col3#x as string) AS t#x]
++- Project [col1#x AS i#x, cast(col2#x as int) AS j#x, col3#x AS t#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -188,7 +188,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J1_TBL VALUES (NULL, NULL, 'null')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j1_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j1_tbl], Append, `spark_catalog`.`default`.`j1_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j1_tbl), [i, j, t]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x, cast(col3#x as string) AS t#x]
++- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x, col3#x AS t#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -196,7 +196,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J1_TBL VALUES (NULL, 0, 'zero')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j1_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j1_tbl], Append, `spark_catalog`.`default`.`j1_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j1_tbl), [i, j, t]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x, cast(col3#x as string) AS t#x]
++- Project [cast(col1#x as int) AS i#x, col2#x AS j#x, col3#x AS t#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -204,7 +204,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J2_TBL VALUES (1, -1)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j2_tbl], Append, `spark_catalog`.`default`.`j2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j2_tbl), [i, k]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS k#x]
++- Project [col1#x AS i#x, col2#x AS k#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -212,7 +212,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J2_TBL VALUES (2, 2)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j2_tbl], Append, `spark_catalog`.`default`.`j2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j2_tbl), [i, k]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS k#x]
++- Project [col1#x AS i#x, col2#x AS k#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -220,7 +220,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J2_TBL VALUES (3, -3)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j2_tbl], Append, `spark_catalog`.`default`.`j2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j2_tbl), [i, k]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS k#x]
++- Project [col1#x AS i#x, col2#x AS k#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -228,7 +228,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J2_TBL VALUES (2, 4)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j2_tbl], Append, `spark_catalog`.`default`.`j2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j2_tbl), [i, k]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS k#x]
++- Project [col1#x AS i#x, col2#x AS k#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -236,7 +236,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J2_TBL VALUES (5, -5)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j2_tbl], Append, `spark_catalog`.`default`.`j2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j2_tbl), [i, k]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS k#x]
++- Project [col1#x AS i#x, col2#x AS k#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -244,7 +244,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J2_TBL VALUES (5, -5)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j2_tbl], Append, `spark_catalog`.`default`.`j2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j2_tbl), [i, k]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS k#x]
++- Project [col1#x AS i#x, col2#x AS k#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -252,7 +252,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J2_TBL VALUES (0, NULL)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j2_tbl], Append, `spark_catalog`.`default`.`j2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j2_tbl), [i, k]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS k#x]
++- Project [col1#x AS i#x, cast(col2#x as int) AS k#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -268,7 +268,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J2_TBL VALUES (NULL, 0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j2_tbl], Append, `spark_catalog`.`default`.`j2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j2_tbl), [i, k]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS k#x]
++- Project [cast(col1#x as int) AS i#x, col2#x AS k#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -674,7 +674,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t3`, false
 INSERT INTO t1 VALUES ( 'bb', 11 )
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [name, n]
-+- Project [cast(col1#x as string) AS name#x, cast(col2#x as int) AS n#x]
++- Project [col1#x AS name#x, col2#x AS n#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -682,7 +682,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO t2 VALUES ( 'bb', 12 )
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t2, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t2], Append, `spark_catalog`.`default`.`t2`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t2), [name, n]
-+- Project [cast(col1#x as string) AS name#x, cast(col2#x as int) AS n#x]
++- Project [col1#x AS name#x, col2#x AS n#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -690,7 +690,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO t2 VALUES ( 'cc', 22 )
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t2, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t2], Append, `spark_catalog`.`default`.`t2`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t2), [name, n]
-+- Project [cast(col1#x as string) AS name#x, cast(col2#x as int) AS n#x]
++- Project [col1#x AS name#x, col2#x AS n#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -698,7 +698,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO t2 VALUES ( 'ee', 42 )
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t2, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t2], Append, `spark_catalog`.`default`.`t2`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t2), [name, n]
-+- Project [cast(col1#x as string) AS name#x, cast(col2#x as int) AS n#x]
++- Project [col1#x AS name#x, col2#x AS n#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -706,7 +706,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO t3 VALUES ( 'bb', 13 )
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t3, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t3], Append, `spark_catalog`.`default`.`t3`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t3), [name, n]
-+- Project [cast(col1#x as string) AS name#x, cast(col2#x as int) AS n#x]
++- Project [col1#x AS name#x, col2#x AS n#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -714,7 +714,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO t3 VALUES ( 'cc', 23 )
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t3, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t3], Append, `spark_catalog`.`default`.`t3`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t3), [name, n]
-+- Project [cast(col1#x as string) AS name#x, cast(col2#x as int) AS n#x]
++- Project [col1#x AS name#x, col2#x AS n#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -722,7 +722,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO t3 VALUES ( 'dd', 33 )
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t3, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t3], Append, `spark_catalog`.`default`.`t3`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t3), [name, n]
-+- Project [cast(col1#x as string) AS name#x, cast(col2#x as int) AS n#x]
++- Project [col1#x AS name#x, col2#x AS n#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3844,7 +3844,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`j2`, false
 INSERT INTO j1 values(1,1),(1,2)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j1], Append, `spark_catalog`.`default`.`j1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j1), [id1, id2]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3852,7 +3852,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO j2 values(1,1)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j2, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j2], Append, `spark_catalog`.`default`.`j2`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j2), [id1, id2]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3860,7 +3860,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO j2 values(1,2)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j2, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j2], Append, `spark_catalog`.`default`.`j2`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j2), [id1, id2]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x]
    +- LocalRelation [col1#x, col2#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/numeric.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/numeric.sql.out
index 6c2ae232917..d5e8edbf020 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/numeric.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/numeric.sql.out
@@ -63,7 +63,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`num_result`, false
 INSERT INTO num_exp_add VALUES (0,0,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -71,7 +71,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (0,0,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -79,7 +79,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (0,0,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -87,7 +87,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (0,0,double('NaN'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -95,7 +95,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (0,1,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -103,7 +103,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (0,1,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -111,7 +111,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (0,1,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -119,7 +119,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (0,1,double('NaN'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -127,7 +127,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (0,2,-34338492.215397047)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -135,7 +135,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (0,2,34338492.215397047)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -143,7 +143,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (0,2,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -151,7 +151,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (0,2,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -159,7 +159,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (0,3,4.31)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -167,7 +167,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (0,3,-4.31)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -175,7 +175,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (0,3,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -183,7 +183,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (0,3,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -191,7 +191,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (0,4,7799461.4119)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -199,7 +199,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (0,4,-7799461.4119)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -207,7 +207,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (0,4,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -215,7 +215,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (0,4,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -223,7 +223,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (0,5,16397.038491)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -231,7 +231,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (0,5,-16397.038491)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -239,7 +239,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (0,5,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -247,7 +247,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (0,5,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -255,7 +255,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (0,6,93901.57763026)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -263,7 +263,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (0,6,-93901.57763026)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -271,7 +271,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (0,6,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -279,7 +279,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (0,6,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -287,7 +287,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (0,7,-83028485)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -295,7 +295,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (0,7,83028485)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -303,7 +303,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (0,7,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -311,7 +311,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (0,7,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -319,7 +319,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (0,8,74881)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -327,7 +327,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (0,8,-74881)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -335,7 +335,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (0,8,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -343,7 +343,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (0,8,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -351,7 +351,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (0,9,-24926804.045047420)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -359,7 +359,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (0,9,24926804.045047420)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -367,7 +367,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (0,9,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -375,7 +375,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (0,9,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -383,7 +383,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (1,0,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -391,7 +391,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (1,0,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -399,7 +399,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (1,0,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -407,7 +407,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (1,0,double('NaN'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -415,7 +415,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (1,1,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -423,7 +423,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (1,1,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -431,7 +431,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (1,1,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -439,7 +439,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (1,1,double('NaN'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -447,7 +447,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (1,2,-34338492.215397047)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -455,7 +455,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (1,2,34338492.215397047)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -463,7 +463,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (1,2,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -471,7 +471,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (1,2,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -479,7 +479,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (1,3,4.31)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -487,7 +487,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (1,3,-4.31)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -495,7 +495,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (1,3,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -503,7 +503,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (1,3,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -511,7 +511,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (1,4,7799461.4119)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -519,7 +519,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (1,4,-7799461.4119)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -527,7 +527,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (1,4,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -535,7 +535,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (1,4,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -543,7 +543,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (1,5,16397.038491)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -551,7 +551,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (1,5,-16397.038491)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -559,7 +559,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (1,5,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -567,7 +567,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (1,5,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -575,7 +575,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (1,6,93901.57763026)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -583,7 +583,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (1,6,-93901.57763026)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -591,7 +591,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (1,6,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -599,7 +599,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (1,6,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -607,7 +607,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (1,7,-83028485)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -615,7 +615,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (1,7,83028485)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -623,7 +623,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (1,7,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -631,7 +631,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (1,7,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -639,7 +639,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (1,8,74881)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -647,7 +647,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (1,8,-74881)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -655,7 +655,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (1,8,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -663,7 +663,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (1,8,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -671,7 +671,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (1,9,-24926804.045047420)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -679,7 +679,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (1,9,24926804.045047420)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -687,7 +687,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (1,9,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -695,7 +695,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (1,9,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -703,7 +703,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (2,0,-34338492.215397047)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -711,7 +711,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (2,0,-34338492.215397047)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -719,7 +719,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (2,0,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -727,7 +727,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (2,0,double('NaN'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -735,7 +735,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (2,1,-34338492.215397047)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -743,7 +743,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (2,1,-34338492.215397047)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -751,7 +751,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (2,1,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -759,7 +759,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (2,1,double('NaN'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -767,7 +767,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (2,2,-68676984.430794094)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -775,7 +775,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (2,2,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -783,7 +783,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (2,2,1179132047626883.596862135856320209)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -791,7 +791,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (2,2,1.00000000000000000000)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -799,7 +799,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (2,3,-34338487.905397047)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -807,7 +807,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (2,3,-34338496.525397047)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -815,7 +815,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (2,3,-147998901.44836127257)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -823,7 +823,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (2,3,-7967167.56737750510440835266)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -831,7 +831,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (2,4,-26539030.803497047)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -839,7 +839,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (2,4,-42137953.627297047)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -847,7 +847,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (2,4,-267821744976817.8111137106593)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -855,7 +855,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (2,4,-4.40267480046830116685)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -863,7 +863,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (2,5,-34322095.176906047)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -871,7 +871,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (2,5,-34354889.253888047)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -879,7 +879,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (2,5,-563049578578.769242506736077)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -887,7 +887,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (2,5,-2094.18866914563535496429)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -895,7 +895,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (2,6,-34244590.637766787)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -903,7 +903,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (2,6,-34432393.793027307)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -911,7 +911,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (2,6,-3224438592470.18449811926184222)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -919,7 +919,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (2,6,-365.68599891479766440940)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -927,7 +927,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (2,7,-117366977.215397047)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -935,7 +935,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (2,7,48689992.784602953)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -943,7 +943,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (2,7,2851072985828710.485883795)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -951,7 +951,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (2,7,.41357483778485235518)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -959,7 +959,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (2,8,-34263611.215397047)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -967,7 +967,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (2,8,-34413373.215397047)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -975,7 +975,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (2,8,-2571300635581.146276407)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -983,7 +983,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (2,8,-458.57416721727870888476)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -991,7 +991,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (2,9,-59265296.260444467)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -999,7 +999,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (2,9,-9411688.170349627)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1007,7 +1007,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (2,9,855948866655588.453741509242968740)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1015,7 +1015,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (2,9,1.37757299946438931811)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1023,7 +1023,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (3,0,4.31)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1031,7 +1031,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (3,0,4.31)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1039,7 +1039,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (3,0,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1047,7 +1047,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (3,0,double('NaN'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1055,7 +1055,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (3,1,4.31)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1063,7 +1063,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (3,1,4.31)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1071,7 +1071,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (3,1,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1079,7 +1079,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (3,1,double('NaN'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1087,7 +1087,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (3,2,-34338487.905397047)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1095,7 +1095,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (3,2,34338496.525397047)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1103,7 +1103,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (3,2,-147998901.44836127257)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1111,7 +1111,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (3,2,-.00000012551512084352)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1119,7 +1119,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (3,3,8.62)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1127,7 +1127,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (3,3,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1135,7 +1135,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (3,3,18.5761)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1143,7 +1143,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (3,3,1.00000000000000000000)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1151,7 +1151,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (3,4,7799465.7219)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1159,7 +1159,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (3,4,-7799457.1019)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1167,7 +1167,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (3,4,33615678.685289)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1175,7 +1175,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (3,4,.00000055260225961552)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1183,7 +1183,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (3,5,16401.348491)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1191,7 +1191,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (3,5,-16392.728491)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1199,7 +1199,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (3,5,70671.23589621)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1207,7 +1207,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (3,5,.00026285234387695504)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1215,7 +1215,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (3,6,93905.88763026)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1223,7 +1223,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (3,6,-93897.26763026)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1231,7 +1231,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (3,6,404715.7995864206)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1239,7 +1239,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (3,6,.00004589912234457595)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1247,7 +1247,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (3,7,-83028480.69)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1255,7 +1255,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (3,7,83028489.31)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1263,7 +1263,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (3,7,-357852770.35)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1271,7 +1271,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (3,7,-.00000005190989574240)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1279,7 +1279,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (3,8,74885.31)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1287,7 +1287,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (3,8,-74876.69)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1295,7 +1295,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (3,8,322737.11)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1303,7 +1303,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (3,8,.00005755799201399553)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1311,7 +1311,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (3,9,-24926799.735047420)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1319,7 +1319,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (3,9,24926808.355047420)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1327,7 +1327,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (3,9,-107434525.43415438020)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1335,7 +1335,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (3,9,-.00000017290624149854)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1343,7 +1343,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (4,0,7799461.4119)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1351,7 +1351,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (4,0,7799461.4119)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1359,7 +1359,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (4,0,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1367,7 +1367,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (4,0,double('NaN'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1375,7 +1375,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (4,1,7799461.4119)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1383,7 +1383,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (4,1,7799461.4119)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1391,7 +1391,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (4,1,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1399,7 +1399,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (4,1,double('NaN'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1407,7 +1407,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (4,2,-26539030.803497047)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1415,7 +1415,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (4,2,42137953.627297047)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1423,7 +1423,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (4,2,-267821744976817.8111137106593)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1431,7 +1431,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (4,2,-.22713465002993920385)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1439,7 +1439,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (4,3,7799465.7219)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1447,7 +1447,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (4,3,7799457.1019)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1455,7 +1455,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (4,3,33615678.685289)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1463,7 +1463,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (4,3,1809619.81714617169373549883)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1471,7 +1471,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (4,4,15598922.8238)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1479,7 +1479,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (4,4,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1487,7 +1487,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (4,4,60831598315717.14146161)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1495,7 +1495,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (4,4,1.00000000000000000000)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1503,7 +1503,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (4,5,7815858.450391)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1511,7 +1511,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (4,5,7783064.373409)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1519,7 +1519,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (4,5,127888068979.9935054429)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1527,7 +1527,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (4,5,475.66281046305802686061)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1535,7 +1535,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (4,6,7893362.98953026)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1543,7 +1543,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (4,6,7705559.83426974)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1551,7 +1551,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (4,6,732381731243.745115764094)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1559,7 +1559,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (4,6,83.05996138436129499606)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1567,7 +1567,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (4,7,-75229023.5881)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1575,7 +1575,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (4,7,90827946.4119)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1583,7 +1583,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (4,7,-647577464846017.9715)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1591,7 +1591,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (4,7,-.09393717604145131637)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1599,7 +1599,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (4,8,7874342.4119)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1607,7 +1607,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (4,8,7724580.4119)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1615,7 +1615,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (4,8,584031469984.4839)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1623,7 +1623,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (4,8,104.15808298366741897143)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1631,7 +1631,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (4,9,-17127342.633147420)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1639,7 +1639,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (4,9,32726265.456947420)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1647,7 +1647,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (4,9,-194415646271340.1815956522980)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1655,7 +1655,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (4,9,-.31289456112403769409)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1663,7 +1663,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (5,0,16397.038491)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1671,7 +1671,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (5,0,16397.038491)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1679,7 +1679,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (5,0,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1687,7 +1687,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (5,0,double('NaN'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1695,7 +1695,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (5,1,16397.038491)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1703,7 +1703,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (5,1,16397.038491)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1711,7 +1711,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (5,1,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1719,7 +1719,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (5,1,double('NaN'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1727,7 +1727,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (5,2,-34322095.176906047)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1735,7 +1735,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (5,2,34354889.253888047)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1743,7 +1743,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (5,2,-563049578578.769242506736077)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1751,7 +1751,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (5,2,-.00047751189505192446)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1759,7 +1759,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (5,3,16401.348491)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1767,7 +1767,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (5,3,16392.728491)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1775,7 +1775,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (5,3,70671.23589621)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1783,7 +1783,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (5,3,3804.41728329466357308584)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1791,7 +1791,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (5,4,7815858.450391)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1799,7 +1799,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (5,4,-7783064.373409)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1807,7 +1807,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (5,4,127888068979.9935054429)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1815,7 +1815,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (5,4,.00210232958726897192)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1823,7 +1823,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (5,5,32794.076982)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1831,7 +1831,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (5,5,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1839,7 +1839,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (5,5,268862871.275335557081)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1847,7 +1847,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (5,5,1.00000000000000000000)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1855,7 +1855,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (5,6,110298.61612126)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1863,7 +1863,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (5,6,-77504.53913926)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1871,7 +1871,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (5,6,1539707782.76899778633766)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1879,7 +1879,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (5,6,.17461941433576102689)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1887,7 +1887,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (5,7,-83012087.961509)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1895,7 +1895,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (5,7,83044882.038491)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1903,7 +1903,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (5,7,-1361421264394.416135)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1911,7 +1911,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (5,7,-.00019748690453643710)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1919,7 +1919,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (5,8,91278.038491)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1927,7 +1927,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (5,8,-58483.961509)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1935,7 +1935,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (5,8,1227826639.244571)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1943,7 +1943,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (5,8,.21897461960978085228)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1951,7 +1951,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (5,9,-24910407.006556420)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1959,7 +1959,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (5,9,24943201.083538420)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1967,7 +1967,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (5,9,-408725765384.257043660243220)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1975,7 +1975,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (5,9,-.00065780749354660427)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1983,7 +1983,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (6,0,93901.57763026)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1991,7 +1991,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (6,0,93901.57763026)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1999,7 +1999,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (6,0,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2007,7 +2007,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (6,0,double('NaN'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2015,7 +2015,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (6,1,93901.57763026)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2023,7 +2023,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (6,1,93901.57763026)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2031,7 +2031,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (6,1,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2039,7 +2039,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (6,1,double('NaN'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2047,7 +2047,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (6,2,-34244590.637766787)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2055,7 +2055,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (6,2,34432393.793027307)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2063,7 +2063,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (6,2,-3224438592470.18449811926184222)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2071,7 +2071,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (6,2,-.00273458651128995823)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2079,7 +2079,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (6,3,93905.88763026)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2087,7 +2087,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (6,3,93897.26763026)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2095,7 +2095,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (6,3,404715.7995864206)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2103,7 +2103,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (6,3,21786.90896293735498839907)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2111,7 +2111,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (6,4,7893362.98953026)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2119,7 +2119,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (6,4,-7705559.83426974)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2127,7 +2127,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (6,4,732381731243.745115764094)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2135,7 +2135,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (6,4,.01203949512295682469)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2143,7 +2143,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (6,5,110298.61612126)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2151,7 +2151,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (6,5,77504.53913926)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2159,7 +2159,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (6,5,1539707782.76899778633766)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2167,7 +2167,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (6,5,5.72674008674192359679)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2175,7 +2175,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (6,6,187803.15526052)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2183,7 +2183,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (6,6,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2191,7 +2191,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (6,6,8817506281.4517452372676676)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2199,7 +2199,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (6,6,1.00000000000000000000)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2207,7 +2207,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (6,7,-82934583.42236974)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2215,7 +2215,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (6,7,83122386.57763026)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2223,7 +2223,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (6,7,-7796505729750.37795610)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2231,7 +2231,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (6,7,-.00113095617281538980)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2239,7 +2239,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (6,8,168782.57763026)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2247,7 +2247,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (6,8,19020.57763026)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2255,7 +2255,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (6,8,7031444034.53149906)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2263,7 +2263,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (6,8,1.25401073209839612184)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2271,7 +2271,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (6,9,-24832902.467417160)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2279,7 +2279,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (6,9,25020705.622677680)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2287,7 +2287,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (6,9,-2340666225110.29929521292692920)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2295,7 +2295,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (6,9,-.00376709254265256789)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2303,7 +2303,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (7,0,-83028485)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2311,7 +2311,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (7,0,-83028485)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2319,7 +2319,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (7,0,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2327,7 +2327,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (7,0,double('NaN'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2335,7 +2335,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (7,1,-83028485)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2343,7 +2343,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (7,1,-83028485)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2351,7 +2351,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (7,1,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2359,7 +2359,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (7,1,double('NaN'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2367,7 +2367,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (7,2,-117366977.215397047)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2375,7 +2375,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (7,2,-48689992.784602953)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2383,7 +2383,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (7,2,2851072985828710.485883795)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2391,7 +2391,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (7,2,2.41794207151503385700)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2399,7 +2399,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (7,3,-83028480.69)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2407,7 +2407,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (7,3,-83028489.31)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2415,7 +2415,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (7,3,-357852770.35)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2423,7 +2423,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (7,3,-19264149.65197215777262180974)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2431,7 +2431,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (7,4,-75229023.5881)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2439,7 +2439,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (7,4,-90827946.4119)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2447,7 +2447,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (7,4,-647577464846017.9715)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2455,7 +2455,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (7,4,-10.64541262725136247686)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2463,7 +2463,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (7,5,-83012087.961509)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2471,7 +2471,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (7,5,-83044882.038491)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2479,7 +2479,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (7,5,-1361421264394.416135)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2487,7 +2487,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (7,5,-5063.62688881730941836574)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2495,7 +2495,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (7,6,-82934583.42236974)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2503,7 +2503,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (7,6,-83122386.57763026)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2511,7 +2511,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (7,6,-7796505729750.37795610)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2519,7 +2519,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (7,6,-884.20756174009028770294)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2527,7 +2527,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (7,7,-166056970)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2535,7 +2535,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (7,7,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2543,7 +2543,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (7,7,6893729321395225)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#xL as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#xL as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#xL]
 
 
@@ -2551,7 +2551,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (7,7,1.00000000000000000000)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2559,7 +2559,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (7,8,-82953604)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2567,7 +2567,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (7,8,-83103366)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2575,7 +2575,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (7,8,-6217255985285)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#xL as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#xL as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#xL]
 
 
@@ -2583,7 +2583,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (7,8,-1108.80577182462841041118)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2591,7 +2591,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (7,9,-107955289.045047420)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2599,7 +2599,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (7,9,-58101680.954952580)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2607,7 +2607,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (7,9,2069634775752159.035758700)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2615,7 +2615,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (7,9,3.33089171198810413382)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2623,7 +2623,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (8,0,74881)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2631,7 +2631,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (8,0,74881)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2639,7 +2639,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (8,0,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2647,7 +2647,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (8,0,double('NaN'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2655,7 +2655,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (8,1,74881)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2663,7 +2663,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (8,1,74881)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2671,7 +2671,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (8,1,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2679,7 +2679,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (8,1,double('NaN'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2687,7 +2687,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (8,2,-34263611.215397047)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2695,7 +2695,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (8,2,34413373.215397047)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2703,7 +2703,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (8,2,-2571300635581.146276407)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2711,7 +2711,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (8,2,-.00218067233500788615)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2719,7 +2719,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (8,3,74885.31)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2727,7 +2727,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (8,3,74876.69)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2735,7 +2735,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (8,3,322737.11)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2743,7 +2743,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (8,3,17373.78190255220417633410)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2751,7 +2751,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (8,4,7874342.4119)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2759,7 +2759,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (8,4,-7724580.4119)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2767,7 +2767,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (8,4,584031469984.4839)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2775,7 +2775,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (8,4,.00960079113741758956)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2783,7 +2783,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (8,5,91278.038491)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2791,7 +2791,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (8,5,58483.961509)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2799,7 +2799,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (8,5,1227826639.244571)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2807,7 +2807,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (8,5,4.56673929509287019456)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2815,7 +2815,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (8,6,168782.57763026)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2823,7 +2823,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (8,6,-19020.57763026)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2831,7 +2831,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (8,6,7031444034.53149906)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2839,7 +2839,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (8,6,.79744134113322314424)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2847,7 +2847,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (8,7,-82953604)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2855,7 +2855,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (8,7,83103366)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2863,7 +2863,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (8,7,-6217255985285)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#xL as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#xL as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#xL]
 
 
@@ -2871,7 +2871,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (8,7,-.00090187120721280172)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2879,7 +2879,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (8,8,149762)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2887,7 +2887,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (8,8,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2895,7 +2895,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (8,8,5607164161)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#xL as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#xL as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#xL]
 
 
@@ -2903,7 +2903,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (8,8,1.00000000000000000000)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2911,7 +2911,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (8,9,-24851923.045047420)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2919,7 +2919,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (8,9,25001685.045047420)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2927,7 +2927,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (8,9,-1866544013697.195857020)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2935,7 +2935,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (8,9,-.00300403532938582735)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2943,7 +2943,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (9,0,-24926804.045047420)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2951,7 +2951,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (9,0,-24926804.045047420)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2959,7 +2959,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (9,0,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2967,7 +2967,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (9,0,double('NaN'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2975,7 +2975,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (9,1,-24926804.045047420)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2983,7 +2983,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (9,1,-24926804.045047420)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2991,7 +2991,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (9,1,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -2999,7 +2999,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (9,1,double('NaN'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -3007,7 +3007,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (9,2,-59265296.260444467)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -3015,7 +3015,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (9,2,9411688.170349627)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -3023,7 +3023,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (9,2,855948866655588.453741509242968740)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -3031,7 +3031,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (9,2,.72591434384152961526)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -3039,7 +3039,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (9,3,-24926799.735047420)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -3047,7 +3047,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (9,3,-24926808.355047420)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -3055,7 +3055,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (9,3,-107434525.43415438020)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -3063,7 +3063,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (9,3,-5783481.21694835730858468677)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -3071,7 +3071,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (9,4,-17127342.633147420)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -3079,7 +3079,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (9,4,-32726265.456947420)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -3087,7 +3087,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (9,4,-194415646271340.1815956522980)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -3095,7 +3095,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (9,4,-3.19596478892958416484)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -3103,7 +3103,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (9,5,-24910407.006556420)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -3111,7 +3111,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (9,5,-24943201.083538420)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -3119,7 +3119,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (9,5,-408725765384.257043660243220)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -3127,7 +3127,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (9,5,-1520.20159364322004505807)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -3135,7 +3135,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (9,6,-24832902.467417160)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -3143,7 +3143,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (9,6,-25020705.622677680)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -3151,7 +3151,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (9,6,-2340666225110.29929521292692920)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -3159,7 +3159,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (9,6,-265.45671195426965751280)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -3167,7 +3167,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (9,7,-107955289.045047420)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -3175,7 +3175,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (9,7,58101680.954952580)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -3183,7 +3183,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (9,7,2069634775752159.035758700)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -3191,7 +3191,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (9,7,.30021990699995814689)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -3199,7 +3199,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (9,8,-24851923.045047420)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -3207,7 +3207,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (9,8,-25001685.045047420)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -3215,7 +3215,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (9,8,-1866544013697.195857020)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -3223,7 +3223,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (9,8,-332.88556569820675471748)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -3231,7 +3231,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_add VALUES (9,9,-49853608.090094840)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_add, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_add], Append, `spark_catalog`.`default`.`num_exp_add`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_add), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -3239,7 +3239,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sub VALUES (9,9,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sub, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sub], Append, `spark_catalog`.`default`.`num_exp_sub`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sub), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -3247,7 +3247,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_mul VALUES (9,9,621345559900192.420120630048656400)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_mul, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_mul], Append, `spark_catalog`.`default`.`num_exp_mul`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_mul), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -3255,7 +3255,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_div VALUES (9,9,1.00000000000000000000)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_div, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_div], Append, `spark_catalog`.`default`.`num_exp_div`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_div), [id1, id2, expected]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x, cast(col3#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -3263,7 +3263,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sqrt VALUES (0,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sqrt, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sqrt], Append, `spark_catalog`.`default`.`num_exp_sqrt`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sqrt), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3271,7 +3271,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sqrt VALUES (1,0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sqrt, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sqrt], Append, `spark_catalog`.`default`.`num_exp_sqrt`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sqrt), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3279,7 +3279,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sqrt VALUES (2,5859.90547836712524903505)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sqrt, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sqrt], Append, `spark_catalog`.`default`.`num_exp_sqrt`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sqrt), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3287,7 +3287,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sqrt VALUES (3,2.07605394920266944396)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sqrt, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sqrt], Append, `spark_catalog`.`default`.`num_exp_sqrt`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sqrt), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3295,7 +3295,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sqrt VALUES (4,2792.75158435189147418923)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sqrt, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sqrt], Append, `spark_catalog`.`default`.`num_exp_sqrt`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sqrt), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3303,7 +3303,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sqrt VALUES (5,128.05092147657509145473)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sqrt, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sqrt], Append, `spark_catalog`.`default`.`num_exp_sqrt`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sqrt), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3311,7 +3311,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sqrt VALUES (6,306.43364311096782703406)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sqrt, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sqrt], Append, `spark_catalog`.`default`.`num_exp_sqrt`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sqrt), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3319,7 +3319,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sqrt VALUES (7,9111.99676251039939975230)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sqrt, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sqrt], Append, `spark_catalog`.`default`.`num_exp_sqrt`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sqrt), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3327,7 +3327,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sqrt VALUES (8,273.64392922189960397542)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sqrt, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sqrt], Append, `spark_catalog`.`default`.`num_exp_sqrt`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sqrt), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3335,7 +3335,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_sqrt VALUES (9,4992.67503899937593364766)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_sqrt, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_sqrt], Append, `spark_catalog`.`default`.`num_exp_sqrt`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_sqrt), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3343,7 +3343,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_ln VALUES (0,double('NaN'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_ln, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_ln], Append, `spark_catalog`.`default`.`num_exp_ln`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_ln), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3351,7 +3351,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_ln VALUES (1,double('NaN'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_ln, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_ln], Append, `spark_catalog`.`default`.`num_exp_ln`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_ln), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3359,7 +3359,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_ln VALUES (2,17.35177750493897715514)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_ln, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_ln], Append, `spark_catalog`.`default`.`num_exp_ln`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_ln), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3367,7 +3367,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_ln VALUES (3,1.46093790411565641971)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_ln, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_ln], Append, `spark_catalog`.`default`.`num_exp_ln`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_ln), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3375,7 +3375,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_ln VALUES (4,15.86956523951936572464)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_ln, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_ln], Append, `spark_catalog`.`default`.`num_exp_ln`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_ln), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3383,7 +3383,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_ln VALUES (5,9.70485601768871834038)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_ln, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_ln], Append, `spark_catalog`.`default`.`num_exp_ln`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_ln), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3391,7 +3391,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_ln VALUES (6,11.45000246622944403127)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_ln, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_ln], Append, `spark_catalog`.`default`.`num_exp_ln`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_ln), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3399,7 +3399,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_ln VALUES (7,18.23469429965478772991)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_ln, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_ln], Append, `spark_catalog`.`default`.`num_exp_ln`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_ln), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3407,7 +3407,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_ln VALUES (8,11.22365546576315513668)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_ln, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_ln], Append, `spark_catalog`.`default`.`num_exp_ln`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_ln), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3415,7 +3415,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_ln VALUES (9,17.03145425013166006962)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_ln, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_ln], Append, `spark_catalog`.`default`.`num_exp_ln`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_ln), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3423,7 +3423,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_log10 VALUES (0,double('NaN'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_log10, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_log10], Append, `spark_catalog`.`default`.`num_exp_log10`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_log10), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3431,7 +3431,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_log10 VALUES (1,double('NaN'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_log10, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_log10], Append, `spark_catalog`.`default`.`num_exp_log10`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_log10), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3439,7 +3439,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_log10 VALUES (2,7.53578122160797276459)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_log10, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_log10], Append, `spark_catalog`.`default`.`num_exp_log10`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_log10), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3447,7 +3447,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_log10 VALUES (3,.63447727016073160075)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_log10, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_log10], Append, `spark_catalog`.`default`.`num_exp_log10`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_log10), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3455,7 +3455,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_log10 VALUES (4,6.89206461372691743345)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_log10, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_log10], Append, `spark_catalog`.`default`.`num_exp_log10`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_log10), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3463,7 +3463,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_log10 VALUES (5,4.21476541614777768626)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_log10, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_log10], Append, `spark_catalog`.`default`.`num_exp_log10`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_log10), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3471,7 +3471,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_log10 VALUES (6,4.97267288886207207671)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_log10, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_log10], Append, `spark_catalog`.`default`.`num_exp_log10`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_log10), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3479,7 +3479,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_log10 VALUES (7,7.91922711353275546914)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_log10, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_log10], Append, `spark_catalog`.`default`.`num_exp_log10`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_log10), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3487,7 +3487,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_log10 VALUES (8,4.87437163556421004138)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_log10, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_log10], Append, `spark_catalog`.`default`.`num_exp_log10`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_log10), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3495,7 +3495,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_log10 VALUES (9,7.39666659961986567059)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_log10, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_log10], Append, `spark_catalog`.`default`.`num_exp_log10`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_log10), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3503,7 +3503,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_power_10_ln VALUES (0,double('NaN'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_power_10_ln, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_power_10_ln], Append, `spark_catalog`.`default`.`num_exp_power_10_ln`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_power_10_ln), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3511,7 +3511,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_power_10_ln VALUES (1,double('NaN'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_power_10_ln, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_power_10_ln], Append, `spark_catalog`.`default`.`num_exp_power_10_ln`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_power_10_ln), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3519,7 +3519,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_power_10_ln VALUES (2,224790267919917955.13261618583642653184)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_power_10_ln, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_power_10_ln], Append, `spark_catalog`.`default`.`num_exp_power_10_ln`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_power_10_ln), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3527,7 +3527,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_power_10_ln VALUES (3,28.90266599445155957393)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_power_10_ln, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_power_10_ln], Append, `spark_catalog`.`default`.`num_exp_power_10_ln`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_power_10_ln), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3535,7 +3535,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_power_10_ln VALUES (4,7405685069594999.07733999469386277636)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_power_10_ln, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_power_10_ln], Append, `spark_catalog`.`default`.`num_exp_power_10_ln`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_power_10_ln), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3543,7 +3543,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_power_10_ln VALUES (5,5068226527.32127265408584640098)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_power_10_ln, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_power_10_ln], Append, `spark_catalog`.`default`.`num_exp_power_10_ln`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_power_10_ln), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3551,7 +3551,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_power_10_ln VALUES (6,281839893606.99372343357047819067)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_power_10_ln, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_power_10_ln], Append, `spark_catalog`.`default`.`num_exp_power_10_ln`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_power_10_ln), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3573,7 +3573,7 @@ org.apache.spark.SparkArithmeticException
 INSERT INTO num_exp_power_10_ln VALUES (8,167361463828.07491320069016125952)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_power_10_ln, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_power_10_ln], Append, `spark_catalog`.`default`.`num_exp_power_10_ln`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_power_10_ln), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3581,7 +3581,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_exp_power_10_ln VALUES (9,107511333880052007.04141124673540337457)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_exp_power_10_ln, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_exp_power_10_ln], Append, `spark_catalog`.`default`.`num_exp_power_10_ln`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_exp_power_10_ln), [id, expected]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS expected#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3589,7 +3589,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_data VALUES (0, 0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_data, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_data], Append, `spark_catalog`.`default`.`num_data`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_data), [id, val]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS val#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS val#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3597,7 +3597,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_data VALUES (1, 0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_data, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_data], Append, `spark_catalog`.`default`.`num_data`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_data), [id, val]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS val#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS val#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3605,7 +3605,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_data VALUES (2, -34338492.215397047)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_data, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_data], Append, `spark_catalog`.`default`.`num_data`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_data), [id, val]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS val#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS val#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3613,7 +3613,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_data VALUES (3, 4.31)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_data, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_data], Append, `spark_catalog`.`default`.`num_data`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_data), [id, val]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS val#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS val#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3621,7 +3621,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_data VALUES (4, 7799461.4119)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_data, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_data], Append, `spark_catalog`.`default`.`num_data`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_data), [id, val]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS val#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS val#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3629,7 +3629,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_data VALUES (5, 16397.038491)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_data, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_data], Append, `spark_catalog`.`default`.`num_data`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_data), [id, val]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS val#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS val#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3637,7 +3637,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_data VALUES (6, 93901.57763026)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_data, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_data], Append, `spark_catalog`.`default`.`num_data`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_data), [id, val]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS val#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS val#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3645,7 +3645,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_data VALUES (7, -83028485)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_data, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_data], Append, `spark_catalog`.`default`.`num_data`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_data), [id, val]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS val#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS val#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3653,7 +3653,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_data VALUES (8, 74881)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_data, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_data], Append, `spark_catalog`.`default`.`num_data`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_data), [id, val]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS val#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS val#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3661,7 +3661,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO num_data VALUES (9, -24926804.045047420)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_data, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_data], Append, `spark_catalog`.`default`.`num_data`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_data), [id, val]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(38,10)) AS val#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(38,10)) AS val#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3684,7 +3684,7 @@ INSERT INTO num_result SELECT t1.id, t2.id, t1.val + t2.val
     FROM num_data t1, num_data t2
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_result, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_result], Append, `spark_catalog`.`default`.`num_result`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_result), [id1, id2, result]
-+- Project [cast(id#x as int) AS id1#x, cast(id#x as int) AS id2#x, cast((val + val)#x as decimal(38,10)) AS result#x]
++- Project [id#x AS id1#x, id#x AS id2#x, cast((val + val)#x as decimal(38,10)) AS result#x]
    +- Project [id#x, id#x, (val#x + val#x) AS (val + val)#x]
       +- Join Inner
          :- SubqueryAlias t1
@@ -3723,7 +3723,7 @@ INSERT INTO num_result SELECT t1.id, t2.id, round(t1.val + t2.val, 10)
     FROM num_data t1, num_data t2
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_result, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_result], Append, `spark_catalog`.`default`.`num_result`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_result), [id1, id2, result]
-+- Project [cast(id#x as int) AS id1#x, cast(id#x as int) AS id2#x, cast(round((val + val), 10)#x as decimal(38,10)) AS result#x]
++- Project [id#x AS id1#x, id#x AS id2#x, cast(round((val + val), 10)#x as decimal(38,10)) AS result#x]
    +- Project [id#x, id#x, round((val#x + val#x), 10) AS round((val + val), 10)#x]
       +- Join Inner
          :- SubqueryAlias t1
@@ -3762,7 +3762,7 @@ INSERT INTO num_result SELECT t1.id, t2.id, t1.val - t2.val
     FROM num_data t1, num_data t2
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_result, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_result], Append, `spark_catalog`.`default`.`num_result`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_result), [id1, id2, result]
-+- Project [cast(id#x as int) AS id1#x, cast(id#x as int) AS id2#x, cast((val - val)#x as decimal(38,10)) AS result#x]
++- Project [id#x AS id1#x, id#x AS id2#x, cast((val - val)#x as decimal(38,10)) AS result#x]
    +- Project [id#x, id#x, (val#x - val#x) AS (val - val)#x]
       +- Join Inner
          :- SubqueryAlias t1
@@ -3801,7 +3801,7 @@ INSERT INTO num_result SELECT t1.id, t2.id, round(t1.val - t2.val, 40)
     FROM num_data t1, num_data t2
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_result, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_result], Append, `spark_catalog`.`default`.`num_result`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_result), [id1, id2, result]
-+- Project [cast(id#x as int) AS id1#x, cast(id#x as int) AS id2#x, cast(round((val - val), 40)#x as decimal(38,10)) AS result#x]
++- Project [id#x AS id1#x, id#x AS id2#x, cast(round((val - val), 40)#x as decimal(38,10)) AS result#x]
    +- Project [id#x, id#x, round((val#x - val#x), 40) AS round((val - val), 40)#x]
       +- Join Inner
          :- SubqueryAlias t1
@@ -3879,7 +3879,7 @@ INSERT INTO num_result SELECT t1.id, t2.id, round(t1.val * t2.val, 30)
     FROM num_data t1, num_data t2
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_result, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_result], Append, `spark_catalog`.`default`.`num_result`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_result), [id1, id2, result]
-+- Project [cast(id#x as int) AS id1#x, cast(id#x as int) AS id2#x, cast(round((val * val), 30)#x as decimal(38,10)) AS result#x]
++- Project [id#x AS id1#x, id#x AS id2#x, cast(round((val * val), 30)#x as decimal(38,10)) AS result#x]
    +- Project [id#x, id#x, round((val#x * val#x), 30) AS round((val * val), 30)#x]
       +- Join Inner
          :- SubqueryAlias t1
@@ -3919,7 +3919,7 @@ INSERT INTO num_result SELECT t1.id, t2.id, t1.val / t2.val
     WHERE t2.val != '0.0'
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_result, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_result], Append, `spark_catalog`.`default`.`num_result`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_result), [id1, id2, result]
-+- Project [cast(id#x as int) AS id1#x, cast(id#x as int) AS id2#x, cast((val / val)#x as decimal(38,10)) AS result#x]
++- Project [id#x AS id1#x, id#x AS id2#x, cast((val / val)#x as decimal(38,10)) AS result#x]
    +- Project [id#x, id#x, (val#x / val#x) AS (val / val)#x]
       +- Filter NOT (cast(val#x as double) = cast(0.0 as double))
          +- Join Inner
@@ -3960,7 +3960,7 @@ INSERT INTO num_result SELECT t1.id, t2.id, round(t1.val / t2.val, 80)
     WHERE t2.val != '0.0'
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_result, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_result], Append, `spark_catalog`.`default`.`num_result`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_result), [id1, id2, result]
-+- Project [cast(id#x as int) AS id1#x, cast(id#x as int) AS id2#x, cast(round((val / val), 80)#x as decimal(38,10)) AS result#x]
++- Project [id#x AS id1#x, id#x AS id2#x, cast(round((val / val), 80)#x as decimal(38,10)) AS result#x]
    +- Project [id#x, id#x, round((val#x / val#x), 80) AS round((val / val), 80)#x]
       +- Filter NOT (cast(val#x as double) = cast(0.0 as double))
          +- Join Inner
@@ -4000,7 +4000,7 @@ INSERT INTO num_result SELECT id, 0, SQRT(ABS(val))
     FROM num_data
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_result, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_result], Append, `spark_catalog`.`default`.`num_result`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_result), [id1, id2, result]
-+- Project [cast(id#x as int) AS id1#x, cast(0#x as int) AS id2#x, cast(SQRT(abs(val))#x as decimal(38,10)) AS result#x]
++- Project [id#x AS id1#x, 0#x AS id2#x, cast(SQRT(abs(val))#x as decimal(38,10)) AS result#x]
    +- Project [id#x, 0 AS 0#x, SQRT(cast(abs(val#x) as double)) AS SQRT(abs(val))#x]
       +- SubqueryAlias spark_catalog.default.num_data
          +- Relation spark_catalog.default.num_data[id#x,val#x] parquet
@@ -4035,7 +4035,7 @@ INSERT INTO num_result SELECT id, 0, LN(ABS(val))
     WHERE val != '0.0'
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_result, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_result], Append, `spark_catalog`.`default`.`num_result`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_result), [id1, id2, result]
-+- Project [cast(id#x as int) AS id1#x, cast(0#x as int) AS id2#x, cast(ln(abs(val))#x as decimal(38,10)) AS result#x]
++- Project [id#x AS id1#x, 0#x AS id2#x, cast(ln(abs(val))#x as decimal(38,10)) AS result#x]
    +- Project [id#x, 0 AS 0#x, ln(cast(abs(val#x) as double)) AS ln(abs(val))#x]
       +- Filter NOT (cast(val#x as double) = cast(0.0 as double))
          +- SubqueryAlias spark_catalog.default.num_data
@@ -4071,7 +4071,7 @@ INSERT INTO num_result SELECT id, 0, LOG(cast('10' as decimal(38, 18)), ABS(val)
     WHERE val != '0.0'
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_result, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_result], Append, `spark_catalog`.`default`.`num_result`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_result), [id1, id2, result]
-+- Project [cast(id#x as int) AS id1#x, cast(0#x as int) AS id2#x, cast(LOG(CAST(10 AS DECIMAL(38,18)), abs(val))#x as decimal(38,10)) AS result#x]
++- Project [id#x AS id1#x, 0#x AS id2#x, cast(LOG(CAST(10 AS DECIMAL(38,18)), abs(val))#x as decimal(38,10)) AS result#x]
    +- Project [id#x, 0 AS 0#x, LOG(cast(cast(10 as decimal(38,18)) as double), cast(abs(val#x) as double)) AS LOG(CAST(10 AS DECIMAL(38,18)), abs(val))#x]
       +- Filter NOT (cast(val#x as double) = cast(0.0 as double))
          +- SubqueryAlias spark_catalog.default.num_data
@@ -4107,7 +4107,7 @@ INSERT INTO num_result SELECT id, 0, POWER(cast('10' as decimal(38, 18)), LN(ABS
     WHERE val != '0.0'
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/num_result, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/num_result], Append, `spark_catalog`.`default`.`num_result`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/num_result), [id1, id2, result]
-+- Project [cast(id#x as int) AS id1#x, cast(0#x as int) AS id2#x, cast(POWER(CAST(10 AS DECIMAL(38,18)), ln(abs(round(val, 200))))#x as decimal(38,10)) AS result#x]
++- Project [id#x AS id1#x, 0#x AS id2#x, cast(POWER(CAST(10 AS DECIMAL(38,18)), ln(abs(round(val, 200))))#x as decimal(38,10)) AS result#x]
    +- Project [id#x, 0 AS 0#x, POWER(cast(cast(10 as decimal(38,18)) as double), ln(cast(abs(round(val#x, 200)) as double))) AS POWER(CAST(10 AS DECIMAL(38,18)), ln(abs(round(val, 200))))#x]
       +- Filter NOT (cast(val#x as double) = cast(0.0 as double))
          +- SubqueryAlias spark_catalog.default.num_data
@@ -4149,7 +4149,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`fract_only`, false
 INSERT INTO fract_only VALUES (1, 0.0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/fract_only, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/fract_only], Append, `spark_catalog`.`default`.`fract_only`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/fract_only), [id, val]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(4,4)) AS val#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(4,4)) AS val#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -4157,7 +4157,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO fract_only VALUES (2, 0.1)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/fract_only, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/fract_only], Append, `spark_catalog`.`default`.`fract_only`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/fract_only), [id, val]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(4,4)) AS val#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(4,4)) AS val#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -4165,7 +4165,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO fract_only VALUES (4, -0.9999)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/fract_only, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/fract_only], Append, `spark_catalog`.`default`.`fract_only`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/fract_only), [id, val]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(4,4)) AS val#x]
++- Project [col1#x AS id#x, col2#x AS val#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -4173,7 +4173,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO fract_only VALUES (5, 0.99994)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/fract_only, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/fract_only], Append, `spark_catalog`.`default`.`fract_only`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/fract_only), [id, val]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(4,4)) AS val#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(4,4)) AS val#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -4181,7 +4181,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO fract_only VALUES (7, 0.00001)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/fract_only, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/fract_only], Append, `spark_catalog`.`default`.`fract_only`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/fract_only), [id, val]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(4,4)) AS val#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(4,4)) AS val#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -4189,7 +4189,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO fract_only VALUES (8, 0.00017)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/fract_only, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/fract_only], Append, `spark_catalog`.`default`.`fract_only`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/fract_only), [id, val]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as decimal(4,4)) AS val#x]
++- Project [col1#x AS id#x, cast(col2#x as decimal(4,4)) AS val#x]
    +- LocalRelation [col1#x, col2#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/select_having.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/select_having.sql.out
index 640ab207579..72b607951ec 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/select_having.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/select_having.sql.out
@@ -9,7 +9,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`test_having`, false
 INSERT INTO test_having VALUES (0, 1, 'XXXX', 'A')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -17,7 +17,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_having VALUES (1, 2, 'AAAA', 'b')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -25,7 +25,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_having VALUES (2, 2, 'AAAA', 'c')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -33,7 +33,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_having VALUES (3, 3, 'BBBB', 'D')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -41,7 +41,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_having VALUES (4, 3, 'BBBB', 'e')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -49,7 +49,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_having VALUES (5, 3, 'bbbb', 'F')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -57,7 +57,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_having VALUES (6, 4, 'cccc', 'g')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -65,7 +65,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_having VALUES (7, 4, 'cccc', 'h')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -73,7 +73,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_having VALUES (8, 4, 'CCCC', 'I')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -81,7 +81,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_having VALUES (9, 4, 'CCCC', 'j')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/select_implicit.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/select_implicit.sql.out
index 83b10b3cb67..e456a80d5e0 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/select_implicit.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/select_implicit.sql.out
@@ -9,7 +9,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`test_missing_target`, fa
 INSERT INTO test_missing_target VALUES (0, 1, 'XXXX', 'A')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -17,7 +17,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_missing_target VALUES (1, 2, 'ABAB', 'b')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -25,7 +25,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_missing_target VALUES (2, 2, 'ABAB', 'c')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -33,7 +33,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_missing_target VALUES (3, 3, 'BBBB', 'D')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -41,7 +41,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_missing_target VALUES (4, 3, 'BBBB', 'e')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -49,7 +49,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_missing_target VALUES (5, 3, 'bbbb', 'F')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -57,7 +57,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_missing_target VALUES (6, 4, 'cccc', 'g')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -65,7 +65,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_missing_target VALUES (7, 4, 'cccc', 'h')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -73,7 +73,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_missing_target VALUES (8, 4, 'CCCC', 'I')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -81,7 +81,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_missing_target VALUES (9, 4, 'CCCC', 'j')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/strings.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/strings.sql.out
index 2a4d79360b5..57d6a813c25 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/strings.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/strings.sql.out
@@ -512,7 +512,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`toasttest`, false
 insert into toasttest values(repeat('1234567890',10000))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/toasttest, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/toasttest], Append, `spark_catalog`.`default`.`toasttest`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/toasttest), [f1]
-+- Project [cast(col1#x as string) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -520,7 +520,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 insert into toasttest values(repeat('1234567890',10000))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/toasttest, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/toasttest], Append, `spark_catalog`.`default`.`toasttest`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/toasttest), [f1]
-+- Project [cast(col1#x as string) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -528,7 +528,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 insert into toasttest values(repeat('1234567890',10000))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/toasttest, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/toasttest], Append, `spark_catalog`.`default`.`toasttest`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/toasttest), [f1]
-+- Project [cast(col1#x as string) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -536,7 +536,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 insert into toasttest values(repeat('1234567890',10000))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/toasttest, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/toasttest], Append, `spark_catalog`.`default`.`toasttest`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/toasttest), [f1]
-+- Project [cast(col1#x as string) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/text.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/text.sql.out
index 474c2401f40..ef7b7a4180b 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/text.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/text.sql.out
@@ -23,7 +23,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`TEXT_TBL`, false
 INSERT INTO TEXT_TBL VALUES ('doh!')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/text_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/text_tbl], Append, `spark_catalog`.`default`.`text_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/text_tbl), [f1]
-+- Project [cast(col1#x as string) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -31,7 +31,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO TEXT_TBL VALUES ('hi de ho neighbor')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/text_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/text_tbl], Append, `spark_catalog`.`default`.`text_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/text_tbl), [f1]
-+- Project [cast(col1#x as string) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/timestamp.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/timestamp.sql.out
index a6c3c278296..3bc151fe619 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/timestamp.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/timestamp.sql.out
@@ -9,7 +9,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`TIMESTAMP_TBL`, false
 INSERT INTO TIMESTAMP_TBL VALUES (timestamp'now')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/timestamp_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/timestamp_tbl], Append, `spark_catalog`.`default`.`timestamp_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/timestamp_tbl), [d1]
-+- Project [cast(col1#x as timestamp) AS d1#x]
++- Project [col1#x AS d1#x]
    +- LocalRelation [col1#x]
 
 
@@ -17,7 +17,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO TIMESTAMP_TBL VALUES (timestamp'now')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/timestamp_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/timestamp_tbl], Append, `spark_catalog`.`default`.`timestamp_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/timestamp_tbl), [d1]
-+- Project [cast(col1#x as timestamp) AS d1#x]
++- Project [col1#x AS d1#x]
    +- LocalRelation [col1#x]
 
 
@@ -25,7 +25,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO TIMESTAMP_TBL VALUES (timestamp'today')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/timestamp_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/timestamp_tbl], Append, `spark_catalog`.`default`.`timestamp_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/timestamp_tbl), [d1]
-+- Project [cast(col1#x as timestamp) AS d1#x]
++- Project [col1#x AS d1#x]
    +- LocalRelation [col1#x]
 
 
@@ -33,7 +33,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO TIMESTAMP_TBL VALUES (timestamp'yesterday')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/timestamp_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/timestamp_tbl], Append, `spark_catalog`.`default`.`timestamp_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/timestamp_tbl), [d1]
-+- Project [cast(col1#x as timestamp) AS d1#x]
++- Project [col1#x AS d1#x]
    +- LocalRelation [col1#x]
 
 
@@ -41,7 +41,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO TIMESTAMP_TBL VALUES (timestamp'tomorrow')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/timestamp_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/timestamp_tbl], Append, `spark_catalog`.`default`.`timestamp_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/timestamp_tbl), [d1]
-+- Project [cast(col1#x as timestamp) AS d1#x]
++- Project [col1#x AS d1#x]
    +- LocalRelation [col1#x]
 
 
@@ -49,7 +49,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO TIMESTAMP_TBL VALUES (timestamp'tomorrow EST')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/timestamp_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/timestamp_tbl], Append, `spark_catalog`.`default`.`timestamp_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/timestamp_tbl), [d1]
-+- Project [cast(col1#x as timestamp) AS d1#x]
++- Project [col1#x AS d1#x]
    +- LocalRelation [col1#x]
 
 
@@ -57,7 +57,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO TIMESTAMP_TBL VALUES (timestamp'tomorrow Zulu')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/timestamp_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/timestamp_tbl], Append, `spark_catalog`.`default`.`timestamp_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/timestamp_tbl), [d1]
-+- Project [cast(col1#x as timestamp) AS d1#x]
++- Project [col1#x AS d1#x]
    +- LocalRelation [col1#x]
 
 
@@ -89,7 +89,7 @@ TruncateTableCommand `spark_catalog`.`default`.`timestamp_tbl`
 INSERT INTO TIMESTAMP_TBL VALUES (timestamp'epoch')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/timestamp_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/timestamp_tbl], Append, `spark_catalog`.`default`.`timestamp_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/timestamp_tbl), [d1]
-+- Project [cast(col1#x as timestamp) AS d1#x]
++- Project [col1#x AS d1#x]
    +- LocalRelation [col1#x]
 
 
@@ -97,7 +97,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO TIMESTAMP_TBL VALUES (timestamp('1997-01-02'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/timestamp_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/timestamp_tbl], Append, `spark_catalog`.`default`.`timestamp_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/timestamp_tbl), [d1]
-+- Project [cast(col1#x as timestamp) AS d1#x]
++- Project [col1#x AS d1#x]
    +- LocalRelation [col1#x]
 
 
@@ -105,7 +105,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO TIMESTAMP_TBL VALUES (timestamp('1997-01-02 03:04:05'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/timestamp_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/timestamp_tbl], Append, `spark_catalog`.`default`.`timestamp_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/timestamp_tbl), [d1]
-+- Project [cast(col1#x as timestamp) AS d1#x]
++- Project [col1#x AS d1#x]
    +- LocalRelation [col1#x]
 
 
@@ -113,7 +113,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO TIMESTAMP_TBL VALUES (timestamp('1997-02-10 17:32:01-08'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/timestamp_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/timestamp_tbl], Append, `spark_catalog`.`default`.`timestamp_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/timestamp_tbl), [d1]
-+- Project [cast(col1#x as timestamp) AS d1#x]
++- Project [col1#x AS d1#x]
    +- LocalRelation [col1#x]
 
 
@@ -121,7 +121,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO TIMESTAMP_TBL VALUES (timestamp('2001-09-22T18:19:20'))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/timestamp_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/timestamp_tbl], Append, `spark_catalog`.`default`.`timestamp_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/timestamp_tbl), [d1]
-+- Project [cast(col1#x as timestamp) AS d1#x]
++- Project [col1#x AS d1#x]
    +- LocalRelation [col1#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/window_part2.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/window_part2.sql.out
index cdcd563de4f..5281494bece 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/window_part2.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/window_part2.sql.out
@@ -24,7 +24,7 @@ INSERT INTO empsalary VALUES
   ('develop', 11, 5200, date '2007-08-15')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/empsalary, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/empsalary], Append, `spark_catalog`.`default`.`empsalary`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/empsalary), [depname, empno, salary, enroll_date]
-+- Project [cast(col1#x as string) AS depname#x, cast(col2#x as int) AS empno#x, cast(col3#x as int) AS salary#x, cast(col4#x as date) AS enroll_date#x]
++- Project [col1#x AS depname#x, col2#x AS empno#x, col3#x AS salary#x, col4#x AS enroll_date#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -280,7 +280,7 @@ insert into numerics values
 (7, 100, 100, 100)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/numerics, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/numerics], Append, `spark_catalog`.`default`.`numerics`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/numerics), [id, f_float4, f_float8, f_numeric]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as float) AS f_float4#x, cast(col3#x as float) AS f_float8#x, cast(col4#x as int) AS f_numeric#x]
++- Project [col1#x AS id#x, cast(col2#x as float) AS f_float4#x, cast(col3#x as float) AS f_float8#x, cast(col4#x as int) AS f_numeric#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/window_part3.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/window_part3.sql.out
index d552c108d46..bc6dc828ad8 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/window_part3.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/window_part3.sql.out
@@ -33,7 +33,7 @@ INSERT INTO empsalary VALUES
   ('develop', 11, 5200, date '2007-08-15')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/empsalary, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/empsalary], Append, `spark_catalog`.`default`.`empsalary`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/empsalary), [depname, empno, salary, enroll_date]
-+- Project [cast(col1#x as string) AS depname#x, cast(col2#x as int) AS empno#x, cast(col3#x as int) AS salary#x, cast(col4#x as date) AS enroll_date#x]
++- Project [col1#x AS depname#x, col2#x AS empno#x, col3#x AS salary#x, col4#x AS enroll_date#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -223,7 +223,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t1`, false
 insert into t1 values (1,1),(1,2),(2,2)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [f1, f2]
-+- Project [cast(col1#x as int) AS f1#x, cast(col2#x as int) AS f2#x]
++- Project [col1#x AS f1#x, col2#x AS f2#x]
    +- LocalRelation [col1#x, col2#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/with.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/with.sql.out
index 00ee071abdc..2081f4a4f32 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/with.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/with.sql.out
@@ -325,7 +325,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`department`, false
 INSERT INTO department VALUES (0, NULL, 'ROOT')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/department, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/department], Append, `spark_catalog`.`default`.`department`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/department), [id, parent_department, name]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as int) AS parent_department#x, cast(col3#x as string) AS name#x]
++- Project [col1#x AS id#x, cast(col2#x as int) AS parent_department#x, col3#x AS name#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -333,7 +333,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO department VALUES (1, 0, 'A')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/department, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/department], Append, `spark_catalog`.`default`.`department`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/department), [id, parent_department, name]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as int) AS parent_department#x, cast(col3#x as string) AS name#x]
++- Project [col1#x AS id#x, col2#x AS parent_department#x, col3#x AS name#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -341,7 +341,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO department VALUES (2, 1, 'B')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/department, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/department], Append, `spark_catalog`.`default`.`department`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/department), [id, parent_department, name]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as int) AS parent_department#x, cast(col3#x as string) AS name#x]
++- Project [col1#x AS id#x, col2#x AS parent_department#x, col3#x AS name#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -349,7 +349,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO department VALUES (3, 2, 'C')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/department, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/department], Append, `spark_catalog`.`default`.`department`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/department), [id, parent_department, name]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as int) AS parent_department#x, cast(col3#x as string) AS name#x]
++- Project [col1#x AS id#x, col2#x AS parent_department#x, col3#x AS name#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -357,7 +357,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO department VALUES (4, 2, 'D')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/department, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/department], Append, `spark_catalog`.`default`.`department`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/department), [id, parent_department, name]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as int) AS parent_department#x, cast(col3#x as string) AS name#x]
++- Project [col1#x AS id#x, col2#x AS parent_department#x, col3#x AS name#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -365,7 +365,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO department VALUES (5, 0, 'E')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/department, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/department], Append, `spark_catalog`.`default`.`department`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/department), [id, parent_department, name]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as int) AS parent_department#x, cast(col3#x as string) AS name#x]
++- Project [col1#x AS id#x, col2#x AS parent_department#x, col3#x AS name#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -373,7 +373,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO department VALUES (6, 4, 'F')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/department, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/department], Append, `spark_catalog`.`default`.`department`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/department), [id, parent_department, name]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as int) AS parent_department#x, cast(col3#x as string) AS name#x]
++- Project [col1#x AS id#x, col2#x AS parent_department#x, col3#x AS name#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -381,7 +381,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO department VALUES (7, 5, 'G')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/department, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/department], Append, `spark_catalog`.`default`.`department`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/department), [id, parent_department, name]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as int) AS parent_department#x, cast(col3#x as string) AS name#x]
++- Project [col1#x AS id#x, col2#x AS parent_department#x, col3#x AS name#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -774,7 +774,7 @@ VALUES (1, NULL), (2, 1), (3,1), (4,2), (5,2), (6,2), (7,3), (8,3),
        (9,4), (10,4), (11,7), (12,7), (13,7), (14, 9), (15,11), (16,11)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/tree, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/tree], Append, `spark_catalog`.`default`.`tree`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/tree), [id, parent_id]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as int) AS parent_id#x]
++- Project [col1#x AS id#x, col2#x AS parent_id#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -902,7 +902,7 @@ insert into graph values
 	(5, 1, 'arc 5 -> 1')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/graph, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/graph], Append, `spark_catalog`.`default`.`graph`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/graph), [f, t, label]
-+- Project [cast(col1#x as int) AS f#x, cast(col2#x as int) AS t#x, cast(col3#x as string) AS label#x]
++- Project [col1#x AS f#x, col2#x AS t#x, col3#x AS label#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -1111,7 +1111,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`y`, false
 INSERT INTO y SELECT EXPLODE(SEQUENCE(1, 10))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/y, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/y], Append, `spark_catalog`.`default`.`y`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/y), [a]
-+- Project [cast(col#x as int) AS a#x]
++- Project [col#x AS a#x]
    +- Project [col#x]
       +- Generate explode(sequence(1, 10, None, Some(America/Los_Angeles))), false, [col#x]
          +- OneRowRelation
@@ -1200,7 +1200,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`y`, false
 INSERT INTO y SELECT EXPLODE(SEQUENCE(1, 10))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/y, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/y], Append, `spark_catalog`.`default`.`y`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/y), [a]
-+- Project [cast(col#x as int) AS a#x]
++- Project [col#x AS a#x]
    +- Project [col#x]
       +- Generate explode(sequence(1, 10, None, Some(America/Los_Angeles))), false, [col#x]
          +- OneRowRelation
@@ -1709,7 +1709,7 @@ TruncateTableCommand `spark_catalog`.`default`.`y`
 INSERT INTO y SELECT EXPLODE(SEQUENCE(1, 3))
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/y, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/y], Append, `spark_catalog`.`default`.`y`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/y), [a]
-+- Project [cast(col#x as int) AS a#x]
++- Project [col#x AS a#x]
    +- Project [col#x]
       +- Generate explode(sequence(1, 3, None, Some(America/Los_Angeles))), false, [col#x]
          +- OneRowRelation
@@ -1763,7 +1763,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`parent`, false
 INSERT INTO parent VALUES ( 1, 'p1' )
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/parent, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/parent], Append, `spark_catalog`.`default`.`parent`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/parent), [id, val]
-+- Project [cast(col1#x as int) AS id#x, cast(col2#x as string) AS val#x]
++- Project [col1#x AS id#x, col2#x AS val#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -1866,7 +1866,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`test`, false
 with test as (select 42) insert into test select * from test
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test], Append, `spark_catalog`.`default`.`test`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test), [i]
-+- Project [cast(42#x as int) AS i#x]
++- Project [42#x AS i#x]
    +- WithCTE
       :- CTERelationDef xxxx, false
       :  +- SubqueryAlias test
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/subquery/in-subquery/in-null-semantics.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/subquery/in-subquery/in-null-semantics.sql.out
index 51fb2455c19..2bb8cafed86 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/subquery/in-subquery/in-null-semantics.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/subquery/in-subquery/in-null-semantics.sql.out
@@ -25,7 +25,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t`, false
 insert into t values (1), (null)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, false, JSON, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [c]
-+- Project [cast(col1#x as int) AS c#x]
++- Project [col1#x AS c#x]
    +- LocalRelation [col1#x]
 
 
@@ -39,7 +39,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t2`, false
 insert into t2 values (2)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t2, false, JSON, [path=file:[not included in comparison]/{warehouse_dir}/t2], Append, `spark_catalog`.`default`.`t2`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t2), [d]
-+- Project [cast(col1#x as int) AS d#x]
++- Project [col1#x AS d#x]
    +- LocalRelation [col1#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/subquery/subquery-nested-data.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/subquery/subquery-nested-data.sql.out
index a218238073d..752c052f2b1 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/subquery/subquery-nested-data.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/subquery/subquery-nested-data.sql.out
@@ -23,7 +23,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`x`, false
 insert into x values (map(1, 2), 3), (map(1, 4), 5), (map(2, 3), 4), (map(5, 6), 7)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/x, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/x], Append, `spark_catalog`.`default`.`x`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/x), [xm, x2]
-+- Project [col1#x AS xm#x, cast(col2#x as int) AS x2#x]
++- Project [col1#x AS xm#x, col2#x AS x2#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -37,7 +37,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`y`, false
 insert into y values (map(1, 2), 10), (map(1, 3), 20), (map(2, 3), 20), (map(8, 3), 20)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/y, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/y], Append, `spark_catalog`.`default`.`y`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/y), [ym, y2]
-+- Project [col1#x AS ym#x, cast(col2#x as int) AS y2#x]
++- Project [col1#x AS ym#x, col2#x AS y2#x]
    +- LocalRelation [col1#x, col2#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/subquery/subquery-offset.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/subquery/subquery-offset.sql.out
index bedb1b07a80..ed89c720ffe 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/subquery/subquery-offset.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/subquery/subquery-offset.sql.out
@@ -23,7 +23,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`x`, false
 insert into x values (1, 1), (2, 2)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/x, false, JSON, [path=file:[not included in comparison]/{warehouse_dir}/x], Append, `spark_catalog`.`default`.`x`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/x), [x1, x2]
-+- Project [cast(col1#x as int) AS x1#x, cast(col2#x as int) AS x2#x]
++- Project [col1#x AS x1#x, col2#x AS x2#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -37,7 +37,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`y`, false
 insert into y values (1, 1), (1, 2), (2, 4)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/y, false, JSON, [path=file:[not included in comparison]/{warehouse_dir}/y], Append, `spark_catalog`.`default`.`y`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/y), [y1, y2]
-+- Project [cast(col1#x as int) AS y1#x, cast(col2#x as int) AS y2#x]
++- Project [col1#x AS y1#x, col2#x AS y2#x]
    +- LocalRelation [col1#x, col2#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/timestamp-ntz.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/timestamp-ntz.sql.out
index 877a7439ade..e099b80f6ba 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/timestamp-ntz.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/timestamp-ntz.sql.out
@@ -284,7 +284,7 @@ INSERT INTO a PARTITION(a=timestamp_ntz'2018-11-17 13:33:33') VALUES (1)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/a, [a=2018-11-17 13:33:33], false, [a#x], Parquet, [path=file:[not included in comparison]/{warehouse_dir}/a], Append, `spark_catalog`.`default`.`a`, org.apache.spark.sql.execution.datasources.CatalogFileIndex(file:[not included in comparison]/{warehouse_dir}/a), [b, a]
 +- Project [b#x, cast(2018-11-17 13:33:33 as timestamp_ntz) AS a#x]
-   +- Project [cast(col1#x as int) AS b#x]
+   +- Project [col1#x AS b#x]
       +- LocalRelation [col1#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/udf/postgreSQL/udf-case.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/udf/postgreSQL/udf-case.sql.out
index c0f00168908..eb25e5ca298 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/udf/postgreSQL/udf-case.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/udf/postgreSQL/udf-case.sql.out
@@ -21,7 +21,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`CASE2_TBL`, false
 INSERT INTO CASE_TBL VALUES (1, 10.1)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case_tbl], Append, `spark_catalog`.`default`.`case_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case_tbl), [i, f]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as double) AS f#x]
++- Project [col1#x AS i#x, cast(col2#x as double) AS f#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -29,7 +29,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO CASE_TBL VALUES (2, 20.2)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case_tbl], Append, `spark_catalog`.`default`.`case_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case_tbl), [i, f]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as double) AS f#x]
++- Project [col1#x AS i#x, cast(col2#x as double) AS f#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -37,7 +37,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO CASE_TBL VALUES (3, -30.3)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case_tbl], Append, `spark_catalog`.`default`.`case_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case_tbl), [i, f]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as double) AS f#x]
++- Project [col1#x AS i#x, cast(col2#x as double) AS f#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -45,7 +45,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO CASE_TBL VALUES (4, NULL)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case_tbl], Append, `spark_catalog`.`default`.`case_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case_tbl), [i, f]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as double) AS f#x]
++- Project [col1#x AS i#x, cast(col2#x as double) AS f#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -53,7 +53,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO CASE2_TBL VALUES (1, -1)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case2_tbl], Append, `spark_catalog`.`default`.`case2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case2_tbl), [i, j]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x]
++- Project [col1#x AS i#x, col2#x AS j#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -61,7 +61,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO CASE2_TBL VALUES (2, -2)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case2_tbl], Append, `spark_catalog`.`default`.`case2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case2_tbl), [i, j]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x]
++- Project [col1#x AS i#x, col2#x AS j#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -69,7 +69,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO CASE2_TBL VALUES (3, -3)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case2_tbl], Append, `spark_catalog`.`default`.`case2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case2_tbl), [i, j]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x]
++- Project [col1#x AS i#x, col2#x AS j#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -77,7 +77,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO CASE2_TBL VALUES (2, -4)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case2_tbl], Append, `spark_catalog`.`default`.`case2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case2_tbl), [i, j]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x]
++- Project [col1#x AS i#x, col2#x AS j#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -85,7 +85,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO CASE2_TBL VALUES (1, NULL)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case2_tbl], Append, `spark_catalog`.`default`.`case2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case2_tbl), [i, j]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x]
++- Project [col1#x AS i#x, cast(col2#x as int) AS j#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -93,7 +93,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO CASE2_TBL VALUES (NULL, -6)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/case2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/case2_tbl], Append, `spark_catalog`.`default`.`case2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/case2_tbl), [i, j]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x]
++- Project [cast(col1#x as int) AS i#x, col2#x AS j#x]
    +- LocalRelation [col1#x, col2#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/udf/postgreSQL/udf-join.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/udf/postgreSQL/udf-join.sql.out
index 80ca1ebeab1..09ef51fcf23 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/udf/postgreSQL/udf-join.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/udf/postgreSQL/udf-join.sql.out
@@ -116,7 +116,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`J2_TBL`, false
 INSERT INTO J1_TBL VALUES (1, 4, 'one')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j1_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j1_tbl], Append, `spark_catalog`.`default`.`j1_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j1_tbl), [i, j, t]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x, cast(col3#x as string) AS t#x]
++- Project [col1#x AS i#x, col2#x AS j#x, col3#x AS t#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -124,7 +124,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J1_TBL VALUES (2, 3, 'two')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j1_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j1_tbl], Append, `spark_catalog`.`default`.`j1_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j1_tbl), [i, j, t]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x, cast(col3#x as string) AS t#x]
++- Project [col1#x AS i#x, col2#x AS j#x, col3#x AS t#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -132,7 +132,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J1_TBL VALUES (3, 2, 'three')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j1_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j1_tbl], Append, `spark_catalog`.`default`.`j1_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j1_tbl), [i, j, t]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x, cast(col3#x as string) AS t#x]
++- Project [col1#x AS i#x, col2#x AS j#x, col3#x AS t#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -140,7 +140,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J1_TBL VALUES (4, 1, 'four')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j1_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j1_tbl], Append, `spark_catalog`.`default`.`j1_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j1_tbl), [i, j, t]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x, cast(col3#x as string) AS t#x]
++- Project [col1#x AS i#x, col2#x AS j#x, col3#x AS t#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -148,7 +148,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J1_TBL VALUES (5, 0, 'five')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j1_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j1_tbl], Append, `spark_catalog`.`default`.`j1_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j1_tbl), [i, j, t]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x, cast(col3#x as string) AS t#x]
++- Project [col1#x AS i#x, col2#x AS j#x, col3#x AS t#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -156,7 +156,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J1_TBL VALUES (6, 6, 'six')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j1_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j1_tbl], Append, `spark_catalog`.`default`.`j1_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j1_tbl), [i, j, t]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x, cast(col3#x as string) AS t#x]
++- Project [col1#x AS i#x, col2#x AS j#x, col3#x AS t#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -164,7 +164,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J1_TBL VALUES (7, 7, 'seven')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j1_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j1_tbl], Append, `spark_catalog`.`default`.`j1_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j1_tbl), [i, j, t]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x, cast(col3#x as string) AS t#x]
++- Project [col1#x AS i#x, col2#x AS j#x, col3#x AS t#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -172,7 +172,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J1_TBL VALUES (8, 8, 'eight')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j1_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j1_tbl], Append, `spark_catalog`.`default`.`j1_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j1_tbl), [i, j, t]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x, cast(col3#x as string) AS t#x]
++- Project [col1#x AS i#x, col2#x AS j#x, col3#x AS t#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -180,7 +180,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J1_TBL VALUES (0, NULL, 'zero')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j1_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j1_tbl], Append, `spark_catalog`.`default`.`j1_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j1_tbl), [i, j, t]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x, cast(col3#x as string) AS t#x]
++- Project [col1#x AS i#x, cast(col2#x as int) AS j#x, col3#x AS t#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -188,7 +188,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J1_TBL VALUES (NULL, NULL, 'null')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j1_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j1_tbl], Append, `spark_catalog`.`default`.`j1_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j1_tbl), [i, j, t]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x, cast(col3#x as string) AS t#x]
++- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x, col3#x AS t#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -196,7 +196,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J1_TBL VALUES (NULL, 0, 'zero')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j1_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j1_tbl], Append, `spark_catalog`.`default`.`j1_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j1_tbl), [i, j, t]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS j#x, cast(col3#x as string) AS t#x]
++- Project [cast(col1#x as int) AS i#x, col2#x AS j#x, col3#x AS t#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -204,7 +204,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J2_TBL VALUES (1, -1)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j2_tbl], Append, `spark_catalog`.`default`.`j2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j2_tbl), [i, k]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS k#x]
++- Project [col1#x AS i#x, col2#x AS k#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -212,7 +212,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J2_TBL VALUES (2, 2)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j2_tbl], Append, `spark_catalog`.`default`.`j2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j2_tbl), [i, k]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS k#x]
++- Project [col1#x AS i#x, col2#x AS k#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -220,7 +220,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J2_TBL VALUES (3, -3)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j2_tbl], Append, `spark_catalog`.`default`.`j2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j2_tbl), [i, k]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS k#x]
++- Project [col1#x AS i#x, col2#x AS k#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -228,7 +228,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J2_TBL VALUES (2, 4)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j2_tbl], Append, `spark_catalog`.`default`.`j2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j2_tbl), [i, k]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS k#x]
++- Project [col1#x AS i#x, col2#x AS k#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -236,7 +236,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J2_TBL VALUES (5, -5)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j2_tbl], Append, `spark_catalog`.`default`.`j2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j2_tbl), [i, k]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS k#x]
++- Project [col1#x AS i#x, col2#x AS k#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -244,7 +244,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J2_TBL VALUES (5, -5)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j2_tbl], Append, `spark_catalog`.`default`.`j2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j2_tbl), [i, k]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS k#x]
++- Project [col1#x AS i#x, col2#x AS k#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -252,7 +252,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J2_TBL VALUES (0, NULL)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j2_tbl], Append, `spark_catalog`.`default`.`j2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j2_tbl), [i, k]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS k#x]
++- Project [col1#x AS i#x, cast(col2#x as int) AS k#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -268,7 +268,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO J2_TBL VALUES (NULL, 0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j2_tbl, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j2_tbl], Append, `spark_catalog`.`default`.`j2_tbl`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j2_tbl), [i, k]
-+- Project [cast(col1#x as int) AS i#x, cast(col2#x as int) AS k#x]
++- Project [cast(col1#x as int) AS i#x, col2#x AS k#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -678,7 +678,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t3`, false
 INSERT INTO t1 VALUES ( 'bb', 11 )
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t1], Append, `spark_catalog`.`default`.`t1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t1), [name, n]
-+- Project [cast(col1#x as string) AS name#x, cast(col2#x as int) AS n#x]
++- Project [col1#x AS name#x, col2#x AS n#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -686,7 +686,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO t2 VALUES ( 'bb', 12 )
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t2, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t2], Append, `spark_catalog`.`default`.`t2`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t2), [name, n]
-+- Project [cast(col1#x as string) AS name#x, cast(col2#x as int) AS n#x]
++- Project [col1#x AS name#x, col2#x AS n#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -694,7 +694,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO t2 VALUES ( 'cc', 22 )
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t2, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t2], Append, `spark_catalog`.`default`.`t2`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t2), [name, n]
-+- Project [cast(col1#x as string) AS name#x, cast(col2#x as int) AS n#x]
++- Project [col1#x AS name#x, col2#x AS n#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -702,7 +702,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO t2 VALUES ( 'ee', 42 )
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t2, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t2], Append, `spark_catalog`.`default`.`t2`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t2), [name, n]
-+- Project [cast(col1#x as string) AS name#x, cast(col2#x as int) AS n#x]
++- Project [col1#x AS name#x, col2#x AS n#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -710,7 +710,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO t3 VALUES ( 'bb', 13 )
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t3, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t3], Append, `spark_catalog`.`default`.`t3`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t3), [name, n]
-+- Project [cast(col1#x as string) AS name#x, cast(col2#x as int) AS n#x]
++- Project [col1#x AS name#x, col2#x AS n#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -718,7 +718,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO t3 VALUES ( 'cc', 23 )
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t3, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t3], Append, `spark_catalog`.`default`.`t3`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t3), [name, n]
-+- Project [cast(col1#x as string) AS name#x, cast(col2#x as int) AS n#x]
++- Project [col1#x AS name#x, col2#x AS n#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -726,7 +726,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO t3 VALUES ( 'dd', 33 )
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t3, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t3], Append, `spark_catalog`.`default`.`t3`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t3), [name, n]
-+- Project [cast(col1#x as string) AS name#x, cast(col2#x as int) AS n#x]
++- Project [col1#x AS name#x, col2#x AS n#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -1724,7 +1724,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`tt3`, false
 INSERT INTO tt3 SELECT x.id, repeat('xyzzy', 100) FROM range(1,10001) x
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/tt3, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/tt3], Append, `spark_catalog`.`default`.`tt3`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/tt3), [f1, f2]
-+- Project [cast(id#xL as int) AS f1#x, cast(repeat(xyzzy, 100)#x as string) AS f2#x]
++- Project [cast(id#xL as int) AS f1#x, repeat(xyzzy, 100)#x AS f2#x]
    +- Project [id#xL, repeat(xyzzy, 100) AS repeat(xyzzy, 100)#x]
       +- SubqueryAlias x
          +- Range (1, 10001, step=1)
@@ -1747,7 +1747,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`tt4`, false
 INSERT INTO tt4 VALUES (0),(1),(9999)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/tt4, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/tt4], Append, `spark_catalog`.`default`.`tt4`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/tt4), [f1]
-+- Project [cast(col1#x as int) AS f1#x]
++- Project [col1#x AS f1#x]
    +- LocalRelation [col1#x]
 
 
@@ -3526,7 +3526,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`j2`, false
 INSERT INTO j1 values(1,1),(1,2)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j1, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j1], Append, `spark_catalog`.`default`.`j1`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j1), [id1, id2]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3534,7 +3534,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO j2 values(1,1)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j2, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j2], Append, `spark_catalog`.`default`.`j2`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j2), [id1, id2]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -3542,7 +3542,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO j2 values(1,2)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/j2, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/j2], Append, `spark_catalog`.`default`.`j2`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/j2), [id1, id2]
-+- Project [cast(col1#x as int) AS id1#x, cast(col2#x as int) AS id2#x]
++- Project [col1#x AS id1#x, col2#x AS id2#x]
    +- LocalRelation [col1#x, col2#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/udf/postgreSQL/udf-select_having.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/udf/postgreSQL/udf-select_having.sql.out
index 6401e3d3ff4..5fbae4d0ee3 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/udf/postgreSQL/udf-select_having.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/udf/postgreSQL/udf-select_having.sql.out
@@ -9,7 +9,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`test_having`, false
 INSERT INTO test_having VALUES (0, 1, 'XXXX', 'A')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -17,7 +17,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_having VALUES (1, 2, 'AAAA', 'b')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -25,7 +25,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_having VALUES (2, 2, 'AAAA', 'c')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -33,7 +33,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_having VALUES (3, 3, 'BBBB', 'D')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -41,7 +41,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_having VALUES (4, 3, 'BBBB', 'e')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -49,7 +49,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_having VALUES (5, 3, 'bbbb', 'F')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -57,7 +57,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_having VALUES (6, 4, 'cccc', 'g')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -65,7 +65,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_having VALUES (7, 4, 'cccc', 'h')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -73,7 +73,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_having VALUES (8, 4, 'CCCC', 'I')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -81,7 +81,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_having VALUES (9, 4, 'CCCC', 'j')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_having, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_having], Append, `spark_catalog`.`default`.`test_having`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_having), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/udf/postgreSQL/udf-select_implicit.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/udf/postgreSQL/udf-select_implicit.sql.out
index 05f47aeace9..0e98a1da48e 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/udf/postgreSQL/udf-select_implicit.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/udf/postgreSQL/udf-select_implicit.sql.out
@@ -9,7 +9,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`test_missing_target`, fa
 INSERT INTO test_missing_target VALUES (0, 1, 'XXXX', 'A')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -17,7 +17,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_missing_target VALUES (1, 2, 'ABAB', 'b')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -25,7 +25,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_missing_target VALUES (2, 2, 'ABAB', 'c')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -33,7 +33,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_missing_target VALUES (3, 3, 'BBBB', 'D')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -41,7 +41,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_missing_target VALUES (4, 3, 'BBBB', 'e')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -49,7 +49,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_missing_target VALUES (5, 3, 'bbbb', 'F')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -57,7 +57,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_missing_target VALUES (6, 4, 'cccc', 'g')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -65,7 +65,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_missing_target VALUES (7, 4, 'cccc', 'h')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -73,7 +73,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_missing_target VALUES (8, 4, 'CCCC', 'I')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
@@ -81,7 +81,7 @@ InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_d
 INSERT INTO test_missing_target VALUES (9, 4, 'CCCC', 'j')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/test_missing_target, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/test_missing_target], Append, `spark_catalog`.`default`.`test_missing_target`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/test_missing_target), [a, b, c, d]
-+- Project [cast(col1#x as int) AS a#x, cast(col2#x as int) AS b#x, cast(col3#x as string) AS c#x, cast(col4#x as string) AS d#x]
++- Project [col1#x AS a#x, col2#x AS b#x, col3#x AS c#x, col4#x AS d#x]
    +- LocalRelation [col1#x, col2#x, col3#x, col4#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/view-schema-binding-config.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/view-schema-binding-config.sql.out
index efa221400b0..5e1d30fbcbf 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/view-schema-binding-config.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/view-schema-binding-config.sql.out
@@ -461,7 +461,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t`, false
 INSERT INTO t VALUES ('1', 2)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [c1, c2]
-+- Project [cast(col1#x as string) AS c1#x, cast(col2#x as int) AS c2#x]
++- Project [col1#x AS c1#x, col2#x AS c2#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -487,7 +487,7 @@ DescribeTableCommand `spark_catalog`.`default`.`v`, true, [col_name#x, data_type
 INSERT INTO t VALUES ('a', 2)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [c1, c2]
-+- Project [cast(col1#x as string) AS c1#x, cast(col2#x as int) AS c2#x]
++- Project [col1#x AS c1#x, col2#x AS c2#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -561,7 +561,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t`, false
 INSERT INTO t VALUES (1, 2)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [c1, c2]
-+- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
++- Project [col1#x AS c1#x, col2#x AS c2#x]
    +- LocalRelation [col1#x, col2#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/view-schema-compensation.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/view-schema-compensation.sql.out
index 64295a6f9bc..ba6f387e8d5 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/view-schema-compensation.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/view-schema-compensation.sql.out
@@ -101,7 +101,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t`, false
 INSERT INTO t VALUES ('1', 2)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [c1, c2]
-+- Project [cast(col1#x as string) AS c1#x, cast(col2#x as int) AS c2#x]
++- Project [col1#x AS c1#x, col2#x AS c2#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -127,7 +127,7 @@ DescribeTableCommand `spark_catalog`.`default`.`v`, true, [col_name#x, data_type
 INSERT INTO t VALUES ('a', 2)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [c1, c2]
-+- Project [cast(col1#x as string) AS c1#x, cast(col2#x as int) AS c2#x]
++- Project [col1#x AS c1#x, col2#x AS c2#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -201,7 +201,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t`, false
 INSERT INTO t VALUES (1, 2)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [c1, c2]
-+- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
++- Project [col1#x AS c1#x, col2#x AS c2#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -321,7 +321,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t`, false
 INSERT INTO t VALUES(1)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [c1]
-+- Project [cast(col1#x as int) AS c1#x]
++- Project [col1#x AS c1#x]
    +- LocalRelation [col1#x]
 
 
@@ -357,7 +357,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t`, false
 INSERT INTO t VALUES('1')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [c1]
-+- Project [cast(col1#x as string) AS c1#x]
++- Project [col1#x AS c1#x]
    +- LocalRelation [col1#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/view-schema-evolution.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/view-schema-evolution.sql.out
index 258edf31d4c..90629cc1bcf 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/view-schema-evolution.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/view-schema-evolution.sql.out
@@ -16,7 +16,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t`, false
 INSERT INTO t VALUES (1, 2)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [c1, c2]
-+- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
++- Project [col1#x AS c1#x, col2#x AS c2#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -63,7 +63,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t`, false
 INSERT INTO t VALUES ('1', 2.0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [c4, c5]
-+- Project [cast(col1#x as string) AS c4#x, cast(col2#x as double) AS c5#x]
++- Project [col1#x AS c4#x, cast(col2#x as double) AS c5#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -101,7 +101,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t`, false
 INSERT INTO t VALUES ('1', 2.0, DATE'2022-01-01')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [c4, c5, c6]
-+- Project [cast(col1#x as string) AS c4#x, cast(col2#x as double) AS c5#x, cast(col3#x as date) AS c6#x]
++- Project [col1#x AS c4#x, cast(col2#x as double) AS c5#x, col3#x AS c6#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -139,7 +139,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t`, false
 INSERT INTO t VALUES (1, 2)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [c1, c2]
-+- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
++- Project [col1#x AS c1#x, col2#x AS c2#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -216,7 +216,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t`, false
 INSERT INTO t VALUES (1, 2)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [c1, c2]
-+- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
++- Project [col1#x AS c1#x, col2#x AS c2#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -264,7 +264,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t`, false
 INSERT INTO t VALUES ('1', 2.0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [c1, c2]
-+- Project [cast(col1#x as string) AS c1#x, cast(col2#x as double) AS c2#x]
++- Project [col1#x AS c1#x, cast(col2#x as double) AS c2#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -303,7 +303,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t`, false
 INSERT INTO t VALUES ('1', 2.0, DATE'2022-01-01')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [c1, c2, c3]
-+- Project [cast(col1#x as string) AS c1#x, cast(col2#x as double) AS c2#x, cast(col3#x as date) AS c3#x]
++- Project [col1#x AS c1#x, cast(col2#x as double) AS c2#x, col3#x AS c3#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -342,7 +342,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t`, false
 INSERT INTO t VALUES (1, 2)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [c1, c2]
-+- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
++- Project [col1#x AS c1#x, col2#x AS c2#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -719,7 +719,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t`, false
 INSERT INTO t VALUES(1)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [c1]
-+- Project [cast(col1#x as int) AS c1#x]
++- Project [col1#x AS c1#x]
    +- LocalRelation [col1#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/view-schema-type-evolution.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/view-schema-type-evolution.sql.out
index 95aa35d59fd..650c8ffd798 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/view-schema-type-evolution.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/view-schema-type-evolution.sql.out
@@ -16,7 +16,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t`, false
 INSERT INTO t VALUES (1, 2)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [c1, c2]
-+- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
++- Project [col1#x AS c1#x, col2#x AS c2#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -64,7 +64,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t`, false
 INSERT INTO t VALUES ('1', 2.0)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [c1, c2]
-+- Project [cast(col1#x as string) AS c1#x, cast(col2#x as double) AS c2#x]
++- Project [col1#x AS c1#x, cast(col2#x as double) AS c2#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -103,7 +103,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t`, false
 INSERT INTO t VALUES ('1', 2.0, DATE'2022-01-01')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [c1, c2, c3]
-+- Project [cast(col1#x as string) AS c1#x, cast(col2#x as double) AS c2#x, cast(col3#x as date) AS c3#x]
++- Project [col1#x AS c1#x, cast(col2#x as double) AS c2#x, col3#x AS c3#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -142,7 +142,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t`, false
 INSERT INTO t VALUES (1, 2)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [c1, c2]
-+- Project [cast(col1#x as int) AS c1#x, cast(col2#x as int) AS c2#x]
++- Project [col1#x AS c1#x, col2#x AS c2#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -367,7 +367,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t`, false
 INSERT INTO t VALUES(1)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [c1]
-+- Project [cast(col1#x as int) AS c1#x]
++- Project [col1#x AS c1#x]
    +- LocalRelation [col1#x]
 
 
@@ -397,7 +397,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t`, false
 INSERT INTO t VALUES('1')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [c1]
-+- Project [cast(col1#x as string) AS c1#x]
++- Project [col1#x AS c1#x]
    +- LocalRelation [col1#x]
 
 
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/view-with-default-collation.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/view-with-default-collation.sql.out
index 697208e9c93..017886e9f49 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/view-with-default-collation.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/view-with-default-collation.sql.out
@@ -357,7 +357,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t`, false
 INSERT INTO t VALUES ('a', 'a'), ('A', 'A'), ('b', 'b')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [c1, c2]
-+- Project [cast(col1#x as string) AS c1#x, cast(col2#x as string collate UTF8_LCASE) AS c2#x]
++- Project [col1#x AS c1#x, cast(col2#x as string collate UTF8_LCASE) AS c2#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -536,7 +536,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t`, false
 INSERT INTO t VALUES ('a', 'b', 1)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [c1, c2, c3]
-+- Project [cast(col1#x as string collate UTF8_LCASE) AS c1#x, cast(col2#x as string) AS c2#x, cast(col3#x as int) AS c3#x]
++- Project [cast(col1#x as string collate UTF8_LCASE) AS c1#x, col2#x AS c2#x, col3#x AS c3#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
@@ -976,7 +976,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t`, false
 INSERT INTO t VALUES ('a', 'a'), ('A', 'A'), ('b', 'b')
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [c1, c2]
-+- Project [cast(col1#x as string) AS c1#x, cast(col2#x as string collate UTF8_LCASE) AS c2#x]
++- Project [col1#x AS c1#x, cast(col2#x as string collate UTF8_LCASE) AS c2#x]
    +- LocalRelation [col1#x, col2#x]
 
 
@@ -1155,7 +1155,7 @@ CreateDataSourceTableCommand `spark_catalog`.`default`.`t`, false
 INSERT INTO t VALUES ('a', 'b', 1)
 -- !query analysis
 InsertIntoHadoopFsRelationCommand file:[not included in comparison]/{warehouse_dir}/t, false, Parquet, [path=file:[not included in comparison]/{warehouse_dir}/t], Append, `spark_catalog`.`default`.`t`, org.apache.spark.sql.execution.datasources.InMemoryFileIndex(file:[not included in comparison]/{warehouse_dir}/t), [c1, c2, c3]
-+- Project [cast(col1#x as string collate UTF8_LCASE) AS c1#x, cast(col2#x as string) AS c2#x, cast(col3#x as int) AS c3#x]
++- Project [cast(col1#x as string collate UTF8_LCASE) AS c1#x, col2#x AS c2#x, col3#x AS c3#x]
    +- LocalRelation [col1#x, col2#x, col3#x]
 
 
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala
index af10fbbbc50..6678f9535fe 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala
@@ -1977,41 +1977,108 @@ class InsertSuite extends DataSourceTest with SharedSparkSession {
     withSQLConf(SQLConf.JSON_GENERATOR_WRITE_NULL_IF_WITH_DEFAULT_VALUE.key -> "true",
       SQLConf.JSON_GENERATOR_IGNORE_NULL_FIELDS.key -> "true") {
       withTable("t") {
-        sql("create table t (a int default 42) using json")
-        sql("insert into t values (null)")
-        checkAnswer(spark.table("t"), Row(null))
+        sql("create table t (a int default 42, b int) using json")
+        sql("insert into t values (null, null)")
+        // nulls should be written for fields with defaults, but not for fields without defaults.
+        checkAnswer(readTableAsText("t"), Row("{\"a\":null}"))
+        // default value is not filled in for existing fields.
+        checkAnswer(spark.table("t"), Row(null, null))
       }
     }
     withSQLConf(SQLConf.JSON_GENERATOR_WRITE_NULL_IF_WITH_DEFAULT_VALUE.key -> "false",
       SQLConf.JSON_GENERATOR_IGNORE_NULL_FIELDS.key -> "true") {
       withTable("t") {
-        sql("create table t (a int default 42) using json")
-        sql("insert into t values (null)")
-        checkAnswer(spark.table("t"), Row(42))
+        sql("create table t (a int default 42, b int) using json")
+        sql("insert into t(a,b) values (null, null)")
+        // nulls should not be written for either field
+        checkAnswer(readTableAsText("t"), Row("{}"))
+        // default value is filled in for missing fields.
+        checkAnswer(spark.table("t"), Row(42, null))
       }
     }
-  }
-
-  test("SPARK-39359 Restrict DEFAULT columns to allowlist of supported data source types") {
-    withSQLConf(SQLConf.DEFAULT_COLUMN_ALLOWED_PROVIDERS.key -> "csv,json,orc") {
-      checkError(
-        exception = intercept[AnalysisException] {
-          sql(s"create table t(a string default 'abc') using parquet")
-        },
-        condition = "DEFAULT_UNSUPPORTED",
-        parameters = Map("statementType" -> "CREATE TABLE", "dataSource" -> "parquet"))
+    // SPARK-52772 complex types get same null handling.
+    withSQLConf(SQLConf.JSON_GENERATOR_WRITE_NULL_IF_WITH_DEFAULT_VALUE.key -> "true",
+      SQLConf.JSON_GENERATOR_IGNORE_NULL_FIELDS.key -> "true") {
       withTable("t") {
-        sql(s"create table t(a string, b int) using parquet")
-        checkError(
-          exception = intercept[AnalysisException] {
-            sql("alter table t add column s bigint default 42")
-          },
-          condition = "DEFAULT_UNSUPPORTED",
-          parameters = Map(
-            "statementType" -> "ALTER TABLE ADD COLUMNS",
-            "dataSource" -> "parquet"))
+        sql("create table t (a struct<x: long> default struct(42), b int) using json")
+        // The cast gets the TableOutputResolver to enter the Struct,Struct case.
+        sql("insert into t values (cast(null as struct<x: int>), null)")
+        // nulls should be written for fields with defaults, but not for fields without defaults.
+        checkAnswer(readTableAsText("t"), Row("{\"a\":null}"))
+        // default value is not filled in for existing fields.
+        checkAnswer(spark.table("t"), Row(null, null))
+      }
+    }
+    withSQLConf(SQLConf.JSON_GENERATOR_WRITE_NULL_IF_WITH_DEFAULT_VALUE.key -> "false",
+      SQLConf.JSON_GENERATOR_IGNORE_NULL_FIELDS.key -> "true") {
+      withTable("t") {
+        sql("create table t (a struct<x: long> default struct(42), b int) using json")
+        sql("insert into t values (cast(null as struct<x: int>), null)")
+        // nulls should not be written for either field
+        checkAnswer(readTableAsText("t"), Row("{}"))
+        // default value is filled in for missing fields.
+        checkAnswer(spark.table("t"), Row(Row(42), null))
+      }
+    }
+    // SPARK-52772 Should not pick up JSON DEFAULT from source
+    withSQLConf(SQLConf.JSON_GENERATOR_WRITE_NULL_IF_WITH_DEFAULT_VALUE.key -> "true",
+      SQLConf.JSON_GENERATOR_IGNORE_NULL_FIELDS.key -> "true") {
+      withTable("t", "u") {
+        sql("create table t (a int default 42, b int) using json")
+        sql("create table u (a int, b int) using json") // NO DEFAULT
+        sql("insert into t values (null, null)")
+        sql("insert into u select a, b from t")
+        // t.a gets explicit null
+        checkAnswer(readTableAsText("t"), Row("{\"a\":null}"))
+        // u.a has null ignored
+        checkAnswer(readTableAsText("u"), Row("{}"))
+        // t.a reads explicit null
+        checkAnswer(spark.table("t"), Row(null, null))
+        // u.a reads implicit null
+        checkAnswer(spark.table("u"), Row(null, null))
       }
     }
+    withSQLConf(SQLConf.JSON_GENERATOR_WRITE_NULL_IF_WITH_DEFAULT_VALUE.key -> "false",
+      SQLConf.JSON_GENERATOR_IGNORE_NULL_FIELDS.key -> "true") {
+      withTable("t", "u") {
+        sql("create table t (a int default null, b int) using json")
+        sql("create table u (a int, b int) using json") // NO DEFAULT
+        sql("insert into t values (null, null)")
+        sql("insert into u select a, b from t")
+        // t.a gets explicit null
+        checkAnswer(readTableAsText("t"), Row("{}"))
+        // u.a has null ignored
+        checkAnswer(readTableAsText("u"), Row("{}"))
+        // t.a missing key becomes default value.
+        checkAnswer(spark.table("t"), Row(null, null))
+        // u.a reads implicit null
+        checkAnswer(spark.table("u"), Row(null, null))
+      }
+    }
+    // SPARK-52772 Should not pick up JSON DEFAULT from source, even after rewrites
+    withSQLConf(SQLConf.JSON_GENERATOR_WRITE_NULL_IF_WITH_DEFAULT_VALUE.key -> "true",
+      SQLConf.JSON_GENERATOR_IGNORE_NULL_FIELDS.key -> "true") {
+      withTable("t", "u") {
+        sql("create table t (a int default 42, b int) using json")
+        sql("create table u (a int, b int) using json") // NO DEFAULT
+        sql("insert into t values (null, null)")
+        sql("insert into u select cast(a as int), b from t")
+        // t.a gets explicit null
+        checkAnswer(readTableAsText("t"), Row("{\"a\":null}"))
+        // u.a has null ignored
+        checkAnswer(readTableAsText("u"), Row("{}"))
+        // t.a reads explicit null
+        checkAnswer(spark.table("t"), Row(null, null))
+        // u.a reads implicit null
+        checkAnswer(spark.table("u"), Row(null, null))
+      }
+    }
+  }
+
+  private def readTableAsText(tableName: String): DataFrame = {
+    val meta = spark.sessionState.catalog.getTableMetadata(TableIdentifier(tableName))
+    val path = meta.location.toString
+    spark.read.text(path)
   }
 
   test("SPARK-39557 INSERT INTO statements with tables with array defaults") {
@@ -2150,11 +2217,8 @@ class InsertSuite extends DataSourceTest with SharedSparkSession {
       Config(
         "parquet",
         useDataFrames = true),
-      // SPARK-47029: ALTER COLUMN DROP DEFAULT fails to work correctly with JSON data sources.
-      /*
       Config(
         "json"),
-        */
       Config(
         "json",
         useDataFrames = true),
