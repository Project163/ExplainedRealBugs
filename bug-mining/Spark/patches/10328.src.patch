diff --git a/python/pyspark/sql/tests/test_observation.py b/python/pyspark/sql/tests/test_observation.py
index 5af709d191c..4c9640ffe2c 100644
--- a/python/pyspark/sql/tests/test_observation.py
+++ b/python/pyspark/sql/tests/test_observation.py
@@ -19,6 +19,7 @@ import time
 import unittest
 
 from pyspark.sql import Row, Observation, functions as F
+from pyspark.sql.types import StructType, LongType
 from pyspark.errors import (
     PySparkAssertionError,
     PySparkTypeError,
@@ -173,22 +174,32 @@ class DataFrameObservationTestsMixin:
 
     def test_observe_on_commands(self):
         df = self.spark.range(50)
+        schema = StructType().add("id", LongType(), nullable=False)
 
         test_table = "test_table"
 
         # DataFrameWriter
-        with self.table(test_table):
-            for command, action in [
-                ("collect", lambda df: df.collect()),
-                ("show", lambda df: df.show(50)),
-                ("save", lambda df: df.write.format("noop").mode("overwrite").save()),
-                ("create", lambda df: df.writeTo(test_table).using("parquet").create()),
-            ]:
-                with self.subTest(command=command):
-                    observation = Observation()
-                    observed_df = df.observe(observation, F.count(F.lit(1)).alias("cnt"))
-                    action(observed_df)
-                    self.assertEqual(observation.get, dict(cnt=50))
+        for cache_enabled in [False, True]:
+            with self.subTest(cache_enabled=cache_enabled), self.sql_conf(
+                {"spark.connect.session.planCache.enabled": cache_enabled}
+            ):
+                for command, action in [
+                    ("collect", lambda df: df.collect()),
+                    ("show", lambda df: df.show(50)),
+                    ("save", lambda df: df.write.format("noop").mode("overwrite").save()),
+                    ("create", lambda df: df.writeTo(test_table).using("parquet").create()),
+                ]:
+                    for select_star in [True, False]:
+                        with self.subTest(command=command, select_star=select_star), self.table(
+                            test_table
+                        ):
+                            observation = Observation()
+                            observed_df = df.observe(observation, F.count(F.lit(1)).alias("cnt"))
+                            if select_star:
+                                observed_df = observed_df.select("*")
+                            self.assertEqual(observed_df.schema, schema)
+                            action(observed_df)
+                            self.assertEqual(observation.get, dict(cnt=50))
 
     def test_observe_with_struct_type(self):
         observation = Observation("struct")
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala
index ad6939422b9..142420ee258 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala
@@ -2064,6 +2064,8 @@ case class CollectMetrics(
   override def doCanonicalize(): LogicalPlan = {
     super.doCanonicalize().asInstanceOf[CollectMetrics].copy(dataframeId = 0L)
   }
+
+  final override val nodePatterns: Seq[TreePattern] = Seq(COLLECT_METRICS)
 }
 
 /**
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala
index 1c5c696e27f..11105e40469 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreePatterns.scala
@@ -127,6 +127,7 @@ object TreePattern extends Enumeration  {
   val AGGREGATE: Value = Value
   val APPEND_COLUMNS: Value = Value
   val AS_OF_JOIN: Value = Value
+  val COLLECT_METRICS: Value = Value
   val COMMAND: Value = Value
   val CTE: Value = Value
   val DESERIALIZE_TO_OBJECT: Value = Value
diff --git a/sql/connect/server/src/main/scala/org/apache/spark/sql/connect/planner/SparkConnectPlanner.scala b/sql/connect/server/src/main/scala/org/apache/spark/sql/connect/planner/SparkConnectPlanner.scala
index 37c6d529b82..8267084c215 100644
--- a/sql/connect/server/src/main/scala/org/apache/spark/sql/connect/planner/SparkConnectPlanner.scala
+++ b/sql/connect/server/src/main/scala/org/apache/spark/sql/connect/planner/SparkConnectPlanner.scala
@@ -42,7 +42,7 @@ import org.apache.spark.connect.proto.WriteStreamOperationStart.TriggerCase
 import org.apache.spark.internal.{Logging, LogKeys}
 import org.apache.spark.internal.LogKeys.{DATAFRAME_ID, SESSION_ID}
 import org.apache.spark.resource.{ExecutorResourceRequest, ResourceProfile, TaskResourceProfile, TaskResourceRequest}
-import org.apache.spark.sql.{AnalysisException, Column, Encoders, ForeachWriter, Observation, Row}
+import org.apache.spark.sql.{AnalysisException, Column, Encoders, ForeachWriter, Row}
 import org.apache.spark.sql.catalyst.{expressions, AliasIdentifier, FunctionIdentifier, QueryPlanningTracker}
 import org.apache.spark.sql.catalyst.analysis.{FunctionRegistry, GlobalTempView, LocalTempView, MultiAlias, NameParameterizedQuery, PosParameterizedQuery, UnresolvedAlias, UnresolvedAttribute, UnresolvedDataFrameStar, UnresolvedDeserializer, UnresolvedExtractValue, UnresolvedFunction, UnresolvedOrdinal, UnresolvedPlanId, UnresolvedRegex, UnresolvedRelation, UnresolvedStar, UnresolvedStarWithColumns, UnresolvedStarWithColumnsRenames, UnresolvedSubqueryColumnAliases, UnresolvedTableValuedFunction, UnresolvedTranspose}
 import org.apache.spark.sql.catalyst.encoders.{encoderFor, AgnosticEncoder, ExpressionEncoder, RowEncoder}
@@ -143,7 +143,7 @@ class SparkConnectPlanner(
    */
   @DeveloperApi
   def transformRelation(rel: proto.Relation, cachePlan: Boolean): LogicalPlan = {
-    sessionHolder.usePlanCache(rel, cachePlan) { rel =>
+    val plan = sessionHolder.usePlanCache(rel, cachePlan) { rel =>
       val plan = rel.getRelTypeCase match {
         // DataFrame API
         case proto.Relation.RelTypeCase.SHOW_STRING => transformShowString(rel.getShowString)
@@ -239,6 +239,19 @@ class SparkConnectPlanner(
       }
       plan
     }
+
+    if (executeHolderOpt.isDefined) {
+      plan.transformUpWithSubqueriesAndPruning(_.containsPattern(TreePattern.COLLECT_METRICS)) {
+        case collectMetrics: CollectMetrics if !collectMetrics.child.isStreaming =>
+          // TODO this might be too complex for no good reason. It might
+          //  be easier to inspect the plan after it completes.
+          val observation = session.observationManager.getOrNewObservation(
+            collectMetrics.name,
+            collectMetrics.dataframeId)
+          executeHolder.addObservation(collectMetrics.name, observation)
+          collectMetrics
+      }
+    } else plan
   }
 
   private def transformRelationPlugin(extension: ProtoAny): LogicalPlan = {
@@ -1375,19 +1388,7 @@ class SparkConnectPlanner(
     val metrics = rel.getMetricsList.asScala.toSeq.map { expr =>
       Column(transformExpression(expr))
     }
-    val name = rel.getName
-    val input = transformRelation(rel.getInput)
-
-    if (input.isStreaming || executeHolderOpt.isEmpty) {
-      CollectMetrics(name, metrics.map(_.named), input, planId)
-    } else {
-      // TODO this might be too complex for no good reason. It might
-      //  be easier to inspect the plan after it completes.
-      val observation = Observation(name)
-      session.observationManager.register(observation, planId)
-      executeHolderOpt.get.addObservation(name, observation)
-      CollectMetrics(name, metrics.map(_.named), input, planId)
-    }
+    CollectMetrics(rel.getName, metrics.map(_.named), transformRelation(rel.getInput), planId)
   }
 
   private def transformDeduplicate(rel: proto.Deduplicate): LogicalPlan = {
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/classic/ObservationManager.scala b/sql/core/src/main/scala/org/apache/spark/sql/classic/ObservationManager.scala
index 1fde17ace30..3edd789b685 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/classic/ObservationManager.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/classic/ObservationManager.scala
@@ -45,6 +45,13 @@ private[sql] class ObservationManager(session: SparkSession) {
     observations.putIfAbsent((observation.name, dataFrameId), observation)
   }
 
+  def getOrNewObservation(name: String, dataFrameId: Long): Observation =
+    observations.computeIfAbsent((name, dataFrameId), { _ =>
+      val observation = Observation(name)
+      observation.markRegistered()
+      observation
+    })
+
   private def tryComplete(qe: QueryExecution): Unit = {
     val allMetrics = qe.observedMetrics
     qe.logical.foreach {
