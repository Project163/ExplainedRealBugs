diff --git a/python/pyspark/sql/conversion.py b/python/pyspark/sql/conversion.py
index 6217ffdf986..a2a4891925d 100644
--- a/python/pyspark/sql/conversion.py
+++ b/python/pyspark/sql/conversion.py
@@ -296,7 +296,11 @@ class LocalDataToArrowConversion:
                     return None
                 else:
                     assert isinstance(value, decimal.Decimal)
-                    return None if value.is_nan() else value
+                    if value.is_nan():
+                        if not nullable:
+                            raise PySparkValueError(f"input for {dataType} must not be None")
+                        return None
+                    return value
 
             return convert_decimal
 
diff --git a/python/pyspark/sql/tests/test_creation.py b/python/pyspark/sql/tests/test_creation.py
index 8936e275449..c5e82bb1b4d 100644
--- a/python/pyspark/sql/tests/test_creation.py
+++ b/python/pyspark/sql/tests/test_creation.py
@@ -24,6 +24,7 @@ from typing import cast
 from pyspark.sql import Row
 import pyspark.sql.functions as F
 from pyspark.sql.types import (
+    DecimalType,
     StructType,
     StructField,
     StringType,
@@ -145,6 +146,12 @@ class DataFrameCreationTestsMixin:
             [Row(value=None)],
         )
 
+    def test_check_decimal_nan(self):
+        data = [Row(dec=Decimal("NaN"))]
+        schema = StructType([StructField("dec", DecimalType(), False)])
+        with self.assertRaises(PySparkValueError):
+            self.spark.createDataFrame(data=data, schema=schema)
+
     def test_invalid_argument_create_dataframe(self):
         with self.assertRaises(PySparkTypeError) as pe:
             self.spark.createDataFrame([(1, 2)], schema=123)
diff --git a/python/pyspark/sql/types.py b/python/pyspark/sql/types.py
index 744815b751e..f525d932b3f 100644
--- a/python/pyspark/sql/types.py
+++ b/python/pyspark/sql/types.py
@@ -2707,7 +2707,9 @@ def _make_type_verifier(
             return "field %s in %s" % (n, name)
 
     def verify_nullability(obj: Any) -> bool:
-        if obj is None:
+        if obj is None or (isinstance(obj, decimal.Decimal) and obj.is_nan()):
+            # Spark's DecimalType doesn't support NaN,
+            # casting DoubleType NaN to DecimalType will return Null.
             if nullable:
                 return True
             else:
