diff --git a/dev/lint-python b/dev/lint-python
index 2c244e0c0b2..9de4c2816bc 100755
--- a/dev/lint-python
+++ b/dev/lint-python
@@ -208,7 +208,7 @@ function sphinx_test {
     #   See also https://github.com/sphinx-doc/sphinx/issues/7551.
     PYTHON_HAS_SPHINX_3_0=$("$PYTHON_EXECUTABLE" -c 'from distutils.version import LooseVersion; import sphinx; print(LooseVersion(sphinx.__version__) < LooseVersion("3.1.0"))')
     if [[ "$PYTHON_HAS_SPHINX_3_0" == "False" ]]; then
-        echo "$PYTHON_EXECUTABLE has Sphinx 3.1+ installed but it requires lower then 3.1. Skipping Sphinx build for now."
+        echo "$PYTHON_EXECUTABLE has Sphinx 3.1+ installed but it requires lower than 3.1. Skipping Sphinx build for now."
         echo
         return
     fi
diff --git a/python/pyspark/sql/avro/functions.py b/python/pyspark/sql/avro/functions.py
index 7e4ceb20cd2..dbb8ab4ac24 100644
--- a/python/pyspark/sql/avro/functions.py
+++ b/python/pyspark/sql/avro/functions.py
@@ -37,7 +37,7 @@ def from_avro(data, jsonFormatSchema, options=None):
 
     Parameters
     ----------
-    data : :class:`Column` or str
+    data : :class:`~pyspark.sql.Column` or str
         the binary column.
     jsonFormatSchema : str
         the avro schema in JSON string format.
@@ -86,7 +86,7 @@ def to_avro(data, jsonFormatSchema=""):
 
     Parameters
     ----------
-    data : :class:`Column` or str
+    data : :class:`~pyspark.sql.Column` or str
         the data column.
     jsonFormatSchema : str, optional
         user-specified output avro schema in JSON string format.
diff --git a/python/pyspark/sql/functions.py b/python/pyspark/sql/functions.py
index 6b5f7eefb28..0eb2499de04 100644
--- a/python/pyspark/sql/functions.py
+++ b/python/pyspark/sql/functions.py
@@ -52,7 +52,7 @@ def _get_get_jvm_function(name, sc):
 def _invoke_function(name, *args):
     """
     Invokes JVM function identified by name with args
-    and wraps the result with :class:`Column`.
+    and wraps the result with :class:`~pyspark.sql.Column`.
     """
     jf = _get_get_jvm_function(name, SparkContext._active_spark_context)
     return Column(jf(*args))
@@ -61,7 +61,7 @@ def _invoke_function(name, *args):
 def _invoke_function_over_column(name, col):
     """
     Invokes unary JVM function identified by name
-    and wraps the result with :class:`Column`.
+    and wraps the result with :class:`~pyspark.sql.Column`.
     """
     return _invoke_function(name, _to_java_column(col))
 
@@ -69,7 +69,7 @@ def _invoke_function_over_column(name, col):
 def _invoke_binary_math_function(name, col1, col2):
     """
     Invokes binary JVM math function identified by name
-    and wraps the result with :class:`Column`.
+    and wraps the result with :class:`~pyspark.sql.Column`.
     """
     return _invoke_function(
         name,
@@ -88,7 +88,7 @@ def _options_to_str(options=None):
 
 def lit(col):
     """
-    Creates a :class:`Column` of literal value.
+    Creates a :class:`~pyspark.sql.Column` of literal value.
 
     .. versionadded:: 1.3.0
 
@@ -103,7 +103,7 @@ def lit(col):
 @since(1.3)
 def col(col):
     """
-    Returns a :class:`Column` based on the given column name.'
+    Returns a :class:`~pyspark.sql.Column` based on the given column name.'
     """
     return _invoke_function("col", col)
 
@@ -111,7 +111,7 @@ def col(col):
 @since(1.3)
 def column(col):
     """
-    Returns a :class:`Column` based on the given column name.'
+    Returns a :class:`~pyspark.sql.Column` based on the given column name.'
     """
     return col(col)
 
@@ -228,7 +228,7 @@ def acos(col):
 
     Returns
     -------
-    :class:`Column`
+    :class:`~pyspark.sql.Column`
         inverse cosine of `col`, as if computed by `java.lang.Math.acos()`
     """
     return _invoke_function_over_column("acos", col)
@@ -242,7 +242,7 @@ def acosh(col):
 
     Returns
     -------
-    :class:`Column`
+    :class:`~pyspark.sql.Column`
     """
     return _invoke_function_over_column("acosh", col)
 
@@ -254,7 +254,7 @@ def asin(col):
 
     Returns
     -------
-    :class:`Column`
+    :class:`~pyspark.sql.Column`
         inverse sine of `col`, as if computed by `java.lang.Math.asin()`
     """
     return _invoke_function_over_column("asin", col)
@@ -268,7 +268,7 @@ def asinh(col):
 
     Returns
     -------
-    :class:`Column`
+    :class:`~pyspark.sql.Column`
     """
     return _invoke_function_over_column("asinh", col)
 
@@ -279,7 +279,7 @@ def atan(col):
 
     Returns
     -------
-    :class:`Column`
+    :class:`~pyspark.sql.Column`
         inverse tangent of `col`, as if computed by `java.lang.Math.atan()`
     """
     return _invoke_function_over_column("atan", col)
@@ -293,7 +293,7 @@ def atanh(col):
 
     Returns
     -------
-    :class:`Column`
+    :class:`~pyspark.sql.Column`
     """
     return _invoke_function_over_column("atanh", col)
 
@@ -320,12 +320,12 @@ def cos(col):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         angle in radians
 
     Returns
     -------
-    :class:`Column`
+    :class:`~pyspark.sql.Column`
         cosine of the angle, as if computed by `java.lang.Math.cos()`.
     """
     return _invoke_function_over_column("cos", col)
@@ -337,12 +337,12 @@ def cosh(col):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         hyperbolic angle
 
     Returns
     -------
-    :class:`Column`
+    :class:`~pyspark.sql.Column`
         hyperbolic cosine of the angle, as if computed by `java.lang.Math.cosh()`
     """
     return _invoke_function_over_column("cosh", col)
@@ -419,11 +419,11 @@ def sin(col):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
 
     Returns
     -------
-    :class:`Column`
+    :class:`~pyspark.sql.Column`
         sine of the angle, as if computed by `java.lang.Math.sin()`
     """
     return _invoke_function_over_column("sin", col)
@@ -435,12 +435,12 @@ def sinh(col):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         hyperbolic angle
 
     Returns
     -------
-    :class:`Column`
+    :class:`~pyspark.sql.Column`
         hyperbolic sine of the given value,
         as if computed by `java.lang.Math.sinh()`
     """
@@ -453,12 +453,12 @@ def tan(col):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         angle in radians
 
     Returns
     -------
-    :class:`Column`
+    :class:`~pyspark.sql.Column`
         tangent of the given value, as if computed by `java.lang.Math.tan()`
     """
     return _invoke_function_over_column("tan", col)
@@ -470,12 +470,12 @@ def tanh(col):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         hyperbolic angle
 
     Returns
     -------
-    :class:`Column`
+    :class:`~pyspark.sql.Column`
         hyperbolic tangent of the given value
         as if computed by `java.lang.Math.tanh()`
     """
@@ -686,12 +686,12 @@ def degrees(col):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         angle in radians
 
     Returns
     -------
-    :class:`Column`
+    :class:`~pyspark.sql.Column`
         angle in degrees, as if computed by `java.lang.Math.toDegrees()`
     """
     return _invoke_function_over_column("degrees", col)
@@ -706,12 +706,12 @@ def radians(col):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         angle in degrees
 
     Returns
     -------
-    :class:`Column`
+    :class:`~pyspark.sql.Column`
         angle in radians, as if computed by `java.lang.Math.toRadians()`
     """
     return _invoke_function_over_column("radians", col)
@@ -723,14 +723,14 @@ def atan2(col1, col2):
 
     Parameters
     ----------
-    col1 : str, :class:`Column` or float
+    col1 : str, :class:`~pyspark.sql.Column` or float
         coordinate on y-axis
-    col2 : str, :class:`Column` or float
+    col2 : str, :class:`~pyspark.sql.Column` or float
         coordinate on x-axis
 
     Returns
     -------
-    :class:`Column`
+    :class:`~pyspark.sql.Column`
         the `theta` component of the point
         (`r`, `theta`)
         in polar coordinates that corresponds to the point
@@ -824,14 +824,14 @@ def approxCountDistinct(col, rsd=None):
 
 
 def approx_count_distinct(col, rsd=None):
-    """Aggregate function: returns a new :class:`Column` for approximate distinct count of
-    column `col`.
+    """Aggregate function: returns a new :class:`~pyspark.sql.Column` for approximate distinct count
+    of column `col`.
 
     .. versionadded:: 2.1.0
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
     rsd : float, optional
         maximum relative standard deviation allowed (default = 0.05).
         For rsd < 0.01, it is more efficient to use :func:`count_distinct`
@@ -898,8 +898,8 @@ def coalesce(*cols):
 
 
 def corr(col1, col2):
-    """Returns a new :class:`Column` for the Pearson Correlation Coefficient for ``col1``
-    and ``col2``.
+    """Returns a new :class:`~pyspark.sql.Column` for the Pearson Correlation Coefficient for
+    ``col1`` and ``col2``.
 
     .. versionadded:: 1.6.0
 
@@ -916,7 +916,8 @@ def corr(col1, col2):
 
 
 def covar_pop(col1, col2):
-    """Returns a new :class:`Column` for the population covariance of ``col1`` and ``col2``.
+    """Returns a new :class:`~pyspark.sql.Column` for the population covariance of ``col1`` and
+    ``col2``.
 
     .. versionadded:: 2.0.0
 
@@ -933,7 +934,8 @@ def covar_pop(col1, col2):
 
 
 def covar_samp(col1, col2):
-    """Returns a new :class:`Column` for the sample covariance of ``col1`` and ``col2``.
+    """Returns a new :class:`~pyspark.sql.Column` for the sample covariance of ``col1`` and
+    ``col2``.
 
     .. versionadded:: 2.0.0
 
@@ -950,7 +952,7 @@ def covar_samp(col1, col2):
 
 
 def countDistinct(col, *cols):
-    """Returns a new :class:`Column` for distinct count of ``col`` or ``cols``.
+    """Returns a new :class:`~pyspark.sql.Column` for distinct count of ``col`` or ``cols``.
 
     An alias of :func:`count_distinct`, and it is encouraged to use :func:`count_distinct`
     directly.
@@ -1405,8 +1407,8 @@ def struct(*cols):
 
     Parameters
     ----------
-    cols : list, set, str or :class:`Column`
-        column names or :class:`Column`\\s to contain in the output struct.
+    cols : list, set, str or :class:`~pyspark.sql.Column`
+        column names or :class:`~pyspark.sql.Column`\\s to contain in the output struct.
 
     Examples
     --------
@@ -1462,16 +1464,17 @@ def least(*cols):
 
 def when(condition, value):
     """Evaluates a list of conditions and returns one of multiple possible result expressions.
-    If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.
+    If :func:`pyspark.sql.Column.otherwise` is not invoked, None is returned for unmatched
+    conditions.
 
     .. versionadded:: 1.4.0
 
     Parameters
     ----------
-    condition : :class:`Column`
-        a boolean :class:`Column` expression.
+    condition : :class:`~pyspark.sql.Column`
+        a boolean :class:`~pyspark.sql.Column` expression.
     value :
-        a literal value, or a :class:`Column` expression.
+        a literal value, or a :class:`~pyspark.sql.Column` expression.
 
     >>> df.select(when(df['age'] == 2, 3).otherwise(4).alias("age")).collect()
     [Row(age=3), Row(age=4)]
@@ -1561,7 +1564,7 @@ def factorial(col):
 def lag(col, offset=1, default=None):
     """
     Window function: returns the value that is `offset` rows before the current row, and
-    `defaultValue` if there is less than `offset` rows before the current row. For example,
+    `default` if there is less than `offset` rows before the current row. For example,
     an `offset` of one will return the previous row at any given point in the window partition.
 
     This is equivalent to the LAG function in SQL.
@@ -1570,7 +1573,7 @@ def lag(col, offset=1, default=None):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         name of column or expression
     offset : int, optional
         number of row to extend
@@ -1584,7 +1587,7 @@ def lag(col, offset=1, default=None):
 def lead(col, offset=1, default=None):
     """
     Window function: returns the value that is `offset` rows after the current row, and
-    `defaultValue` if there is less than `offset` rows after the current row. For example,
+    `default` if there is less than `offset` rows after the current row. For example,
     an `offset` of one will return the next row at any given point in the window partition.
 
     This is equivalent to the LEAD function in SQL.
@@ -1593,7 +1596,7 @@ def lead(col, offset=1, default=None):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         name of column or expression
     offset : int, optional
         number of row to extend
@@ -1618,7 +1621,7 @@ def nth_value(col, offset, ignoreNulls=False):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         name of column or expression
     offset : int, optional
         number of row to use as the value
@@ -1945,7 +1948,7 @@ def months_between(date1, date2, roundOff=True):
 
 
 def to_date(col, format=None):
-    """Converts a :class:`Column` into :class:`pyspark.sql.types.DateType`
+    """Converts a :class:`~pyspark.sql.Column` into :class:`pyspark.sql.types.DateType`
     using the optionally specified format. Specify formats according to `datetime pattern`_.
     By default, it follows casting rules to :class:`pyspark.sql.types.DateType` if the format
     is omitted. Equivalent to ``col.cast("date")``.
@@ -1973,7 +1976,7 @@ def to_date(col, format=None):
 
 
 def to_timestamp(col, format=None):
-    """Converts a :class:`Column` into :class:`pyspark.sql.types.TimestampType`
+    """Converts a :class:`~pyspark.sql.Column` into :class:`pyspark.sql.types.TimestampType`
     using the optionally specified format. Specify formats according to `datetime pattern`_.
     By default, it follows casting rules to :class:`pyspark.sql.types.TimestampType` if the format
     is omitted. Equivalent to ``col.cast("timestamp")``.
@@ -2008,7 +2011,7 @@ def trunc(date, format):
 
     Parameters
     ----------
-    date : :class:`Column` or str
+    date : :class:`~pyspark.sql.Column` or str
     format : str
         'year', 'yyyy', 'yy' or 'month', 'mon', 'mm'
 
@@ -2035,7 +2038,7 @@ def date_trunc(format, timestamp):
     format : str
         'year', 'yyyy', 'yy', 'month', 'mon', 'mm',
         'day', 'dd', 'hour', 'minute', 'second', 'week', 'quarter'
-    timestamp : :class:`Column` or str
+    timestamp : :class:`~pyspark.sql.Column` or str
 
     Examples
     --------
@@ -2147,9 +2150,9 @@ def from_utc_timestamp(timestamp, tz):
 
     Parameters
     ----------
-    timestamp : :class:`Column` or str
+    timestamp : :class:`~pyspark.sql.Column` or str
         the column that contains timestamps
-    tz : :class:`Column` or str
+    tz : :class:`~pyspark.sql.Column` or str
         A string detailing the time zone ID that the input should be adjusted to. It should
         be in the format of either region-based zone IDs or zone offsets. Region IDs must
         have the form 'area/city', such as 'America/Los_Angeles'. Zone offsets must be in
@@ -2158,7 +2161,7 @@ def from_utc_timestamp(timestamp, tz):
         because they can be ambiguous.
 
         .. versionchanged:: 2.4
-           `tz` can take a :class:`Column` containing timezone ID strings.
+           `tz` can take a :class:`~pyspark.sql.Column` containing timezone ID strings.
 
     Examples
     --------
@@ -2193,9 +2196,9 @@ def to_utc_timestamp(timestamp, tz):
 
     Parameters
     ----------
-    timestamp : :class:`Column` or str
+    timestamp : :class:`~pyspark.sql.Column` or str
         the column that contains timestamps
-    tz : :class:`Column` or str
+    tz : :class:`~pyspark.sql.Column` or str
         A string detailing the time zone ID that the input should be adjusted to. It should
         be in the format of either region-based zone IDs or zone offsets. Region IDs must
         have the form 'area/city', such as 'America/Los_Angeles'. Zone offsets must be in
@@ -2204,7 +2207,7 @@ def to_utc_timestamp(timestamp, tz):
         because they can be ambiguous.
 
         .. versionchanged:: 2.4.0
-           `tz` can take a :class:`Column` containing timezone ID strings.
+           `tz` can take a :class:`~pyspark.sql.Column` containing timezone ID strings.
 
     Examples
     --------
@@ -2558,7 +2561,7 @@ def format_number(col, d):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         the column name of the numeric value to be formatted
     d : int
         the N decimal places
@@ -2580,8 +2583,8 @@ def format_string(format, *cols):
     ----------
     format : str
         string that can contain embedded format tags and used as result column's value
-    cols : :class:`Column` or str
-        column names or :class:`Column`\\s to be used in formatting
+    cols : :class:`~pyspark.sql.Column` or str
+        column names or :class:`~pyspark.sql.Column`\\s to be used in formatting
 
     Examples
     --------
@@ -2719,7 +2722,7 @@ def locate(substr, str, pos=1):
     ----------
     substr : str
         a string
-    str : :class:`Column` or str
+    str : :class:`~pyspark.sql.Column` or str
         a Column of :class:`pyspark.sql.types.StringType`
     pos : int, optional
         start position (zero based)
@@ -2795,7 +2798,7 @@ def split(str, pattern, limit=-1):
 
     Parameters
     ----------
-    str : :class:`Column` or str
+    str : :class:`~pyspark.sql.Column` or str
         a string expression to split
     pattern : str
         a string representing a regular expression. The regex string should be
@@ -2983,8 +2986,8 @@ def create_map(*cols):
 
     Parameters
     ----------
-    cols : :class:`Column` or str
-        column names or :class:`Column`\\s that are
+    cols : :class:`~pyspark.sql.Column` or str
+        column names or :class:`~pyspark.sql.Column`\\s that are
         grouped as key-value pairs, e.g. (key1, value1, key2, value2, ...).
 
     Examples
@@ -3008,9 +3011,9 @@ def map_from_arrays(col1, col2):
 
     Parameters
     ----------
-    col1 : :class:`Column` or str
+    col1 : :class:`~pyspark.sql.Column` or str
         name of column containing a set of keys. All elements should not be null
-    col2 : :class:`Column` or str
+    col2 : :class:`~pyspark.sql.Column` or str
         name of column containing a set of values
 
     Examples
@@ -3034,8 +3037,8 @@ def array(*cols):
 
     Parameters
     ----------
-    cols : :class:`Column` or str
-        column names or :class:`Column`\\s that have
+    cols : :class:`~pyspark.sql.Column` or str
+        column names or :class:`~pyspark.sql.Column`\\s that have
         the same data type.
 
     Examples
@@ -3061,7 +3064,7 @@ def array_contains(col, value):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         name of column containing array
     value :
         value or column to check for in array
@@ -3106,11 +3109,11 @@ def slice(x, start, length):
 
     Parameters
     ----------
-    x : :class:`Column` or str
+    x : :class:`~pyspark.sql.Column` or str
         the array to be sliced
-    start : :class:`Column` or int
+    start : :class:`~pyspark.sql.Column` or int
         the starting index
-    length : :class:`Column` or int
+    length : :class:`~pyspark.sql.Column` or int
         the length of the slice
 
     Examples
@@ -3202,7 +3205,7 @@ def element_at(col, extraction):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         name of column containing array or map
     extraction :
         index to check for in array or key to check for in map
@@ -3234,7 +3237,7 @@ def array_remove(col, element):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         name of column containing array
     element :
         element to be removed from the array
@@ -3257,7 +3260,7 @@ def array_distinct(col):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         name of column or expression
 
     Examples
@@ -3279,9 +3282,9 @@ def array_intersect(col1, col2):
 
     Parameters
     ----------
-    col1 : :class:`Column` or str
+    col1 : :class:`~pyspark.sql.Column` or str
         name of column containing array
-    col2 : :class:`Column` or str
+    col2 : :class:`~pyspark.sql.Column` or str
         name of column containing array
 
     Examples
@@ -3304,9 +3307,9 @@ def array_union(col1, col2):
 
     Parameters
     ----------
-    col1 : :class:`Column` or str
+    col1 : :class:`~pyspark.sql.Column` or str
         name of column containing array
-    col2 : :class:`Column` or str
+    col2 : :class:`~pyspark.sql.Column` or str
         name of column containing array
 
     Examples
@@ -3329,9 +3332,9 @@ def array_except(col1, col2):
 
     Parameters
     ----------
-    col1 : :class:`Column` or str
+    col1 : :class:`~pyspark.sql.Column` or str
         name of column containing array
-    col2 : :class:`Column` or str
+    col2 : :class:`~pyspark.sql.Column` or str
         name of column containing array
 
     Examples
@@ -3485,7 +3488,7 @@ def get_json_object(col, path):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         string column in json format
     path : str
         path to the json object to extract
@@ -3510,7 +3513,7 @@ def json_tuple(col, *fields):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         string column in json format
     fields : str
         fields to extract
@@ -3537,7 +3540,7 @@ def from_json(col, schema, options=None):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         string column in json format
     schema : :class:`DataType` or str
         a StructType or ArrayType of StructType to use when parsing the json column.
@@ -3592,7 +3595,7 @@ def to_json(col, options=None):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         name of column containing a struct, an array or a map.
     options : dict, optional
         options to control converting. accepts the same options as the JSON datasource.
@@ -3638,7 +3641,7 @@ def schema_of_json(json, options=None):
 
     Parameters
     ----------
-    json : :class:`Column` or str
+    json : :class:`~pyspark.sql.Column` or str
         a JSON string or a foldable string column containing a JSON string.
     options : dict, optional
         options to control parsing. accepts the same options as the JSON datasource
@@ -3675,7 +3678,7 @@ def schema_of_csv(csv, options=None):
 
     Parameters
     ----------
-    csv : :class:`Column` or str
+    csv : :class:`~pyspark.sql.Column` or str
         a CSV string or a foldable string column containing a CSV string.
     options : dict, optional
         options to control parsing. accepts the same options as the CSV datasource
@@ -3709,7 +3712,7 @@ def to_csv(col, options=None):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         name of column containing a struct.
     options: dict, optional
         options to control converting. accepts the same options as the CSV datasource.
@@ -3736,7 +3739,7 @@ def size(col):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         name of column or expression
 
     Examples
@@ -3757,7 +3760,7 @@ def array_min(col):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         name of column or expression
 
     Examples
@@ -3778,7 +3781,7 @@ def array_max(col):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         name of column or expression
 
     Examples
@@ -3802,7 +3805,7 @@ def sort_array(col, asc=True):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         name of column or expression
     asc : bool, optional
 
@@ -3827,7 +3830,7 @@ def array_sort(col):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         name of column or expression
 
     Examples
@@ -3848,7 +3851,7 @@ def shuffle(col):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         name of column or expression
 
     Notes
@@ -3873,7 +3876,7 @@ def reverse(col):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         name of column or expression
 
     Examples
@@ -3899,7 +3902,7 @@ def flatten(col):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         name of column or expression
 
     Examples
@@ -3920,7 +3923,7 @@ def map_keys(col):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         name of column or expression
 
     Examples
@@ -3946,7 +3949,7 @@ def map_values(col):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         name of column or expression
 
     Examples
@@ -3972,7 +3975,7 @@ def map_entries(col):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         name of column or expression
 
     Examples
@@ -3998,7 +4001,7 @@ def map_from_entries(col):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         name of column or expression
 
     Examples
@@ -4044,7 +4047,7 @@ def arrays_zip(*cols):
 
     Parameters
     ----------
-    cols : :class:`Column` or str
+    cols : :class:`~pyspark.sql.Column` or str
         columns of arrays to be merged.
 
     Examples
@@ -4065,8 +4068,8 @@ def map_concat(*cols):
 
     Parameters
     ----------
-    cols : :class:`Column` or str
-        column names or :class:`Column`\\s
+    cols : :class:`~pyspark.sql.Column` or str
+        column names or :class:`~pyspark.sql.Column`\\s
 
     Examples
     --------
@@ -4120,9 +4123,9 @@ def from_csv(col, schema, options=None):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         string column in CSV format
-    schema :class:`Column` or str
+    schema :class:`~pyspark.sql.Column` or str
         a string with schema in DDL format to use when parsing the CSV column.
     options : dict, optional
         options to control parsing. accepts the same options as the CSV datasource
@@ -4267,7 +4270,7 @@ def transform(col, f):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         name of column or expression
     f : function
         a function that is applied to each element of the input array.
@@ -4277,14 +4280,14 @@ def transform(col, f):
         - Binary ``(x: Column, i: Column) -> Column...``, where the second argument is
             a 0-based index of the element.
 
-        and can use methods of :class:`pyspark.sql.Column`, functions defined in
+        and can use methods of :class:`~pyspark.sql.Column`, functions defined in
         :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.
         Python ``UserDefinedFunctions`` are not supported
         (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).
 
     Returns
     -------
-    :class:`pyspark.sql.Column`
+    :class:`~pyspark.sql.Column`
 
     Examples
     --------
@@ -4316,15 +4319,15 @@ def exists(col, f):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         name of column or expression
     f : function
         ``(x: Column) -> Column: ...``  returning the Boolean expression.
-        Can use methods of :class:`pyspark.sql.Column`, functions defined in
+        Can use methods of :class:`~pyspark.sql.Column`, functions defined in
         :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.
         Python ``UserDefinedFunctions`` are not supported
         (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).
-    :return: a :class:`pyspark.sql.Column`
+    :return: a :class:`~pyspark.sql.Column`
 
     Examples
     --------
@@ -4348,18 +4351,18 @@ def forall(col, f):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         name of column or expression
     f : function
         ``(x: Column) -> Column: ...``  returning the Boolean expression.
-        Can use methods of :class:`pyspark.sql.Column`, functions defined in
+        Can use methods of :class:`~pyspark.sql.Column`, functions defined in
         :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.
         Python ``UserDefinedFunctions`` are not supported
         (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).
 
     Returns
     -------
-    :class:`pyspark.sql.Column`
+    :class:`~pyspark.sql.Column`
 
     Examples
     --------
@@ -4387,7 +4390,7 @@ def filter(col, f):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         name of column or expression
     f : function
         A function that returns the Boolean expression.
@@ -4397,14 +4400,14 @@ def filter(col, f):
         - Binary ``(x: Column, i: Column) -> Column...``, where the second argument is
             a 0-based index of the element.
 
-        and can use methods of :class:`pyspark.sql.Column`, functions defined in
+        and can use methods of :class:`~pyspark.sql.Column`, functions defined in
         :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.
         Python ``UserDefinedFunctions`` are not supported
         (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).
 
     Returns
     -------
-    :class:`pyspark.sql.Column`
+    :class:`~pyspark.sql.Column`
 
     Examples
     --------
@@ -4432,7 +4435,7 @@ def aggregate(col, initialValue, merge, finish=None):
     and reduces this to a single state. The final state is converted into the final result
     by applying a finish function.
 
-    Both functions can use methods of :class:`pyspark.sql.Column`, functions defined in
+    Both functions can use methods of :class:`~pyspark.sql.Column`, functions defined in
     :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.
     Python ``UserDefinedFunctions`` are not supported
     (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).
@@ -4441,9 +4444,9 @@ def aggregate(col, initialValue, merge, finish=None):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         name of column or expression
-    initialValue : :class:`Column` or str
+    initialValue : :class:`~pyspark.sql.Column` or str
         initial value. Name of column or expression
     merge : function
         a binary function ``(acc: Column, x: Column) -> Column...`` returning expression
@@ -4454,7 +4457,7 @@ def aggregate(col, initialValue, merge, finish=None):
 
     Returns
     -------
-    :class:`pyspark.sql.Column`
+    :class:`~pyspark.sql.Column`
 
     Examples
     --------
@@ -4509,20 +4512,20 @@ def zip_with(left, right, f):
 
     Parameters
     ----------
-    left : :class:`Column` or str
+    left : :class:`~pyspark.sql.Column` or str
         name of the first column or expression
-    right : :class:`Column` or str
+    right : :class:`~pyspark.sql.Column` or str
         name of the second column or expression
     f : function
         a binary function ``(x1: Column, x2: Column) -> Column...``
-        Can use methods of :class:`pyspark.sql.Column`, functions defined in
+        Can use methods of :class:`~pyspark.sql.Column`, functions defined in
         :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.
         Python ``UserDefinedFunctions`` are not supported
         (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).
 
     Returns
     -------
-    :class:`pyspark.sql.Column`
+    :class:`~pyspark.sql.Column`
 
     Examples
     --------
@@ -4554,18 +4557,18 @@ def transform_keys(col, f):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         name of column or expression
     f : function
         a binary function ``(k: Column, v: Column) -> Column...``
-        Can use methods of :class:`pyspark.sql.Column`, functions defined in
+        Can use methods of :class:`~pyspark.sql.Column`, functions defined in
         :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.
         Python ``UserDefinedFunctions`` are not supported
         (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).
 
     Returns
     -------
-    :class:`pyspark.sql.Column`
+    :class:`~pyspark.sql.Column`
 
     Examples
     --------
@@ -4591,18 +4594,18 @@ def transform_values(col, f):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         name of column or expression
     f : function
         a binary function ``(k: Column, v: Column) -> Column...``
-        Can use methods of :class:`pyspark.sql.Column`, functions defined in
+        Can use methods of :class:`~pyspark.sql.Column`, functions defined in
         :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.
         Python ``UserDefinedFunctions`` are not supported
         (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).
 
     Returns
     -------
-    :class:`pyspark.sql.Column`
+    :class:`~pyspark.sql.Column`
 
     Examples
     --------
@@ -4627,18 +4630,18 @@ def map_filter(col, f):
 
     Parameters
     ----------
-    col : :class:`Column` or str
+    col : :class:`~pyspark.sql.Column` or str
         name of column or expression
     f : function
         a binary function ``(k: Column, v: Column) -> Column...``
-        Can use methods of :class:`pyspark.sql.Column`, functions defined in
+        Can use methods of :class:`~pyspark.sql.Column`, functions defined in
         :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.
         Python ``UserDefinedFunctions`` are not supported
         (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).
 
     Returns
     -------
-    :class:`pyspark.sql.Column`
+    :class:`~pyspark.sql.Column`
 
     Examples
     --------
@@ -4663,20 +4666,20 @@ def map_zip_with(col1, col2, f):
 
     Parameters
     ----------
-    col1 : :class:`Column` or str
+    col1 : :class:`~pyspark.sql.Column` or str
         name of the first column or expression
-    col2 : :class:`Column` or str
+    col2 : :class:`~pyspark.sql.Column` or str
         name of the second column or expression
     f : function
         a ternary function ``(k: Column, v1: Column, v2: Column) -> Column...``
-        Can use methods of :class:`pyspark.sql.Column`, functions defined in
+        Can use methods of :class:`~pyspark.sql.Column`, functions defined in
         :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.
         Python ``UserDefinedFunctions`` are not supported
         (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).
 
     Returns
     -------
-    :class:`pyspark.sql.Column`
+    :class:`~pyspark.sql.Column`
 
     Examples
     --------
