diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala
index a85f3409c43..184e63179d4 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala
@@ -352,10 +352,19 @@ object PartitioningUtils {
    */
   def getPathFragment(spec: TablePartitionSpec, partitionSchema: StructType): String = {
     partitionSchema.map { field =>
-      escapePathName(field.name) + "=" + getPartitionValueString(spec(field.name))
+      escapePathName(field.name) + "=" +
+        getPartitionValueString(
+          removeLeadingZerosFromNumberTypePartition(spec(field.name), field.dataType))
     }.mkString("/")
   }
 
+  def removeLeadingZerosFromNumberTypePartition(value: String, dataType: DataType): String =
+    dataType match {
+      case ByteType | ShortType | IntegerType | LongType | FloatType | DoubleType =>
+        castPartValueToDesiredType(dataType, value, null).toString
+      case _ => value
+    }
+
   def getPathFragment(spec: TablePartitionSpec, partitionColumns: Seq[Attribute]): String = {
     getPathFragment(spec, StructType.fromAttributes(partitionColumns))
   }
@@ -523,9 +532,9 @@ object PartitioningUtils {
     case _ if value == DEFAULT_PARTITION_NAME => null
     case NullType => null
     case StringType => UTF8String.fromString(unescapePathName(value))
-    case IntegerType => Integer.parseInt(value)
+    case ByteType | ShortType | IntegerType => Integer.parseInt(value)
     case LongType => JLong.parseLong(value)
-    case DoubleType => JDouble.parseDouble(value)
+    case FloatType | DoubleType => JDouble.parseDouble(value)
     case _: DecimalType => Literal(new JBigDecimal(value)).value
     case DateType =>
       Cast(Literal(value), DateType, Some(zoneId.getId)).eval()
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetPartitionDiscoverySuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetPartitionDiscoverySuite.scala
index defb96d6da5..a8507b7dd97 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetPartitionDiscoverySuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetPartitionDiscoverySuite.scala
@@ -1250,6 +1250,13 @@ class ParquetV2PartitionDiscoverySuite extends ParquetPartitionDiscoverySuite {
       .sparkConf
       .set(SQLConf.USE_V1_SOURCE_LIST, "")
 
+  test("SPARK-35561: remove leading zeros from empty static number type partition") {
+    val spec = Map("p_int"-> "010", "p_float"-> "01.00")
+    val schema = new StructType().add("p_int", "int").add("p_float", "float")
+    val path = PartitioningUtils.getPathFragment(spec, schema)
+    assert("p_int=10/p_float=1.0" === path)
+  }
+
   test("read partitioned table - partition key included in Parquet file") {
     withTempDir { base =>
       for {
