diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/CollectMetricsExec.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/CollectMetricsExec.scala
index 0a487bac776..19da27611bf 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/CollectMetricsExec.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/CollectMetricsExec.scala
@@ -21,6 +21,7 @@ import org.apache.spark.rdd.RDD
 import org.apache.spark.sql.Row
 import org.apache.spark.sql.catalyst.{CatalystTypeConverters, InternalRow}
 import org.apache.spark.sql.catalyst.expressions.{Attribute, NamedExpression, SortOrder}
+import org.apache.spark.sql.catalyst.plans.logical.{CollectMetrics, LocalRelation, LogicalPlan}
 import org.apache.spark.sql.catalyst.plans.physical.Partitioning
 import org.apache.spark.sql.catalyst.types.DataTypeUtils
 import org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanHelper
@@ -98,13 +99,30 @@ object CollectMetricsExec extends AdaptiveSparkPlanHelper {
   /**
    * Recursively collect all collected metrics from a query tree.
    */
-  def collect(plan: SparkPlan): Map[String, Row] = {
-    val metrics = collectWithSubqueries(plan) {
+  def collect(physicalPlan: SparkPlan, analyzedOpt: Option[LogicalPlan]): Map[String, Row] = {
+    val collectorsInLogicalPlan = analyzedOpt.map {
+      _.collectWithSubqueries {
+        case c: CollectMetrics => c
+      }
+    }.getOrElse(Seq.empty[CollectMetrics])
+    val metrics = collectWithSubqueries(physicalPlan) {
       case collector: CollectMetricsExec =>
         Map(collector.name -> collector.collectedMetrics)
       case tableScan: InMemoryTableScanExec =>
-        CollectMetricsExec.collect(tableScan.relation.cachedPlan)
+        CollectMetricsExec.collect(
+          tableScan.relation.cachedPlan, tableScan.relation.cachedPlan.logicalLink)
     }
-    metrics.reduceOption(_ ++ _).getOrElse(Map.empty)
+    val metricsCollected = metrics.reduceOption(_ ++ _).getOrElse(Map.empty)
+    val initialMetricsForMissing = collectorsInLogicalPlan.flatMap { collector =>
+      if (!metricsCollected.contains(collector.name)) {
+        val exec = CollectMetricsExec(collector.name, collector.metrics,
+          EmptyRelationExec(LocalRelation(collector.child.output)))
+        val initialMetrics = exec.collectedMetrics
+        Some((collector.name -> initialMetrics))
+      } else {
+        None
+      }
+    }.toMap
+    metricsCollected ++ initialMetricsForMissing
   }
 }
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala
index 6ff2c5d4b9d..1fa2ebce8ab 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala
@@ -243,7 +243,7 @@ class QueryExecution(
   def toRdd: RDD[InternalRow] = lazyToRdd.get
 
   /** Get the metrics observed during the execution of the query plan. */
-  def observedMetrics: Map[String, Row] = CollectMetricsExec.collect(executedPlan)
+  def observedMetrics: Map[String, Row] = CollectMetricsExec.collect(executedPlan, Some(analyzed))
 
   protected def preparations: Seq[Rule[SparkPlan]] = {
     QueryExecution.preparations(sparkSession,
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala
index e3346684285..d123800aa20 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala
@@ -4941,6 +4941,20 @@ class SQLQuerySuite extends QueryTest with SharedSparkSession with AdaptiveSpark
       Row(Array(0), Array(0)), Row(Array(1), Array(1)), Row(Array(2), Array(2)))
     checkAnswer(df, expectedAnswer)
   }
+
+  test("SPARK-50007: default metrics is provided even observe node is pruned out") {
+    val namedObservation = Observation("observation")
+    spark.range(10)
+      .observe(namedObservation, count(lit(1)).as("rows"))
+      // Enforce PruneFilters to come into play and prune subtree. We could do the same
+      // with the reproducer of SPARK-48267, but let's just be simpler.
+      .filter(expr("false"))
+      .collect()
+
+    // This should produce the default value of metrics. Before SPARK-50007, the test fails
+    // because `namedObservation.getOrEmpty` is an empty Map.
+    assert(namedObservation.getOrEmpty.get("rows") === Some(0L))
+  }
 }
 
 case class Foo(bar: Option[String])
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingQueryOptimizationCorrectnessSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingQueryOptimizationCorrectnessSuite.scala
index f651bfb7f3c..641f4668de3 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingQueryOptimizationCorrectnessSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingQueryOptimizationCorrectnessSuite.scala
@@ -586,4 +586,28 @@ class StreamingQueryOptimizationCorrectnessSuite extends StreamTest {
       }
     )
   }
+
+  test("SPARK-50007: default metrics is provided even observe node is pruned out") {
+    // We disable SPARK-49699 to test the case observe node is pruned out.
+    withSQLConf(SQLConf.PRUNE_FILTERS_CAN_PRUNE_STREAMING_SUBPLAN.key -> "true") {
+      val input1 = MemoryStream[Int]
+      val df = input1.toDF()
+        .withColumn("eventTime", timestamp_seconds($"value"))
+        .observe("observation", count(lit(1)).as("rows"))
+        // Enforce PruneFilters to come into play and prune subtree. We could do the same
+        // with the reproducer of SPARK-48267, but let's just be simpler.
+        .filter(expr("false"))
+
+      testStream(df)(
+        AddData(input1, 1, 2, 3),
+        CheckNewAnswer(),
+        Execute { qe =>
+          val observeRow = qe.lastExecution.observedMetrics.get("observation")
+          // This should produce the default value of metrics. Before SPARK-50007, the test fails
+          // because `observeRow` is None. (Spark fails to find the metrics by name.)
+          assert(observeRow.get.getAs[Long]("rows") == 0L)
+        }
+      )
+    }
+  }
 }
