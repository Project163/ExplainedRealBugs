diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala
index 9ae0824987e..32a2aa3909d 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala
@@ -242,6 +242,34 @@ object AnalysisContext {
   }
 }
 
+object Analyzer {
+  // List of configurations that should be passed on when resolving views and SQL UDF.
+  private val RETAINED_ANALYSIS_FLAGS = Seq(
+    // retainedHiveConfigs
+    // TODO: remove these Hive-related configs after the `RelationConversions` is moved to
+    // optimization phase.
+    "spark.sql.hive.convertMetastoreParquet",
+    "spark.sql.hive.convertMetastoreOrc",
+    "spark.sql.hive.convertInsertingPartitionedTable",
+    "spark.sql.hive.convertInsertingUnpartitionedTable",
+    "spark.sql.hive.convertMetastoreCtas",
+    // retainedLoggingConfigs
+    "spark.sql.planChangeLog.level",
+    "spark.sql.expressionTreeChangeLog.level"
+  )
+
+  def retainResolutionConfigsForAnalysis(newConf: SQLConf, existingConf: SQLConf): Unit = {
+    val retainedConfigs = existingConf.getAllConfs.filter { case (key, _) =>
+      // Also apply catalog configs
+      RETAINED_ANALYSIS_FLAGS.contains(key) || key.startsWith("spark.sql.catalog.")
+    }
+
+    retainedConfigs.foreach { case (k, v) =>
+      newConf.settings.put(k, v)
+    }
+  }
+}
+
 /**
  * Provides a logical query plan analyzer, which translates [[UnresolvedAttribute]]s and
  * [[UnresolvedRelation]]s into fully typed objects using information in a [[SessionCatalog]].
@@ -2486,9 +2514,12 @@ class Analyzer(override val catalogManager: CatalogManager) extends RuleExecutor
       val plan = v1SessionCatalog.makeSQLFunctionPlan(f.name, f.function, f.inputs)
       val resolved = SQLFunctionContext.withSQLFunction {
         // Resolve the SQL function plan using its context.
-        val conf = new SQLConf()
-        f.function.getSQLConfigs.foreach { case (k, v) => conf.settings.put(k, v) }
-        SQLConf.withExistingConf(conf) {
+        val newConf = new SQLConf()
+        f.function.getSQLConfigs.foreach { case (k, v) => newConf.settings.put(k, v) }
+        if (conf.getConf(SQLConf.APPLY_SESSION_CONF_OVERRIDES_TO_FUNCTION_RESOLUTION)) {
+          Analyzer.retainResolutionConfigsForAnalysis(newConf = newConf, existingConf = conf)
+        }
+        SQLConf.withExistingConf(newConf) {
           executeSameContext(plan)
         }
       }
@@ -2779,9 +2810,12 @@ class Analyzer(override val catalogManager: CatalogManager) extends RuleExecutor
       _.containsPattern(SQL_TABLE_FUNCTION)) {
       case SQLTableFunction(name, function, inputs, output) =>
         // Resolve the SQL table function plan using its function context.
-        val conf = new SQLConf()
-        function.getSQLConfigs.foreach { case (k, v) => conf.settings.put(k, v) }
-        val resolved = SQLConf.withExistingConf(conf) {
+        val newConf = new SQLConf()
+        function.getSQLConfigs.foreach { case (k, v) => newConf.settings.put(k, v) }
+        if (conf.getConf(SQLConf.APPLY_SESSION_CONF_OVERRIDES_TO_FUNCTION_RESOLUTION)) {
+          Analyzer.retainResolutionConfigsForAnalysis(newConf = newConf, existingConf = conf)
+        }
+        val resolved = SQLConf.withExistingConf(newConf) {
           val plan = v1SessionCatalog.makeSQLTableFunctionPlan(name, function, inputs, output)
           SQLFunctionContext.withSQLFunction {
             executeSameContext(plan)
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala
index d1c6ec08151..e421368dd3e 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala
@@ -18,7 +18,7 @@
 package org.apache.spark.sql.catalyst.plans.logical
 
 import org.apache.spark.sql.catalyst.{AliasIdentifier, InternalRow, SQLConfHelper}
-import org.apache.spark.sql.catalyst.analysis.{AnsiTypeCoercion, MultiInstanceRelation, Resolver, TypeCoercion, TypeCoercionBase, UnresolvedUnaryNode}
+import org.apache.spark.sql.catalyst.analysis.{Analyzer, AnsiTypeCoercion, MultiInstanceRelation, Resolver, TypeCoercion, TypeCoercionBase, UnresolvedUnaryNode}
 import org.apache.spark.sql.catalyst.catalog.{CatalogStorageFormat, CatalogTable}
 import org.apache.spark.sql.catalyst.catalog.CatalogTable.VIEW_STORING_ANALYZED_PLAN
 import org.apache.spark.sql.catalyst.expressions._
@@ -853,33 +853,11 @@ object View {
     // For temporary view, we always use captured sql configs
     if (activeConf.useCurrentSQLConfigsForView && !isTempView) return activeConf
 
-    // We retain below configs from current session because they are not captured by view
-    // as optimization configs but they are still needed during the view resolution.
-    // TODO: remove this `retainedHiveConfigs` after the `RelationConversions` is moved to
-    // optimization phase.
-    val retainedHiveConfigs = Seq(
-      "spark.sql.hive.convertMetastoreParquet",
-      "spark.sql.hive.convertMetastoreOrc",
-      "spark.sql.hive.convertInsertingPartitionedTable",
-      "spark.sql.hive.convertInsertingUnpartitionedTable",
-      "spark.sql.hive.convertMetastoreCtas"
-    )
-
-    val retainedLoggingConfigs = Seq(
-      "spark.sql.planChangeLog.level",
-      "spark.sql.expressionTreeChangeLog.level"
-    )
-
-    val retainedConfigs = activeConf.getAllConfs.filter { case (key, _) =>
-      retainedHiveConfigs.contains(key) || retainedLoggingConfigs.contains(key) || key.startsWith(
-        "spark.sql.catalog."
-      )
-    }
-
     val sqlConf = new SQLConf()
-    for ((k, v) <- configs ++ retainedConfigs) {
+    for ((k, v) <- configs) {
       sqlConf.settings.put(k, v)
     }
+    Analyzer.retainResolutionConfigsForAnalysis(newConf = sqlConf, existingConf = activeConf)
     sqlConf
   }
 }
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
index 3147b44f4dd..478b92de0b8 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
@@ -2035,6 +2035,15 @@ object SQLConf {
       .booleanConf
       .createWithDefault(true)
 
+  val APPLY_SESSION_CONF_OVERRIDES_TO_FUNCTION_RESOLUTION =
+    buildConf("spark.sql.analyzer.sqlFunctionResolution.applyConfOverrides")
+      .internal()
+      .version("4.0.1")
+      .doc("When true, applies the conf overrides for certain feature flags during the " +
+        "resolution of user-defined sql table valued functions, consistent with view resolution.")
+      .booleanConf
+      .createWithDefault(true)
+
   // Whether to retain group by columns or not in GroupedData.agg.
   val DATAFRAME_RETAIN_GROUP_COLUMNS = buildConf("spark.sql.retainGroupColumns")
     .version("1.4.0")
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/analysis/AnalysisConfOverrideSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/analysis/AnalysisConfOverrideSuite.scala
new file mode 100644
index 00000000000..6f2b348e011
--- /dev/null
+++ b/sql/core/src/test/scala/org/apache/spark/sql/analysis/AnalysisConfOverrideSuite.scala
@@ -0,0 +1,202 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.spark.sql.analysis
+
+import org.apache.spark.SparkConf
+import org.apache.spark.sql.SparkSessionExtensions
+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan
+import org.apache.spark.sql.catalyst.rules.Rule
+import org.apache.spark.sql.test.SharedSparkSession
+
+class AnalysisConfOverrideSuite extends SharedSparkSession {
+
+  override protected def sparkConf: SparkConf = {
+    super.sparkConf
+      .set("spark.sql.extensions", "com.databricks.sql.ConfOverrideValidationExtensions")
+  }
+
+  override def beforeAll(): Unit = {
+    super.beforeAll()
+    spark.sql("SELECT id as a FROM range(10)").createTempView("table")
+    spark.sql("SELECT id as a, (id + 1) as b FROM range(10)").createTempView("table2")
+  }
+
+  override def afterAll(): Unit = {
+    spark.catalog.dropTempView("table")
+    spark.catalog.dropTempView("table2")
+    super.afterAll()
+  }
+
+  private def testOverride(testName: String)(f: (String, String) => Unit): Unit = {
+    test(testName) {
+      val key = "spark.sql.catalog.x.y"
+      val value = "true"
+      withSQLConf(key -> value) {
+        f
+      }
+    }
+  }
+
+  testOverride("simple plan") { case (key, value) =>
+    ValidateConfOverrideRule.withConfValidationEnabled(key, value) {
+      spark.sql("SELECT * FROM TaBlE")
+    }
+  }
+
+  testOverride("CTE") { case (key, value) =>
+    ValidateConfOverrideRule.withConfValidationEnabled(key, value) {
+      spark.sql(
+        """WITH cte AS (SELECT * FROM TaBlE)
+          |SELECT * FROM cte
+          |""".stripMargin)
+    }
+  }
+
+  testOverride("Subquery") { case (key, value) =>
+    ValidateConfOverrideRule.withConfValidationEnabled(key, value) {
+      spark.sql(
+        """
+          |SELECT * FROM TaBlE WHERE a in (SELECT a FROM table2)
+          |""".stripMargin)
+    }
+  }
+
+  testOverride("View") { case (key, value) =>
+    withTable("test_table", "test_table2") {
+      spark.sql("CREATE TABLE test_table AS SELECT id as a FROM range(10)")
+      spark.sql("CREATE TABLE test_table2 AS SELECT id as a, (id + 1) as b FROM range(10)")
+      withView("test_view") {
+        spark.sql("CREATE VIEW test_view AS " +
+          "SELECT * FROM test_table WHERE a in (SELECT a FROM test_table2)")
+
+        ValidateConfOverrideRule.withConfValidationEnabled(key, value) {
+          spark.sql("SELECT * FROM test_view")
+        }
+      }
+    }
+  }
+
+  testOverride("user defined SQL functions") { case (key, value) =>
+    withTable("test_table", "test_table2") {
+      spark.sql("CREATE TABLE test_table AS SELECT id as a FROM range(10)")
+      spark.sql("CREATE TABLE test_table2 AS SELECT id as a, (id + 1) as b FROM range(10)")
+      withUserDefinedFunction("f1" -> true, "f2" -> false, "f3" -> false) {
+        spark.sql(
+          """CREATE OR REPLACE TEMPORARY FUNCTION f1() RETURNS TABLE (a bigint)
+            |RETURN SELECT * FROM test_table WHERE a in (SELECT a FROM test_table2)
+            |""".stripMargin
+        )
+        spark.sql(
+          """CREATE OR REPLACE FUNCTION f2() RETURNS TABLE (a bigint)
+            |RETURN SELECT * FROM test_table WHERE a in (SELECT a FROM test_table2)
+            |""".stripMargin
+        )
+        spark.sql(
+          """CREATE OR REPLACE FUNCTION f3(in bigint) RETURNS (out bigint)
+            |RETURN in + 1
+            |""".stripMargin
+        )
+        ValidateConfOverrideRule.withConfValidationEnabled(key, value) {
+          spark.sql("SELECT * FROM f1()")
+        }
+        ValidateConfOverrideRule.withConfValidationEnabled(key, value) {
+          spark.sql("SELECT * FROM f2()")
+        }
+        ValidateConfOverrideRule.withConfValidationEnabled(key, value) {
+          spark.sql("SELECT f3(1)")
+        }
+      }
+    }
+  }
+
+  testOverride("user defined SQL functions - test conf disabled") { case (key, value) =>
+    withTable("test_table", "test_table2") {
+      spark.sql("CREATE TABLE test_table AS SELECT id as a FROM range(10)")
+      spark.sql("CREATE TABLE test_table2 AS SELECT id as a, (id + 1) as b FROM range(10)")
+      // turn the flag off to maintain former behavior
+      withSQLConf("spark.sql.analyzer.sqlFunctionResolution.applyConfOverrides" -> "false") {
+        withUserDefinedFunction("f1" -> true, "f2" -> false, "f3" -> false) {
+          spark.sql(
+            """CREATE OR REPLACE TEMPORARY FUNCTION f1() RETURNS TABLE (a bigint)
+              |RETURN SELECT * FROM test_table WHERE a in (SELECT a FROM test_table2)
+              |""".stripMargin
+          )
+          spark.sql(
+            """CREATE OR REPLACE FUNCTION f2() RETURNS TABLE (a bigint)
+              |RETURN SELECT * FROM test_table WHERE a in (SELECT a FROM test_table2)
+              |""".stripMargin
+          )
+          spark.sql(
+            """CREATE OR REPLACE FUNCTION f3(in bigint) RETURNS (out bigint)
+              |RETURN in + 1
+              |""".stripMargin
+          )
+          intercept[AssertionError] {
+            ValidateConfOverrideRule.withConfValidationEnabled(key, value) {
+              spark.sql("SELECT * FROM f1()")
+            }
+          }
+          intercept[AssertionError] {
+            ValidateConfOverrideRule.withConfValidationEnabled(key, value) {
+              spark.sql("SELECT * FROM f2()")
+            }
+          }
+          intercept[AssertionError] {
+            ValidateConfOverrideRule.withConfValidationEnabled(key, value) {
+              spark.sql("SELECT f3(1)")
+            }
+          }
+        }
+      }
+    }
+  }
+}
+
+/** Utility singleton object to orchestrate the test. */
+object ValidateConfOverrideRule {
+  private var confToCheck: Option[(String, String)] = None
+  private var isCalled: Boolean = false
+
+  def withConfValidationEnabled(key: String, value: String)(f: => Unit): Unit = {
+    try {
+      confToCheck = Some(key -> value)
+      f
+      assert(isCalled, "The rule was enabled, but not called. This is a test setup error.")
+    } finally {
+      isCalled = false
+      confToCheck = None
+    }
+  }
+}
+
+class ValidateConfOverrideRule extends Rule[LogicalPlan] {
+  override def apply(plan: LogicalPlan): LogicalPlan = {
+    ValidateConfOverrideRule.confToCheck.foreach { case (k, v) =>
+      assert(conf.getConfString(k) == v,
+        s"The feature wasn't enabled within plan:\n$plan")
+      ValidateConfOverrideRule.isCalled = true
+    }
+    plan
+  }
+}
+
+class ConfOverrideValidationExtensions extends (SparkSessionExtensions => Unit) {
+  override def apply(extensions: SparkSessionExtensions): Unit = {
+    extensions.injectResolutionRule(_ => new ValidateConfOverrideRule)
+  }
+}
