diff --git a/python/pyspark/ml/base.py b/python/pyspark/ml/base.py
index fabfc3253e6..f1ae1232503 100644
--- a/python/pyspark/ml/base.py
+++ b/python/pyspark/ml/base.py
@@ -68,14 +68,13 @@ class _FitMultipleIterator(object):
 
 
 @inherit_doc
-class Estimator(Params):
+class Estimator(Params, metaclass=ABCMeta):
     """
     Abstract class for estimators that fit models to data.
 
     .. versionadded:: 1.3.0
     """
-
-    __metaclass__ = ABCMeta
+    pass
 
     @abstractmethod
     def _fit(self, dataset):
@@ -134,14 +133,13 @@ class Estimator(Params):
 
 
 @inherit_doc
-class Transformer(Params):
+class Transformer(Params, metaclass=ABCMeta):
     """
     Abstract class for transformers that transform one dataset into another.
 
     .. versionadded:: 1.3.0
     """
-
-    __metaclass__ = ABCMeta
+    pass
 
     @abstractmethod
     def _transform(self, dataset):
@@ -174,14 +172,13 @@ class Transformer(Params):
 
 
 @inherit_doc
-class Model(Transformer):
+class Model(Transformer, metaclass=ABCMeta):
     """
     Abstract class for models that are fitted by estimators.
 
     .. versionadded:: 1.4.0
     """
-
-    __metaclass__ = ABCMeta
+    pass
 
 
 @inherit_doc
@@ -258,13 +255,11 @@ class _PredictorParams(HasLabelCol, HasFeaturesCol, HasPredictionCol):
 
 
 @inherit_doc
-class Predictor(Estimator, _PredictorParams):
+class Predictor(Estimator, _PredictorParams, metaclass=ABCMeta):
     """
     Estimator for prediction tasks (regression and classification).
     """
 
-    __metaclass__ = ABCMeta
-
     @since("3.0.0")
     def setLabelCol(self, value):
         """
@@ -288,13 +283,11 @@ class Predictor(Estimator, _PredictorParams):
 
 
 @inherit_doc
-class PredictionModel(Model, _PredictorParams):
+class PredictionModel(Model, _PredictorParams, metaclass=ABCMeta):
     """
     Model for prediction tasks (regression and classification).
     """
 
-    __metaclass__ = ABCMeta
-
     @since("3.0.0")
     def setFeaturesCol(self, value):
         """
diff --git a/python/pyspark/ml/classification.py b/python/pyspark/ml/classification.py
index 6df42521124..b5261b30d89 100644
--- a/python/pyspark/ml/classification.py
+++ b/python/pyspark/ml/classification.py
@@ -73,14 +73,12 @@ class _ClassifierParams(HasRawPredictionCol, _PredictorParams):
 
 
 @inherit_doc
-class Classifier(Predictor, _ClassifierParams):
+class Classifier(Predictor, _ClassifierParams, metaclass=ABCMeta):
     """
     Classifier for classification tasks.
     Classes are indexed {0, 1, ..., numClasses - 1}.
     """
 
-    __metaclass__ = ABCMeta
-
     @since("3.0.0")
     def setRawPredictionCol(self, value):
         """
@@ -90,14 +88,12 @@ class Classifier(Predictor, _ClassifierParams):
 
 
 @inherit_doc
-class ClassificationModel(PredictionModel, _ClassifierParams):
+class ClassificationModel(PredictionModel, _ClassifierParams, metaclass=ABCMeta):
     """
     Model produced by a ``Classifier``.
     Classes are indexed {0, 1, ..., numClasses - 1}.
     """
 
-    __metaclass__ = ABCMeta
-
     @since("3.0.0")
     def setRawPredictionCol(self, value):
         """
@@ -133,13 +129,12 @@ class _ProbabilisticClassifierParams(HasProbabilityCol, HasThresholds, _Classifi
 
 
 @inherit_doc
-class ProbabilisticClassifier(Classifier, _ProbabilisticClassifierParams):
+class ProbabilisticClassifier(Classifier, _ProbabilisticClassifierParams,
+                              metaclass=ABCMeta):
     """
     Probabilistic Classifier for classification tasks.
     """
 
-    __metaclass__ = ABCMeta
-
     @since("3.0.0")
     def setProbabilityCol(self, value):
         """
@@ -157,13 +152,12 @@ class ProbabilisticClassifier(Classifier, _ProbabilisticClassifierParams):
 
 @inherit_doc
 class ProbabilisticClassificationModel(ClassificationModel,
-                                       _ProbabilisticClassifierParams):
+                                       _ProbabilisticClassifierParams,
+                                       metaclass=ABCMeta):
     """
     Model produced by a ``ProbabilisticClassifier``.
     """
 
-    __metaclass__ = ABCMeta
-
     @since("3.0.0")
     def setProbabilityCol(self, value):
         """
@@ -188,14 +182,12 @@ class ProbabilisticClassificationModel(ClassificationModel,
 
 
 @inherit_doc
-class _JavaClassifier(Classifier, JavaPredictor):
+class _JavaClassifier(Classifier, JavaPredictor, metaclass=ABCMeta):
     """
     Java Classifier for classification tasks.
     Classes are indexed {0, 1, ..., numClasses - 1}.
     """
 
-    __metaclass__ = ABCMeta
-
     @since("3.0.0")
     def setRawPredictionCol(self, value):
         """
@@ -229,12 +221,12 @@ class _JavaClassificationModel(ClassificationModel, JavaPredictionModel):
 
 
 @inherit_doc
-class _JavaProbabilisticClassifier(ProbabilisticClassifier, _JavaClassifier):
+class _JavaProbabilisticClassifier(ProbabilisticClassifier, _JavaClassifier,
+                                   metaclass=ABCMeta):
     """
     Java Probabilistic Classifier for classification tasks.
     """
-
-    __metaclass__ = ABCMeta
+    pass
 
 
 @inherit_doc
diff --git a/python/pyspark/ml/evaluation.py b/python/pyspark/ml/evaluation.py
index a69a57f5885..354921e9e04 100644
--- a/python/pyspark/ml/evaluation.py
+++ b/python/pyspark/ml/evaluation.py
@@ -32,14 +32,13 @@ __all__ = ['Evaluator', 'BinaryClassificationEvaluator', 'RegressionEvaluator',
 
 
 @inherit_doc
-class Evaluator(Params):
+class Evaluator(Params, metaclass=ABCMeta):
     """
     Base class for evaluators that compute metrics from predictions.
 
     .. versionadded:: 1.4.0
     """
-
-    __metaclass__ = ABCMeta
+    pass
 
     @abstractmethod
     def _evaluate(self, dataset):
@@ -84,14 +83,12 @@ class Evaluator(Params):
 
 
 @inherit_doc
-class JavaEvaluator(JavaParams, Evaluator):
+class JavaEvaluator(JavaParams, Evaluator, metaclass=ABCMeta):
     """
     Base class for :py:class:`Evaluator`s that wrap Java/Scala
     implementations.
     """
 
-    __metaclass__ = ABCMeta
-
     def _evaluate(self, dataset):
         """
         Evaluates the output.
diff --git a/python/pyspark/ml/param/__init__.py b/python/pyspark/ml/param/__init__.py
index 95f3c32b8bc..1853a8816ff 100644
--- a/python/pyspark/ml/param/__init__.py
+++ b/python/pyspark/ml/param/__init__.py
@@ -223,7 +223,7 @@ class TypeConverters(object):
             raise TypeError("Boolean Param requires value of type bool. Found %s." % type(value))
 
 
-class Params(Identifiable):
+class Params(Identifiable, metaclass=ABCMeta):
     """
     Components that take parameters. This also provides an internal
     param map to store parameter values attached to the instance.
@@ -231,8 +231,6 @@ class Params(Identifiable):
     .. versionadded:: 1.3.0
     """
 
-    __metaclass__ = ABCMeta
-
     def __init__(self):
         super(Params, self).__init__()
         #: internal param map for user-supplied values param map
diff --git a/python/pyspark/ml/regression.py b/python/pyspark/ml/regression.py
index 6bd32ed1d63..e1b7ffb63f8 100644
--- a/python/pyspark/ml/regression.py
+++ b/python/pyspark/ml/regression.py
@@ -48,45 +48,41 @@ __all__ = ['AFTSurvivalRegression', 'AFTSurvivalRegressionModel',
            'FMRegressor', 'FMRegressionModel']
 
 
-class Regressor(Predictor, _PredictorParams):
+class Regressor(Predictor, _PredictorParams, metaclass=ABCMeta):
     """
     Regressor for regression tasks.
 
     .. versionadded:: 3.0.0
     """
+    pass
 
-    __metaclass__ = ABCMeta
 
-
-class RegressionModel(PredictionModel, _PredictorParams):
+class RegressionModel(PredictionModel, _PredictorParams, metaclass=ABCMeta):
     """
     Model produced by a ``Regressor``.
 
     .. versionadded:: 3.0.0
     """
-
-    __metaclass__ = ABCMeta
+    pass
 
 
-class _JavaRegressor(Regressor, JavaPredictor):
+class _JavaRegressor(Regressor, JavaPredictor, metaclass=ABCMeta):
     """
     Java Regressor for regression tasks.
 
     .. versionadded:: 3.0.0
     """
+    pass
 
-    __metaclass__ = ABCMeta
 
-
-class _JavaRegressionModel(RegressionModel, JavaPredictionModel):
+class _JavaRegressionModel(RegressionModel, JavaPredictionModel, metaclass=ABCMeta):
     """
     Java Model produced by a ``_JavaRegressor``.
     To be mixed in with :class:`pyspark.ml.JavaModel`
 
     .. versionadded:: 3.0.0
     """
-
-    __metaclass__ = ABCMeta
+    pass
 
 
 class _LinearRegressionParams(_PredictorParams, HasRegParam, HasElasticNetParam, HasMaxIter,
diff --git a/python/pyspark/ml/wrapper.py b/python/pyspark/ml/wrapper.py
index c1d060a51cf..da52788afea 100644
--- a/python/pyspark/ml/wrapper.py
+++ b/python/pyspark/ml/wrapper.py
@@ -109,7 +109,7 @@ class JavaWrapper(object):
 
 
 @inherit_doc
-class JavaParams(JavaWrapper, Params):
+class JavaParams(JavaWrapper, Params, metaclass=ABCMeta):
     """
     Utility class to help create wrapper classes from Java/Scala
     implementations of pipeline components.
@@ -117,8 +117,6 @@ class JavaParams(JavaWrapper, Params):
     #: The param values in the Java object should be
     #: synced with the Python wrapper in fit/transform/evaluate/copy.
 
-    __metaclass__ = ABCMeta
-
     def _make_java_param_pair(self, param, value):
         """
         Makes a Java param pair.
@@ -287,14 +285,12 @@ class JavaParams(JavaWrapper, Params):
 
 
 @inherit_doc
-class JavaEstimator(JavaParams, Estimator):
+class JavaEstimator(JavaParams, Estimator, metaclass=ABCMeta):
     """
     Base class for :py:class:`Estimator`s that wrap Java/Scala
     implementations.
     """
 
-    __metaclass__ = ABCMeta
-
     @abstractmethod
     def _create_model(self, java_model):
         """
@@ -321,30 +317,26 @@ class JavaEstimator(JavaParams, Estimator):
 
 
 @inherit_doc
-class JavaTransformer(JavaParams, Transformer):
+class JavaTransformer(JavaParams, Transformer, metaclass=ABCMeta):
     """
     Base class for :py:class:`Transformer`s that wrap Java/Scala
     implementations. Subclasses should ensure they have the transformer Java object
     available as _java_obj.
     """
 
-    __metaclass__ = ABCMeta
-
     def _transform(self, dataset):
         self._transfer_params_to_java()
         return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)
 
 
 @inherit_doc
-class JavaModel(JavaTransformer, Model):
+class JavaModel(JavaTransformer, Model, metaclass=ABCMeta):
     """
     Base class for :py:class:`Model`s that wrap Java/Scala
     implementations. Subclasses should inherit this class before
     param mix-ins, because this sets the UID from the Java model.
     """
 
-    __metaclass__ = ABCMeta
-
     def __init__(self, java_model=None):
         """
         Initialize this instance with a Java model object.
@@ -374,12 +366,11 @@ class JavaModel(JavaTransformer, Model):
 
 
 @inherit_doc
-class JavaPredictor(Predictor, JavaEstimator, _PredictorParams):
+class JavaPredictor(Predictor, JavaEstimator, _PredictorParams, metaclass=ABCMeta):
     """
     (Private) Java Estimator for prediction tasks (regression and classification).
     """
-
-    __metaclass__ = ABCMeta
+    pass
 
 
 @inherit_doc
diff --git a/python/pyspark/sql/types.py b/python/pyspark/sql/types.py
index 43f3a853187..5a89d5ab9a7 100644
--- a/python/pyspark/sql/types.py
+++ b/python/pyspark/sql/types.py
@@ -102,13 +102,12 @@ class DataTypeSingleton(type):
         return cls._instances[cls]
 
 
-class NullType(DataType):
+class NullType(DataType, metaclass=DataTypeSingleton):
     """Null type.
 
     The data type representing None, used for the types that cannot be inferred.
     """
-
-    __metaclass__ = DataTypeSingleton
+    pass
 
 
 class AtomicType(DataType):
@@ -121,11 +120,10 @@ class NumericType(AtomicType):
     """
 
 
-class IntegralType(NumericType):
+class IntegralType(NumericType, metaclass=DataTypeSingleton):
     """Integral data types.
     """
-
-    __metaclass__ = DataTypeSingleton
+    pass
 
 
 class FractionalType(NumericType):
@@ -133,33 +131,28 @@ class FractionalType(NumericType):
     """
 
 
-class StringType(AtomicType):
+class StringType(AtomicType, metaclass=DataTypeSingleton):
     """String data type.
     """
-
-    __metaclass__ = DataTypeSingleton
+    pass
 
 
-class BinaryType(AtomicType):
+class BinaryType(AtomicType, metaclass=DataTypeSingleton):
     """Binary (byte array) data type.
     """
+    pass
 
-    __metaclass__ = DataTypeSingleton
 
-
-class BooleanType(AtomicType):
+class BooleanType(AtomicType, metaclass=DataTypeSingleton):
     """Boolean data type.
     """
-
-    __metaclass__ = DataTypeSingleton
+    pass
 
 
-class DateType(AtomicType):
+class DateType(AtomicType, metaclass=DataTypeSingleton):
     """Date (datetime.date) data type.
     """
 
-    __metaclass__ = DataTypeSingleton
-
     EPOCH_ORDINAL = datetime.datetime(1970, 1, 1).toordinal()
 
     def needConversion(self):
@@ -174,12 +167,10 @@ class DateType(AtomicType):
             return datetime.date.fromordinal(v + self.EPOCH_ORDINAL)
 
 
-class TimestampType(AtomicType):
+class TimestampType(AtomicType, metaclass=DataTypeSingleton):
     """Timestamp (datetime.datetime) data type.
     """
 
-    __metaclass__ = DataTypeSingleton
-
     def needConversion(self):
         return True
 
@@ -226,18 +217,16 @@ class DecimalType(FractionalType):
         return "DecimalType(%d,%d)" % (self.precision, self.scale)
 
 
-class DoubleType(FractionalType):
+class DoubleType(FractionalType, metaclass=DataTypeSingleton):
     """Double data type, representing double precision floats.
     """
+    pass
 
-    __metaclass__ = DataTypeSingleton
 
-
-class FloatType(FractionalType):
+class FloatType(FractionalType, metaclass=DataTypeSingleton):
     """Float data type, representing single precision floats.
     """
-
-    __metaclass__ = DataTypeSingleton
+    pass
 
 
 class ByteType(IntegralType):
