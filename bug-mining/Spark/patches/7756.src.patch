diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala
index 1d5a884d6e1..829025f3dc9 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala
@@ -28,6 +28,7 @@ import org.apache.spark.rdd.RDD
 import org.apache.spark.sql.{AnalysisException, Row, SparkSession}
 import org.apache.spark.sql.catalyst.{InternalRow, QueryPlanningTracker}
 import org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker
+import org.apache.spark.sql.catalyst.expressions.SubqueryExpression
 import org.apache.spark.sql.catalyst.expressions.codegen.ByteCodeStats
 import org.apache.spark.sql.catalyst.plans.QueryPlan
 import org.apache.spark.sql.catalyst.plans.logical.{LogicalPlan, ReturnAnswer}
@@ -253,6 +254,12 @@ class QueryExecution(
 
     // trigger to compute stats for logical plans
     try {
+      optimizedPlan.foreach(_.expressions.foreach(_.foreach {
+        case subqueryExpression: SubqueryExpression =>
+          // trigger subquery's child plan stats propagation
+          subqueryExpression.plan.stats
+        case _ =>
+      }))
       optimizedPlan.stats
     } catch {
       case e: AnalysisException => append(e.toString + "\n")
diff --git a/sql/core/src/test/resources/sql-tests/inputs/explain-cbo.sql b/sql/core/src/test/resources/sql-tests/inputs/explain-cbo.sql
new file mode 100644
index 00000000000..eeb2180f7a5
--- /dev/null
+++ b/sql/core/src/test/resources/sql-tests/inputs/explain-cbo.sql
@@ -0,0 +1,27 @@
+--SET spark.sql.cbo.enabled=true
+--SET spark.sql.maxMetadataStringLength = 500
+
+CREATE TABLE explain_temp1(a INT, b INT) USING PARQUET;
+CREATE TABLE explain_temp2(c INT, d INT) USING PARQUET;
+
+ANALYZE TABLE explain_temp1 COMPUTE STATISTICS FOR ALL COLUMNS;
+ANALYZE TABLE explain_temp2 COMPUTE STATISTICS FOR ALL COLUMNS;
+
+EXPLAIN COST WITH max_store_sales AS
+(
+  SELECT max(csales) tpcds_cmax
+  FROM (
+    SELECT sum(b) csales
+    FROM explain_temp1 WHERE a < 100
+  ) x
+),
+best_ss_customer AS
+(
+  SELECT c
+  FROM explain_temp2
+  WHERE d > (SELECT * FROM max_store_sales)
+)
+SELECT c FROM best_ss_customer;
+
+DROP TABLE explain_temp1;
+DROP TABLE explain_temp2;
diff --git a/sql/core/src/test/resources/sql-tests/results/explain-cbo.sql.out b/sql/core/src/test/resources/sql-tests/results/explain-cbo.sql.out
new file mode 100644
index 00000000000..e4d40fc38f1
--- /dev/null
+++ b/sql/core/src/test/resources/sql-tests/results/explain-cbo.sql.out
@@ -0,0 +1,96 @@
+-- Automatically generated by SQLQueryTestSuite
+-- Number of queries: 7
+
+
+-- !query
+CREATE TABLE explain_temp1(a INT, b INT) USING PARQUET
+-- !query schema
+struct<>
+-- !query output
+
+
+
+-- !query
+CREATE TABLE explain_temp2(c INT, d INT) USING PARQUET
+-- !query schema
+struct<>
+-- !query output
+
+
+
+-- !query
+ANALYZE TABLE explain_temp1 COMPUTE STATISTICS FOR ALL COLUMNS
+-- !query schema
+struct<>
+-- !query output
+
+
+
+-- !query
+ANALYZE TABLE explain_temp2 COMPUTE STATISTICS FOR ALL COLUMNS
+-- !query schema
+struct<>
+-- !query output
+
+
+
+-- !query
+EXPLAIN COST WITH max_store_sales AS
+(
+  SELECT max(csales) tpcds_cmax
+  FROM (
+    SELECT sum(b) csales
+    FROM explain_temp1 WHERE a < 100
+  ) x
+),
+best_ss_customer AS
+(
+  SELECT c
+  FROM explain_temp2
+  WHERE d > (SELECT * FROM max_store_sales)
+)
+SELECT c FROM best_ss_customer
+-- !query schema
+struct<plan:string>
+-- !query output
+== Optimized Logical Plan ==
+Project [c#x], Statistics(sizeInBytes=1.0 B, rowCount=0)
++- Filter (isnotnull(d#x) AND (cast(d#x as bigint) > scalar-subquery#x [])), Statistics(sizeInBytes=1.0 B, rowCount=0)
+   :  +- Aggregate [max(csales#xL) AS tpcds_cmax#xL], Statistics(sizeInBytes=16.0 B, rowCount=1)
+   :     +- Aggregate [sum(b#x) AS csales#xL], Statistics(sizeInBytes=16.0 B, rowCount=1)
+   :        +- Project [b#x], Statistics(sizeInBytes=1.0 B, rowCount=0)
+   :           +- Filter (isnotnull(a#x) AND (a#x < 100)), Statistics(sizeInBytes=1.0 B, rowCount=0)
+   :              +- Relation[a#x,b#x] parquet, Statistics(sizeInBytes=1.0 B, rowCount=0)
+   +- Relation[c#x,d#x] parquet, Statistics(sizeInBytes=1.0 B, rowCount=0)
+
+== Physical Plan ==
+AdaptiveSparkPlan isFinalPlan=false
++- Project [c#x]
+   +- Filter (isnotnull(d#x) AND (cast(d#x as bigint) > Subquery subquery#x, [id=#x]))
+      :  +- Subquery subquery#x, [id=#x]
+      :     +- AdaptiveSparkPlan isFinalPlan=false
+      :        +- HashAggregate(keys=[], functions=[max(csales#xL)], output=[tpcds_cmax#xL])
+      :           +- HashAggregate(keys=[], functions=[partial_max(csales#xL)], output=[max#xL])
+      :              +- HashAggregate(keys=[], functions=[sum(b#x)], output=[csales#xL])
+      :                 +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#x]
+      :                    +- HashAggregate(keys=[], functions=[partial_sum(b#x)], output=[sum#xL])
+      :                       +- Project [b#x]
+      :                          +- Filter (isnotnull(a#x) AND (a#x < 100))
+      :                             +- FileScan parquet default.explain_temp1[a#x,b#x] Batched: true, DataFilters: [isnotnull(a#x), (a#x < 100)], Format: Parquet, Location [not included in comparison]/{warehouse_dir}/explain_temp1], PartitionFilters: [], PushedFilters: [IsNotNull(a), LessThan(a,100)], ReadSchema: struct<a:int,b:int>
+      +- FileScan parquet default.explain_temp2[c#x,d#x] Batched: true, DataFilters: [isnotnull(d#x)], Format: Parquet, Location [not included in comparison]/{warehouse_dir}/explain_temp2], PartitionFilters: [], PushedFilters: [IsNotNull(d)], ReadSchema: struct<c:int,d:int>
+
+
+-- !query
+DROP TABLE explain_temp1
+-- !query schema
+struct<>
+-- !query output
+
+
+
+-- !query
+DROP TABLE explain_temp2
+-- !query schema
+struct<>
+-- !query output
+
