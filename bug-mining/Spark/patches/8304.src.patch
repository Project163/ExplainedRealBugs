diff --git a/python/pyspark/sql/utils.py b/python/pyspark/sql/utils.py
index 60c138d7bf3..30f22728231 100644
--- a/python/pyspark/sql/utils.py
+++ b/python/pyspark/sql/utils.py
@@ -27,6 +27,7 @@ from py4j.java_gateway import (  # type: ignore[import]
 from py4j.protocol import Py4JJavaError  # type: ignore[import]
 
 from pyspark import SparkContext
+from pyspark.find_spark_home import _find_spark_home
 
 if TYPE_CHECKING:
     from pyspark.sql.context import SQLContext
@@ -245,12 +246,7 @@ def require_test_compiled() -> None:
     import os
     import glob
 
-    try:
-        spark_home = os.environ["SPARK_HOME"]
-    except KeyError:
-        raise RuntimeError("SPARK_HOME is not defined in environment")
-
-    test_class_path = os.path.join(spark_home, "sql", "core", "target", "*", "test-classes")
+    test_class_path = os.path.join(_find_spark_home(), "sql", "core", "target", "*", "test-classes")
     paths = glob.glob(test_class_path)
 
     if len(paths) == 0:
diff --git a/python/pyspark/testing/utils.py b/python/pyspark/testing/utils.py
index b97bfe6b16b..bfa07dc9459 100644
--- a/python/pyspark/testing/utils.py
+++ b/python/pyspark/testing/utils.py
@@ -23,6 +23,7 @@ import unittest
 from time import time, sleep
 
 from pyspark import SparkContext, SparkConf
+from pyspark.find_spark_home import _find_spark_home
 
 
 have_scipy = False
@@ -43,7 +44,7 @@ except ImportError:
     pass
 
 
-SPARK_HOME = os.environ["SPARK_HOME"]
+SPARK_HOME = _find_spark_home()
 
 
 def read_int(b):
@@ -152,7 +153,7 @@ class ByteArrayOutput:
 def search_jar(project_relative_path, sbt_jar_name_prefix, mvn_jar_name_prefix):
     # Note that 'sbt_jar_name_prefix' and 'mvn_jar_name_prefix' are used since the prefix can
     # vary for SBT or Maven specifically. See also SPARK-26856
-    project_full_path = os.path.join(os.environ["SPARK_HOME"], project_relative_path)
+    project_full_path = os.path.join(SPARK_HOME, project_relative_path)
 
     # We should ignore the following jars
     ignored_jar_suffixes = ("javadoc.jar", "sources.jar", "test-sources.jar", "tests.jar")
diff --git a/python/pyspark/tests/test_appsubmit.py b/python/pyspark/tests/test_appsubmit.py
index d43c4ec0d7e..ff9f3e4f160 100644
--- a/python/pyspark/tests/test_appsubmit.py
+++ b/python/pyspark/tests/test_appsubmit.py
@@ -23,13 +23,15 @@ import tempfile
 import unittest
 import zipfile
 
+from pyspark.testing.utils import SPARK_HOME
+
 
 class SparkSubmitTests(unittest.TestCase):
     def setUp(self):
         self.programDir = tempfile.mkdtemp()
         tmp_dir = tempfile.gettempdir()
         self.sparkSubmit = [
-            os.path.join(os.environ.get("SPARK_HOME"), "bin", "spark-submit"),
+            os.path.join(SPARK_HOME, "bin", "spark-submit"),
             "--conf",
             "spark.driver.extraJavaOptions=-Djava.io.tmpdir={0}".format(tmp_dir),
             "--conf",
