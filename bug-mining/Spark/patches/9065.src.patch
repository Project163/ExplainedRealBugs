diff --git a/connector/connect/client/jvm/src/test/scala/org/apache/spark/sql/connect/client/util/IntegrationTestUtils.scala b/connector/connect/client/jvm/src/test/scala/org/apache/spark/sql/connect/client/util/IntegrationTestUtils.scala
index 7e34726b48e..1c17a3fc36b 100644
--- a/connector/connect/client/jvm/src/test/scala/org/apache/spark/sql/connect/client/util/IntegrationTestUtils.scala
+++ b/connector/connect/client/jvm/src/test/scala/org/apache/spark/sql/connect/client/util/IntegrationTestUtils.scala
@@ -23,6 +23,8 @@ import scala.util.Properties.versionNumberString
 
 import org.scalatest.Assertions.fail
 
+import org.apache.spark.util.Utils
+
 object IntegrationTestUtils {
 
   // System properties used for testing and debugging
@@ -77,6 +79,16 @@ object IntegrationTestUtils {
     Files.exists(Paths.get(filePath))
   }
 
+  private[sql] def cleanUpHiveClassesDirIfNeeded(): Unit = {
+    def delete(f: File): Unit = {
+      if (f.exists()) {
+        Utils.deleteRecursively(f)
+      }
+    }
+    delete(new File(s"$sparkHome/sql/hive/target/$scalaDir/classes"))
+    delete(new File(s"$sparkHome/sql/hive/target/$scalaDir/test-classes"))
+  }
+
   /**
    * Find a jar in the Spark project artifacts. It requires a build first (e.g. build/sbt package,
    * build/mvn clean install -DskipTests) so that this method can find the jar in the target
diff --git a/connector/connect/client/jvm/src/test/scala/org/apache/spark/sql/connect/client/util/RemoteSparkSession.scala b/connector/connect/client/jvm/src/test/scala/org/apache/spark/sql/connect/client/util/RemoteSparkSession.scala
index b9edf9ac1a5..e05828606d0 100644
--- a/connector/connect/client/jvm/src/test/scala/org/apache/spark/sql/connect/client/util/RemoteSparkSession.scala
+++ b/connector/connect/client/jvm/src/test/scala/org/apache/spark/sql/connect/client/util/RemoteSparkSession.scala
@@ -117,6 +117,10 @@ object SparkConnectServerUtils {
             "1. Test with maven: run `build/mvn install -DskipTests -Phive` before testing\n" +
             "2. Test with sbt: run test with `-Phive` profile")
         // scalastyle:on println
+        // SPARK-43647: Proactively cleaning the `classes` and `test-classes` dir of hive
+        // module to avoid unexpected loading of `DataSourceRegister` in hive module during
+        // testing without `-Phive` profile.
+        IntegrationTestUtils.cleanUpHiveClassesDirIfNeeded()
         "in-memory"
       }
       Seq("--conf", s"spark.sql.catalogImplementation=$catalogImplementation")
