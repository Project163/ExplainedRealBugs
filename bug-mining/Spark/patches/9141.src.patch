diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/QueryPlanningTracker.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/QueryPlanningTracker.scala
index 35551d8ba77..c42bf7e9aed 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/QueryPlanningTracker.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/QueryPlanningTracker.scala
@@ -19,6 +19,7 @@ package org.apache.spark.sql.catalyst
 
 import scala.collection.JavaConverters._
 
+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan
 import org.apache.spark.util.BoundedPriorityQueue
 
 
@@ -89,8 +90,32 @@ object QueryPlanningTracker {
   }
 }
 
+/**
+ * Callbacks after planning phase completion.
+ */
+abstract class QueryPlanningTrackerCallback {
+  /**
+   * Called when query has been analyzed.
+   *
+   * @param tracker tracker that triggered the callback.
+   * @param analyzedPlan The plan after analysis,
+   *                     see @org.apache.spark.sql.catalyst.analysis.Analyzer
+   */
+  def analyzed(tracker: QueryPlanningTracker, analyzedPlan: LogicalPlan): Unit
 
-class QueryPlanningTracker {
+  /**
+   * Called when query is ready for execution.
+   * This is after analysis for eager commands and after planning for other queries.
+   * @param tracker tracker that triggered the callback.
+   */
+  def readyForExecution(tracker: QueryPlanningTracker): Unit
+}
+
+/**
+ * @param trackerCallback Callback to be notified of planning phase completion.
+ */
+class QueryPlanningTracker(
+    trackerCallback: Option[QueryPlanningTrackerCallback] = None) {
 
   import QueryPlanningTracker._
 
@@ -101,6 +126,8 @@ class QueryPlanningTracker {
   // From a phase to its start time and end time, in ms.
   private val phasesMap = new java.util.HashMap[String, PhaseSummary]
 
+  private var readyForExecution = false
+
   /**
    * Measure the start and end time of a phase. Note that if this function is called multiple
    * times for the same phase, the recorded start time will be the start time of the first call,
@@ -120,6 +147,29 @@ class QueryPlanningTracker {
     ret
   }
 
+  /**
+   * Set when the query has been analysed.
+   * Can be called multiple times upon plan change.
+   * @param analyzedPlan The plan after analysis,
+   *                     see @org.apache.spark.sql.catalyst.analysis.Analyzer
+   */
+  private[sql] def setAnalyzed(analyzedPlan: LogicalPlan): Unit = {
+    trackerCallback.foreach(_.analyzed(this, analyzedPlan))
+  }
+
+  /**
+   * Set when the query is ready for execution. This is after analysis for
+   * eager commands and after planning for other queries.
+   * see @link org.apache.spark.sql.execution.CommandExecutionMode
+   * When called multiple times, ignores subsequent call.
+   */
+  private[sql] def setReadyForExecution(): Unit = {
+    if (!readyForExecution) {
+      readyForExecution = true
+      trackerCallback.foreach(_.readyForExecution(this))
+    }
+  }
+
   /**
    * Record a specific invocation of a rule.
    *
diff --git a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/QueryPlanningTrackerSuite.scala b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/QueryPlanningTrackerSuite.scala
index 9593a720e42..972b98780bc 100644
--- a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/QueryPlanningTrackerSuite.scala
+++ b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/QueryPlanningTrackerSuite.scala
@@ -17,7 +17,11 @@
 
 package org.apache.spark.sql.catalyst
 
+import org.mockito.Mockito.{times, verify}
+import org.scalatestplus.mockito.MockitoSugar.mock
+
 import org.apache.spark.SparkFunSuite
+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan
 
 class QueryPlanningTrackerSuite extends SparkFunSuite {
 
@@ -86,4 +90,19 @@ class QueryPlanningTrackerSuite extends SparkFunSuite {
     // k > total size
     assert(t.topRulesByTime(10).size == 4)
   }
+
+  test("test ready for execution callback") {
+    val mockCallback = mock[QueryPlanningTrackerCallback]
+    val mockPlan1 = mock[LogicalPlan]
+    val mockPlan2 = mock[LogicalPlan]
+    val t = new QueryPlanningTracker(Some(mockCallback))
+    t.setAnalyzed(mockPlan1)
+    verify(mockCallback, times(1)).analyzed(t, mockPlan1)
+    t.setAnalyzed(mockPlan2)
+    verify(mockCallback, times(1)).analyzed(t, mockPlan2)
+    t.setReadyForExecution()
+    verify(mockCallback, times(1)).readyForExecution(t)
+    t.setReadyForExecution()
+    verify(mockCallback, times(1)).readyForExecution(t)
+  }
 }
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala b/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala
index 34b7c21db1a..4de6b944bc8 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala
@@ -32,6 +32,7 @@ import org.apache.spark.sql.connector.catalog.{CatalogPlugin, CatalogV2Implicits
 import org.apache.spark.sql.connector.catalog.TableCapability._
 import org.apache.spark.sql.connector.expressions.{FieldReference, IdentityTransform, Transform}
 import org.apache.spark.sql.errors.QueryCompilationErrors
+import org.apache.spark.sql.execution.QueryExecution
 import org.apache.spark.sql.execution.command.DDLUtils
 import org.apache.spark.sql.execution.datasources.{CreateTable, DataSource, DataSourceUtils, LogicalRelation}
 import org.apache.spark.sql.execution.datasources.v2._
@@ -854,7 +855,7 @@ final class DataFrameWriter[T] private[sql](ds: Dataset[T]) {
    * user-registered callback functions.
    */
   private def runCommand(session: SparkSession)(command: LogicalPlan): Unit = {
-    val qe = session.sessionState.executePlan(command)
+    val qe = new QueryExecution(session, command, df.queryExecution.tracker)
     qe.assertCommandExecuted()
   }
 
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriterV2.scala b/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriterV2.scala
index 6202fede568..7ca9c7ef71d 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriterV2.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriterV2.scala
@@ -26,6 +26,7 @@ import org.apache.spark.sql.catalyst.expressions.{Attribute, Bucket, Days, Hours
 import org.apache.spark.sql.catalyst.plans.logical.{AppendData, CreateTableAsSelect, LogicalPlan, OptionList, OverwriteByExpression, OverwritePartitionsDynamic, ReplaceTableAsSelect, UnresolvedTableSpec}
 import org.apache.spark.sql.connector.expressions.{LogicalExpressions, NamedReference, Transform}
 import org.apache.spark.sql.errors.QueryCompilationErrors
+import org.apache.spark.sql.execution.QueryExecution
 import org.apache.spark.sql.types.IntegerType
 
 /**
@@ -191,7 +192,7 @@ final class DataFrameWriterV2[T] private[sql](table: String, ds: Dataset[T])
    * callback functions.
    */
   private def runCommand(command: LogicalPlan): Unit = {
-    val qe = sparkSession.sessionState.executePlan(command)
+    val qe = new QueryExecution(sparkSession, command, df.queryExecution.tracker)
     qe.assertCommandExecuted()
   }
 
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala b/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala
index 87093c5c906..1cf122eaa68 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala
@@ -610,6 +610,34 @@ class SparkSession private(
    |  Everything else  |
    * ----------------- */
 
+  /**
+   * Executes a SQL query substituting positional parameters by the given arguments,
+   * returning the result as a `DataFrame`.
+   * This API eagerly runs DDL/DML commands, but not for SELECT queries.
+   *
+   * @param sqlText A SQL statement with positional parameters to execute.
+   * @param args An array of Java/Scala objects that can be converted to
+   *             SQL literal expressions. See
+   *             <a href="https://spark.apache.org/docs/latest/sql-ref-datatypes.html">
+   *             Supported Data Types</a> for supported value types in Scala/Java.
+   *             For example, 1, "Steven", LocalDate.of(2023, 4, 2).
+   *             A value can be also a `Column` of literal expression, in that case
+   *             it is taken as is.
+   * @param tracker A tracker that can notify when query is ready for execution
+   */
+  private[sql] def sql(sqlText: String, args: Array[_], tracker: QueryPlanningTracker): DataFrame =
+    withActive {
+      val plan = tracker.measurePhase(QueryPlanningTracker.PARSING) {
+        val parsedPlan = sessionState.sqlParser.parsePlan(sqlText)
+        if (args.nonEmpty) {
+          PosParameterizedQuery(parsedPlan, args.map(lit(_).expr))
+        } else {
+          parsedPlan
+        }
+      }
+      Dataset.ofRows(self, plan, tracker)
+    }
+
   /**
    * Executes a SQL query substituting positional parameters by the given arguments,
    * returning the result as a `DataFrame`.
@@ -627,18 +655,41 @@ class SparkSession private(
    * @since 3.5.0
    */
   @Experimental
-  def sql(sqlText: String, args: Array[_]): DataFrame = withActive {
-    val tracker = new QueryPlanningTracker
-    val plan = tracker.measurePhase(QueryPlanningTracker.PARSING) {
-      val parsedPlan = sessionState.sqlParser.parsePlan(sqlText)
-      if (args.nonEmpty) {
-        PosParameterizedQuery(parsedPlan, args.map(lit(_).expr))
-      } else {
-        parsedPlan
+  def sql(sqlText: String, args: Array[_]): DataFrame = {
+    sql(sqlText, args, new QueryPlanningTracker)
+  }
+
+  /**
+   * Executes a SQL query substituting named parameters by the given arguments,
+   * returning the result as a `DataFrame`.
+   * This API eagerly runs DDL/DML commands, but not for SELECT queries.
+   *
+   * @param sqlText A SQL statement with named parameters to execute.
+   * @param args A map of parameter names to Java/Scala objects that can be converted to
+   *             SQL literal expressions. See
+   *             <a href="https://spark.apache.org/docs/latest/sql-ref-datatypes.html">
+   *             Supported Data Types</a> for supported value types in Scala/Java.
+   *             For example, map keys: "rank", "name", "birthdate";
+   *             map values: 1, "Steven", LocalDate.of(2023, 4, 2).
+   *             Map value can be also a `Column` of literal expression, in that case
+   *             it is taken as is.
+   * @param tracker A tracker that can notify when query is ready for execution
+   */
+  private[sql] def sql(
+      sqlText: String,
+      args: Map[String, Any],
+      tracker: QueryPlanningTracker): DataFrame =
+    withActive {
+      val plan = tracker.measurePhase(QueryPlanningTracker.PARSING) {
+        val parsedPlan = sessionState.sqlParser.parsePlan(sqlText)
+        if (args.nonEmpty) {
+          NameParameterizedQuery(parsedPlan, args.mapValues(lit(_).expr).toMap)
+        } else {
+          parsedPlan
+        }
       }
+      Dataset.ofRows(self, plan, tracker)
     }
-    Dataset.ofRows(self, plan, tracker)
-  }
 
   /**
    * Executes a SQL query substituting named parameters by the given arguments,
@@ -658,17 +709,8 @@ class SparkSession private(
    * @since 3.4.0
    */
   @Experimental
-  def sql(sqlText: String, args: Map[String, Any]): DataFrame = withActive {
-    val tracker = new QueryPlanningTracker
-    val plan = tracker.measurePhase(QueryPlanningTracker.PARSING) {
-      val parsedPlan = sessionState.sqlParser.parsePlan(sqlText)
-      if (args.nonEmpty) {
-        NameParameterizedQuery(parsedPlan, args.mapValues(lit(_).expr).toMap)
-      } else {
-        parsedPlan
-      }
-    }
-    Dataset.ofRows(self, plan, tracker)
+  def sql(sqlText: String, args: Map[String, Any]): DataFrame = {
+    sql(sqlText, args, new QueryPlanningTracker)
   }
 
   /**
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala
index db18cdeaa65..fd9d141414e 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala
@@ -71,9 +71,13 @@ class QueryExecution(
     }
   }
 
-  lazy val analyzed: LogicalPlan = executePhase(QueryPlanningTracker.ANALYSIS) {
-    // We can't clone `logical` here, which will reset the `_analyzed` flag.
-    sparkSession.sessionState.analyzer.executeAndCheck(logical, tracker)
+  lazy val analyzed: LogicalPlan = {
+    val plan = executePhase(QueryPlanningTracker.ANALYSIS) {
+      // We can't clone `logical` here, which will reset the `_analyzed` flag.
+      sparkSession.sessionState.analyzer.executeAndCheck(logical, tracker)
+    }
+    tracker.setAnalyzed(plan)
+    plan
   }
 
   lazy val commandExecuted: LogicalPlan = mode match {
@@ -93,6 +97,11 @@ class QueryExecution(
 
   private def eagerlyExecuteCommands(p: LogicalPlan) = p transformDown {
     case c: Command =>
+      // Since Command execution will eagerly take place here,
+      // and in most cases be the bulk of time and effort,
+      // with the rest of processing of the root plan being just outputting command results,
+      // for eagerly executed commands we mark this place as beginning of execution.
+      tracker.setReadyForExecution()
       val qe = sparkSession.sessionState.executePlan(c, CommandExecutionMode.NON_ROOT)
       val result = SQLExecution.withNewExecutionId(qe, Some(commandExecutionName(c))) {
         qe.executedPlan.executeCollect()
@@ -150,7 +159,7 @@ class QueryExecution(
     }
   }
 
-  private def assertOptimized(): Unit = optimizedPlan
+  def assertOptimized(): Unit = optimizedPlan
 
   lazy val sparkPlan: SparkPlan = {
     // We need to materialize the optimizedPlan here because sparkPlan is also tracked under
@@ -163,19 +172,27 @@ class QueryExecution(
     }
   }
 
+  def assertSparkPlanPrepared(): Unit = sparkPlan
+
   // executedPlan should not be used to initialize any SparkPlan. It should be
   // only used for execution.
   lazy val executedPlan: SparkPlan = {
     // We need to materialize the optimizedPlan here, before tracking the planning phase, to ensure
     // that the optimization time is not counted as part of the planning phase.
     assertOptimized()
-    executePhase(QueryPlanningTracker.PLANNING) {
+    val plan = executePhase(QueryPlanningTracker.PLANNING) {
       // clone the plan to avoid sharing the plan instance between different stages like analyzing,
       // optimizing and planning.
       QueryExecution.prepareForExecution(preparations, sparkPlan.clone())
     }
+    // Note: For eagerly executed command it might have already been called in
+    // `eagerlyExecutedCommand` and is a noop here.
+    tracker.setReadyForExecution()
+    plan
   }
 
+  def assertExecutedPlanPrepared(): Unit = executedPlan
+
   /**
    * Internal version of the RDD. Avoids copies and has no schema.
    * Note for callers: Spark may apply various optimization including reusing object: this means
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/execution/QueryExecutionSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/execution/QueryExecutionSuite.scala
index d2a101b2395..5265095faa8 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/execution/QueryExecutionSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/execution/QueryExecutionSuite.scala
@@ -18,7 +18,8 @@ package org.apache.spark.sql.execution
 
 import scala.io.Source
 
-import org.apache.spark.sql.{AnalysisException, FastOperator}
+import org.apache.spark.sql.{AnalysisException, Dataset, FastOperator}
+import org.apache.spark.sql.catalyst.{QueryPlanningTracker, QueryPlanningTrackerCallback}
 import org.apache.spark.sql.catalyst.analysis.UnresolvedNamespace
 import org.apache.spark.sql.catalyst.expressions.UnsafeRow
 import org.apache.spark.sql.catalyst.plans.QueryPlan
@@ -244,18 +245,27 @@ class QueryExecutionSuite extends SharedSparkSession {
   }
 
   test("SPARK-35378: Eagerly execute non-root Command") {
-    def qe(logicalPlan: LogicalPlan): QueryExecution = new QueryExecution(spark, logicalPlan)
+    val mockCallback1 = MockCallbackEagerCommand()
+    val mockCallback2 = MockCallbackEagerCommand()
+    def qe(logicalPlan: LogicalPlan, callback: QueryPlanningTrackerCallback): QueryExecution =
+      new QueryExecution(spark, logicalPlan, new QueryPlanningTracker(Some(callback)))
 
     val showTables = ShowTables(UnresolvedNamespace(Seq.empty[String]), None)
-    val showTablesQe = qe(showTables)
+    val showTablesQe = qe(showTables, mockCallback1)
+    showTablesQe.assertAnalyzed
+    mockCallback1.assertAnalyzed
     assert(showTablesQe.commandExecuted.isInstanceOf[CommandResult])
+    mockCallback1.assertCommandExecuted
     assert(showTablesQe.executedPlan.isInstanceOf[CommandResultExec])
     val showTablesResultExec = showTablesQe.executedPlan.asInstanceOf[CommandResultExec]
     assert(showTablesResultExec.commandPhysicalPlan.isInstanceOf[ShowTablesExec])
 
     val project = Project(showTables.output, SubqueryAlias("s", showTables))
-    val projectQe = qe(project)
+    val projectQe = qe(project, mockCallback2)
+    projectQe.assertAnalyzed
+    mockCallback2.assertAnalyzed
     assert(projectQe.commandExecuted.isInstanceOf[Project])
+    mockCallback2.assertCommandExecuted
     assert(projectQe.commandExecuted.children.length == 1)
     assert(projectQe.commandExecuted.children(0).isInstanceOf[SubqueryAlias])
     assert(projectQe.commandExecuted.children(0).children.length == 1)
@@ -265,6 +275,39 @@ class QueryExecutionSuite extends SharedSparkSession {
     assert(cmdResultExec.commandPhysicalPlan.isInstanceOf[ShowTablesExec])
   }
 
+  test("SPARK-44145: non eagerly executed command setReadyForExecution") {
+    val mockCallback = MockCallback()
+
+    val showTables = ShowTables(UnresolvedNamespace(Seq.empty[String]), None)
+    val showTablesQe = new QueryExecution(
+      spark,
+      showTables,
+      new QueryPlanningTracker(Some(mockCallback)),
+      CommandExecutionMode.SKIP)
+    showTablesQe.assertAnalyzed
+    mockCallback.assertAnalyzed
+    showTablesQe.assertOptimized
+    mockCallback.assertOptimized
+    showTablesQe.assertSparkPlanPrepared
+    mockCallback.assertSparkPlanPrepared
+    showTablesQe.assertExecutedPlanPrepared
+    mockCallback.assertExecutedPlanPrepared
+  }
+
+  test("SPARK-44145: Plan setReadyForExecution") {
+    val mockCallback = MockCallback()
+    val plan: LogicalPlan = org.apache.spark.sql.catalyst.plans.logical.Range(0, 1, 1, 1)
+    val df = Dataset.ofRows(spark, plan, new QueryPlanningTracker(Some(mockCallback)))
+    df.queryExecution.assertAnalyzed
+    mockCallback.assertAnalyzed
+    df.queryExecution.assertOptimized
+    mockCallback.assertOptimized
+    df.queryExecution.assertSparkPlanPrepared
+    mockCallback.assertSparkPlanPrepared
+    df.queryExecution.assertExecutedPlanPrepared
+    mockCallback.assertExecutedPlanPrepared
+  }
+
   test("SPARK-35378: Return UnsafeRow in CommandResultExecCheck execute methods") {
     val plan = spark.sql("SHOW FUNCTIONS").queryExecution.executedPlan
     assert(plan.isInstanceOf[CommandResultExec])
@@ -290,4 +333,81 @@ class QueryExecutionSuite extends SharedSparkSession {
       }
     }
   }
+
+  case class MockCallbackEagerCommand(
+      var trackerAnalyzed: QueryPlanningTracker = null,
+      var trackerReadyForExecution: QueryPlanningTracker = null)
+      extends QueryPlanningTrackerCallback {
+    def analyzed(trackerFromCallback: QueryPlanningTracker, plan: LogicalPlan): Unit = {
+      trackerAnalyzed = trackerFromCallback
+      assert(trackerAnalyzed.phases.keySet.contains(QueryPlanningTracker.ANALYSIS))
+      assert(!trackerAnalyzed.phases.keySet.contains(QueryPlanningTracker.OPTIMIZATION))
+      assert(!trackerAnalyzed.phases.keySet.contains(QueryPlanningTracker.PLANNING))
+      assert(plan != null)
+    }
+    def readyForExecution(trackerFromCallback: QueryPlanningTracker): Unit = {
+      trackerReadyForExecution = trackerFromCallback
+      assert(trackerReadyForExecution.phases.keySet.contains(QueryPlanningTracker.ANALYSIS))
+      assert(!trackerReadyForExecution.phases.keySet.contains(QueryPlanningTracker.OPTIMIZATION))
+      assert(!trackerReadyForExecution.phases.keySet.contains(QueryPlanningTracker.PLANNING))
+    }
+    def assertAnalyzed(): Unit = {
+      assert(trackerAnalyzed != null)
+      assert(trackerReadyForExecution == null)
+    }
+    def assertCommandExecuted(): Unit = {
+      assert(trackerAnalyzed != null)
+      assert(trackerReadyForExecution != null)
+    }
+    def assertOptimized(): Unit = {
+      assert(trackerAnalyzed != null)
+      assert(trackerReadyForExecution != null)
+    }
+    def assertSparkPlanPrepared(): Unit = {
+      assert(trackerAnalyzed != null)
+      assert(trackerReadyForExecution != null)
+    }
+    def assertExecutedPlanPrepared(): Unit = {
+      assert(trackerAnalyzed != null)
+      assert(trackerReadyForExecution != null)
+    }
+  }
+  case class MockCallback(
+      var trackerAnalyzed: QueryPlanningTracker = null,
+      var trackerReadyForExecution: QueryPlanningTracker = null)
+      extends QueryPlanningTrackerCallback {
+    def analyzed(trackerFromCallback: QueryPlanningTracker, plan: LogicalPlan): Unit = {
+      trackerAnalyzed = trackerFromCallback
+      assert(trackerAnalyzed.phases.keySet.contains(QueryPlanningTracker.ANALYSIS))
+      assert(!trackerAnalyzed.phases.keySet.contains(QueryPlanningTracker.OPTIMIZATION))
+      assert(!trackerAnalyzed.phases.keySet.contains(QueryPlanningTracker.PLANNING))
+      assert(plan != null)
+    }
+    def readyForExecution(trackerFromCallback: QueryPlanningTracker): Unit = {
+      trackerReadyForExecution = trackerFromCallback
+      assert(trackerReadyForExecution.phases.keySet.contains(QueryPlanningTracker.ANALYSIS))
+      assert(trackerReadyForExecution.phases.keySet.contains(QueryPlanningTracker.OPTIMIZATION))
+      assert(trackerReadyForExecution.phases.keySet.contains(QueryPlanningTracker.PLANNING))
+    }
+    def assertAnalyzed(): Unit = {
+      assert(trackerAnalyzed != null)
+      assert(trackerReadyForExecution == null)
+    }
+    def assertCommandExecuted(): Unit = {
+      assert(trackerAnalyzed != null)
+      assert(trackerReadyForExecution == null)
+    }
+    def assertOptimized(): Unit = {
+      assert(trackerAnalyzed != null)
+      assert(trackerReadyForExecution == null)
+    }
+    def assertSparkPlanPrepared(): Unit = {
+      assert(trackerAnalyzed != null)
+      assert(trackerReadyForExecution == null)
+    }
+    def assertExecutedPlanPrepared(): Unit = {
+      assert(trackerAnalyzed != null)
+      assert(trackerReadyForExecution != null)
+    }
+  }
 }
