diff --git a/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ClientSuite.scala b/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ClientSuite.scala
index ba116c27716..b7fb409ebc3 100644
--- a/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ClientSuite.scala
+++ b/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ClientSuite.scala
@@ -55,7 +55,9 @@ import org.apache.spark.resource.ResourceID
 import org.apache.spark.resource.ResourceUtils.AMOUNT
 import org.apache.spark.util.{SparkConfWithEnv, Utils}
 
-class ClientSuite extends SparkFunSuite with Matchers {
+class ClientSuite extends SparkFunSuite
+    with Matchers
+    with ResourceRequestTestHelper {
   private def doReturn(value: Any) = org.mockito.Mockito.doReturn(value, Seq.empty: _*)
 
   import Client._
@@ -473,24 +475,24 @@ class ClientSuite extends SparkFunSuite with Matchers {
   ).foreach { case (deployMode, prefix) =>
     test(s"custom resource request ($deployMode mode)") {
       val resources = Map("fpga" -> 2, "gpu" -> 3)
-      ResourceRequestTestHelper.initializeResourceTypes(resources.keys.toSeq)
+      withResourceTypes(resources.keys.toSeq) {
+        val conf = new SparkConf().set(SUBMIT_DEPLOY_MODE, deployMode)
+        resources.foreach { case (name, v) =>
+          conf.set(s"${prefix}${name}.${AMOUNT}", v.toString)
+        }
 
-      val conf = new SparkConf().set(SUBMIT_DEPLOY_MODE, deployMode)
-      resources.foreach { case (name, v) =>
-        conf.set(s"${prefix}${name}.${AMOUNT}", v.toString)
-      }
+        val appContext = Records.newRecord(classOf[ApplicationSubmissionContext])
+        val getNewApplicationResponse = Records.newRecord(classOf[GetNewApplicationResponse])
+        val containerLaunchContext = Records.newRecord(classOf[ContainerLaunchContext])
 
-      val appContext = Records.newRecord(classOf[ApplicationSubmissionContext])
-      val getNewApplicationResponse = Records.newRecord(classOf[GetNewApplicationResponse])
-      val containerLaunchContext = Records.newRecord(classOf[ContainerLaunchContext])
+        val client = new Client(new ClientArguments(Array()), conf, null)
+        client.createApplicationSubmissionContext(
+          new YarnClientApplication(getNewApplicationResponse, appContext),
+          containerLaunchContext)
 
-      val client = new Client(new ClientArguments(Array()), conf, null)
-      client.createApplicationSubmissionContext(
-        new YarnClientApplication(getNewApplicationResponse, appContext),
-        containerLaunchContext)
-
-      resources.foreach { case (name, value) =>
-        appContext.getResource.getResourceInformation(name).getValue should be (value)
+        resources.foreach { case (name, value) =>
+          appContext.getResource.getResourceInformation(name).getValue should be (value)
+        }
       }
     }
   }
@@ -498,43 +500,45 @@ class ClientSuite extends SparkFunSuite with Matchers {
   test("custom driver resource request yarn config and spark config fails") {
     val conf = new SparkConf().set(SUBMIT_DEPLOY_MODE, "cluster")
     val resources = Map(conf.get(YARN_GPU_DEVICE) -> "gpu", conf.get(YARN_FPGA_DEVICE) -> "fpga")
-    ResourceRequestTestHelper.initializeResourceTypes(resources.keys.toSeq)
-    resources.keys.foreach { yarnName =>
-      conf.set(s"${YARN_DRIVER_RESOURCE_TYPES_PREFIX}${yarnName}.${AMOUNT}", "2")
-    }
-    resources.values.foreach { rName =>
-      conf.set(new ResourceID(SPARK_DRIVER_PREFIX, rName).amountConf, "3")
-    }
+    withResourceTypes(resources.keys.toSeq) {
+      resources.keys.foreach { yarnName =>
+        conf.set(s"${YARN_DRIVER_RESOURCE_TYPES_PREFIX}${yarnName}.${AMOUNT}", "2")
+      }
+      resources.values.foreach { rName =>
+        conf.set(new ResourceID(SPARK_DRIVER_PREFIX, rName).amountConf, "3")
+      }
 
-    val error = intercept[SparkException] {
-      ResourceRequestHelper.validateResources(conf)
-    }.getMessage()
+      val error = intercept[SparkException] {
+        ResourceRequestHelper.validateResources(conf)
+      }.getMessage()
 
-    assert(error.contains("Do not use spark.yarn.driver.resource.yarn.io/fpga.amount," +
-      " please use spark.driver.resource.fpga.amount"))
-    assert(error.contains("Do not use spark.yarn.driver.resource.yarn.io/gpu.amount," +
-      " please use spark.driver.resource.gpu.amount"))
+      assert(error.contains("Do not use spark.yarn.driver.resource.yarn.io/fpga.amount," +
+        " please use spark.driver.resource.fpga.amount"))
+      assert(error.contains("Do not use spark.yarn.driver.resource.yarn.io/gpu.amount," +
+        " please use spark.driver.resource.gpu.amount"))
+    }
   }
 
   test("custom executor resource request yarn config and spark config fails") {
     val conf = new SparkConf().set(SUBMIT_DEPLOY_MODE, "cluster")
     val resources = Map(conf.get(YARN_GPU_DEVICE) -> "gpu", conf.get(YARN_FPGA_DEVICE) -> "fpga")
-    ResourceRequestTestHelper.initializeResourceTypes(resources.keys.toSeq)
-    resources.keys.foreach { yarnName =>
-      conf.set(s"${YARN_EXECUTOR_RESOURCE_TYPES_PREFIX}${yarnName}.${AMOUNT}", "2")
-    }
-    resources.values.foreach { rName =>
-      conf.set(new ResourceID(SPARK_EXECUTOR_PREFIX, rName).amountConf, "3")
-    }
+    withResourceTypes(resources.keys.toSeq) {
+      resources.keys.foreach { yarnName =>
+        conf.set(s"${YARN_EXECUTOR_RESOURCE_TYPES_PREFIX}${yarnName}.${AMOUNT}", "2")
+      }
+      resources.values.foreach { rName =>
+        conf.set(new ResourceID(SPARK_EXECUTOR_PREFIX, rName).amountConf, "3")
+      }
 
-    val error = intercept[SparkException] {
-      ResourceRequestHelper.validateResources(conf)
-    }.getMessage()
+      val error = intercept[SparkException] {
+        ResourceRequestHelper.validateResources(conf)
+      }.getMessage()
 
-    assert(error.contains("Do not use spark.yarn.executor.resource.yarn.io/fpga.amount," +
-      " please use spark.executor.resource.fpga.amount"))
-    assert(error.contains("Do not use spark.yarn.executor.resource.yarn.io/gpu.amount," +
-      " please use spark.executor.resource.gpu.amount"))
+      assert(error.contains("Do not use spark.yarn.executor.resource.yarn.io/fpga.amount," +
+        " please use spark.executor.resource.fpga.amount"))
+      assert(error.contains("Do not use spark.yarn.executor.resource.yarn.io/gpu.amount," +
+        " please use spark.executor.resource.gpu.amount"))
+    }
   }
 
 
@@ -545,30 +549,30 @@ class ClientSuite extends SparkFunSuite with Matchers {
       conf.get(YARN_FPGA_DEVICE) -> "fpga",
       yarnMadeupResource -> "madeup")
 
-    ResourceRequestTestHelper.initializeResourceTypes(resources.keys.toSeq)
-
-    resources.values.foreach { rName =>
-      conf.set(new ResourceID(SPARK_DRIVER_PREFIX, rName).amountConf, "3")
-    }
-    // also just set yarn one that we don't convert
-    conf.set(s"${YARN_DRIVER_RESOURCE_TYPES_PREFIX}${yarnMadeupResource}.${AMOUNT}", "5")
-    val appContext = Records.newRecord(classOf[ApplicationSubmissionContext])
-    val getNewApplicationResponse = Records.newRecord(classOf[GetNewApplicationResponse])
-    val containerLaunchContext = Records.newRecord(classOf[ContainerLaunchContext])
+    withResourceTypes(resources.keys.toSeq) {
+      resources.values.foreach { rName =>
+        conf.set(new ResourceID(SPARK_DRIVER_PREFIX, rName).amountConf, "3")
+      }
+      // also just set yarn one that we don't convert
+      conf.set(s"${YARN_DRIVER_RESOURCE_TYPES_PREFIX}${yarnMadeupResource}.${AMOUNT}", "5")
+      val appContext = Records.newRecord(classOf[ApplicationSubmissionContext])
+      val getNewApplicationResponse = Records.newRecord(classOf[GetNewApplicationResponse])
+      val containerLaunchContext = Records.newRecord(classOf[ContainerLaunchContext])
 
-    val client = new Client(new ClientArguments(Array()), conf, null)
-    val newContext = client.createApplicationSubmissionContext(
-      new YarnClientApplication(getNewApplicationResponse, appContext),
-      containerLaunchContext)
+      val client = new Client(new ClientArguments(Array()), conf, null)
+      val newContext = client.createApplicationSubmissionContext(
+        new YarnClientApplication(getNewApplicationResponse, appContext),
+        containerLaunchContext)
 
-    val yarnRInfo = newContext.getResource.getResources
-    val allResourceInfo = yarnRInfo.map(rInfo => (rInfo.getName -> rInfo.getValue)).toMap
-    assert(allResourceInfo.get(conf.get(YARN_GPU_DEVICE)).nonEmpty)
-    assert(allResourceInfo.get(conf.get(YARN_GPU_DEVICE)).get === 3)
-    assert(allResourceInfo.get(conf.get(YARN_FPGA_DEVICE)).nonEmpty)
-    assert(allResourceInfo.get(conf.get(YARN_FPGA_DEVICE)).get === 3)
-    assert(allResourceInfo.get(yarnMadeupResource).nonEmpty)
-    assert(allResourceInfo.get(yarnMadeupResource).get === 5)
+      val yarnRInfo = newContext.getResource.getResources
+      val allResourceInfo = yarnRInfo.map(rInfo => (rInfo.getName -> rInfo.getValue)).toMap
+      assert(allResourceInfo.get(conf.get(YARN_GPU_DEVICE)).nonEmpty)
+      assert(allResourceInfo.get(conf.get(YARN_GPU_DEVICE)).get === 3)
+      assert(allResourceInfo.get(conf.get(YARN_FPGA_DEVICE)).nonEmpty)
+      assert(allResourceInfo.get(conf.get(YARN_FPGA_DEVICE)).get === 3)
+      assert(allResourceInfo.get(yarnMadeupResource).nonEmpty)
+      assert(allResourceInfo.get(yarnMadeupResource).get === 5)
+    }
   }
 
   test("gpu/fpga spark resources mapped to custom yarn resources") {
@@ -579,26 +583,26 @@ class ClientSuite extends SparkFunSuite with Matchers {
     conf.set(YARN_FPGA_DEVICE.key, fpgaCustomName)
     val resources = Map(gpuCustomName -> "gpu",
       fpgaCustomName -> "fpga")
+    withResourceTypes(resources.keys.toSeq) {
+      resources.values.foreach { rName =>
+        conf.set(new ResourceID(SPARK_DRIVER_PREFIX, rName).amountConf, "3")
+      }
+      val appContext = Records.newRecord(classOf[ApplicationSubmissionContext])
+      val getNewApplicationResponse = Records.newRecord(classOf[GetNewApplicationResponse])
+      val containerLaunchContext = Records.newRecord(classOf[ContainerLaunchContext])
 
-    ResourceRequestTestHelper.initializeResourceTypes(resources.keys.toSeq)
-    resources.values.foreach { rName =>
-      conf.set(new ResourceID(SPARK_DRIVER_PREFIX, rName).amountConf, "3")
-    }
-    val appContext = Records.newRecord(classOf[ApplicationSubmissionContext])
-    val getNewApplicationResponse = Records.newRecord(classOf[GetNewApplicationResponse])
-    val containerLaunchContext = Records.newRecord(classOf[ContainerLaunchContext])
-
-    val client = new Client(new ClientArguments(Array()), conf, null)
-    val newContext = client.createApplicationSubmissionContext(
-      new YarnClientApplication(getNewApplicationResponse, appContext),
-      containerLaunchContext)
+      val client = new Client(new ClientArguments(Array()), conf, null)
+      val newContext = client.createApplicationSubmissionContext(
+        new YarnClientApplication(getNewApplicationResponse, appContext),
+        containerLaunchContext)
 
-    val yarnRInfo = newContext.getResource.getResources
-    val allResourceInfo = yarnRInfo.map(rInfo => (rInfo.getName -> rInfo.getValue)).toMap
-    assert(allResourceInfo.get(gpuCustomName).nonEmpty)
-    assert(allResourceInfo.get(gpuCustomName).get === 3)
-    assert(allResourceInfo.get(fpgaCustomName).nonEmpty)
-    assert(allResourceInfo.get(fpgaCustomName).get === 3)
+      val yarnRInfo = newContext.getResource.getResources
+      val allResourceInfo = yarnRInfo.map(rInfo => (rInfo.getName -> rInfo.getValue)).toMap
+      assert(allResourceInfo.get(gpuCustomName).nonEmpty)
+      assert(allResourceInfo.get(gpuCustomName).get === 3)
+      assert(allResourceInfo.get(fpgaCustomName).nonEmpty)
+      assert(allResourceInfo.get(fpgaCustomName).get === 3)
+    }
   }
 
   test("test yarn jars path not exists") {
diff --git a/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ResourceRequestHelperSuite.scala b/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ResourceRequestHelperSuite.scala
index 53b6d192e72..56ec5a801e4 100644
--- a/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ResourceRequestHelperSuite.scala
+++ b/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ResourceRequestHelperSuite.scala
@@ -28,7 +28,9 @@ import org.apache.spark.deploy.yarn.config._
 import org.apache.spark.internal.config.{DRIVER_CORES, DRIVER_MEMORY, EXECUTOR_CORES, EXECUTOR_MEMORY}
 import org.apache.spark.resource.ResourceUtils.AMOUNT
 
-class ResourceRequestHelperSuite extends SparkFunSuite with Matchers {
+class ResourceRequestHelperSuite extends SparkFunSuite
+    with Matchers
+    with ResourceRequestTestHelper {
 
   private val CUSTOM_RES_1 = "custom-resource-type-1"
   private val CUSTOM_RES_2 = "custom-resource-type-2"
@@ -104,17 +106,16 @@ class ResourceRequestHelperSuite extends SparkFunSuite with Matchers {
       val requests = resources.map { case (rName, rValue, rUnit) =>
         (rName, rValue.toString + rUnit)
       }.toMap
-
-      ResourceRequestTestHelper.initializeResourceTypes(resourceDefs)
-
-      val resource = createResource()
-      setResourceRequests(requests, resource)
-
-      resources.foreach { case (rName, rValue, rUnit) =>
-        val requested = resource.getResourceInformation(rName)
-        assert(requested.getName === rName)
-        assert(requested.getValue === rValue)
-        assert(requested.getUnits === rUnit)
+      withResourceTypes(resourceDefs) {
+        val resource = createResource()
+        setResourceRequests(requests, resource)
+
+        resources.foreach { case (rName, rValue, rUnit) =>
+          val requested = resource.getResourceInformation(rName)
+          assert(requested.getName === rName)
+          assert(requested.getValue === rValue)
+          assert(requested.getUnits === rUnit)
+        }
       }
     }
   }
@@ -125,13 +126,13 @@ class ResourceRequestHelperSuite extends SparkFunSuite with Matchers {
     ("invalid unit", CUSTOM_RES_1, "123ppp")
   ).foreach { case (name, key, value) =>
     test(s"invalid request: $name") {
-      ResourceRequestTestHelper.initializeResourceTypes(Seq(key))
-
-      val resource = createResource()
-      val thrown = intercept[IllegalArgumentException] {
-        setResourceRequests(Map(key -> value), resource)
+      withResourceTypes(Seq(key)) {
+        val resource = createResource()
+        val thrown = intercept[IllegalArgumentException] {
+          setResourceRequests(Map(key -> value), resource)
+        }
+        thrown.getMessage should include (key)
       }
-      thrown.getMessage should include (key)
     }
   }
 
diff --git a/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ResourceRequestTestHelper.scala b/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ResourceRequestTestHelper.scala
index 19c842f0928..826bded50f8 100644
--- a/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ResourceRequestTestHelper.scala
+++ b/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/ResourceRequestTestHelper.scala
@@ -22,7 +22,7 @@ import scala.collection.JavaConverters._
 import org.apache.hadoop.yarn.api.records.ResourceTypeInfo
 import org.apache.hadoop.yarn.util.resource.ResourceUtils
 
-object ResourceRequestTestHelper {
+trait ResourceRequestTestHelper {
   def initializeResourceTypes(resourceTypes: Seq[String]): Unit = {
     // ResourceUtils.reinitializeResources() is the YARN-way
     // to specify resources for the execution of the tests.
@@ -38,4 +38,15 @@ object ResourceRequestTestHelper {
 
     ResourceUtils.reinitializeResources(allResourceTypes.asJava)
   }
+
+  /**
+   * `initializeResourceTypes` with inputs, call `f` and
+   * restore `resourceTypes`` as default value.
+   */
+  def withResourceTypes(resourceTypes: Seq[String])(f: => Unit): Unit = {
+    initializeResourceTypes(resourceTypes)
+    try f finally {
+      initializeResourceTypes(Seq.empty)
+    }
+  }
 }
diff --git a/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnAllocatorSuite.scala b/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnAllocatorSuite.scala
index 055edfbf767..f6f2e1b11d5 100644
--- a/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnAllocatorSuite.scala
+++ b/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnAllocatorSuite.scala
@@ -62,7 +62,10 @@ class MockResolver extends SparkRackResolver(SparkHadoopUtil.get.conf) {
 
 }
 
-class YarnAllocatorSuite extends SparkFunSuite with Matchers with PrivateMethodTester {
+class YarnAllocatorSuite extends SparkFunSuite
+    with Matchers
+    with PrivateMethodTester
+    with ResourceRequestTestHelper {
   val conf = new YarnConfiguration()
   val sparkConf = new SparkConf()
   sparkConf.set(DRIVER_HOST_ADDRESS, "localhost")
@@ -191,135 +194,141 @@ class YarnAllocatorSuite extends SparkFunSuite with Matchers with PrivateMethodT
 
   test("single container allocated with ResourceProfile") {
     val yarnResources = Seq(sparkConf.get(YARN_GPU_DEVICE))
-    ResourceRequestTestHelper.initializeResourceTypes(yarnResources)
-    // create default profile so we get a different id to test below
-    val defaultRProf = ResourceProfile.getOrCreateDefaultProfile(sparkConf)
-    val execReq = new ExecutorResourceRequests().resource("gpu", 6)
-    val taskReq = new TaskResourceRequests().resource("gpu", 1)
-    val rprof = new ResourceProfile(execReq.requests, taskReq.requests)
-    // request a single container and receive it
-    val (handler, _) = createAllocator(0)
+    withResourceTypes(yarnResources) {
+      // create default profile so we get a different id to test below
+      val defaultRProf = ResourceProfile.getOrCreateDefaultProfile(sparkConf)
+      val execReq = new ExecutorResourceRequests().resource("gpu", 6)
+      val taskReq = new TaskResourceRequests().resource("gpu", 1)
+      val rprof = new ResourceProfile(execReq.requests, taskReq.requests)
+      // request a single container and receive it
+      val (handler, _) = createAllocator(0)
+
+      val resourceProfileToTotalExecs = mutable.HashMap(defaultRProf -> 0, rprof -> 1)
+      val numLocalityAwareTasksPerResourceProfileId = mutable.HashMap(rprof.id -> 0)
+      handler.requestTotalExecutorsWithPreferredLocalities(resourceProfileToTotalExecs.toMap,
+        numLocalityAwareTasksPerResourceProfileId.toMap, Map.empty, Set.empty)
 
-    val resourceProfileToTotalExecs = mutable.HashMap(defaultRProf -> 0, rprof -> 1)
-    val numLocalityAwareTasksPerResourceProfileId = mutable.HashMap(rprof.id -> 0)
-    handler.requestTotalExecutorsWithPreferredLocalities(resourceProfileToTotalExecs.toMap,
-      numLocalityAwareTasksPerResourceProfileId.toMap, Map.empty, Set.empty)
+      handler.updateResourceRequests()
+      handler.getNumExecutorsRunning should be (0)
+      handler.getNumContainersPendingAllocate should be (1)
 
-    handler.updateResourceRequests()
-    handler.getNumExecutorsRunning should be (0)
-    handler.getNumContainersPendingAllocate should be (1)
+      val container = createContainer("host1", priority = Priority.newInstance(rprof.id))
+      handler.handleAllocatedContainers(Array(container))
 
-    val container = createContainer("host1", priority = Priority.newInstance(rprof.id))
-    handler.handleAllocatedContainers(Array(container))
+      handler.getNumExecutorsRunning should be (1)
+      handler.allocatedContainerToHostMap.get(container.getId).get should be ("host1")
+      val hostTocontainer = handler.allocatedHostToContainersMapPerRPId(rprof.id)
+      hostTocontainer.get("host1").get should contain(container.getId)
 
-    handler.getNumExecutorsRunning should be (1)
-    handler.allocatedContainerToHostMap.get(container.getId).get should be ("host1")
-    val hostTocontainer = handler.allocatedHostToContainersMapPerRPId(rprof.id)
-    hostTocontainer.get("host1").get should contain(container.getId)
-
-    val size = rmClient.getMatchingRequests(container.getPriority, "host1", containerResource).size
-    size should be (0)
+      val size =
+        rmClient.getMatchingRequests(container.getPriority, "host1", containerResource).size
+      size should be (0)
 
-    ResourceProfile.reInitDefaultProfile(sparkConf)
+      ResourceProfile.reInitDefaultProfile(sparkConf)
+    }
   }
 
   test("multiple containers allocated with ResourceProfiles") {
     val yarnResources = Seq(sparkConf.get(YARN_GPU_DEVICE), sparkConf.get(YARN_FPGA_DEVICE))
-    ResourceRequestTestHelper.initializeResourceTypes(yarnResources)
-    // create default profile so we get a different id to test below
-    val defaultRProf = ResourceProfile.getOrCreateDefaultProfile(sparkConf)
-    val execReq = new ExecutorResourceRequests().resource("gpu", 6)
-    val taskReq = new TaskResourceRequests().resource("gpu", 1)
-    val rprof = new ResourceProfile(execReq.requests, taskReq.requests)
+    withResourceTypes(yarnResources) {
+      // create default profile so we get a different id to test below
+      val defaultRProf = ResourceProfile.getOrCreateDefaultProfile(sparkConf)
+      val execReq = new ExecutorResourceRequests().resource("gpu", 6)
+      val taskReq = new TaskResourceRequests().resource("gpu", 1)
+      val rprof = new ResourceProfile(execReq.requests, taskReq.requests)
 
-    val execReq2 = new ExecutorResourceRequests().memory("8g").resource("fpga", 2)
-    val taskReq2 = new TaskResourceRequests().resource("fpga", 1)
-    val rprof2 = new ResourceProfile(execReq2.requests, taskReq2.requests)
+      val execReq2 = new ExecutorResourceRequests().memory("8g").resource("fpga", 2)
+      val taskReq2 = new TaskResourceRequests().resource("fpga", 1)
+      val rprof2 = new ResourceProfile(execReq2.requests, taskReq2.requests)
 
 
-    // request a single container and receive it
-    val (handler, _) = createAllocator(1)
-    val resourceProfileToTotalExecs = mutable.HashMap(defaultRProf -> 0, rprof -> 1, rprof2 -> 2)
-    val numLocalityAwareTasksPerResourceProfileId = mutable.HashMap(rprof.id -> 0, rprof2.id -> 0)
-    handler.requestTotalExecutorsWithPreferredLocalities(resourceProfileToTotalExecs.toMap,
-      numLocalityAwareTasksPerResourceProfileId.toMap, Map.empty, Set.empty)
-
-    handler.updateResourceRequests()
-    handler.getNumExecutorsRunning should be (0)
-    handler.getNumContainersPendingAllocate should be (3)
-
-    val containerResourcerp2 = Resource.newInstance(10240, 5)
-
-    val container = createContainer("host1", priority = Priority.newInstance(rprof.id))
-    val container2 = createContainer("host2", resource = containerResourcerp2,
-      priority = Priority.newInstance(rprof2.id))
-    val container3 = createContainer("host3", resource = containerResourcerp2,
-      priority = Priority.newInstance(rprof2.id))
-    handler.handleAllocatedContainers(Array(container, container2, container3))
+      // request a single container and receive it
+      val (handler, _) = createAllocator(1)
+      val resourceProfileToTotalExecs = mutable.HashMap(defaultRProf -> 0, rprof -> 1, rprof2 -> 2)
+      val numLocalityAwareTasksPerResourceProfileId = mutable.HashMap(rprof.id -> 0, rprof2.id -> 0)
+      handler.requestTotalExecutorsWithPreferredLocalities(resourceProfileToTotalExecs.toMap,
+        numLocalityAwareTasksPerResourceProfileId.toMap, Map.empty, Set.empty)
 
-    handler.getNumExecutorsRunning should be (3)
-    handler.allocatedContainerToHostMap.get(container.getId).get should be ("host1")
-    handler.allocatedContainerToHostMap.get(container2.getId).get should be ("host2")
-    handler.allocatedContainerToHostMap.get(container3.getId).get should be ("host3")
-
-    val hostTocontainer = handler.allocatedHostToContainersMapPerRPId(rprof.id)
-    hostTocontainer.get("host1").get should contain(container.getId)
-    val hostTocontainer2 = handler.allocatedHostToContainersMapPerRPId(rprof2.id)
-    hostTocontainer2.get("host2").get should contain(container2.getId)
-    hostTocontainer2.get("host3").get should contain(container3.getId)
-
-    val size = rmClient.getMatchingRequests(container.getPriority, "host1", containerResource).size
-    size should be (0)
-
-    ResourceProfile.reInitDefaultProfile(sparkConf)
+      handler.updateResourceRequests()
+      handler.getNumExecutorsRunning should be (0)
+      handler.getNumContainersPendingAllocate should be (3)
+
+      val containerResourcerp2 = Resource.newInstance(10240, 5)
+
+      val container = createContainer("host1", priority = Priority.newInstance(rprof.id))
+      val container2 = createContainer("host2", resource = containerResourcerp2,
+        priority = Priority.newInstance(rprof2.id))
+      val container3 = createContainer("host3", resource = containerResourcerp2,
+        priority = Priority.newInstance(rprof2.id))
+      handler.handleAllocatedContainers(Array(container, container2, container3))
+
+      handler.getNumExecutorsRunning should be (3)
+      handler.allocatedContainerToHostMap.get(container.getId).get should be ("host1")
+      handler.allocatedContainerToHostMap.get(container2.getId).get should be ("host2")
+      handler.allocatedContainerToHostMap.get(container3.getId).get should be ("host3")
+
+      val hostTocontainer = handler.allocatedHostToContainersMapPerRPId(rprof.id)
+      hostTocontainer.get("host1").get should contain(container.getId)
+      val hostTocontainer2 = handler.allocatedHostToContainersMapPerRPId(rprof2.id)
+      hostTocontainer2.get("host2").get should contain(container2.getId)
+      hostTocontainer2.get("host3").get should contain(container3.getId)
+
+      val size =
+        rmClient.getMatchingRequests(container.getPriority, "host1", containerResource).size
+      size should be (0)
+
+      ResourceProfile.reInitDefaultProfile(sparkConf)
+    }
   }
 
   test("custom resource requested from yarn") {
-    ResourceRequestTestHelper.initializeResourceTypes(List("gpu"))
-
-    val mockAmClient = mock(classOf[AMRMClient[ContainerRequest]])
-    val (handler, _) = createAllocator(1, mockAmClient,
-      Map(s"${YARN_EXECUTOR_RESOURCE_TYPES_PREFIX}${GPU}.${AMOUNT}" -> "2G"))
-
-    handler.updateResourceRequests()
-    val defaultResource = handler.rpIdToYarnResource.get(defaultRPId)
-    val container = createContainer("host1", resource = defaultResource)
-    handler.handleAllocatedContainers(Array(container))
-
-    // get amount of memory and vcores from resource, so effectively skipping their validation
-    val expectedResources = Resource.newInstance(defaultResource.getMemorySize(),
-      defaultResource.getVirtualCores)
-    setResourceRequests(Map("gpu" -> "2G"), expectedResources)
-    val captor = ArgumentCaptor.forClass(classOf[ContainerRequest])
+    withResourceTypes(List("gpu")) {
+      val mockAmClient = mock(classOf[AMRMClient[ContainerRequest]])
+      val (handler, _) = createAllocator(1, mockAmClient,
+        Map(s"${YARN_EXECUTOR_RESOURCE_TYPES_PREFIX}${GPU}.${AMOUNT}" -> "2G"))
 
-    verify(mockAmClient).addContainerRequest(captor.capture())
-    val containerRequest: ContainerRequest = captor.getValue
-    assert(containerRequest.getCapability === expectedResources)
+      handler.updateResourceRequests()
+      val defaultResource = handler.rpIdToYarnResource.get(defaultRPId)
+      val container = createContainer("host1", resource = defaultResource)
+      handler.handleAllocatedContainers(Array(container))
+
+      // get amount of memory and vcores from resource, so effectively skipping their validation
+      val expectedResources = Resource.newInstance(defaultResource.getMemorySize(),
+        defaultResource.getVirtualCores)
+      setResourceRequests(Map("gpu" -> "2G"), expectedResources)
+      val captor = ArgumentCaptor.forClass(classOf[ContainerRequest])
+
+      verify(mockAmClient).addContainerRequest(captor.capture())
+      val containerRequest: ContainerRequest = captor.getValue
+      assert(containerRequest.getCapability === expectedResources)
+    }
   }
 
   test("custom spark resource mapped to yarn resource configs") {
     val yarnMadeupResource = "yarn.io/madeup"
     val yarnResources = Seq(sparkConf.get(YARN_GPU_DEVICE), sparkConf.get(YARN_FPGA_DEVICE),
       yarnMadeupResource)
-    ResourceRequestTestHelper.initializeResourceTypes(yarnResources)
-    val mockAmClient = mock(classOf[AMRMClient[ContainerRequest]])
-    val madeupConfigName = s"${YARN_EXECUTOR_RESOURCE_TYPES_PREFIX}${yarnMadeupResource}.${AMOUNT}"
-    val sparkResources =
-      Map(EXECUTOR_GPU_ID.amountConf -> "3",
-        EXECUTOR_FPGA_ID.amountConf -> "2",
-        madeupConfigName -> "5")
-    val (handler, _) = createAllocator(1, mockAmClient, sparkResources)
+    withResourceTypes(yarnResources) {
+      val mockAmClient = mock(classOf[AMRMClient[ContainerRequest]])
+      val madeupConfigName =
+        s"${YARN_EXECUTOR_RESOURCE_TYPES_PREFIX}${yarnMadeupResource}.${AMOUNT}"
+      val sparkResources =
+        Map(EXECUTOR_GPU_ID.amountConf -> "3",
+          EXECUTOR_FPGA_ID.amountConf -> "2",
+          madeupConfigName -> "5")
+      val (handler, _) = createAllocator(1, mockAmClient, sparkResources)
 
-    handler.updateResourceRequests()
-    val defaultResource = handler.rpIdToYarnResource.get(defaultRPId)
-    val yarnRInfo = defaultResource.getResources
-    val allResourceInfo = yarnRInfo.map( rInfo => (rInfo.getName -> rInfo.getValue) ).toMap
-    assert(allResourceInfo.get(sparkConf.get(YARN_GPU_DEVICE)).nonEmpty)
-    assert(allResourceInfo.get(sparkConf.get(YARN_GPU_DEVICE)).get === 3)
-    assert(allResourceInfo.get(sparkConf.get(YARN_FPGA_DEVICE)).nonEmpty)
-    assert(allResourceInfo.get(sparkConf.get(YARN_FPGA_DEVICE)).get === 2)
-    assert(allResourceInfo.get(yarnMadeupResource).nonEmpty)
-    assert(allResourceInfo.get(yarnMadeupResource).get === 5)
+      handler.updateResourceRequests()
+      val defaultResource = handler.rpIdToYarnResource.get(defaultRPId)
+      val yarnRInfo = defaultResource.getResources
+      val allResourceInfo = yarnRInfo.map( rInfo => (rInfo.getName -> rInfo.getValue) ).toMap
+      assert(allResourceInfo.get(sparkConf.get(YARN_GPU_DEVICE)).nonEmpty)
+      assert(allResourceInfo.get(sparkConf.get(YARN_GPU_DEVICE)).get === 3)
+      assert(allResourceInfo.get(sparkConf.get(YARN_FPGA_DEVICE)).nonEmpty)
+      assert(allResourceInfo.get(sparkConf.get(YARN_FPGA_DEVICE)).get === 2)
+      assert(allResourceInfo.get(yarnMadeupResource).nonEmpty)
+      assert(allResourceInfo.get(yarnMadeupResource).get === 5)
+    }
   }
 
   test("gpu/fpga spark resource mapped to custom yarn resource") {
@@ -331,21 +340,22 @@ class YarnAllocatorSuite extends SparkFunSuite with Matchers with PrivateMethodT
       sparkConf.set(YARN_GPU_DEVICE.key, gpuCustomName)
       sparkConf.set(YARN_FPGA_DEVICE.key, fpgaCustomName)
       val yarnResources = Seq(gpuCustomName, fpgaCustomName)
-      ResourceRequestTestHelper.initializeResourceTypes(yarnResources)
-      val mockAmClient = mock(classOf[AMRMClient[ContainerRequest]])
-      val sparkResources =
-        Map(EXECUTOR_GPU_ID.amountConf -> "3",
-          EXECUTOR_FPGA_ID.amountConf -> "2")
-      val (handler, _) = createAllocator(1, mockAmClient, sparkResources)
-
-      handler.updateResourceRequests()
-      val defaultResource = handler.rpIdToYarnResource.get(defaultRPId)
-      val yarnRInfo = defaultResource.getResources
-      val allResourceInfo = yarnRInfo.map(rInfo => (rInfo.getName -> rInfo.getValue)).toMap
-      assert(allResourceInfo.get(gpuCustomName).nonEmpty)
-      assert(allResourceInfo.get(gpuCustomName).get === 3)
-      assert(allResourceInfo.get(fpgaCustomName).nonEmpty)
-      assert(allResourceInfo.get(fpgaCustomName).get === 2)
+      withResourceTypes(yarnResources) {
+        val mockAmClient = mock(classOf[AMRMClient[ContainerRequest]])
+        val sparkResources =
+          Map(EXECUTOR_GPU_ID.amountConf -> "3",
+            EXECUTOR_FPGA_ID.amountConf -> "2")
+        val (handler, _) = createAllocator(1, mockAmClient, sparkResources)
+
+        handler.updateResourceRequests()
+        val defaultResource = handler.rpIdToYarnResource.get(defaultRPId)
+        val yarnRInfo = defaultResource.getResources
+        val allResourceInfo = yarnRInfo.map(rInfo => (rInfo.getName -> rInfo.getValue)).toMap
+        assert(allResourceInfo.get(gpuCustomName).nonEmpty)
+        assert(allResourceInfo.get(gpuCustomName).get === 3)
+        assert(allResourceInfo.get(fpgaCustomName).nonEmpty)
+        assert(allResourceInfo.get(fpgaCustomName).get === 2)
+      }
     } finally {
       sparkConf.set(YARN_GPU_DEVICE.key, originalGpu)
       sparkConf.set(YARN_FPGA_DEVICE.key, originalFpga)
