diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/Exchange.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/Exchange.scala
index 1a5b7599bb7..ea0778b5f60 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/Exchange.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/Exchange.scala
@@ -29,6 +29,7 @@ import org.apache.spark.sql.catalyst.rules.Rule
 import org.apache.spark.sql.execution.{LeafExecNode, SparkPlan, UnaryExecNode}
 import org.apache.spark.sql.internal.SQLConf
 import org.apache.spark.sql.types.StructType
+import org.apache.spark.sql.vectorized.ColumnarBatch
 
 /**
  * Base class for operators that exchange data among multiple threads or processes.
@@ -49,6 +50,8 @@ abstract class Exchange extends UnaryExecNode {
 case class ReusedExchangeExec(override val output: Seq[Attribute], child: Exchange)
   extends LeafExecNode {
 
+  override def supportsColumnar: Boolean = child.supportsColumnar
+
   // Ignore this wrapper for canonicalizing.
   override def doCanonicalize(): SparkPlan = child.canonicalized
 
@@ -56,6 +59,10 @@ case class ReusedExchangeExec(override val output: Seq[Attribute], child: Exchan
     child.execute()
   }
 
+  override def doExecuteColumnar(): RDD[ColumnarBatch] = {
+    child.executeColumnar()
+  }
+
   override protected[sql] def doExecuteBroadcast[T](): broadcast.Broadcast[T] = {
     child.executeBroadcast()
   }
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/execution/ExchangeSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/execution/ExchangeSuite.scala
index 4828c497373..fb97e15e4df 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/execution/ExchangeSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/execution/ExchangeSuite.scala
@@ -19,13 +19,28 @@ package org.apache.spark.sql.execution
 
 import scala.util.Random
 
+import org.apache.spark.rdd.RDD
 import org.apache.spark.sql.{Dataset, Row}
+import org.apache.spark.sql.catalyst.InternalRow
 import org.apache.spark.sql.catalyst.expressions.{Alias, Literal}
 import org.apache.spark.sql.catalyst.plans.physical.{HashPartitioning, IdentityBroadcastMode, SinglePartition}
-import org.apache.spark.sql.execution.exchange.{BroadcastExchangeExec, ReusedExchangeExec, ShuffleExchangeExec}
+import org.apache.spark.sql.execution.exchange._
 import org.apache.spark.sql.execution.joins.HashedRelationBroadcastMode
 import org.apache.spark.sql.internal.SQLConf
 import org.apache.spark.sql.test.SharedSparkSession
+import org.apache.spark.sql.vectorized.ColumnarBatch
+
+class RanColumnar extends RuntimeException
+class RanRowBased extends RuntimeException
+
+case class ColumnarExchange(child: SparkPlan) extends Exchange {
+
+  override def supportsColumnar: Boolean = true
+
+  override protected def doExecute(): RDD[InternalRow] = throw new RanRowBased
+
+  override protected def doExecuteColumnar(): RDD[ColumnarBatch] = throw new RanColumnar
+}
 
 class ExchangeSuite extends SparkPlanTest with SharedSparkSession {
   import testImplicits._
@@ -105,6 +120,15 @@ class ExchangeSuite extends SparkPlanTest with SharedSparkSession {
     assert(exchange5 sameResult exchange4)
   }
 
+  test("Columnar exchange works") {
+    val df = spark.range(10)
+    val plan = df.queryExecution.executedPlan
+    val exchange = ColumnarExchange(plan)
+    val reused = ReusedExchangeExec(plan.output, exchange)
+
+    assertThrows[RanColumnar](reused.executeColumnar())
+  }
+
   test("SPARK-23207: Make repartition() generate consistent output") {
     def assertConsistency(ds: Dataset[java.lang.Long]): Unit = {
       ds.persist()
