diff --git a/python/docs/source/reference/pyspark.sql.rst b/python/docs/source/reference/pyspark.sql.rst
index 022b675dbbc..71e5685121c 100644
--- a/python/docs/source/reference/pyspark.sql.rst
+++ b/python/docs/source/reference/pyspark.sql.rst
@@ -466,6 +466,7 @@ Functions
     lower
     lpad
     ltrim
+    make_date
     map_concat
     map_entries
     map_filter
diff --git a/python/pyspark/sql/functions.py b/python/pyspark/sql/functions.py
index 30ffac3246e..6713853f230 100644
--- a/python/pyspark/sql/functions.py
+++ b/python/pyspark/sql/functions.py
@@ -2131,6 +2131,35 @@ def weekofyear(col: "ColumnOrName") -> Column:
     return Column(sc._jvm.functions.weekofyear(_to_java_column(col)))
 
 
+def make_date(year: "ColumnOrName", month: "ColumnOrName", day: "ColumnOrName") -> Column:
+    """
+    Returns a column with a date built from the year, month and day columns.
+
+    .. versionadded:: 3.3.0
+
+    Parameters
+    ----------
+    year : :class:`~pyspark.sql.Column` or str
+        The year to build the date
+    month : :class:`~pyspark.sql.Column` or str
+        The month to build the date
+    day : :class:`~pyspark.sql.Column` or str
+        The day to build the date
+
+    Examples
+    --------
+    >>> df = spark.createDataFrame([(2020, 6, 26)], ['Y', 'M', 'D'])
+    >>> df.select(make_date(df.Y, df.M, df.D).alias("datefield")).collect()
+    [Row(datefield=datetime.date(2020, 6, 26))]
+    """
+    sc = SparkContext._active_spark_context  # type: ignore[attr-defined]
+    year_col = _to_java_column(year)
+    month_col = _to_java_column(month)
+    day_col = _to_java_column(day)
+    jc = sc._jvm.functions.make_date(year_col, month_col, day_col)
+    return Column(jc)
+
+
 def date_add(start: "ColumnOrName", days: int) -> Column:
     """
     Returns the date that is `days` days after `start`
diff --git a/python/pyspark/sql/tests/test_functions.py b/python/pyspark/sql/tests/test_functions.py
index 2aecff8a147..3a115dfe710 100644
--- a/python/pyspark/sql/tests/test_functions.py
+++ b/python/pyspark/sql/tests/test_functions.py
@@ -25,7 +25,7 @@ from pyspark.sql import Row, Window, types
 from pyspark.sql.functions import udf, input_file_name, col, percentile_approx, \
     lit, assert_true, sum_distinct, sumDistinct, shiftleft, shiftLeft, shiftRight, \
     shiftright, shiftrightunsigned, shiftRightUnsigned, octet_length, bit_length, \
-    sec, csc, cot
+    sec, csc, cot, make_date
 from pyspark.testing.sqlutils import ReusedSQLTestCase, SQLTestUtils
 
 
@@ -241,6 +241,14 @@ class FunctionsTests(ReusedSQLTestCase):
         row = df.select(dayofweek(df.date)).first()
         self.assertEqual(row[0], 2)
 
+    def test_make_date(self):
+        # SPARK-36554: expose make_date expression
+        df = self.spark.createDataFrame([(2020, 6, 26)], ['Y', 'M', 'D'])
+        row_from_col = df.select(make_date(df.Y, df.M, df.D)).first()
+        self.assertEqual(row_from_col[0], datetime.date(2020, 6, 26))
+        row_from_name = df.select(make_date("Y", "M", "D")).first()
+        self.assertEqual(row_from_name[0], datetime.date(2020, 6, 26))
+
     def test_expr(self):
         from pyspark.sql import functions
         row = Row(a="length string", b=75)
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/functions.scala b/sql/core/src/main/scala/org/apache/spark/sql/functions.scala
index 9cdcdc5d972..6264e879642 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/functions.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/functions.scala
@@ -3235,6 +3235,15 @@ object functions {
    */
   def minute(e: Column): Column = withExpr { Minute(e.expr) }
 
+  /**
+   * @return A date created from year, month and day fields.
+   * @group datetime_funcs
+   * @since 3.3.0
+   */
+  def make_date(year: Column, month: Column, day: Column): Column = withExpr {
+    MakeDate(year.expr, month.expr, day.expr)
+  }
+
   /**
    * Returns number of months between dates `start` and `end`.
    *
