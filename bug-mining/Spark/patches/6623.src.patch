diff --git a/core/src/main/scala/org/apache/spark/BarrierTaskContext.scala b/core/src/main/scala/org/apache/spark/BarrierTaskContext.scala
index a841508578a..5afd8a5d866 100644
--- a/core/src/main/scala/org/apache/spark/BarrierTaskContext.scala
+++ b/core/src/main/scala/org/apache/spark/BarrierTaskContext.scala
@@ -128,18 +128,22 @@ class BarrierTaskContext private[spark] (
       // Wait the RPC future to be completed, but every 1 second it will jump out waiting
       // and check whether current spark task is killed. If killed, then throw
       // a `TaskKilledException`, otherwise continue wait RPC until it completes.
-      while(!abortableRpcFuture.toFuture.isCompleted) {
-        if (taskContext.isInterrupted()) {
-          val reason = taskContext.getKillReason().get
-          abortableRpcFuture.abort(reason)
-          throw new TaskKilledException(reason)
-        }
-        // wait RPC future for at most 1 second
-        try {
-          ThreadUtils.awaitResult(abortableRpcFuture.toFuture, 1.second)
-        } catch {
-          case _: TimeoutException => Unit // await future time reach 1 second.
+      try {
+        while (!abortableRpcFuture.toFuture.isCompleted) {
+          // wait RPC future for at most 1 second
+          try {
+            ThreadUtils.awaitResult(abortableRpcFuture.toFuture, 1.second)
+          } catch {
+            case _: TimeoutException | _: InterruptedException =>
+              // If `TimeoutException` thrown, waiting RPC future reach 1 second.
+              // If `InterruptedException` thrown, it is possible this task is killed.
+              // So in this two cases, we should check whether task is killed and then
+              // throw `TaskKilledException`
+              taskContext.killTaskIfInterrupted()
+          }
         }
+      } finally {
+        abortableRpcFuture.abort(taskContext.getKillReason().getOrElse("Unknown reason."))
       }
 
       barrierEpoch += 1
diff --git a/core/src/test/scala/org/apache/spark/scheduler/BarrierTaskContextSuite.scala b/core/src/test/scala/org/apache/spark/scheduler/BarrierTaskContextSuite.scala
index 101d8331485..8d5f04ac765 100644
--- a/core/src/test/scala/org/apache/spark/scheduler/BarrierTaskContextSuite.scala
+++ b/core/src/test/scala/org/apache/spark/scheduler/BarrierTaskContextSuite.scala
@@ -156,13 +156,9 @@ class BarrierTaskContextSuite extends SparkFunSuite with LocalSparkContext {
     assert(error.contains("within 1 second(s)"))
   }
 
-  test("barrier task killed") {
-    val conf = new SparkConf()
-      .set("spark.barrier.sync.timeout", "1")
-      .set(TEST_NO_STAGE_RETRY, true)
-      .setMaster("local-cluster[4, 1, 1024]")
-      .setAppName("test-cluster")
-    sc = new SparkContext(conf)
+
+  def testBarrierTaskKilled(sc: SparkContext, interruptOnCancel: Boolean): Unit = {
+    sc.setLocalProperty(SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL, interruptOnCancel.toString)
 
     withTempDir { dir =>
       val killedFlagFile = "barrier.task.killed"
@@ -204,4 +200,16 @@ class BarrierTaskContextSuite extends SparkFunSuite with LocalSparkContext {
       assert(new File(dir, killedFlagFile).exists(), "Expect barrier task being killed.")
     }
   }
+
+  test("barrier task killed") {
+    val conf = new SparkConf()
+      .set("spark.barrier.sync.timeout", "1")
+      .set(TEST_NO_STAGE_RETRY, true)
+      .setMaster("local-cluster[4, 1, 1024]")
+      .setAppName("test-cluster")
+    sc = new SparkContext(conf)
+
+    testBarrierTaskKilled(sc, true)
+    testBarrierTaskKilled(sc, false)
+  }
 }
