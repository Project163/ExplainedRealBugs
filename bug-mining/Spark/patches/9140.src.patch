diff --git a/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/expressions/UserDefinedFunction.scala b/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/expressions/UserDefinedFunction.scala
index d911b7efe29..7bce4b5b31a 100644
--- a/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/expressions/UserDefinedFunction.scala
+++ b/connector/connect/client/jvm/src/main/scala/org/apache/spark/sql/expressions/UserDefinedFunction.scala
@@ -92,26 +92,23 @@ sealed abstract class UserDefinedFunction {
 /**
  * Holder class for a scalar user-defined function and it's input/output encoder(s).
  */
-case class ScalarUserDefinedFunction(
-    function: AnyRef,
-    inputEncoders: Seq[AgnosticEncoder[_]],
-    outputEncoder: AgnosticEncoder[_],
+case class ScalarUserDefinedFunction private (
+    // SPARK-43198: Eagerly serialize to prevent the UDF from containing a reference to this class.
+    serializedUdfPacket: Array[Byte],
+    inputTypes: Seq[proto.DataType],
+    outputType: proto.DataType,
     name: Option[String],
     override val nullable: Boolean,
     override val deterministic: Boolean)
     extends UserDefinedFunction {
 
-  // SPARK-43198: Eagerly serialize to prevent the UDF from containing a reference to this class.
-  private[this] val udf = {
-    val udfPacketBytes =
-      SparkSerDeUtils.serialize(UdfPacket(function, inputEncoders, outputEncoder))
+  private[this] lazy val udf = {
     val scalaUdfBuilder = proto.ScalarScalaUDF
       .newBuilder()
-      .setPayload(ByteString.copyFrom(udfPacketBytes))
+      .setPayload(ByteString.copyFrom(serializedUdfPacket))
       // Send the real inputs and return types to obtain the types without deser the udf bytes.
-      .addAllInputTypes(
-        inputEncoders.map(_.dataType).map(DataTypeProtoConverter.toConnectProtoType).asJava)
-      .setOutputType(DataTypeProtoConverter.toConnectProtoType(outputEncoder.dataType))
+      .addAllInputTypes(inputTypes.asJava)
+      .setOutputType(outputType)
       .setNullable(nullable)
 
     scalaUdfBuilder.build()
@@ -154,10 +151,12 @@ object ScalarUserDefinedFunction {
       function: AnyRef,
       inputEncoders: Seq[AgnosticEncoder[_]],
       outputEncoder: AgnosticEncoder[_]): ScalarUserDefinedFunction = {
+    val udfPacketBytes =
+      SparkSerDeUtils.serialize(UdfPacket(function, inputEncoders, outputEncoder))
     ScalarUserDefinedFunction(
-      function = function,
-      inputEncoders = inputEncoders,
-      outputEncoder = outputEncoder,
+      serializedUdfPacket = udfPacketBytes,
+      inputTypes = inputEncoders.map(_.dataType).map(DataTypeProtoConverter.toConnectProtoType),
+      outputType = DataTypeProtoConverter.toConnectProtoType(outputEncoder.dataType),
       name = None,
       nullable = true,
       deterministic = true)
diff --git a/connector/connect/client/jvm/src/test/scala/org/apache/spark/sql/application/ReplE2ESuite.scala b/connector/connect/client/jvm/src/test/scala/org/apache/spark/sql/application/ReplE2ESuite.scala
index 676ad6b090e..40841aa3b39 100644
--- a/connector/connect/client/jvm/src/test/scala/org/apache/spark/sql/application/ReplE2ESuite.scala
+++ b/connector/connect/client/jvm/src/test/scala/org/apache/spark/sql/application/ReplE2ESuite.scala
@@ -158,6 +158,17 @@ class ReplE2ESuite extends RemoteSparkSession with BeforeAndAfterEach {
     assertContains("Array[Int] = Array(5, 47, 89, 131, 173)", output)
   }
 
+  test("Updating UDF properties") {
+    val input = """
+        |class A(x: Int) { def get = x * 7 }
+        |val myUdf = udf((x: Int) => new A(x).get)
+        |val modifiedUdf = myUdf.withName("myUdf").asNondeterministic()
+        |spark.range(5).select(modifiedUdf(col("id"))).as[Int].collect()
+      """.stripMargin
+    val output = runCommandsInShell(input)
+    assertContains("Array[Int] = Array(0, 7, 14, 21, 28)", output)
+  }
+
   test("SPARK-43198: Filter does not throw ammonite-related class initialization exception") {
     val input = """
         |spark.range(10).filter(n => n % 2 == 0).collect()
