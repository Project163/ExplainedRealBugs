diff --git a/common/utils/src/main/resources/error/error-classes.json b/common/utils/src/main/resources/error/error-classes.json
index b7cf429c53b..99aeca33dfb 100644
--- a/common/utils/src/main/resources/error/error-classes.json
+++ b/common/utils/src/main/resources/error/error-classes.json
@@ -4496,6 +4496,11 @@
           "Expressions referencing the outer query are not supported outside of WHERE/HAVING clauses: <sqlExprs>."
         ]
       },
+      "HIGHER_ORDER_FUNCTION" : {
+        "message" : [
+          "Subquery expressions are not supported within higher-order functions. Please remove all subquery expressions from higher-order functions and then try the query again."
+        ]
+      },
       "LATERAL_JOIN_CONDITION_NON_DETERMINISTIC" : {
         "message" : [
           "Lateral join condition cannot be non-deterministic: <condition>."
diff --git a/docs/sql-error-conditions-unsupported-subquery-expression-category-error-class.md b/docs/sql-error-conditions-unsupported-subquery-expression-category-error-class.md
index 00c142cce5b..34c070dc97e 100644
--- a/docs/sql-error-conditions-unsupported-subquery-expression-category-error-class.md
+++ b/docs/sql-error-conditions-unsupported-subquery-expression-category-error-class.md
@@ -50,6 +50,10 @@ A correlated outer name reference within a subquery expression body was not foun
 
 Expressions referencing the outer query are not supported outside of WHERE/HAVING clauses: `<sqlExprs>`.
 
+## HIGHER_ORDER_FUNCTION
+
+Subquery expressions are not supported within higher-order functions. Please remove all subquery expressions from higher-order functions and then try the query again.
+
 ## LATERAL_JOIN_CONDITION_NON_DETERMINISTIC
 
 Lateral join condition cannot be non-deterministic: `<condition>`.
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/higherOrderFunctions.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/higherOrderFunctions.scala
index 492adf62c78..1d6656fc642 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/higherOrderFunctions.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/higherOrderFunctions.scala
@@ -22,6 +22,7 @@ import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan
 import org.apache.spark.sql.catalyst.rules.Rule
 import org.apache.spark.sql.catalyst.trees.TreePattern._
 import org.apache.spark.sql.catalyst.util.TypeUtils.{toSQLConf, toSQLId}
+import org.apache.spark.sql.errors.QueryCompilationErrors
 import org.apache.spark.sql.internal.SQLConf
 import org.apache.spark.sql.types.DataType
 
@@ -113,9 +114,11 @@ object ResolveLambdaVariables extends Rule[LogicalPlan] {
     case _ if e.resolved => e
 
     case h: HigherOrderFunction if h.argumentsResolved && h.checkArgumentDataTypes().isSuccess =>
+      checkForSubqueryExpressions(e)
       h.bind(createLambda).mapChildren(resolve(_, parentLambdaMap))
 
     case l: LambdaFunction if !l.bound =>
+      checkForSubqueryExpressions(e)
       // Do not resolve an unbound lambda function. If we see such a lambda function this means
       // that either the higher order function has yet to be resolved, or that we are seeing
       // dangling lambda function.
@@ -138,4 +141,16 @@ object ResolveLambdaVariables extends Rule[LogicalPlan] {
     case _ =>
       e.mapChildren(resolve(_, parentLambdaMap))
   }
+
+  /**
+   * SPARK-47509: There is a correctness bug when subquery expressions appear within lambdas or
+   * higher-order functions. Here we check for that case and return an error if the corresponding
+   * configuration indicates to do so.
+   */
+  private def checkForSubqueryExpressions(expression: Expression): Unit = {
+    if (expression.containsPattern(PLAN_EXPRESSION) &&
+      !conf.getConf(SQLConf.ALLOW_SUBQUERY_EXPRESSIONS_IN_LAMBDAS_AND_HIGHER_ORDER_FUNCTIONS)) {
+      throw QueryCompilationErrors.subqueryExpressionInLambdaOrHigherOrderFunctionNotAllowedError()
+    }
+  }
 }
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala
index d717c5382a7..c3b7faff300 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala
@@ -620,6 +620,12 @@ private[sql] object QueryCompilationErrors extends QueryErrorsBase with Compilat
       messageParameters = Map("prettyName" -> toSQLId(prettyName), "syntax" -> toSQLStmt(syntax)))
   }
 
+  def subqueryExpressionInLambdaOrHigherOrderFunctionNotAllowedError(): Throwable = {
+    new AnalysisException(
+      errorClass = "UNSUPPORTED_SUBQUERY_EXPRESSION_CATEGORY.HIGHER_ORDER_FUNCTION",
+      messageParameters = Map.empty)
+  }
+
   def nonDeterministicFilterInAggregateError(filterExpr: Expression): Throwable = {
     new AnalysisException(
       errorClass = "INVALID_AGGREGATE_FILTER.NON_DETERMINISTIC",
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
index 6f28248ac85..4934aae3e90 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
@@ -4640,6 +4640,16 @@ object SQLConf {
       .booleanConf
       .createWithDefault(false)
 
+  val ALLOW_SUBQUERY_EXPRESSIONS_IN_LAMBDAS_AND_HIGHER_ORDER_FUNCTIONS =
+    buildConf("spark.sql.analyzer.allowSubqueryExpressionsInLambdasOrHigherOrderFunctions")
+      .internal()
+      .doc("When set to false, the analyzer will throw an error if a subquery expression appears " +
+        "in a lambda function or higher-order function. When set to true, it restores the legacy " +
+        "behavior of allowing subquery eexpressions in lambda functions or higher-order functions.")
+      .version("4.0.0")
+      .booleanConf
+      .createWithDefault(false)
+
   // Deprecate "spark.connect.copyFromLocalToFs.allowDestLocal" in favor of this config. This is
   // currently optional because we don't want to break existing users who are using the old config.
   // If this config is set, then we override the deprecated config.
diff --git a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ResolveSubquerySuite.scala b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ResolveSubquerySuite.scala
index af46c3e7b82..4e17f4624f7 100644
--- a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ResolveSubquerySuite.scala
+++ b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ResolveSubquerySuite.scala
@@ -19,10 +19,11 @@ package org.apache.spark.sql.catalyst.analysis
 
 import org.apache.spark.sql.catalyst.dsl.expressions._
 import org.apache.spark.sql.catalyst.dsl.plans._
-import org.apache.spark.sql.catalyst.expressions.{CreateArray, Expression, GetStructField, InSubquery, LateralSubquery, ListQuery, OuterReference, ScalarSubquery}
+import org.apache.spark.sql.catalyst.expressions.{Alias, CreateArray, Expression, GetStructField, InSubquery, LambdaFunction, LateralSubquery, ListQuery, OuterReference, ScalarSubquery, UnresolvedNamedLambdaVariable}
 import org.apache.spark.sql.catalyst.expressions.aggregate.Count
 import org.apache.spark.sql.catalyst.plans.{Inner, JoinType}
 import org.apache.spark.sql.catalyst.plans.logical._
+import org.apache.spark.sql.types.{ArrayType, IntegerType}
 
 /**
  * Unit tests for [[ResolveSubquery]].
@@ -270,4 +271,32 @@ class ResolveSubquerySuite extends AnalysisTest {
       ), Seq(a, b)).as("sub") :: Nil, t1)
     )
   }
+
+  test("SPARK-47509: Incorrect results for subquery expressions in LambdaFunctions") {
+    val data = LocalRelation(Seq(
+      $"key".int,
+      $"values1".array(IntegerType),
+      $"values2".array(ArrayType(ArrayType(IntegerType)))))
+
+    def plan(e: Expression): LogicalPlan = data.select(e.as("res"))
+
+    def lv(s: Symbol): UnresolvedNamedLambdaVariable =
+      UnresolvedNamedLambdaVariable(Seq(s.name))
+
+    val lambdaPlanScanFromTable: LogicalPlan = plan(
+      LambdaFunction(
+        function = lv(Symbol("x")) + lv(Symbol("X")),
+        arguments = Alias(
+          child = ScalarSubquery(
+            plan(lv(Symbol("x")))),
+          name = "alias")()
+          :: lv(Symbol("X"))
+          :: Nil))
+
+    assertAnalysisErrorClass(
+      inputPlan = lambdaPlanScanFromTable,
+      expectedErrorClass =
+        "UNSUPPORTED_SUBQUERY_EXPRESSION_CATEGORY.HIGHER_ORDER_FUNCTION",
+      expectedMessageParameters = Map.empty[String, String])
+  }
 }
