diff --git a/python/pyspark/errors/error_classes.py b/python/pyspark/errors/error_classes.py
index bdd28347fff..3ad8e5d9703 100644
--- a/python/pyspark/errors/error_classes.py
+++ b/python/pyspark/errors/error_classes.py
@@ -74,6 +74,11 @@ ERROR_CLASSES_JSON = """
       "All items in `<arg_name>` should be in <allowed_types>, got <item_type>."
     ]
   },
+  "INVALID_WHEN_USAGE": {
+    "message": [
+      "when() can only be applied on a Column previously generated by when() function, and cannot be applied once otherwise() is applied."
+    ]
+  },
   "JVM_ATTRIBUTE_NOT_SUPPORTED" : {
     "message" : [
       "Attribute `<attr_name>` is not supported in Spark Connect as it depends on the JVM. If you need to use this attribute, do not use Spark Connect when creating your session."
diff --git a/python/pyspark/sql/connect/column.py b/python/pyspark/sql/connect/column.py
index f30a5f258f2..26cb1477cee 100644
--- a/python/pyspark/sql/connect/column.py
+++ b/python/pyspark/sql/connect/column.py
@@ -31,7 +31,7 @@ from typing import (
     Optional,
 )
 
-from pyspark.errors import PySparkTypeError, PySparkAttributeError
+from pyspark.errors import PySparkTypeError, PySparkAttributeError, PySparkValueError
 from pyspark.sql.types import DataType
 from pyspark.sql.column import Column as PySparkColumn
 
@@ -171,12 +171,16 @@ class Column:
             )
 
         if not isinstance(self._expr, CaseWhen):
-            raise TypeError(
-                "when() can only be applied on a Column previously generated by when() function"
+            raise PySparkTypeError(
+                error_class="INVALID_WHEN_USAGE",
+                message_parameters={},
             )
 
         if self._expr._else_value is not None:
-            raise TypeError("when() cannot be applied once otherwise() is applied")
+            raise PySparkTypeError(
+                error_class="INVALID_WHEN_USAGE",
+                message_parameters={},
+            )
 
         if isinstance(value, Column):
             _value = value._expr
@@ -426,7 +430,12 @@ class Column:
                 dropField = DropField(dropField, fieldName)
 
         if dropField is None:
-            raise ValueError("dropFields requires at least 1 field")
+            raise PySparkValueError(
+                error_class="CANNOT_BE_EMPTY",
+                message_parameters={
+                    "item": "dropFields",
+                },
+            )
 
         return Column(dropField)
 
@@ -444,7 +453,10 @@ class Column:
     def __getitem__(self, k: Any) -> "Column":
         if isinstance(k, slice):
             if k.step is not None:
-                raise ValueError("slice with step is not supported.")
+                raise PySparkValueError(
+                    error_class="SLICE_WITH_STEP",
+                    message_parameters={},
+                )
             return self.substr(k.start, k.stop)
         else:
             return Column(UnresolvedExtractValue(self._expr, LiteralExpression._from_value(k)))
@@ -456,9 +468,9 @@ class Column:
         )
 
     def __nonzero__(self) -> None:
-        raise ValueError(
-            "Cannot convert column into bool: please use '&' for 'and', '|' for 'or', "
-            "'~' for 'not' when building DataFrame boolean expressions."
+        raise PySparkValueError(
+            error_class="CANNOT_CONVERT_COLUMN_INTO_BOOL",
+            message_parameters={},
         )
 
     __bool__ = __nonzero__
diff --git a/python/pyspark/sql/tests/connect/test_connect_column.py b/python/pyspark/sql/tests/connect/test_connect_column.py
index d50aa5b90a9..30be5fb1498 100644
--- a/python/pyspark/sql/tests/connect/test_connect_column.py
+++ b/python/pyspark/sql/tests/connect/test_connect_column.py
@@ -40,7 +40,7 @@ from pyspark.sql.types import (
     DecimalType,
     BooleanType,
 )
-from pyspark.errors import PySparkTypeError
+from pyspark.errors import PySparkTypeError, PySparkValueError
 from pyspark.errors.exceptions.connect import SparkConnectException
 from pyspark.testing.connectutils import should_test_connect
 from pyspark.sql.tests.connect.test_connect_basic import SparkConnectSQLTestCase
@@ -954,12 +954,15 @@ class SparkConnectColumnTests(SparkConnectSQLTestCase):
             message_parameters={"arg_name": "fieldName", "arg_type": "int"},
         )
 
-        with self.assertRaisesRegex(
-            ValueError,
-            "dropFields requires at least 1 field",
-        ):
+        with self.assertRaises(PySparkValueError) as pe:
             cdf.select(cdf.x.dropFields()).show()
 
+        self.check_error(
+            exception=pe.exception,
+            error_class="CANNOT_BE_EMPTY",
+            message_parameters={"item": "dropFields"},
+        )
+
     def test_column_string_ops(self):
         # SPARK-41764: test string ops
         query = """
