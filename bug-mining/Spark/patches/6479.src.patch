diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala
index 2672583ec17..f24d6f168da 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala
@@ -2327,7 +2327,7 @@ class Analyzer(
         expected.flatMap { tableAttr =>
           query.resolveQuoted(tableAttr.name, resolver) match {
             case Some(queryExpr) =>
-              checkField(tableAttr, queryExpr, err => errors += err)
+              checkField(tableAttr, queryExpr, byName, err => errors += err)
             case None =>
               errors += s"Cannot find data for output column '${tableAttr.name}'"
               None
@@ -2345,7 +2345,7 @@ class Analyzer(
 
         query.output.zip(expected).flatMap {
           case (queryExpr, tableAttr) =>
-            checkField(tableAttr, queryExpr, err => errors += err)
+            checkField(tableAttr, queryExpr, byName, err => errors += err)
         }
       }
 
@@ -2360,11 +2360,12 @@ class Analyzer(
     private def checkField(
         tableAttr: Attribute,
         queryExpr: NamedExpression,
+        byName: Boolean,
         addError: String => Unit): Option[NamedExpression] = {
 
       // run the type check first to ensure type errors are present
       val canWrite = DataType.canWrite(
-        queryExpr.dataType, tableAttr.dataType, resolver, tableAttr.name, addError)
+        queryExpr.dataType, tableAttr.dataType, byName, resolver, tableAttr.name, addError)
 
       if (queryExpr.nullable && !tableAttr.nullable) {
         addError(s"Cannot write nullable values to non-null column '${tableAttr.name}'")
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/types/DataType.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/types/DataType.scala
index c987088a623..a35e971d088 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/types/DataType.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/types/DataType.scala
@@ -353,10 +353,9 @@ object DataType {
    *   compatible (read allows nulls or write does not contain nulls).
    * - Both types are maps and the map key and value types are compatible, and value nullability
    *   is compatible  (read allows nulls or write does not contain nulls).
-   * - Both types are structs and each field in the read struct is present in the write struct and
-   *   compatible (including nullability), or is nullable if the write struct does not contain the
-   *   field. Write-side structs are not compatible if they contain fields that are not present in
-   *   the read-side struct.
+   * - Both types are structs and have the same number of fields. The type and nullability of each
+   *   field from read/write is compatible. If byName is true, the name of each field from
+   *   read/write needs to be the same.
    * - Both types are atomic and the write type can be safely cast to the read type.
    *
    * Extra fields in write-side structs are not allowed to avoid accidentally writing data that
@@ -369,14 +368,15 @@ object DataType {
   def canWrite(
       write: DataType,
       read: DataType,
+      byName: Boolean,
       resolver: Resolver,
       context: String,
-      addError: String => Unit = (_: String) => {}): Boolean = {
+      addError: String => Unit): Boolean = {
     (write, read) match {
       case (wArr: ArrayType, rArr: ArrayType) =>
         // run compatibility check first to produce all error messages
-        val typesCompatible =
-          canWrite(wArr.elementType, rArr.elementType, resolver, context + ".element", addError)
+        val typesCompatible = canWrite(
+          wArr.elementType, rArr.elementType, byName, resolver, context + ".element", addError)
 
         if (wArr.containsNull && !rArr.containsNull) {
           addError(s"Cannot write nullable elements to array of non-nulls: '$context'")
@@ -390,31 +390,30 @@ object DataType {
         // read. map keys can be missing fields as long as they are nullable in the read schema.
 
         // run compatibility check first to produce all error messages
-        val keyCompatible =
-          canWrite(wMap.keyType, rMap.keyType, resolver, context + ".key", addError)
-        val valueCompatible =
-          canWrite(wMap.valueType, rMap.valueType, resolver, context + ".value", addError)
-        val typesCompatible = keyCompatible && valueCompatible
+        val keyCompatible = canWrite(
+          wMap.keyType, rMap.keyType, byName, resolver, context + ".key", addError)
+        val valueCompatible = canWrite(
+          wMap.valueType, rMap.valueType, byName, resolver, context + ".value", addError)
 
         if (wMap.valueContainsNull && !rMap.valueContainsNull) {
           addError(s"Cannot write nullable values to map of non-nulls: '$context'")
           false
         } else {
-          typesCompatible
+          keyCompatible && valueCompatible
         }
 
       case (StructType(writeFields), StructType(readFields)) =>
         var fieldCompatible = true
-        readFields.zip(writeFields).foreach {
-          case (rField, wField) =>
-            val namesMatch = resolver(wField.name, rField.name) || isSparkGeneratedName(wField.name)
+        readFields.zip(writeFields).zipWithIndex.foreach {
+          case ((rField, wField), i) =>
+            val nameMatch = resolver(wField.name, rField.name) || isSparkGeneratedName(wField.name)
             val fieldContext = s"$context.${rField.name}"
-            val typesCompatible =
-              canWrite(wField.dataType, rField.dataType, resolver, fieldContext, addError)
+            val typesCompatible = canWrite(
+              wField.dataType, rField.dataType, byName, resolver, fieldContext, addError)
 
-            if (!namesMatch) {
-              addError(s"Struct '$context' field name does not match (may be out of order): " +
-                  s"expected '${rField.name}', found '${wField.name}'")
+            if (byName && !nameMatch) {
+              addError(s"Struct '$context' $i-th field name does not match " +
+                s"(may be out of order): expected '${rField.name}', found '${wField.name}'")
               fieldCompatible = false
             } else if (!rField.nullable && wField.nullable) {
               addError(s"Cannot write nullable values to non-null field: '$fieldContext'")
@@ -427,7 +426,7 @@ object DataType {
 
         if (readFields.size > writeFields.size) {
           val missingFieldsStr = readFields.takeRight(readFields.size - writeFields.size)
-                  .map(f => s"'${f.name}'").mkString(", ")
+            .map(f => s"'${f.name}'").mkString(", ")
           if (missingFieldsStr.nonEmpty) {
             addError(s"Struct '$context' missing fields: $missingFieldsStr")
             fieldCompatible = false
@@ -435,7 +434,7 @@ object DataType {
 
         } else if (writeFields.size > readFields.size) {
           val extraFieldsStr = writeFields.takeRight(writeFields.size - readFields.size)
-              .map(f => s"'${f.name}'").mkString(", ")
+            .map(f => s"'${f.name}'").mkString(", ")
           addError(s"Cannot write extra fields to struct '$context': $extraFieldsStr")
           fieldCompatible = false
         }
diff --git a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/DataSourceV2AnalysisSuite.scala b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/DataSourceV2AnalysisSuite.scala
index 48b43fcccac..58923f26c1e 100644
--- a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/DataSourceV2AnalysisSuite.scala
+++ b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/DataSourceV2AnalysisSuite.scala
@@ -473,6 +473,43 @@ abstract class DataSourceV2AnalysisSuite extends AnalysisTest {
     }
   }
 
+  test("check fields of struct type column") {
+    val tableWithStructCol = TestRelation(
+      new StructType().add(
+        "col", new StructType().add("a", IntegerType).add("b", IntegerType)
+      ).toAttributes
+    )
+
+    val query = TestRelation(
+      new StructType().add(
+        "col", new StructType().add("x", IntegerType).add("y", IntegerType)
+      ).toAttributes
+    )
+
+    withClue("byName") {
+      val parsedPlan = byName(tableWithStructCol, query)
+      assertNotResolved(parsedPlan)
+      assertAnalysisError(parsedPlan, Seq(
+        "Cannot write incompatible data to table", "'table-name'",
+        "Struct 'col' 0-th field name does not match", "expected 'a', found 'x'",
+        "Struct 'col' 1-th field name does not match", "expected 'b', found 'y'"))
+    }
+
+    withClue("byPosition") {
+      val parsedPlan = byPosition(tableWithStructCol, query)
+      assertNotResolved(parsedPlan)
+
+      val expectedQuery = Project(Seq(Alias(
+        Cast(
+          query.output.head,
+          new StructType().add("a", IntegerType).add("b", IntegerType),
+          Some(conf.sessionLocalTimeZone)),
+        "col")()),
+        query)
+      checkAnalysis(parsedPlan, byPosition(tableWithStructCol, expectedQuery))
+    }
+  }
+
   def assertNotResolved(logicalPlan: LogicalPlan): Unit = {
     assert(!logicalPlan.resolved, s"Plan should not be resolved: $logicalPlan")
   }
diff --git a/sql/catalyst/src/test/scala/org/apache/spark/sql/types/DataTypeWriteCompatibilitySuite.scala b/sql/catalyst/src/test/scala/org/apache/spark/sql/types/DataTypeWriteCompatibilitySuite.scala
index 9cc9894f204..6b5fc5f0d44 100644
--- a/sql/catalyst/src/test/scala/org/apache/spark/sql/types/DataTypeWriteCompatibilitySuite.scala
+++ b/sql/catalyst/src/test/scala/org/apache/spark/sql/types/DataTypeWriteCompatibilitySuite.scala
@@ -189,6 +189,14 @@ class DataTypeWriteCompatibilitySuite extends SparkFunSuite {
       "Should allow widening float fields x and y to double")
   }
 
+  test("Check struct type: ignore field name mismatch with byPosition mode") {
+    val nameMismatchFields = StructType(Seq(
+      StructField("a", FloatType, nullable = false),
+      StructField("b", FloatType, nullable = false)))
+    assertAllowed(nameMismatchFields, point2, "t",
+      "Should allow field name mismatch with byPosition mode", false)
+  }
+
   ignore("Check struct types: missing optional field is allowed") {
     // built-in data sources do not yet support missing fields when optional
     assertAllowed(point2, point3, "t",
@@ -370,9 +378,14 @@ class DataTypeWriteCompatibilitySuite extends SparkFunSuite {
 
   // Helper functions
 
-  def assertAllowed(writeType: DataType, readType: DataType, name: String, desc: String): Unit = {
+  def assertAllowed(
+      writeType: DataType,
+      readType: DataType,
+      name: String,
+      desc: String,
+      byName: Boolean = true): Unit = {
     assert(
-      DataType.canWrite(writeType, readType, analysis.caseSensitiveResolution, name,
+      DataType.canWrite(writeType, readType, byName, analysis.caseSensitiveResolution, name,
         errMsg => fail(s"Should not produce errors but was called with: $errMsg")), desc)
   }
 
@@ -392,13 +405,14 @@ class DataTypeWriteCompatibilitySuite extends SparkFunSuite {
       readType: DataType,
       name: String,
       desc: String,
-      numErrs: Int)
-      (errFunc: Seq[String] => Unit): Unit = {
+      numErrs: Int,
+      byName: Boolean = true)
+      (checkErrors: Seq[String] => Unit): Unit = {
     val errs = new mutable.ArrayBuffer[String]()
     assert(
-      DataType.canWrite(writeType, readType, analysis.caseSensitiveResolution, name,
+      DataType.canWrite(writeType, readType, byName, analysis.caseSensitiveResolution, name,
         errMsg => errs += errMsg) === false, desc)
     assert(errs.size === numErrs, s"Should produce $numErrs error messages")
-    errFunc(errs)
+    checkErrors(errs)
   }
 }
