diff --git a/sql/connect/client/jvm/src/test/scala/org/apache/spark/sql/connect/ClientE2ETestSuite.scala b/sql/connect/client/jvm/src/test/scala/org/apache/spark/sql/connect/ClientE2ETestSuite.scala
index 19263cdbed6..566584f7e7a 100644
--- a/sql/connect/client/jvm/src/test/scala/org/apache/spark/sql/connect/ClientE2ETestSuite.scala
+++ b/sql/connect/client/jvm/src/test/scala/org/apache/spark/sql/connect/ClientE2ETestSuite.scala
@@ -1058,7 +1058,12 @@ class ClientE2ETestSuite
 
   private def checkSameResult[E](expected: scala.collection.Seq[E], dataset: Dataset[E]): Unit = {
     dataset.withResult { result =>
-      assert(expected === result.iterator.toBuffer)
+      val iter = result.iterator
+      try {
+        assert(expected === iter.toBuffer)
+      } finally {
+        iter.close()
+      }
     }
   }
 
@@ -1233,9 +1238,9 @@ class ClientE2ETestSuite
       .filter("id > 5 and id < 9")
 
     df.withResult { result =>
+      // build and verify the destructive iterator
+      val iterator = result.destructiveIterator
       try {
-        // build and verify the destructive iterator
-        val iterator = result.destructiveIterator
         // resultMap Map is empty before traversing the result iterator
         assertResultsMapEmpty(result)
         val buffer = mutable.Set.empty[Long]
@@ -1250,7 +1255,7 @@ class ClientE2ETestSuite
         val expectedResult = Set(6L, 7L, 8L)
         assert(buffer.size === 3 && expectedResult == buffer)
       } finally {
-        result.close()
+        iterator.close()
       }
     }
   }
@@ -1565,12 +1570,12 @@ class ClientE2ETestSuite
     val ob1Metrics = Map("ob1" -> new GenericRowWithSchema(Array(0, 49, 98), ob1Schema))
     val ob2Metrics = Map("ob2" -> new GenericRowWithSchema(Array(-1, 48, 97), ob2Schema))
 
-    val obMetrics = observedDf.collectResult().getObservedMetrics
-    assert(df.collectResult().getObservedMetrics === Map.empty)
-    assert(observedDf.collectResult().getObservedMetrics === ob1Metrics)
+    val obMetrics = observedDf.withResult(_.getObservedMetrics)
+    assert(df.withResult(_.getObservedMetrics) === Map.empty)
+    assert(observedDf.withResult(_.getObservedMetrics) === ob1Metrics)
     assert(obMetrics.map(_._2.schema) === Seq(ob1Schema))
 
-    val obObMetrics = observedObservedDf.collectResult().getObservedMetrics
+    val obObMetrics = observedObservedDf.withResult(_.getObservedMetrics)
     assert(obObMetrics === ob1Metrics ++ ob2Metrics)
     assert(obObMetrics.map(_._2.schema).exists(_.equals(ob1Schema)))
     assert(obObMetrics.map(_._2.schema).exists(_.equals(ob2Schema)))
@@ -1579,7 +1584,7 @@ class ClientE2ETestSuite
   for (collectFunc <- Seq(
       ("collect", (df: DataFrame) => df.collect()),
       ("collectAsList", (df: DataFrame) => df.collectAsList()),
-      ("collectResult", (df: DataFrame) => df.collectResult().length),
+      ("collectResult", (df: DataFrame) => df.withResult(_.length)),
       ("write", (df: DataFrame) => df.write.format("noop").mode("append").save())))
     test(
       "Observation.get is blocked until the query is finished, " +
diff --git a/sql/connect/client/jvm/src/test/scala/org/apache/spark/sql/connect/SparkSessionE2ESuite.scala b/sql/connect/client/jvm/src/test/scala/org/apache/spark/sql/connect/SparkSessionE2ESuite.scala
index e28d8587a41..1d022489b70 100644
--- a/sql/connect/client/jvm/src/test/scala/org/apache/spark/sql/connect/SparkSessionE2ESuite.scala
+++ b/sql/connect/client/jvm/src/test/scala/org/apache/spark/sql/connect/SparkSessionE2ESuite.scala
@@ -308,34 +308,36 @@ class SparkSessionE2ESuite extends ConnectFunSuite with RemoteSparkSession {
   }
 
   test("progress is available for the spark result") {
-    val result = spark
+    spark
       .range(10000)
       .repartition(1000)
-      .collectResult()
-    assert(result.length == 10000)
-    assert(result.progress.stages.map(_.numTasks).sum > 100)
-    assert(result.progress.stages.map(_.completedTasks).sum > 100)
+      .withResult { result =>
+        assert(result.length == 10000)
+        assert(result.progress.stages.map(_.numTasks).sum > 100)
+        assert(result.progress.stages.map(_.completedTasks).sum > 100)
+      }
   }
 
   test("interrupt operation") {
     val session = spark
     import session.implicits._
 
-    val result = spark
+    spark
       .range(10)
       .map(n => {
         Thread.sleep(5000); n
       })
-      .collectResult()
-    // cancel
-    val operationId = result.operationId
-    val canceledId = spark.interruptOperation(operationId)
-    assert(canceledId == Seq(operationId))
-    // and check that it got canceled
-    val e = intercept[SparkException] {
-      result.toArray
-    }
-    assert(e.getMessage contains "OPERATION_CANCELED")
+      .withResult { result =>
+        // cancel
+        val operationId = result.operationId
+        val canceledId = spark.interruptOperation(operationId)
+        assert(canceledId == Seq(operationId))
+        // and check that it got canceled
+        val e = intercept[SparkException] {
+          result.toArray
+        }
+        assert(e.getMessage contains "OPERATION_CANCELED")
+      }
   }
 
   test("option propagation") {
