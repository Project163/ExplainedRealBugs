diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala
index 6eb2ffef44e..51f310dcc04 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FileStreamSource.scala
@@ -69,12 +69,15 @@ class FileStreamSource(
   private val sourceCleaner: Option[FileStreamSourceCleaner] = FileStreamSourceCleaner(
     fs, qualifiedBasePath, sourceOptions, hadoopConf)
 
-  private val optionsWithPartitionBasePath = sourceOptions.optionMapWithoutPath ++ {
-    if (!SparkHadoopUtil.get.isGlobPath(new Path(path)) && options.contains("path")) {
-      Map("basePath" -> path)
-    } else {
-      Map()
-    }}
+  private val optionsForInnerDataSource = sourceOptions.optionMapWithoutPath ++ {
+    val pathOption =
+      if (!SparkHadoopUtil.get.isGlobPath(new Path(path)) && options.contains("path")) {
+        Map("basePath" -> path)
+      } else {
+        Map()
+      }
+    pathOption ++ Map(DataSource.GLOB_PATHS_KEY -> "false")
+  }
 
   private val metadataLog =
     new FileStreamSourceLog(FileStreamSourceLog.VERSION, sparkSession, metadataPath)
@@ -243,7 +246,7 @@ class FileStreamSource(
         userSpecifiedSchema = Some(schema),
         partitionColumns = partitionColumns,
         className = fileFormatClassName,
-        options = optionsWithPartitionBasePath)
+        options = optionsForInnerDataSource)
     Dataset.ofRows(sparkSession, LogicalRelation(newDataSource.resolveRelation(
       checkFilesExist = false), isStreaming = true))
   }
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/streaming/FileStreamSourceSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/streaming/FileStreamSourceSuite.scala
index e5229c5f253..58ccf80d901 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/streaming/FileStreamSourceSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/streaming/FileStreamSourceSuite.scala
@@ -474,47 +474,59 @@ class FileStreamSourceSuite extends FileStreamSourceTest {
     }
   }
 
-  test("SPARK-21996 read from text files -- file name has space") {
-    withTempDirs { case (src, tmp) =>
-      val textStream = createFileStream("text", src.getCanonicalPath)
-      val filtered = textStream.filter($"value" contains "keep")
+  test("SPARK-21996/SPARK-43343 read from text files -- file name has special chars") {
+    Seq(" ", "[", "[123]").foreach { special_str =>
+      withTempDirs { case (src, tmp) =>
+        val textStream = createFileStream("text", src.getCanonicalPath)
+        val filtered = textStream.filter($"value" contains "keep")
 
-      testStream(filtered)(
-        AddTextFileData("drop1\nkeep2\nkeep3", src, tmp, "text text"),
-        CheckAnswer("keep2", "keep3")
-      )
+        testStream(filtered)(
+          AddTextFileData("drop1\nkeep2\nkeep3", src, tmp, s"text${special_str}text"),
+          CheckAnswer("keep2", "keep3")
+        )
+      }
     }
   }
 
-  test("SPARK-21996 read from text files generated by file sink -- file name has space") {
+  test(
+    "SPARK-21996/SPARK-43343 read from text files generated by file sink --" +
+    "file name has special chars") {
     val testTableName = "FileStreamSourceTest"
-    withTable(testTableName) {
-      withTempDirs { case (src, checkpoint) =>
-        val output = new File(src, "text text")
-        val inputData = MemoryStream[String]
-        val ds = inputData.toDS()
-
-        val query = ds.writeStream
-          .option("checkpointLocation", checkpoint.getCanonicalPath)
-          .format("text")
-          .start(output.getCanonicalPath)
+    Seq(" ", "[", "[123]").foreach{ special_str =>
+      withTable(testTableName) {
+        withTempDirs { case (src, checkpoint) =>
+          val output = new File(src, s"text${special_str}text")
+          val inputData = MemoryStream[String]
+          val ds = inputData.toDS()
+
+          val query = ds.writeStream
+            .option("checkpointLocation", checkpoint.getCanonicalPath)
+            .format("text")
+            .start(output.getCanonicalPath)
 
-        try {
-          inputData.addData("foo")
-          failAfter(streamingTimeout) {
-            query.processAllAvailable()
+          try {
+            inputData.addData("foo")
+            failAfter(streamingTimeout) {
+              query.processAllAvailable()
+            }
+          } finally {
+            query.stop()
           }
-        } finally {
-          query.stop()
-        }
 
-        val df2 = spark.readStream.format("text").load(output.getCanonicalPath)
-        val query2 = df2.writeStream.format("memory").queryName(testTableName).start()
-        try {
-          query2.processAllAvailable()
-          checkDatasetUnorderly(spark.table(testTableName).as[String], "foo")
-        } finally {
-          query2.stop()
+          // '[' and ']' need to be escaped. Otherwise, it will be treated as glob pattern
+          // and won't match the file.
+          val input_path = output
+              .getCanonicalPath
+              .replace("[", "\\[")
+              .replace("]", "\\]")
+          val df2 = spark.readStream.format("text").load(input_path)
+          val query2 = df2.writeStream.format("memory").queryName(testTableName).start()
+          try {
+            query2.processAllAvailable()
+            checkDatasetUnorderly(spark.table(testTableName).as[String], "foo")
+          } finally {
+            query2.stop()
+          }
         }
       }
     }
