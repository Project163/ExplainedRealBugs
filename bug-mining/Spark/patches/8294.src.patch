diff --git a/python/pyspark/sql/pandas/serializers.py b/python/pyspark/sql/pandas/serializers.py
index 47c98c814da..992e82b403a 100644
--- a/python/pyspark/sql/pandas/serializers.py
+++ b/python/pyspark/sql/pandas/serializers.py
@@ -215,7 +215,10 @@ class ArrowStreamPandasSerializer(ArrowStreamSerializer):
         series = ((s, None) if not isinstance(s, (list, tuple)) else s for s in series)
 
         def create_array(s, t):
-            mask = s.isnull()
+            if hasattr(s.array, "__arrow_array__"):
+                mask = None
+            else:
+                mask = s.isnull()
             # Ensure timestamp series are in expected form for Spark internal representation
             if t is not None and pa.types.is_timestamp(t) and t.tz is not None:
                 s = _check_series_convert_timestamps_internal(s, self._timezone)
diff --git a/python/pyspark/sql/tests/test_arrow.py b/python/pyspark/sql/tests/test_arrow.py
index 99705fbb725..ff42ade1407 100644
--- a/python/pyspark/sql/tests/test_arrow.py
+++ b/python/pyspark/sql/tests/test_arrow.py
@@ -548,6 +548,27 @@ class ArrowTests(ReusedSQLTestCase):
                 self.assertEqual(m, map_data[i])
                 self.assertEqual(m_arrow, map_data[i])
 
+    def test_createDataFrame_with_string_dtype(self):
+        # SPARK-34521: spark.createDataFrame does not support Pandas StringDtype extension type
+        with self.sql_conf({"spark.sql.execution.arrow.pyspark.enabled": True}):
+            data = [["abc"], ["def"], [None], ["ghi"], [None]]
+            pandas_df = pd.DataFrame(data, columns=["col"], dtype="string")
+            schema = StructType([StructField("col", StringType(), True)])
+            df = self.spark.createDataFrame(pandas_df, schema=schema)
+
+            # dtypes won't match. Pandas has two different ways to store string columns:
+            # using ndarray (when dtype isn't specified) or using a StringArray when dtype="string".
+            # When calling dataframe#toPandas() it will use the ndarray version.
+            # Changing that to use a StringArray would be backwards incompatible.
+            assert_frame_equal(pandas_df, df.toPandas(), check_dtype=False)
+
+    def test_createDataFrame_with_int64(self):
+        # SPARK-34521: spark.createDataFrame does not support Pandas StringDtype extension type
+        with self.sql_conf({"spark.sql.execution.arrow.pyspark.enabled": True}):
+            pandas_df = pd.DataFrame({"col": [1, 2, 3, None]}, dtype="Int64")
+            df = self.spark.createDataFrame(pandas_df)
+            assert_frame_equal(pandas_df, df.toPandas(), check_dtype=False)
+
     def test_toPandas_with_map_type(self):
         pdf = pd.DataFrame(
             {"id": [0, 1, 2, 3], "m": [{}, {"a": 1}, {"a": 1, "b": 2}, {"a": 1, "b": 2, "c": 3}]}
