diff --git a/connector/connect/common/src/test/resources/query-tests/explain-results/unionByName.explain b/connector/connect/common/src/test/resources/query-tests/explain-results/unionByName.explain
index ed960186ad4..6ec8eb37f50 100644
--- a/connector/connect/common/src/test/resources/query-tests/explain-results/unionByName.explain
+++ b/connector/connect/common/src/test/resources/query-tests/explain-results/unionByName.explain
@@ -2,4 +2,5 @@ Union false, false
 :- Project [id#0L, a#0]
 :  +- LocalRelation <empty>, [id#0L, a#0, b#0]
 +- Project [id#0L, a#0]
-   +- LocalRelation <empty>, [a#0, id#0L, payload#0]
+   +- Project [a#0, id#0L]
+      +- LocalRelation <empty>, [a#0, id#0L, payload#0]
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveUnion.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveUnion.scala
index 4aa5c3ebf5a..7b6edc849b0 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveUnion.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveUnion.scala
@@ -20,7 +20,6 @@ package org.apache.spark.sql.catalyst.analysis
 import scala.collection.mutable
 
 import org.apache.spark.sql.catalyst.expressions._
-import org.apache.spark.sql.catalyst.optimizer.CombineUnions
 import org.apache.spark.sql.catalyst.plans.logical.{LogicalPlan, Project, Union}
 import org.apache.spark.sql.catalyst.rules.Rule
 import org.apache.spark.sql.catalyst.trees.TreePattern.UNION
@@ -207,10 +206,9 @@ object ResolveUnion extends Rule[LogicalPlan] {
     case e if !e.childrenResolved => e
 
     case Union(children, byName, allowMissingCol) if byName =>
-      val union = children.reduceLeft { (left, right) =>
+      children.reduceLeft { (left, right) =>
         checkColumnNames(left, right)
         unionTwoSides(left, right, allowMissingCol)
       }
-      CombineUnions(union)
   }
 }
diff --git a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ResolveUnionSuite.scala b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ResolveUnionSuite.scala
index 5c7ad0067a4..820b517f257 100644
--- a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ResolveUnionSuite.scala
+++ b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ResolveUnionSuite.scala
@@ -69,7 +69,7 @@ class ResolveUnionSuite extends AnalysisTest {
     val nullAttr2 = Alias(Literal(null, DoubleType), "d")()
     val projected3 =
       Project(Seq(table2.output(3), table2.output(0), nullAttr1, nullAttr2), table4)
-    val expected3 = Union(table1 :: projected2 :: projected3 :: Nil)
+    val expected3 = Union(Union(table1 :: projected2 :: Nil) :: projected3 :: Nil)
     comparePlans(analyzed3, expected3)
   }
 }
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala b/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala
index eda017937d9..fd8421fa096 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala
@@ -2408,7 +2408,11 @@ class Dataset[T] private[sql](
    * @since 3.1.0
    */
   def unionByName(other: Dataset[T], allowMissingColumns: Boolean): Dataset[T] = withSetOperator {
-    combineUnions(Union(logicalPlan :: other.logicalPlan :: Nil, true, allowMissingColumns))
+    // We need to resolve the by-name Union first, as the underlying Unions are already resolved
+    // and we can only combine adjacent Unions if they are all resolved.
+    val resolvedUnion = sparkSession.sessionState.executePlan(
+      Union(logicalPlan :: other.logicalPlan :: Nil, true, allowMissingColumns))
+    combineUnions(resolvedUnion.analyzed)
   }
 
   /**
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/DatasetCacheSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/DatasetCacheSuite.scala
index a657c6212aa..e3f87228068 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/DatasetCacheSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/DatasetCacheSuite.scala
@@ -279,19 +279,34 @@ class DatasetCacheSuite extends QueryTest
     val df2 = Seq(2 -> 2).toDF("i", "j")
     val df3 = Seq(3 -> 3).toDF("i", "j")
 
-    withClue("positive") {
+    withClue("positive: union by position") {
       val unionDf = df1.union(df2).select($"i")
       unionDf.cache()
       val finalDf = unionDf.union(df3.select($"i"))
       assert(finalDf.queryExecution.executedPlan.exists(_.isInstanceOf[InMemoryTableScanExec]))
     }
 
-    withClue("negative") {
+    withClue("positive: union by name") {
+      val unionDf = df1.unionByName(df2).select($"i")
+      unionDf.cache()
+      val finalDf = unionDf.unionByName(df3.select($"i"))
+      assert(finalDf.queryExecution.executedPlan.exists(_.isInstanceOf[InMemoryTableScanExec]))
+    }
+
+    withClue("negative: union by position") {
       val unionDf = df1.union(df2)
       unionDf.cache()
       val finalDf = unionDf.union(df3)
       // It's by design to break caching here.
       assert(!finalDf.queryExecution.executedPlan.exists(_.isInstanceOf[InMemoryTableScanExec]))
     }
+
+    withClue("negative: union by name") {
+      val unionDf = df1.unionByName(df2)
+      unionDf.cache()
+      val finalDf = unionDf.unionByName(df3)
+      // It's by design to break caching here.
+      assert(!finalDf.queryExecution.executedPlan.exists(_.isInstanceOf[InMemoryTableScanExec]))
+    }
   }
 }
