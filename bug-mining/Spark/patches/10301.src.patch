diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/python/streaming/TransformWithStateInPySparkDeserializer.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/python/streaming/TransformWithStateInPySparkDeserializer.scala
index 25f84f0be4c..50859560659 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/python/streaming/TransformWithStateInPySparkDeserializer.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/python/streaming/TransformWithStateInPySparkDeserializer.scala
@@ -37,7 +37,7 @@ import org.apache.spark.sql.vectorized.{ArrowColumnVector, ColumnarBatch, Column
  */
 class TransformWithStateInPySparkDeserializer(deserializer: ExpressionEncoder.Deserializer[Row])
   extends Logging {
-  private val allocator = ArrowUtils.rootAllocator.newChildAllocator(
+  private lazy val allocator = ArrowUtils.rootAllocator.newChildAllocator(
         s"stdin reader for transformWithStateInPySpark state socket", 0, Long.MaxValue)
 
   /**
@@ -78,4 +78,8 @@ class TransformWithStateInPySparkDeserializer(deserializer: ExpressionEncoder.De
 
     rows.toSeq
   }
+
+  def close(): Unit = {
+    allocator.close()
+  }
 }
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/python/streaming/TransformWithStateInPySparkStateServer.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/python/streaming/TransformWithStateInPySparkStateServer.scala
index 59acf434035..e2633dbbb13 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/python/streaming/TransformWithStateInPySparkStateServer.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/python/streaming/TransformWithStateInPySparkStateServer.scala
@@ -457,70 +457,78 @@ class TransformWithStateInPySparkStateServer(
       return
     }
     val listStateInfo = listStates(stateName)
-    val deserializer = if (deserializerForTest != null) {
-      deserializerForTest
-    } else {
-      new TransformWithStateInPySparkDeserializer(listStateInfo.deserializer)
-    }
-    message.getMethodCase match {
-      case ListStateCall.MethodCase.EXISTS =>
-        if (listStateInfo.listState.exists()) {
+    var deserializer: TransformWithStateInPySparkDeserializer = null
+    try {
+      deserializer = if (deserializerForTest != null) {
+        deserializerForTest
+      } else {
+        new TransformWithStateInPySparkDeserializer(listStateInfo.deserializer)
+      }
+      message.getMethodCase match {
+        case ListStateCall.MethodCase.EXISTS =>
+          if (listStateInfo.listState.exists()) {
+            sendResponse(0)
+          } else {
+            // Send status code 2 to indicate that the list state doesn't have a value yet.
+            sendResponse(2, s"state $stateName doesn't exist")
+          }
+        case ListStateCall.MethodCase.LISTSTATEPUT =>
+          val rows = if (message.getListStatePut.getFetchWithArrow) {
+            deserializer.readArrowBatches(inputStream)
+          } else {
+            val elements = message.getListStatePut.getValueList.asScala
+            elements.map { e =>
+              PythonSQLUtils.toJVMRow(
+                e.toByteArray,
+                listStateInfo.schema,
+                listStateInfo.deserializer)
+            }
+          }
+          listStateInfo.listState.put(rows.toArray)
           sendResponse(0)
-        } else {
-          // Send status code 2 to indicate that the list state doesn't have a value yet.
-          sendResponse(2, s"state $stateName doesn't exist")
-        }
-      case ListStateCall.MethodCase.LISTSTATEPUT =>
-        val rows = if (message.getListStatePut.getFetchWithArrow) {
-          deserializer.readArrowBatches(inputStream)
-        } else {
-          val elements = message.getListStatePut.getValueList.asScala
-          elements.map { e =>
-            PythonSQLUtils.toJVMRow(
-              e.toByteArray,
-              listStateInfo.schema,
-              listStateInfo.deserializer)
+        case ListStateCall.MethodCase.LISTSTATEGET =>
+          val iteratorId = message.getListStateGet.getIteratorId
+          var iteratorOption = iterators.get(iteratorId)
+          if (iteratorOption.isEmpty) {
+            iteratorOption = Some(listStateInfo.listState.get())
+            iterators.put(iteratorId, iteratorOption.get)
           }
-        }
-        listStateInfo.listState.put(rows.toArray)
-        sendResponse(0)
-      case ListStateCall.MethodCase.LISTSTATEGET =>
-        val iteratorId = message.getListStateGet.getIteratorId
-        var iteratorOption = iterators.get(iteratorId)
-        if (iteratorOption.isEmpty) {
-          iteratorOption = Some(listStateInfo.listState.get())
-          iterators.put(iteratorId, iteratorOption.get)
-        }
-        if (!iteratorOption.get.hasNext) {
-          sendResponse(2, s"List state $stateName doesn't contain any value.")
-        } else {
-          sendResponseWithListGet(0, iter = iteratorOption.get)
-        }
-      case ListStateCall.MethodCase.APPENDVALUE =>
-        val byteArray = message.getAppendValue.getValue.toByteArray
-        val newRow = PythonSQLUtils.toJVMRow(byteArray, listStateInfo.schema,
-          listStateInfo.deserializer)
-        listStateInfo.listState.appendValue(newRow)
-        sendResponse(0)
-      case ListStateCall.MethodCase.APPENDLIST =>
-        val rows = if (message.getAppendList.getFetchWithArrow) {
-          deserializer.readArrowBatches(inputStream)
-        } else {
-          val elements = message.getAppendList.getValueList.asScala
-          elements.map { e =>
-            PythonSQLUtils.toJVMRow(
-              e.toByteArray,
-              listStateInfo.schema,
-              listStateInfo.deserializer)
+          if (!iteratorOption.get.hasNext) {
+            sendResponse(2, s"List state $stateName doesn't contain any value.")
+          } else {
+            sendResponseWithListGet(0, iter = iteratorOption.get)
           }
-        }
-        listStateInfo.listState.appendList(rows.toArray)
-        sendResponse(0)
-      case ListStateCall.MethodCase.CLEAR =>
-        listStates(stateName).listState.clear()
-        sendResponse(0)
-      case _ =>
-        throw new IllegalArgumentException("Invalid method call")
+        case ListStateCall.MethodCase.APPENDVALUE =>
+          val byteArray = message.getAppendValue.getValue.toByteArray
+          val newRow =
+            PythonSQLUtils.toJVMRow(byteArray, listStateInfo.schema, listStateInfo.deserializer)
+          listStateInfo.listState.appendValue(newRow)
+          sendResponse(0)
+        case ListStateCall.MethodCase.APPENDLIST =>
+          val rows = if (message.getAppendList.getFetchWithArrow) {
+            deserializer.readArrowBatches(inputStream)
+          } else {
+            val elements = message.getAppendList.getValueList.asScala
+            elements.map { e =>
+              PythonSQLUtils.toJVMRow(
+                e.toByteArray,
+                listStateInfo.schema,
+                listStateInfo.deserializer)
+            }
+          }
+          listStateInfo.listState.appendList(rows.toArray)
+          sendResponse(0)
+        case ListStateCall.MethodCase.CLEAR =>
+          listStates(stateName).listState.clear()
+          sendResponse(0)
+        case _ =>
+          throw new IllegalArgumentException("Invalid method call")
+      }
+    } finally {
+      // Close the deserializer to free up resources.
+      if (deserializer != null) {
+        deserializer.close()
+      }
     }
   }
 
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/execution/python/streaming/TransformWithStateInPySparkStateServerSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/execution/python/streaming/TransformWithStateInPySparkStateServerSuite.scala
index 013aa375c30..e253a6aa45c 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/execution/python/streaming/TransformWithStateInPySparkStateServerSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/execution/python/streaming/TransformWithStateInPySparkStateServerSuite.scala
@@ -329,6 +329,7 @@ class TransformWithStateInPySparkStateServerSuite extends SparkFunSuite with Bef
     stateServer.handleListStateRequest(message)
     // Verify that the data is not read from Arrow stream. It is inlined.
     verify(transformWithStateInPySparkDeserializer, times(0)).readArrowBatches(any)
+    verify(transformWithStateInPySparkDeserializer).close()
     verify(listState).put(any)
   }
 
@@ -337,6 +338,7 @@ class TransformWithStateInPySparkStateServerSuite extends SparkFunSuite with Bef
       .setListStatePut(ListStatePut.newBuilder().setFetchWithArrow(true).build()).build()
     stateServer.handleListStateRequest(message)
     verify(transformWithStateInPySparkDeserializer).readArrowBatches(any)
+    verify(transformWithStateInPySparkDeserializer).close()
     verify(listState).put(any)
   }
 
@@ -345,6 +347,7 @@ class TransformWithStateInPySparkStateServerSuite extends SparkFunSuite with Bef
     val message = ListStateCall.newBuilder().setStateName(stateName)
       .setAppendValue(AppendValue.newBuilder().setValue(byteString).build()).build()
     stateServer.handleListStateRequest(message)
+    verify(transformWithStateInPySparkDeserializer).close()
     verify(listState).appendValue(any[Row])
   }
 
@@ -354,6 +357,7 @@ class TransformWithStateInPySparkStateServerSuite extends SparkFunSuite with Bef
     stateServer.handleListStateRequest(message)
     // Verify that the data is not read from Arrow stream. It is inlined.
     verify(transformWithStateInPySparkDeserializer, times(0)).readArrowBatches(any)
+    verify(transformWithStateInPySparkDeserializer).close()
     verify(listState).appendList(any)
   }
 
@@ -362,6 +366,7 @@ class TransformWithStateInPySparkStateServerSuite extends SparkFunSuite with Bef
       .setAppendList(AppendList.newBuilder().setFetchWithArrow(true).build()).build()
     stateServer.handleListStateRequest(message)
     verify(transformWithStateInPySparkDeserializer).readArrowBatches(any)
+    verify(transformWithStateInPySparkDeserializer).close()
     verify(listState).appendList(any)
   }
 
