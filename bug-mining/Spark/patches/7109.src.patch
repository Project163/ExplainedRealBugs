diff --git a/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala b/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala
index 0b09f1fa2db..29ad9ceae6d 100644
--- a/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala
+++ b/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala
@@ -645,7 +645,6 @@ private[spark] class ExecutorAllocationManager(
         resourceProfileIdToStageAttempt.getOrElseUpdate(
           profId, new mutable.HashSet[StageAttempt]) += stageAttempt
         numExecutorsToAddPerResourceProfileId.getOrElseUpdate(profId, 1)
-        numExecutorsTargetPerResourceProfileId.getOrElseUpdate(profId, initialNumExecutors)
 
         // Compute the number of tasks requested by the stage on each host
         var numTasksPending = 0
@@ -661,9 +660,20 @@ private[spark] class ExecutorAllocationManager(
         }
         stageAttemptToExecutorPlacementHints.put(stageAttempt,
           (numTasksPending, hostToLocalTaskCountPerStage.toMap, profId))
-
         // Update the executor placement hints
         updateExecutorPlacementHints()
+
+        if (!numExecutorsTargetPerResourceProfileId.contains(profId)) {
+          numExecutorsTargetPerResourceProfileId.put(profId, initialNumExecutors)
+          if (initialNumExecutors > 0) {
+            logDebug(s"requesting executors, rpId: $profId, initial number is $initialNumExecutors")
+            // we need to trigger a schedule since we add an initial number here.
+            client.requestTotalExecutors(
+              numExecutorsTargetPerResourceProfileId.toMap,
+              numLocalityAwareTasksPerResourceProfileId.toMap,
+              rpIdToHostToLocalTaskCount)
+          }
+        }
       }
     }
 
diff --git a/core/src/test/scala/org/apache/spark/ExecutorAllocationManagerSuite.scala b/core/src/test/scala/org/apache/spark/ExecutorAllocationManagerSuite.scala
index 9fac1199a80..807f0eb808f 100644
--- a/core/src/test/scala/org/apache/spark/ExecutorAllocationManagerSuite.scala
+++ b/core/src/test/scala/org/apache/spark/ExecutorAllocationManagerSuite.scala
@@ -22,7 +22,7 @@ import java.util.concurrent.TimeUnit
 import scala.collection.mutable
 
 import org.mockito.ArgumentMatchers.{any, eq => meq}
-import org.mockito.Mockito.{mock, never, verify, when}
+import org.mockito.Mockito.{mock, never, times, verify, when}
 import org.scalatest.PrivateMethodTester
 
 import org.apache.spark.executor.ExecutorMetrics
@@ -265,6 +265,26 @@ class ExecutorAllocationManagerSuite extends SparkFunSuite {
     assert(numExecutorsTarget(manager, rprof1.id) === 10)
   }
 
+  test("add executors multiple profiles initial num same as needed") {
+    // test when the initial number of executors equals the number needed for the first
+    // stage using a non default profile to make sure we request the intitial number
+    // properly. Here initial is 2, each executor in ResourceProfile 1 can have 2 tasks
+    // per executor, and start a stage with 4 tasks, which would need 2 executors.
+    val clock = new ManualClock(8888L)
+    val manager = createManager(createConf(0, 10, 2), clock)
+    val rp1 = new ResourceProfileBuilder()
+    val execReqs = new ExecutorResourceRequests().cores(2).resource("gpu", 2)
+    val taskReqs = new TaskResourceRequests().cpus(1).resource("gpu", 1)
+    rp1.require(execReqs).require(taskReqs)
+    val rprof1 = rp1.build
+    rpManager.addResourceProfile(rprof1)
+    when(client.requestTotalExecutors(any(), any(), any())).thenReturn(true)
+    post(SparkListenerStageSubmitted(createStageInfo(1, 4, rp = rprof1)))
+    // called once on start and a second time on stage submit with initial number
+    verify(client, times(2)).requestTotalExecutors(any(), any(), any())
+    assert(numExecutorsTarget(manager, rprof1.id) === 2)
+  }
+
   test("remove executors multiple profiles") {
     val manager = createManager(createConf(5, 10, 5))
     val rp1 = new ResourceProfileBuilder()
