diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala
index 5571178832d..9fc4873c248 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala
@@ -20,15 +20,16 @@ package org.apache.spark.sql.catalyst.optimizer
 import scala.annotation.tailrec
 import scala.util.control.NonFatal
 
-import org.apache.spark.internal.LogKeys.JOIN_CONDITION
-import org.apache.spark.internal.MDC
+import org.apache.spark.internal.{Logging, MDC}
+import org.apache.spark.internal.LogKeys.{HASH_JOIN_KEYS, JOIN_CONDITION}
 import org.apache.spark.sql.catalyst.expressions._
 import org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression
-import org.apache.spark.sql.catalyst.planning.{ExtractEquiJoinKeys, ExtractFiltersAndInnerJoins}
+import org.apache.spark.sql.catalyst.planning.{ExtractEquiJoinKeys, ExtractFiltersAndInnerJoins, ExtractSingleColumnNullAwareAntiJoin}
 import org.apache.spark.sql.catalyst.plans._
 import org.apache.spark.sql.catalyst.plans.logical._
 import org.apache.spark.sql.catalyst.rules._
 import org.apache.spark.sql.catalyst.trees.TreePattern._
+import org.apache.spark.sql.catalyst.util.UnsafeRowUtils
 import org.apache.spark.sql.errors.QueryCompilationErrors
 import org.apache.spark.sql.internal.SQLConf
 import org.apache.spark.util.Utils
@@ -286,7 +287,7 @@ case object BuildRight extends BuildSide
 
 case object BuildLeft extends BuildSide
 
-trait JoinSelectionHelper {
+trait JoinSelectionHelper extends Logging {
 
   def getBroadcastBuildSide(
       join: Join,
@@ -394,9 +395,32 @@ trait JoinSelectionHelper {
     }
   }
 
-  def canPlanAsBroadcastHashJoin(join: Join, conf: SQLConf): Boolean = {
-    getBroadcastBuildSide(join, hintOnly = true, conf).isDefined ||
-      getBroadcastBuildSide(join, hintOnly = false, conf).isDefined
+  protected def hashJoinSupported
+      (leftKeys: Seq[Expression], rightKeys: Seq[Expression]): Boolean = {
+    val result = leftKeys.concat(rightKeys).forall(e => UnsafeRowUtils.isBinaryStable(e.dataType))
+    if (!result) {
+      val keysNotSupportingHashJoin = leftKeys.concat(rightKeys).filterNot(
+        e => UnsafeRowUtils.isBinaryStable(e.dataType))
+      logWarning(log"Hash based joins are not supported due to joining on keys that don't " +
+        log"support binary equality. Keys not supporting hash joins: " +
+        log"${
+          MDC(HASH_JOIN_KEYS, keysNotSupportingHashJoin.map(
+            e => e.toString + " due to DataType: " + e.dataType.typeName).mkString(", "))
+        }")
+    }
+    result
+  }
+
+  def canPlanAsBroadcastHashJoin(join: Join, conf: SQLConf): Boolean = join match {
+    case ExtractEquiJoinKeys(_, leftKeys, rightKeys, _, _, _, _, _) =>
+      val hashJoinSupport = hashJoinSupported(leftKeys, rightKeys)
+      val noShufflePlannedBefore =
+        !hashJoinSupport || getShuffleHashJoinBuildSide(join, hintOnly = true, conf).isEmpty
+      getBroadcastBuildSide(join, hintOnly = true, conf).isDefined ||
+        (noShufflePlannedBefore &&
+          getBroadcastBuildSide(join, hintOnly = false, conf).isDefined)
+    case ExtractSingleColumnNullAwareAntiJoin(_, _) => true
+    case _ => false
   }
 
   def canPruneLeft(joinType: JoinType): Boolean = joinType match {
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala
index 9e14d13b5cb..f0682e6b9af 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala
@@ -20,8 +20,6 @@ package org.apache.spark.sql.execution
 import java.util.Locale
 
 import org.apache.spark.{SparkException, SparkUnsupportedOperationException}
-import org.apache.spark.internal.LogKeys.HASH_JOIN_KEYS
-import org.apache.spark.internal.MDC
 import org.apache.spark.rdd.RDD
 import org.apache.spark.sql.{execution, AnalysisException, Strategy}
 import org.apache.spark.sql.catalyst.InternalRow
@@ -33,7 +31,6 @@ import org.apache.spark.sql.catalyst.plans._
 import org.apache.spark.sql.catalyst.plans.logical._
 import org.apache.spark.sql.catalyst.streaming.{InternalOutputModes, StreamingRelationV2}
 import org.apache.spark.sql.catalyst.types.DataTypeUtils
-import org.apache.spark.sql.catalyst.util.UnsafeRowUtils
 import org.apache.spark.sql.errors.{QueryCompilationErrors, QueryExecutionErrors}
 import org.apache.spark.sql.execution.aggregate.AggUtils
 import org.apache.spark.sql.execution.columnar.{InMemoryRelation, InMemoryTableScanExec}
@@ -208,20 +205,6 @@ abstract class SparkStrategies extends QueryPlanner[SparkPlan] {
       }
     }
 
-    private def hashJoinSupported
-        (leftKeys: Seq[Expression], rightKeys: Seq[Expression]): Boolean = {
-      val result = leftKeys.concat(rightKeys).forall(e => UnsafeRowUtils.isBinaryStable(e.dataType))
-      if (!result) {
-        val keysNotSupportingHashJoin = leftKeys.concat(rightKeys).filterNot(
-          e => UnsafeRowUtils.isBinaryStable(e.dataType))
-        logWarning(log"Hash based joins are not supported due to joining on keys that don't " +
-          log"support binary equality. Keys not supporting hash joins: " +
-          log"${MDC(HASH_JOIN_KEYS, keysNotSupportingHashJoin.map(
-            e => e.toString + " due to DataType: " + e.dataType.typeName).mkString(", "))}")
-      }
-      result
-    }
-
     def apply(plan: LogicalPlan): Seq[SparkPlan] = plan match {
 
       // If it is an equi-join, we first look at the join hints w.r.t. the following order:
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/JoinSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/JoinSuite.scala
index be6862f5b96..fcb937d82ba 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/JoinSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/JoinSuite.scala
@@ -27,7 +27,7 @@ import org.apache.spark.internal.config.SHUFFLE_SPILL_NUM_ELEMENTS_FORCE_SPILL_T
 import org.apache.spark.sql.catalyst.TableIdentifier
 import org.apache.spark.sql.catalyst.analysis.UnresolvedRelation
 import org.apache.spark.sql.catalyst.expressions.{Ascending, GenericRow, SortOrder}
-import org.apache.spark.sql.catalyst.optimizer.{BuildLeft, BuildRight}
+import org.apache.spark.sql.catalyst.optimizer.{BuildLeft, BuildRight, JoinSelectionHelper}
 import org.apache.spark.sql.catalyst.plans.logical.{Filter, HintInfo, Join, JoinHint, NO_BROADCAST_AND_REPLICATION}
 import org.apache.spark.sql.execution.{BinaryExecNode, FilterExec, ProjectExec, SortExec, SparkPlan, WholeStageCodegenExec}
 import org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanHelper
@@ -40,7 +40,8 @@ import org.apache.spark.sql.types.StructType
 import org.apache.spark.tags.SlowSQLTest
 
 @SlowSQLTest
-class JoinSuite extends QueryTest with SharedSparkSession with AdaptiveSparkPlanHelper {
+class JoinSuite extends QueryTest with SharedSparkSession with AdaptiveSparkPlanHelper
+  with JoinSelectionHelper {
   import testImplicits._
 
   setupTestData()
@@ -61,6 +62,7 @@ class JoinSuite extends QueryTest with SharedSparkSession with AdaptiveSparkPlan
     val sqlString = pair._1
     val c = pair._2
     val df = sql(sqlString)
+    val optimized = df.queryExecution.optimizedPlan
     val physical = df.queryExecution.sparkPlan
     val operators = physical.collect {
       case j: BroadcastHashJoinExec => j
@@ -74,6 +76,10 @@ class JoinSuite extends QueryTest with SharedSparkSession with AdaptiveSparkPlan
     if (operators.head.getClass != c) {
       fail(s"$sqlString expected operator: $c, but got ${operators.head}\n physical: \n$physical")
     }
+    assert(
+      canPlanAsBroadcastHashJoin(optimized.asInstanceOf[Join], conf) ===
+        operators.head.isInstanceOf[BroadcastHashJoinExec],
+      "canPlanAsBroadcastHashJoin not in sync with join selection codepath!")
     operators.head
   }
 
@@ -89,11 +95,13 @@ class JoinSuite extends QueryTest with SharedSparkSession with AdaptiveSparkPlan
       val planned = spark.sessionState.planner.JoinSelection(join)
       assert(planned.size == 1)
       assert(planned.head.isInstanceOf[CartesianProductExec])
+      assert(!canPlanAsBroadcastHashJoin(join, conf))
 
       val plannedWithHint = spark.sessionState.planner.JoinSelection(joinWithHint)
       assert(plannedWithHint.size == 1)
       assert(plannedWithHint.head.isInstanceOf[BroadcastNestedLoopJoinExec])
       assert(plannedWithHint.head.asInstanceOf[BroadcastNestedLoopJoinExec].buildSide == BuildLeft)
+      assert(!canPlanAsBroadcastHashJoin(joinWithHint, conf))
     }
   }
 
@@ -112,10 +120,12 @@ class JoinSuite extends QueryTest with SharedSparkSession with AdaptiveSparkPlan
     val planned = spark.sessionState.planner.JoinSelection(join)
     assert(planned.size == 1)
     assert(planned.head.isInstanceOf[BroadcastHashJoinExec])
+    assert(canPlanAsBroadcastHashJoin(join, conf))
 
     val plannedWithHint = spark.sessionState.planner.JoinSelection(joinWithHint)
     assert(plannedWithHint.size == 1)
     assert(plannedWithHint.head.isInstanceOf[SortMergeJoinExec])
+    assert(!canPlanAsBroadcastHashJoin(joinWithHint, conf))
   }
 
   test("NO_BROADCAST_AND_REPLICATION controls build side in BNLJ") {
@@ -131,11 +141,13 @@ class JoinSuite extends QueryTest with SharedSparkSession with AdaptiveSparkPlan
     assert(planned.size == 1)
     assert(planned.head.isInstanceOf[BroadcastNestedLoopJoinExec])
     assert(planned.head.asInstanceOf[BroadcastNestedLoopJoinExec].buildSide == BuildRight)
+    assert(!canPlanAsBroadcastHashJoin(join, conf))
 
     val plannedWithHint = spark.sessionState.planner.JoinSelection(joinWithHint)
     assert(plannedWithHint.size == 1)
     assert(plannedWithHint.head.isInstanceOf[BroadcastNestedLoopJoinExec])
     assert(plannedWithHint.head.asInstanceOf[BroadcastNestedLoopJoinExec].buildSide == BuildLeft)
+    assert(!canPlanAsBroadcastHashJoin(joinWithHint, conf))
   }
 
   test("join operator selection") {
@@ -191,6 +203,16 @@ class JoinSuite extends QueryTest with SharedSparkSession with AdaptiveSparkPlan
 //    ).foreach { case (query, joinClass) => assertJoin(query, joinClass) }
 //  }
 
+  test("broadcastable join with shuffle join hint") {
+    spark.sharedState.cacheManager.clearCache()
+    sql("CACHE TABLE testData")
+    // Make sure it's planned as broadcast join without the hint.
+    assertJoin("SELECT * FROM testData JOIN testData2 ON key = a",
+      classOf[BroadcastHashJoinExec])
+    assertJoin("SELECT /*+ SHUFFLE_HASH(testData) */ * FROM testData JOIN testData2 ON key = a",
+      classOf[ShuffledHashJoinExec])
+  }
+
   test("broadcasted hash join operator selection") {
     spark.sharedState.cacheManager.clearCache()
     sql("CACHE TABLE testData")
