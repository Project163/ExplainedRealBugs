diff --git a/python/pyspark/sql/connect/plan.py b/python/pyspark/sql/connect/plan.py
index 2184744d3c1..ebfe23d16d1 100644
--- a/python/pyspark/sql/connect/plan.py
+++ b/python/pyspark/sql/connect/plan.py
@@ -1179,8 +1179,11 @@ class SQL(LogicalPlan):
             if isinstance(args, Dict):
                 for k, v in args.items():
                     assert isinstance(k, str)
-            else:
-                assert isinstance(args, List)
+            elif not isinstance(args, List):
+                raise PySparkTypeError(
+                    error_class="INVALID_TYPE",
+                    message_parameters={"arg_name": "args", "arg_type": str(type(args))},
+                )
 
         self._query = query
         self._args = args
diff --git a/python/pyspark/sql/session.py b/python/pyspark/sql/session.py
index df07cb84a76..53c97b8d43f 100644
--- a/python/pyspark/sql/session.py
+++ b/python/pyspark/sql/session.py
@@ -1679,11 +1679,16 @@ class SparkSession(SparkConversionMixin):
         try:
             if isinstance(args, Dict):
                 litArgs = {k: _to_java_column(lit(v)) for k, v in (args or {}).items()}
-            else:
+            elif args is None or isinstance(args, List):
                 assert self._jvm is not None
                 litArgs = self._jvm.PythonUtils.toArray(
                     [_to_java_column(lit(v)) for v in (args or [])]
                 )
+            else:
+                raise PySparkTypeError(
+                    error_class="INVALID_TYPE",
+                    message_parameters={"arg_name": "args", "arg_type": str(type(args))},
+                )
             return DataFrame(self._jsparkSession.sql(sqlQuery, litArgs), self)
         finally:
             if len(kwargs) > 0:
diff --git a/python/pyspark/sql/tests/connect/test_connect_basic.py b/python/pyspark/sql/tests/connect/test_connect_basic.py
index 88c4754029b..8aef2d8a852 100755
--- a/python/pyspark/sql/tests/connect/test_connect_basic.py
+++ b/python/pyspark/sql/tests/connect/test_connect_basic.py
@@ -1400,6 +1400,18 @@ class SparkConnectBasicTests(SparkConnectSQLTestCase):
         df2 = self.spark.sql(sqlText, args=[SF.array(SF.lit(1)), 7])
         self.assert_eq(df.toPandas(), df2.toPandas())
 
+    def test_sql_with_invalid_args(self):
+        sqlText = "SELECT ?, ?, ?"
+        for session in [self.connect, self.spark]:
+            with self.assertRaises(PySparkTypeError) as pe:
+                session.sql(sqlText, args={1, 2, 3})
+
+            self.check_error(
+                exception=pe.exception,
+                error_class="INVALID_TYPE",
+                message_parameters={"arg_name": "args", "arg_type": "<class 'set'>"},
+            )
+
     def test_head(self):
         # SPARK-41002: test `head` API in Python Client
         df = self.connect.read.table(self.tbl_name)
