diff --git a/core/src/test/scala/org/apache/spark/storage/BlockManagerDecommissionIntegrationSuite.scala b/core/src/test/scala/org/apache/spark/storage/BlockManagerDecommissionIntegrationSuite.scala
index ddec2ee7b9e..8999a121bcd 100644
--- a/core/src/test/scala/org/apache/spark/storage/BlockManagerDecommissionIntegrationSuite.scala
+++ b/core/src/test/scala/org/apache/spark/storage/BlockManagerDecommissionIntegrationSuite.scala
@@ -81,25 +81,25 @@ class BlockManagerDecommissionIntegrationSuite extends SparkFunSuite with LocalS
     }
   }
 
-  testRetry(s"verify that an already running task which is going to cache data succeeds " +
-    s"on a decommissioned executor after task start") {
+  testRetry("verify that an already running task which is going to cache data succeeds " +
+    "on a decommissioned executor after task start") {
     runDecomTest(true, false, TaskStarted)
   }
 
-  test(s"verify that an already running task which is going to cache data succeeds " +
-    s"on a decommissioned executor after one task ends but before job ends") {
+  test("verify that an already running task which is going to cache data succeeds " +
+    "on a decommissioned executor after one task ends but before job ends") {
     runDecomTest(true, false, TaskEnded)
   }
 
-  test(s"verify that shuffle blocks are migrated") {
+  test("verify that shuffle blocks are migrated") {
     runDecomTest(false, true, JobEnded)
   }
 
-  test(s"verify that both migrations can work at the same time") {
+  test("verify that both migrations can work at the same time") {
     runDecomTest(true, true, JobEnded)
   }
 
-  test(s"SPARK-36782 not deadlock if MapOutput uses broadcast") {
+  test("SPARK-36782 not deadlock if MapOutput uses broadcast") {
     runDecomTest(false, true, JobEnded, forceMapOutputBroadcast = true)
   }
 
@@ -141,25 +141,25 @@ class BlockManagerDecommissionIntegrationSuite extends SparkFunSuite with LocalS
     val input = sc.parallelize(1 to numParts, numParts)
     val accum = sc.longAccumulator("mapperRunAccumulator")
 
-    val sleepIntervalMs = whenToDecom match {
-      // Increase the window of time b/w task started and ended so that we can decom within that.
-      case TaskStarted => 10000
-      // Make one task take a really short time so that we can decommission right after it is
-      // done but before its peers are done.
-      case TaskEnded =>
-        if (TaskContext.getPartitionId() == 0) {
-          100
-        } else {
-          1000
-        }
-      // No sleep otherwise
-      case _ => 0
-    }
-
     // Create a new RDD where we have sleep in each partition, we are also increasing
     // the value of accumulator in each partition
     val baseRdd = input.mapPartitions { x =>
       accum.add(1)
+
+      val sleepIntervalMs = whenToDecom match {
+        // Increase the window of time b/w task started and ended so that we can decom within that.
+        case "TASK_STARTED" => 10000
+        // Make one task take a really short time so that we can decommission right after it is
+        // done but before its peers are done.
+        case "TASK_ENDED" =>
+          if (TaskContext.getPartitionId() == 0) {
+            100
+          } else {
+            1000
+          }
+        // No sleep otherwise
+        case _ => 0
+      }
       if (sleepIntervalMs > 0) {
         Thread.sleep(sleepIntervalMs)
       }
