diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/ParseDriver.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/ParseDriver.scala
index 727d35d5c91..1fb23c4a71e 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/ParseDriver.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/ParseDriver.scala
@@ -114,25 +114,28 @@ abstract class AbstractSqlParser extends ParserInterface with SQLConfHelper with
     parser.addParseListener(UnclosedCommentProcessor(command, tokenStream))
     parser.removeErrorListeners()
     parser.addErrorListener(ParseErrorListener)
-    parser.setErrorHandler(new SparkParserErrorStrategy())
     parser.legacy_setops_precedence_enabled = conf.setOpsPrecedenceEnforced
     parser.legacy_exponent_literal_as_decimal_enabled = conf.exponentLiteralAsDecimalEnabled
     parser.SQL_standard_keyword_behavior = conf.enforceReservedKeywords
     parser.double_quoted_identifiers = conf.doubleQuotedIdentifiers
 
+    // https://github.com/antlr/antlr4/issues/192#issuecomment-15238595
+    // Save a great deal of time on correct inputs by using a two-stage parsing strategy.
     try {
       try {
-        // first, try parsing with potentially faster SLL mode
+        // first, try parsing with potentially faster SLL mode w/ SparkParserBailErrorStrategy
+        parser.setErrorHandler(new SparkParserBailErrorStrategy())
         parser.getInterpreter.setPredictionMode(PredictionMode.SLL)
         toResult(parser)
       }
       catch {
         case e: ParseCancellationException =>
-          // if we fail, parse with LL mode
+          // if we fail, parse with LL mode w/ SparkParserErrorStrategy
           tokenStream.seek(0) // rewind input stream
           parser.reset()
 
           // Try Again.
+          parser.setErrorHandler(new SparkParserErrorStrategy())
           parser.getInterpreter.setPredictionMode(PredictionMode.LL)
           toResult(parser)
       }
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/SparkParserErrorStrategy.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/SparkParserErrorStrategy.scala
index 9cc8fa8dcf8..99e63d78383 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/SparkParserErrorStrategy.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/SparkParserErrorStrategy.scala
@@ -18,6 +18,7 @@
 package org.apache.spark.sql.catalyst.parser
 
 import org.antlr.v4.runtime.{DefaultErrorStrategy, InputMismatchException, IntStream, NoViableAltException, Parser, ParserRuleContext, RecognitionException, Recognizer, Token}
+import org.antlr.v4.runtime.misc.ParseCancellationException
 
 /**
  * A [[SparkRecognitionException]] extends the [[RecognitionException]] with more information
@@ -112,3 +113,46 @@ class SparkParserErrorStrategy() extends DefaultErrorStrategy {
     }
   }
 }
+
+/**
+ * Inspired by [[org.antlr.v4.runtime.BailErrorStrategy]], which is used in two-stage parsing:
+ * This error strategy allows the first stage of two-stage parsing to immediately terminate
+ * if an error is encountered, and immediately fall back to the second stage. In addition to
+ * avoiding wasted work by attempting to recover from errors here, the empty implementation
+ * of sync improves the performance of the first stage.
+ */
+class SparkParserBailErrorStrategy() extends SparkParserErrorStrategy {
+
+  /**
+   * Instead of recovering from exception e, re-throw it wrapped
+   * in a [[ParseCancellationException]] so it is not caught by the
+   * rule function catches.  Use [[Exception#getCause]] to get the
+   * original [[RecognitionException]].
+   */
+  override def recover(recognizer: Parser, e: RecognitionException): Unit = {
+    var context = recognizer.getContext
+    while (context != null) {
+      context.exception = e
+      context = context.getParent
+    }
+    throw new ParseCancellationException(e)
+  }
+
+  /**
+   * Make sure we don't attempt to recover inline; if the parser
+   * successfully recovers, it won't throw an exception.
+   */
+  @throws[RecognitionException]
+  override def recoverInline(recognizer: Parser): Token = {
+    val e = new InputMismatchException(recognizer)
+    var context = recognizer.getContext
+    while (context != null) {
+      context.exception = e
+      context = context.getParent
+    }
+    throw new ParseCancellationException(e)
+  }
+
+  /** Make sure we don't attempt to recover from problems in subrules. */
+  override def sync(recognizer: Parser): Unit = {}
+}
diff --git a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala
index a16fa28b7bf..0c6aa0dd25c 100644
--- a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala
+++ b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala
@@ -66,7 +66,7 @@ class DDLParserSuite extends AnalysisTest {
     checkError(
       exception = parseException(sql),
       errorClass = "PARSE_SYNTAX_ERROR",
-      parameters = Map("error" -> "':'", "hint" -> ": extra input ':'"))
+      parameters = Map("error" -> "':'", "hint" -> ""))
   }
 
   test("create/replace table - with IF NOT EXISTS") {
diff --git a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/ExpressionParserSuite.scala b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/ExpressionParserSuite.scala
index 8d08d07249e..c011f49b7aa 100644
--- a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/ExpressionParserSuite.scala
+++ b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/ExpressionParserSuite.scala
@@ -725,7 +725,7 @@ class ExpressionParserSuite extends AnalysisTest {
     checkError(
       exception = parseException(".e3"),
       errorClass = "PARSE_SYNTAX_ERROR",
-      parameters = Map("error" -> "'.'", "hint" -> ": extra input '.'"))
+      parameters = Map("error" -> "'.'", "hint" -> ""))
 
     // Tiny Int Literal
     assertEqual("10Y", Literal(10.toByte))
diff --git a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala
index 3b5a2401335..76be620f7bc 100644
--- a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala
+++ b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala
@@ -244,6 +244,12 @@ class PlanParserSuite extends AnalysisTest {
         stop = 25))
   }
 
+  test("SPARK-42552: select and union without parentheses") {
+    val plan = Distinct(OneRowRelation().select(Literal(1))
+      .union(OneRowRelation().select(Literal(1))))
+    assertEqual("select 1 union select 1", plan)
+  }
+
   test("set operations") {
     val a = table("a").select(star())
     val b = table("b").select(star())
diff --git a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/TableSchemaParserSuite.scala b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/TableSchemaParserSuite.scala
index a7e2054dfaf..a56ab8616df 100644
--- a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/TableSchemaParserSuite.scala
+++ b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/TableSchemaParserSuite.scala
@@ -87,7 +87,7 @@ class TableSchemaParserSuite extends SparkFunSuite {
     checkError(
       exception = parseException("a INT,, b long"),
       errorClass = "PARSE_SYNTAX_ERROR",
-      parameters = Map("error" -> "','", "hint" -> ": extra input ','"))
+      parameters = Map("error" -> "','", "hint" -> ""))
     checkError(
       exception = parseException("a INT, b long,,"),
       errorClass = "PARSE_SYNTAX_ERROR",
diff --git a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/union.sql.out b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/union.sql.out
index f08678bf93b..1d78ad43569 100644
--- a/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/union.sql.out
+++ b/sql/core/src/test/resources/sql-tests/analyzer-results/postgreSQL/union.sql.out
@@ -100,29 +100,33 @@ Union false, false
 -- !query
 SELECT 1 AS three UNION SELECT 2 UNION SELECT 3 ORDER BY 1
 -- !query analysis
-org.apache.spark.sql.catalyst.parser.ParseException
-{
-  "errorClass" : "PARSE_SYNTAX_ERROR",
-  "sqlState" : "42601",
-  "messageParameters" : {
-    "error" : "'SELECT'",
-    "hint" : ""
-  }
-}
+Sort [three#x ASC NULLS FIRST], true
++- Distinct
+   +- Union false, false
+      :- Distinct
+      :  +- Union false, false
+      :     :- Project [1 AS three#x]
+      :     :  +- OneRowRelation
+      :     +- Project [2 AS 2#x]
+      :        +- OneRowRelation
+      +- Project [3 AS 3#x]
+         +- OneRowRelation
 
 
 -- !query
 SELECT 1 AS two UNION SELECT 2 UNION SELECT 2 ORDER BY 1
 -- !query analysis
-org.apache.spark.sql.catalyst.parser.ParseException
-{
-  "errorClass" : "PARSE_SYNTAX_ERROR",
-  "sqlState" : "42601",
-  "messageParameters" : {
-    "error" : "'SELECT'",
-    "hint" : ""
-  }
-}
+Sort [two#x ASC NULLS FIRST], true
++- Distinct
+   +- Union false, false
+      :- Distinct
+      :  +- Union false, false
+      :     :- Project [1 AS two#x]
+      :     :  +- OneRowRelation
+      :     +- Project [2 AS 2#x]
+      :        +- OneRowRelation
+      +- Project [2 AS 2#x]
+         +- OneRowRelation
 
 
 -- !query
@@ -221,29 +225,37 @@ Sort [two#x ASC NULLS FIRST], true
 -- !query
 SELECT 1.1 AS three UNION SELECT 2 UNION SELECT 3 ORDER BY 1
 -- !query analysis
-org.apache.spark.sql.catalyst.parser.ParseException
-{
-  "errorClass" : "PARSE_SYNTAX_ERROR",
-  "sqlState" : "42601",
-  "messageParameters" : {
-    "error" : "'SELECT'",
-    "hint" : ""
-  }
-}
+Sort [three#x ASC NULLS FIRST], true
++- Distinct
+   +- Union false, false
+      :- Distinct
+      :  +- Union false, false
+      :     :- Project [cast(three#x as decimal(11,1)) AS three#x]
+      :     :  +- Project [1.1 AS three#x]
+      :     :     +- OneRowRelation
+      :     +- Project [cast(2#x as decimal(11,1)) AS 2#x]
+      :        +- Project [2 AS 2#x]
+      :           +- OneRowRelation
+      +- Project [cast(3#x as decimal(11,1)) AS 3#x]
+         +- Project [3 AS 3#x]
+            +- OneRowRelation
 
 
 -- !query
 SELECT double(1.1) AS two UNION SELECT 2 UNION SELECT double(2.0) ORDER BY 1
 -- !query analysis
-org.apache.spark.sql.catalyst.parser.ParseException
-{
-  "errorClass" : "PARSE_SYNTAX_ERROR",
-  "sqlState" : "42601",
-  "messageParameters" : {
-    "error" : "'SELECT'",
-    "hint" : ""
-  }
-}
+Sort [two#x ASC NULLS FIRST], true
++- Distinct
+   +- Union false, false
+      :- Distinct
+      :  +- Union false, false
+      :     :- Project [cast(1.1 as double) AS two#x]
+      :     :  +- OneRowRelation
+      :     +- Project [cast(2#x as double) AS 2#x]
+      :        +- Project [2 AS 2#x]
+      :           +- OneRowRelation
+      +- Project [cast(2.0 as double) AS 2.0#x]
+         +- OneRowRelation
 
 
 -- !query
@@ -606,57 +618,59 @@ Sort [q1#xL ASC NULLS FIRST], true
 -- !query
 (SELECT 1,2,3 UNION SELECT 4,5,6) INTERSECT SELECT 4,5,6
 -- !query analysis
-org.apache.spark.sql.catalyst.parser.ParseException
-{
-  "errorClass" : "PARSE_SYNTAX_ERROR",
-  "sqlState" : "42601",
-  "messageParameters" : {
-    "error" : "'SELECT'",
-    "hint" : ""
-  }
-}
+Intersect false
+:- Distinct
+:  +- Union false, false
+:     :- Project [1 AS 1#x, 2 AS 2#x, 3 AS 3#x]
+:     :  +- OneRowRelation
+:     +- Project [4 AS 4#x, 5 AS 5#x, 6 AS 6#x]
+:        +- OneRowRelation
++- Project [4 AS 4#x, 5 AS 5#x, 6 AS 6#x]
+   +- OneRowRelation
 
 
 -- !query
 (SELECT 1,2,3 UNION SELECT 4,5,6 ORDER BY 1,2) INTERSECT SELECT 4,5,6
 -- !query analysis
-org.apache.spark.sql.catalyst.parser.ParseException
-{
-  "errorClass" : "PARSE_SYNTAX_ERROR",
-  "sqlState" : "42601",
-  "messageParameters" : {
-    "error" : "'SELECT'",
-    "hint" : ""
-  }
-}
+Intersect false
+:- Sort [1#x ASC NULLS FIRST, 2#x ASC NULLS FIRST], true
+:  +- Distinct
+:     +- Union false, false
+:        :- Project [1 AS 1#x, 2 AS 2#x, 3 AS 3#x]
+:        :  +- OneRowRelation
+:        +- Project [4 AS 4#x, 5 AS 5#x, 6 AS 6#x]
+:           +- OneRowRelation
++- Project [4 AS 4#x, 5 AS 5#x, 6 AS 6#x]
+   +- OneRowRelation
 
 
 -- !query
 (SELECT 1,2,3 UNION SELECT 4,5,6) EXCEPT SELECT 4,5,6
 -- !query analysis
-org.apache.spark.sql.catalyst.parser.ParseException
-{
-  "errorClass" : "PARSE_SYNTAX_ERROR",
-  "sqlState" : "42601",
-  "messageParameters" : {
-    "error" : "'SELECT'",
-    "hint" : ""
-  }
-}
+Except false
+:- Distinct
+:  +- Union false, false
+:     :- Project [1 AS 1#x, 2 AS 2#x, 3 AS 3#x]
+:     :  +- OneRowRelation
+:     +- Project [4 AS 4#x, 5 AS 5#x, 6 AS 6#x]
+:        +- OneRowRelation
++- Project [4 AS 4#x, 5 AS 5#x, 6 AS 6#x]
+   +- OneRowRelation
 
 
 -- !query
 (SELECT 1,2,3 UNION SELECT 4,5,6 ORDER BY 1,2) EXCEPT SELECT 4,5,6
 -- !query analysis
-org.apache.spark.sql.catalyst.parser.ParseException
-{
-  "errorClass" : "PARSE_SYNTAX_ERROR",
-  "sqlState" : "42601",
-  "messageParameters" : {
-    "error" : "'SELECT'",
-    "hint" : ""
-  }
-}
+Except false
+:- Sort [1#x ASC NULLS FIRST, 2#x ASC NULLS FIRST], true
+:  +- Distinct
+:     +- Union false, false
+:        :- Project [1 AS 1#x, 2 AS 2#x, 3 AS 3#x]
+:        :  +- OneRowRelation
+:        +- Project [4 AS 4#x, 5 AS 5#x, 6 AS 6#x]
+:           +- OneRowRelation
++- Project [4 AS 4#x, 5 AS 5#x, 6 AS 6#x]
+   +- OneRowRelation
 
 
 -- !query
@@ -1164,15 +1178,14 @@ Except All true
 -- !query
 SELECT cast('3.4' as decimal(38, 18)) UNION SELECT 'foo'
 -- !query analysis
-org.apache.spark.sql.catalyst.parser.ParseException
-{
-  "errorClass" : "PARSE_SYNTAX_ERROR",
-  "sqlState" : "42601",
-  "messageParameters" : {
-    "error" : "'SELECT'",
-    "hint" : ""
-  }
-}
+Distinct
++- Union false, false
+   :- Project [cast(CAST(3.4 AS DECIMAL(38,18))#x as double) AS CAST(3.4 AS DECIMAL(38,18))#x]
+   :  +- Project [cast(3.4 as decimal(38,18)) AS CAST(3.4 AS DECIMAL(38,18))#x]
+   :     +- OneRowRelation
+   +- Project [cast(foo#x as double) AS foo#x]
+      +- Project [foo AS foo#x]
+         +- OneRowRelation
 
 
 -- !query
diff --git a/sql/core/src/test/resources/sql-tests/results/postgreSQL/union.sql.out b/sql/core/src/test/resources/sql-tests/results/postgreSQL/union.sql.out
index 3079dd18604..e75313bb18a 100644
--- a/sql/core/src/test/resources/sql-tests/results/postgreSQL/union.sql.out
+++ b/sql/core/src/test/resources/sql-tests/results/postgreSQL/union.sql.out
@@ -73,33 +73,20 @@ struct<two:int>
 -- !query
 SELECT 1 AS three UNION SELECT 2 UNION SELECT 3 ORDER BY 1
 -- !query schema
-struct<>
+struct<three:int>
 -- !query output
-org.apache.spark.sql.catalyst.parser.ParseException
-{
-  "errorClass" : "PARSE_SYNTAX_ERROR",
-  "sqlState" : "42601",
-  "messageParameters" : {
-    "error" : "'SELECT'",
-    "hint" : ""
-  }
-}
+1
+2
+3
 
 
 -- !query
 SELECT 1 AS two UNION SELECT 2 UNION SELECT 2 ORDER BY 1
 -- !query schema
-struct<>
+struct<two:int>
 -- !query output
-org.apache.spark.sql.catalyst.parser.ParseException
-{
-  "errorClass" : "PARSE_SYNTAX_ERROR",
-  "sqlState" : "42601",
-  "messageParameters" : {
-    "error" : "'SELECT'",
-    "hint" : ""
-  }
-}
+1
+2
 
 
 -- !query
@@ -168,33 +155,20 @@ struct<two:double>
 -- !query
 SELECT 1.1 AS three UNION SELECT 2 UNION SELECT 3 ORDER BY 1
 -- !query schema
-struct<>
+struct<three:decimal(11,1)>
 -- !query output
-org.apache.spark.sql.catalyst.parser.ParseException
-{
-  "errorClass" : "PARSE_SYNTAX_ERROR",
-  "sqlState" : "42601",
-  "messageParameters" : {
-    "error" : "'SELECT'",
-    "hint" : ""
-  }
-}
+1.1
+2.0
+3.0
 
 
 -- !query
 SELECT double(1.1) AS two UNION SELECT 2 UNION SELECT double(2.0) ORDER BY 1
 -- !query schema
-struct<>
+struct<two:double>
 -- !query output
-org.apache.spark.sql.catalyst.parser.ParseException
-{
-  "errorClass" : "PARSE_SYNTAX_ERROR",
-  "sqlState" : "42601",
-  "messageParameters" : {
-    "error" : "'SELECT'",
-    "hint" : ""
-  }
-}
+1.1
+2.0
 
 
 -- !query
@@ -382,65 +356,33 @@ struct<q1:bigint>
 -- !query
 (SELECT 1,2,3 UNION SELECT 4,5,6) INTERSECT SELECT 4,5,6
 -- !query schema
-struct<>
+struct<1:int,2:int,3:int>
 -- !query output
-org.apache.spark.sql.catalyst.parser.ParseException
-{
-  "errorClass" : "PARSE_SYNTAX_ERROR",
-  "sqlState" : "42601",
-  "messageParameters" : {
-    "error" : "'SELECT'",
-    "hint" : ""
-  }
-}
+4	5	6
 
 
 -- !query
 (SELECT 1,2,3 UNION SELECT 4,5,6 ORDER BY 1,2) INTERSECT SELECT 4,5,6
 -- !query schema
-struct<>
+struct<1:int,2:int,3:int>
 -- !query output
-org.apache.spark.sql.catalyst.parser.ParseException
-{
-  "errorClass" : "PARSE_SYNTAX_ERROR",
-  "sqlState" : "42601",
-  "messageParameters" : {
-    "error" : "'SELECT'",
-    "hint" : ""
-  }
-}
+4	5	6
 
 
 -- !query
 (SELECT 1,2,3 UNION SELECT 4,5,6) EXCEPT SELECT 4,5,6
 -- !query schema
-struct<>
+struct<1:int,2:int,3:int>
 -- !query output
-org.apache.spark.sql.catalyst.parser.ParseException
-{
-  "errorClass" : "PARSE_SYNTAX_ERROR",
-  "sqlState" : "42601",
-  "messageParameters" : {
-    "error" : "'SELECT'",
-    "hint" : ""
-  }
-}
+1	2	3
 
 
 -- !query
 (SELECT 1,2,3 UNION SELECT 4,5,6 ORDER BY 1,2) EXCEPT SELECT 4,5,6
 -- !query schema
-struct<>
+struct<1:int,2:int,3:int>
 -- !query output
-org.apache.spark.sql.catalyst.parser.ParseException
-{
-  "errorClass" : "PARSE_SYNTAX_ERROR",
-  "sqlState" : "42601",
-  "messageParameters" : {
-    "error" : "'SELECT'",
-    "hint" : ""
-  }
-}
+1	2	3
 
 
 -- !query
@@ -753,14 +695,23 @@ SELECT cast('3.4' as decimal(38, 18)) UNION SELECT 'foo'
 -- !query schema
 struct<>
 -- !query output
-org.apache.spark.sql.catalyst.parser.ParseException
+org.apache.spark.SparkNumberFormatException
 {
-  "errorClass" : "PARSE_SYNTAX_ERROR",
-  "sqlState" : "42601",
+  "errorClass" : "CAST_INVALID_INPUT",
+  "sqlState" : "22018",
   "messageParameters" : {
-    "error" : "'SELECT'",
-    "hint" : ""
-  }
+    "ansiConfig" : "\"spark.sql.ansi.enabled\"",
+    "expression" : "'foo'",
+    "sourceType" : "\"STRING\"",
+    "targetType" : "\"DOUBLE\""
+  },
+  "queryContext" : [ {
+    "objectType" : "",
+    "objectName" : "",
+    "startIndex" : 1,
+    "stopIndex" : 56,
+    "fragment" : "SELECT cast('3.4' as decimal(38, 18)) UNION SELECT 'foo'"
+  } ]
 }
 
 
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/execution/command/AlterTableRenameParserSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/execution/command/AlterTableRenameParserSuite.scala
index c2305feb511..098750c929e 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/execution/command/AlterTableRenameParserSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/execution/command/AlterTableRenameParserSuite.scala
@@ -51,6 +51,6 @@ class AlterTableRenameParserSuite extends AnalysisTest {
     checkError(
       exception = parseException(parsePlan)(sql2),
       errorClass = "PARSE_SYNTAX_ERROR",
-      parameters = Map("error" -> "'.'", "hint" -> ": extra input '.'"))
+      parameters = Map("error" -> "'.'", "hint" -> ""))
   }
 }
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala
index 2cf4792b8c1..013e7227aef 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/execution/command/PlanResolutionSuite.scala
@@ -2377,7 +2377,7 @@ class PlanResolutionSuite extends AnalysisTest {
     checkError(
       exception = parseException(parsePlan)(sql),
       errorClass = "PARSE_SYNTAX_ERROR",
-      parameters = Map("error" -> "':'", "hint" -> ": extra input ':'"))
+      parameters = Map("error" -> "':'", "hint" -> ""))
   }
 
   test("create hive table - table file format") {
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCWriteSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCWriteSuite.scala
index 4c64f7d5f48..486255f1a3b 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCWriteSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCWriteSuite.scala
@@ -515,7 +515,7 @@ class JDBCWriteSuite extends SharedSparkSession with BeforeAndAfter {
           .jdbc(url1, "TEST.USERDBTYPETEST", properties)
       },
       errorClass = "PARSE_SYNTAX_ERROR",
-      parameters = Map("error" -> "'`'", "hint" -> ": extra input '`'"))
+      parameters = Map("error" -> "'`'", "hint" -> ""))
   }
 
   test("SPARK-10849: jdbc CreateTableColumnTypes duplicate columns") {
