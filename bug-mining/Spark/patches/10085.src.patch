diff --git a/python/pyspark/sql/connect/session.py b/python/pyspark/sql/connect/session.py
index 79677262eb1..76c6050e761 100644
--- a/python/pyspark/sql/connect/session.py
+++ b/python/pyspark/sql/connect/session.py
@@ -1074,11 +1074,14 @@ class SparkSession:
                 overwrite_conf["spark.connect.grpc.binding.port"] = "0"
 
             origin_remote = os.environ.get("SPARK_REMOTE", None)
+            origin_connect_mode = os.environ.get("SPARK_CONNECT_MODE", None)
             try:
+                # So SparkSubmit thinks no remote is set in order to
+                # start the regular PySpark session.
                 if origin_remote is not None:
-                    # So SparkSubmit thinks no remote is set in order to
-                    # start the regular PySpark session.
                     del os.environ["SPARK_REMOTE"]
+                if origin_connect_mode is not None:
+                    del os.environ["SPARK_CONNECT_MODE"]
 
                 # The regular PySpark session is registered as an active session
                 # so would not be garbage-collected.
@@ -1096,6 +1099,8 @@ class SparkSession:
             finally:
                 if origin_remote is not None:
                     os.environ["SPARK_REMOTE"] = origin_remote
+                if origin_connect_mode is not None:
+                    os.environ["SPARK_CONNECT_MODE"] = origin_connect_mode
         else:
             raise PySparkRuntimeError(
                 errorClass="SESSION_OR_CONTEXT_EXISTS",
diff --git a/sql/connect/common/src/main/scala/org/apache/spark/sql/connect/SparkSession.scala b/sql/connect/common/src/main/scala/org/apache/spark/sql/connect/SparkSession.scala
index 0af7c7b6d97..b8e4a49261b 100644
--- a/sql/connect/common/src/main/scala/org/apache/spark/sql/connect/SparkSession.scala
+++ b/sql/connect/common/src/main/scala/org/apache/spark/sql/connect/SparkSession.scala
@@ -744,7 +744,7 @@ object SparkSession extends SparkSessionCompanion with Logging {
     lazy val isAPIModeConnect =
       Option(System.getProperty(org.apache.spark.sql.SparkSessionBuilder.API_MODE_KEY))
         .getOrElse("classic")
-        .toLowerCase(Locale.ROOT) == "connect"
+        .toLowerCase(Locale.ROOT) == "connect" || System.getenv("SPARK_CONNECT_MODE") == "1"
     val remoteString = sparkOptions
       .get("spark.remote")
       .orElse(Option(System.getProperty("spark.remote"))) // Set from Spark Submit
@@ -778,6 +778,7 @@ object SparkSession extends SparkSessionCompanion with Logging {
           val pb = new ProcessBuilder(args: _*)
           // So don't exclude spark-sql jar in classpath
           pb.environment().remove(SparkConnectClient.SPARK_REMOTE)
+          pb.environment().remove("SPARK_CONNECT_MODE")
           pb.environment().put("SPARK_IDENT_STRING", serverId)
           pb.environment().put("HOSTNAME", "local")
           pb.environment().put("SPARK_CONNECT_AUTHENTICATE_TOKEN", token)
