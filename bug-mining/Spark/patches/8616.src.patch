diff --git a/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedDeltaBinaryPackedReader.java b/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedDeltaBinaryPackedReader.java
index d6d7d8ea2b0..071720d4dbf 100644
--- a/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedDeltaBinaryPackedReader.java
+++ b/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedDeltaBinaryPackedReader.java
@@ -300,8 +300,12 @@ public class VectorizedDeltaBinaryPackedReader extends VectorizedReaderBase {
         bitWidths[currentMiniBlock]);
     for (int j = 0; j < miniBlockSizeInValues; j += 8) {
       ByteBuffer buffer = in.slice(packer.getBitWidth());
-      packer.unpack8Values(buffer.array(),
-        buffer.arrayOffset() + buffer.position(), unpackedValuesBuffer, j);
+      if (buffer.hasArray()) {
+        packer.unpack8Values(buffer.array(),
+          buffer.arrayOffset() + buffer.position(), unpackedValuesBuffer, j);
+      } else {
+        packer.unpack8Values(buffer, buffer.position(), unpackedValuesBuffer, j);
+      }
     }
     remainingInMiniBlock = miniBlockSizeInValues;
     currentMiniBlock++;
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetDeltaEncodingSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetDeltaEncodingSuite.scala
index 844fa543145..c56e867b8bc 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetDeltaEncodingSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetDeltaEncodingSuite.scala
@@ -221,7 +221,6 @@ abstract class ParquetDeltaEncodingSuite[T] extends ParquetCompatibilityTest
         data(i) = getNextRandom
       }
       shouldReadAndWrite(data, size)
-      writer.reset()
     }
   }
 
@@ -231,19 +230,33 @@ abstract class ParquetDeltaEncodingSuite[T] extends ParquetCompatibilityTest
   }
 
   private def shouldReadAndWrite(data: Array[T], length: Int): Unit = {
-    writeData(data, length)
-    reader = new VectorizedDeltaBinaryPackedReader
-    val page = writer.getBytes.toByteArray
+    // SPARK-40052: Check that we can handle direct and non-direct byte buffers depending on the
+    // implementation of ByteBufferInputStream.
+    for (useDirect <- Seq(true, false)) {
+      writeData(data, length)
+      reader = new VectorizedDeltaBinaryPackedReader
+      val page = writer.getBytes.toByteArray
+
+      assert(estimatedSize(length) >= page.length)
+      writableColumnVector = new OnHeapColumnVector(data.length, getSparkSqlType)
+
+      val buf = if (useDirect) {
+        ByteBuffer.allocateDirect(page.length)
+      } else {
+        ByteBuffer.allocate(page.length)
+      }
+      buf.put(page)
+      buf.flip()
 
-    assert(estimatedSize(length) >= page.length)
-    writableColumnVector = new OnHeapColumnVector(data.length, getSparkSqlType)
-    reader.initFromPage(100, ByteBufferInputStream.wrap(ByteBuffer.wrap(page)))
-    readData(length, writableColumnVector, 0)
-    for (i <- 0 until length) {
-      assert(data(i) == readDataFromVector(writableColumnVector, i))
+      reader.initFromPage(100, ByteBufferInputStream.wrap(buf))
+      readData(length, writableColumnVector, 0)
+      for (i <- 0 until length) {
+        assert(data(i) == readDataFromVector(writableColumnVector, i))
+      }
+
+      writer.reset()
     }
   }
-
 }
 
 class ParquetDeltaEncodingInteger extends ParquetDeltaEncodingSuite[Int] {
