diff --git a/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcSourceSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcSourceSuite.scala
index a90d1937214..19477adec39 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcSourceSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcSourceSuite.scala
@@ -25,6 +25,7 @@ import java.util.Locale
 
 import org.apache.hadoop.conf.Configuration
 import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}
+import org.apache.logging.log4j.Level
 import org.apache.orc.OrcConf.COMPRESS
 import org.apache.orc.OrcFile
 import org.apache.orc.OrcProto.ColumnEncoding.Kind.{DICTIONARY_V2, DIRECT, DIRECT_V2}
@@ -553,6 +554,32 @@ abstract class OrcSuite
       assert(files.nonEmpty && files.forall(_.getName.contains("zstd")))
     }
   }
+
+  test("SPARK-37841: Skip updating stats for files not been created") {
+    withTempPath { path =>
+      val logAppender = new LogAppender()
+
+      withLogAppender(logAppender, level = Option(Level.WARN)) {
+        spark.range(0, 3, 1, 4).write.orc(path.getCanonicalPath)
+      }
+      val events = logAppender.loggingEvents
+      assert {
+        !events.exists { _.getMessage.getFormattedMessage
+          .contains("This could be due to the output format not writing empty files")
+        }
+      }
+    }
+  }
+
+  test("SPARK-37841: ORC sources write empty file with schema") {
+    withTempPath { path =>
+      val canonicalPath = path.getCanonicalPath
+      // creates an empty data set
+      spark.range(1, 1, 1, 1).write.orc(canonicalPath)
+      assert(spark.read.orc(canonicalPath).isEmpty,
+        "ORC sources shall write an empty file contains meta if necessary")
+    }
+  }
 }
 
 abstract class OrcSourceSuite extends OrcSuite with SharedSparkSession {
diff --git a/sql/hive/src/main/scala/org/apache/spark/sql/hive/orc/OrcFileFormat.scala b/sql/hive/src/main/scala/org/apache/spark/sql/hive/orc/OrcFileFormat.scala
index 36f329f74c4..aff014261ba 100644
--- a/sql/hive/src/main/scala/org/apache/spark/sql/hive/orc/OrcFileFormat.scala
+++ b/sql/hive/src/main/scala/org/apache/spark/sql/hive/orc/OrcFileFormat.scala
@@ -18,7 +18,6 @@
 package org.apache.spark.sql.hive.orc
 
 import java.net.URI
-import java.nio.charset.StandardCharsets.UTF_8
 import java.util.Properties
 
 import scala.collection.JavaConverters._
@@ -42,16 +41,15 @@ import org.apache.hadoop.mapreduce.lib.input.FileInputFormat
 import org.apache.orc.OrcConf
 import org.apache.orc.OrcConf.COMPRESS
 
-import org.apache.spark.{SPARK_VERSION_SHORT, TaskContext}
+import org.apache.spark.TaskContext
 import org.apache.spark.internal.Logging
-import org.apache.spark.sql.SPARK_VERSION_METADATA_KEY
 import org.apache.spark.sql.SparkSession
 import org.apache.spark.sql.catalyst.InternalRow
 import org.apache.spark.sql.catalyst.expressions._
 import org.apache.spark.sql.execution.datasources._
 import org.apache.spark.sql.execution.datasources.orc.{OrcFilters, OrcOptions, OrcUtils}
 import org.apache.spark.sql.hive.{HiveInspectors, HiveShim}
-import org.apache.spark.sql.sources.{Filter, _}
+import org.apache.spark.sql.sources._
 import org.apache.spark.sql.types._
 import org.apache.spark.util.SerializableConfiguration
 
@@ -240,7 +238,7 @@ private[orc] class OrcSerializer(dataSchema: StructType, conf: Configuration)
   }
 
   // Object inspector converted from the schema of the relation to be serialized.
-  private[this] val structOI = {
+  val structOI = {
     val typeInfo = TypeInfoUtils.getTypeInfoFromTypeString(dataSchema.catalogString)
     OrcStruct.createObjectInspector(typeInfo.asInstanceOf[StructTypeInfo])
       .asInstanceOf[SettableStructObjectInspector]
@@ -276,16 +274,11 @@ private[orc] class OrcOutputWriter(
     val path: String,
     dataSchema: StructType,
     context: TaskAttemptContext)
-  extends OutputWriter {
+  extends OutputWriter with Logging {
 
   private[this] val serializer = new OrcSerializer(dataSchema, context.getConfiguration)
 
-  // `OrcRecordWriter.close()` creates an empty file if no rows are written at all.  We use this
-  // flag to decide whether `OrcRecordWriter.close()` needs to be called.
-  private var recordWriterInstantiated = false
-
-  private lazy val recordWriter: RecordWriter[NullWritable, Writable] = {
-    recordWriterInstantiated = true
+  private val recordWriter: RecordWriter[NullWritable, Writable] = {
     new OrcOutputFormat().getRecordWriter(
       new Path(path).getFileSystem(context.getConfiguration),
       context.getConfiguration.asInstanceOf[JobConf],
@@ -299,11 +292,28 @@ private[orc] class OrcOutputWriter(
   }
 
   override def close(): Unit = {
-    if (recordWriterInstantiated) {
+    try {
+      OrcUtils.addSparkVersionMetadata(getOrCreateInternalWriter())
+    } catch {
+      case NonFatal(e) => log.warn(e.toString, e)
+    }
+    recordWriter.close(Reporter.NULL)
+  }
+
+  private def getOrCreateInternalWriter(): Writer = {
+    val writerField = recordWriter.getClass.getDeclaredField("writer")
+    writerField.setAccessible(true)
+    var writer = writerField.get(recordWriter).asInstanceOf[Writer]
+    if (writer == null) {
       // Hive ORC initializes its private `writer` field at the first write.
-      OrcFileFormat.addSparkVersionMetadata(recordWriter)
-      recordWriter.close(Reporter.NULL)
+      // For empty write task, we need to create it manually to record our meta.
+      val options = OrcFile.writerOptions(context.getConfiguration)
+      options.inspector(serializer.structOI)
+      writer = OrcFile.createWriter(new Path(path), options)
+      // set the writer to make it flush meta on close
+      writerField.set(recordWriter, writer)
     }
+    writer
   }
 }
 
@@ -366,18 +376,4 @@ private[orc] object OrcFileFormat extends HiveInspectors with Logging {
     val (sortedIDs, sortedNames) = ids.zip(requestedSchema.fieldNames).sorted.unzip
     HiveShim.appendReadColumns(conf, sortedIDs, sortedNames)
   }
-
-  /**
-   * Add a metadata specifying Spark version.
-   */
-  def addSparkVersionMetadata(recordWriter: RecordWriter[NullWritable, Writable]): Unit = {
-    try {
-      val writerField = recordWriter.getClass.getDeclaredField("writer")
-      writerField.setAccessible(true)
-      val writer = writerField.get(recordWriter).asInstanceOf[Writer]
-      writer.addUserMetadata(SPARK_VERSION_METADATA_KEY, UTF_8.encode(SPARK_VERSION_SHORT))
-    } catch {
-      case NonFatal(e) => log.warn(e.toString, e)
-    }
-  }
 }
diff --git a/sql/hive/src/test/scala/org/apache/spark/sql/hive/orc/HiveOrcQuerySuite.scala b/sql/hive/src/test/scala/org/apache/spark/sql/hive/orc/HiveOrcQuerySuite.scala
index 280ffb05fc1..2bd5c21ee8a 100644
--- a/sql/hive/src/test/scala/org/apache/spark/sql/hive/orc/HiveOrcQuerySuite.scala
+++ b/sql/hive/src/test/scala/org/apache/spark/sql/hive/orc/HiveOrcQuerySuite.scala
@@ -20,6 +20,7 @@ package org.apache.spark.sql.hive.orc
 import java.io.File
 
 import com.google.common.io.Files
+import org.apache.hadoop.fs.Path
 import org.apache.orc.OrcConf
 
 import org.apache.spark.sql.{AnalysisException, Row}
@@ -51,13 +52,8 @@ class HiveOrcQuerySuite extends OrcQueryTest with TestHiveSingleton {
           val emptyDF = Seq.empty[(Int, String)].toDF("key", "value").coalesce(1)
           emptyDF.createOrReplaceTempView("empty")
 
-          // This creates 1 empty ORC file with Hive ORC SerDe.  We are using this trick because
-          // Spark SQL ORC data source always avoids write empty ORC files.
-          spark.sql(
-            s"""INSERT INTO TABLE empty_orc
-               |SELECT key, value FROM empty
-             """.stripMargin)
-
+          val zeroPath = new Path(path, "zero.orc")
+          zeroPath.getFileSystem(spark.sessionState.newHadoopConf()).create(zeroPath)
           val errorMessage = intercept[AnalysisException] {
             spark.read.orc(path)
           }.getMessage
