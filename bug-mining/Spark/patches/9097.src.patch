diff --git a/connector/connect/server/src/main/scala/org/apache/spark/sql/connect/artifact/SparkConnectArtifactManager.scala b/connector/connect/server/src/main/scala/org/apache/spark/sql/connect/artifact/SparkConnectArtifactManager.scala
index efb407cff00..05c0a597722 100644
--- a/connector/connect/server/src/main/scala/org/apache/spark/sql/connect/artifact/SparkConnectArtifactManager.scala
+++ b/connector/connect/server/src/main/scala/org/apache/spark/sql/connect/artifact/SparkConnectArtifactManager.scala
@@ -29,6 +29,7 @@ import scala.reflect.ClassTag
 import org.apache.hadoop.fs.{LocalFileSystem, Path => FSPath}
 
 import org.apache.spark.{SparkContext, SparkEnv}
+import org.apache.spark.sql.connect.artifact.util.ArtifactUtils
 import org.apache.spark.sql.connect.config.Connect.CONNECT_COPY_FROM_LOCAL_TO_FS_ALLOW_DEST_LOCAL
 import org.apache.spark.sql.connect.service.SessionHolder
 import org.apache.spark.storage.{CacheId, StorageLevel}
@@ -67,7 +68,7 @@ class SparkConnectArtifactManager private[connect] {
   private[connect] lazy val classArtifactDir = SparkEnv.get.conf
     .getOption("spark.repl.class.outputDir")
     .map(p => Paths.get(p))
-    .getOrElse(artifactRootPath.resolve("classes"))
+    .getOrElse(ArtifactUtils.concatenatePaths(artifactRootPath, "classes"))
 
   private[connect] lazy val classArtifactUri: String =
     SparkEnv.get.conf.getOption("spark.repl.class.uri") match {
@@ -127,13 +128,14 @@ class SparkConnectArtifactManager private[connect] {
       }(catchBlock = { tmpFile.delete() })
     } else if (remoteRelativePath.startsWith(s"classes${File.separator}")) {
       // Move class files to common location (shared among all users)
-      val target = classArtifactDir.resolve(
+      val target = ArtifactUtils.concatenatePaths(
+        classArtifactDir,
         remoteRelativePath.toString.stripPrefix(s"classes${File.separator}"))
       Files.createDirectories(target.getParent)
       // Allow overwriting class files to capture updates to classes.
       Files.move(serverLocalStagingPath, target, StandardCopyOption.REPLACE_EXISTING)
     } else {
-      val target = artifactRootPath.resolve(remoteRelativePath)
+      val target = ArtifactUtils.concatenatePaths(artifactRootPath, remoteRelativePath)
       Files.createDirectories(target.getParent)
       // Disallow overwriting jars because spark doesn't support removing jars that were
       // previously added,
diff --git a/connector/connect/server/src/main/scala/org/apache/spark/sql/connect/artifact/util/ArtifactUtils.scala b/connector/connect/server/src/main/scala/org/apache/spark/sql/connect/artifact/util/ArtifactUtils.scala
new file mode 100644
index 00000000000..e338471ad5f
--- /dev/null
+++ b/connector/connect/server/src/main/scala/org/apache/spark/sql/connect/artifact/util/ArtifactUtils.scala
@@ -0,0 +1,43 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.spark.sql.connect.artifact.util
+
+import java.nio.file.{Path, Paths}
+
+object ArtifactUtils {
+
+  private[connect] def concatenatePaths(basePath: Path, otherPath: Path): Path = {
+    require(!otherPath.isAbsolute)
+    // We avoid using the `.resolve()` method here to ensure that we're concatenating the two
+    // paths.
+    val concatenatedPath = Paths.get(basePath.toString + "/" + otherPath.toString)
+    // Note: The normalized resulting path may still reference parent directories if the
+    // `otherPath` contains sufficient number of parent operators (i.e "..").
+    // Example: `basePath` = "/base", `otherPath` = "subdir/../../file.txt"
+    // Then, `concatenatedPath` = "/base/subdir/../../file.txt"
+    // and `normalizedPath` = "/base/file.txt".
+    val normalizedPath = concatenatedPath.normalize()
+    // Verify that the prefix of the `normalizedPath` starts with `basePath/`.
+    require(normalizedPath != basePath && normalizedPath.startsWith(basePath + "/"))
+    normalizedPath
+  }
+
+  private[connect] def concatenatePaths(basePath: Path, otherPath: String): Path = {
+    concatenatePaths(basePath, Paths.get(otherPath))
+  }
+}
diff --git a/connector/connect/server/src/main/scala/org/apache/spark/sql/connect/service/SparkConnectAddArtifactsHandler.scala b/connector/connect/server/src/main/scala/org/apache/spark/sql/connect/service/SparkConnectAddArtifactsHandler.scala
index ff6c881a721..179ff1b3ec9 100644
--- a/connector/connect/server/src/main/scala/org/apache/spark/sql/connect/service/SparkConnectAddArtifactsHandler.scala
+++ b/connector/connect/server/src/main/scala/org/apache/spark/sql/connect/service/SparkConnectAddArtifactsHandler.scala
@@ -30,6 +30,7 @@ import org.apache.spark.connect.proto
 import org.apache.spark.connect.proto.{AddArtifactsRequest, AddArtifactsResponse}
 import org.apache.spark.connect.proto.AddArtifactsResponse.ArtifactSummary
 import org.apache.spark.sql.connect.artifact.SparkConnectArtifactManager
+import org.apache.spark.sql.connect.artifact.util.ArtifactUtils
 import org.apache.spark.util.Utils
 
 /**
@@ -169,7 +170,16 @@ class SparkConnectAddArtifactsHandler(val responseObserver: StreamObserver[AddAr
       }
 
     val path: Path = Paths.get(canonicalFileName)
-    val stagedPath: Path = stagingDir.resolve(path)
+    val stagedPath: Path =
+      try {
+        ArtifactUtils.concatenatePaths(stagingDir, path)
+      } catch {
+        case _: IllegalArgumentException =>
+          throw new IllegalArgumentException(
+            s"Artifact with name: $name is invalid. The `name` " +
+              s"must be a relative path and cannot reference parent/sibling/nephew directories.")
+        case NonFatal(e) => throw e
+      }
 
     Files.createDirectories(stagedPath.getParent)
 
diff --git a/connector/connect/server/src/test/scala/org/apache/spark/sql/connect/service/AddArtifactsHandlerSuite.scala b/connector/connect/server/src/test/scala/org/apache/spark/sql/connect/service/AddArtifactsHandlerSuite.scala
index 9a4c029bb74..f11c9b2969e 100644
--- a/connector/connect/server/src/test/scala/org/apache/spark/sql/connect/service/AddArtifactsHandlerSuite.scala
+++ b/connector/connect/server/src/test/scala/org/apache/spark/sql/connect/service/AddArtifactsHandlerSuite.scala
@@ -314,4 +314,85 @@ class AddArtifactsHandlerSuite extends SharedSparkSession with ResourceHelper {
       handler.forceCleanUp()
     }
   }
+
+  private def createDummyArtifactRequests(name: String): Seq[proto.AddArtifactsRequest] = {
+    val bytes = ByteString.EMPTY
+    val context = proto.UserContext
+      .newBuilder()
+      .setUserId("c1")
+      .build()
+
+    val singleChunkArtifact = proto.AddArtifactsRequest.SingleChunkArtifact
+      .newBuilder()
+      .setName(name)
+      .setData(
+        proto.AddArtifactsRequest.ArtifactChunk
+          .newBuilder()
+          .setData(bytes)
+          // Set a dummy CRC value
+          .setCrc(12345)
+          .build())
+      .build()
+
+    val singleChunkArtifactRequest = AddArtifactsRequest
+      .newBuilder()
+      .setSessionId("abc")
+      .setUserContext(context)
+      .setBatch(
+        proto.AddArtifactsRequest.Batch.newBuilder().addArtifacts(singleChunkArtifact).build())
+      .build()
+
+    val beginChunkedArtifact = proto.AddArtifactsRequest.BeginChunkedArtifact
+      .newBuilder()
+      .setName(name)
+      .setNumChunks(1)
+      .setTotalBytes(1)
+      .setInitialChunk(
+        proto.AddArtifactsRequest.ArtifactChunk.newBuilder().setData(bytes).setCrc(12345).build())
+      .build()
+
+    val beginChunkArtifactRequest = AddArtifactsRequest
+      .newBuilder()
+      .setSessionId("abc")
+      .setUserContext(context)
+      .setBeginChunk(beginChunkedArtifact)
+      .build()
+
+    Seq(singleChunkArtifactRequest, beginChunkArtifactRequest)
+  }
+
+  test("Artifacts names are not allowed to be absolute paths") {
+    val promise = Promise[AddArtifactsResponse]
+    val handler = new TestAddArtifactsHandler(new DummyStreamObserver(promise))
+    try {
+      val name = "/absolute/path/"
+      val request = createDummyArtifactRequests(name)
+      request.foreach { req =>
+        intercept[IllegalArgumentException] {
+          handler.onNext(req)
+        }
+      }
+      handler.onCompleted()
+    } finally {
+      handler.forceCleanUp()
+    }
+  }
+
+  test("Artifact name/paths cannot reference parent/sibling/nephew directories") {
+    val promise = Promise[AddArtifactsResponse]
+    val handler = new TestAddArtifactsHandler(new DummyStreamObserver(promise))
+    try {
+      val names = Seq("..", "../sibling", "../nephew/directory", "a/../../b", "x/../y/../..")
+      val request = names.flatMap(createDummyArtifactRequests)
+      request.foreach { req =>
+        intercept[IllegalArgumentException] {
+          handler.onNext(req)
+        }
+      }
+      handler.onCompleted()
+    } finally {
+      handler.forceCleanUp()
+    }
+  }
+
 }
