diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/ShufflePartitionsUtil.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/ShufflePartitionsUtil.scala
index 837764b0b05..4ef7d33bbfa 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/ShufflePartitionsUtil.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/ShufflePartitionsUtil.scala
@@ -362,11 +362,15 @@ object ShufflePartitionsUtil extends Logging {
   }
 
   /**
-   * Get the map size of the specific reduce shuffle Id.
+   * Get the map size of the specific shuffle and reduce ID. Note that, some map outputs can be
+   * missing due to issues like executor lost. The size will be -1 for missing map outputs and the
+   * caller side should take care of it.
    */
   private def getMapSizesForReduceId(shuffleId: Int, partitionId: Int): Array[Long] = {
     val mapOutputTracker = SparkEnv.get.mapOutputTracker.asInstanceOf[MapOutputTrackerMaster]
-    mapOutputTracker.shuffleStatuses(shuffleId).mapStatuses.map{_.getSizeForBlock(partitionId)}
+    mapOutputTracker.shuffleStatuses(shuffleId).withMapStatuses(_.map { stat =>
+      if (stat == null) -1 else stat.getSizeForBlock(partitionId)
+    })
   }
 
   /**
@@ -378,6 +382,7 @@ object ShufflePartitionsUtil extends Logging {
       reducerId: Int,
       targetSize: Long): Option[Seq[PartialReducerPartitionSpec]] = {
     val mapPartitionSizes = getMapSizesForReduceId(shuffleId, reducerId)
+    if (mapPartitionSizes.exists(_ < 0)) return None
     val mapStartIndices = splitSizeListByTargetSize(mapPartitionSizes, targetSize)
     if (mapStartIndices.length > 1) {
       Some(mapStartIndices.indices.map { i =>
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/execution/ShufflePartitionsUtilSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/execution/ShufflePartitionsUtilSuite.scala
index a38caa74228..9f70c8aeca2 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/execution/ShufflePartitionsUtilSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/execution/ShufflePartitionsUtilSuite.scala
@@ -17,10 +17,12 @@
 
 package org.apache.spark.sql.execution
 
-import org.apache.spark.{MapOutputStatistics, SparkFunSuite}
+import org.apache.spark.{LocalSparkContext, MapOutputStatistics, MapOutputTrackerMaster, SparkConf, SparkContext, SparkEnv, SparkFunSuite}
+import org.apache.spark.scheduler.MapStatus
 import org.apache.spark.sql.execution.adaptive.ShufflePartitionsUtil
+import org.apache.spark.storage.BlockManagerId
 
-class ShufflePartitionsUtilSuite extends SparkFunSuite {
+class ShufflePartitionsUtilSuite extends SparkFunSuite with LocalSparkContext {
 
   private def checkEstimation(
       bytesByPartitionIdArray: Array[Array[Long]],
@@ -765,4 +767,31 @@ class ShufflePartitionsUtilSuite extends SparkFunSuite {
       targetSize, 1, 0)
     assert(coalesced == Seq(expected1, expected2))
   }
+
+  test("SPARK-36228: Skip splitting a skewed partition when some map outputs are removed") {
+    sc = new SparkContext(new SparkConf().setAppName("test").setMaster("local[2]"))
+    val mapOutputTracker = SparkEnv.get.mapOutputTracker.asInstanceOf[MapOutputTrackerMaster]
+    mapOutputTracker.registerShuffle(shuffleId = 10, numMaps = 2, numReduces = 1)
+    mapOutputTracker.registerMapOutput(shuffleId = 10, mapIndex = 0, MapStatus(
+      BlockManagerId("a", "hostA", port = 1000),
+      Array(MapStatus.compressSize(10)),
+      mapTaskId = 5))
+    mapOutputTracker.registerMapOutput(shuffleId = 10, mapIndex = 1, MapStatus(
+      BlockManagerId("b", "hostB", port = 1000),
+      Array(MapStatus.compressSize(20)),
+      mapTaskId = 6))
+
+    val skewPartitionSpecs = ShufflePartitionsUtil.createSkewPartitionSpecs(
+      shuffleId = 10, reducerId = 0, targetSize = 2)
+    assert(skewPartitionSpecs.isDefined)
+    // Returns 2 partition specs because there are 2 mappers.
+    assert(skewPartitionSpecs.get.size == 2)
+
+    // As if one map output is removed
+    mapOutputTracker.unregisterMapOutput(
+      shuffleId = 10, mapIndex = 0, BlockManagerId("a", "hostA", port = 1000))
+    val skewPartitionSpecsAfterRemoval = ShufflePartitionsUtil.createSkewPartitionSpecs(
+      shuffleId = 10, reducerId = 0, targetSize = 2)
+    assert(skewPartitionSpecsAfterRemoval.isEmpty)
+  }
 }
