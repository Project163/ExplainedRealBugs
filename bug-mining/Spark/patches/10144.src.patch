diff --git a/common/utils/src/main/resources/error/error-conditions.json b/common/utils/src/main/resources/error/error-conditions.json
index 8962cc3821f..41101dfbdfd 100644
--- a/common/utils/src/main/resources/error/error-conditions.json
+++ b/common/utils/src/main/resources/error/error-conditions.json
@@ -5243,6 +5243,12 @@
     ],
     "sqlState" : "XXKST"
   },
+  "TRANSFORM_WITH_STATE_USER_FUNCTION_ERROR" : {
+    "message" : [
+      "An error occurred in the user-defined function <function> of the StatefulProcessor. Reason: <reason>."
+    ],
+    "sqlState" : "39000"
+  },
   "TRANSPOSE_EXCEED_ROW_LIMIT" : {
     "message" : [
       "Number of rows exceeds the allowed limit of <maxValues> for TRANSPOSE. If this was intended, set <config> to at least the current row count."
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/TransformWithStateExec.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/TransformWithStateExec.scala
index b2702bc0192..94ad569458d 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/TransformWithStateExec.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/TransformWithStateExec.scala
@@ -21,6 +21,7 @@ import java.util.concurrent.TimeUnit.NANOSECONDS
 
 import org.apache.hadoop.conf.Configuration
 
+import org.apache.spark.SparkThrowable
 import org.apache.spark.broadcast.Broadcast
 import org.apache.spark.rdd.RDD
 import org.apache.spark.sql.catalyst.InternalRow
@@ -121,7 +122,9 @@ case class TransformWithStateExec(
     val driverProcessorHandle = new DriverStatefulProcessorHandleImpl(timeMode, keyEncoder)
     driverProcessorHandle.setHandleState(StatefulProcessorHandleState.PRE_INIT)
     statefulProcessor.setHandle(driverProcessorHandle)
-    statefulProcessor.init(outputMode, timeMode)
+    withStatefulProcessorErrorHandling("init") {
+      statefulProcessor.init(outputMode, timeMode)
+    }
     driverProcessorHandle
   }
 
@@ -131,7 +134,7 @@ case class TransformWithStateExec(
    * This instance of the stateful processor won't be used again.
    */
   private def closeProcessorHandle(): Unit = {
-    statefulProcessor.close()
+    closeStatefulProcessor()
     statefulProcessor.setHandle(null)
   }
 
@@ -261,7 +264,7 @@ case class TransformWithStateExec(
 
     val getOutputRow = ObjectOperator.wrapObjectToRow(outputObjectType)
 
-    val keyObj = getKeyObj(keyRow)  // convert key to objects
+    val keyObj = getKeyObj(keyRow) // convert key to objects
     val valueObjIter = valueRowIter.map(getValueObj.apply)
 
     // The statefulProcessor's handleInputRows method may create an eager iterator,
@@ -270,11 +273,13 @@ case class TransformWithStateExec(
     // methods on the iterator are invoked. This is done with the wrapper class
     // at the end of this method.
     ImplicitGroupingKeyTracker.setImplicitKey(keyObj)
-    val mappedIterator = statefulProcessor.handleInputRows(
-      keyObj,
-      valueObjIter,
-      new TimerValuesImpl(batchTimestampMs, eventTimeWatermarkForEviction)).map { obj =>
-      getOutputRow(obj)
+    val mappedIterator = withStatefulProcessorErrorHandling("handleInputRows") {
+     statefulProcessor.handleInputRows(
+        keyObj,
+        valueObjIter,
+        new TimerValuesImpl(batchTimestampMs, eventTimeWatermarkForEviction)).map { obj =>
+        getOutputRow(obj)
+      }
     }
     ImplicitGroupingKeyTracker.removeImplicitKey()
 
@@ -291,14 +296,15 @@ case class TransformWithStateExec(
     val keyObj = getKeyObj(keyRow) // convert key to objects
     ImplicitGroupingKeyTracker.setImplicitKey(keyObj)
     val initStateObjIter = initStateIter.map(getInitStateValueObj.apply)
-
-    initStateObjIter.foreach { initState =>
-      // allow multiple initial state rows on the same grouping key for integration
-      // with state data source reader with initial state
-      statefulProcessor
-        .asInstanceOf[StatefulProcessorWithInitialState[Any, Any, Any, Any]]
-        .handleInitialState(keyObj, initState,
-          new TimerValuesImpl(batchTimestampMs, eventTimeWatermarkForEviction))
+    withStatefulProcessorErrorHandling("handleInitialState") {
+      initStateObjIter.foreach { initState =>
+        // allow multiple initial state rows on the same grouping key for integration
+        // with state data source reader with initial state
+        statefulProcessor
+          .asInstanceOf[StatefulProcessorWithInitialState[Any, Any, Any, Any]]
+          .handleInitialState(keyObj, initState,
+            new TimerValuesImpl(batchTimestampMs, eventTimeWatermarkForEviction))
+      }
     }
     ImplicitGroupingKeyTracker.removeImplicitKey()
   }
@@ -317,11 +323,13 @@ case class TransformWithStateExec(
       processorHandle: StatefulProcessorHandleImpl): Iterator[InternalRow] = {
     val getOutputRow = ObjectOperator.wrapObjectToRow(outputObjectType)
     ImplicitGroupingKeyTracker.setImplicitKey(keyObj)
-    val mappedIterator = statefulProcessor.handleExpiredTimer(
-      keyObj,
-      new TimerValuesImpl(batchTimestampMs, eventTimeWatermarkForEviction),
-      new ExpiredTimerInfoImpl(Some(expiryTimestampMs))).map { obj =>
-      getOutputRow(obj)
+    val mappedIterator = withStatefulProcessorErrorHandling("handleExpiredTimer") {
+      statefulProcessor.handleExpiredTimer(
+        keyObj,
+        new TimerValuesImpl(batchTimestampMs, eventTimeWatermarkForEviction),
+        new ExpiredTimerInfoImpl(Some(expiryTimestampMs))).map { obj =>
+        getOutputRow(obj)
+      }
     }
     ImplicitGroupingKeyTracker.removeImplicitKey()
 
@@ -435,12 +443,18 @@ case class TransformWithStateExec(
       }
       setStoreMetrics(store)
       setOperatorMetrics()
-      statefulProcessor.close()
+      closeStatefulProcessor()
       statefulProcessor.setHandle(null)
       processorHandle.setHandleState(StatefulProcessorHandleState.CLOSED)
     })
   }
 
+  def closeStatefulProcessor(): Unit = {
+    withStatefulProcessorErrorHandling("close") {
+      statefulProcessor.close()
+    }
+  }
+
   // operator specific metrics
   override def customStatefulOperatorMetrics: Seq[StatefulOperatorCustomMetric] = {
     Seq(
@@ -564,6 +578,26 @@ case class TransformWithStateExec(
     }
   }
 
+  /**
+   * Executes a block of code with standardized error handling for StatefulProcessor
+   * operations. Rethrows SparkThrowables directly and wraps other exceptions in
+   * TransformWithStateUserFunctionException with the provided function name.
+   *
+   * @param functionName The name of the function being executed (for error reporting)
+   * @param block The code block to execute with error handling
+   * @return The result of the block execution
+   */
+  private def withStatefulProcessorErrorHandling[R](functionName: String)(block: => R): R = {
+    try {
+      block
+    } catch {
+      case st: Exception with SparkThrowable if st.getCondition != null =>
+        throw st
+      case e: Exception =>
+        throw TransformWithStateUserFunctionException(e, functionName)
+    }
+  }
+
   override def supportsSchemaEvolution: Boolean = true
 
   /**
@@ -603,7 +637,7 @@ case class TransformWithStateExec(
     val outputIterator = f(store)
     CompletionIterator[InternalRow, Iterator[InternalRow]](outputIterator.iterator, {
       stateStoreProvider.close()
-      statefulProcessor.close()
+      closeStatefulProcessor()
     })
   }
 
@@ -620,7 +654,9 @@ case class TransformWithStateExec(
       isStreaming, batchTimestampMs, metrics)
     assert(processorHandle.getHandleState == StatefulProcessorHandleState.CREATED)
     statefulProcessor.setHandle(processorHandle)
-    statefulProcessor.init(outputMode, timeMode)
+    withStatefulProcessorErrorHandling("init") {
+      statefulProcessor.init(outputMode, timeMode)
+    }
     processorHandle.setHandleState(StatefulProcessorHandleState.INITIALIZED)
     processDataWithPartition(singleIterator, store, processorHandle)
   }
@@ -634,7 +670,9 @@ case class TransformWithStateExec(
       keyEncoder, timeMode, isStreaming, batchTimestampMs, metrics)
     assert(processorHandle.getHandleState == StatefulProcessorHandleState.CREATED)
     statefulProcessor.setHandle(processorHandle)
-    statefulProcessor.init(outputMode, timeMode)
+    withStatefulProcessorErrorHandling("init") {
+      statefulProcessor.init(outputMode, timeMode)
+    }
     processorHandle.setHandleState(StatefulProcessorHandleState.INITIALIZED)
 
     val initialStateProcTimeMs = longMetric("initialStateProcessingTimeMs")
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreErrors.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreErrors.scala
index b3bfce752fc..af96db4a503 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreErrors.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreErrors.scala
@@ -370,6 +370,16 @@ class TWSSchemaMustBeNullable(
       "columnFamilyName" -> columnFamilyName,
       "schema" -> schema))
 
+private[sql] case class TransformWithStateUserFunctionException(
+    cause: Throwable,
+    functionName: String)
+  extends SparkException(
+    errorClass = "TRANSFORM_WITH_STATE_USER_FUNCTION_ERROR",
+    messageParameters = Map(
+      "reason" -> Option(cause.getMessage).getOrElse(""),
+      "function" -> functionName),
+    cause = cause)
+
 class StateStoreInvalidValueSchemaEvolution(
     oldValueSchema: String,
     newValueSchema: String)
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/streaming/TransformWithStateSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/streaming/TransformWithStateSuite.scala
index ec17adde2bf..148c451f37a 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/streaming/TransformWithStateSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/streaming/TransformWithStateSuite.scala
@@ -74,6 +74,117 @@ case class EvolvedState(
     score: Double           // Should default to 0.0
 )
 
+class ClassifiedTimerErrorProcessor
+  extends StatefulProcessor[String, String, (String, BasicState)] {
+
+  @transient var state: ValueState[BasicState] = _
+
+  override def init(outputMode: OutputMode, timeMode: TimeMode): Unit = {
+    state = getHandle.getValueState[BasicState](
+      "testState",
+      Encoders.product[BasicState],
+      TTLConfig.NONE)
+  }
+
+  override def handleInputRows(
+      key: String,
+      rows: Iterator[String],
+      timerValues: TimerValues): Iterator[(String, BasicState)] = {
+    getHandle.registerTimer(timerValues.getCurrentProcessingTimeInMs() + 1000)
+    Seq.empty.iterator
+  }
+
+  override def handleExpiredTimer(
+      key: String,
+      timerValues: TimerValues,
+      expiredTimerInfo: ExpiredTimerInfo): Iterator[(String, BasicState)] = {
+    throw StateStoreErrors.multipleColumnFamiliesNotSupported("dummy val")
+  }
+}
+
+class UnclassifiedTimerErrorProcessor
+  extends StatefulProcessor[String, String, (String, BasicState)] {
+
+  @transient var state: ValueState[BasicState] = _
+
+  override def init(outputMode: OutputMode, timeMode: TimeMode): Unit = {
+    state = getHandle.getValueState[BasicState](
+      "testState",
+      Encoders.product[BasicState],
+      TTLConfig.NONE)
+  }
+
+  override def handleInputRows(
+      key: String,
+      rows: Iterator[String],
+      timerValues: TimerValues): Iterator[(String, BasicState)] = {
+    getHandle.registerTimer(timerValues.getCurrentProcessingTimeInMs() + 1000)
+    Seq.empty.iterator
+  }
+
+  override def handleExpiredTimer(
+      key: String,
+      timerValues: TimerValues,
+      expiredTimerInfo: ExpiredTimerInfo): Iterator[(String, BasicState)] = {
+    throw new IllegalStateException("dummy unclassified error")
+    Seq.empty.iterator
+  }
+}
+
+class ClassifiedErrorInitialStateProcessor
+  extends StatefulProcessorWithInitialState[String, String, (String, BasicState), String] {
+
+  @transient var state: ValueState[BasicState] = _
+
+  override def init(outputMode: OutputMode, timeMode: TimeMode): Unit = {
+    state = getHandle.getValueState[BasicState](
+      "testState",
+      Encoders.product[BasicState],
+      TTLConfig.NONE)
+  }
+
+  override def handleInitialState(
+      key: String,
+      initialState: String,
+      timerValues: TimerValues): Unit = {
+    throw StateStoreErrors.multipleColumnFamiliesNotSupported("dummy val")
+  }
+
+  override def handleInputRows(
+      key: String,
+      rows: Iterator[String],
+      timerValues: TimerValues): Iterator[(String, BasicState)] = {
+    Seq.empty.iterator
+  }
+}
+
+class UnclassifiedErrorInitialStateProcessor
+  extends StatefulProcessorWithInitialState[String, String, (String, BasicState), String] {
+
+  @transient var state: ValueState[BasicState] = _
+
+  override def init(outputMode: OutputMode, timeMode: TimeMode): Unit = {
+    state = getHandle.getValueState[BasicState](
+      "testState",
+      Encoders.product[BasicState],
+      TTLConfig.NONE)
+  }
+
+  override def handleInitialState(
+      key: String,
+      initialState: String,
+      timerValues: TimerValues): Unit = {
+    throw new IllegalStateException("dummy unclassified error")
+  }
+
+  override def handleInputRows(
+      key: String,
+      rows: Iterator[String],
+      timerValues: TimerValues): Iterator[(String, BasicState)] = {
+    Seq.empty.iterator
+  }
+}
+
 // Processor with initial schema
 class DefaultValueInitialProcessor
   extends StatefulProcessor[String, String, (String, BasicState)] {
@@ -100,6 +211,47 @@ class DefaultValueInitialProcessor
   }
 }
 
+class UnclassifiedErrorProcessor
+  extends StatefulProcessor[String, String, (String, BasicState)] {
+
+  @transient var state: ValueState[BasicState] = _
+
+  override def init(outputMode: OutputMode, timeMode: TimeMode): Unit = {
+    state = getHandle.getValueState[BasicState](
+      "testState",
+      Encoders.product[BasicState],
+      TTLConfig.NONE)
+  }
+
+  override def handleInputRows(
+      key: String,
+      rows: Iterator[String],
+      timerValues: TimerValues): Iterator[(String, BasicState)] = {
+    throw new IllegalStateException("dummy unclassified error")
+    Seq.empty.iterator
+  }
+}
+
+class ClassifiedErrorProcessor
+  extends StatefulProcessor[String, String, (String, BasicState)] {
+
+  @transient var state: ValueState[BasicState] = _
+
+  override def init(outputMode: OutputMode, timeMode: TimeMode): Unit = {
+    state = getHandle.getValueState[BasicState](
+      "testState",
+      Encoders.product[BasicState],
+      TTLConfig.NONE)
+  }
+
+  override def handleInputRows(
+      key: String,
+      rows: Iterator[String],
+      timerValues: TimerValues): Iterator[(String, BasicState)] = {
+    throw StateStoreErrors.multipleColumnFamiliesNotSupported("dummy val")
+  }
+}
+
 // Evolved processor with additional primitive fields
 class DefaultValueEvolvedProcessor
   extends StatefulProcessor[String, String, (String, EvolvedState)] {
@@ -2416,6 +2568,170 @@ class TransformWithStateValidationSuite extends StateStoreMetricsTest {
     )
   }
 
+  test("transformWithState - check that error within handleInputRows is classified") {
+    withSQLConf(
+      SQLConf.STATE_STORE_PROVIDER_CLASS.key -> classOf[RocksDBStateStoreProvider].getName,
+      SQLConf.SHUFFLE_PARTITIONS.key ->
+        TransformWithStateSuiteUtils.NUM_SHUFFLE_PARTITIONS.toString) {
+
+      val inputData = MemoryStream[String]
+      val result = inputData.toDS()
+        .groupByKey(x => x)
+        .transformWithState(new UnclassifiedErrorProcessor(),
+          TimeMode.None(),
+          OutputMode.Update())
+
+      testStream(result, OutputMode.Update())(
+        AddData(inputData, "a"),
+        ExpectFailure[TransformWithStateUserFunctionException] { error =>
+          checkError(
+            error.asInstanceOf[SparkException],
+            condition = "TRANSFORM_WITH_STATE_USER_FUNCTION_ERROR",
+            parameters = Map(
+              "reason" -> "dummy unclassified error",
+              "function" -> "handleInputRows")
+          )
+        }
+      )
+    }
+  }
+
+  test("transformWithState - check that classified error is thrown from handleInputRows") {
+    withSQLConf(
+      SQLConf.STATE_STORE_PROVIDER_CLASS.key -> classOf[RocksDBStateStoreProvider].getName,
+      SQLConf.SHUFFLE_PARTITIONS.key ->
+        TransformWithStateSuiteUtils.NUM_SHUFFLE_PARTITIONS.toString) {
+
+      val inputData = MemoryStream[String]
+      val result = inputData.toDS()
+        .groupByKey(x => x)
+        .transformWithState(new ClassifiedErrorProcessor(),
+          TimeMode.None(),
+          OutputMode.Update())
+
+      testStream(result, OutputMode.Update())(
+        AddData(inputData, "a"),
+        ExpectFailure[StateStoreMultipleColumnFamiliesNotSupportedException] { error =>
+          assert(error.getMessage.contains("not supported"))
+        }
+      )
+    }
+  }
+
+  test("transformWithState - check that error within handleInitialState is classified") {
+    withSQLConf(
+      SQLConf.STATE_STORE_PROVIDER_CLASS.key -> classOf[RocksDBStateStoreProvider].getName,
+      SQLConf.SHUFFLE_PARTITIONS.key ->
+        TransformWithStateSuiteUtils.NUM_SHUFFLE_PARTITIONS.toString) {
+
+      val inputData = MemoryStream[String]
+      val initDf = Seq("init_1").toDS().groupByKey(x => x)
+      val result = inputData.toDS()
+        .groupByKey(x => x)
+        .transformWithState(new UnclassifiedErrorInitialStateProcessor(),
+          TimeMode.None(),
+          OutputMode.Update(),
+          initDf)
+
+      testStream(result, OutputMode.Update())(
+        AddData(inputData, "a"),
+        ExpectFailure[TransformWithStateUserFunctionException] { error =>
+          checkError(
+            error.asInstanceOf[SparkException],
+            condition = "TRANSFORM_WITH_STATE_USER_FUNCTION_ERROR",
+            parameters = Map(
+              "reason" -> "dummy unclassified error",
+              "function" -> "handleInitialState")
+          )
+        }
+      )
+    }
+  }
+
+  test("transformWithState - check that classified error is thrown from handleInitialState") {
+    withSQLConf(
+      SQLConf.STATE_STORE_PROVIDER_CLASS.key -> classOf[RocksDBStateStoreProvider].getName,
+      SQLConf.SHUFFLE_PARTITIONS.key ->
+        TransformWithStateSuiteUtils.NUM_SHUFFLE_PARTITIONS.toString) {
+
+      val inputData = MemoryStream[String]
+      val initDf = Seq("init_1").toDS().groupByKey(x => x)
+      val result = inputData.toDS()
+        .groupByKey(x => x)
+        .transformWithState(new ClassifiedErrorInitialStateProcessor(),
+          TimeMode.None(),
+          OutputMode.Update(),
+          initDf)
+
+      testStream(result, OutputMode.Update())(
+        AddData(inputData, "a"),
+        ExpectFailure[StateStoreMultipleColumnFamiliesNotSupportedException] { error =>
+          assert(error.getMessage.contains("not supported"))
+        }
+      )
+    }
+  }
+
+  test("transformWithState - check that error within handleExpiredTimer is classified") {
+    withSQLConf(
+      SQLConf.STATE_STORE_PROVIDER_CLASS.key -> classOf[RocksDBStateStoreProvider].getName,
+      SQLConf.SHUFFLE_PARTITIONS.key ->
+        TransformWithStateSuiteUtils.NUM_SHUFFLE_PARTITIONS.toString) {
+
+      val clock = new StreamManualClock
+      val inputData = MemoryStream[String]
+      val result = inputData.toDS()
+        .groupByKey(x => x)
+        .transformWithState(new UnclassifiedTimerErrorProcessor(),
+          TimeMode.ProcessingTime(),
+          OutputMode.Update())
+
+      testStream(result, OutputMode.Update())(
+        StartStream(Trigger.ProcessingTime("1 second"), triggerClock = clock),
+        AddData(inputData, "a"),
+        AdvanceManualClock(1 * 1000),
+        CheckNewAnswer(),
+        AdvanceManualClock(2 * 1000),
+        ExpectFailure[TransformWithStateUserFunctionException] { error =>
+          checkError(
+            error.asInstanceOf[SparkException],
+            condition = "TRANSFORM_WITH_STATE_USER_FUNCTION_ERROR",
+            parameters = Map(
+              "reason" -> "dummy unclassified error",
+              "function" -> "handleExpiredTimer")
+          )
+        }
+      )
+    }
+  }
+
+  test("transformWithState - check that classified error is thrown from handleExpiredTimer") {
+    withSQLConf(
+      SQLConf.STATE_STORE_PROVIDER_CLASS.key -> classOf[RocksDBStateStoreProvider].getName,
+      SQLConf.SHUFFLE_PARTITIONS.key ->
+        TransformWithStateSuiteUtils.NUM_SHUFFLE_PARTITIONS.toString) {
+
+      val clock = new StreamManualClock
+      val inputData = MemoryStream[String]
+      val result = inputData.toDS()
+        .groupByKey(x => x)
+        .transformWithState(new ClassifiedTimerErrorProcessor(),
+          TimeMode.ProcessingTime(),
+          OutputMode.Update())
+
+      testStream(result, OutputMode.Update())(
+        StartStream(Trigger.ProcessingTime("1 second"), triggerClock = clock),
+        AddData(inputData, "a"),
+        AdvanceManualClock(1 * 1000),
+        CheckNewAnswer(),
+        AdvanceManualClock(2 * 1000),
+        ExpectFailure[StateStoreMultipleColumnFamiliesNotSupportedException] { error =>
+          assert(error.getMessage.contains("not supported"))
+        }
+      )
+    }
+  }
+
   test("transformWithState - ValueState.exists() should fail in init") {
     withSQLConf(
       SQLConf.STATE_STORE_PROVIDER_CLASS.key -> classOf[RocksDBStateStoreProvider].getName,
