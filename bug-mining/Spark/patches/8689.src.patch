diff --git a/core/src/main/scala/org/apache/spark/executor/ExecutorMetricsPoller.scala b/core/src/main/scala/org/apache/spark/executor/ExecutorMetricsPoller.scala
index 0cdb306af74..69182dbc27f 100644
--- a/core/src/main/scala/org/apache/spark/executor/ExecutorMetricsPoller.scala
+++ b/core/src/main/scala/org/apache/spark/executor/ExecutorMetricsPoller.scala
@@ -18,7 +18,7 @@ package org.apache.spark.executor
 
 import java.lang.Long.{MAX_VALUE => LONG_MAX_VALUE}
 import java.util.concurrent.{ConcurrentHashMap, TimeUnit}
-import java.util.concurrent.atomic.{AtomicLong, AtomicLongArray}
+import java.util.concurrent.atomic.AtomicLongArray
 
 import scala.collection.mutable.HashMap
 
@@ -53,7 +53,7 @@ private[spark] class ExecutorMetricsPoller(
 
   type StageKey = (Int, Int)
   // Task Count and Metric Peaks
-  private[executor] case class TCMP(count: AtomicLong, peaks: AtomicLongArray)
+  private[executor] case class TCMP(count: Long, peaks: AtomicLongArray)
 
   // Map of (stageId, stageAttemptId) to (count of running tasks, executor metric peaks)
   private[executor] val stageTCMP = new ConcurrentHashMap[StageKey, TCMP]
@@ -112,10 +112,13 @@ private[spark] class ExecutorMetricsPoller(
 
     // Put a new entry in stageTCMP for the stage if there isn't one already.
     // Increment the task count.
-    val countAndPeaks = stageTCMP.computeIfAbsent((stageId, stageAttemptId),
-      _ => TCMP(new AtomicLong(0), new AtomicLongArray(ExecutorMetricType.numMetrics)))
-    val stageCount = countAndPeaks.count.incrementAndGet()
-    logDebug(s"stageTCMP: ($stageId, $stageAttemptId) -> $stageCount")
+    val countAndPeaks = stageTCMP.compute((stageId, stageAttemptId), (k: StageKey, v: TCMP) =>
+      if (v == null) {
+        TCMP(1L, new AtomicLongArray(ExecutorMetricType.numMetrics))
+      } else {
+        TCMP(v.count + 1, v.peaks)
+      })
+    logDebug(s"stageTCMP: ($stageId, $stageAttemptId) -> ${countAndPeaks.count}")
   }
 
   /**
@@ -126,10 +129,10 @@ private[spark] class ExecutorMetricsPoller(
     // Decrement the task count.
 
     def decrementCount(stage: StageKey, countAndPeaks: TCMP): TCMP = {
-      val countValue = countAndPeaks.count.decrementAndGet()
+      val countValue = countAndPeaks.count - 1
       assert(countValue >= 0, "task count shouldn't below 0")
       logDebug(s"stageTCMP: (${stage._1}, ${stage._2}) -> " + countValue)
-      countAndPeaks
+      TCMP(countValue, countAndPeaks.peaks)
     }
 
     stageTCMP.computeIfPresent((stageId, stageAttemptId), decrementCount)
@@ -172,7 +175,7 @@ private[spark] class ExecutorMetricsPoller(
     stageTCMP.replaceAll(getUpdateAndResetPeaks)
 
     def removeIfInactive(k: StageKey, v: TCMP): TCMP = {
-      if (v.count.get == 0) {
+      if (v.count == 0) {
         logDebug(s"removing (${k._1}, ${k._2}) from stageTCMP")
         null
       } else {
diff --git a/core/src/test/scala/org/apache/spark/executor/ExecutorMetricsPollerSuite.scala b/core/src/test/scala/org/apache/spark/executor/ExecutorMetricsPollerSuite.scala
index e471864ae24..11593a04720 100644
--- a/core/src/test/scala/org/apache/spark/executor/ExecutorMetricsPollerSuite.scala
+++ b/core/src/test/scala/org/apache/spark/executor/ExecutorMetricsPollerSuite.scala
@@ -30,13 +30,13 @@ class ExecutorMetricsPollerSuite extends SparkFunSuite {
     // stage (0, 0) has an active task, so it remains on stageTCMP after heartbeat.
     assert(poller.getExecutorUpdates.size === 1)
     assert(poller.stageTCMP.size === 1)
-    assert(poller.stageTCMP.get((0, 0)).count.get === 1)
+    assert(poller.stageTCMP.get((0, 0)).count === 1)
 
     poller.onTaskCompletion(0L, 0, 0)
     // stage (0, 0) doesn't have active tasks, but its entry will be kept until next
     // heartbeat.
     assert(poller.stageTCMP.size === 1)
-    assert(poller.stageTCMP.get((0, 0)).count.get === 0)
+    assert(poller.stageTCMP.get((0, 0)).count === 0)
 
     // the next heartbeat will report the peak metrics of stage (0, 0) during the
     // previous heartbeat interval, then remove it from stageTCMP.
