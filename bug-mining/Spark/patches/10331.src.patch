diff --git a/python/pyspark/sql/conversion.py b/python/pyspark/sql/conversion.py
index f1aa55c2039..1ca9c869eb3 100644
--- a/python/pyspark/sql/conversion.py
+++ b/python/pyspark/sql/conversion.py
@@ -80,6 +80,7 @@ class LocalDataToArrowConversion:
             return True
         elif isinstance(dataType, DecimalType):
             # Convert Decimal('NaN') to None
+            # Rescale Decimal values
             return True
         elif isinstance(dataType, StringType):
             # Coercion to StringType is allowed
@@ -306,6 +307,8 @@ class LocalDataToArrowConversion:
             return convert_timestamp_ntz
 
         elif isinstance(dataType, DecimalType):
+            exp = decimal.Decimal(f"1E-{dataType.scale}")
+            ctx = decimal.Context(prec=dataType.precision, rounding=decimal.ROUND_HALF_EVEN)
 
             def convert_decimal(value: Any) -> Any:
                 if value is None:
@@ -321,7 +324,8 @@ class LocalDataToArrowConversion:
                         if not nullable:
                             raise PySparkValueError(f"input for {dataType} must not be None")
                         return None
-                    return value
+
+                    return value.quantize(exp, context=ctx)
 
             return convert_decimal
 
diff --git a/python/pyspark/sql/tests/arrow/test_arrow_python_udf.py b/python/pyspark/sql/tests/arrow/test_arrow_python_udf.py
index a054261304c..c00565c814b 100644
--- a/python/pyspark/sql/tests/arrow/test_arrow_python_udf.py
+++ b/python/pyspark/sql/tests/arrow/test_arrow_python_udf.py
@@ -15,13 +15,21 @@
 # limitations under the License.
 #
 
+from decimal import Decimal
 import unittest
 
 from pyspark.errors import AnalysisException, PythonException, PySparkNotImplementedError
 from pyspark.sql import Row
 from pyspark.sql.functions import udf
 from pyspark.sql.tests.test_udf import BaseUDFTestsMixin
-from pyspark.sql.types import DayTimeIntervalType, VarcharType, StructType, StructField, StringType
+from pyspark.sql.types import (
+    DayTimeIntervalType,
+    DecimalType,
+    VarcharType,
+    StructType,
+    StructField,
+    StringType,
+)
 from pyspark.testing.sqlutils import (
     have_pandas,
     have_pyarrow,
@@ -181,8 +189,6 @@ class ArrowPythonUDFTestsMixin(BaseUDFTestsMixin):
             df_floating_value.select(udf(lambda x: x, "decimal")("value").alias("res")).collect()
 
     def test_arrow_udf_int_to_decimal_coercion(self):
-        from decimal import Decimal
-
         with self.sql_conf(
             {"spark.sql.legacy.execution.pythonUDF.pandas.conversion.enabled": False}
         ):
@@ -235,6 +241,19 @@ class ArrowPythonUDFTestsMixin(BaseUDFTestsMixin):
                 ):
                     df.select(high_precision_udf("id").alias("decimal_val")).collect()
 
+    def test_decimal_round(self):
+        with self.sql_conf(
+            {"spark.sql.legacy.execution.pythonUDF.pandas.conversion.enabled": False}
+        ):
+            df = self.spark.sql("SELECT DOUBLE(1.234) AS v")
+
+            @udf(returnType=DecimalType(38, 18))
+            def f(v: float):
+                return Decimal(v)
+
+            rounded = df.select(f("v").alias("d")).first().d
+            self.assertEqual(rounded, Decimal("1.233999999999999986"))
+
     def test_err_return_type(self):
         with self.assertRaises(PySparkNotImplementedError) as pe:
             udf(lambda x: x, VarcharType(10), useArrow=True)
diff --git a/python/pyspark/sql/tests/test_creation.py b/python/pyspark/sql/tests/test_creation.py
index fb78b4c1463..533172d229b 100644
--- a/python/pyspark/sql/tests/test_creation.py
+++ b/python/pyspark/sql/tests/test_creation.py
@@ -167,6 +167,11 @@ class DataFrameCreationTestsMixin:
         with self.assertRaises(PySparkValueError):
             self.spark.createDataFrame(data=data, schema=schema)
 
+    def test_decimal_round(self):
+        df = self.spark.createDataFrame([(Decimal(1.234),)], ["d"])
+        rounded = df.first().d
+        self.assertEqual(rounded, Decimal("1.233999999999999986"))
+
     def test_invalid_argument_create_dataframe(self):
         with self.assertRaises(PySparkTypeError) as pe:
             self.spark.createDataFrame([(1, 2)], schema=123)
diff --git a/python/pyspark/sql/tests/test_python_datasource.py b/python/pyspark/sql/tests/test_python_datasource.py
index db7df10f3f1..04d9d22dd99 100644
--- a/python/pyspark/sql/tests/test_python_datasource.py
+++ b/python/pyspark/sql/tests/test_python_datasource.py
@@ -20,7 +20,7 @@ import tempfile
 import unittest
 from datetime import datetime
 from decimal import Decimal
-from typing import Callable, Iterable, List, Union
+from typing import Callable, Iterable, List, Union, Iterator, Tuple
 
 from pyspark.errors import AnalysisException, PythonException
 from pyspark.sql.datasource import (
@@ -48,7 +48,7 @@ from pyspark.sql.datasource import (
 )
 from pyspark.sql.functions import spark_partition_id
 from pyspark.sql.session import SparkSession
-from pyspark.sql.types import Row, StructType, VariantVal
+from pyspark.sql.types import Row, StructField, StructType, IntegerType, DecimalType, VariantVal
 from pyspark.testing import assertDataFrameEqual
 from pyspark.testing.sqlutils import (
     SPARK_HOME,
@@ -755,6 +755,33 @@ class BasePythonDataSourceTestsMixin:
         ):
             df.write.format("test").mode("append").saveAsTable("test_table")
 
+    def test_decimal_round(self):
+        class SimpleDataSource(DataSource):
+            @classmethod
+            def name(cls) -> str:
+                return "simple_decimal"
+
+            def schema(self) -> StructType:
+                return StructType(
+                    [
+                        StructField("i", IntegerType()),
+                        StructField("d", DecimalType(38, 18)),
+                    ]
+                )
+
+            def reader(self, schema: StructType) -> DataSourceReader:
+                return SimpleDataSourceReader()
+
+        class SimpleDataSourceReader(DataSourceReader):
+            def read(self, partition: InputPartition) -> Iterator[Tuple]:
+                yield (1, Decimal(1.234))
+
+        self.spark.dataSource.register(SimpleDataSource)
+        df = self.spark.read.format("simple_decimal").load()
+
+        rounded = df.select("d").first().d
+        self.assertEqual(rounded, Decimal("1.233999999999999986"))
+
     @unittest.skipIf(
         "pypy" in platform.python_implementation().lower(), "cannot run in environment pypy"
     )
diff --git a/python/pyspark/sql/tests/test_udf.py b/python/pyspark/sql/tests/test_udf.py
index 9d130a7f525..0e0d8643409 100644
--- a/python/pyspark/sql/tests/test_udf.py
+++ b/python/pyspark/sql/tests/test_udf.py
@@ -193,13 +193,16 @@ class BaseUDFTestsMixin(object):
             df.agg(sum(udf_random_col())).collect()
 
     def test_chained_udf(self):
-        self.spark.catalog.registerFunction("double", lambda x: x + x, IntegerType())
-        [row] = self.spark.sql("SELECT double(1)").collect()
-        self.assertEqual(row[0], 2)
-        [row] = self.spark.sql("SELECT double(double(1))").collect()
-        self.assertEqual(row[0], 4)
-        [row] = self.spark.sql("SELECT double(double(1) + 1)").collect()
-        self.assertEqual(row[0], 6)
+        self.spark.catalog.registerFunction("double_int", lambda x: x + x, IntegerType())
+        try:
+            [row] = self.spark.sql("SELECT double_int(1)").collect()
+            self.assertEqual(row[0], 2)
+            [row] = self.spark.sql("SELECT double_int(double_int(1))").collect()
+            self.assertEqual(row[0], 4)
+            [row] = self.spark.sql("SELECT double_int(double_int(1) + 1)").collect()
+            self.assertEqual(row[0], 6)
+        finally:
+            self.spark.sql("DROP TEMPORARY FUNCTION IF EXISTS double_int")
 
     def test_single_udf_with_repeated_argument(self):
         # regression test for SPARK-20685
diff --git a/python/pyspark/sql/tests/test_udtf.py b/python/pyspark/sql/tests/test_udtf.py
index b006ac6c14d..9d5bb736c40 100644
--- a/python/pyspark/sql/tests/test_udtf.py
+++ b/python/pyspark/sql/tests/test_udtf.py
@@ -14,6 +14,8 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
+
+from decimal import Decimal
 import datetime
 import os
 import platform
@@ -3610,6 +3612,19 @@ class UDTFArrowTestsMixin(LegacyUDTFArrowTestsMixin):
                 with self.assertRaisesRegex(PythonException, "UDTF_ARROW_TYPE_CONVERSION_ERROR"):
                     udtf(TestUDTF, returnType=ret_type)().collect()
 
+    def test_decimal_round(self):
+        with self.sql_conf(
+            {"spark.sql.legacy.execution.pythonUDTF.pandas.conversion.enabled": False}
+        ):
+
+            @udtf(returnType="a: DOUBLE, d: DECIMAL(38, 18)")
+            class Float2Decimal:
+                def eval(self, v: float):
+                    yield v, Decimal(v)
+
+            rounded = Float2Decimal(lit(1.234)).first().d
+            self.assertEqual(rounded, Decimal("1.233999999999999986"))
+
 
 class UDTFArrowTests(UDTFArrowTestsMixin, ReusedSQLTestCase):
     @classmethod
