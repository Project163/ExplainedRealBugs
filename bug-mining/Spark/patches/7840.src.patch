diff --git a/core/src/main/java/org/apache/spark/memory/MemoryConsumer.java b/core/src/main/java/org/apache/spark/memory/MemoryConsumer.java
index 9a9d0c79465..1d361aeb24e 100644
--- a/core/src/main/java/org/apache/spark/memory/MemoryConsumer.java
+++ b/core/src/main/java/org/apache/spark/memory/MemoryConsumer.java
@@ -40,8 +40,8 @@ public abstract class MemoryConsumer {
     this.mode = mode;
   }
 
-  protected MemoryConsumer(TaskMemoryManager taskMemoryManager) {
-    this(taskMemoryManager, taskMemoryManager.pageSizeBytes(), MemoryMode.ON_HEAP);
+  protected MemoryConsumer(TaskMemoryManager taskMemoryManager, MemoryMode mode) {
+    this(taskMemoryManager, taskMemoryManager.pageSizeBytes(), mode);
   }
 
   /**
diff --git a/core/src/main/scala/org/apache/spark/util/collection/Spillable.scala b/core/src/main/scala/org/apache/spark/util/collection/Spillable.scala
index 1983b000285..fe488f9cf0d 100644
--- a/core/src/main/scala/org/apache/spark/util/collection/Spillable.scala
+++ b/core/src/main/scala/org/apache/spark/util/collection/Spillable.scala
@@ -27,7 +27,7 @@ import org.apache.spark.memory.{MemoryConsumer, MemoryMode, TaskMemoryManager}
  * has been exceeded.
  */
 private[spark] abstract class Spillable[C](taskMemoryManager: TaskMemoryManager)
-  extends MemoryConsumer(taskMemoryManager) with Logging {
+  extends MemoryConsumer(taskMemoryManager, MemoryMode.ON_HEAP) with Logging {
   /**
    * Spills the current in-memory collection to disk, and releases the memory.
    *
diff --git a/sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/RowBasedKeyValueBatch.java b/sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/RowBasedKeyValueBatch.java
index 9613c872eab..40d360d84fb 100644
--- a/sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/RowBasedKeyValueBatch.java
+++ b/sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/RowBasedKeyValueBatch.java
@@ -95,7 +95,7 @@ public abstract class RowBasedKeyValueBatch extends MemoryConsumer implements Cl
 
   protected RowBasedKeyValueBatch(StructType keySchema, StructType valueSchema, int maxRows,
                                 TaskMemoryManager manager) {
-    super(manager, manager.pageSizeBytes(), manager.getTungstenMemoryMode());
+    super(manager, manager.getTungstenMemoryMode());
 
     this.keySchema = keySchema;
     this.valueSchema = valueSchema;
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashedRelation.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashedRelation.scala
index e25b9dd47e2..10c3ec000b0 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashedRelation.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashedRelation.scala
@@ -523,7 +523,7 @@ private[joins] object UnsafeHashedRelation {
  * see http://java-performance.info/implementing-world-fastest-java-int-to-int-hash-map/
  */
 private[execution] final class LongToUnsafeRowMap(val mm: TaskMemoryManager, capacity: Int)
-  extends MemoryConsumer(mm) with Externalizable with KryoSerializable {
+  extends MemoryConsumer(mm, MemoryMode.ON_HEAP) with Externalizable with KryoSerializable {
 
   // Whether the keys are stored in dense mode or not.
   private var isDense = false
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/python/RowQueue.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/python/RowQueue.scala
index eb12641f548..006c2a058ae 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/python/RowQueue.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/python/RowQueue.scala
@@ -174,7 +174,7 @@ private[python] case class HybridRowQueue(
     tempDir: File,
     numFields: Int,
     serMgr: SerializerManager)
-  extends MemoryConsumer(memManager) with RowQueue {
+  extends MemoryConsumer(memManager, memManager.getTungstenMemoryMode) with RowQueue {
 
   // Each buffer should have at least one row
   private var queues = new java.util.LinkedList[RowQueue]()
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/execution/python/RowQueueSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/execution/python/RowQueueSuite.scala
index 06077c94b66..4314e0d0ee3 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/execution/python/RowQueueSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/execution/python/RowQueueSuite.scala
@@ -21,7 +21,7 @@ import java.io.File
 
 import org.apache.spark.{SparkConf, SparkFunSuite}
 import org.apache.spark.internal.config._
-import org.apache.spark.memory.{TaskMemoryManager, TestMemoryManager}
+import org.apache.spark.memory.{MemoryMode, TaskMemoryManager, TestMemoryManager}
 import org.apache.spark.security.{CryptoStreamUtils, EncryptionFunSuite}
 import org.apache.spark.serializer.{JavaSerializer, SerializerManager}
 import org.apache.spark.sql.catalyst.expressions.UnsafeRow
@@ -94,48 +94,54 @@ class RowQueueSuite extends SparkFunSuite with EncryptionFunSuite {
     queue.close()
   }
 
-  encryptionTest("hybrid queue") { conf =>
-    val serManager = createSerializerManager(conf)
-    val mem = new TestMemoryManager(conf)
-    mem.limit(4<<10)
-    val taskM = new TaskMemoryManager(mem, 0)
-    val queue = HybridRowQueue(taskM, Utils.createTempDir().getCanonicalFile, 1, serManager)
-    val row = new UnsafeRow(1)
-    row.pointTo(new Array[Byte](16), 16)
-    val n = (4<<10) / 16 * 3
-    var i = 0
-    while (i < n) {
-      row.setLong(0, i)
-      assert(queue.add(row), "fail to add")
-      i += 1
-    }
-    assert(queue.numQueues() > 1, "should have more than one queue")
-    queue.spill(1<<20, null)
-    i = 0
-    while (i < n) {
-      val row = queue.remove()
-      assert(row != null, "fail to poll")
-      assert(row.getLong(0) == i, "does not match")
-      i += 1
-    }
+  Seq(true, false).foreach { isOffHeap =>
+    encryptionTest(s"hybrid queue (offHeap=$isOffHeap)") { conf =>
+      conf.set(MEMORY_OFFHEAP_ENABLED, isOffHeap)
+      if (isOffHeap) conf.set(MEMORY_OFFHEAP_SIZE, 1000L)
+      val serManager = createSerializerManager(conf)
+      val mem = new TestMemoryManager(conf)
+      mem.limit(4<<10)
+      val taskM = new TaskMemoryManager(mem, 0)
+      val queue = HybridRowQueue(taskM, Utils.createTempDir().getCanonicalFile, 1, serManager)
+      val mode = if (isOffHeap) MemoryMode.OFF_HEAP else MemoryMode.ON_HEAP
+      assert(queue.getMode === mode)
+      val row = new UnsafeRow(1)
+      row.pointTo(new Array[Byte](16), 16)
+      val n = (4<<10) / 16 * 3
+      var i = 0
+      while (i < n) {
+        row.setLong(0, i)
+        assert(queue.add(row), "fail to add")
+        i += 1
+      }
+      assert(queue.numQueues() > 1, "should have more than one queue")
+      queue.spill(1<<20, null)
+      i = 0
+      while (i < n) {
+        val row = queue.remove()
+        assert(row != null, "fail to poll")
+        assert(row.getLong(0) == i, "does not match")
+        i += 1
+      }
 
-    // fill again and spill
-    i = 0
-    while (i < n) {
-      row.setLong(0, i)
-      assert(queue.add(row), "fail to add")
-      i += 1
-    }
-    assert(queue.numQueues() > 1, "should have more than one queue")
-    queue.spill(1<<20, null)
-    assert(queue.numQueues() > 1, "should have more than one queue")
-    i = 0
-    while (i < n) {
-      val row = queue.remove()
-      assert(row != null, "fail to poll")
-      assert(row.getLong(0) == i, "does not match")
-      i += 1
+      // fill again and spill
+      i = 0
+      while (i < n) {
+        row.setLong(0, i)
+        assert(queue.add(row), "fail to add")
+        i += 1
+      }
+      assert(queue.numQueues() > 1, "should have more than one queue")
+      queue.spill(1<<20, null)
+      assert(queue.numQueues() > 1, "should have more than one queue")
+      i = 0
+      while (i < n) {
+        val row = queue.remove()
+        assert(row != null, "fail to poll")
+        assert(row.getLong(0) == i, "does not match")
+        i += 1
+      }
+      queue.close()
     }
-    queue.close()
   }
 }
