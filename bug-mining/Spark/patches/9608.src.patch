diff --git a/docs/sql-migration-guide.md b/docs/sql-migration-guide.md
index 3f9702a6b84..2694abb44e0 100644
--- a/docs/sql-migration-guide.md
+++ b/docs/sql-migration-guide.md
@@ -39,7 +39,6 @@ license: |
 - Since Spark 4.0, the default value of `spark.sql.orc.compression.codec` is changed from `snappy` to `zstd`. To restore the previous behavior, set `spark.sql.orc.compression.codec` to `snappy`.
 - Since Spark 4.0, `SELECT (*)` is equivalent to `SELECT struct(*)` instead of `SELECT *`. To restore the previous behavior, set `spark.sql.legacy.ignoreParenthesesAroundStar` to `true`.
 - Since Spark 4.0, the SQL config `spark.sql.legacy.allowZeroIndexInFormatString` is deprecated. Consider to change `strfmt` of the `format_string` function to use 1-based indexes. The first argument must be referenced by "1$", the second by "2$", etc.
-- Since Spark 4.0, the function `to_csv` no longer supports input with the data type `STRUCT`, `ARRAY`, `MAP`, `VARIANT` and `BINARY` (because the `CSV specification` does not have standards for these data types and cannot be read back using `from_csv`), Spark will throw `DATATYPE_MISMATCH.UNSUPPORTED_INPUT_TYPE` exception.
 - Since Spark 4.0, JDBC read option `preferTimestampNTZ=true` will not convert Postgres TIMESTAMP WITH TIME ZONE and TIME WITH TIME ZONE data types to TimestampNTZType, which is available in Spark 3.5. 
 - Since Spark 4.0, JDBC read option `preferTimestampNTZ=true` will not convert MySQL TIMESTAMP to TimestampNTZType, which is available in Spark 3.5. MySQL DATETIME is not affected.
 
diff --git a/python/pyspark/sql/functions/builtin.py b/python/pyspark/sql/functions/builtin.py
index f9d96778b88..a31465a7787 100644
--- a/python/pyspark/sql/functions/builtin.py
+++ b/python/pyspark/sql/functions/builtin.py
@@ -15543,6 +15543,8 @@ def schema_of_csv(csv: Union[Column, str], options: Optional[Dict[str, str]] = N
     return _invoke_function("schema_of_csv", col, _options_to_str(options))
 
 
+# TODO(SPARK-46654) Re-enable the `Example 2` test after fixing the display
+#  difference between Regular Spark and Spark Connect on `df.show`.
 @_try_remote_functions
 def to_csv(col: "ColumnOrName", options: Optional[Dict[str, str]] = None) -> Column:
     """
@@ -15584,7 +15586,19 @@ def to_csv(col: "ColumnOrName", options: Optional[Dict[str, str]] = None) -> Col
     |      2,Alice|
     +-------------+
 
-    Example 2: Converting a StructType with null values to a CSV string
+    Example 2: Converting a complex StructType to a CSV string
+
+    >>> from pyspark.sql import Row, functions as sf
+    >>> data = [(1, Row(age=2, name='Alice', scores=[100, 200, 300]))]
+    >>> df = spark.createDataFrame(data, ("key", "value"))
+    >>> df.select(sf.to_csv(df.value)).show(truncate=False) # doctest: +SKIP
+    +-----------------------+
+    |to_csv(value)          |
+    +-----------------------+
+    |2,Alice,"[100,200,300]"|
+    +-----------------------+
+
+    Example 3: Converting a StructType with null values to a CSV string
 
     >>> from pyspark.sql import Row, functions as sf
     >>> from pyspark.sql.types import StructType, StructField, IntegerType, StringType
@@ -15604,7 +15618,7 @@ def to_csv(col: "ColumnOrName", options: Optional[Dict[str, str]] = None) -> Col
     |       ,Alice|
     +-------------+
 
-    Example 3: Converting a StructType with different data types to a CSV string
+    Example 4: Converting a StructType with different data types to a CSV string
 
     >>> from pyspark.sql import Row, functions as sf
     >>> data = [(1, Row(age=2, name='Alice', isStudent=True))]
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/csvExpressions.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/csvExpressions.scala
index ff696ebaf60..a2d17617a10 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/csvExpressions.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/csvExpressions.scala
@@ -19,18 +19,15 @@ package org.apache.spark.sql.catalyst.expressions
 
 import java.io.CharArrayWriter
 
-import scala.annotation.tailrec
-
 import com.univocity.parsers.csv.CsvParser
 
-import org.apache.spark.SparkException
+import org.apache.spark.{SparkException, SparkIllegalArgumentException}
 import org.apache.spark.sql.catalyst.InternalRow
 import org.apache.spark.sql.catalyst.analysis.TypeCheckResult
-import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{DataTypeMismatch, TypeCheckSuccess}
+import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.DataTypeMismatch
 import org.apache.spark.sql.catalyst.csv._
 import org.apache.spark.sql.catalyst.expressions.codegen.{CodegenContext, CodegenFallback, ExprCode}
 import org.apache.spark.sql.catalyst.util._
-import org.apache.spark.sql.catalyst.util.TypeUtils._
 import org.apache.spark.sql.errors.{QueryCompilationErrors, QueryErrorsBase}
 import org.apache.spark.sql.internal.SQLConf
 import org.apache.spark.sql.types._
@@ -263,33 +260,16 @@ case class StructsToCsv(
       child = child,
       timeZoneId = None)
 
-  override def checkInputDataTypes(): TypeCheckResult = {
-    child.dataType match {
-      case schema: StructType if schema.map(_.dataType).forall(
-        dt => isSupportedDataType(dt)) => TypeCheckSuccess
-      case _ => DataTypeMismatch(
-        errorSubClass = "UNSUPPORTED_INPUT_TYPE",
-        messageParameters = Map(
-          "functionName" -> toSQLId(prettyName),
-          "dataType" -> toSQLType(child.dataType)
-        )
-      )
-    }
-  }
-
-  @tailrec
-  private def isSupportedDataType(dataType: DataType): Boolean = dataType match {
-    case _: VariantType | BinaryType => false
-    case _: AtomicType | CalendarIntervalType => true
-    case udt: UserDefinedType[_] => isSupportedDataType(udt.sqlType)
-    case _ => false
-  }
-
   @transient
   lazy val writer = new CharArrayWriter()
 
   @transient
-  lazy val inputSchema: StructType = child.dataType.asInstanceOf[StructType]
+  lazy val inputSchema: StructType = child.dataType match {
+    case st: StructType => st
+    case other => throw new SparkIllegalArgumentException(
+      errorClass = "_LEGACY_ERROR_TEMP_3234",
+      messageParameters = Map("other" -> other.catalogString))
+  }
 
   @transient
   lazy val gen = new UnivocityGenerator(
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/CsvFunctionsSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/CsvFunctionsSuite.scala
index c3d69b34ff9..5c6891f6a7f 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/CsvFunctionsSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/CsvFunctionsSuite.scala
@@ -17,7 +17,6 @@
 
 package org.apache.spark.sql
 
-import java.nio.charset.StandardCharsets
 import java.text.SimpleDateFormat
 import java.time.{Duration, LocalDateTime, Period}
 import java.util.Locale
@@ -32,7 +31,6 @@ import org.apache.spark.sql.test.SharedSparkSession
 import org.apache.spark.sql.types._
 import org.apache.spark.sql.types.DayTimeIntervalType.{DAY, HOUR, MINUTE, SECOND}
 import org.apache.spark.sql.types.YearMonthIntervalType.{MONTH, YEAR}
-import org.apache.spark.unsafe.types._
 
 class CsvFunctionsSuite extends QueryTest with SharedSparkSession {
   import testImplicits._
@@ -178,7 +176,7 @@ class CsvFunctionsSuite extends QueryTest with SharedSparkSession {
 
     checkError(
       exception = intercept[SparkUnsupportedOperationException] {
-        df.select(from_csv(lit("any value"), schema, options)).collect()
+        df.select(from_csv(to_csv($"value"), schema, options)).collect()
       },
       errorClass = "UNSUPPORTED_DATATYPE",
       parameters = Map("typeName" -> toSQLType(valueType))
@@ -296,19 +294,10 @@ class CsvFunctionsSuite extends QueryTest with SharedSparkSession {
   }
 
   test("to_csv with option (nullValue)") {
-    val rows = new java.util.ArrayList[Row]()
-    rows.add(Row(1L, Row(2L, "Alice", null)))
-
-    val valueType = StructType(Seq(
-      StructField("age", LongType),
-      StructField("name", StringType),
-      StructField("score", IntegerType)))
-
-    val schema = StructType(Seq(StructField("key", LongType), StructField("value", valueType)))
-    val df = spark.createDataFrame(rows, schema)
-
+    val df = Seq(Tuple1(Tuple1(null))).toDF("a")
     val options = Map("nullValue" -> "-").asJava
-    checkAnswer(df.select(to_csv($"value", options)), Row("2,Alice,-") :: Nil)
+
+    checkAnswer(df.select(to_csv($"a", options)), Row("-") :: Nil)
   }
 
   test("to_csv with option (dateFormat)") {
@@ -614,210 +603,4 @@ class CsvFunctionsSuite extends QueryTest with SharedSparkSession {
       $"csv", schema_of_csv("1,2\n2"), Map.empty[String, String].asJava))
     checkAnswer(actual, Row(Row(1, "2\n2")))
   }
-
-  test("SPARK-46654: from_csv/to_csv does not support ArrayType data") {
-    val rows = new java.util.ArrayList[Row]()
-    rows.add(Row(1L, Row(2L, "Alice", Array(100L, 200L, null, 300L))))
-
-    val valueSchema = StructType(Seq(
-      StructField("age", LongType),
-      StructField("name", StringType),
-      StructField("scores", ArrayType(LongType))))
-    val schema = StructType(Seq(
-      StructField("key", LongType),
-      StructField("value", valueSchema)))
-
-    val df = spark.createDataFrame(rows, schema)
-
-    checkError(
-      exception = intercept[AnalysisException] {
-        df.select(to_csv($"value")).collect()
-      },
-      errorClass = "DATATYPE_MISMATCH.UNSUPPORTED_INPUT_TYPE",
-      parameters = Map(
-        "functionName" -> "`to_csv`",
-        "dataType" -> "\"STRUCT<age: BIGINT, name: STRING, scores: ARRAY<BIGINT>>\"",
-        "sqlExpr" -> "\"to_csv(value)\""),
-      context = ExpectedContext(fragment = "to_csv", getCurrentClassCallSitePattern)
-    )
-
-    checkError(
-      exception = intercept[SparkUnsupportedOperationException] {
-        df.select(from_csv(lit("data"), valueSchema, Map.empty[String, String])).collect()
-      },
-      errorClass = "UNSUPPORTED_DATATYPE",
-      parameters = Map("typeName" -> "\"ARRAY<BIGINT>\"")
-    )
-  }
-
-  test("SPARK-46654: from_csv/to_csv does not support MapType data") {
-    val rows = new java.util.ArrayList[Row]()
-    rows.add(Row(1L, Row(2L, "Alice",
-      Map("math" -> 100L, "english" -> 200L, "science" -> null))))
-
-    val valueSchema = StructType(Seq(
-      StructField("age", LongType),
-      StructField("name", StringType),
-      StructField("scores", MapType(StringType, LongType))))
-    val schema = StructType(Seq(
-      StructField("key", LongType),
-      StructField("value", valueSchema)))
-
-    val df = spark.createDataFrame(rows, schema)
-
-    checkError(
-      exception = intercept[AnalysisException] {
-        df.select(to_csv($"value")).collect()
-      },
-      errorClass = "DATATYPE_MISMATCH.UNSUPPORTED_INPUT_TYPE",
-      parameters = Map(
-        "functionName" -> "`to_csv`",
-        "dataType" -> "\"STRUCT<age: BIGINT, name: STRING, scores: MAP<STRING, BIGINT>>\"",
-        "sqlExpr" -> "\"to_csv(value)\""),
-      context = ExpectedContext(fragment = "to_csv", getCurrentClassCallSitePattern)
-    )
-
-    checkError(
-      exception = intercept[SparkUnsupportedOperationException] {
-        df.select(from_csv(lit("data"), valueSchema, Map.empty[String, String])).collect()
-      },
-      errorClass = "UNSUPPORTED_DATATYPE",
-      parameters = Map("typeName" -> "\"MAP<STRING, BIGINT>\"")
-    )
-  }
-
-  test("SPARK-46654: from_csv/to_csv does not support StructType data") {
-    val rows = new java.util.ArrayList[Row]()
-    rows.add(Row(1L, Row(2L, "Alice", Row(100L, 200L, null))))
-
-    val valueSchema = StructType(Seq(
-      StructField("age", LongType),
-      StructField("name", StringType),
-      StructField("scores", StructType(Seq(
-        StructField("id1", LongType),
-        StructField("id2", LongType),
-        StructField("id3", LongType))))))
-    val schema = StructType(Seq(
-      StructField("key", LongType),
-      StructField("value", valueSchema)))
-
-    val df = spark.createDataFrame(rows, schema)
-
-    checkError(
-      exception = intercept[AnalysisException] {
-        df.select(to_csv($"value")).collect()
-      },
-      errorClass = "DATATYPE_MISMATCH.UNSUPPORTED_INPUT_TYPE",
-      parameters = Map(
-        "functionName" -> "`to_csv`",
-        "dataType" -> ("\"STRUCT<age: BIGINT, name: STRING, " +
-          "scores: STRUCT<id1: BIGINT, id2: BIGINT, id3: BIGINT>>\""),
-        "sqlExpr" -> "\"to_csv(value)\""),
-      context = ExpectedContext(fragment = "to_csv", getCurrentClassCallSitePattern)
-    )
-
-    checkError(
-      exception = intercept[SparkUnsupportedOperationException] {
-        df.select(from_csv(lit("data"), valueSchema, Map.empty[String, String])).collect()
-      },
-      errorClass = "UNSUPPORTED_DATATYPE",
-      parameters = Map("typeName" -> "\"STRUCT<id1: BIGINT, id2: BIGINT, id3: BIGINT>\"")
-    )
-  }
-
-  test("SPARK-46654: from_csv/to_csv does not support VariantType data") {
-    val rows = new java.util.ArrayList[Row]()
-    rows.add(Row(1L, Row(2L, "Alice", new VariantVal(Array[Byte](1, 2, 3), Array[Byte](4, 5)))))
-
-    val valueSchema = StructType(Seq(
-      StructField("age", LongType),
-      StructField("name", StringType),
-      StructField("v", VariantType)))
-    val schema = StructType(Seq(
-      StructField("key", LongType),
-      StructField("value", valueSchema)))
-
-    val df = spark.createDataFrame(rows, schema)
-
-    checkError(
-      exception = intercept[AnalysisException] {
-        df.select(to_csv($"value")).collect()
-      },
-      errorClass = "DATATYPE_MISMATCH.UNSUPPORTED_INPUT_TYPE",
-      parameters = Map(
-        "functionName" -> "`to_csv`",
-        "dataType" -> "\"STRUCT<age: BIGINT, name: STRING, v: VARIANT>\"",
-        "sqlExpr" -> "\"to_csv(value)\""),
-      context = ExpectedContext(fragment = "to_csv", getCurrentClassCallSitePattern)
-    )
-
-    checkError(
-      exception = intercept[SparkUnsupportedOperationException] {
-        df.select(from_csv(lit("data"), valueSchema, Map.empty[String, String])).collect()
-      },
-      errorClass = "UNSUPPORTED_DATATYPE",
-      parameters = Map("typeName" -> "\"VARIANT\"")
-    )
-  }
-
-  test("SPARK-46654: from_csv/to_csv does not support BinaryType data") {
-    val rows = new java.util.ArrayList[Row]()
-    rows.add(Row(1L, Row(2L, "Alice", "b".getBytes(StandardCharsets.UTF_8))))
-
-    val valueSchema = StructType(Seq(
-      StructField("age", LongType),
-      StructField("name", StringType),
-      StructField("b", BinaryType)))
-    val schema = StructType(Seq(
-      StructField("key", LongType),
-      StructField("value", valueSchema)))
-
-    val df = spark.createDataFrame(rows, schema)
-
-    checkError(
-      exception = intercept[AnalysisException] {
-        df.select(to_csv($"value")).collect()
-      },
-      errorClass = "DATATYPE_MISMATCH.UNSUPPORTED_INPUT_TYPE",
-      parameters = Map(
-        "functionName" -> "`to_csv`",
-        "dataType" -> "\"STRUCT<age: BIGINT, name: STRING, b: BINARY>\"",
-        "sqlExpr" -> "\"to_csv(value)\""),
-      context = ExpectedContext(fragment = "to_csv", getCurrentClassCallSitePattern)
-    )
-
-    checkError(
-      exception = intercept[SparkUnsupportedOperationException] {
-        df.select(from_csv(lit("data"), valueSchema, Map.empty[String, String])).collect()
-      },
-      errorClass = "UNSUPPORTED_DATATYPE",
-      parameters = Map("typeName" -> "\"BINARY\"")
-    )
-  }
-
-  test("SPARK-46654: from_csv/to_csv does not support NullType data") {
-    val df = Seq(Tuple1(Tuple1(null))).toDF("value")
-    val valueSchema = df.schema
-    val options = Map("nullValue" -> "-")
-
-    checkError(
-      exception = intercept[AnalysisException] {
-        df.select(to_csv($"value", options.asJava)).collect()
-      },
-      errorClass = "DATATYPE_MISMATCH.UNSUPPORTED_INPUT_TYPE",
-      parameters = Map(
-        "functionName" -> "`to_csv`",
-        "dataType" -> "\"STRUCT<_1: VOID>\"",
-        "sqlExpr" -> "\"to_csv(value)\""),
-      context = ExpectedContext(fragment = "to_csv", getCurrentClassCallSitePattern)
-    )
-
-    checkError(
-      exception = intercept[SparkUnsupportedOperationException] {
-        df.select(from_csv(lit("data"), valueSchema, options)).collect()
-      },
-      errorClass = "UNSUPPORTED_DATATYPE",
-      parameters = Map("typeName" -> "\"STRUCT<_1: VOID>\"")
-    )
-  }
 }
