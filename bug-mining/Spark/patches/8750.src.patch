diff --git a/connector/protobuf/src/test/scala/org/apache/spark/sql/protobuf/ProtobufCatalystDataConversionSuite.scala b/connector/protobuf/src/test/scala/org/apache/spark/sql/protobuf/ProtobufCatalystDataConversionSuite.scala
index ec75ebb5507..3e9273835e3 100644
--- a/connector/protobuf/src/test/scala/org/apache/spark/sql/protobuf/ProtobufCatalystDataConversionSuite.scala
+++ b/connector/protobuf/src/test/scala/org/apache/spark/sql/protobuf/ProtobufCatalystDataConversionSuite.scala
@@ -34,9 +34,10 @@ import org.apache.spark.unsafe.types.UTF8String
 class ProtobufCatalystDataConversionSuite
     extends SparkFunSuite
     with SharedSparkSession
-    with ExpressionEvalHelper {
+    with ExpressionEvalHelper
+    with ProtobufTestBase {
 
-  private val testFileDesc = testFile("catalyst_types.desc").replace("file:/", "/")
+  private val testFileDesc = testFile("catalyst_types.desc", "protobuf/catalyst_types.desc")
   private val javaClassNamePrefix = "org.apache.spark.sql.protobuf.protos.CatalystTypes$"
 
   private def checkResultWithEval(
diff --git a/connector/protobuf/src/test/scala/org/apache/spark/sql/protobuf/ProtobufFunctionsSuite.scala b/connector/protobuf/src/test/scala/org/apache/spark/sql/protobuf/ProtobufFunctionsSuite.scala
index 00ec56f90a6..e493bc66ca7 100644
--- a/connector/protobuf/src/test/scala/org/apache/spark/sql/protobuf/ProtobufFunctionsSuite.scala
+++ b/connector/protobuf/src/test/scala/org/apache/spark/sql/protobuf/ProtobufFunctionsSuite.scala
@@ -32,11 +32,12 @@ import org.apache.spark.sql.protobuf.utils.ProtobufUtils
 import org.apache.spark.sql.test.SharedSparkSession
 import org.apache.spark.sql.types.{DayTimeIntervalType, IntegerType, StringType, StructField, StructType, TimestampType}
 
-class ProtobufFunctionsSuite extends QueryTest with SharedSparkSession with Serializable {
+class ProtobufFunctionsSuite extends QueryTest with SharedSparkSession with ProtobufTestBase
+  with Serializable {
 
   import testImplicits._
 
-  val testFileDesc = testFile("functions_suite.desc").replace("file:/", "/")
+  val testFileDesc = testFile("functions_suite.desc", "protobuf/functions_suite.desc")
   private val javaClassNamePrefix = "org.apache.spark.sql.protobuf.protos.SimpleMessageProtos$"
 
   /**
@@ -458,7 +459,7 @@ class ProtobufFunctionsSuite extends QueryTest with SharedSparkSession with Seri
   }
 
   test("Handle extra fields : oldProducer -> newConsumer") {
-    val testFileDesc = testFile("catalyst_types.desc").replace("file:/", "/")
+    val testFileDesc = testFile("catalyst_types.desc", "protobuf/catalyst_types.desc")
     val oldProducer = ProtobufUtils.buildDescriptor(testFileDesc, "oldProducer")
     val newConsumer = ProtobufUtils.buildDescriptor(testFileDesc, "newConsumer")
 
@@ -498,7 +499,7 @@ class ProtobufFunctionsSuite extends QueryTest with SharedSparkSession with Seri
   }
 
   test("Handle extra fields : newProducer -> oldConsumer") {
-    val testFileDesc = testFile("catalyst_types.desc").replace("file:/", "/")
+    val testFileDesc = testFile("catalyst_types.desc", "protobuf/catalyst_types.desc")
     val newProducer = ProtobufUtils.buildDescriptor(testFileDesc, "newProducer")
     val oldConsumer = ProtobufUtils.buildDescriptor(testFileDesc, "oldConsumer")
 
@@ -680,7 +681,8 @@ class ProtobufFunctionsSuite extends QueryTest with SharedSparkSession with Seri
 
   test("raise cannot construct protobuf descriptor error") {
     val df = Seq(ByteString.empty().toByteArray).toDF("value")
-    val testFileDescriptor = testFile("basicmessage_noimports.desc").replace("file:/", "/")
+    val testFileDescriptor =
+      testFile("basicmessage_noimports.desc", "protobuf/basicmessage_noimports.desc")
 
     val e = intercept[AnalysisException] {
       df.select(functions.from_protobuf($"value", "BasicMessage", testFileDescriptor) as 'sample)
diff --git a/connector/protobuf/src/test/scala/org/apache/spark/sql/protobuf/ProtobufSerdeSuite.scala b/connector/protobuf/src/test/scala/org/apache/spark/sql/protobuf/ProtobufSerdeSuite.scala
index 22b9d58bbd4..87ed5340943 100644
--- a/connector/protobuf/src/test/scala/org/apache/spark/sql/protobuf/ProtobufSerdeSuite.scala
+++ b/connector/protobuf/src/test/scala/org/apache/spark/sql/protobuf/ProtobufSerdeSuite.scala
@@ -31,12 +31,12 @@ import org.apache.spark.sql.types.{IntegerType, StructType}
  * Tests for [[ProtobufSerializer]] and [[ProtobufDeserializer]] with a more specific focus on
  * those classes.
  */
-class ProtobufSerdeSuite extends SharedSparkSession {
+class ProtobufSerdeSuite extends SharedSparkSession with ProtobufTestBase {
 
   import ProtoSerdeSuite._
   import ProtoSerdeSuite.MatchType._
 
-  val testFileDesc = testFile("serde_suite.desc").replace("file:/", "/")
+  val testFileDesc = testFile("serde_suite.desc", "protobuf/serde_suite.desc")
   private val javaClassNamePrefix = "org.apache.spark.sql.protobuf.protos.SerdeSuiteProtos$"
 
   test("Test basic conversion") {
@@ -179,7 +179,7 @@ class ProtobufSerdeSuite extends SharedSparkSession {
 
   test("raise cannot parse and construct protobuf descriptor error") {
     // passing serde_suite.proto instead serde_suite.desc
-    var testFileDesc = testFile("serde_suite.proto").replace("file:/", "/")
+    var testFileDesc = testFile("serde_suite.proto", "protobuf/serde_suite.proto")
     val e1 = intercept[AnalysisException] {
       ProtobufUtils.buildDescriptor(testFileDesc, "FieldMissingInSQLRoot")
     }
@@ -189,7 +189,7 @@ class ProtobufSerdeSuite extends SharedSparkSession {
       errorClass = "CANNOT_PARSE_PROTOBUF_DESCRIPTOR",
       parameters = Map("descFilePath" -> testFileDesc))
 
-    testFileDesc = testFile("basicmessage_noimports.desc").replace("file:/", "/")
+    testFileDesc = testFile("basicmessage_noimports.desc", "protobuf/basicmessage_noimports.desc")
     val e2 = intercept[AnalysisException] {
       ProtobufUtils.buildDescriptor(testFileDesc, "FieldMissingInSQLRoot")
     }
diff --git a/connector/protobuf/src/test/scala/org/apache/spark/sql/protobuf/ProtobufTestBase.scala b/connector/protobuf/src/test/scala/org/apache/spark/sql/protobuf/ProtobufTestBase.scala
new file mode 100644
index 00000000000..831b4a26c06
--- /dev/null
+++ b/connector/protobuf/src/test/scala/org/apache/spark/sql/protobuf/ProtobufTestBase.scala
@@ -0,0 +1,37 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.spark.sql.protobuf
+
+import org.apache.spark.sql.test.SQLTestUtils
+
+trait ProtobufTestBase extends SQLTestUtils {
+
+  /**
+   * Returns full path to the given file in the resource folder,
+   * if the first choice throw NPE, try to return the full path of alternative.
+   * The result path doesn't contain the `file:/` protocol part.
+   */
+  protected def testFile(fileName: String, alternateFileName: String): String = {
+    val ret = try {
+      testFile(fileName)
+    } catch {
+      case _: NullPointerException => testFile(alternateFileName)
+    }
+    ret.replace("file:/", "/")
+  }
+}
