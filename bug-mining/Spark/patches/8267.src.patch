diff --git a/python/pyspark/__init__.py b/python/pyspark/__init__.py
index 70392fb1df4..aab95aded04 100644
--- a/python/pyspark/__init__.py
+++ b/python/pyspark/__init__.py
@@ -57,7 +57,7 @@ from pyspark.util import InheritableThread, inheritable_thread_target
 from pyspark.storagelevel import StorageLevel
 from pyspark.accumulators import Accumulator, AccumulatorParam
 from pyspark.broadcast import Broadcast
-from pyspark.serializers import MarshalSerializer, PickleSerializer
+from pyspark.serializers import MarshalSerializer, CPickleSerializer
 from pyspark.taskcontext import TaskContext, BarrierTaskContext, BarrierTaskInfo
 from pyspark.profiler import Profiler, BasicProfiler
 from pyspark.version import __version__
@@ -136,7 +136,7 @@ __all__ = [
     "Accumulator",
     "AccumulatorParam",
     "MarshalSerializer",
-    "PickleSerializer",
+    "CPickleSerializer",
     "StatusTracker",
     "SparkJobInfo",
     "SparkStageInfo",
diff --git a/python/pyspark/__init__.pyi b/python/pyspark/__init__.pyi
index 35df545ee64..fb045f2e5c5 100644
--- a/python/pyspark/__init__.pyi
+++ b/python/pyspark/__init__.pyi
@@ -38,7 +38,7 @@ from pyspark.profiler import (  # noqa: F401
 from pyspark.rdd import RDD as RDD, RDDBarrier as RDDBarrier  # noqa: F401
 from pyspark.serializers import (  # noqa: F401
     MarshalSerializer as MarshalSerializer,
-    PickleSerializer as PickleSerializer,
+    CPickleSerializer as CPickleSerializer,
 )
 from pyspark.status import (  # noqa: F401
     SparkJobInfo as SparkJobInfo,
diff --git a/python/pyspark/accumulators.py b/python/pyspark/accumulators.py
index c43ebe417b5..2ea2a4952e0 100644
--- a/python/pyspark/accumulators.py
+++ b/python/pyspark/accumulators.py
@@ -20,13 +20,13 @@ import select
 import struct
 import socketserver as SocketServer
 import threading
-from pyspark.serializers import read_int, PickleSerializer
+from pyspark.serializers import read_int, CPickleSerializer
 
 
 __all__ = ["Accumulator", "AccumulatorParam"]
 
 
-pickleSer = PickleSerializer()
+pickleSer = CPickleSerializer()
 
 # Holds accumulators registered on the current machine, keyed by ID. This is then used to send
 # the local accumulator updates back to the driver program at the end of a task.
diff --git a/python/pyspark/context.py b/python/pyspark/context.py
index 2c789947af9..d8d24337852 100644
--- a/python/pyspark/context.py
+++ b/python/pyspark/context.py
@@ -35,7 +35,7 @@ from pyspark.conf import SparkConf
 from pyspark.files import SparkFiles
 from pyspark.java_gateway import launch_gateway, local_connect_and_auth
 from pyspark.serializers import (
-    PickleSerializer,
+    CPickleSerializer,
     BatchedSerializer,
     UTF8Deserializer,
     PairDeserializer,
@@ -142,7 +142,7 @@ class SparkContext(object):
         pyFiles=None,
         environment=None,
         batchSize=0,
-        serializer=PickleSerializer(),
+        serializer=CPickleSerializer(),
         conf=None,
         gateway=None,
         jsc=None,
@@ -814,7 +814,7 @@ class SparkContext(object):
                and value Writable classes
             2. Serialization is attempted via Pickle pickling
             3. If this fails, the fallback is to call 'toString' on each key and value
-            4. :class:`PickleSerializer` is used to deserialize pickled objects on the Python side
+            4. :class:`CPickleSerializer` is used to deserialize pickled objects on the Python side
 
         Parameters
         ----------
diff --git a/python/pyspark/ml/common.py b/python/pyspark/ml/common.py
index 194a492aff7..61b20f131d2 100644
--- a/python/pyspark/ml/common.py
+++ b/python/pyspark/ml/common.py
@@ -27,7 +27,7 @@ from py4j.java_collections import JavaArray, JavaList
 
 import pyspark.context
 from pyspark import RDD, SparkContext
-from pyspark.serializers import PickleSerializer, AutoBatchedSerializer
+from pyspark.serializers import CPickleSerializer, AutoBatchedSerializer
 from pyspark.sql import DataFrame, SparkSession
 
 # Hack for support float('inf') in Py4j
@@ -65,7 +65,7 @@ def _to_java_object_rdd(rdd: RDD) -> JavaObject:
     It will convert each Python object into Java object by Pickle, whenever the
     RDD is serialized in batch or not.
     """
-    rdd = rdd._reserialize(AutoBatchedSerializer(PickleSerializer()))  # type: ignore[attr-defined]
+    rdd = rdd._reserialize(AutoBatchedSerializer(CPickleSerializer()))  # type: ignore[attr-defined]
     return rdd.ctx._jvm.org.apache.spark.ml.python.MLSerDe.pythonToJava(rdd._jrdd, True)  # type: ignore[attr-defined]
 
 
@@ -84,7 +84,7 @@ def _py2java(sc: SparkContext, obj: Any) -> JavaObject:
     elif isinstance(obj, (int, float, bool, bytes, str)):
         pass
     else:
-        data = bytearray(PickleSerializer().dumps(obj))
+        data = bytearray(CPickleSerializer().dumps(obj))
         obj = sc._jvm.org.apache.spark.ml.python.MLSerDe.loads(data)  # type: ignore[attr-defined]
     return obj
 
@@ -113,7 +113,7 @@ def _java2py(sc: SparkContext, r: "JavaObjectOrPickleDump", encoding: str = "byt
                 pass  # not picklable
 
     if isinstance(r, (bytearray, bytes)):
-        r = PickleSerializer().loads(bytes(r), encoding=encoding)
+        r = CPickleSerializer().loads(bytes(r), encoding=encoding)
     return r
 
 
diff --git a/python/pyspark/ml/tests/test_linalg.py b/python/pyspark/ml/tests/test_linalg.py
index 5db6c048bfc..dfdd32e98eb 100644
--- a/python/pyspark/ml/tests/test_linalg.py
+++ b/python/pyspark/ml/tests/test_linalg.py
@@ -20,7 +20,7 @@ import array as pyarray
 
 from numpy import arange, array, array_equal, inf, ones, tile, zeros
 
-from pyspark.serializers import PickleSerializer
+from pyspark.serializers import CPickleSerializer
 from pyspark.ml.linalg import (
     DenseMatrix,
     DenseVector,
@@ -37,7 +37,7 @@ from pyspark.sql import Row
 
 class VectorTests(MLlibTestCase):
     def _test_serialize(self, v):
-        ser = PickleSerializer()
+        ser = CPickleSerializer()
         self.assertEqual(v, ser.loads(ser.dumps(v)))
         jvec = self.sc._jvm.org.apache.spark.ml.python.MLSerDe.loads(bytearray(ser.dumps(v)))
         nv = ser.loads(bytes(self.sc._jvm.org.apache.spark.ml.python.MLSerDe.dumps(jvec)))
diff --git a/python/pyspark/mllib/common.py b/python/pyspark/mllib/common.py
index 540545dc111..5f109be2a17 100644
--- a/python/pyspark/mllib/common.py
+++ b/python/pyspark/mllib/common.py
@@ -27,7 +27,7 @@ from py4j.java_collections import JavaArray, JavaList
 
 import pyspark.context
 from pyspark import RDD, SparkContext
-from pyspark.serializers import PickleSerializer, AutoBatchedSerializer
+from pyspark.serializers import CPickleSerializer, AutoBatchedSerializer
 from pyspark.sql import DataFrame, SparkSession
 
 # Hack for support float('inf') in Py4j
@@ -67,7 +67,7 @@ def _to_java_object_rdd(rdd: RDD) -> JavaObject:
     It will convert each Python object into Java object by Pickle, whenever the
     RDD is serialized in batch or not.
     """
-    rdd = rdd._reserialize(AutoBatchedSerializer(PickleSerializer()))  # type: ignore[attr-defined]
+    rdd = rdd._reserialize(AutoBatchedSerializer(CPickleSerializer()))  # type: ignore[attr-defined]
     return rdd.ctx._jvm.org.apache.spark.mllib.api.python.SerDe.pythonToJava(rdd._jrdd, True)  # type: ignore[attr-defined]
 
 
@@ -86,7 +86,7 @@ def _py2java(sc: SparkContext, obj: Any) -> JavaObject:
     elif isinstance(obj, (int, float, bool, bytes, str)):
         pass
     else:
-        data = bytearray(PickleSerializer().dumps(obj))
+        data = bytearray(CPickleSerializer().dumps(obj))
         obj = sc._jvm.org.apache.spark.mllib.api.python.SerDe.loads(data)  # type: ignore[attr-defined]
     return obj
 
@@ -115,7 +115,7 @@ def _java2py(sc: SparkContext, r: "JavaObjectOrPickleDump", encoding: str = "byt
                 pass  # not pickable
 
     if isinstance(r, (bytearray, bytes)):
-        r = PickleSerializer().loads(bytes(r), encoding=encoding)
+        r = CPickleSerializer().loads(bytes(r), encoding=encoding)
     return r
 
 
diff --git a/python/pyspark/mllib/tests/test_algorithms.py b/python/pyspark/mllib/tests/test_algorithms.py
index 6927b75e3d4..fd9f348f31b 100644
--- a/python/pyspark/mllib/tests/test_algorithms.py
+++ b/python/pyspark/mllib/tests/test_algorithms.py
@@ -26,7 +26,7 @@ from py4j.protocol import Py4JJavaError
 from pyspark.mllib.fpm import FPGrowth
 from pyspark.mllib.recommendation import Rating
 from pyspark.mllib.regression import LabeledPoint
-from pyspark.serializers import PickleSerializer
+from pyspark.serializers import CPickleSerializer
 from pyspark.testing.mllibutils import MLlibTestCase
 
 
@@ -303,7 +303,7 @@ class ListTests(MLlibTestCase):
 
 class ALSTests(MLlibTestCase):
     def test_als_ratings_serialize(self):
-        ser = PickleSerializer()
+        ser = CPickleSerializer()
         r = Rating(7, 1123, 3.14)
         jr = self.sc._jvm.org.apache.spark.mllib.api.python.SerDe.loads(bytearray(ser.dumps(r)))
         nr = ser.loads(bytes(self.sc._jvm.org.apache.spark.mllib.api.python.SerDe.dumps(jr)))
@@ -312,7 +312,7 @@ class ALSTests(MLlibTestCase):
         self.assertAlmostEqual(r.rating, nr.rating, 2)
 
     def test_als_ratings_id_long_error(self):
-        ser = PickleSerializer()
+        ser = CPickleSerializer()
         r = Rating(1205640308657491975, 50233468418, 1.0)
         # rating user id exceeds max int value, should fail when pickled
         self.assertRaises(
diff --git a/python/pyspark/mllib/tests/test_linalg.py b/python/pyspark/mllib/tests/test_linalg.py
index e43482dc415..d60396b633c 100644
--- a/python/pyspark/mllib/tests/test_linalg.py
+++ b/python/pyspark/mllib/tests/test_linalg.py
@@ -21,7 +21,7 @@ import unittest
 from numpy import array, array_equal, zeros, arange, tile, ones, inf
 
 import pyspark.ml.linalg as newlinalg
-from pyspark.serializers import PickleSerializer
+from pyspark.serializers import CPickleSerializer
 from pyspark.mllib.linalg import (  # type: ignore[attr-defined]
     Vector,
     SparseVector,
@@ -43,7 +43,7 @@ from pyspark.testing.utils import have_scipy
 
 class VectorTests(MLlibTestCase):
     def _test_serialize(self, v):
-        ser = PickleSerializer()
+        ser = CPickleSerializer()
         self.assertEqual(v, ser.loads(ser.dumps(v)))
         jvec = self.sc._jvm.org.apache.spark.mllib.api.python.SerDe.loads(bytearray(ser.dumps(v)))
         nv = ser.loads(bytes(self.sc._jvm.org.apache.spark.mllib.api.python.SerDe.dumps(jvec)))
@@ -512,7 +512,7 @@ class SciPyTests(MLlibTestCase):
     def test_serialize(self):
         from scipy.sparse import lil_matrix
 
-        ser = PickleSerializer()
+        ser = CPickleSerializer()
         lil = lil_matrix((4, 1))
         lil[1, 0] = 1
         lil[3, 0] = 2
diff --git a/python/pyspark/rdd.py b/python/pyspark/rdd.py
index b997932c807..2452d692370 100644
--- a/python/pyspark/rdd.py
+++ b/python/pyspark/rdd.py
@@ -39,7 +39,7 @@ from pyspark.serializers import (
     CartesianDeserializer,
     CloudPickleSerializer,
     PairDeserializer,
-    PickleSerializer,
+    CPickleSerializer,
     pack_long,
     read_int,
     write_int,
@@ -259,7 +259,7 @@ class RDD(object):
     operated on in parallel.
     """
 
-    def __init__(self, jrdd, ctx, jrdd_deserializer=AutoBatchedSerializer(PickleSerializer())):
+    def __init__(self, jrdd, ctx, jrdd_deserializer=AutoBatchedSerializer(CPickleSerializer())):
         self._jrdd = jrdd
         self.is_cached = False
         self.is_checkpointed = False
@@ -270,7 +270,7 @@ class RDD(object):
         self.partitioner = None
 
     def _pickled(self):
-        return self._reserialize(AutoBatchedSerializer(PickleSerializer()))
+        return self._reserialize(AutoBatchedSerializer(CPickleSerializer()))
 
     def id(self):
         """
@@ -1841,7 +1841,7 @@ class RDD(object):
     def saveAsPickleFile(self, path, batchSize=10):
         """
         Save this RDD as a SequenceFile of serialized objects. The serializer
-        used is :class:`pyspark.serializers.PickleSerializer`, default batch size
+        used is :class:`pyspark.serializers.CPickleSerializer`, default batch size
         is 10.
 
         Examples
@@ -1854,9 +1854,9 @@ class RDD(object):
         ['1', '2', 'rdd', 'spark']
         """
         if batchSize == 0:
-            ser = AutoBatchedSerializer(PickleSerializer())
+            ser = AutoBatchedSerializer(CPickleSerializer())
         else:
-            ser = BatchedSerializer(PickleSerializer(), batchSize)
+            ser = BatchedSerializer(CPickleSerializer(), batchSize)
         self._reserialize(ser)._jrdd.saveAsObjectFile(path)
 
     def saveAsTextFile(self, path, compressionCodecClass=None):
@@ -2520,7 +2520,7 @@ class RDD(object):
             # Decrease the batch size in order to distribute evenly the elements across output
             # partitions. Otherwise, repartition will possibly produce highly skewed partitions.
             batchSize = min(10, self.ctx._batchSize or 1024)
-            ser = BatchedSerializer(PickleSerializer(), batchSize)
+            ser = BatchedSerializer(CPickleSerializer(), batchSize)
             selfCopy = self._reserialize(ser)
             jrdd_deserializer = selfCopy._jrdd_deserializer
             jrdd = selfCopy._jrdd.coalesce(numPartitions, shuffle)
@@ -2551,7 +2551,7 @@ class RDD(object):
             return 1  # not batched
 
         def batch_as(rdd, batchSize):
-            return rdd._reserialize(BatchedSerializer(PickleSerializer(), batchSize))
+            return rdd._reserialize(BatchedSerializer(CPickleSerializer(), batchSize))
 
         my_batch = get_batch_size(self._jrdd_deserializer)
         other_batch = get_batch_size(other._jrdd_deserializer)
diff --git a/python/pyspark/serializers.py b/python/pyspark/serializers.py
index 06eed0a3bc4..766ea64d905 100644
--- a/python/pyspark/serializers.py
+++ b/python/pyspark/serializers.py
@@ -19,7 +19,7 @@
 PySpark supports custom serializers for transferring data; this can improve
 performance.
 
-By default, PySpark uses :class:`PickleSerializer` to serialize objects using Python's
+By default, PySpark uses :class:`CloudPickleSerializer` to serialize objects using Python's
 `cPickle` serializer, which can serialize nearly any Python object.
 Other serializers, like :class:`MarshalSerializer`, support fewer datatypes but can be
 faster.
@@ -69,7 +69,13 @@ from pyspark import cloudpickle
 from pyspark.util import print_exec  # type: ignore
 
 
-__all__ = ["PickleSerializer", "MarshalSerializer", "UTF8Deserializer"]
+__all__ = [
+    "PickleSerializer",
+    "CPickleSerializer",
+    "CloudPickleSerializer",
+    "MarshalSerializer",
+    "UTF8Deserializer",
+]
 
 
 class SpecialLengths(object):
@@ -344,78 +350,81 @@ class NoOpSerializer(FramedSerializer):
         return obj
 
 
-# Hack namedtuple, make it picklable
-
-__cls = {}  # type: ignore
-
-
-def _restore(name, fields, value):
-    """Restore an object of namedtuple"""
-    k = (name, fields)
-    cls = __cls.get(k)
-    if cls is None:
-        cls = collections.namedtuple(name, fields)
-        __cls[k] = cls
-    return cls(*value)
-
-
-def _hack_namedtuple(cls):
-    """Make class generated by namedtuple picklable"""
-    name = cls.__name__
-    fields = cls._fields
-
-    def __reduce__(self):
-        return (_restore, (name, fields, tuple(self)))
-
-    cls.__reduce__ = __reduce__
-    cls._is_namedtuple_ = True
-    return cls
-
-
-def _hijack_namedtuple():
-    """Hack namedtuple() to make it picklable"""
-    # hijack only one time
-    if hasattr(collections.namedtuple, "__hijack"):
-        return
-
-    global _old_namedtuple  # or it will put in closure
-    global _old_namedtuple_kwdefaults  # or it will put in closure too
-
-    def _copy_func(f):
-        return types.FunctionType(
-            f.__code__, f.__globals__, f.__name__, f.__defaults__, f.__closure__
-        )
-
-    _old_namedtuple = _copy_func(collections.namedtuple)
-    _old_namedtuple_kwdefaults = collections.namedtuple.__kwdefaults__
-
-    def namedtuple(*args, **kwargs):
-        for k, v in _old_namedtuple_kwdefaults.items():
-            kwargs[k] = kwargs.get(k, v)
-        cls = _old_namedtuple(*args, **kwargs)
-        return _hack_namedtuple(cls)
-
-    # replace namedtuple with the new one
-    collections.namedtuple.__globals__["_old_namedtuple_kwdefaults"] = _old_namedtuple_kwdefaults
-    collections.namedtuple.__globals__["_old_namedtuple"] = _old_namedtuple
-    collections.namedtuple.__globals__["_hack_namedtuple"] = _hack_namedtuple
-    collections.namedtuple.__code__ = namedtuple.__code__
-    collections.namedtuple.__hijack = 1
-
-    # hack the cls already generated by namedtuple.
-    # Those created in other modules can be pickled as normal,
-    # so only hack those in __main__ module
-    for n, o in sys.modules["__main__"].__dict__.items():
-        if (
-            type(o) is type
-            and o.__base__ is tuple
-            and hasattr(o, "_fields")
-            and "__reduce__" not in o.__dict__
-        ):
-            _hack_namedtuple(o)  # hack inplace
-
+if sys.version_info < (3, 8):
+    # Hack namedtuple, make it picklable.
+    # For Python 3.8+, we use CPickle-based cloudpickle.
+    # For Python 3.7 and below, we use legacy build-in CPickle which
+    # requires namedtuple hack.
+    # The whole hack here should be removed once we drop Python 3.7.
+
+    __cls = {}  # type: ignore
+
+    def _restore(name, fields, value):
+        """Restore an object of namedtuple"""
+        k = (name, fields)
+        cls = __cls.get(k)
+        if cls is None:
+            cls = collections.namedtuple(name, fields)
+            __cls[k] = cls
+        return cls(*value)
+
+    def _hack_namedtuple(cls):
+        """Make class generated by namedtuple picklable"""
+        name = cls.__name__
+        fields = cls._fields
+
+        def __reduce__(self):
+            return (_restore, (name, fields, tuple(self)))
+
+        cls.__reduce__ = __reduce__
+        cls._is_namedtuple_ = True
+        return cls
+
+    def _hijack_namedtuple():
+        """Hack namedtuple() to make it picklable"""
+        # hijack only one time
+        if hasattr(collections.namedtuple, "__hijack"):
+            return
 
-_hijack_namedtuple()
+        global _old_namedtuple  # or it will put in closure
+        global _old_namedtuple_kwdefaults  # or it will put in closure too
+
+        def _copy_func(f):
+            return types.FunctionType(
+                f.__code__, f.__globals__, f.__name__, f.__defaults__, f.__closure__
+            )
+
+        _old_namedtuple = _copy_func(collections.namedtuple)
+        _old_namedtuple_kwdefaults = collections.namedtuple.__kwdefaults__
+
+        def namedtuple(*args, **kwargs):
+            for k, v in _old_namedtuple_kwdefaults.items():
+                kwargs[k] = kwargs.get(k, v)
+            cls = _old_namedtuple(*args, **kwargs)
+            return _hack_namedtuple(cls)
+
+        # replace namedtuple with the new one
+        collections.namedtuple.__globals__[
+            "_old_namedtuple_kwdefaults"
+        ] = _old_namedtuple_kwdefaults
+        collections.namedtuple.__globals__["_old_namedtuple"] = _old_namedtuple
+        collections.namedtuple.__globals__["_hack_namedtuple"] = _hack_namedtuple
+        collections.namedtuple.__code__ = namedtuple.__code__
+        collections.namedtuple.__hijack = 1
+
+        # hack the cls already generated by namedtuple.
+        # Those created in other modules can be pickled as normal,
+        # so only hack those in __main__ module
+        for n, o in sys.modules["__main__"].__dict__.items():
+            if (
+                type(o) is type
+                and o.__base__ is tuple
+                and hasattr(o, "_fields")
+                and "__reduce__" not in o.__dict__
+            ):
+                _hack_namedtuple(o)  # hack inplace
+
+    _hijack_namedtuple()
 
 
 class PickleSerializer(FramedSerializer):
@@ -436,7 +445,7 @@ class PickleSerializer(FramedSerializer):
         return pickle.loads(obj, encoding=encoding)
 
 
-class CloudPickleSerializer(PickleSerializer):
+class CloudPickleSerializer(FramedSerializer):
     def dumps(self, obj):
         try:
             return cloudpickle.dumps(obj, pickle_protocol)
@@ -451,6 +460,15 @@ class CloudPickleSerializer(PickleSerializer):
             print_exec(sys.stderr)
             raise pickle.PicklingError(msg)
 
+    def loads(self, obj, encoding="bytes"):
+        return cloudpickle.loads(obj, encoding=encoding)
+
+
+if sys.version_info < (3, 8):
+    CPickleSerializer = PickleSerializer
+else:
+    CPickleSerializer = CloudPickleSerializer
+
 
 class MarshalSerializer(FramedSerializer):
 
@@ -459,7 +477,7 @@ class MarshalSerializer(FramedSerializer):
 
         http://docs.python.org/2/library/marshal.html
 
-    This serializer is faster than PickleSerializer but supports fewer datatypes.
+    This serializer is faster than CloudPickleSerializer but supports fewer datatypes.
     """
 
     def dumps(self, obj):
diff --git a/python/pyspark/shuffle.py b/python/pyspark/shuffle.py
index 9dbe314d29b..bd455667f36 100644
--- a/python/pyspark/shuffle.py
+++ b/python/pyspark/shuffle.py
@@ -28,7 +28,7 @@ import sys
 import heapq
 from pyspark.serializers import (
     BatchedSerializer,
-    PickleSerializer,
+    CPickleSerializer,
     FlattenedValuesSerializer,
     CompressedSerializer,
     AutoBatchedSerializer,
@@ -140,8 +140,8 @@ class Merger(object):
 
 
 def _compressed_serializer(self, serializer=None):
-    # always use PickleSerializer to simplify implementation
-    ser = PickleSerializer()
+    # always use CPickleSerializer to simplify implementation
+    ser = CPickleSerializer()
     return AutoBatchedSerializer(CompressedSerializer(ser))
 
 
@@ -609,7 +609,7 @@ class ExternalList(object):
             os.makedirs(d)
         p = os.path.join(d, str(id(self)))
         self._file = open(p, "w+b", 65536)
-        self._ser = BatchedSerializer(CompressedSerializer(PickleSerializer()), 1024)
+        self._ser = BatchedSerializer(CompressedSerializer(CPickleSerializer()), 1024)
         os.unlink(p)
 
     def __del__(self):
diff --git a/python/pyspark/sql/dataframe.py b/python/pyspark/sql/dataframe.py
index 337cad534fa..160e7c3841a 100644
--- a/python/pyspark/sql/dataframe.py
+++ b/python/pyspark/sql/dataframe.py
@@ -47,7 +47,7 @@ from pyspark.rdd import (  # type: ignore[attr-defined]
     _load_from_socket,
     _local_iterator_from_socket,
 )
-from pyspark.serializers import BatchedSerializer, PickleSerializer, UTF8Deserializer
+from pyspark.serializers import BatchedSerializer, CPickleSerializer, UTF8Deserializer
 from pyspark.storagelevel import StorageLevel
 from pyspark.traceback_utils import SCCallSiteSync
 from pyspark.sql.column import Column, _to_seq, _to_list, _to_java_column
@@ -121,7 +121,7 @@ class DataFrame(PandasMapOpsMixin, PandasConversionMixin):
         """Returns the content as an :class:`pyspark.RDD` of :class:`Row`."""
         if self._lazy_rdd is None:
             jrdd = self._jdf.javaToPython()
-            self._lazy_rdd = RDD(jrdd, self.sql_ctx._sc, BatchedSerializer(PickleSerializer()))
+            self._lazy_rdd = RDD(jrdd, self.sql_ctx._sc, BatchedSerializer(CPickleSerializer()))
         return self._lazy_rdd
 
     @property  # type: ignore[misc]
@@ -592,7 +592,7 @@ class DataFrame(PandasMapOpsMixin, PandasConversionMixin):
                 max_num_rows,
                 self.sql_ctx._conf.replEagerEvalTruncate(),  # type: ignore[attr-defined]
             )
-            rows = list(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))
+            rows = list(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))
             head = rows[0]
             row_data = rows[1:]
             has_more_data = len(row_data) > max_num_rows
@@ -769,7 +769,7 @@ class DataFrame(PandasMapOpsMixin, PandasConversionMixin):
         """
         with SCCallSiteSync(self._sc) as css:
             sock_info = self._jdf.collectToPython()
-        return list(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))
+        return list(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))
 
     def toLocalIterator(self, prefetchPartitions: bool = False) -> Iterator[Row]:
         """
@@ -792,7 +792,7 @@ class DataFrame(PandasMapOpsMixin, PandasConversionMixin):
         """
         with SCCallSiteSync(self._sc) as css:
             sock_info = self._jdf.toPythonIterator(prefetchPartitions)
-        return _local_iterator_from_socket(sock_info, BatchedSerializer(PickleSerializer()))
+        return _local_iterator_from_socket(sock_info, BatchedSerializer(CPickleSerializer()))
 
     def limit(self, num: int) -> "DataFrame":
         """Limits the result count to the number specified.
@@ -837,7 +837,7 @@ class DataFrame(PandasMapOpsMixin, PandasConversionMixin):
         """
         with SCCallSiteSync(self._sc):
             sock_info = self._jdf.tailToPython(num)
-        return list(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))
+        return list(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))
 
     def foreach(self, f: Callable[[Row], None]) -> None:
         """Applies the ``f`` function to all :class:`Row` of this :class:`DataFrame`.
diff --git a/python/pyspark/sql/streaming.py b/python/pyspark/sql/streaming.py
index 53a098ce498..74593d07000 100644
--- a/python/pyspark/sql/streaming.py
+++ b/python/pyspark/sql/streaming.py
@@ -1200,7 +1200,7 @@ class DataStreamWriter(object):
         """
 
         from pyspark.rdd import _wrap_function  # type: ignore[attr-defined]
-        from pyspark.serializers import PickleSerializer, AutoBatchedSerializer
+        from pyspark.serializers import CPickleSerializer, AutoBatchedSerializer
         from pyspark.taskcontext import TaskContext
 
         if callable(f):
@@ -1268,7 +1268,7 @@ class DataStreamWriter(object):
 
             func = func_with_open_process_close  # type: ignore[assignment]
 
-        serializer = AutoBatchedSerializer(PickleSerializer())
+        serializer = AutoBatchedSerializer(CPickleSerializer())
         wrapped_func = _wrap_function(self._spark._sc, func, serializer, serializer)
         jForeachWriter = self._spark._sc._jvm.org.apache.spark.sql.execution.python.PythonForeachWriter(  # type: ignore[attr-defined]
             wrapped_func, self._df._jdf.schema()
diff --git a/python/pyspark/tests/test_rdd.py b/python/pyspark/tests/test_rdd.py
index a2c9ce90e94..51c1149080b 100644
--- a/python/pyspark/tests/test_rdd.py
+++ b/python/pyspark/tests/test_rdd.py
@@ -29,7 +29,7 @@ from pyspark.resource import ExecutorResourceRequests, ResourceProfileBuilder, T
 from pyspark.serializers import (
     CloudPickleSerializer,
     BatchedSerializer,
-    PickleSerializer,
+    CPickleSerializer,
     MarshalSerializer,
     UTF8Deserializer,
     NoOpSerializer,
@@ -446,7 +446,7 @@ class RDDTests(ReusedPySparkTestCase):
         a = self.sc.parallelize(range(5))
         b = self.sc.parallelize(range(100, 105))
         self.assertEqual(a.zip(b).collect(), [(0, 100), (1, 101), (2, 102), (3, 103), (4, 104)])
-        a = a._reserialize(BatchedSerializer(PickleSerializer(), 2))
+        a = a._reserialize(BatchedSerializer(CPickleSerializer(), 2))
         b = b._reserialize(MarshalSerializer())
         self.assertEqual(a.zip(b).collect(), [(0, 100), (1, 101), (2, 102), (3, 103), (4, 104)])
         # regression test for SPARK-4841
diff --git a/python/pyspark/tests/test_serializers.py b/python/pyspark/tests/test_serializers.py
index 3a9e14dd16a..019f5279bc5 100644
--- a/python/pyspark/tests/test_serializers.py
+++ b/python/pyspark/tests/test_serializers.py
@@ -29,7 +29,7 @@ from pyspark.serializers import (
     PairDeserializer,
     FlattenedValuesSerializer,
     CartesianDeserializer,
-    PickleSerializer,
+    CPickleSerializer,
     UTF8Deserializer,
     MarshalSerializer,
 )
@@ -46,15 +46,13 @@ from pyspark.testing.utils import (
 class SerializationTestCase(unittest.TestCase):
     def test_namedtuple(self):
         from collections import namedtuple
-        from pickle import dumps, loads
+        from pyspark.cloudpickle import dumps, loads
 
         P = namedtuple("P", "x y")
         p1 = P(1, 3)
         p2 = loads(dumps(p1, 2))
         self.assertEqual(p1, p2)
 
-        from pyspark.cloudpickle import dumps
-
         P2 = loads(dumps(P))
         p3 = P2(1, 3)
         self.assertEqual(p1, p3)
@@ -132,7 +130,7 @@ class SerializationTestCase(unittest.TestCase):
         ser.dumps(foo)
 
     def test_compressed_serializer(self):
-        ser = CompressedSerializer(PickleSerializer())
+        ser = CompressedSerializer(CPickleSerializer())
         from io import BytesIO as StringIO
 
         io = StringIO()
@@ -147,15 +145,15 @@ class SerializationTestCase(unittest.TestCase):
     def test_hash_serializer(self):
         hash(NoOpSerializer())
         hash(UTF8Deserializer())
-        hash(PickleSerializer())
+        hash(CPickleSerializer())
         hash(MarshalSerializer())
         hash(AutoSerializer())
-        hash(BatchedSerializer(PickleSerializer()))
+        hash(BatchedSerializer(CPickleSerializer()))
         hash(AutoBatchedSerializer(MarshalSerializer()))
         hash(PairDeserializer(NoOpSerializer(), UTF8Deserializer()))
         hash(CartesianDeserializer(NoOpSerializer(), UTF8Deserializer()))
-        hash(CompressedSerializer(PickleSerializer()))
-        hash(FlattenedValuesSerializer(PickleSerializer()))
+        hash(CompressedSerializer(CPickleSerializer()))
+        hash(FlattenedValuesSerializer(CPickleSerializer()))
 
 
 @unittest.skipIf(not have_scipy, "SciPy not installed")
diff --git a/python/pyspark/tests/test_shuffle.py b/python/pyspark/tests/test_shuffle.py
index 805a47dd1e3..5d69b67fc3e 100644
--- a/python/pyspark/tests/test_shuffle.py
+++ b/python/pyspark/tests/test_shuffle.py
@@ -19,7 +19,7 @@ import unittest
 
 from py4j.protocol import Py4JJavaError
 
-from pyspark import shuffle, PickleSerializer, SparkConf, SparkContext
+from pyspark import shuffle, CPickleSerializer, SparkConf, SparkContext
 from pyspark.shuffle import Aggregator, ExternalMerger, ExternalSorter
 
 
@@ -80,7 +80,7 @@ class MergerTests(unittest.TestCase):
             self.assertEqual(k, len(vs))
             self.assertEqual(list(range(k)), list(vs))
 
-        ser = PickleSerializer()
+        ser = CPickleSerializer()
         l = ser.loads(ser.dumps(list(gen_gs(50002, 30000))))
         for k, vs in l:
             self.assertEqual(k, len(vs))
diff --git a/python/pyspark/worker.py b/python/pyspark/worker.py
index c2200b20fe2..1935e27d663 100644
--- a/python/pyspark/worker.py
+++ b/python/pyspark/worker.py
@@ -50,7 +50,7 @@ from pyspark.serializers import (
     read_int,
     SpecialLengths,
     UTF8Deserializer,
-    PickleSerializer,
+    CPickleSerializer,
     BatchedSerializer,
 )
 from pyspark.sql.pandas.serializers import (
@@ -63,7 +63,7 @@ from pyspark.sql.types import StructType
 from pyspark.util import fail_on_stopiteration, try_simplify_traceback  # type: ignore
 from pyspark import shuffle
 
-pickleSer = PickleSerializer()
+pickleSer = CPickleSerializer()
 utf8_deserializer = UTF8Deserializer()
 
 
@@ -367,7 +367,7 @@ def read_udfs(pickleSer, infile, eval_type):
                 timezone, safecheck, assign_cols_by_name, df_for_struct
             )
     else:
-        ser = BatchedSerializer(PickleSerializer(), 100)
+        ser = BatchedSerializer(CPickleSerializer(), 100)
 
     num_udfs = read_int(infile)
 
