diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/limit.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/limit.scala
index 0dc4a69c075..37fe2565d8f 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/limit.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/limit.scala
@@ -294,6 +294,7 @@ case class TakeOrderedAndProjectExec(
     val data = if (offset > 0) limited.drop(offset) else limited
     if (projectList != child.output) {
       val proj = UnsafeProjection.create(projectList, child.output)
+      proj.initialize(0)
       data.map(r => proj(r).copy())
     } else {
       data
@@ -335,11 +336,12 @@ case class TakeOrderedAndProjectExec(
             writeMetrics),
           readMetrics)
       }
-      singlePartitionRDD.mapPartitionsInternal { iter =>
+      singlePartitionRDD.mapPartitionsWithIndexInternal { (idx, iter) =>
         val limited = Utils.takeOrdered(iter.map(_.copy()), limit)(ord)
         val topK = if (offset > 0) limited.drop(offset) else limited
         if (projectList != child.output) {
           val proj = UnsafeProjection.create(projectList, child.output)
+          proj.initialize(idx)
           topK.map(r => proj(r))
         } else {
           topK
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/execution/TakeOrderedAndProjectSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/execution/TakeOrderedAndProjectSuite.scala
index 647d46f8fbf..c0ed9777e4e 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/execution/TakeOrderedAndProjectSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/execution/TakeOrderedAndProjectSuite.scala
@@ -21,7 +21,7 @@ import scala.util.Random
 
 import org.apache.spark.sql.{DataFrame, Row}
 import org.apache.spark.sql.catalyst.dsl.expressions._
-import org.apache.spark.sql.catalyst.expressions.Literal
+import org.apache.spark.sql.catalyst.expressions.{Alias, Literal, Rand}
 import org.apache.spark.sql.test.SharedSparkSession
 import org.apache.spark.sql.types._
 
@@ -127,4 +127,38 @@ class TakeOrderedAndProjectSuite extends SparkPlanTest with SharedSparkSession {
       }
     }
   }
+
+  test("SPARK-47104: Non-deterministic expressions in projection") {
+    val expected = (input: SparkPlan) => {
+      GlobalLimitExec(limit,
+        LocalLimitExec(limit,
+          SortExec(sortOrder, true, input)))
+    }
+    val schema = StructType.fromDDL("a int, b int, c double")
+    val rdd = sparkContext.parallelize(
+      Seq(Row(1, 2, 0.0953472826424725d),
+        Row(2, 3, 0.5234194256885571d),
+        Row(3, 4, 0.7604953758285915d)), 1)
+    val df = spark.createDataFrame(rdd, schema)
+    val projection = df.queryExecution.sparkPlan.output.take(2) :+
+      Alias(Rand(Literal(0, IntegerType)), "_uuid")()
+
+    // test executeCollect
+    checkThatPlansAgree(
+      df,
+      input =>
+        TakeOrderedAndProjectExec(limit, sortOrder, projection,
+          SortExec(sortOrder, false, input)),
+      input => expected(input),
+      sortAnswers = false)
+
+    // test doExecute
+    checkThatPlansAgree(
+      df,
+      input =>
+        noOpFilter(TakeOrderedAndProjectExec(limit, sortOrder, projection,
+          SortExec(sortOrder, false, input))),
+      input => expected(input),
+      sortAnswers = false)
+  }
 }
