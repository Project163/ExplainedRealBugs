diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/collect.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/collect.scala
index 0a4882bfada..3270c6e87e2 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/collect.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/collect.scala
@@ -25,7 +25,7 @@ import org.apache.spark.sql.catalyst.analysis.{FunctionRegistry, TypeCheckResult
 import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.DataTypeMismatch
 import org.apache.spark.sql.catalyst.expressions._
 import org.apache.spark.sql.catalyst.trees.UnaryLike
-import org.apache.spark.sql.catalyst.util.{ArrayData, GenericArrayData, TypeUtils}
+import org.apache.spark.sql.catalyst.util.{ArrayData, GenericArrayData, TypeUtils, UnsafeRowUtils}
 import org.apache.spark.sql.errors.{QueryCompilationErrors, QueryErrorsBase}
 import org.apache.spark.sql.types._
 import org.apache.spark.util.BoundedPriorityQueue
@@ -145,6 +145,7 @@ case class CollectList(
   """,
   group = "agg_funcs",
   since = "2.0.0")
+// TODO: Make CollectSet collation aware
 case class CollectSet(
     child: Expression,
     mutableAggBufferOffset: Int = 0,
@@ -178,14 +179,15 @@ case class CollectSet(
   }
 
   override def checkInputDataTypes(): TypeCheckResult = {
-    if (!child.dataType.existsRecursively(_.isInstanceOf[MapType])) {
+    if (!child.dataType.existsRecursively(_.isInstanceOf[MapType]) &&
+        UnsafeRowUtils.isBinaryStable(child.dataType)) {
       TypeCheckResult.TypeCheckSuccess
     } else {
       DataTypeMismatch(
         errorSubClass = "UNSUPPORTED_INPUT_TYPE",
         messageParameters = Map(
           "functionName" -> toSQLId(prettyName),
-          "dataType" -> toSQLType(MapType)
+          "dataType" -> (s"${toSQLType(MapType)} " + "or \"COLLATED STRING\"")
         )
       )
     }
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/CollationSQLExpressionsSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/CollationSQLExpressionsSuite.scala
index ce6818652d2..d568cd77050 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/CollationSQLExpressionsSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/CollationSQLExpressionsSuite.scala
@@ -2819,16 +2819,24 @@ class CollationSQLExpressionsSuite
     }
   }
 
-  test("collect_set supports collation") {
+  test("collect_set does not support collation") {
     val collation = "UNICODE"
     val query = s"SELECT collect_set(col) FROM VALUES ('a'), ('b'), ('a') AS tab(col);"
     withSQLConf(SqlApiConf.DEFAULT_COLLATION -> collation) {
-      val result = sql(query).collect().head.getSeq[String](0).toSet
-      val expected = Set("a", "b")
-      assert(result == expected)
-      // check result row data type
-      val dataType = ArrayType(StringType(collation), false)
-      assert(sql(query).schema.head.dataType == dataType)
+      checkError(
+        exception = intercept[AnalysisException] {
+          sql(query)
+        },
+        condition = "DATATYPE_MISMATCH.UNSUPPORTED_INPUT_TYPE",
+        sqlState = Some("42K09"),
+        parameters = Map(
+          "functionName" -> "`collect_set`",
+          "dataType" -> "\"MAP\" or \"COLLATED STRING\"",
+          "sqlExpr" -> "\"collect_set(col)\""),
+        context = ExpectedContext(
+          fragment = "collect_set(col)",
+          start = 7,
+          stop = 22))
     }
   }
 
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala
index e80c3b23a7d..25f4d9f6235 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala
@@ -648,7 +648,7 @@ class DataFrameAggregateSuite extends QueryTest
       condition = "DATATYPE_MISMATCH.UNSUPPORTED_INPUT_TYPE",
       parameters = Map(
         "functionName" -> "`collect_set`",
-        "dataType" -> "\"MAP\"",
+        "dataType" -> "\"MAP\" or \"COLLATED STRING\"",
         "sqlExpr" -> "\"collect_set(b)\""
       ),
       context = ExpectedContext(
