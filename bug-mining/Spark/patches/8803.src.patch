diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala
index ccaf07a4d52..7eb911b559b 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala
@@ -980,7 +980,7 @@ class Analyzer(override val catalogManager: CatalogManager)
         if (metaCols.isEmpty) {
           node
         } else {
-          val newNode = addMetadataCol(node, attr => metaCols.exists(_.exprId == attr.exprId))
+          val newNode = addMetadataCol(node, metaCols.map(_.exprId).toSet)
           // We should not change the output schema of the plan. We should project away the extra
           // metadata columns if necessary.
           if (newNode.sameOutput(node)) {
@@ -1016,16 +1016,24 @@ class Analyzer(override val catalogManager: CatalogManager)
 
     private def addMetadataCol(
         plan: LogicalPlan,
-        isRequired: Attribute => Boolean): LogicalPlan = plan match {
-      case s: ExposesMetadataColumns if s.metadataOutput.exists(isRequired) =>
+        requiredAttrIds: Set[ExprId]): LogicalPlan = plan match {
+      case s: ExposesMetadataColumns if s.metadataOutput.exists( a =>
+        requiredAttrIds.contains(a.exprId)) =>
         s.withMetadataColumns()
-      case p: Project if p.metadataOutput.exists(isRequired) =>
+      case p: Project if p.metadataOutput.exists(a => requiredAttrIds.contains(a.exprId)) =>
         val newProj = p.copy(
           projectList = p.projectList ++ p.metadataOutput,
-          child = addMetadataCol(p.child, isRequired))
+          child = addMetadataCol(p.child, requiredAttrIds))
         newProj.copyTagsFrom(p)
         newProj
-      case _ => plan.withNewChildren(plan.children.map(addMetadataCol(_, isRequired)))
+      case u: Union if u.metadataOutput.exists(a => requiredAttrIds.contains(a.exprId)) =>
+        u.withNewChildren(u.children.map { child =>
+          // The children of a Union will have the same attributes with different expression IDs
+          val exprIdMap = u.metadataOutput.map(_.exprId)
+            .zip(child.metadataOutput.map(_.exprId)).toMap
+          addMetadataCol(child, requiredAttrIds.map(a => exprIdMap.getOrElse(a, a)))
+        })
+      case _ => plan.withNewChildren(plan.children.map(addMetadataCol(_, requiredAttrIds)))
     }
   }
 
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala
index eb1ad48a117..586e344df5e 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala
@@ -449,22 +449,54 @@ case class Union(
       AttributeSet.fromAttributeSets(children.map(_.outputSet)).size
   }
 
-  // updating nullability to make all the children consistent
-  override def output: Seq[Attribute] = {
-    children.map(_.output).transpose.map { attrs =>
-      val firstAttr = attrs.head
-      val nullable = attrs.exists(_.nullable)
-      val newDt = attrs.map(_.dataType).reduce(StructType.unionLikeMerge)
-      if (firstAttr.dataType == newDt) {
-        firstAttr.withNullability(nullable)
-      } else {
-        AttributeReference(firstAttr.name, newDt, nullable, firstAttr.metadata)(
-          firstAttr.exprId, firstAttr.qualifier)
-      }
+  /**
+   * Merges a sequence of attributes to have a common datatype and updates the
+   * nullability to be consistent with the attributes being merged.
+   */
+  private def mergeAttributes(attributes: Seq[Attribute]): Attribute = {
+    val firstAttr = attributes.head
+    val nullable = attributes.exists(_.nullable)
+    val newDt = attributes.map(_.dataType).reduce(StructType.unionLikeMerge)
+    if (firstAttr.dataType == newDt) {
+      firstAttr.withNullability(nullable)
+    } else {
+      AttributeReference(firstAttr.name, newDt, nullable, firstAttr.metadata)(
+        firstAttr.exprId, firstAttr.qualifier)
     }
   }
 
-  override def metadataOutput: Seq[Attribute] = Nil
+  override def output: Seq[Attribute] = children.map(_.output).transpose.map(mergeAttributes)
+
+  override def metadataOutput: Seq[Attribute] = {
+    val childrenMetadataOutput = children.map(_.metadataOutput)
+    // This follows similar code in `CheckAnalysis` to check if the output of a Union is correct,
+    // but just silently doesn't return an output instead of throwing an error. It also ensures
+    // that the attribute and data type names are the same.
+    val refDataTypes = childrenMetadataOutput.head.map(_.dataType)
+    val refAttrNames = childrenMetadataOutput.head.map(_.name)
+    childrenMetadataOutput.tail.foreach { childMetadataOutput =>
+      // We can only propagate the metadata output correctly if every child has the same
+      // number of columns
+      if (childMetadataOutput.length != refDataTypes.length) return Nil
+      // Check if the data types match by name and type
+      val childDataTypes = childMetadataOutput.map(_.dataType)
+      childDataTypes.zip(refDataTypes).foreach { case (dt1, dt2) =>
+        if (!DataType.equalsStructurally(dt1, dt2, true) ||
+           !DataType.equalsStructurallyByName(dt1, dt2, conf.resolver)) {
+          return Nil
+        }
+      }
+      // Check that the names of the attributes match
+      val childAttrNames = childMetadataOutput.map(_.name)
+      childAttrNames.zip(refAttrNames).foreach { case (attrName1, attrName2) =>
+        if (!conf.resolver(attrName1, attrName2)) {
+          return Nil
+        }
+      }
+    }
+    // If the metadata output matches, merge the attributes and return them
+    childrenMetadataOutput.transpose.map(mergeAttributes)
+  }
 
   override lazy val resolved: Boolean = {
     // allChildrenCompatible needs to be evaluated after childrenResolved
diff --git a/sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryBaseTable.scala b/sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryBaseTable.scala
index 5874480468d..12653cae402 100644
--- a/sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryBaseTable.scala
+++ b/sql/catalyst/src/test/scala/org/apache/spark/sql/connector/catalog/InMemoryBaseTable.scala
@@ -71,7 +71,7 @@ abstract class InMemoryBaseTable(
 
   // purposely exposes a metadata column that conflicts with a data column in some tests
   override val metadataColumns: Array[MetadataColumn] = Array(IndexColumn, PartitionKeyColumn)
-  private val metadataColumnNames = metadataColumns.map(_.name).toSet -- schema.map(_.name)
+  private lazy val metadataColumnNames = metadataColumns.map(_.name).toSet -- schema.map(_.name)
 
   private val allowUnsupportedTransforms =
     properties.getOrDefault("allow-unsupported-transforms", "false").toBoolean
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/connector/MetadataColumnSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/connector/MetadataColumnSuite.scala
index 9abf0fd59e6..2fd3c2b105d 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/connector/MetadataColumnSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/connector/MetadataColumnSuite.scala
@@ -17,13 +17,28 @@
 
 package org.apache.spark.sql.connector
 
+import java.io.{File, FilenameFilter}
+
 import org.apache.spark.sql.{AnalysisException, Row}
+import org.apache.spark.sql.connector.catalog.{Identifier, InMemoryCatalog, InMemoryTable, MetadataColumn, Table}
+import org.apache.spark.sql.connector.expressions.Transform
 import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation
 import org.apache.spark.sql.functions.struct
+import org.apache.spark.sql.types.{DataType, IntegerType, StringType, StructField, StructType}
 
 class MetadataColumnSuite extends DatasourceV2SQLBase {
   import testImplicits._
 
+  override protected def beforeEach(): Unit = {
+    super.beforeEach()
+    spark.conf.set("spark.sql.catalog.testCatalog", classOf[MetadataTestCatalog].getName)
+    spark.conf.set("spark.sql.catalog.typeMismatch", classOf[MetadataTypeMismatchCatalog].getName)
+    spark.conf.set(
+      "spark.sql.catalog.nameMismatch", classOf[MetadataAttrNameMismatchCatalog].getName)
+    spark.conf.set(
+      "spark.sql.catalog.fieldNameMismatch", classOf[MetadataFieldNameMismatchCatalog].getName)
+  }
+
   private val tbl = "testcat.t"
 
   private def prepareTable(): Unit = {
@@ -249,4 +264,220 @@ class MetadataColumnSuite extends DatasourceV2SQLBase {
       }
     }
   }
+
+  test("SPARK-41498: Metadata column is propagated through union") {
+    withTable(tbl) {
+      prepareTable()
+      val df = spark.table(tbl)
+      val dfQuery = df.union(df).select("id", "data", "index", "_partition")
+      val expectedAnswer = Seq(Row(1, "a", 0, "3/1"), Row(2, "b", 0, "0/2"), Row(3, "c", 0, "1/3"))
+      checkAnswer(dfQuery, expectedAnswer ++ expectedAnswer)
+    }
+  }
+
+  test("SPARK-41498: Nested metadata column is propagated through union") {
+    withTempDir { dir =>
+      spark.range(start = 0, end = 10, step = 1, numPartitions = 1)
+        .write.mode("overwrite").save(dir.getAbsolutePath)
+      val df = spark.read.load(dir.getAbsolutePath)
+      val dfQuery = df.union(df).select("_metadata.file_path")
+
+      val filePath = dir.listFiles(new FilenameFilter {
+        override def accept(dir: File, name: String): Boolean = name.endsWith(".parquet")
+      }).map(_.getAbsolutePath)
+      assert(filePath.length == 1)
+      val expectedAnswer = (1 to 20).map(_ => Row("file:" ++ filePath.head))
+      checkAnswer(dfQuery, expectedAnswer)
+    }
+  }
+
+  test("SPARK-41498: Metadata column is not propagated when children of Union " +
+    "have metadata output of different size") {
+    withTable(tbl) {
+      prepareTable()
+      withTempDir { dir =>
+        spark.range(start = 10, end = 20).selectExpr("bigint(id) as id", "string(id) as data")
+          .write.mode("overwrite").save(dir.getAbsolutePath)
+        val df1 = spark.table(tbl)
+        val df2 = spark.read.load(dir.getAbsolutePath)
+
+        // Make sure one df contains a metadata column and the other does not
+        assert(!df1.queryExecution.analyzed.metadataOutput.exists(_.name == "_metadata"))
+        assert(df2.queryExecution.analyzed.metadataOutput.exists(_.name == "_metadata"))
+
+        assert(df1.union(df2).queryExecution.analyzed.metadataOutput.isEmpty)
+      }
+    }
+  }
+
+  test("SPARK-41498: Metadata column is not propagated when children of Union " +
+    "have a type mismatch in a metadata column") {
+    val tbl = "testCatalog.t"
+    val typeMismatchTbl = "typeMismatch.t"
+    withTable(tbl, typeMismatchTbl) {
+      spark.range(10).write.saveAsTable(tbl)
+      val df = spark.table(tbl)
+      spark.range(10).write.saveAsTable(typeMismatchTbl)
+      val typeMismatchDf = spark.table(typeMismatchTbl)
+      assert(df.union(typeMismatchDf).queryExecution.analyzed.metadataOutput.isEmpty)
+    }
+  }
+
+  test("SPARK-41498: Metadata column is not propagated when children of Union " +
+    "have an attribute name mismatch in a metadata column") {
+    val tbl = "testCatalog.t"
+    val nameMismatchTbl = "nameMismatch.t"
+    withTable(tbl, nameMismatchTbl) {
+      spark.range(10).write.saveAsTable(tbl)
+      val df = spark.table(tbl)
+      spark.range(10).write.saveAsTable(nameMismatchTbl)
+      val nameMismatchDf = spark.table(nameMismatchTbl)
+      assert(df.union(nameMismatchDf).queryExecution.analyzed.metadataOutput.isEmpty)
+    }
+  }
+
+  test("SPARK-41498: Metadata column is not propagated when children of Union " +
+    "have a field name mismatch in a metadata column") {
+    val tbl = "testCatalog.t"
+    val fieldNameMismatchTbl = "fieldNameMismatch.t"
+    withTable(tbl, fieldNameMismatchTbl) {
+      spark.range(10).write.saveAsTable(tbl)
+      val df = spark.table(tbl)
+      spark.range(10).write.saveAsTable(fieldNameMismatchTbl)
+      val fieldNameMismatchDf = spark.table(fieldNameMismatchTbl)
+      assert(df.union(fieldNameMismatchDf).queryExecution.analyzed.metadataOutput.isEmpty)
+    }
+  }
+}
+
+class MetadataTestTable(
+    name: String,
+    schema: StructType,
+    partitioning: Array[Transform],
+    properties: java.util.Map[String, String])
+  extends InMemoryTable(name, schema, partitioning, properties) {
+
+  override val metadataColumns: Array[MetadataColumn] =
+    Array(
+      new MetadataColumn {
+        override def name: String = "_metadata"
+        override def dataType: DataType = StructType(StructField("index", IntegerType) :: Nil)
+        override def comment: String = ""
+      }
+    )
+}
+
+class TypeMismatchTable(
+    name: String,
+    schema: StructType,
+    partitioning: Array[Transform],
+    properties: java.util.Map[String, String])
+  extends InMemoryTable(name, schema, partitioning, properties) {
+
+  override val metadataColumns: Array[MetadataColumn] =
+    Array(
+      new MetadataColumn {
+        override def name: String = "_metadata"
+        override def dataType: DataType = StructType(StructField("index", StringType) :: Nil)
+        override def comment: String =
+          "Used to create a type mismatch with the metadata col in `MetadataTestTable`"
+      }
+    )
+}
+
+class AttrNameMismatchTable(
+    name: String,
+    schema: StructType,
+    partitioning: Array[Transform],
+    properties: java.util.Map[String, String])
+  extends InMemoryTable(name, schema, partitioning, properties) {
+  override val metadataColumns: Array[MetadataColumn] =
+    Array(
+      new MetadataColumn {
+        override def name: String = "wrongName"
+        override def dataType: DataType = StructType(StructField("index", IntegerType) :: Nil)
+        override def comment: String =
+          "Used to create a name mismatch with the metadata col in `MetadataTestTable`"
+      })
+}
+
+class FieldNameMismatchTable(
+    name: String,
+    schema: StructType,
+    partitioning: Array[Transform],
+    properties: java.util.Map[String, String])
+  extends InMemoryTable(name, schema, partitioning, properties) {
+  override val metadataColumns: Array[MetadataColumn] =
+    Array(
+      new MetadataColumn {
+        override def name: String = "_metadata"
+        override def dataType: DataType = StructType(StructField("wrongName", IntegerType) :: Nil)
+        override def comment: String =
+          "Used to create a name mismatch with the struct field in the metadata col of " +
+            "`MetadataTestTable`"
+      })
+}
+
+class MetadataTestCatalog extends InMemoryCatalog {
+  override def createTable(
+      ident: Identifier,
+      schema: StructType,
+      partitions: Array[Transform],
+      properties: java.util.Map[String, String]): Table = {
+    import org.apache.spark.sql.connector.catalog.CatalogV2Implicits._
+
+    val tableName = s"$name.${ident.quoted}"
+    val tbl = new MetadataTestTable(tableName, schema, partitions, properties)
+    tables.put(ident, tbl)
+    namespaces.putIfAbsent(ident.namespace.toList, Map())
+    tbl
+  }
+}
+
+class MetadataTypeMismatchCatalog extends InMemoryCatalog {
+  override def createTable(
+      ident: Identifier,
+      schema: StructType,
+      partitions: Array[Transform],
+      properties: java.util.Map[String, String]): Table = {
+    import org.apache.spark.sql.connector.catalog.CatalogV2Implicits._
+
+    val tableName = s"$name.${ident.quoted}"
+    val tbl = new TypeMismatchTable(tableName, schema, partitions, properties)
+    tables.put(ident, tbl)
+    namespaces.putIfAbsent(ident.namespace.toList, Map())
+    tbl
+  }
+}
+
+class MetadataAttrNameMismatchCatalog extends InMemoryCatalog {
+  override def createTable(
+      ident: Identifier,
+      schema: StructType,
+      partitions: Array[Transform],
+      properties: java.util.Map[String, String]): Table = {
+    import org.apache.spark.sql.connector.catalog.CatalogV2Implicits._
+
+    val tableName = s"$name.${ident.quoted}"
+    val tbl = new AttrNameMismatchTable(tableName, schema, partitions, properties)
+    tables.put(ident, tbl)
+    namespaces.putIfAbsent(ident.namespace.toList, Map())
+    tbl
+  }
+}
+
+class MetadataFieldNameMismatchCatalog extends InMemoryCatalog {
+  override def createTable(
+      ident: Identifier,
+      schema: StructType,
+      partitions: Array[Transform],
+      properties: java.util.Map[String, String]): Table = {
+    import org.apache.spark.sql.connector.catalog.CatalogV2Implicits._
+
+    val tableName = s"$name.${ident.quoted}"
+    val tbl = new FieldNameMismatchTable(tableName, schema, partitions, properties)
+    tables.put(ident, tbl)
+    namespaces.putIfAbsent(ident.namespace.toList, Map())
+    tbl
+  }
 }
