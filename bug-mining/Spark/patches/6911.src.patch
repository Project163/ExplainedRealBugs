diff --git a/core/src/main/scala/org/apache/spark/internal/config/package.scala b/core/src/main/scala/org/apache/spark/internal/config/package.scala
index 0e872be9d3c..40b05cf96d1 100644
--- a/core/src/main/scala/org/apache/spark/internal/config/package.scala
+++ b/core/src/main/scala/org/apache/spark/internal/config/package.scala
@@ -1507,7 +1507,8 @@ package object config {
         "longer time than the threshold. This config helps speculate stage with very few " +
         "tasks. Regular speculation configs may also apply if the executor slots are " +
         "large enough. E.g. tasks might be re-launched if there are enough successful runs " +
-        "even though the threshold hasn't been reached.")
+        "even though the threshold hasn't been reached. The number of slots is computed based " +
+        "on the conf values of spark.executor.cores and spark.task.cpus minimum 1.")
       .timeConf(TimeUnit.MILLISECONDS)
       .createOptional
 
diff --git a/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala b/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala
index e026e90ee64..11a1dc154ac 100644
--- a/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala
+++ b/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala
@@ -87,7 +87,13 @@ private[spark] class TaskSetManager(
   // number of slots on a single executor, would the task manager speculative run the tasks if
   // their duration is longer than the given threshold. In this way, we wouldn't speculate too
   // aggressively but still handle basic cases.
-  val speculationTasksLessEqToSlots = numTasks <= (conf.get(EXECUTOR_CORES) / sched.CPUS_PER_TASK)
+  // SPARK-30417: #cores per executor might not be set in spark conf for standalone mode, then
+  // the value of the conf would 1 by default. However, the executor would use all the cores on
+  // the worker. Therefore, CPUS_PER_TASK is okay to be greater than 1 without setting #cores.
+  // To handle this case, we assume the minimum number of slots is 1.
+  // TODO: use the actual number of slots for standalone mode.
+  val speculationTasksLessEqToSlots =
+    numTasks <= Math.max(conf.get(EXECUTOR_CORES) / sched.CPUS_PER_TASK, 1)
 
   // For each task, tracks whether a copy of the task has succeeded. A task will also be
   // marked as "succeeded" if it failed with a fetch failure, in which case it should not
diff --git a/core/src/test/scala/org/apache/spark/scheduler/TaskSetManagerSuite.scala b/core/src/test/scala/org/apache/spark/scheduler/TaskSetManagerSuite.scala
index ab64e3b1674..76db225f953 100644
--- a/core/src/test/scala/org/apache/spark/scheduler/TaskSetManagerSuite.scala
+++ b/core/src/test/scala/org/apache/spark/scheduler/TaskSetManagerSuite.scala
@@ -1791,13 +1791,14 @@ class TaskSetManagerSuite
       speculationThresholdOpt: Option[String],
       speculationQuantile: Double,
       numTasks: Int,
-      numSlots: Int): (TaskSetManager, ManualClock) = {
+      numExecutorCores: Int,
+      numCoresPerTask: Int): (TaskSetManager, ManualClock) = {
     sc = new SparkContext("local", "test")
     sc.conf.set(config.SPECULATION_ENABLED, true)
     sc.conf.set(config.SPECULATION_QUANTILE.key, speculationQuantile.toString)
     // Set the number of slots per executor
-    sc.conf.set(config.EXECUTOR_CORES.key, numSlots.toString)
-    sc.conf.set(config.CPUS_PER_TASK.key, "1")
+    sc.conf.set(config.EXECUTOR_CORES.key, numExecutorCores.toString)
+    sc.conf.set(config.CPUS_PER_TASK.key, numCoresPerTask.toString)
     if (speculationThresholdOpt.isDefined) {
       sc.conf.set(config.SPECULATION_TASK_DURATION_THRESHOLD.key, speculationThresholdOpt.get)
     }
@@ -1823,9 +1824,10 @@ class TaskSetManagerSuite
       // Set the threshold to be 60 minutes
       if (speculationThresholdProvided) Some("60min") else None,
       // Set the quantile to be 1.0 so that regular speculation would not be triggered
-      1.0,
+      speculationQuantile = 1.0,
       numTasks,
-      numSlots
+      numSlots,
+      numCoresPerTask = 1
     )
 
     // if the time threshold has not been exceeded, no speculative run should be triggered
@@ -1834,7 +1836,7 @@ class TaskSetManagerSuite
     assert(sched.speculativeTasks.size == 0)
 
     // Now the task should have been running for 60 minutes and 1 second
-    clock.advance(1)
+    clock.advance(1000)
     if (speculationThresholdProvided && numSlots >= numTasks) {
       assert(manager.checkSpeculatableTasks(0))
       assert(sched.speculativeTasks.size == numTasks)
@@ -1871,7 +1873,8 @@ class TaskSetManagerSuite
       Some("60min"),
       speculationQuantile = 0.5,
       numTasks = 2,
-      numSlots = 2
+      numExecutorCores = 2,
+      numCoresPerTask = 1
     )
 
     // Task duration can't be 0, advance 1 sec
@@ -1885,6 +1888,26 @@ class TaskSetManagerSuite
     assert(sched.speculativeTasks.size == 1)
   }
 
+  test("SPARK-30417 when spark.task.cpus is greater than spark.executor.cores due to " +
+    "standalone settings, speculate if there is only one task in the stage") {
+    val (manager, clock) = testSpeculationDurationSetup(
+      Some("60min"),
+      // Set the quantile to be 1.0 so that regular speculation would not be triggered
+      speculationQuantile = 1.0,
+      numTasks = 1,
+      numExecutorCores = 1,
+      numCoresPerTask = 2
+    )
+
+    clock.advance(1000*60*60)
+    assert(!manager.checkSpeculatableTasks(0))
+    assert(sched.speculativeTasks.size == 0)
+    // Now the task should have been running for 60 minutes and 1 second
+    clock.advance(1000)
+    assert(manager.checkSpeculatableTasks(0))
+    assert(sched.speculativeTasks.size == 1)
+  }
+
   test("TaskOutputFileAlreadyExistException lead to task set abortion") {
     sc = new SparkContext("local", "test")
     sched = new FakeTaskScheduler(sc, ("exec1", "host1"))
diff --git a/docs/configuration.md b/docs/configuration.md
index 6ad7afb339e..aebd35aa6e7 100644
--- a/docs/configuration.md
+++ b/docs/configuration.md
@@ -2054,7 +2054,8 @@ Apart from these, the following properties are also available, and may be useful
     slots on a single executor and the task is taking longer time than the threshold. This config
     helps speculate stage with very few tasks. Regular speculation configs may also apply if the
     executor slots are large enough. E.g. tasks might be re-launched if there are enough successful
-    runs even though the threshold hasn't been reached.
+    runs even though the threshold hasn't been reached. The number of slots is computed based on
+    the conf values of spark.executor.cores and spark.task.cpus minimum 1.
     Default unit is bytes, unless otherwise specified.
   </td>
 </tr>
