diff --git a/python/docs/source/user_guide/sql/type_conversions.rst b/python/docs/source/user_guide/sql/type_conversions.rst
index b63e7dfa885..2f13701995e 100644
--- a/python/docs/source/user_guide/sql/type_conversions.rst
+++ b/python/docs/source/user_guide/sql/type_conversions.rst
@@ -64,6 +64,9 @@ are listed below:
     * - spark.sql.timestampType
       - If set to `TIMESTAMP_NTZ`, the default timestamp type is ``TimestampNTZType``. Otherwise, the default timestamp type is TimestampType.
       - ""
+    * - spark.sql.execution.pandas.inferPandasDictAsMap
+      - When enabled, Pandas dictionaries are inferred as MapType. Otherwise, they are inferred as StructType.
+      - False
 
 All Conversions
 ---------------
diff --git a/python/pyspark/sql/connect/session.py b/python/pyspark/sql/connect/session.py
index 9de8579b12b..8b7e403667c 100644
--- a/python/pyspark/sql/connect/session.py
+++ b/python/pyspark/sql/connect/session.py
@@ -68,7 +68,12 @@ from pyspark.sql.connect.readwriter import DataFrameReader
 from pyspark.sql.connect.streaming.readwriter import DataStreamReader
 from pyspark.sql.connect.streaming.query import StreamingQueryManager
 from pyspark.sql.pandas.serializers import ArrowStreamPandasSerializer
-from pyspark.sql.pandas.types import to_arrow_schema, to_arrow_type, _deduplicate_field_names
+from pyspark.sql.pandas.types import (
+    to_arrow_schema,
+    to_arrow_type,
+    _deduplicate_field_names,
+    from_arrow_type,
+)
 from pyspark.sql.profiler import Profile
 from pyspark.sql.session import classproperty, SparkSession as PySparkSession
 from pyspark.sql.types import (
@@ -81,6 +86,8 @@ from pyspark.sql.types import (
     StructType,
     AtomicType,
     TimestampType,
+    MapType,
+    StringType,
 )
 from pyspark.sql.utils import to_str
 from pyspark.errors import (
@@ -419,6 +426,28 @@ class SparkSession:
             # If no schema supplied by user then get the names of columns only
             if schema is None:
                 _cols = [str(x) if not isinstance(x, str) else x for x in data.columns]
+                infer_pandas_dict_as_map = (
+                    str(self.conf.get("spark.sql.execution.pandas.inferPandasDictAsMap")).lower()
+                    == "true"
+                )
+                if infer_pandas_dict_as_map:
+                    struct = StructType()
+                    pa_schema = pa.Schema.from_pandas(data)
+                    spark_type: Union[MapType, DataType]
+                    for field in pa_schema:
+                        field_type = field.type
+                        if isinstance(field_type, pa.StructType):
+                            if len(field_type) == 0:
+                                raise PySparkValueError(
+                                    error_class="CANNOT_INFER_EMPTY_SCHEMA",
+                                    message_parameters={},
+                                )
+                            arrow_type = field_type.field(0).type
+                            spark_type = MapType(StringType(), from_arrow_type(arrow_type))
+                        else:
+                            spark_type = from_arrow_type(field_type)
+                        struct.add(field.name, spark_type, nullable=field.nullable)
+                    schema = struct
             elif isinstance(schema, (list, tuple)) and cast(int, _num_cols) < len(data.columns):
                 assert isinstance(_cols, list)
                 _cols.extend([f"_{i + 1}" for i in range(cast(int, _num_cols), len(data.columns))])
diff --git a/python/pyspark/sql/pandas/conversion.py b/python/pyspark/sql/pandas/conversion.py
index d958b95795b..891bab63b3d 100644
--- a/python/pyspark/sql/pandas/conversion.py
+++ b/python/pyspark/sql/pandas/conversion.py
@@ -32,10 +32,18 @@ from pyspark.loose_version import LooseVersion
 from pyspark.rdd import _load_from_socket
 from pyspark.sql.pandas.serializers import ArrowCollectSerializer
 from pyspark.sql.pandas.types import _dedup_names
-from pyspark.sql.types import ArrayType, MapType, TimestampType, StructType, DataType, _create_row
+from pyspark.sql.types import (
+    ArrayType,
+    MapType,
+    TimestampType,
+    StructType,
+    DataType,
+    _create_row,
+    StringType,
+)
 from pyspark.sql.utils import is_timestamp_ntz_preferred
 from pyspark.traceback_utils import SCCallSiteSync
-from pyspark.errors import PySparkTypeError
+from pyspark.errors import PySparkTypeError, PySparkValueError
 
 if TYPE_CHECKING:
     import numpy as np
@@ -600,15 +608,39 @@ class SparkConversionMixin:
         )
         import pyarrow as pa
 
+        infer_pandas_dict_as_map = (
+            str(self.conf.get("spark.sql.execution.pandas.inferPandasDictAsMap")).lower() == "true"
+        )
+
         # Create the Spark schema from list of names passed in with Arrow types
         if isinstance(schema, (list, tuple)):
             arrow_schema = pa.Schema.from_pandas(pdf, preserve_index=False)
-            struct = StructType()
             prefer_timestamp_ntz = is_timestamp_ntz_preferred()
-            for name, field in zip(schema, arrow_schema):
-                struct.add(
-                    name, from_arrow_type(field.type, prefer_timestamp_ntz), nullable=field.nullable
-                )
+            struct = StructType()
+            if infer_pandas_dict_as_map:
+                spark_type: Union[MapType, DataType]
+                for name, field in zip(schema, arrow_schema):
+                    field_type = field.type
+                    if isinstance(field_type, pa.StructType):
+                        if len(field_type) == 0:
+                            raise PySparkValueError(
+                                error_class="CANNOT_INFER_EMPTY_SCHEMA",
+                                message_parameters={},
+                            )
+                        arrow_type = field_type.field(0).type
+                        spark_type = MapType(
+                            StringType(), from_arrow_type(arrow_type, prefer_timestamp_ntz)
+                        )
+                    else:
+                        spark_type = from_arrow_type(field_type)
+                    struct.add(name, spark_type, nullable=field.nullable)
+            else:
+                for name, field in zip(schema, arrow_schema):
+                    struct.add(
+                        name,
+                        from_arrow_type(field.type, prefer_timestamp_ntz),
+                        nullable=field.nullable,
+                    )
             schema = struct
 
         # Determine arrow types to coerce data when creating batches
diff --git a/python/pyspark/sql/tests/test_creation.py b/python/pyspark/sql/tests/test_creation.py
index 272f7de2b4d..cd1fd922a6a 100644
--- a/python/pyspark/sql/tests/test_creation.py
+++ b/python/pyspark/sql/tests/test_creation.py
@@ -22,6 +22,7 @@ import time
 import unittest
 
 from pyspark.sql import Row
+import pyspark.sql.functions as F
 from pyspark.sql.types import (
     DateType,
     TimestampType,
@@ -157,6 +158,57 @@ class DataFrameCreationTestsMixin:
             message_parameters={},
         )
 
+    def test_schema_inference_from_pandas_with_dict(self):
+        # SPARK-47543: test for verifying if inferring `dict` as `MapType` work properly.
+        import pandas as pd
+
+        pdf = pd.DataFrame({"str_col": ["second"], "dict_col": [{"first": 0.7, "second": 0.3}]})
+
+        with self.sql_conf(
+            {
+                "spark.sql.execution.arrow.pyspark.enabled": True,
+                "spark.sql.execution.arrow.pyspark.fallback.enabled": False,
+                "spark.sql.execution.pandas.inferPandasDictAsMap": True,
+            }
+        ):
+            sdf = self.spark.createDataFrame(pdf)
+            self.assertEqual(
+                sdf.withColumn("test", F.col("dict_col")[F.col("str_col")]).collect(),
+                [Row(str_col="second", dict_col={"first": 0.7, "second": 0.3}, test=0.3)],
+            )
+
+            # Empty dict should fail
+            pdf_empty_struct = pd.DataFrame({"str_col": ["second"], "dict_col": [{}]})
+
+            with self.assertRaises(PySparkValueError) as pe:
+                self.spark.createDataFrame(pdf_empty_struct)
+
+            self.check_error(
+                exception=pe.exception,
+                error_class="CANNOT_INFER_EMPTY_SCHEMA",
+                message_parameters={},
+            )
+
+            # Dict has different types of values should fail
+            pdf_different_type = pd.DataFrame(
+                {"str_col": ["second"], "dict_col": [{"first": 0.7, "second": "0.3"}]}
+            )
+            self.assertRaises(
+                PySparkValueError, lambda: self.spark.createDataFrame(pdf_different_type)
+            )
+
+        with self.sql_conf(
+            {
+                "spark.sql.execution.arrow.pyspark.enabled": False,
+                "spark.sql.execution.pandas.inferPandasDictAsMap": True,
+            }
+        ):
+            sdf = self.spark.createDataFrame(pdf)
+            self.assertEqual(
+                sdf.withColumn("test", F.col("dict_col")[F.col("str_col")]).collect(),
+                [Row(str_col="second", dict_col={"first": 0.7, "second": 0.3}, test=0.3)],
+            )
+
 
 class DataFrameCreationTests(
     DataFrameCreationTestsMixin,
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
index 9a5e6b271a1..72831d8b32a 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
@@ -4409,6 +4409,14 @@ object SQLConf {
     .booleanConf
     .createWithDefault(false)
 
+  val INFER_PANDAS_DICT_AS_MAP = buildConf("spark.sql.execution.pandas.inferPandasDictAsMap")
+    .doc("When true, spark.createDataFrame will infer dict from Pandas DataFrame " +
+      "as a MapType. When false, spark.createDataFrame infers dict from Pandas DataFrame " +
+      "as a StructType which is default inferring from PyArrow.")
+    .version("4.0.0")
+    .booleanConf
+    .createWithDefault(false)
+
   val LEGACY_INFER_ARRAY_TYPE_FROM_FIRST_ELEMENT =
     buildConf("spark.sql.pyspark.legacy.inferArrayTypeFromFirstElement.enabled")
       .doc("PySpark's SparkSession.createDataFrame infers the element type of an array from all " +
@@ -5601,6 +5609,8 @@ class SQLConf extends Serializable with Logging with SqlApiConf {
 
   def inferDictAsStruct: Boolean = getConf(SQLConf.INFER_NESTED_DICT_AS_STRUCT)
 
+  def inferPandasDictAsMap: Boolean = getConf(SQLConf.INFER_PANDAS_DICT_AS_MAP)
+
   def legacyInferArrayTypeFromFirstElement: Boolean = getConf(
     SQLConf.LEGACY_INFER_ARRAY_TYPE_FROM_FIRST_ELEMENT)
 
