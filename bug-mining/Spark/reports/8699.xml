<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 19:31:12 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-40521] PartitionsAlreadyExistException in Hive V1 Command V1 reports all partitions instead of the conflicting partition</title>
                <link>https://issues.apache.org/jira/browse/SPARK-40521</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;PartitionsAlreadyExistException in Hive V1 Command V1 reports all partitions instead of the conflicting partition&lt;/p&gt;

&lt;p&gt;When I run:&lt;br/&gt;
AlterTableAddPartitionSuiteBase for Hive&lt;br/&gt;
The test:&#160;partition already exists&lt;br/&gt;
Fails in my my local build ONLY in that mode because it reports two partitions as conflicting where there should be only one. In all other modes the test succeeds.&lt;br/&gt;
The test is passing on master because the test does not check the partitions themselves.&lt;/p&gt;

&lt;p&gt;Repro on master: Note that c1 = 1 does not already exist. It should NOT be listed&#160;&lt;/p&gt;

&lt;p&gt;create table t(c1 int, c2 int) partitioned by (c1);&lt;/p&gt;

&lt;p&gt;alter table t add partition (c1 = 2);&lt;/p&gt;

&lt;p&gt;alter table t add partition (c1 = 1) partition (c1 = 2);&lt;/p&gt;

&lt;p&gt;22/09/21 09:30:09 ERROR Hive: AlreadyExistsException(message:Partition already exists: Partition(values:&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;, dbName:default, tableName:t, createTime:0, lastAccessTime:0, sd:StorageDescriptor(cols:&lt;span class=&quot;error&quot;&gt;&amp;#91;FieldSchema(name:c2, type:int, comment:null)&amp;#93;&lt;/span&gt;, location:&lt;a href=&quot;file:/Users/serge.rielau/spark/spark-warehouse/t/c1=2&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;file:/Users/serge.rielau/spark/spark-warehouse/t/c1=2&lt;/a&gt;, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), parameters:null))&lt;/p&gt;

&lt;p&gt; at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.startAddPartition(HiveMetaStore.java:2744)&lt;/p&gt;

&lt;p&gt; at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.add_partitions_core(HiveMetaStore.java:2442)&lt;/p&gt;

&lt;p&gt; at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.add_partitions_req(HiveMetaStore.java:2560)&lt;/p&gt;

&lt;p&gt; at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;/p&gt;

&lt;p&gt; at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&lt;/p&gt;

&lt;p&gt; at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;/p&gt;

&lt;p&gt; at java.base/java.lang.reflect.Method.invoke(Method.java:566)&lt;/p&gt;

&lt;p&gt; at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)&lt;/p&gt;

&lt;p&gt; at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)&lt;/p&gt;

&lt;p&gt; at com.sun.proxy.$Proxy31.add_partitions_req(Unknown Source)&lt;/p&gt;

&lt;p&gt; at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.add_partitions(HiveMetaStoreClient.java:625)&lt;/p&gt;

&lt;p&gt; at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;/p&gt;

&lt;p&gt; at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&lt;/p&gt;

&lt;p&gt; at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;/p&gt;

&lt;p&gt; at java.base/java.lang.reflect.Method.invoke(Method.java:566)&lt;/p&gt;

&lt;p&gt; at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)&lt;/p&gt;

&lt;p&gt; at com.sun.proxy.$Proxy32.add_partitions(Unknown Source)&lt;/p&gt;

&lt;p&gt; at org.apache.hadoop.hive.ql.metadata.Hive.createPartitions(Hive.java:2103)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.hive.client.Shim_v0_13.createPartitions(HiveShim.scala:763)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createPartitions$1(HiveClientImpl.scala:631)&lt;/p&gt;

&lt;p&gt; at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:296)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:227)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:226)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:276)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.hive.client.HiveClientImpl.createPartitions(HiveClientImpl.scala:624)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createPartitions$1(HiveExternalCatalog.scala:1039)&lt;/p&gt;

&lt;p&gt; at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.hive.HiveExternalCatalog.createPartitions(HiveExternalCatalog.scala:1021)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createPartitions(ExternalCatalogWithListener.scala:201)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createPartitions(SessionCatalog.scala:1169)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.execution.command.AlterTableAddPartitionCommand.$anonfun$run$17(ddl.scala:514)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.execution.command.AlterTableAddPartitionCommand.$anonfun$run$17$adapted(ddl.scala:513)&lt;/p&gt;

&lt;p&gt; at scala.collection.Iterator.foreach(Iterator.scala:943)&lt;/p&gt;

&lt;p&gt; at scala.collection.Iterator.foreach$(Iterator.scala:943)&lt;/p&gt;

&lt;p&gt; at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.execution.command.AlterTableAddPartitionCommand.run(ddl.scala:513)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:111)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:171)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)&lt;/p&gt;

&lt;p&gt; at org.apache.spark.sql.Dataset.&amp;lt;init&amp;gt;(Dataset.scala:219)&lt;/p&gt;

&lt;p&gt;...&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;The following partitions already exists in table &apos;t&apos; database &apos;default&apos;:&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;#de350b&quot;&gt;&lt;b&gt;Map(c1 -&amp;gt; 1)&lt;/b&gt;&lt;/font&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;#de350b&quot;&gt;&lt;b&gt;===&lt;/b&gt;&lt;/font&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Map(c1 -&amp;gt; 2)&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;spark-sql&amp;gt;&#160;&lt;/p&gt;</description>
                <environment></environment>
        <key id="13482597">SPARK-40521</key>
            <summary>PartitionsAlreadyExistException in Hive V1 Command V1 reports all partitions instead of the conflicting partition</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Minor</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="maxgekk">Max Gekk</assignee>
                                    <reporter username="srielau">Serge Rielau</reporter>
                        <labels>
                    </labels>
                <created>Wed, 21 Sep 2022 14:27:17 +0000</created>
                <updated>Fri, 7 Oct 2022 12:40:57 +0000</updated>
                            <resolved>Fri, 7 Oct 2022 12:40:57 +0000</resolved>
                                    <version>3.4.0</version>
                                    <fixVersion>3.4.0</fixVersion>
                                    <component>Spark Core</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="17607897" author="JIRAUSER288374" created="Wed, 21 Sep 2022 17:10:23 +0000"  >&lt;p&gt;Hive does return the offending partition. We just need to dig it out  &lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/13049584/13049584_Screen+Shot+2022-09-21+at+10.08.44+AM.png&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt;&lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/13049583/13049583_Screen+Shot+2022-09-21+at+10.08.52+AM.png&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt;&lt;/p&gt;</comment>
                            <comment id="17613557" author="apachespark" created="Thu, 6 Oct 2022 14:12:35 +0000"  >&lt;p&gt;User &apos;MaxGekk&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/38134&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/38134&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17613558" author="apachespark" created="Thu, 6 Oct 2022 14:13:28 +0000"  >&lt;p&gt;User &apos;MaxGekk&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/38134&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/38134&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17614065" author="maxgekk" created="Fri, 7 Oct 2022 12:40:57 +0000"  >&lt;p&gt;Issue resolved by pull request 38134&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/38134&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/38134&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="13049584" name="Screen Shot 2022-09-21 at 10.08.44 AM.png" size="17709" author="srielau" created="Wed, 21 Sep 2022 17:09:26 +0000"/>
                            <attachment id="13049583" name="Screen Shot 2022-09-21 at 10.08.52 AM.png" size="74039" author="srielau" created="Wed, 21 Sep 2022 17:09:26 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            3 years, 5 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z18qnc:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311620" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Shepherd</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>cloud_fan</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>