<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:24:34 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-5741] Support the path contains comma in HiveContext</title>
                <link>https://issues.apache.org/jira/browse/SPARK-5741</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;When run ```select * from nzhang_part where hr = &apos;file,&apos;;```, it throws exception ```java.lang.IllegalArgumentException: Can not create a Path from an empty string```. Because the path of hdfs contains comma, and FileInputFormat.setInputPaths will split path by comma.&lt;/p&gt;

&lt;p&gt;###############################&lt;br/&gt;
SQL&lt;br/&gt;
###############################&lt;br/&gt;
set hive.merge.mapfiles=true; &lt;br/&gt;
set hive.merge.mapredfiles=true;&lt;br/&gt;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;&lt;br/&gt;
set hive.exec.dynamic.partition=true;&lt;br/&gt;
set hive.exec.dynamic.partition.mode=nonstrict;&lt;/p&gt;

&lt;p&gt;create table nzhang_part like srcpart;&lt;/p&gt;

&lt;p&gt;insert overwrite table nzhang_part partition (ds=&apos;2010-08-15&apos;, hr) select key, value, hr from srcpart where ds=&apos;2008-04-08&apos;;&lt;/p&gt;

&lt;p&gt;insert overwrite table nzhang_part partition (ds=&apos;2010-08-15&apos;, hr=11) select key, value from srcpart where ds=&apos;2008-04-08&apos;;&lt;/p&gt;

&lt;p&gt;insert overwrite table nzhang_part partition (ds=&apos;2010-08-15&apos;, hr)  &lt;br/&gt;
select * from (&lt;br/&gt;
    select key, value, hr from srcpart where ds=&apos;2008-04-08&apos;&lt;br/&gt;
    union all&lt;br/&gt;
    select &apos;1&apos; as key, &apos;1&apos; as value, &apos;file,&apos; as hr from src limit 1) s;&lt;/p&gt;

&lt;p&gt;select * from nzhang_part where hr = &apos;file,&apos;;&lt;/p&gt;

&lt;p&gt;###############################&lt;br/&gt;
Error log&lt;br/&gt;
###############################&lt;br/&gt;
15/02/10 14:33:16 ERROR SparkSQLDriver: Failed in &lt;span class=&quot;error&quot;&gt;&amp;#91;select * from nzhang_part where hr = &amp;#39;file,&amp;#39;&amp;#93;&lt;/span&gt;&lt;br/&gt;
java.lang.IllegalArgumentException: Can not create a Path from an empty string&lt;br/&gt;
        at org.apache.hadoop.fs.Path.checkPathArg(Path.java:127)&lt;br/&gt;
        at org.apache.hadoop.fs.Path.&amp;lt;init&amp;gt;(Path.java:135)&lt;br/&gt;
        at org.apache.hadoop.util.StringUtils.stringToPath(StringUtils.java:241)&lt;br/&gt;
        at org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:400)&lt;br/&gt;
        at org.apache.spark.sql.hive.HadoopTableReader$.initializeLocalJobConfFunc(TableReader.scala:251)&lt;br/&gt;
        at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$11.apply(TableReader.scala:229)&lt;br/&gt;
        at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$11.apply(TableReader.scala:229)&lt;br/&gt;
        at org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$6.apply(HadoopRDD.scala:172)&lt;br/&gt;
        at org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$6.apply(HadoopRDD.scala:172)&lt;br/&gt;
        at scala.Option.map(Option.scala:145)&lt;br/&gt;
        at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:172)&lt;br/&gt;
        at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:196)&lt;br/&gt;
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:223)&lt;br/&gt;
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:221)&lt;br/&gt;
        at scala.Option.getOrElse(Option.scala:120)&lt;br/&gt;
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:221)&lt;br/&gt;
        at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)&lt;br/&gt;
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:223)&lt;br/&gt;
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:221)&lt;br/&gt;
        at scala.Option.getOrElse(Option.scala:120)&lt;br/&gt;
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:221)&lt;br/&gt;
        at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)&lt;br/&gt;
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:223)&lt;br/&gt;
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:221)&lt;br/&gt;
        at scala.Option.getOrElse(Option.scala:120)&lt;br/&gt;
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:221)&lt;br/&gt;
        at org.apache.spark.rdd.UnionRDD$$anonfun$1.apply(UnionRDD.scala:66)&lt;br/&gt;
        at org.apache.spark.rdd.UnionRDD$$anonfun$1.apply(UnionRDD.scala:66)&lt;br/&gt;
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)&lt;br/&gt;
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)&lt;br/&gt;
        at scala.collection.immutable.List.foreach(List.scala:318)&lt;br/&gt;
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)&lt;br/&gt;
        at scala.collection.AbstractTraversable.map(Traversable.scala:105)&lt;br/&gt;
        at org.apache.spark.rdd.UnionRDD.getPartitions(UnionRDD.scala:66)&lt;br/&gt;
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:223)&lt;br/&gt;
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:221)&lt;br/&gt;
        at scala.Option.getOrElse(Option.scala:120)&lt;br/&gt;
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:221)&lt;br/&gt;
        at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)&lt;br/&gt;
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:223)&lt;br/&gt;
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:221)&lt;br/&gt;
        at scala.Option.getOrElse(Option.scala:120)&lt;br/&gt;
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:221)&lt;br/&gt;
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1391)&lt;br/&gt;
        at org.apache.spark.rdd.RDD.collect(RDD.scala:811)&lt;br/&gt;
        at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:81)&lt;br/&gt;
        at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:446)&lt;br/&gt;
        at org.apache.spark.sql.hive.thriftserver.AbstractSparkSQLDriver.run(AbstractSparkSQLDriver.scala:58)&lt;br/&gt;
        at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:275)&lt;br/&gt;
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:423)&lt;br/&gt;
        at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:211)&lt;br/&gt;
        at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)&lt;br/&gt;
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)&lt;br/&gt;
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
        at java.lang.reflect.Method.invoke(Method.java:606)&lt;br/&gt;
        at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:403)&lt;br/&gt;
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:77)&lt;br/&gt;
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)&lt;/p&gt;
</description>
                <environment></environment>
        <key id="12774164">SPARK-5741</key>
            <summary>Support the path contains comma in HiveContext</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="waterman">Yadong Qi</assignee>
                                    <reporter username="waterman">Yadong Qi</reporter>
                        <labels>
                    </labels>
                <created>Wed, 11 Feb 2015 10:47:30 +0000</created>
                <updated>Thu, 27 Aug 2015 20:22:28 +0000</updated>
                            <resolved>Mon, 2 Mar 2015 18:13:43 +0000</resolved>
                                    <version>1.2.0</version>
                                    <fixVersion>1.3.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="14316035" author="apachespark" created="Wed, 11 Feb 2015 11:00:52 +0000"  >&lt;p&gt;User &apos;watermen&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/4532&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/4532&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14343519" author="marmbrus" created="Mon, 2 Mar 2015 18:13:43 +0000"  >&lt;p&gt;Issue resolved by pull request 4532&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/4532&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/4532&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14717053" author="koert" created="Thu, 27 Aug 2015 16:55:28 +0000"  >&lt;p&gt;i realize i am late to the party but...&lt;/p&gt;

&lt;p&gt;by doing this you are losing a very important functionality: passing in multiple input paths comma separated. globs only cover a very limited subset of what you can do with multiple paths. for example selecting partitions (by  day) for the last 30 days cannot be expressed with a glob.&lt;/p&gt;

&lt;p&gt;so you are giving up major functionality just to be able to pass in a character that people would generally advice should not be part of a filename anyhow? doesnt sound like a good idea to me.&lt;/p&gt;</comment>
                            <comment id="14717165" author="marmbrus" created="Thu, 27 Aug 2015 17:51:32 +0000"  >&lt;p&gt;What format are you trying to read?  There &lt;a href=&quot;https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala#L258&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;are still ways to read more than one file&lt;/a&gt;, they just don&apos;t rely on brittle string munging anymore.&lt;/p&gt;</comment>
                            <comment id="14717235" author="koert" created="Thu, 27 Aug 2015 18:27:02 +0000"  >&lt;p&gt;i am reading avro and csv mostly. but we try to support multiple inputs across a wide range of formats (currently avro, csv, json, and parquet).&lt;/p&gt;

&lt;p&gt;i realize parquet supports it, but it does so by explicitly working around the general infrastructure.&lt;/p&gt;

&lt;p&gt;i am sympathetic to the idea of no longer doing string munging, but that poses some challenges since the main vehicle to carry this information is a Map&lt;span class=&quot;error&quot;&gt;&amp;#91;String, String&amp;#93;&lt;/span&gt; (DataFrameReader.extraOptions).&lt;/p&gt;

&lt;p&gt;if we could come up with a general way to do this that does not involve string munging, i am happy to work on it. the ideal api in my view would be something like:&lt;br/&gt;
sqlContext.read.format(...).paths(&quot;a&quot;, &quot;b&quot;)&lt;/p&gt;

&lt;p&gt;alternatively this could be expressed as a union operation of many dataframes, but i do not have the knowledge of the relevant code to understand if that is feasible, scalable and will support predicate pushdown and such. but if that works then i have no need for multiple inputs in DataFrameReader...&lt;/p&gt;

&lt;p&gt;from what i know from other projects such as scalding, i think its is a very common request to be able to support multiple paths, and you would exclude a significant userbase by not supporting it. but thats just a guess...&lt;/p&gt;</comment>
                            <comment id="14717258" author="marmbrus" created="Thu, 27 Aug 2015 18:39:58 +0000"  >&lt;p&gt;It was originally just parquet that would support more than one file, but now all HadoopFSRelations should. (which covers all but CSV, and we should upgrade that library too)  I would be in favor of generalizing this support for at least these sources given the following constraints:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;We must keep source/binary compatibility.&lt;/li&gt;
	&lt;li&gt;We should give good errors when the source does not support this feature.&lt;/li&gt;
	&lt;li&gt;For consistency, I&apos;d prefer if we can just add a &lt;tt&gt;load(path: String*)&lt;/tt&gt; (but I&apos;m not sure if this is possible given the above).&lt;/li&gt;
	&lt;li&gt;&lt;tt&gt;paths(path: *)&lt;/tt&gt; is okay, but I think I&apos;d prefer if it was not the terminal operator.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14717450" author="koert" created="Thu, 27 Aug 2015 20:22:02 +0000"  >&lt;p&gt;given the requirement of source/binary compatibility i do not think it can be done without some kind of string munging.&lt;/p&gt;

&lt;p&gt;however the string munging could be restricted to a separate variable, set with paths(path: *) so path does not get polluted. this variable would be exclusively for HadoopFsRelationProvider, and an error thrown in ResolvedDataSource if any other RelationProvider is used with this variable set. also an error would be thrown if path and paths are both set.&lt;/p&gt;

&lt;p&gt;does this sound reasonable? if not i will keep looking for other solutions&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12763748">SPARK-4967</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 12 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i25h4f:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12329359">1.4.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>