<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:50:46 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-19268] File does not exist: /tmp/temporary-157b89c1-27bb-49f3-a70c-ca1b75022b4d/state/0/2/1.delta</title>
                <link>https://issues.apache.org/jira/browse/SPARK-19268</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;blockquote&gt;&lt;p&gt;./run-example sql.streaming.JavaStructuredKafkaWordCount 192.168.3.110:9092 subscribe topic03&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;when i run the spark example raises the following error:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Exception in thread &quot;main&quot; 17/01/17 14:13:41 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(4)&lt;br/&gt;
org.apache.spark.sql.streaming.StreamingQueryException: Job aborted due to stage failure: Task 2 in stage 9.0 failed 1 times, most recent failure: Lost task 2.0 in stage 9.0 (TID 46, localhost, executor driver): java.lang.IllegalStateException: Error reading delta file /tmp/temporary-157b89c1-27bb-49f3-a70c-ca1b75022b4d/state/0/2/1.delta of HDFSStateStoreProvider&lt;span class=&quot;error&quot;&gt;&amp;#91;id = (op=0, part=2), dir = /tmp/temporary-157b89c1-27bb-49f3-a70c-ca1b75022b4d/state/0/2&amp;#93;&lt;/span&gt;: /tmp/temporary-157b89c1-27bb-49f3-a70c-ca1b75022b4d/state/0/2/1.delta does not exist&lt;br/&gt;
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:354)&lt;br/&gt;
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$$anonfun$org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$loadMap$1$$anonfun$6.apply(HDFSBackedStateStoreProvider.scala:306)&lt;br/&gt;
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$$anonfun$org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$loadMap$1$$anonfun$6.apply(HDFSBackedStateStoreProvider.scala:303)&lt;br/&gt;
	at scala.Option.getOrElse(Option.scala:121)&lt;br/&gt;
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$$anonfun$org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$loadMap$1.apply(HDFSBackedStateStoreProvider.scala:303)&lt;br/&gt;
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$$anonfun$org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$loadMap$1.apply(HDFSBackedStateStoreProvider.scala:302)&lt;br/&gt;
	at scala.Option.getOrElse(Option.scala:121)&lt;br/&gt;
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$loadMap(HDFSBackedStateStoreProvider.scala:302)&lt;br/&gt;
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getStore(HDFSBackedStateStoreProvider.scala:220)&lt;br/&gt;
	at org.apache.spark.sql.execution.streaming.state.StateStore$.get(StateStore.scala:151)&lt;br/&gt;
	at org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:61)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)&lt;br/&gt;
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)&lt;br/&gt;
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)&lt;br/&gt;
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)&lt;br/&gt;
	at org.apache.spark.scheduler.Task.run(Task.scala:99)&lt;br/&gt;
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
Caused by: java.io.FileNotFoundException: File does not exist: /tmp/temporary-157b89c1-27bb-49f3-a70c-ca1b75022b4d/state/0/2/1.delta&lt;br/&gt;
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)&lt;br/&gt;
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)&lt;br/&gt;
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)&lt;br/&gt;
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)&lt;br/&gt;
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)&lt;br/&gt;
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:587)&lt;br/&gt;
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)&lt;br/&gt;
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)&lt;br/&gt;
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)&lt;br/&gt;
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)&lt;br/&gt;
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)&lt;br/&gt;
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)&lt;br/&gt;
	at java.security.AccessController.doPrivileged(Native Method)&lt;br/&gt;
	at javax.security.auth.Subject.doAs(Subject.java:415)&lt;br/&gt;
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)&lt;br/&gt;
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)&lt;/p&gt;

&lt;p&gt;	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)&lt;br/&gt;
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&lt;br/&gt;
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)&lt;br/&gt;
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)&lt;br/&gt;
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)&lt;br/&gt;
	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1228)&lt;br/&gt;
	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1213)&lt;br/&gt;
	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1201)&lt;br/&gt;
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:306)&lt;br/&gt;
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:272)&lt;br/&gt;
	at org.apache.hadoop.hdfs.DFSInputStream.&amp;lt;init&amp;gt;(DFSInputStream.java:264)&lt;br/&gt;
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1526)&lt;br/&gt;
	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:304)&lt;br/&gt;
	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:299)&lt;br/&gt;
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)&lt;br/&gt;
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:312)&lt;br/&gt;
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)&lt;br/&gt;
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:351)&lt;br/&gt;
	... 24 more&lt;/p&gt;&lt;/blockquote&gt;


&lt;p&gt;I checked my spark configuration file, found the value of &lt;tt&gt;spark.sql.adaptive.enabled&lt;/tt&gt; is &lt;tt&gt;true&lt;/tt&gt;, to modify it into &lt;tt&gt;false&lt;/tt&gt;, the example program can work.&lt;/p&gt;

&lt;p&gt;Is this a bug?&lt;/p&gt;

&lt;p&gt;thanks!&lt;/p&gt;</description>
                <environment>&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;hadoop2.7&lt;/li&gt;
	&lt;li&gt;Java 7&lt;/li&gt;
&lt;/ul&gt;
</environment>
        <key id="13035744">SPARK-19268</key>
            <summary>File does not exist: /tmp/temporary-157b89c1-27bb-49f3-a70c-ca1b75022b4d/state/0/2/1.delta</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="zsxwing">Shixiong Zhu</assignee>
                                    <reporter username="liyan">liyan</reporter>
                        <labels>
                    </labels>
                <created>Wed, 18 Jan 2017 01:37:40 +0000</created>
                <updated>Tue, 28 Nov 2017 11:31:22 +0000</updated>
                            <resolved>Tue, 24 Jan 2017 06:31:29 +0000</resolved>
                                    <version>2.0.0</version>
                    <version>2.0.1</version>
                    <version>2.0.2</version>
                    <version>2.1.0</version>
                                    <fixVersion>2.1.1</fixVersion>
                    <fixVersion>2.2.0</fixVersion>
                                    <component>Structured Streaming</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="15830706" author="zsxwing" created="Thu, 19 Jan 2017 22:15:29 +0000"  >&lt;p&gt;Right now Structured Streaming doesn&apos;t support &quot;spark.sql.adaptive.enabled&quot;. We should add an explicit error message.&lt;/p&gt;</comment>
                            <comment id="15835294" author="apachespark" created="Mon, 23 Jan 2017 22:21:04 +0000"  >&lt;p&gt;User &apos;zsxwing&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/16683&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/16683&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16000374" author="skrishna" created="Mon, 8 May 2017 07:38:36 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zsxwing&quot; class=&quot;user-hover&quot; rel=&quot;zsxwing&quot;&gt;zsxwing&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I have tested with spark 2.1.1, still the same problem exist. FYI.. I am running in Mesos environment. Do I need to add any config settings?&lt;/p&gt;

&lt;p&gt;thanks&lt;/p&gt;</comment>
                            <comment id="16001631" author="zsxwing" created="Mon, 8 May 2017 21:59:41 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=skrishna&quot; class=&quot;user-hover&quot; rel=&quot;skrishna&quot;&gt;skrishna&lt;/a&gt; could you provide your codes, or the output of &quot;dataset.explain(true)&quot;, please? Perhaps there is another bug in aggregation.&lt;/p&gt;</comment>
                            <comment id="16266544" author="mgaido" created="Mon, 27 Nov 2017 09:18:05 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zsxwing&quot; class=&quot;user-hover&quot; rel=&quot;zsxwing&quot;&gt;zsxwing&lt;/a&gt; I am hitting this too and I am running 2.2.0. My code looks very similar to the one of the &lt;tt&gt;org.apache.spark.examples.sql.streaming.StructuredNetworkWordCountWindowed&lt;/tt&gt; example.&lt;/p&gt;</comment>
                            <comment id="16268602" author="mgaido" created="Tue, 28 Nov 2017 11:31:22 +0000"  >&lt;p&gt;In my case, deleting `_spark_metadata` solved the issue. Thus likely this is caused by a bad status of the `_spark_metadata` dir. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zsxwing&quot; class=&quot;user-hover&quot; rel=&quot;zsxwing&quot;&gt;zsxwing&lt;/a&gt; Should we reopen this or create a new ticket?&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 51 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i38up3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>