<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:38:55 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-13711] Apache Spark driver stopping JVM when master not available </title>
                <link>https://issues.apache.org/jira/browse/SPARK-13711</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;In my application Java spark context is created with an unavailable master URL (you may assume master is down for a maintenance). When creating Java spark context it leads to stopping JVM that runs spark driver with JVM exit code 50.&lt;/p&gt;

&lt;p&gt;When I checked the logs I found SparkUncaughtExceptionHandler calling the System.exit. My program should run forever. &lt;/p&gt;

&lt;p&gt;package test.mains;&lt;/p&gt;

&lt;p&gt;import org.apache.spark.SparkConf;&lt;br/&gt;
import org.apache.spark.api.java.JavaSparkContext;&lt;/p&gt;

&lt;p&gt;public class CheckJavaSparkContext {&lt;/p&gt;

&lt;p&gt;    public static void main(String[] args) {&lt;/p&gt;

&lt;p&gt;        SparkConf conf = new SparkConf();&lt;br/&gt;
        conf.setAppName(&quot;test&quot;);&lt;br/&gt;
        conf.setMaster(&quot;spark://sunshinee:7077&quot;);&lt;/p&gt;

&lt;p&gt;        try &lt;/p&gt;
{
            new JavaSparkContext(conf);
        }
&lt;p&gt; catch (Throwable e) &lt;/p&gt;
{
            System.out.println(&quot;Caught an exception : &quot; + e.getMessage());           
        }

&lt;p&gt;        System.out.println(&quot;Waiting to complete...&quot;);&lt;br/&gt;
        while (true) {&lt;br/&gt;
        }&lt;br/&gt;
    }&lt;/p&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;p&gt;Output log&lt;/p&gt;

&lt;p&gt;Using Spark&apos;s default log4j profile: org/apache/spark/log4j-defaults.properties&lt;br/&gt;
SLF4J: Class path contains multiple SLF4J bindings.&lt;br/&gt;
SLF4J: Found binding in &lt;span class=&quot;error&quot;&gt;&amp;#91;jar:file:/data/downloads/spark-1.6.0-bin-hadoop2.6/lib/spark-assembly-1.6.0-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class&amp;#93;&lt;/span&gt;&lt;br/&gt;
SLF4J: Found binding in &lt;span class=&quot;error&quot;&gt;&amp;#91;jar:file:/data/downloads/spark-1.6.0-bin-hadoop2.6/lib/spark-examples-1.6.0-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class&amp;#93;&lt;/span&gt;&lt;br/&gt;
SLF4J: See &lt;a href=&quot;http://www.slf4j.org/codes.html#multiple_bindings&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.slf4j.org/codes.html#multiple_bindings&lt;/a&gt; for an explanation.&lt;br/&gt;
SLF4J: Actual binding is of type &lt;span class=&quot;error&quot;&gt;&amp;#91;org.slf4j.impl.Log4jLoggerFactory&amp;#93;&lt;/span&gt;&lt;br/&gt;
16/03/04 18:01:15 INFO SparkContext: Running Spark version 1.6.0&lt;br/&gt;
16/03/04 18:01:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable&lt;br/&gt;
16/03/04 18:01:17 WARN Utils: Your hostname, pesamara-mobl-vm1 resolves to a loopback address: 127.0.0.1; using 10.30.9.107 instead (on interface eth0)&lt;br/&gt;
16/03/04 18:01:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address&lt;br/&gt;
16/03/04 18:01:18 INFO SecurityManager: Changing view acls to: ps40233&lt;br/&gt;
16/03/04 18:01:18 INFO SecurityManager: Changing modify acls to: ps40233&lt;br/&gt;
16/03/04 18:01:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(ps40233); users with modify permissions: Set(ps40233)&lt;br/&gt;
16/03/04 18:01:19 INFO Utils: Successfully started service &apos;sparkDriver&apos; on port 55309.&lt;br/&gt;
16/03/04 18:01:21 INFO Slf4jLogger: Slf4jLogger started&lt;br/&gt;
16/03/04 18:01:21 INFO Remoting: Starting remoting&lt;br/&gt;
16/03/04 18:01:22 INFO Remoting: Remoting started; listening on addresses :&lt;span class=&quot;error&quot;&gt;&amp;#91;akka.tcp://sparkDriverActorSystem@10.30.9.107:52128&amp;#93;&lt;/span&gt;&lt;br/&gt;
16/03/04 18:01:22 INFO Utils: Successfully started service &apos;sparkDriverActorSystem&apos; on port 52128.&lt;br/&gt;
16/03/04 18:01:22 INFO SparkEnv: Registering MapOutputTracker&lt;br/&gt;
16/03/04 18:01:22 INFO SparkEnv: Registering BlockManagerMaster&lt;br/&gt;
16/03/04 18:01:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-87c20178-357d-4252-a46a-62a755568a98&lt;br/&gt;
16/03/04 18:01:22 INFO MemoryStore: MemoryStore started with capacity 457.7 MB&lt;br/&gt;
16/03/04 18:01:22 INFO SparkEnv: Registering OutputCommitCoordinator&lt;br/&gt;
16/03/04 18:01:23 INFO Utils: Successfully started service &apos;SparkUI&apos; on port 4040.&lt;br/&gt;
16/03/04 18:01:23 INFO SparkUI: Started SparkUI at &lt;a href=&quot;http://10.30.9.107:4040&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://10.30.9.107:4040&lt;/a&gt;&lt;br/&gt;
16/03/04 18:01:24 INFO AppClient$ClientEndpoint: Connecting to master spark://sunshinee:7077...&lt;br/&gt;
16/03/04 18:01:24 WARN AppClient$ClientEndpoint: Failed to connect to master sunshinee:7077&lt;br/&gt;
java.io.IOException: Failed to connect to sunshinee:7077&lt;br/&gt;
        at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216) at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)&lt;br/&gt;
        at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)&lt;br/&gt;
        at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)&lt;br/&gt;
        at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:200)&lt;br/&gt;
        at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:187)&lt;br/&gt;
        at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:183)&lt;br/&gt;
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
Caused by: java.nio.channels.UnresolvedAddressException&lt;br/&gt;
        at sun.nio.ch.Net.checkAddress(Net.java:123)&lt;br/&gt;
        at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:621)&lt;br/&gt;
        at io.netty.channel.socket.nio.NioSocketChannel.doConnect(NioSocketChannel.java:209)&lt;br/&gt;
        at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.connect(AbstractNioChannel.java:207)&lt;br/&gt;
        at io.netty.channel.DefaultChannelPipeline$HeadContext.connect(DefaultChannelPipeline.java:1097)&lt;br/&gt;
        at io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:471)&lt;br/&gt;
        at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:456)&lt;br/&gt;
        at io.netty.channel.ChannelOutboundHandlerAdapter.connect(ChannelOutboundHandlerAdapter.java:47)&lt;br/&gt;
        at io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:471)&lt;br/&gt;
        at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:456)&lt;br/&gt;
        at io.netty.channel.ChannelDuplexHandler.connect(ChannelDuplexHandler.java:50)&lt;br/&gt;
        at io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:471)&lt;br/&gt;
        at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:456)&lt;br/&gt;
        at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:438)&lt;br/&gt;
        at io.netty.channel.DefaultChannelPipeline.connect(DefaultChannelPipeline.java:908)&lt;br/&gt;
        at io.netty.channel.AbstractChannel.connect(AbstractChannel.java:203)&lt;br/&gt;
        at io.netty.bootstrap.Bootstrap$2.run(Bootstrap.java:166)&lt;br/&gt;
        at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)&lt;br/&gt;
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)&lt;br/&gt;
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)&lt;br/&gt;
        ... 1 more&lt;br/&gt;
16/03/04 18:01:44 INFO AppClient$ClientEndpoint: Connecting to master spark://sunshinee:7077...&lt;br/&gt;
16/03/04 18:01:44 WARN AppClient$ClientEndpoint: Failed to connect to master sunshinee:7077&lt;br/&gt;
java.io.IOException: Failed to connect to sunshinee:7077&lt;br/&gt;
        at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)&lt;br/&gt;
        at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)&lt;br/&gt;
        at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:200)&lt;br/&gt;
        at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:187)&lt;br/&gt;
        at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:183)&lt;br/&gt;
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
Caused by: java.nio.channels.UnresolvedAddressException&lt;br/&gt;
        at sun.nio.ch.Net.checkAddress(Net.java:123)&lt;br/&gt;
        at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:621)&lt;br/&gt;
        at io.netty.channel.socket.nio.NioSocketChannel.doConnect(NioSocketChannel.java:209)&lt;br/&gt;
        at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.connect(AbstractNioChannel.java:207)&lt;br/&gt;
        at io.netty.channel.DefaultChannelPipeline$HeadContext.connect(DefaultChannelPipeline.java:1097)&lt;br/&gt;
        at io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:471)&lt;br/&gt;
        at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:456)&lt;br/&gt;
        at io.netty.channel.ChannelOutboundHandlerAdapter.connect(ChannelOutboundHandlerAdapter.java:47)&lt;br/&gt;
        at io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:471)&lt;br/&gt;
        at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:456)&lt;br/&gt;
        at io.netty.channel.ChannelDuplexHandler.connect(ChannelDuplexHandler.java:50)&lt;br/&gt;
        at io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:471)&lt;br/&gt;
        at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:456)&lt;br/&gt;
        at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:438)&lt;br/&gt;
        at io.netty.channel.DefaultChannelPipeline.connect(DefaultChannelPipeline.java:908)&lt;br/&gt;
        at io.netty.channel.AbstractChannel.connect(AbstractChannel.java:203)&lt;br/&gt;
        at io.netty.bootstrap.Bootstrap$2.run(Bootstrap.java:166)&lt;br/&gt;
        at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)&lt;br/&gt;
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)&lt;br/&gt;
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)&lt;br/&gt;
        ... 1 more&lt;br/&gt;
16/03/04 18:02:04 INFO AppClient$ClientEndpoint: Connecting to master spark://sunshinee:7077...&lt;br/&gt;
16/03/04 18:02:04 INFO AppClient$ClientEndpoint: Connecting to master spark://sunshinee:7077...&lt;br/&gt;
16/03/04 18:02:04 WARN AppClient$ClientEndpoint: Failed to connect to master sunshinee:7077&lt;br/&gt;
java.io.IOException: Failed to connect to sunshinee:7077&lt;br/&gt;
        at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)&lt;br/&gt;
        at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)&lt;br/&gt;
        at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:200)&lt;br/&gt;
        at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:187)&lt;br/&gt;
        at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:183)&lt;br/&gt;
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
Caused by: java.nio.channels.UnresolvedAddressException&lt;br/&gt;
        at sun.nio.ch.Net.checkAddress(Net.java:123)&lt;br/&gt;
        at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:621)&lt;br/&gt;
        at io.netty.channel.socket.nio.NioSocketChannel.doConnect(NioSocketChannel.java:209)&lt;br/&gt;
        at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.connect(AbstractNioChannel.java:207)&lt;br/&gt;
        at io.netty.channel.DefaultChannelPipeline$HeadContext.connect(DefaultChannelPipeline.java:1097)&lt;br/&gt;
        at io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:471)&lt;br/&gt;
        at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:456)&lt;br/&gt;
        at io.netty.channel.ChannelOutboundHandlerAdapter.connect(ChannelOutboundHandlerAdapter.java:47)&lt;br/&gt;
        at io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:471)&lt;br/&gt;
        at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:456)&lt;br/&gt;
        at io.netty.channel.ChannelDuplexHandler.connect(ChannelDuplexHandler.java:50)&lt;br/&gt;
        at io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:471)&lt;br/&gt;
        at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:456)&lt;br/&gt;
        at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:438)&lt;br/&gt;
        at io.netty.channel.DefaultChannelPipeline.connect(DefaultChannelPipeline.java:908)&lt;br/&gt;
        at io.netty.channel.AbstractChannel.connect(AbstractChannel.java:203)&lt;br/&gt;
        at io.netty.bootstrap.Bootstrap$2.run(Bootstrap.java:166)&lt;br/&gt;
        at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)&lt;br/&gt;
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)&lt;br/&gt;
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)&lt;br/&gt;
        ... 1 more&lt;br/&gt;
16/03/04 18:02:24 INFO AppClient$ClientEndpoint: Connecting to master spark://sunshinee:7077...&lt;br/&gt;
16/03/04 18:02:24 INFO AppClient$ClientEndpoint: Connecting to master spark://sunshinee:7077...&lt;br/&gt;
16/03/04 18:02:24 WARN SparkDeploySchedulerBackend: Application ID is not initialized yet.&lt;br/&gt;
16/03/04 18:02:24 ERROR SparkDeploySchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.&lt;br/&gt;
16/03/04 18:02:24 WARN AppClient$ClientEndpoint: Failed to connect to master sunshinee:7077&lt;br/&gt;
java.io.IOException: Failed to connect to sunshinee:7077&lt;br/&gt;
        at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)&lt;br/&gt;
        at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)&lt;br/&gt;
        at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:200)&lt;br/&gt;
        at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:187)&lt;br/&gt;
        at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:183)&lt;br/&gt;
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
Caused by: java.nio.channels.UnresolvedAddressException&lt;br/&gt;
        at sun.nio.ch.Net.checkAddress(Net.java:123)&lt;br/&gt;
        at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:621)&lt;br/&gt;
        at io.netty.channel.socket.nio.NioSocketChannel.doConnect(NioSocketChannel.java:209)&lt;br/&gt;
        at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.connect(AbstractNioChannel.java:207)&lt;br/&gt;
        at io.netty.channel.DefaultChannelPipeline$HeadContext.connect(DefaultChannelPipeline.java:1097)&lt;br/&gt;
        at io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:471)&lt;br/&gt;
        at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:456)&lt;br/&gt;
        at io.netty.channel.ChannelOutboundHandlerAdapter.connect(ChannelOutboundHandlerAdapter.java:47)&lt;br/&gt;
        at io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:471)&lt;br/&gt;
        at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:456)&lt;br/&gt;
        at io.netty.channel.ChannelDuplexHandler.connect(ChannelDuplexHandler.java:50)&lt;br/&gt;
        at io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:471)&lt;br/&gt;
        at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:456)&lt;br/&gt;
        at io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:438)&lt;br/&gt;
        at io.netty.channel.DefaultChannelPipeline.connect(DefaultChannelPipeline.java:908)&lt;br/&gt;
        at io.netty.channel.AbstractChannel.connect(AbstractChannel.java:203)&lt;br/&gt;
        at io.netty.bootstrap.Bootstrap$2.run(Bootstrap.java:166)&lt;br/&gt;
        at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)&lt;br/&gt;
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)&lt;br/&gt;
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)&lt;br/&gt;
        ... 1 more&lt;br/&gt;
16/03/04 18:02:24 INFO Utils: Successfully started service &apos;org.apache.spark.network.netty.NettyBlockTransferService&apos; on port 44298.&lt;br/&gt;
16/03/04 18:02:24 INFO NettyBlockTransferService: Server created on 44298&lt;br/&gt;
16/03/04 18:02:24 INFO BlockManagerMaster: Trying to register BlockManager&lt;br/&gt;
16/03/04 18:02:24 INFO BlockManagerMasterEndpoint: Registering block manager 10.30.9.107:44298 with 457.7 MB RAM, BlockManagerId(driver, 10.30.9.107, 44298)&lt;br/&gt;
16/03/04 18:02:24 INFO BlockManagerMaster: Registered BlockManager&lt;br/&gt;
16/03/04 18:02:24 INFO SparkUI: Stopped Spark web UI at &lt;a href=&quot;http://10.30.9.107:4040&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://10.30.9.107:4040&lt;/a&gt;&lt;br/&gt;
16/03/04 18:02:24 INFO SparkDeploySchedulerBackend: Shutting down all executors&lt;br/&gt;
16/03/04 18:02:24 INFO SparkDeploySchedulerBackend: Asking each executor to shut down&lt;br/&gt;
16/03/04 18:02:24 WARN AppClient$ClientEndpoint: Drop UnregisterApplication(null) because has not yet connected to master&lt;br/&gt;
16/03/04 18:02:24 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread&lt;span class=&quot;error&quot;&gt;&amp;#91;appclient-registration-retry-thread,5,main&amp;#93;&lt;/span&gt;&lt;br/&gt;
java.lang.InterruptedException&lt;br/&gt;
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1039)&lt;br/&gt;
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1328)&lt;br/&gt;
        at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:208)&lt;br/&gt;
        at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:218)&lt;br/&gt;
        at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)&lt;br/&gt;
        at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)&lt;br/&gt;
        at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)&lt;br/&gt;
        at scala.concurrent.Await$.result(package.scala:107)&lt;br/&gt;
        at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)&lt;br/&gt;
        at org.apache.spark.deploy.client.AppClient.stop(AppClient.scala:290)&lt;br/&gt;
        at org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend.org$apache$spark$scheduler$cluster$SparkDeploySchedulerBackend$$stop(SparkDeploySchedulerBackend.scala:198)&lt;br/&gt;
        at org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend.stop(SparkDeploySchedulerBackend.scala:101)&lt;br/&gt;
        at org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:446)&lt;br/&gt;
        at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1582)&lt;br/&gt;
        at org.apache.spark.SparkContext$$anonfun$stop$7.apply$mcV$sp(SparkContext.scala:1731)&lt;br/&gt;
        at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1229)&lt;br/&gt;
        at org.apache.spark.SparkContext.stop(SparkContext.scala:1730)&lt;br/&gt;
        at org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend.dead(SparkDeploySchedulerBackend.scala:127)&lt;br/&gt;
        at org.apache.spark.deploy.client.AppClient$ClientEndpoint.markDead(AppClient.scala:264)&lt;br/&gt;
        at org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anon$2$$anonfun$run$1.apply$mcV$sp(AppClient.scala:134)&lt;br/&gt;
        at org.apache.spark.util.Utils$.tryOrExit(Utils.scala:1163)&lt;br/&gt;
        at org.apache.spark.deploy.client.AppClient$ClientEndpoint$$anon$2.run(AppClient.scala:129)&lt;br/&gt;
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)&lt;br/&gt;
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)&lt;br/&gt;
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)&lt;br/&gt;
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
16/03/04 18:02:24 INFO DiskBlockManager: Shutdown hook called&lt;br/&gt;
16/03/04 18:02:24 INFO ShutdownHookManager: Shutdown hook called&lt;br/&gt;
16/03/04 18:02:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-ea68a0fa-4f0d-4dbb-8407-cce90ef78a52&lt;br/&gt;
16/03/04 18:02:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-ea68a0fa-4f0d-4dbb-8407-cce90ef78a52/userFiles-db548748-a55c-4406-adcb-c09e63b118bd&lt;br/&gt;
Java Result: 50 &lt;/p&gt;</description>
                <environment></environment>
        <key id="12947620">SPARK-13711</key>
            <summary>Apache Spark driver stopping JVM when master not available </summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="zsxwing">Shixiong Zhu</assignee>
                                    <reporter username="erangac">Era</reporter>
                        <labels>
                    </labels>
                <created>Mon, 7 Mar 2016 06:16:49 +0000</created>
                <updated>Tue, 8 Mar 2016 04:59:03 +0000</updated>
                            <resolved>Tue, 8 Mar 2016 04:59:03 +0000</resolved>
                                    <version>1.4.1</version>
                    <version>1.6.0</version>
                                    <fixVersion>1.6.2</fixVersion>
                    <fixVersion>2.0.0</fixVersion>
                                    <component>Spark Core</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="15182674" author="zsxwing" created="Mon, 7 Mar 2016 07:11:42 +0000"  >&lt;p&gt;This is the correct behavior. Driver needs to talk with Master to request and launch executors.&lt;/p&gt;</comment>
                            <comment id="15182681" author="zsxwing" created="Mon, 7 Mar 2016 07:17:53 +0000"  >&lt;p&gt;Sorry, I misread the description. Did you run in the client mode? I think SparkUncaughtExceptionHandler should not be set for driver.&lt;/p&gt;</comment>
                            <comment id="15182729" author="erangac" created="Mon, 7 Mar 2016 08:09:25 +0000"  >&lt;p&gt;This is my Java based driver program. I am not using SparkUncaughtExceptionHandler but Java Spark context use it internally. &lt;/p&gt;</comment>
                            <comment id="15184016" author="apachespark" created="Mon, 7 Mar 2016 23:32:04 +0000"  >&lt;p&gt;User &apos;zsxwing&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/11566&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/11566&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 37 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2u9n3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>