<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:14:52 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-2282] PySpark crashes if too many tasks complete quickly</title>
                <link>https://issues.apache.org/jira/browse/SPARK-2282</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Upon every task completion, PythonAccumulatorParam constructs a new socket to the Accumulator server running inside the pyspark daemon. This can cause a buildup of used ephemeral ports from sockets in the TIME_WAIT termination stage, which will cause the SparkContext to crash if too many tasks complete too quickly. We ran into this bug with 17k tasks completing in 15 seconds.&lt;/p&gt;

&lt;p&gt;This bug can be fixed outside of Spark by ensuring these properties are set (on a linux server);&lt;br/&gt;
echo &quot;1&quot; &amp;gt; /proc/sys/net/ipv4/tcp_tw_reuse&lt;br/&gt;
echo &quot;1&quot; &amp;gt; /proc/sys/net/ipv4/tcp_tw_recycle&lt;/p&gt;

&lt;p&gt;or by adding the SO_REUSEADDR option to the Socket creation within Spark.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12723773">SPARK-2282</key>
            <summary>PySpark crashes if too many tasks complete quickly</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="ilikerps">Aaron Davidson</assignee>
                                    <reporter username="ilikerps">Aaron Davidson</reporter>
                        <labels>
                    </labels>
                <created>Wed, 25 Jun 2014 23:04:34 +0000</created>
                <updated>Thu, 31 Jul 2014 22:37:19 +0000</updated>
                            <resolved>Fri, 4 Jul 2014 06:03:52 +0000</resolved>
                                    <version>0.9.1</version>
                    <version>1.0.0</version>
                    <version>1.0.1</version>
                                    <fixVersion>0.9.2</fixVersion>
                    <fixVersion>1.0.0</fixVersion>
                    <fixVersion>1.0.1</fixVersion>
                    <fixVersion>1.1.0</fixVersion>
                                    <component>PySpark</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="14062916" author="carlilek" created="Wed, 16 Jul 2014 00:13:03 +0000"  >&lt;p&gt;We may be running into this issue on our cluster. Any input on whether this property needs to be set on all nodes or on only the master? I ask because we dynamically spin up spark clusters on a larger general purpose compute cluster, so I&apos;m hesitant to start changing sysctls willy nilly unless I absolutely have to. &lt;/p&gt;

&lt;p&gt;Alternately, is that SO_REUSEADDR merely a setting one can chnage in one of the conf files, or is that within the software written for spark? (I&apos;m coming at this from a sysadmin point of view, so the former would be much easier!)&lt;/p&gt;

&lt;p&gt;Odd thing is that we&apos;re seeing it on 1.0.1, in which it is supposed to be fixed...&lt;br/&gt;
Thanks, &lt;br/&gt;
Ken&lt;/p&gt;</comment>
                            <comment id="14063211" author="ilikerps" created="Wed, 16 Jul 2014 06:26:31 +0000"  >&lt;p&gt;This should actually only be necessary on the master. Use of the SO_REUSEADDR property (equivalently, sysctl tcp_tw_reuse) means that the number of used sockets will increase to the maximum number of ephemeral ports, but then should remain constant. It&apos;s possible that if another process tries to allocate an ephemeral port during this time, it will fail.&lt;/p&gt;

&lt;p&gt;While tcp_tw_reuse is generally considered &quot;safe&quot;, setting &lt;b&gt;tcp_tw_recycle&lt;/b&gt; can lead to unexpected packet arrival from closed streams (though it&apos;s very unlikely), but is a more guaranteed solution. This should cause the connections to be recycled immediately after the TCP teardown, and thus no buildup of sockets should occur.&lt;/p&gt;

&lt;p&gt;Please let me know if setting either of these parameters helps on the driver machine. You can also verify that this problem is occurring by doing a &lt;tt&gt;netstat -lpn&lt;/tt&gt; during execution, iirc, which should display an inordinate number of open connections on the Spark Driver process and on a Python daemon one.&lt;/p&gt;</comment>
                            <comment id="14064996" author="carlilek" created="Thu, 17 Jul 2014 15:06:57 +0000"  >&lt;p&gt;So we&apos;ve just given this a try with a 32 node cluster. Without the two sysctl commands, it obviously failed, using this code in pyspark: &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;data = sc.parallelize(range(0,30000000), 2000).map(lambda x: range(0,300))
data.cache()
data.count()
&lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; i in range(0,20): data.count()
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Unfortunately, with the two sysctls implemented on all nodes in the cluster, it also failed. Here&apos;s the java errors we see: &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;14/07/17 10:55:37 ERROR DAGSchedulerActorSupervisor: eventProcesserActor failed; shutting down SparkContext
java.net.NoRouteToHostException: Cannot assign requested address
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
        at java.net.Socket.connect(Socket.java:579)
        at java.net.Socket.connect(Socket.java:528)
        at java.net.Socket.&amp;lt;init&amp;gt;(Socket.java:425)
        at java.net.Socket.&amp;lt;init&amp;gt;(Socket.java:208)
        at org.apache.spark.api.python.PythonAccumulatorParam.addInPlace(PythonRDD.scala:404)
        at org.apache.spark.api.python.PythonAccumulatorParam.addInPlace(PythonRDD.scala:387)
        at org.apache.spark.Accumulable.$plus$plus$eq(Accumulators.scala:72)
        at org.apache.spark.Accumulators$$anonfun$add$2.apply(Accumulators.scala:280)
        at org.apache.spark.Accumulators$$anonfun$add$2.apply(Accumulators.scala:278)
        at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
        at scala.collection.mutable.HashTable$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreachEntry(HashTable.scala:226)
        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
        at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
        at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
        at org.apache.spark.Accumulators$.add(Accumulators.scala:278)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:820)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1226)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
        at akka.actor.ActorCell.invoke(ActorCell.scala:456)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
        at akka.dispatch.Mailbox.run(Mailbox.scala:219)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Traceback (most recent call last):
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;&amp;lt;stdin&amp;gt;&quot;&lt;/span&gt;, line 1, in &amp;lt;module&amp;gt;
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/usr/local/spark-current/python/pyspark/rdd.py&quot;&lt;/span&gt;, line 708, in count
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; self.mapPartitions(lambda i: [sum(1 &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; _ in i)]).sum()
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/usr/local/spark-current/python/pyspark/rdd.py&quot;&lt;/span&gt;, line 699, in sum
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; self.mapPartitions(lambda x: [sum(x)]).reduce(&lt;span class=&quot;code-keyword&quot;&gt;operator&lt;/span&gt;.add)
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/usr/local/spark-current/python/pyspark/rdd.py&quot;&lt;/span&gt;, line 619, in reduce
    vals = self.mapPartitions(func).collect()
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/usr/local/spark-current/python/pyspark/rdd.py&quot;&lt;/span&gt;, line 583, in collect
    bytesInJava = self._jrdd.collect().iterator()
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/usr/local/spark-current/python/lib/py4j-0.8.1-src.zip/py4j/java_gateway.py&quot;&lt;/span&gt;, line 537, in __call__
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/usr/local/spark-current/python/lib/py4j-0.8.1-src.zip/py4j/protocol.py&quot;&lt;/span&gt;, line 300, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; calling o158.collect.
: org.apache.spark.SparkException: Job 14 cancelled as part of cancellation of all jobs
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1044)
        at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1009)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:499)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:499)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:499)
        at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
        at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:499)
        at org.apache.spark.scheduler.DAGSchedulerActorSupervisor$$anonfun$2.applyOrElse(DAGScheduler.scala:1170)
        at org.apache.spark.scheduler.DAGSchedulerActorSupervisor$$anonfun$2.applyOrElse(DAGScheduler.scala:1166)
        at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:295)
        at akka.actor.dungeon.FaultHandling$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;handleFailure(FaultHandling.scala:253)
        at akka.actor.ActorCell.handleFailure(ActorCell.scala:338)
        at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:423)
        at akka.actor.ActorCell.systemInvoke(ActorCell.scala:447)
        at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:262)
        at akka.dispatch.Mailbox.run(Mailbox.scala:218)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

&amp;gt;&amp;gt;&amp;gt; 14/07/17 10:55:38 ERROR OneForOneStrategy: Cannot assign requested address
java.net.NoRouteToHostException: Cannot assign requested address
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
        at java.net.Socket.connect(Socket.java:579)
        at java.net.Socket.connect(Socket.java:528)
        at java.net.Socket.&amp;lt;init&amp;gt;(Socket.java:425)
        at java.net.Socket.&amp;lt;init&amp;gt;(Socket.java:208)
        at org.apache.spark.api.python.PythonAccumulatorParam.addInPlace(PythonRDD.scala:404)
        at org.apache.spark.api.python.PythonAccumulatorParam.addInPlace(PythonRDD.scala:387)
        at org.apache.spark.Accumulable.$plus$plus$eq(Accumulators.scala:72)
        at org.apache.spark.Accumulators$$anonfun$add$2.apply(Accumulators.scala:280)
        at org.apache.spark.Accumulators$$anonfun$add$2.apply(Accumulators.scala:278)
        at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
        at scala.collection.mutable.HashTable$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreachEntry(HashTable.scala:226)
        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
        at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
        at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
        at org.apache.spark.Accumulators$.add(Accumulators.scala:278)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:820)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1226)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
        at akka.actor.ActorCell.invoke(ActorCell.scala:456)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
        at akka.dispatch.Mailbox.run(Mailbox.scala:219)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I did not see an inordinate number of connections from netstat; here&apos;s a sample output at around 10 iterations. &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Every 2.0s: netstat -lpn                                                                                                             Thu Jul 17 11:06:09 2014

Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address               Foreign Address             State       PID/Program name
tcp        0      0 10.38.103.37:7077           0.0.0.0:*                   LISTEN      17945/java
tcp        0      0 0.0.0.0:1191                0.0.0.0:*                   LISTEN      5870/mmfsd
tcp        0      0 0.0.0.0:4040                0.0.0.0:*                   LISTEN      19916/java
tcp        0      0 10.38.103.37:43721          0.0.0.0:*                   LISTEN      19916/java
tcp        0      0 0.0.0.0:111                 0.0.0.0:*                   LISTEN      3385/rpcbind
tcp        0      0 0.0.0.0:8080                0.0.0.0:*                   LISTEN      17945/java
tcp        0      0 0.0.0.0:42833               0.0.0.0:*                   LISTEN      19916/java
tcp        0      0 0.0.0.0:37429               0.0.0.0:*                   LISTEN      19916/java
tcp        0      0 0.0.0.0:22                  0.0.0.0:*                   LISTEN      3827/sshd
tcp        0      0 127.0.0.1:25                0.0.0.0:*                   LISTEN      5937/master
tcp        0      0 127.0.0.1:55227             0.0.0.0:*                   LISTEN      19907/python
tcp        0      0 127.0.0.1:37723             0.0.0.0:*                   LISTEN      19916/java
tcp        0      0 0.0.0.0:6463                0.0.0.0:*                   LISTEN      4688/sge_execd
tcp        0      0 0.0.0.0:35072               0.0.0.0:*                   LISTEN      -
tcp        0      0 0.0.0.0:47457               0.0.0.0:*                   LISTEN      3495/rpc.statd
tcp        0      0 0.0.0.0:5666                0.0.0.0:*                   LISTEN      18741/nrpe
tcp        0      0 0.0.0.0:48451               0.0.0.0:*                   LISTEN      19916/java
udp        0      0 0.0.0.0:41547               0.0.0.0:*                               -
udp        0      0 0.0.0.0:56397               0.0.0.0:*                               14452/rsyslogd
udp        0      0 0.0.0.0:51920               0.0.0.0:*                               3495/rpc.statd
udp        0      0 0.0.0.0:111                 0.0.0.0:*                               3385/rpcbind
udp        0      0 0.0.0.0:1012                0.0.0.0:*                               3385/rpcbind
udp        0      0 0.0.0.0:631                 0.0.0.0:*                               3324/portreserve
udp        0      0 10.38.103.37:123            0.0.0.0:*                               5700/ntpd
udp        0      0 10.36.103.37:123            0.0.0.0:*                               5700/ntpd
udp        0      0 127.0.0.1:123               0.0.0.0:*                               5700/ntpd
udp        0      0 0.0.0.0:123                 0.0.0.0:*                               5700/ntpd
udp        0      0 0.0.0.0:703                 0.0.0.0:*                               3495/rpc.statd
Active UNIX domain sockets (only servers)
Proto RefCnt Flags       Type       State         I-Node PID/Program name    Path
unix  2      [ ACC ]     STREAM     LISTENING     14434  3671/sssd_nss       /&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/sss/pipes/nss
unix  2      [ ACC ]     STREAM     LISTENING     14442  3672/sssd_pam       /&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/sss/pipes/pam
unix  2      [ ACC ]     STREAM     LISTENING     21858  5937/master         &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt;/cleanup
unix  2      [ ACC ]     STREAM     LISTENING     21865  5937/master         &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt;/tlsmgr
unix  2      [ ACC ]     STREAM     LISTENING     21869  5937/master         &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt;/rewrite
unix  2      [ ACC ]     STREAM     LISTENING     21873  5937/master         &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt;/bounce
unix  2      [ ACC ]     STREAM     LISTENING     21877  5937/master         &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt;/defer
unix  2      [ ACC ]     STREAM     LISTENING     21881  5937/master         &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt;/trace
unix  2      [ ACC ]     STREAM     LISTENING     21885  5937/master         &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt;/verify
unix  2      [ ACC ]     STREAM     LISTENING     21889  5937/master         &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt;/flush
unix  2      [ ACC ]     STREAM     LISTENING     21893  5937/master         &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt;/proxymap
unix  2      [ ACC ]     STREAM     LISTENING     21897  5937/master         &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt;/proxywrite
unix  2      [ ACC ]     STREAM     LISTENING     21901  5937/master         &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt;/smtp
unix  2      [ ACC ]     STREAM     LISTENING     21905  5937/master         &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt;/relay
unix  2      [ ACC ]     STREAM     LISTENING     21909  5937/master         &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt;/showq
unix  2      [ ACC ]     STREAM     LISTENING     21913  5937/master         &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt;/error
unix  2      [ ACC ]     STREAM     LISTENING     21917  5937/master         &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt;/retry
unix  2      [ ACC ]     STREAM     LISTENING     21921  5937/master         &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt;/discard
unix  2      [ ACC ]     STREAM     LISTENING     21925  5937/master         &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt;/local
unix  2      [ ACC ]     STREAM     LISTENING     21929  5937/master         &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt;/virtual
unix  2      [ ACC ]     STREAM     LISTENING     21933  5937/master         &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt;/lmtp
unix  2      [ ACC ]     STREAM     LISTENING     21937  5937/master         &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt;/anvil
unix  2      [ ACC ]     STREAM     LISTENING     21941  5937/master         &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt;/scache
unix  2      [ ACC ]     STREAM     LISTENING     23356  5870/mmfsd          /&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/mmfs/mmpmon/mmpmonSocket
unix  2      [ ACC ]     STREAM     LISTENING     8544   1/init              @/com/ubuntu/upstart
unix  2      [ ACC ]     STREAM     LISTENING     14408  3669/sssd           /&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/sss/pipes/&lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt;/sbus-monitor
unix  2      [ ACC ]     STREAM     LISTENING     13457  3385/rpcbind        /&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/run/rpcbind.sock
unix  2      [ ACC ]     STREAM     LISTENING     14967  3813/mcelog         /&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/run/mcelog-client
unix  2      [ ACC ]     STREAM     LISTENING     13603  3477/dbus-daemon    /&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/run/dbus/system_bus_socket
unix  2      [ ACC ]     STREAM     LISTENING     14444  3672/sssd_pam       /&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/sss/pipes/&lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt;/pam
unix  2      [ ACC ]     STREAM     LISTENING     14017  3608/hald           @/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/run/hald/dbus-bOXq61fPG8
unix  2      [ ACC ]     STREAM     LISTENING     21315  5740/uuidd          /&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/run/uuidd/request
unix  2      [ ACC ]     STREAM     LISTENING     14012  3608/hald           @/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/run/hald/dbus-cI10ZZX1oL
unix  2      [ ACC ]     STREAM     LISTENING     14417  3670/sssd_be        /&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/sss/pipes/&lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt;/sbus-dp_default.3670
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The cluster is composed of 32 identical Dell R620s with 2x SandyBridge 8 core Xeons (16 cores total/server), 128GB RAM, and 10Gb ethernet. Our latency is ~0.1ms between all servers in the cluster. 1 node runs as the master, with the other 31 as slaves. &lt;/p&gt;

&lt;p&gt;Please let me know if you need more information or whether this looks like a different bug. &lt;/p&gt;</comment>
                            <comment id="14065025" author="carlilek" created="Thu, 17 Jul 2014 15:22:17 +0000"  >&lt;p&gt;A little more info: &lt;br/&gt;
Nodes are running Scientific Linux 6.3 (Linux 2.6.32-279.el6.x86_64 #1 SMP Thu Jun 21 07:08:44 CDT 2012 x86_64 x86_64 x86_64 GNU/Linux)&lt;br/&gt;
Spark is run against Python 2.7.6, Java 1.7.0.25, and Scala 2.10.3. &lt;/p&gt;

&lt;p&gt;spark-env.sh&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;#!/usr/bin/env bash
ulimit -n 65535
export SCALA_HOME=/usr/local/scala-2.10.3
export SPARK_WORKER_DIR=/scratch/spark/work
export JAVA_HOME=/usr/local/jdk1.7.0_25
export SPARK_LOG_DIR=~/.spark/logs/$JOB_ID/
export SPARK_EXECUTOR_MEMORY=100g
export SPARK_DRIVER_MEMORY=100g
export SPARK_WORKER_MEMORY=100g
export SPARK_LOCAL_DIRS=/scratch/spark/tmp
export PYSPARK_PYTHON=/usr/local/python-2.7.6/bin/python
export SPARK_SLAVES=/scratch/spark/tmp/slaves
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;spark-defaults.conf:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;spark.akka.timeout=300 
spark.storage.blockManagerHeartBeatMs=30000 
spark.akka.retry.wait=30 
spark.akka.frameSize=10000
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="14065121" author="ilikerps" created="Thu, 17 Jul 2014 16:53:59 +0000"  >&lt;p&gt;This problem does look identical. I think I gave you the wrong netstat command, as &quot;-l&quot; only show listening sockets. Try with &quot;-a&quot; instead to see all open connections to confirm this, but the rest of your symptoms align perfectly.&lt;/p&gt;

&lt;p&gt;I did a little Googling around for your specific kernel version, and it turns out &lt;a href=&quot;http://lists.openwall.net/netdev/2011/07/13/39&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;someone else&lt;/a&gt; has had success with tcp_tw_recycle on 2.6.32. Could you try to make absolutely sure that the sysctl is taking effect? Perhaps you can try adding &quot;net.ipv4.tcp_tw_recycle = 1&quot; to /etc/sysctl.conf and then running a &quot;sysctl -p&quot; before restarting pyspark.&lt;/p&gt;</comment>
                            <comment id="14065272" author="carlilek" created="Thu, 17 Jul 2014 18:14:24 +0000"  >&lt;p&gt;So I&apos;ve tried a few different things at this point, and I see the behavior regardless of how I have the sysctls set. Using a &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;watch -n1 &lt;span class=&quot;code-quote&quot;&gt;&quot;netstat -anp | grep TIME_WAIT | wc -l&quot;&lt;/span&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt; command, I can see the number of ephemeral ports used climb up and up and up (to somewhere north of 29000), and then it crashes out, and the number of TIME_WAITs gradually decreases. If I use a 5 node cluster, I can see it climbing, but it decreases while it is running as well, and the 20 iterations manage to complete. The maximum I have seen with a 5 node is ~19000 TIME_WAITs. &lt;/p&gt;

&lt;p&gt;I see the exact same behavior with the sysctls turned off, so I have to assume that the code change has worked around the issue. However, our cluster has very, very fast communication between nodes, so we still run up against the core issue (that of Spark using A TON of ephemeral ports for pyspark) with larger worker counts (ie, greater than, say, 10). &lt;/p&gt;</comment>
                            <comment id="14065306" author="ilikerps" created="Thu, 17 Jul 2014 18:27:02 +0000"  >&lt;p&gt;This problem is kinda silly because we&apos;re accumulating these updates from a single thread in the DAGScheduler, so we should only really have one socket open at a time, but it&apos;s very short lived. We could just reuse the connection with a relatively minor refactor of accumulators.py and PythonAccumulatorParam.&lt;/p&gt;</comment>
                            <comment id="14065549" author="carlilek" created="Thu, 17 Jul 2014 21:05:23 +0000"  >&lt;p&gt;Awesome. I was afraid we were trying to chase down something else here. Glad to hear that it&apos;s a known issue and that you&apos;ve got a good idea how to fix it. Thanks for the quick response!&lt;/p&gt;

&lt;p&gt;--Ken&lt;/p&gt;</comment>
                            <comment id="14068082" author="apachespark" created="Sun, 20 Jul 2014 23:45:19 +0000"  >&lt;p&gt;User &apos;aarondav&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/1503&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/1503&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14068083" author="ilikerps" created="Sun, 20 Jul 2014 23:48:35 +0000"  >&lt;p&gt;Hey Ken,&lt;/p&gt;

&lt;p&gt;I created &lt;a href=&quot;https://github.com/apache/spark/pull/1503&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;PR 1503&lt;/a&gt; to implement the solution I mentioned. It would be great if you could try testing out this patch on your cluster.&lt;/p&gt;

&lt;p&gt;While testing, I noticed that the ephemeral ports were still growing with number of tasks due to how we launch new tasks on the PySpark daemon. However, this should only affect workers, and the rate of buildup should be divided by the number of workers. In other words, it should only ever be a problem on a very small cluster.&lt;/p&gt;</comment>
                            <comment id="14068091" author="carlilek" created="Mon, 21 Jul 2014 01:12:25 +0000"  >&lt;p&gt;Hi Aaron, &lt;/p&gt;

&lt;p&gt;I have pulled the spark-master repo and implemented the pull request. In testing with the script above, I was able to iterate 20 and 40 times successfully on a 30 node cluster. This is looking good. I&apos;ll leave it to the scientist who uses the cluster to do some more testing, but I think at first blush, this is a great solution. Thanks for the quick work!&lt;/p&gt;

&lt;p&gt;-Ken&lt;/p&gt;</comment>
                            <comment id="14069758" author="freeman-lab" created="Tue, 22 Jul 2014 03:18:03 +0000"  >&lt;p&gt;Hi all, I&apos;m &quot;the scientist&quot;, a couple updates from more real world testing, looking very promising!&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Set-up: 60 node cluster, an analysis with iterative updates (essentially a sequence of two map-reduce steps on each iteration), data cached and counted before starting iterations&lt;/li&gt;
	&lt;li&gt;250 GB data set, 4000 tasks / stage, ~6 seconds for each stage to complete. Before the patch I reliably hit the error after about 5 iterations, with the patch 20+ complete.&lt;/li&gt;
	&lt;li&gt;2.3 TB data set, 26000 tasks / stage, ~27 seconds for each stage to complete. Before the patch more than one iteration always failed, with the patch 20+ complete.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;So it&apos;s looking really good. I can also try the other extreme (very small cluster) to see if that issue manifests. Aaron, big thanks for helping with this, it&apos;s a big deal for our workflows, so really terrific to get to the bottom of it!&lt;/p&gt;

&lt;p&gt;&amp;#8211; Jeremy&lt;/p&gt;</comment>
                            <comment id="14070367" author="carlilek" created="Tue, 22 Jul 2014 15:10:13 +0000"  >&lt;p&gt;Hi Aaron, &lt;/p&gt;

&lt;p&gt;Another question for you. Would it work for me to just drop the two changed files into our install of Spark 1.0.1 release copy, or is that likely to cause issues? &lt;/p&gt;

&lt;p&gt;Thanks, &lt;br/&gt;
Ken&lt;/p&gt;</comment>
                            <comment id="14070528" author="ilikerps" created="Tue, 22 Jul 2014 17:08:56 +0000"  >&lt;p&gt;Great to hear! These files haven&apos;t been changed since the 1.0.1 release besides this patch, so it should be fine to just drop them in. (A generally safer option would be to do a git merge, though, against Spark&apos;s refs/pull/1503/head branch.)&lt;/p&gt;</comment>
                            <comment id="14071039" author="pwendell" created="Tue, 22 Jul 2014 22:31:12 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=carlilek&quot; class=&quot;user-hover&quot; rel=&quot;carlilek&quot;&gt;carlilek&lt;/a&gt; I&apos;d actually recommend just pulling Spark from the branch-1.0 maintaince branch. We usually recommend users do this since we only add stability fixes on those branches.&lt;/p&gt;</comment>
                            <comment id="14071192" author="ilikerps" created="Wed, 23 Jul 2014 00:38:37 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pwendell&quot; class=&quot;user-hover&quot; rel=&quot;pwendell&quot;&gt;pwendell&lt;/a&gt; That would in general be the right solution, but this particular change hasn&apos;t been merged yet (referring to the second PR on this bug, which is a more complete fix).&lt;/p&gt;</comment>
                            <comment id="14071197" author="pwendell" created="Wed, 23 Jul 2014 00:43:08 +0000"  >&lt;p&gt;Ah my b. I was confused.&lt;/p&gt;</comment>
                            <comment id="14071669" author="carlilek" created="Wed, 23 Jul 2014 12:51:08 +0000"  >&lt;p&gt;Well, something didn&apos;t work quite right.. our copy of 1.0.1 is the prebuilt copy for Hadoop 1/CDH3. So I did a git init in that directory, then did a git pull &lt;a href=&quot;https://github.com/apache/spark/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/&lt;/a&gt; +refs/pull/1503/head&lt;/p&gt;

&lt;p&gt;Well, that didn&apos;t work... &lt;/p&gt;

&lt;p&gt;I don&apos;t expect you to solve my git noob problems, so I&apos;ll work with someone here to figure it out. &lt;/p&gt;</comment>
                            <comment id="14071767" author="carlilek" created="Wed, 23 Jul 2014 14:22:23 +0000"  >&lt;p&gt;Merging just the two files also did not work. I received a bunch of these errors during the test: &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Exception happened during processing of request from (&lt;span class=&quot;code-quote&quot;&gt;&apos;127.0.0.1&apos;&lt;/span&gt;, 33116)
Traceback (most recent call last):
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/usr/local/python-2.7.6/lib/python2.7/SocketServer.py&quot;&lt;/span&gt;, line 295, in _handle_request_noblock
    self.process_request(request, client_address)
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/usr/local/python-2.7.6/lib/python2.7/SocketServer.py&quot;&lt;/span&gt;, line 321, in process_request
    self.finish_request(request, client_address)
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/usr/local/python-2.7.6/lib/python2.7/SocketServer.py&quot;&lt;/span&gt;, line 334, in finish_request
    self.RequestHandlerClass(request, client_address, self)
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/usr/local/python-2.7.6/lib/python2.7/SocketServer.py&quot;&lt;/span&gt;, line 649, in __init__
    self.handle()
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/usr/local/spark-current/python/pyspark/accumulators.py&quot;&lt;/span&gt;, line 224, in handle
    num_updates = read_int(self.rfile)
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/usr/local/spark-current/python/pyspark/serializers.py&quot;&lt;/span&gt;, line 337, in read_int
    raise EOFError
EOFError
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;And then it errored out with the usual java thing. &lt;/p&gt;</comment>
                            <comment id="14081616" author="joshrosen" created="Thu, 31 Jul 2014 22:36:59 +0000"  >&lt;p&gt;Merged the improved fix from &lt;a href=&quot;https://github.com/apache/spark/pull/1503&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/1503&lt;/a&gt; into 1.1.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>401958</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            11 years, 16 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1x6qn:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>402026</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>