<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:56:32 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-22223] ObjectHashAggregate introduces unnecessary shuffle</title>
                <link>https://issues.apache.org/jira/browse/SPARK-22223</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Since Spark 2.2 the &lt;tt&gt;groupBy&lt;/tt&gt; plus &lt;tt&gt;collect_list&lt;/tt&gt; makes use of unnecessary shuffle when the partitions at previous step are based on looser criteria than the current &lt;tt&gt;groupBy&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;For example:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-comment&quot;&gt;//sample data from https://github.com/databricks/Spark-The-Definitive-Guide/tree/master/data/retail-data
&lt;/span&gt;
&lt;span class=&quot;code-comment&quot;&gt;//Read the data and repartitions by &lt;span class=&quot;code-quote&quot;&gt;&quot;Country&quot;&lt;/span&gt;
&lt;/span&gt;val retailDF = spark.sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;Select * from online_retail&quot;&lt;/span&gt;)
    .repartition(col(&lt;span class=&quot;code-quote&quot;&gt;&quot;Country&quot;&lt;/span&gt;))

&lt;span class=&quot;code-comment&quot;&gt;//Group the data and collect.
&lt;/span&gt;val aggregatedDF = retailDF
  .withColumn(&lt;span class=&quot;code-quote&quot;&gt;&quot;Good&quot;&lt;/span&gt;, expr(&lt;span class=&quot;code-quote&quot;&gt;&quot;(StockCode, UnitPrice, Quantity, Description)&quot;&lt;/span&gt;))
  .groupBy(&lt;span class=&quot;code-quote&quot;&gt;&quot;Country&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;CustomerID&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;InvoiceNo&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;InvoiceDate&quot;&lt;/span&gt;)
  .agg(collect_list(&lt;span class=&quot;code-quote&quot;&gt;&quot;Good&quot;&lt;/span&gt;).as(&lt;span class=&quot;code-quote&quot;&gt;&quot;Goods&quot;&lt;/span&gt;))
  .withColumn(&lt;span class=&quot;code-quote&quot;&gt;&quot;Invoice&quot;&lt;/span&gt;, expr(&lt;span class=&quot;code-quote&quot;&gt;&quot;(InvoiceNo, InvoiceDate, Goods)&quot;&lt;/span&gt;))
  .groupBy(&lt;span class=&quot;code-quote&quot;&gt;&quot;Country&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;CustomerID&quot;&lt;/span&gt;)
  .agg(collect_list(&lt;span class=&quot;code-quote&quot;&gt;&quot;Invoice&quot;&lt;/span&gt;).as(&lt;span class=&quot;code-quote&quot;&gt;&quot;Invoices&quot;&lt;/span&gt;))
  .withColumn(&lt;span class=&quot;code-quote&quot;&gt;&quot;Customer&quot;&lt;/span&gt;, expr(&lt;span class=&quot;code-quote&quot;&gt;&quot;(CustomerID, Invoices)&quot;&lt;/span&gt;))
  .groupBy(&lt;span class=&quot;code-quote&quot;&gt;&quot;Country&quot;&lt;/span&gt;)
  .agg(collect_list(&lt;span class=&quot;code-quote&quot;&gt;&quot;Customer&quot;&lt;/span&gt;).as(&lt;span class=&quot;code-quote&quot;&gt;&quot;Customers&quot;&lt;/span&gt;))
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Without disabling the &lt;tt&gt;ObjectHashAggregate&lt;/tt&gt; one gets the following physical plan:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;== Physical Plan ==
ObjectHashAggregate(keys=[Country#14], functions=[finalmerge_collect_list(merge buf#317) AS collect_list(Customer#299, 0, 0)#310])
+- Exchange hashpartitioning(Country#14, 200)
   +- ObjectHashAggregate(keys=[Country#14], functions=[partial_collect_list(Customer#299, 0, 0) AS buf#317])
      +- *Project [Country#14, named_struct(CustomerID, CustomerID#13, Invoices, Invoices#294) AS Customer#299]
         +- ObjectHashAggregate(keys=[Country#14, CustomerID#13], functions=[finalmerge_collect_list(merge buf#319) AS collect_list(Invoice#278, 0, 0)#293])
            +- Exchange hashpartitioning(Country#14, CustomerID#13, 200)
               +- ObjectHashAggregate(keys=[Country#14, CustomerID#13], functions=[partial_collect_list(Invoice#278, 0, 0) AS buf#319])
                  +- *Project [Country#14, CustomerID#13, named_struct(InvoiceNo, InvoiceNo#7, InvoiceDate, InvoiceDate#11, Goods, Goods#271) AS Invoice#278]
                     +- ObjectHashAggregate(keys=[Country#14, CustomerID#13, InvoiceNo#7, InvoiceDate#11], functions=[finalmerge_collect_list(merge buf#321) AS collect_list(Good#249, 0, 0)#270])
                        +- Exchange hashpartitioning(Country#14, CustomerID#13, InvoiceNo#7, InvoiceDate#11, 200)
                           +- ObjectHashAggregate(keys=[Country#14, CustomerID#13, InvoiceNo#7, InvoiceDate#11], functions=[partial_collect_list(Good#249, 0, 0) AS buf#321])
                              +- *Project [InvoiceNo#7, InvoiceDate#11, CustomerID#13, Country#14, named_struct(StockCode, StockCode#8, UnitPrice, UnitPrice#12, Quantity, Quantity#10, Description, Description#9) AS Good#249]
                                 +- Exchange hashpartitioning(Country#14, 200)
                                    +- *FileScan csv default.online_retail[InvoiceNo#7,StockCode#8,Description#9,Quantity#10,InvoiceDate#11,UnitPrice#12,CustomerID#13,Country#14] Batched: false, Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/scgc0grb1506404260438], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&amp;lt;InvoiceNo:string,StockCode:string,Description:string,Quantity:string,InvoiceDate:string,Un...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;With Spark 2.1.0 or when &lt;tt&gt;ObjectHashAggregate&lt;/tt&gt; is disabled, one gets a more efficient:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;== Physical Plan ==
SortAggregate(key=[Country#14], functions=[finalmerge_collect_list(merge buf#3834) AS collect_list(Customer#299, 0, 0)#310])
+- SortAggregate(key=[Country#14], functions=[partial_collect_list(Customer#299, 0, 0) AS buf#3834])
   +- *Project [Country#14, named_struct(CustomerID, CustomerID#13, Invoices, Invoices#294) AS Customer#299]
      +- SortAggregate(key=[Country#14, CustomerID#13], functions=[finalmerge_collect_list(merge buf#319) AS collect_list(Invoice#278, 0, 0)#293])
         +- SortAggregate(key=[Country#14, CustomerID#13], functions=[partial_collect_list(Invoice#278, 0, 0) AS buf#319])
            +- *Project [Country#14, CustomerID#13, named_struct(InvoiceNo, InvoiceNo#7, InvoiceDate, InvoiceDate#11, Goods, Goods#271) AS Invoice#278]
               +- SortAggregate(key=[Country#14, CustomerID#13, InvoiceNo#7, InvoiceDate#11], functions=[finalmerge_collect_list(merge buf#321) AS collect_list(Good#249, 0, 0)#270])
                  +- SortAggregate(key=[Country#14, CustomerID#13, InvoiceNo#7, InvoiceDate#11], functions=[partial_collect_list(Good#249, 0, 0) AS buf#321])
                     +- *Sort [Country#14 ASC NULLS FIRST, CustomerID#13 ASC NULLS FIRST, InvoiceNo#7 ASC NULLS FIRST, InvoiceDate#11 ASC NULLS FIRST], false, 0
                        +- *Project [InvoiceNo#7, InvoiceDate#11, CustomerID#13, Country#14, named_struct(StockCode, StockCode#8, UnitPrice, UnitPrice#12, Quantity, Quantity#10, Description, Description#9) AS Good#249]
                           +- Exchange hashpartitioning(Country#14, 200)
                              +- *FileScan csv default.online_retail[InvoiceNo#7,StockCode#8,Description#9,Quantity#10,InvoiceDate#11,UnitPrice#12,CustomerID#13,Country#14] Batched: false, Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/scgc0grb1506404260438], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&amp;lt;InvoiceNo:string,StockCode:string,Description:string,Quantity:string,InvoiceDate:string,Un...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this example, a quick run on DataBricks Notebook showed that by manually disabling the &lt;tt&gt;ObjectHashAggregate&lt;/tt&gt; one gets around 16s execution time versus the 25s needed when &lt;tt&gt;ObjectHashAggregate&lt;/tt&gt; is enabled.&lt;/p&gt;

&lt;p&gt;The use of the &lt;tt&gt;ObjectHashAggregate&lt;/tt&gt; in the &lt;tt&gt;groupBy&lt;/tt&gt; was introduced with &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-17949&quot; title=&quot;Introduce a JVM object based aggregate operator&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-17949&quot;&gt;&lt;del&gt;SPARK-17949&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</description>
                <environment>&lt;p&gt;Spark 2.2.0 and following.&lt;br/&gt;
&lt;tt&gt;spark.sql.execution.useObjectHashAggregateExec = true&lt;/tt&gt;&lt;/p&gt;</environment>
        <key id="13107896">SPARK-22223</key>
            <summary>ObjectHashAggregate introduces unnecessary shuffle</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="viirya">L. C. Hsieh</assignee>
                                    <reporter username="mcs">Michele Costantino Soccio</reporter>
                        <labels>
                    </labels>
                <created>Mon, 9 Oct 2017 06:13:24 +0000</created>
                <updated>Sun, 17 May 2020 17:58:37 +0000</updated>
                            <resolved>Mon, 16 Oct 2017 05:48:56 +0000</resolved>
                                    <version>2.2.0</version>
                                    <fixVersion>2.2.1</fixVersion>
                    <fixVersion>2.3.0</fixVersion>
                                    <component>Optimizer</component>
                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="16199597" author="maropu" created="Wed, 11 Oct 2017 00:05:16 +0000"  >&lt;p&gt;The hash-based aggregate implementation requires the partitioning that you select in group-by clauses. In the looser case you suggested, it seems we need shuffles. Do I misunderstand your suggestion?&lt;/p&gt;</comment>
                            <comment id="16199831" author="mcs" created="Wed, 11 Oct 2017 05:52:11 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=maropu&quot; class=&quot;user-hover&quot; rel=&quot;maropu&quot;&gt;maropu&lt;/a&gt; Not sure I understand your observation.&lt;/p&gt;

&lt;p&gt;In the hash-based aggregate implementation Spark does a repartitioning and a shuffle at each &lt;tt&gt;groupBy&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;In the sort-based aggregate one, Spark does no repartition and no shuffle.&lt;/p&gt;

&lt;p&gt;I would like to ignore whether Spark prefers sort-based or hash-based aggregation, but I do not want Spark to shuffle and repartition when there is no need for it.&lt;/p&gt;</comment>
                            <comment id="16199837" author="maropu" created="Wed, 11 Oct 2017 06:00:38 +0000"  >&lt;p&gt;Probably, this ticket is related to &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-16026&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/SPARK-16026&lt;/a&gt;; the better strategy of aggregates depends on costs.&lt;/p&gt;</comment>
                            <comment id="16205038" author="apachespark" created="Sun, 15 Oct 2017 06:07:03 +0000"  >&lt;p&gt;User &apos;viirya&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/19501&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/19501&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16205458" author="cloud_fan" created="Mon, 16 Oct 2017 05:48:56 +0000"  >&lt;p&gt;Issue resolved by pull request 19501&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/19501&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/19501&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="13109346">SPARK-22276</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 5 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3l0kn:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>