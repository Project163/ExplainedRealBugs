<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:34:01 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-11246] [1.5] Table cache for Parquet broken in 1.5</title>
                <link>https://issues.apache.org/jira/browse/SPARK-11246</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Since upgrading to 1.5.1, using the &lt;tt&gt;CACHE TABLE&lt;/tt&gt; works great for all tables except for parquet tables, likely related to the parquet native reader.&lt;/p&gt;

&lt;p&gt;Here are steps for parquet table:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;create table test_parquet stored as parquet as select 1;
explain select * from test_parquet;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With output:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;== Physical Plan ==
Scan ParquetRelation[hdfs:&lt;span class=&quot;code-comment&quot;&gt;//192.168.99.9/user/hive/warehouse/test_parquet][_c0#141]&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And then caching:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;cache table test_parquet;
explain select * from test_parquet;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With output:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;== Physical Plan ==
Scan ParquetRelation[hdfs:&lt;span class=&quot;code-comment&quot;&gt;//192.168.99.9/user/hive/warehouse/test_parquet][_c0#174]&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note it isn&apos;t cached. I have included spark log output for the &lt;tt&gt;cache table&lt;/tt&gt; and &lt;tt&gt;explain&lt;/tt&gt; statements below.&lt;/p&gt;

&lt;p&gt;&amp;#8212;&lt;/p&gt;

&lt;p&gt;Here&apos;s the same for non-parquet table:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;cache table test_no_parquet;
explain select * from test_no_parquet;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With output:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;== Physical Plan ==
HiveTableScan [_c0#210], (MetastoreRelation &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;, test_no_parquet, None)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And then caching:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;cache table test_no_parquet;
explain select * from test_no_parquet;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With output:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;== Physical Plan ==
InMemoryColumnarTableScan [_c0#229], (InMemoryRelation [_c0#229], &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, 10000, StorageLevel(&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;, &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, 1), (HiveTableScan [_c0#211], (MetastoreRelation &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;, test_no_parquet, None)), Some(test_no_parquet))
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Not that the table seems to be cached.&lt;br/&gt;
&amp;#8212;&lt;/p&gt;

&lt;p&gt;Note that if the flag &lt;tt&gt;spark.sql.hive.convertMetastoreParquet&lt;/tt&gt; is set to &lt;tt&gt;false&lt;/tt&gt;, parquet tables work the same as non-parquet tables with caching. This is a reasonable workaround for us, but ideally, we would like to benefit from the native reading.&lt;/p&gt;

&lt;p&gt;&amp;#8212;&lt;/p&gt;

&lt;p&gt;Spark logs for &lt;tt&gt;cache table&lt;/tt&gt; for &lt;tt&gt;test_parquet&lt;/tt&gt;:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;15/10/21 21:22:05 INFO thriftserver.SparkExecuteStatementOperation: Running query &lt;span class=&quot;code-quote&quot;&gt;&apos;cache table test_parquet&apos;&lt;/span&gt; with 20ee2ab9-5242-4783-81cf-46115ed72610
15/10/21 21:22:05 INFO metastore.HiveMetaStore: 49: get_table : db=&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt; tbl=test_parquet
15/10/21 21:22:05 INFO HiveMetaStore.audit: ugi=vagrant	ip=unknown-ip-addr	cmd=get_table : db=&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt; tbl=test_parquet
15/10/21 21:22:05 INFO metastore.HiveMetaStore: 49: Opening raw store with implemenation &lt;span class=&quot;code-keyword&quot;&gt;class:&lt;/span&gt;org.apache.hadoop.hive.metastore.ObjectStore
15/10/21 21:22:05 INFO metastore.ObjectStore: ObjectStore, initialize called
15/10/21 21:22:05 INFO DataNucleus.Query: Reading in results &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; query &lt;span class=&quot;code-quote&quot;&gt;&quot;org.datanucleus.store.rdbms.query.SQLQuery@0&quot;&lt;/span&gt; since the connection used is closing
15/10/21 21:22:05 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is MYSQL
15/10/21 21:22:05 INFO metastore.ObjectStore: Initialized ObjectStore
15/10/21 21:22:05 INFO storage.MemoryStore: ensureFreeSpace(215680) called with curMem=4196713, maxMem=139009720
15/10/21 21:22:05 INFO storage.MemoryStore: Block broadcast_59 stored as values in memory (estimated size 210.6 KB, free 128.4 MB)
15/10/21 21:22:05 INFO storage.MemoryStore: ensureFreeSpace(20265) called with curMem=4412393, maxMem=139009720
15/10/21 21:22:05 INFO storage.MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 19.8 KB, free 128.3 MB)
15/10/21 21:22:05 INFO storage.BlockManagerInfo: Added broadcast_59_piece0 in memory on 192.168.99.9:50262 (size: 19.8 KB, free: 132.2 MB)
15/10/21 21:22:05 INFO spark.SparkContext: Created broadcast 59 from run at AccessController.java:-2
15/10/21 21:22:05 INFO metastore.HiveMetaStore: 49: get_table : db=&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt; tbl=test_parquet
15/10/21 21:22:05 INFO HiveMetaStore.audit: ugi=vagrant	ip=unknown-ip-addr	cmd=get_table : db=&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt; tbl=test_parquet
15/10/21 21:22:05 INFO storage.MemoryStore: ensureFreeSpace(215680) called with curMem=4432658, maxMem=139009720
15/10/21 21:22:05 INFO storage.MemoryStore: Block broadcast_60 stored as values in memory (estimated size 210.6 KB, free 128.1 MB)
15/10/21 21:22:05 INFO storage.BlockManagerInfo: Removed broadcast_58_piece0 on 192.168.99.9:50262 in memory (size: 19.8 KB, free: 132.2 MB)
15/10/21 21:22:05 INFO storage.BlockManagerInfo: Removed broadcast_57_piece0 on 192.168.99.9:50262 in memory (size: 21.1 KB, free: 132.2 MB)
15/10/21 21:22:05 INFO storage.BlockManagerInfo: Removed broadcast_57_piece0 on slave2:46912 in memory (size: 21.1 KB, free: 534.5 MB)
15/10/21 21:22:05 INFO storage.BlockManagerInfo: Removed broadcast_57_piece0 on slave0:46599 in memory (size: 21.1 KB, free: 534.3 MB)
15/10/21 21:22:05 INFO spark.ContextCleaner: Cleaned accumulator 86
15/10/21 21:22:05 INFO spark.ContextCleaner: Cleaned accumulator 84
15/10/21 21:22:05 INFO storage.MemoryStore: ensureFreeSpace(20265) called with curMem=4327620, maxMem=139009720
15/10/21 21:22:05 INFO storage.MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 19.8 KB, free 128.4 MB)
15/10/21 21:22:05 INFO storage.BlockManagerInfo: Added broadcast_60_piece0 in memory on 192.168.99.9:50262 (size: 19.8 KB, free: 132.2 MB)
15/10/21 21:22:05 INFO spark.SparkContext: Created broadcast 60 from run at AccessController.java:-2
15/10/21 21:22:05 INFO spark.SparkContext: Starting job: run at AccessController.java:-2
15/10/21 21:22:05 INFO parquet.ParquetRelation: Reading Parquet file(s) from hdfs:&lt;span class=&quot;code-comment&quot;&gt;//192.168.99.9/user/hive/warehouse/test_parquet/part-r-00000-7cf64eb9-76ca-47c7-92aa-eb5ba879faae.gz.parquet
&lt;/span&gt;15/10/21 21:22:05 INFO scheduler.DAGScheduler: Registering RDD 171 (run at AccessController.java:-2)
15/10/21 21:22:05 INFO scheduler.DAGScheduler: Got job 24 (run at AccessController.java:-2) with 1 output partitions
15/10/21 21:22:05 INFO scheduler.DAGScheduler: Final stage: ResultStage 34(run at AccessController.java:-2)
15/10/21 21:22:05 INFO scheduler.DAGScheduler: Parents of &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; stage: List(ShuffleMapStage 33)
15/10/21 21:22:05 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 33)
15/10/21 21:22:05 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 33 (MapPartitionsRDD[171] at run at AccessController.java:-2), which has no missing parents
15/10/21 21:22:05 INFO storage.MemoryStore: ensureFreeSpace(9472) called with curMem=4347885, maxMem=139009720
15/10/21 21:22:05 INFO storage.MemoryStore: Block broadcast_61 stored as values in memory (estimated size 9.3 KB, free 128.4 MB)
15/10/21 21:22:05 INFO storage.MemoryStore: ensureFreeSpace(4838) called with curMem=4357357, maxMem=139009720
15/10/21 21:22:05 INFO storage.MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 4.7 KB, free 128.4 MB)
15/10/21 21:22:05 INFO storage.BlockManagerInfo: Added broadcast_61_piece0 in memory on 192.168.99.9:50262 (size: 4.7 KB, free: 132.2 MB)
15/10/21 21:22:05 INFO spark.SparkContext: Created broadcast 61 from broadcast at DAGScheduler.scala:861
15/10/21 21:22:05 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 33 (MapPartitionsRDD[171] at run at AccessController.java:-2)
15/10/21 21:22:05 INFO cluster.YarnScheduler: Adding task set 33.0 with 1 tasks
15/10/21 21:22:05 INFO scheduler.FairSchedulableBuilder: Added task set TaskSet_33 tasks to pool &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;
15/10/21 21:22:05 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 33.0 (TID 45, slave2, NODE_LOCAL, 2234 bytes)
15/10/21 21:22:05 INFO storage.BlockManagerInfo: Added broadcast_61_piece0 in memory on slave2:46912 (size: 4.7 KB, free: 534.5 MB)
15/10/21 21:22:05 INFO storage.BlockManagerInfo: Added broadcast_60_piece0 in memory on slave2:46912 (size: 19.8 KB, free: 534.4 MB)
15/10/21 21:22:05 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 33.0 (TID 45) in 105 ms on slave2 (1/1)
15/10/21 21:22:05 INFO cluster.YarnScheduler: Removed TaskSet 33.0, whose tasks have all completed, from pool &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;
15/10/21 21:22:05 INFO scheduler.DAGScheduler: ShuffleMapStage 33 (run at AccessController.java:-2) finished in 0.105 s
15/10/21 21:22:05 INFO scheduler.DAGScheduler: looking &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; newly runnable stages
15/10/21 21:22:05 INFO scheduler.DAGScheduler: running: Set()
15/10/21 21:22:05 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 34)
15/10/21 21:22:05 INFO scheduler.DAGScheduler: failed: Set()
15/10/21 21:22:05 INFO scheduler.DAGScheduler: Missing parents &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; ResultStage 34: List()
15/10/21 21:22:05 INFO scheduler.StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@532f49c8
15/10/21 21:22:05 INFO scheduler.DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[174] at run at AccessController.java:-2), which is now runnable
15/10/21 21:22:05 INFO scheduler.StatsReportListener: task runtime:(count: 1, mean: 105.000000, stdev: 0.000000, max: 105.000000, min: 105.000000)
15/10/21 21:22:05 INFO scheduler.StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/10/21 21:22:05 INFO scheduler.StatsReportListener: 	105.0 ms	105.0 ms	105.0 ms	105.0 ms	105.0 ms	105.0 ms	105.0 ms	105.0 ms	105.0 ms
15/10/21 21:22:05 INFO scheduler.StatsReportListener: shuffle bytes written:(count: 1, mean: 49.000000, stdev: 0.000000, max: 49.000000, min: 49.000000)
15/10/21 21:22:05 INFO scheduler.StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/10/21 21:22:05 INFO scheduler.StatsReportListener: 	49.0 B	49.0 B	49.0 B	49.0 B	49.0 B	49.0 B	49.0 B	49.0 B	49.0 B
15/10/21 21:22:05 INFO storage.MemoryStore: ensureFreeSpace(10440) called with curMem=4362195, maxMem=139009720
15/10/21 21:22:05 INFO scheduler.StatsReportListener: task result size:(count: 1, mean: 2381.000000, stdev: 0.000000, max: 2381.000000, min: 2381.000000)
15/10/21 21:22:05 INFO scheduler.StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/10/21 21:22:05 INFO scheduler.StatsReportListener: 	2.3 KB	2.3 KB	2.3 KB	2.3 KB	2.3 KB	2.3 KB	2.3 KB	2.3 KB	2.3 KB
15/10/21 21:22:05 INFO storage.MemoryStore: Block broadcast_62 stored as values in memory (estimated size 10.2 KB, free 128.4 MB)
15/10/21 21:22:05 INFO scheduler.StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 68.571429, stdev: 0.000000, max: 68.571429, min: 68.571429)
15/10/21 21:22:05 INFO scheduler.StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/10/21 21:22:05 INFO scheduler.StatsReportListener: 	69 %	69 %	69 %	69 %	69 %	69 %	69 %	69 %	69 %
15/10/21 21:22:05 INFO scheduler.StatsReportListener: other time pct: (count: 1, mean: 31.428571, stdev: 0.000000, max: 31.428571, min: 31.428571)
15/10/21 21:22:05 INFO scheduler.StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/10/21 21:22:05 INFO scheduler.StatsReportListener: 	31 %	31 %	31 %	31 %	31 %	31 %	31 %	31 %	31 %
15/10/21 21:22:05 INFO storage.MemoryStore: ensureFreeSpace(5358) called with curMem=4372635, maxMem=139009720
15/10/21 21:22:05 INFO storage.MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 5.2 KB, free 128.4 MB)
15/10/21 21:22:05 INFO storage.BlockManagerInfo: Added broadcast_62_piece0 in memory on 192.168.99.9:50262 (size: 5.2 KB, free: 132.2 MB)
15/10/21 21:22:05 INFO spark.SparkContext: Created broadcast 62 from broadcast at DAGScheduler.scala:861
15/10/21 21:22:05 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[174] at run at AccessController.java:-2)
15/10/21 21:22:05 INFO cluster.YarnScheduler: Adding task set 34.0 with 1 tasks
15/10/21 21:22:05 INFO scheduler.FairSchedulableBuilder: Added task set TaskSet_34 tasks to pool &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;
15/10/21 21:22:05 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 34.0 (TID 46, slave2, PROCESS_LOCAL, 1914 bytes)
15/10/21 21:22:05 INFO storage.BlockManagerInfo: Added broadcast_62_piece0 in memory on slave2:46912 (size: 5.2 KB, free: 534.4 MB)
15/10/21 21:22:05 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; shuffle 9 to slave2:43867
15/10/21 21:22:05 INFO spark.MapOutputTrackerMaster: Size of output statuses &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; shuffle 9 is 135 bytes
15/10/21 21:22:05 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 34.0 (TID 46) in 48 ms on slave2 (1/1)
15/10/21 21:22:05 INFO cluster.YarnScheduler: Removed TaskSet 34.0, whose tasks have all completed, from pool &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;
15/10/21 21:22:05 INFO scheduler.DAGScheduler: ResultStage 34 (run at AccessController.java:-2) finished in 0.047 s
15/10/21 21:22:05 INFO scheduler.StatsReportListener: Finished stage: org.apache.spark.scheduler.StageInfo@37a20848
15/10/21 21:22:05 INFO scheduler.StatsReportListener: task runtime:(count: 1, mean: 48.000000, stdev: 0.000000, max: 48.000000, min: 48.000000)
15/10/21 21:22:05 INFO scheduler.StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/10/21 21:22:05 INFO scheduler.StatsReportListener: 	48.0 ms	48.0 ms	48.0 ms	48.0 ms	48.0 ms	48.0 ms	48.0 ms	48.0 ms	48.0 ms
15/10/21 21:22:05 INFO scheduler.StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/10/21 21:22:05 INFO scheduler.StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/10/21 21:22:05 INFO scheduler.StatsReportListener: 	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms	0.0 ms
15/10/21 21:22:05 INFO scheduler.StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/10/21 21:22:05 INFO scheduler.StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/10/21 21:22:05 INFO scheduler.StatsReportListener: 	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B	0.0 B
15/10/21 21:22:05 INFO scheduler.StatsReportListener: task result size:(count: 1, mean: 1737.000000, stdev: 0.000000, max: 1737.000000, min: 1737.000000)
15/10/21 21:22:05 INFO scheduler.StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/10/21 21:22:05 INFO scheduler.StatsReportListener: 	1737.0 B	1737.0 B	1737.0 B	1737.0 B	1737.0 B	1737.0 B	1737.0 B	1737.0 B	1737.0 B
15/10/21 21:22:05 INFO scheduler.StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 29.166667, stdev: 0.000000, max: 29.166667, min: 29.166667)
15/10/21 21:22:05 INFO scheduler.StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/10/21 21:22:05 INFO scheduler.StatsReportListener: 	29 %	29 %	29 %	29 %	29 %	29 %	29 %	29 %	29 %
15/10/21 21:22:05 INFO scheduler.StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)
15/10/21 21:22:05 INFO scheduler.StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/10/21 21:22:05 INFO scheduler.StatsReportListener: 	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %	 0 %
15/10/21 21:22:05 INFO scheduler.StatsReportListener: other time pct: (count: 1, mean: 70.833333, stdev: 0.000000, max: 70.833333, min: 70.833333)
15/10/21 21:22:05 INFO scheduler.StatsReportListener: 	0%	5%	10%	25%	50%	75%	90%	95%	100%
15/10/21 21:22:05 INFO scheduler.StatsReportListener: 	71 %	71 %	71 %	71 %	71 %	71 %	71 %	71 %	71 %
15/10/21 21:22:05 INFO scheduler.DAGScheduler: Job 24 finished: run at AccessController.java:-2, took 0.175295 s
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Spark logs for &lt;tt&gt;explain&lt;/tt&gt; for &lt;tt&gt;test_parquet&lt;/tt&gt;:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;15/10/21 21:23:19 INFO thriftserver.SparkExecuteStatementOperation: Running query &lt;span class=&quot;code-quote&quot;&gt;&apos;explain select * from test_parquet&apos;&lt;/span&gt; with bae9c0bf-57f9-4c80-b745-3f0202469f3f
15/10/21 21:23:19 INFO parse.ParseDriver: Parsing command: explain select * from test_parquet
15/10/21 21:23:19 INFO parse.ParseDriver: Parse Completed
15/10/21 21:23:19 INFO metastore.HiveMetaStore: 50: get_table : db=&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt; tbl=test_parquet
15/10/21 21:23:19 INFO HiveMetaStore.audit: ugi=vagrant	ip=unknown-ip-addr	cmd=get_table : db=&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt; tbl=test_parquet
15/10/21 21:23:19 INFO metastore.HiveMetaStore: 50: Opening raw store with implemenation &lt;span class=&quot;code-keyword&quot;&gt;class:&lt;/span&gt;org.apache.hadoop.hive.metastore.ObjectStore
15/10/21 21:23:19 INFO metastore.ObjectStore: ObjectStore, initialize called
15/10/21 21:23:19 INFO DataNucleus.Query: Reading in results &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; query &lt;span class=&quot;code-quote&quot;&gt;&quot;org.datanucleus.store.rdbms.query.SQLQuery@0&quot;&lt;/span&gt; since the connection used is closing
15/10/21 21:23:19 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is MYSQL
15/10/21 21:23:19 INFO metastore.ObjectStore: Initialized ObjectStore
15/10/21 21:23:19 INFO storage.MemoryStore: ensureFreeSpace(215680) called with curMem=4377993, maxMem=139009720
15/10/21 21:23:19 INFO storage.MemoryStore: Block broadcast_63 stored as values in memory (estimated size 210.6 KB, free 128.2 MB)
15/10/21 21:23:19 INFO storage.MemoryStore: ensureFreeSpace(20265) called with curMem=4593673, maxMem=139009720
15/10/21 21:23:19 INFO storage.MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 19.8 KB, free 128.2 MB)
15/10/21 21:23:19 INFO storage.BlockManagerInfo: Added broadcast_63_piece0 in memory on 192.168.99.9:50262 (size: 19.8 KB, free: 132.2 MB)
15/10/21 21:23:19 INFO spark.SparkContext: Created broadcast 63 from run at AccessController.java:-2
15/10/21 21:23:19 INFO thriftserver.SparkExecuteStatementOperation: Result Schema: List(plan#262)
15/10/21 21:23:19 INFO thriftserver.SparkExecuteStatementOperation: Result Schema: List(plan#262)
15/10/21 21:23:19 INFO thriftserver.SparkExecuteStatementOperation: Result Schema: List(plan#262)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="12906888">SPARK-11246</key>
            <summary>[1.5] Table cache for Parquet broken in 1.5</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="xwu0226">Xin Wu</assignee>
                                    <reporter username="dyross">David Ross</reporter>
                        <labels>
                    </labels>
                <created>Wed, 21 Oct 2015 21:23:31 +0000</created>
                <updated>Tue, 8 Dec 2015 04:45:06 +0000</updated>
                            <resolved>Thu, 29 Oct 2015 14:58:36 +0000</resolved>
                                    <version>1.5.1</version>
                                    <fixVersion>1.5.2</fixVersion>
                    <fixVersion>1.6.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>6</votes>
                                    <watches>9</watches>
                                                                                                                <comments>
                            <comment id="14968293" author="xwu0226" created="Thu, 22 Oct 2015 00:55:21 +0000"  >&lt;p&gt;I am able to recreate this issue on 1.6.0 with hive 1.2.1.. I am looking into it. &lt;/p&gt;</comment>
                            <comment id="14970071" author="xwu0226" created="Thu, 22 Oct 2015 23:02:48 +0000"  >&lt;p&gt;Root cause found, fix is being tested. will submit PR shortly.&lt;/p&gt;</comment>
                            <comment id="14977940" author="apachespark" created="Wed, 28 Oct 2015 08:06:07 +0000"  >&lt;p&gt;User &apos;xwu0226&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/9326&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/9326&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14980569" author="yhuai" created="Thu, 29 Oct 2015 14:58:36 +0000"  >&lt;p&gt;Issue resolved by pull request 9326&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/9326&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/9326&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14980570" author="yhuai" created="Thu, 29 Oct 2015 14:59:05 +0000"  >&lt;p&gt;I picked it in branch 1.5. Will update the fix version later.&lt;/p&gt;</comment>
                            <comment id="14980590" author="yhuai" created="Thu, 29 Oct 2015 15:09:05 +0000"  >&lt;p&gt;OK I add 1.5.3 as the fix version. If we re-cut 1.5.2, I will change the fix version.&lt;/p&gt;</comment>
                            <comment id="14980652" author="xwu0226" created="Thu, 29 Oct 2015 15:37:51 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yhuai&quot; class=&quot;user-hover&quot; rel=&quot;yhuai&quot;&gt;yhuai&lt;/a&gt; Thank you!&lt;/p&gt;</comment>
                            <comment id="15002954" author="tumanian@gmail.com" created="Thu, 12 Nov 2015 21:20:24 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yhuai&quot; class=&quot;user-hover&quot; rel=&quot;yhuai&quot;&gt;yhuai&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I am running into an issue that might be related to this. In short, when doing reads from in memory cached tables, i see that spark is scanning all the columns, as opposed to non-cached parquet tables, when only the required columns are read.  For example, i have a table sales_report which  is ~800MB on disk stored as parquet, with about 200 columns.  When running a query on the non cached table:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;select  count(distinct  make ) from sales_report; 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;  
&lt;p&gt;I see on the spark UI that the job reads about 5MB.&lt;br/&gt;
Looking at the explain of the query, i see that it only reads the necessary column from parquet, as expected. &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt; explain select  count(distinct make) from sales_report;
| == Physical Plan ==                                                                                            
                                                             |
| TungstenAggregate(key=[], functions=[(count(make#7871),mode=Complete,isDistinct=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)], output=[_c0#7870L])                       
|  TungstenAggregate(key=[make#7871], functions=[], output=[make#7871])                                                
|   TungstenExchange SinglePartition                                                                                
|    TungstenAggregate(key=[make#7871], functions=[], output=[make#7871])                                     
|     Scan ParquetRelation[hdfs:&lt;span class=&quot;code-comment&quot;&gt;//redacted:8020/apps/hive/warehouse/redacted/sales_report][make#7871]                               
&lt;/span&gt;+----------------------------------------------------------------------------------------------------------------+
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;However when I run &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;cache table sales_report
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and do explain on the same query, i see that spark is trying to read all the columns in the table. When running the query, it reads 800MB from the memory. &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;explain select  count(distinct make) from sales_report; 

| == Physical Plan ==                                                                                                                                                               
| TungstenAggregate(key=[], functions=[(count(make#13337),mode=Complete,isDistinct=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)], output=[_c0#17135L])                                                                     
|  TungstenAggregate(key=[make#13337], functions=[], output=[make#13337])                                                                                                           
|   TungstenExchange SinglePartition                                                                                                                                                
|    TungstenAggregate(key=[make#13337], functions=[], output=[make#13337])                                                                                                         
|     InMemoryColumnarTableScan [make#13337], (InMemoryRelation [make#13337,makeref#13338,registration#13339,chassis#13340,derivative#13341,derivativeid#13342,registrationdate#133 |
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I tried disabling Tungsten, but that didn&apos;t change this behavior. &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt; set  spark.sql.tungsten.enabled=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;;
 explain select  count(distinct  make ) from sales_report; 
| == Physical Plan ==                                                                                                                                                               
| Aggregate &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;, [CombineAndCount(partialSets#19514) AS _c0#18328L]                                                                                                               
|  Exchange SinglePartition                                                                                                                                                         
|   Aggregate &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, [AddToHashSet(make#13337) AS partialSets#19514]                                                                                                                 
|    InMemoryColumnarTableScan [make#13337], (InMemoryRelation [make#13337,makeref#13338,registration#13339,chassis#13340,derivative#13341,derivativeid#13342,registrationdate#1334 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt; 

&lt;p&gt;I don&apos;t know if this is a bug or expected behavior, but it results in queries over cached and uncached tables having the same run time over large datasets.  I am running Spark 1.5.0. &lt;/p&gt;

&lt;p&gt;Thanks&lt;br/&gt;
Gurgen &lt;/p&gt;</comment>
                            <comment id="15002959" author="marmbrus" created="Thu, 12 Nov 2015 21:25:01 +0000"  >&lt;p&gt;When building the cache, we are going to read all of the columns the first time.  Subsuquent scans will pass over only the columns required.&lt;/p&gt;

&lt;p&gt;Note you should avoid using caching if the table does not fit in memory as the construction of in-memory buffers is pretty expensive.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="12919563">SPARK-12167</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 1 week, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2nciv:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12333643">1.5.2</customfieldvalue>
    <customfieldvalue id="12333969">1.5.3</customfieldvalue>
    <customfieldvalue id="12333083">1.6.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>