<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 19:03:11 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-25922] [K8] Spark Driver/Executor &quot;spark-app-selector&quot; label mismatch</title>
                <link>https://issues.apache.org/jira/browse/SPARK-25922</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I have been testing Spark 2.4.0 RC4 on Kubernetes&#160; to run Python Spark Applications and running into an issue where the AppId label on the driver and executors mis-match. I am using the &lt;a href=&quot;https://github.com/GoogleCloudPlatform/spark-on-k8s-operator&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/GoogleCloudPlatform/spark-on-k8s-operator&lt;/a&gt;&#160;to run these applications.&#160;&lt;/p&gt;

&lt;p&gt;I see a spark.app.id of the form&#160;spark-* as&#160; &quot;spark-app-selector&quot; label&#160;on the driver as well as in the K8 config-map which gets created for the driver via spark-submit . My guess is this is coming from &lt;a href=&quot;https://github.com/apache/spark/blob/f6cc354d83c2c9a757f9b507aadd4dbdc5825cca/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesClientApplication.scala#L211&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/f6cc354d83c2c9a757f9b507aadd4dbdc5825cca/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesClientApplication.scala#L211&lt;/a&gt;&#160;&lt;/p&gt;

&lt;p&gt;But when the driver actually comes up&#160;and brings up executors etc. , I see that the &quot;spark-app-selector&quot; label on the executors&#160;as well as the spark.app.Id config within the user-code on the driver is something of the form&#160;spark-application-* ( probably from &lt;a href=&quot;https://github.com/apache/spark/blob/b19a28dea098c7d6188f8540429c50f42952d678/core/src/main/scala/org/apache/spark/SparkContext.scala#L511&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/b19a28dea098c7d6188f8540429c50f42952d678/core/src/main/scala/org/apache/spark/SparkContext.scala#L511&lt;/a&gt;&#160;&amp;amp;&#160;&lt;a href=&quot;https://github.com/apache/spark/blob/bfb74394a5513134ea1da9fcf4a1783b77dd64e4/core/src/main/scala/org/apache/spark/scheduler/SchedulerBackend.scala#L26)&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/bfb74394a5513134ea1da9fcf4a1783b77dd64e4/core/src/main/scala/org/apache/spark/scheduler/SchedulerBackend.scala#L26&lt;/a&gt;&#160;)&lt;/p&gt;

&lt;p&gt;We were consuming this&#160;&quot;spark-app-selector&quot; label on the Driver Pod to get the App Id and use it&#160;to look-up the app in SparkHistory server (among other use-cases). but due to this mis-match, this logic no longer works.&#160;This was working fine in Spark 2.2 fork for Kubernetes which i was using earlier.&#160;Is this expected behavior and if yes, what&apos;s the correct way to fetch the applicationId from outside the application ?&#160;&#160;&lt;/p&gt;

&lt;p&gt;Let me know if I can provide any more details or if I am doing something wrong. Here is an example run with different &lt;b&gt;spark-app-selector&lt;/b&gt; label on the driver/executor :&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Name: pyfiles-driver
Namespace: &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;
Priority: 0
PriorityClassName: &amp;lt;none&amp;gt;
Start Time: Thu, 01 Nov 2018 18:19:46 -0700
Labels: spark-app-selector=spark-b78bb10feebf4e2d98c11d7b6320e18f
 spark-role=driver
 sparkoperator.k8s.io/app-name=pyfiles
 sparkoperator.k8s.io/launched-by-spark-&lt;span class=&quot;code-keyword&quot;&gt;operator&lt;/span&gt;=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;
 version=2.4.0
Status: Running



Name: pyfiles-1541121585642-exec-1
Namespace: &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;
Priority: 0
PriorityClassName: &amp;lt;none&amp;gt;
Start Time: Thu, 01 Nov 2018 18:24:02 -0700
Labels: spark-app-selector=spark-application-1541121829445
 spark-exec-id=1
 spark-role=executor
 sparkoperator.k8s.io/app-name=pyfiles
 sparkoperator.k8s.io/launched-by-spark-&lt;span class=&quot;code-keyword&quot;&gt;operator&lt;/span&gt;=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;
 version=2.4.0
Status: Pending
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</description>
                <environment>&lt;p&gt;Spark 2.4.0 RC4&lt;/p&gt;</environment>
        <key id="13195782">SPARK-25922</key>
            <summary>[K8] Spark Driver/Executor &quot;spark-app-selector&quot; label mismatch</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="suxingfate">Wang, Xinglong</assignee>
                                    <reporter username="akhurana">Anmol Khurana</reporter>
                        <labels>
                    </labels>
                <created>Thu, 1 Nov 2018 23:41:50 +0000</created>
                <updated>Sun, 17 May 2020 18:26:53 +0000</updated>
                            <resolved>Fri, 15 Feb 2019 18:09:22 +0000</resolved>
                                    <version>2.4.0</version>
                                    <fixVersion>2.4.1</fixVersion>
                    <fixVersion>3.0.0</fixVersion>
                                    <component>Kubernetes</component>
                    <component>Spark Core</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="16677276" author="liyinan926" created="Tue, 6 Nov 2018 20:30:27 +0000"  >&lt;p&gt;The application ID&#160;used to set the &lt;tt&gt;spark-app-selector&lt;/tt&gt; label for the driver pod is from this line &lt;a href=&quot;https://github.com/apache/spark/blob/3404a73f4cf7be37e574026d08ad5cf82cfac871/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesClientApplication.scala#L217.&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/3404a73f4cf7be37e574026d08ad5cf82cfac871/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesClientApplication.scala#L217.&lt;/a&gt;&#160;The application ID used to set the &lt;tt&gt;spark-app-selector&lt;/tt&gt; label for the executor pod is from this line &lt;a href=&quot;https://github.com/apache/spark/blob/5264164a67df498b73facae207eda12ee133be7d/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterSchedulerBackend.scala#L87,&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/5264164a67df498b73facae207eda12ee133be7d/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterSchedulerBackend.scala#L87&lt;/a&gt;. Agreed that it&apos;s problematic that two different labels are used.&lt;/p&gt;</comment>
                            <comment id="16721346" author="githubbot" created="Fri, 14 Dec 2018 12:23:54 +0000"  >&lt;p&gt;suxingfate opened a new pull request #23322: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25922&quot; title=&quot;[K8] Spark Driver/Executor &amp;quot;spark-app-selector&amp;quot; label mismatch&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25922&quot;&gt;&lt;del&gt;SPARK-25922&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;K8&amp;#93;&lt;/span&gt; Spark Driver/Executor spark-app-selector label mism&#8230;&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/23322&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/23322&lt;/a&gt;&lt;/p&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;What changes were proposed in this pull request?&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;   In K8S Cluster mode, the algorithm to generate spark-app-selector/spark.app.id of spark driver is different with spark executor.&lt;br/&gt;
   This patch consolidated the algorithm for driver and executor to have a universal logic. This will help to monitor resource&lt;br/&gt;
   consumption from K8S perspective.&lt;br/&gt;
   In K8S Client mode, this makes sure it will also use the same algorithm to generate spark-app-selector/spark.app.id for executors.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;How was this patch tested?&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;   Manually run.&quot;&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16721349" author="suxingfate" created="Fri, 14 Dec 2018 12:28:09 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=liyinan926&quot; class=&quot;user-hover&quot; rel=&quot;liyinan926&quot;&gt;liyinan926&lt;/a&gt;,&lt;/p&gt;

&lt;p&gt;Recently while evaluating spark on k8s, We are setting up tools to monitor resource usage from k8s perspective and also met this problem.&#160;&lt;/p&gt;

&lt;p&gt;I created a pull request for this. Could you please help to review?&lt;/p&gt;

&lt;p&gt;Thanks,&lt;/p&gt;</comment>
                            <comment id="16723392" author="githubbot" created="Mon, 17 Dec 2018 21:40:45 +0000"  >&lt;p&gt;asfgit closed pull request #23322: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25922&quot; title=&quot;[K8] Spark Driver/Executor &amp;quot;spark-app-selector&amp;quot; label mismatch&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25922&quot;&gt;&lt;del&gt;SPARK-25922&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;K8&amp;#93;&lt;/span&gt; Spark Driver/Executor &quot;spark-app-selector&quot; label mismatch&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/23322&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/23322&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterSchedulerBackend.scala b/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterSchedulerBackend.scala&lt;br/&gt;
index 68f6f2e46e316..03f5da2bb0bce 100644&lt;br/&gt;
&amp;#8212; a/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterSchedulerBackend.scala&lt;br/&gt;
+++ b/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterSchedulerBackend.scala&lt;br/&gt;
@@ -18,9 +18,10 @@ package org.apache.spark.scheduler.cluster.k8s&lt;/p&gt;

&lt;p&gt; import java.util.concurrent.ExecutorService&lt;/p&gt;

&lt;p&gt;-import io.fabric8.kubernetes.client.KubernetesClient&lt;br/&gt;
 import scala.concurrent.&lt;/p&gt;
{ExecutionContext, Future}

&lt;p&gt;+import io.fabric8.kubernetes.client.KubernetesClient&lt;br/&gt;
+&lt;br/&gt;
 import org.apache.spark.SparkContext&lt;br/&gt;
 import org.apache.spark.deploy.k8s.Config._&lt;br/&gt;
 import org.apache.spark.deploy.k8s.Constants._&lt;br/&gt;
@@ -39,10 +40,10 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class KubernetesClusterSchedulerBackend(&lt;br/&gt;
     lifecycleEventHandler: ExecutorPodsLifecycleManager,&lt;br/&gt;
     watchEvents: ExecutorPodsWatchSnapshotSource,&lt;br/&gt;
     pollEvents: ExecutorPodsPollingSnapshotSource)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;extends CoarseGrainedSchedulerBackend(scheduler, sc.env.rpcEnv) {&lt;br/&gt;
+    extends CoarseGrainedSchedulerBackend(scheduler, sc.env.rpcEnv) {&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private implicit val requestExecutorContext = ExecutionContext.fromExecutorService(&lt;/li&gt;
	&lt;li&gt;requestExecutorsService)&lt;br/&gt;
+  private implicit val requestExecutorContext =&lt;br/&gt;
+    ExecutionContext.fromExecutorService(requestExecutorsService)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   protected override val minRegisteredRatio =&lt;br/&gt;
     if (conf.getOption(&quot;spark.scheduler.minRegisteredResourcesRatio&quot;).isEmpty) &lt;/p&gt;
{
@@ -60,6 +61,17 @@ private[spark] class KubernetesClusterSchedulerBackend(
     removeExecutor(executorId, reason)
   }

&lt;p&gt;+  /**&lt;br/&gt;
+   * Get an application ID associated with the job.&lt;br/&gt;
+   * This returns the string value of spark.app.id if set, otherwise&lt;br/&gt;
+   * the locally-generated ID from the superclass.&lt;br/&gt;
+   *&lt;br/&gt;
+   * @return The application ID&lt;br/&gt;
+   */&lt;br/&gt;
+  override def applicationId(): String = &lt;/p&gt;
{
+    conf.getOption(&quot;spark.app.id&quot;).map(_.toString).getOrElse(super.applicationId)
+  }
&lt;p&gt;+&lt;br/&gt;
   override def start(): Unit = {&lt;br/&gt;
     super.start()&lt;br/&gt;
     if (!Utils.isDynamicAllocationEnabled(conf)) {&lt;br/&gt;
@@ -88,7 +100,8 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class KubernetesClusterSchedulerBackend(&lt;/p&gt;

&lt;p&gt;     if (shouldDeleteExecutors) {&lt;br/&gt;
       Utils.tryLogNonFatalError &lt;/p&gt;
{
-        kubernetesClient.pods()
+        kubernetesClient
+          .pods()
           .withLabel(SPARK_APP_ID_LABEL, applicationId())
           .withLabel(SPARK_ROLE_LABEL, SPARK_POD_EXECUTOR_ROLE)
           .delete()
@@ -120,7 +133,8 @@ private[spark] class KubernetesClusterSchedulerBackend(
   }

&lt;p&gt;   override def doKillExecutors(executorIds: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;): Future&lt;span class=&quot;error&quot;&gt;&amp;#91;Boolean&amp;#93;&lt;/span&gt; = Future&lt;span class=&quot;error&quot;&gt;&amp;#91;Boolean&amp;#93;&lt;/span&gt; &lt;/p&gt;
{
-    kubernetesClient.pods()
+    kubernetesClient
+      .pods()
       .withLabel(SPARK_APP_ID_LABEL, applicationId())
       .withLabel(SPARK_ROLE_LABEL, SPARK_POD_EXECUTOR_ROLE)
       .withLabelIn(SPARK_EXECUTOR_ID_LABEL, executorIds: _*)
@@ -133,7 +147,7 @@ private[spark] class KubernetesClusterSchedulerBackend(
   }

&lt;p&gt;   private class KubernetesDriverEndpoint(rpcEnv: RpcEnv, sparkProperties: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;(String, String)&amp;#93;&lt;/span&gt;)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;extends DriverEndpoint(rpcEnv, sparkProperties) {&lt;br/&gt;
+      extends DriverEndpoint(rpcEnv, sparkProperties) {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     override def onDisconnected(rpcAddress: RpcAddress): Unit = {&lt;br/&gt;
       // Don&apos;t do anything besides disabling the executor - allow the Kubernetes API events to&lt;br/&gt;
diff --git a/resource-managers/kubernetes/core/src/test/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterSchedulerBackendSuite.scala b/resource-managers/kubernetes/core/src/test/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterSchedulerBackendSuite.scala&lt;br/&gt;
index 75232f7b98b04..6e182bed459f8 100644&lt;br/&gt;
&amp;#8212; a/resource-managers/kubernetes/core/src/test/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterSchedulerBackendSuite.scala&lt;br/&gt;
+++ b/resource-managers/kubernetes/core/src/test/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterSchedulerBackendSuite.scala&lt;br/&gt;
@@ -37,6 +37,7 @@ class KubernetesClusterSchedulerBackendSuite extends SparkFunSuite with BeforeAn&lt;br/&gt;
   private val requestExecutorsService = new DeterministicScheduler()&lt;br/&gt;
   private val sparkConf = new SparkConf(false)&lt;br/&gt;
     .set(&quot;spark.executor.instances&quot;, &quot;3&quot;)&lt;br/&gt;
+    .set(&quot;spark.app.id&quot;, TEST_SPARK_APP_ID)&lt;/p&gt;

&lt;p&gt;   @Mock&lt;br/&gt;
   private var sc: SparkContext = _&lt;br/&gt;
@@ -87,8 +88,10 @@ class KubernetesClusterSchedulerBackendSuite extends SparkFunSuite with BeforeAn&lt;br/&gt;
     when(sc.env).thenReturn(env)&lt;br/&gt;
     when(env.rpcEnv).thenReturn(rpcEnv)&lt;br/&gt;
     driverEndpoint = ArgumentCaptor.forClass(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;RpcEndpoint&amp;#93;&lt;/span&gt;)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;when(rpcEnv.setupEndpoint(&lt;/li&gt;
	&lt;li&gt;mockitoEq(CoarseGrainedSchedulerBackend.ENDPOINT_NAME), driverEndpoint.capture()))&lt;br/&gt;
+    when(&lt;br/&gt;
+      rpcEnv.setupEndpoint(&lt;br/&gt;
+        mockitoEq(CoarseGrainedSchedulerBackend.ENDPOINT_NAME),&lt;br/&gt;
+        driverEndpoint.capture()))&lt;br/&gt;
       .thenReturn(driverEndpointRef)&lt;br/&gt;
     when(kubernetesClient.pods()).thenReturn(podOperations)&lt;br/&gt;
     schedulerBackendUnderTest = new KubernetesClusterSchedulerBackend(&lt;br/&gt;
@@ -100,9 +103,7 @@ class KubernetesClusterSchedulerBackendSuite extends SparkFunSuite with BeforeAn&lt;br/&gt;
       podAllocator,&lt;br/&gt;
       lifecycleEventHandler,&lt;br/&gt;
       watchEvents,&lt;/li&gt;
	&lt;li&gt;pollEvents) 
{
-      override def applicationId(): String = TEST_SPARK_APP_ID
-    }
&lt;p&gt;+      pollEvents)&lt;br/&gt;
   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   test(&quot;Start all components&quot;) {&lt;br/&gt;
@@ -127,8 +128,7 @@ class KubernetesClusterSchedulerBackendSuite extends SparkFunSuite with BeforeAn&lt;/p&gt;

&lt;p&gt;   test(&quot;Remove executor&quot;) &lt;/p&gt;
{
     schedulerBackendUnderTest.start()
-    schedulerBackendUnderTest.doRemoveExecutor(
-      &quot;1&quot;, ExecutorKilled)
+    schedulerBackendUnderTest.doRemoveExecutor(&quot;1&quot;, ExecutorKilled)
     verify(driverEndpointRef).send(RemoveExecutor(&quot;1&quot;, ExecutorKilled))
   }





&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16769561" author="vanzin" created="Fri, 15 Feb 2019 18:09:22 +0000"  >&lt;p&gt;Issue resolved by pull request 23779&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/23779&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/23779&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 39 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|s001og:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>