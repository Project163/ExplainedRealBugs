<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:17:33 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-3039] Spark assembly for new hadoop API (hadoop 2) contains avro-mapred for hadoop 1 API</title>
                <link>https://issues.apache.org/jira/browse/SPARK-3039</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;The spark assembly contains the artifact &quot;org.apache.avro:avro-mapred&quot; as a dependency of &quot;org.spark-project.hive:hive-serde&quot;.&lt;/p&gt;

&lt;p&gt;The avro-mapred package provides a hadoop FileInputFormat to read and write avro files. There are two versions of this package, distinguished by a classifier. avro-mapred for the new Hadoop API uses the classifier &quot;hadoop2&quot;. avro-mapred for the old Hadoop API uses no classifier.&lt;/p&gt;

&lt;p&gt;E.g. when reading avro files using &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;sc.newAPIHadoopFile[AvroKey[SomeClass]],NullWritable,AvroKeyInputFormat[SomeClass]](&lt;span class=&quot;code-quote&quot;&gt;&quot;hdfs:&lt;span class=&quot;code-comment&quot;&gt;//path/to/file.avro&quot;&lt;/span&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following error occurs:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;java.lang.IncompatibleClassChangeError: Found &lt;span class=&quot;code-keyword&quot;&gt;interface&lt;/span&gt; org.apache.hadoop.mapreduce.TaskAttemptContext, but &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;was expected
        at org.apache.avro.mapreduce.AvroKeyInputFormat.createRecordReader(AvroKeyInputFormat.java:47)
        at org.apache.spark.rdd.NewHadoopRDD$$anon$1.&amp;lt;init&amp;gt;(NewHadoopRDD.scala:111)
        at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:99)
        at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:61)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
        at org.apache.spark.scheduler.Task.run(Task.scala:51)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:744)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This error usually is a hint that there was a mix up of the old and the new Hadoop API. As a work-around, if avro-mapred for hadoop2 is &quot;forced&quot; to appear before the version that is bundled with Spark, reading avro files works fine. &lt;/p&gt;

&lt;p&gt;Also, if Spark is built using avro-mapred for hadoop2, it works fine as well.&lt;/p&gt;


</description>
                <environment>&lt;p&gt;hadoop2, hadoop-2.4.0, HDP-2.1&lt;/p&gt;</environment>
        <key id="12734060">SPARK-3039</key>
            <summary>Spark assembly for new hadoop API (hadoop 2) contains avro-mapred for hadoop 1 API</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="bbossy">Bertrand Bossy</assignee>
                                    <reporter username="bbossy">Bertrand Bossy</reporter>
                        <labels>
                    </labels>
                <created>Thu, 14 Aug 2014 13:34:53 +0000</created>
                <updated>Sun, 8 Feb 2015 13:32:55 +0000</updated>
                            <resolved>Sun, 8 Feb 2015 10:36:02 +0000</resolved>
                                    <version>0.9.1</version>
                    <version>1.0.0</version>
                    <version>1.1.0</version>
                    <version>1.2.0</version>
                                    <fixVersion>1.3.0</fixVersion>
                                    <component>Build</component>
                    <component>Input/Output</component>
                    <component>Spark Core</component>
                        <due></due>
                            <votes>4</votes>
                                    <watches>8</watches>
                                                                                                                <comments>
                            <comment id="14096973" author="apachespark" created="Thu, 14 Aug 2014 13:57:23 +0000"  >&lt;p&gt;User &apos;bbossy&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/1945&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/1945&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14098398" author="bbossy" created="Fri, 15 Aug 2014 09:55:34 +0000"  >&lt;p&gt;Also need to update the README: See &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-3069&quot; title=&quot;Build instructions in README are outdated&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-3069&quot;&gt;&lt;del&gt;SPARK-3069&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://github.com/apache/spark/pull/1945&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/1945&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14133565" author="pwendell" created="Mon, 15 Sep 2014 04:12:42 +0000"  >&lt;p&gt;Resolved by:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/1945&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/1945&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14209486" author="bbossy" created="Thu, 13 Nov 2014 08:23:38 +0000"  >&lt;p&gt;Needs more fixes, since although I got the build to work for hadoop-2.4, yarn and hive with the sbt build. It doesn&apos;t work with the maven build, which AFAIK is required for pySpark. Dependency management seems to be quite different in sbt and maven&lt;/p&gt;</comment>
                            <comment id="14237648" author="derrickburns" created="Mon, 8 Dec 2014 09:09:46 +0000"  >&lt;p&gt;I get the same bug when attempting to save a RDD as a parquet file when using Hadoop 1.0.4&lt;/p&gt;</comment>
                            <comment id="14237880" author="bbossy" created="Mon, 8 Dec 2014 14:08:56 +0000"  >&lt;p&gt;@&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=derrickburns&quot; class=&quot;user-hover&quot; rel=&quot;derrickburns&quot;&gt;derrickburns&lt;/a&gt;: Can you post some more info, such as spark version/distribution used, etc..? Spark&apos;s build system has received some updates. I still have to verify this, but AFAIK, these kind of issues should not be present in future releases.&lt;/p&gt;

&lt;p&gt;Also: have a careful look at the stack trace:&lt;br/&gt;
if you have &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Found &lt;span class=&quot;code-keyword&quot;&gt;interface&lt;/span&gt; org.apache.hadoop.mapreduce.TaskAttemptContext, but &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;was expected&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt; then avro-mapred for hadoop1 was found, but avro-mapred for hadoop2 was expected. However, if you have&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Found &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.hadoop.mapreduce.TaskAttemptContext, but &lt;span class=&quot;code-keyword&quot;&gt;interface&lt;/span&gt; was expected&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt; or something similar, it&apos;s the other way round&lt;/p&gt;</comment>
                            <comment id="14238197" author="derrickburns" created="Mon, 8 Dec 2014 18:23:52 +0000"  >&lt;p&gt;Running in local mode. Spark 1.1.1/Hadoop 1.0.4 built using maven with:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-xml&quot;&gt;      &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;dependency&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;groupId&amp;gt;&lt;/span&gt;org.apache.spark&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/groupId&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;artifactId&amp;gt;&lt;/span&gt;spark-core_2.10&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/artifactId&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;version&amp;gt;&lt;/span&gt;1.1.1&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/version&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/dependency&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;dependency&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;groupId&amp;gt;&lt;/span&gt;org.apache.spark&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/groupId&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;artifactId&amp;gt;&lt;/span&gt;spark-sql_2.10&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/artifactId&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;version&amp;gt;&lt;/span&gt;1.1.1&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/version&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/dependency&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;dependency&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;groupId&amp;gt;&lt;/span&gt;org.apache.spark&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/groupId&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;artifactId&amp;gt;&lt;/span&gt;spark-mllib_2.10&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/artifactId&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;version&amp;gt;&lt;/span&gt;1.1.1&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/version&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/dependency&amp;gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The code is rather trivial:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    val sc = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; SparkContext(sparkConf)
    val sqlContext = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; org.apache.spark.sql.SQLContext(sc)
    val tweets = sqlContext.jsonFile(path).cache()
    tweets.saveAsParquetFile(&lt;span class=&quot;code-quote&quot;&gt;&quot;tweets.parquet&quot;&lt;/span&gt;)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here is the stack trace:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;java.lang.IncompatibleClassChangeError: Found &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.hadoop.mapreduce.TaskAttemptContext, but &lt;span class=&quot;code-keyword&quot;&gt;interface&lt;/span&gt; was expected
	at org.apache.spark.sql.parquet.AppendingParquetOutputFormat.getDefaultWorkFile(ParquetTableOperations.scala:334)
	at parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:251)
	at org.apache.spark.sql.parquet.InsertIntoParquetTable.org$apache$spark$sql$parquet$InsertIntoParquetTable$$writeShard$1(ParquetTableOperations.scala:300)
	at org.apache.spark.sql.parquet.InsertIntoParquetTable$$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:318)
	at org.apache.spark.sql.parquet.InsertIntoParquetTable$$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:318)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
	at org.apache.spark.scheduler.Task.run(Task.scala:54)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:744)
14/12/08 10:21:06 ERROR executor.ExecutorUncaughtExceptionHandler: Uncaught exception in thread &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;[Executor task launch worker-0,5,main]
java.lang.IncompatibleClassChangeError: Found &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.hadoop.mapreduce.TaskAttemptContext, but &lt;span class=&quot;code-keyword&quot;&gt;interface&lt;/span&gt; was expected
	at org.apache.spark.sql.parquet.AppendingParquetOutputFormat.getDefaultWorkFile(ParquetTableOperations.scala:334)
	at parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:251)
	at org.apache.spark.sql.parquet.InsertIntoParquetTable.org$apache$spark$sql$parquet$InsertIntoParquetTable$$writeShard$1(ParquetTableOperations.scala:300)
	at org.apache.spark.sql.parquet.InsertIntoParquetTable$$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:318)
	at org.apache.spark.sql.parquet.InsertIntoParquetTable$$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:318)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
	at org.apache.spark.scheduler.Task.run(Task.scala:54)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:744)

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="14259306" author="pwendell" created="Sat, 27 Dec 2014 07:53:27 +0000"  >&lt;p&gt;This fix did appear in Spark 1.2.0 so I&apos;m closing this issue.&lt;/p&gt;</comment>
                            <comment id="14301780" author="medale" created="Mon, 2 Feb 2015 19:50:52 +0000"  >&lt;p&gt;For me, Spark 1.2.0 either downloading spark-1.2.0-bin-hadoop2.4.tgz or&lt;br/&gt;
compiling the source with &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;mvn -Pyarn -Phadoop-2.4 -Phive-0.13.1 -DskipTests clean &lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;still had the same problem:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;java.lang.IncompatibleClassChangeError: Found interface org.apache.hadoop.mapreduce.TaskAttemptContext, but class was expected
	at org.apache.avro.mapreduce.AvroRecordReaderBase.initialize(AvroRecordReaderBase.java:87)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.&amp;lt;init&amp;gt;(NewHadoopRDD.scala:135)

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Starting the build with a clean .m2/repository, the repository afterwards contained:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;avro-mapred/1.7.5 (with the default jar - i.e. hadoop1)&lt;/li&gt;
	&lt;li&gt;avro-mapred/1.7.6 with the avro-mapred-1.7.6-hadoop2.jar (the one we want).&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Seemed that sharding these two dependencies into the spark-assembly-jar resulted in the error above&lt;br/&gt;
at least in the downloaded hadoop2.4 spark bin and my own build.&lt;/p&gt;

&lt;p&gt;Running the following (after doing a mvn install and by-hand copy of all the spark artifacts into my local repo for spark-repl/yarn):&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt; mvn -Pyarn -Phadoop-2.4 -Phive -DskipTests dependency:tree -Dincludes=org.apache.avro:avro-mapred
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Showed that the culprit was in the Hive project, namely org.spark-project.hive:hive-exec&apos;s&lt;br/&gt;
dependency on 1.7.5.&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Building Spark Project Hive 1.2.0
[INFO] ------------------------------------------------------------------------
[INFO]
[INFO] --- maven-dependency-plugin:2.4:tree (default-cli) @ spark-hive_2.10 ---
[INFO] org.apache.spark:spark-hive_2.10:jar:1.2.0
[INFO] +- org.spark-project.hive:hive-exec:jar:0.13.1a:compile
[INFO] |  \- org.apache.avro:avro-mapred:jar:1.7.5:compile
[INFO] \- org.apache.avro:avro-mapred:jar:hadoop2:1.7.6:compile
[INFO]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Editing spark-1.2.0/sql/hive/pom.xml and excluding avro-mapred from hive-exec,&lt;br/&gt;
then recompile, fixed the problem and the resulting dist works well against&lt;br/&gt;
Avro/Hadoop2 code:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-xml&quot;&gt;    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;dependency&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;groupId&amp;gt;&lt;/span&gt;org.spark-project.hive&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/groupId&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;artifactId&amp;gt;&lt;/span&gt;hive-exec&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/artifactId&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;version&amp;gt;&lt;/span&gt;${hive.version}&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/version&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;exclusions&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;exclusion&amp;gt;&lt;/span&gt;
          &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;groupId&amp;gt;&lt;/span&gt;commons-logging&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/groupId&amp;gt;&lt;/span&gt;
          &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;artifactId&amp;gt;&lt;/span&gt;commons-logging&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/artifactId&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/exclusion&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;exclusion&amp;gt;&lt;/span&gt;
          &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;groupId&amp;gt;&lt;/span&gt;com.esotericsoftware.kryo&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/groupId&amp;gt;&lt;/span&gt;
          &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;artifactId&amp;gt;&lt;/span&gt;kryo&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/artifactId&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/exclusion&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;exclusion&amp;gt;&lt;/span&gt;
          &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;groupId&amp;gt;&lt;/span&gt;org.apache.avro&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/groupId&amp;gt;&lt;/span&gt;
          &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;artifactId&amp;gt;&lt;/span&gt;avro-mapred&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/artifactId&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/exclusion&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/exclusions&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/dependency&amp;gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Just the last exclusion added. For version 1.2.1-rc2 the change is here: &lt;a href=&quot;https://github.com/medale/spark/compare/apache:v1.2.1-rc2...medale:avro-hadoop2-v1.2.1-rc2&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/medale/spark/compare/apache:v1.2.1-rc2...medale:avro-hadoop2-v1.2.1-rc2&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14302027" author="apachespark" created="Mon, 2 Feb 2015 22:05:38 +0000"  >&lt;p&gt;User &apos;medale&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/4315&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/4315&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14302716" author="srowen" created="Tue, 3 Feb 2015 03:21:12 +0000"  >&lt;p&gt;Sounds like there&apos;s a lingering version of this issue from the Hive dependency.&lt;/p&gt;</comment>
                            <comment id="14304971" author="bbossy" created="Wed, 4 Feb 2015 11:22:56 +0000"  >&lt;p&gt;hmm.. Is there a way to test the contents of the assembly jar, or what jars get packaged? I fear that this will come up again..&lt;/p&gt;</comment>
                            <comment id="14304986" author="srowen" created="Wed, 4 Feb 2015 11:42:37 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bbossy&quot; class=&quot;user-hover&quot; rel=&quot;bbossy&quot;&gt;bbossy&lt;/a&gt; See the good analysis and additional fix in the PR above: &lt;a href=&quot;https://github.com/apache/spark/pull/4315&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/4315&lt;/a&gt;  &lt;/p&gt;</comment>
                            <comment id="14304991" author="medale" created="Wed, 4 Feb 2015 11:49:01 +0000"  >&lt;p&gt;The Aapche Maven Enforcer plugin has a dependency convergence rule that could be added to ensure that dependencies/transitive dependencies don&apos;t clash. Maybe this could be added to a &quot;deploy&quot; profile to be checked (initially set fail to false until all dependency convergence errors are fixed) during release builds initially. See &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-5584&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/SPARK-5584&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For the spark-1.3.0-SNAPSHOT, looks like the fix for &quot;&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-4048&quot; title=&quot;Enhance and extend hadoop-provided profile&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-4048&quot;&gt;&lt;del&gt;SPARK-4048&lt;/del&gt;&lt;/a&gt; Enhance and extend hadoop-provided profile&quot; introduced a new scope attribute in the Maven build:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;grep -R hive.deps.scope *
assembly/pom.xml: &amp;lt;hive.deps.scope&amp;gt;provided&amp;lt;/hive.deps.scope&amp;gt;
examples/pom.xml: &amp;lt;hive.deps.scope&amp;gt;provided&amp;lt;/hive.deps.scope&amp;gt;
pom.xml:    &amp;lt;hive.deps.scope&amp;gt;compile&amp;lt;/hive.deps.scope&amp;gt;
pom.xml:        &amp;lt;scope&amp;gt;${hive.deps.scope}&amp;lt;/scope&amp;gt;
pom.xml:        &amp;lt;scope&amp;gt;${hive.deps.scope}&amp;lt;/scope&amp;gt;
pom.xml:        &amp;lt;scope&amp;gt;${hive.deps.scope}&amp;lt;/scope&amp;gt;
pom.xml:        &amp;lt;scope&amp;gt;${hive.deps.scope}&amp;lt;/scope&amp;gt;
pom.xml:        &amp;lt;scope&amp;gt;${hive.deps.scope}&amp;lt;/scope&amp;gt;
pom.xml:        &amp;lt;scope&amp;gt;${hive.deps.scope}&amp;lt;/scope&amp;gt;
pom.xml:        &amp;lt;scope&amp;gt;${hive.deps.scope}&amp;lt;/scope&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;avro-mapred and hive-exec are marked with that scope so neither library nor their dependencies will be included in the spark assembly jar. This means that Spark jobs that want to use Avro-based InputFormat from avro-mapred have to include their desired avro-mapred version in their jars. So this particular problem will be gone but still need to prevent this class of problems by ensuring dependency convergence.&lt;/p&gt;</comment>
                            <comment id="14304993" author="srowen" created="Wed, 4 Feb 2015 11:53:56 +0000"  >&lt;p&gt;I think Spark&apos;s dependency tree is already well too complex to actually avoid conflicts. It doesn&apos;t converge, but manages to work. This is indeed a big source of complexity and problems. I&apos;d love to whittle down the extent and number of permutations to support but this is a story for a little later.&lt;/p&gt;</comment>
                            <comment id="14311262" author="srowen" created="Sun, 8 Feb 2015 10:36:02 +0000"  >&lt;p&gt;Issue resolved by pull request 4315&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/4315&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/4315&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="12748503">SPARK-3965</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>412088</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 41 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1yw5r:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>412078</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12327642">1.3.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>