<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:12:38 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-1126] spark-submit script for running compiled binaries</title>
                <link>https://issues.apache.org/jira/browse/SPARK-1126</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;It would be useful to have a script, roughly similar to the &quot;hadoop jar&quot; command that is used for running compiled binaries against Spark.&lt;/p&gt;

&lt;p&gt;The script would do two things:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Set up the Spark classpath on the client side, so that users don&apos;t need to know where Spark jars are installed or bundle all of Spark inside their app jar.&lt;/li&gt;
	&lt;li&gt;Provide a layer over the different modes that apps can be run, so that the same spark-jar invocation could run the driver inside a YARN application master or in the client process, depending on the cluster setup.&lt;/li&gt;
&lt;/ul&gt;
</description>
                <environment></environment>
        <key id="12704526">SPARK-1126</key>
            <summary>spark-submit script for running compiled binaries</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="sandyr">Sandy Ryza</assignee>
                                    <reporter username="sandyryza">Sandy Ryza</reporter>
                        <labels>
                    </labels>
                <created>Mon, 24 Feb 2014 00:42:15 +0000</created>
                <updated>Sun, 30 Mar 2014 05:40:55 +0000</updated>
                            <resolved>Sun, 30 Mar 2014 05:40:55 +0000</resolved>
                                    <version>0.9.0</version>
                                    <fixVersion>1.0.0</fixVersion>
                                    <component>Spark Core</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>8</watches>
                                                                                                                <comments>
                            <comment id="13953834" author="matei" created="Mon, 24 Feb 2014 15:52:55 +0000"  >&lt;p&gt;Just as another requirement, we might want the same script to work for Python. It can be called spark-app or spark-submit or something.&lt;/p&gt;</comment>
                            <comment id="13953844" author="sandy" created="Mon, 24 Feb 2014 17:40:46 +0000"  >&lt;p&gt;That makes sense to me. So I&apos;m thinking:&lt;/p&gt;

&lt;p&gt;spark-app &amp;lt;app jar&amp;gt; &amp;lt;main class&amp;gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;lt;args&amp;gt;&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;where args are:&lt;br/&gt;
worker-memory - Memory requested from scheduler per executor.&lt;br/&gt;
worker-cores - Cores requested from scheduler per executor.&lt;br/&gt;
num-workers - Number of executors.&lt;br/&gt;
master-memory - Memory requested from scheduler for driver.  Only applies on yarn-standalone mode.&lt;br/&gt;
master-cores - Memory requested from scheduler for driver.  Only applies on yarn-standalone mode.&lt;br/&gt;
master-max-heap - Max heap size for the driver JVM.&lt;br/&gt;
deploy-mode - yarn-client, yarn-standalone, standalone-standalone, or standalone-client (could maybe use better names here, the confusing thing is that &quot;standalone&quot; refers to both a cluster manager and a deploy mode)&lt;br/&gt;
supervise - Whether to automatically restart driver on failure.  Only works on standalone-standalone mode, though we should be able to add support for this in yarn-standalone as well.&lt;br/&gt;
add-jars - Additional jars that should be on the driver and executor classpaths.&lt;br/&gt;
files - Files to place next to all executors.  Only works in yarn-standalone and yarn-client mode.&lt;br/&gt;
archives - Archives to extract next to all executors.  Only works in yarn-standalone and yarn-client mode.&lt;br/&gt;
queue - Queue/pool to submit the application to.  Only works in yarn-standalone and yarn-client mode.&lt;br/&gt;
args - Arguments to pass to the driver.&lt;/p&gt;

&lt;p&gt;It would be nice for deploy-mode to be settable by an environment variable.  Passing the option would override it.  This would allow cluster operators to set up a default deploy mode for their cluster and not require users to think about it.  A user could specify a particular deploy mode if it matters to them.&lt;/p&gt;

&lt;p&gt;Because many of these don&apos;t apply to every mode, it&apos;s also worth considering some sort of mechanism for delegating options down to particular modes.  But I think this might be hairier and not add much.&lt;/p&gt;</comment>
                            <comment id="13953850" author="matei" created="Tue, 25 Feb 2014 23:29:55 +0000"  >&lt;p&gt;Hey Sandy, a few comments here:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;As I mentioned above, we probably want this to work for Python too in a complete design. In that case you&apos;d probably take either a .py file or a .egg.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;I think deploy-mode should be separated into two pieces: a cluster URL and a flag for whether to run the driver on the cluster versus locally. Our cluster URL system already supports specifying different types of clusters, so it would be ideal to reuse it here.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Supported deploy modes should include Mesos (maybe you don&apos;t allow running the driver in Mesos right now, but it can certainly be outside as a client).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Some of these flags overlap with settings you can put in your SparkConf object today, such as worker memory, cluster URL (if that becomes a flag) and to some extent JARs. How will these be passed through? One option is to set the Java system properties for them when you execute the user&apos;s app (e.g. -Dspark.executor.memory=2g), which are going to populate the Conf by default unless the user overwrites them in their program. I don&apos;t know how much of an issue the latter will be but we can create a workaround if it is.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;The master-memory and master-cores should really be called driver-memory and driver-cores, and they apply to in-cluster submission on the standalone mode too.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;If we want this to become the standard, the script should also work on Windows. This makes it considerably hairier, to the point where we might want this to be a Scala class, though in that case the JVM startup overhead for launching it is kind of painful.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Anyway, I do think this would be a great feature to have, but before you implement it, it would be great to see a more detailed design that takes into account these points. In particular the main thing I&apos;m worried about is creating inconsistency across languages, operating systems or deployment modes. If we make this change and update all the docs on deploying applications and such, it would be nice to only have to make it once.&lt;/p&gt;</comment>
                            <comment id="13953858" author="sandy" created="Wed, 26 Feb 2014 11:05:19 +0000"  >&lt;p&gt;Thanks for taking a look Matei.  I attached a design doc with an amended version of what I posted above.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Our cluster URL system already supports specifying different types of clusters, so it would be ideal to reuse it here.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;In the YARN case, the cluster URL encapsulates both the cluster manager and the deploy mode (client vs. standalone), while in the standalone case, it includes only the former.  The design splits these apart for spark-app&apos;s arguments, but I wanted to highlight this.&lt;/p&gt;

&lt;p&gt;Another thing I wanted to ask about was memory configuraton.  In Hadoop, the memory requested as a cap from the cluster manager is controlled separately from the memory given as max heap opts to the JVM.  While this is clunky for a lot of reasons, an advantage is that it allows accounting for a process using memory off-heap, either through direct buffers or by forking a subprocess.  If Spark wants to handle these situations, it might make sense to eventually make the amount of padding between the JVM heap and requested memory configurable?&lt;/p&gt;

&lt;p&gt;Working on non-Linux platforms will be very difficult if the script is written in bash.  Even across Linux and Macs, there appears to be no shared utility for parsing long-form options.  A python script could avoid the JVM startup overhead.  I suppose this adds a python dependency, but most platforms now include python by default.  Scala also sounds reasonable to me if that makes the most sense to you.&lt;/p&gt;</comment>
                            <comment id="13953838" author="patrick" created="Wed, 26 Feb 2014 14:01:09 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sandy&quot; class=&quot;user-hover&quot; rel=&quot;sandy&quot;&gt;sandy&lt;/a&gt; Hey Sandy - is it not possible to just manually parse the options in bash rather than use a library? Second, what are the semantics of addJars, is this going to add a jar where the driver program is (and then jars are distributed through the normal path through Spark)? Just want to be clear because there is also addJar inside of Spark context... &lt;/p&gt;</comment>
                            <comment id="13953859" author="sandy" created="Wed, 26 Feb 2014 15:12:47 +0000"  >&lt;p&gt;I suppose bash is turing complete, but it would be very painful and error-prone to use it for this.  It also wouldn&apos;t solve the Windows issue.&lt;/p&gt;

&lt;p&gt;yarn-standalone mode supports an addJars parameter, which will place local jars on the cluster and add them to the YARN distributed cache so that they can be localized for containers.  I misunderstood and thought that other deploy modes had a similar way of specifying jars to add via command line.  So we can just document that it only works for yarn-standalone mode.  To promote consistency, it could also make sense to add environment variables that would allow other deploy modes to add jars outside of code.  We could also possibly reform the functionality in yarn-standalone mode somehow.&lt;/p&gt;</comment>
                            <comment id="13953860" author="matei" created="Wed, 26 Feb 2014 16:14:41 +0000"  >&lt;p&gt;Python isn&apos;t available by default on Windows, so we probably can&apos;t use that.&lt;/p&gt;

&lt;p&gt;Regarding the cluster URL, it&apos;s okay if we change URL formats slightly as part of this feature so that you use &quot;yarn&quot; for both in-cluster and out-of-cluster clients, but you have a separate flag for &quot;run the client in the cluster&quot;. I don&apos;t think we should combine these two properties (what type of cluster is it and do I want the driver inside) into one flag, because you just end up with flags that are all possible combinations of the two features.&lt;/p&gt;

&lt;p&gt;Regarding adding JARs, I believe driver submission in the standalone cluster could also be extended to take multiple JARs. It&apos;s okay if the option is only for YARN at first but it will be very confusing for users if they have to submit one way to YARN and another way to standalone clusters, so I&apos;d look into adding that.&lt;/p&gt;</comment>
                            <comment id="13953861" author="sandy" created="Wed, 26 Feb 2014 17:36:17 +0000"  >&lt;blockquote&gt;&lt;p&gt;Regarding the cluster URL, it&apos;s okay if we change URL formats slightly as part of this feature&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;By this, do you mean changing the yarn URL formats in existing code or just as interpreted by the spark-app script?  I.e. should this still work: &quot;MASTER=yarn-client ./bin/spark-shell&quot;?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Regarding adding JARs, I believe driver submission in the standalone cluster could also be extended to take multiple JARs.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Cool, I filed &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-1142&quot; title=&quot;Allow adding jars on app submission, outside of code&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-1142&quot;&gt;&lt;del&gt;SPARK-1142&lt;/del&gt;&lt;/a&gt; for this work.&lt;/p&gt;</comment>
                            <comment id="13953887" author="sandy" created="Fri, 28 Feb 2014 16:02:04 +0000"  >&lt;p&gt;Would it be best to just go with Scala?  In non-python cases, we can avoid extra JVM startup time by running the user class in-process instead of forking a new JVM.&lt;/p&gt;</comment>
                            <comment id="13953920" author="matei" created="Sun, 2 Mar 2014 20:23:46 +0000"  >&lt;p&gt;Yes, I think it&apos;s fine to do it in Scala.&lt;/p&gt;</comment>
                            <comment id="13953989" author="berngp" created="Thu, 6 Mar 2014 14:06:15 +0000"  >&lt;p&gt;Is there any feature-branch that is covering this work. Will like to contribute.&lt;/p&gt;</comment>
                            <comment id="13953988" author="sandy" created="Thu, 6 Mar 2014 14:07:35 +0000"  >&lt;p&gt;Here&apos;s the pull request: &lt;a href=&quot;https://github.com/apache/spark/pull/86&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/86&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13954002" author="berngp" created="Thu, 6 Mar 2014 17:13:34 +0000"  >&lt;p&gt;Thanks, will look into it too.&lt;/p&gt;</comment>
                            <comment id="13954453" author="githubbot" created="Sat, 29 Mar 2014 21:30:06 +0000"  >&lt;p&gt;Github user pwendell commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/spark/pull/86#discussion_r11095306&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/86#discussion_r11095306&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: docs/cluster-overview.md &amp;#8212;&lt;br/&gt;
    @@ -50,6 +50,47 @@ The system currently supports three cluster managers:&lt;br/&gt;
     In addition, Spark&apos;s &lt;span class=&quot;error&quot;&gt;&amp;#91;EC2 launch scripts&amp;#93;&lt;/span&gt;(ec2-scripts.html) make it easy to launch a standalone&lt;br/&gt;
     cluster on Amazon EC2.&lt;/p&gt;

&lt;p&gt;    +# Launching Applications&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Alright let&apos;s punt this to a broader doc clean-up for 1.0 which we can do during the QA phase. I think that ideally yes, we should replace all of the mentions of the other clients with this.&lt;/p&gt;</comment>
                            <comment id="13954464" author="githubbot" created="Sat, 29 Mar 2014 21:41:02 +0000"  >&lt;p&gt;Github user pwendell commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/spark/pull/86#issuecomment-39009977&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/86#issuecomment-39009977&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Hey @sryza I&apos;m going to submit a PR with some suggested follow-on changes, but I think we can go ahead and merge this for now as a starting point. Thanks for your work on this!&lt;/p&gt;</comment>
                            <comment id="13954477" author="githubbot" created="Sat, 29 Mar 2014 22:31:41 +0000"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/spark/pull/86&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/86&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12704881">SPARK-1318</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>382784</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            11 years, 34 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1ty3j:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>383052</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>