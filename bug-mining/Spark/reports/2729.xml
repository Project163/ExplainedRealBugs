<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:34:47 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-11447] Null comparison requires type information but type extraction fails for complex types</title>
                <link>https://issues.apache.org/jira/browse/SPARK-11447</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;While comparing a Column to a null literal, comparison works only if type of null literal matches type of the Column it&apos;s being compared to. Example scala code (can be run from spark shell):&lt;/p&gt;


&lt;p&gt;import org.apache.spark.sql._&lt;br/&gt;
import org.apache.spark.sql.types._&lt;br/&gt;
import org.apache.spark.sql.catalyst.expressions._&lt;/p&gt;

&lt;p&gt;val inputRowsData = Seq(Seq(&quot;abc&quot;),Seq(null),Seq(&quot;xyz&quot;))&lt;br/&gt;
val inputRows = for(seq &amp;lt;- inputRowsData) yield Row.fromSeq(seq)&lt;br/&gt;
val dfSchema = StructType(Seq(StructField(&quot;column&quot;, StringType, true)))&lt;br/&gt;
val df = sqlContext.createDataFrame(sc.makeRDD(inputRows), dfSchema)&lt;/p&gt;

&lt;p&gt;//DOESN&apos;T WORK&lt;br/&gt;
val filteredDF = df.filter(df(&quot;column&quot;) &amp;lt;=&amp;gt; (new Column(Literal(null))))&lt;/p&gt;

&lt;p&gt;//WORKS&lt;br/&gt;
val filteredDF = df.filter(df(&quot;column&quot;) &amp;lt;=&amp;gt; (new Column(Literal.create(null, SparkleFunctions.dataType(df(&quot;column&quot;))))))&lt;/p&gt;


&lt;p&gt;Why should type information be required for a null comparison? If it&apos;s required, it&apos;s not always possible to extract type information from complex  types (e.g. StructType). Following scala code (can be run from spark shell), throws org.apache.spark.sql.catalyst.analysis.UnresolvedException:&lt;/p&gt;


&lt;p&gt;import org.apache.spark.sql._&lt;br/&gt;
import org.apache.spark.sql.types._&lt;br/&gt;
import org.apache.spark.sql.catalyst.expressions._&lt;/p&gt;

&lt;p&gt;val inputRowsData = Seq(Seq(Row.fromSeq(Seq(&quot;abc&quot;, &quot;def&quot;))),Seq(Row.fromSeq(Seq(null, &quot;123&quot;))),Seq(Row.fromSeq(Seq(&quot;ghi&quot;, &quot;jkl&quot;))))&lt;br/&gt;
val inputRows = for(seq &amp;lt;- inputRowsData) yield Row.fromSeq(seq)&lt;br/&gt;
val dfSchema = StructType(Seq(StructField(&quot;column&quot;, StructType(Seq(StructField(&quot;p1&quot;, StringType, true), StructField(&quot;p2&quot;, StringType, true))), true)))&lt;/p&gt;

&lt;p&gt;val filteredDF = df.filter(df(&quot;column&quot;)(&quot;p1&quot;) &amp;lt;=&amp;gt; (new Column(Literal.create(null, SparkleFunctions.dataType(df(&quot;column&quot;)(&quot;p1&quot;))))))&lt;/p&gt;

&lt;p&gt;org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to dataType on unresolved object, tree: column#0&lt;span class=&quot;error&quot;&gt;&amp;#91;p1&amp;#93;&lt;/span&gt;&lt;br/&gt;
	at org.apache.spark.sql.catalyst.analysis.UnresolvedExtractValue.dataType(unresolved.scala:243)&lt;br/&gt;
	at org.apache.spark.sql.ArithmeticFunctions$class.dataType(ArithmeticFunctions.scala:76)&lt;br/&gt;
	at org.apache.spark.sql.SparkleFunctions$.dataType(SparkleFunctions.scala:14)&lt;br/&gt;
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:38)&lt;br/&gt;
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:43)&lt;br/&gt;
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:45)&lt;br/&gt;
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:47)&lt;br/&gt;
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:49)&lt;br/&gt;
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:51)&lt;br/&gt;
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:53)&lt;br/&gt;
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:55)&lt;br/&gt;
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:57)&lt;br/&gt;
	at $iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:59)&lt;br/&gt;
	at $iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:61)&lt;br/&gt;
	at $iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:63)&lt;br/&gt;
	at $iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:65)&lt;br/&gt;
	at $iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:67)&lt;br/&gt;
	at &amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:69)&lt;br/&gt;
	at .&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:73)&lt;br/&gt;
	at .&amp;lt;clinit&amp;gt;(&amp;lt;console&amp;gt;)&lt;br/&gt;
	at .&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:7)&lt;br/&gt;
	at .&amp;lt;clinit&amp;gt;(&amp;lt;console&amp;gt;)&lt;br/&gt;
	at $print(&amp;lt;console&amp;gt;)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:606)&lt;br/&gt;
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)&lt;br/&gt;
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1340)&lt;br/&gt;
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)&lt;br/&gt;
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)&lt;br/&gt;
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)&lt;br/&gt;
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:857)&lt;br/&gt;
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:902)&lt;br/&gt;
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:814)&lt;br/&gt;
	at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:657)&lt;br/&gt;
	at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:665)&lt;br/&gt;
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$loop(SparkILoop.scala:670)&lt;br/&gt;
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:997)&lt;br/&gt;
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)&lt;br/&gt;
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)&lt;br/&gt;
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)&lt;br/&gt;
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:945)&lt;br/&gt;
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1059)&lt;br/&gt;
	at org.apache.spark.repl.Main$.main(Main.scala:31)&lt;br/&gt;
	at org.apache.spark.repl.Main.main(Main.scala)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:606)&lt;br/&gt;
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)&lt;br/&gt;
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)&lt;br/&gt;
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)&lt;br/&gt;
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)&lt;br/&gt;
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)&lt;/p&gt;</description>
                <environment></environment>
        <key id="12909543">SPARK-11447</key>
            <summary>Null comparison requires type information but type extraction fails for complex types</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="kevinyu98">kevin yu</assignee>
                                    <reporter username="kapilsingh5050">Kapil Singh</reporter>
                        <labels>
                    </labels>
                <created>Mon, 2 Nov 2015 03:38:27 +0000</created>
                <updated>Tue, 17 Nov 2015 06:55:33 +0000</updated>
                            <resolved>Tue, 17 Nov 2015 06:55:33 +0000</resolved>
                                    <version>1.5.1</version>
                                    <fixVersion>1.6.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="14985569" author="kevinyu98" created="Mon, 2 Nov 2015 17:21:08 +0000"  >&lt;p&gt;Hello Kapil:&lt;/p&gt;

&lt;p&gt;When you say Doesn&apos;t work, does it mean that you got exception? &lt;/p&gt;

&lt;p&gt;I tried spark 1.5 , and this scala code works for me. &lt;br/&gt;
Can you verify which spark version you are running? I saw you put 1.5.1 there. &lt;/p&gt;

&lt;p&gt;//DOESN&apos;T WORK&lt;br/&gt;
val filteredDF = df.filter(df(&quot;column&quot;) &amp;lt;=&amp;gt; (new Column(Literal(null))))&lt;/p&gt;

&lt;p&gt;I run this on my spark shell at the latest version&lt;/p&gt;

&lt;p&gt;scala&amp;gt; val filteredDF = df.filter(df(&quot;column&quot;) &amp;lt;=&amp;gt; (new Column(Literal(null))))&lt;br/&gt;
filteredDF: org.apache.spark.sql.DataFrame = &lt;span class=&quot;error&quot;&gt;&amp;#91;column: string&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;thanks.&lt;/p&gt;
</comment>
                            <comment id="14986552" author="kapilsingh5050" created="Tue, 3 Nov 2015 02:49:19 +0000"  >&lt;p&gt;Hi Kevin,&lt;/p&gt;

&lt;p&gt;My bad, should have been more explicit. The code runs but the result is incorrect:&lt;/p&gt;

&lt;p&gt;val filteredDF = df.filter(df(&quot;column&quot;) &amp;lt;=&amp;gt; (new Column(Literal(null))))&lt;br/&gt;
filteredDF.show&lt;/p&gt;

&lt;p&gt;gives:&lt;/p&gt;

&lt;p&gt;&lt;ins&gt;------&lt;/ins&gt;&lt;/p&gt;
&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;column&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;&lt;ins&gt;------&lt;/ins&gt;&lt;/p&gt;
&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;   abc&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  null&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;   xyz&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;&lt;ins&gt;------&lt;/ins&gt;&lt;/p&gt;

&lt;p&gt;which is not correct. The result should include only the row with null value.&lt;/p&gt;

&lt;p&gt;In the second version where I&apos;m passing type information:&lt;/p&gt;

&lt;p&gt;val filteredDF = df.filter(df(&quot;column&quot;) &amp;lt;=&amp;gt; (new Column(Literal.create(null, SparkleFunctions.dataType(df(&quot;column&quot;))))))&lt;br/&gt;
filteredDF.show&lt;/p&gt;

&lt;p&gt;result is:&lt;/p&gt;

&lt;p&gt;&lt;ins&gt;------&lt;/ins&gt;&lt;/p&gt;
&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;column&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;&lt;ins&gt;------&lt;/ins&gt;&lt;/p&gt;
&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  null&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;&lt;ins&gt;------&lt;/ins&gt;&lt;/p&gt;

&lt;p&gt;which is correct. Here SparkleFunctions.dataType is just a method I defined to be able to get type information from Column.expr outside &quot;org.apache.spark.sql&quot; package:&lt;/p&gt;

&lt;p&gt;package org.apache.spark.sql&lt;br/&gt;
object SparkleFunctions {&lt;br/&gt;
  def dataType(c: Column): DataType = c.expr.dataType&lt;br/&gt;
}&lt;/p&gt;</comment>
                            <comment id="14987096" author="kapilsingh5050" created="Tue, 3 Nov 2015 11:08:16 +0000"  >&lt;p&gt;On second look, I seem to have identified the issue. Take a look at lines 283-286 here:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/blob/a01cbf5daac148f39cd97299780f542abc41d1e9/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/HiveTypeCoercion.scala&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/a01cbf5daac148f39cd97299780f542abc41d1e9/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/HiveTypeCoercion.scala&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If one of the types in BinaryComparison is StringType and other is NullType, during analyzed plan computation, this forces DoubleType on the StringType. Later while enforcing this cast (lines 340-343 in &lt;a href=&quot;https://github.com/apache/spark/blob/a01cbf5daac148f39cd97299780f542abc41d1e9/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/a01cbf5daac148f39cd97299780f542abc41d1e9/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala&lt;/a&gt;), conversion to double fails if string is not actually a number and a default null value is assigned to result. This manifests as null comparison resulting true for all string values. &lt;/p&gt;</comment>
                            <comment id="14992271" author="kevinyu98" created="Thu, 5 Nov 2015 19:11:21 +0000"  >&lt;p&gt;Hello Kapil : Thanks a lot. I am looking into it now. Kevin&lt;/p&gt;</comment>
                            <comment id="14999990" author="kevinyu98" created="Wed, 11 Nov 2015 06:13:59 +0000"  >&lt;p&gt;Hi Kapil: I have a possible fix ready, now I am working on the test case. &lt;/p&gt;

&lt;p&gt;Kevin&lt;/p&gt;</comment>
                            <comment id="15005799" author="apachespark" created="Sun, 15 Nov 2015 08:22:04 +0000"  >&lt;p&gt;User &apos;kevinyu98&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/9720&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/9720&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15008158" author="yhuai" created="Tue, 17 Nov 2015 06:55:33 +0000"  >&lt;p&gt;Issue resolved by pull request 9720&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/9720&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/9720&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 1 week ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2nstr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>