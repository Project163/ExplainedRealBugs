<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:28:57 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-8406] Race condition when writing Parquet files</title>
                <link>https://issues.apache.org/jira/browse/SPARK-8406</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;To support appending, the Parquet data source tries to find out the max part number of part-files in the destination directory (the &amp;lt;id&amp;gt; in output file name &quot;part-r-&amp;lt;id&amp;gt;.gz.parquet&quot;) at the beginning of the write job. In 1.3.0, this step happens on driver side before any files are written. However, in 1.4.0, this is moved to task side. Thus, for tasks scheduled later, they may see wrong max part number generated by newly written files by other finished tasks within the same job. This actually causes a race condition. In most cases, this only causes nonconsecutive IDs in output file names. But when the DataFrame contains thousands of RDD partitions, it&apos;s likely that two tasks may choose the same part number, thus one of them gets overwritten by the other.&lt;/p&gt;

&lt;p&gt;The following Spark shell snippet can reproduce nonconsecutive part numbers:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;sqlContext.range(0, 128).repartition(16).write.mode(&lt;span class=&quot;code-quote&quot;&gt;&quot;overwrite&quot;&lt;/span&gt;).parquet(&lt;span class=&quot;code-quote&quot;&gt;&quot;foo&quot;&lt;/span&gt;)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&quot;16&quot; can be replaced with any integer that is greater than the default parallelism on your machine (usually it means core number, on my machine it&apos;s 8).&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;-rw-r--r--   3 lian supergroup          0 2015-06-17 00:06 /user/lian/foo/_SUCCESS
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00001.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00002.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00003.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00004.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00005.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00006.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00007.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00008.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00017.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00018.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00019.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00020.gz.parquet
-rw-r--r--   3 lian supergroup        352 2015-06-17 00:06 /user/lian/foo/part-r-00021.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00022.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00023.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00024.gz.parquet
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And here is another Spark shell snippet for reproducing overwriting:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;sqlContext.range(0, 10000).repartition(500).write.mode(&lt;span class=&quot;code-quote&quot;&gt;&quot;overwrite&quot;&lt;/span&gt;).parquet(&lt;span class=&quot;code-quote&quot;&gt;&quot;foo&quot;&lt;/span&gt;)
sqlContext.read.parquet(&lt;span class=&quot;code-quote&quot;&gt;&quot;foo&quot;&lt;/span&gt;).count()
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Expected answer should be &lt;tt&gt;10000&lt;/tt&gt;, but you may see a number like &lt;tt&gt;9960&lt;/tt&gt; due to overwriting. The actual number varies for different runs and different nodes.&lt;/p&gt;

&lt;p&gt;Notice that the newly added ORC data source is less likely to hit this issue because it uses task ID and &lt;tt&gt;System.currentTimeMills()&lt;/tt&gt; to generate the output file name. Thus, the ORC data source may hit this issue only when two tasks with the same task ID (which means they are in two concurrent jobs) are writing to the same location within the same millisecond.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12838410">SPARK-8406</key>
            <summary>Race condition when writing Parquet files</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="lian cheng">Cheng Lian</assignee>
                                    <reporter username="lian cheng">Cheng Lian</reporter>
                        <labels>
                    </labels>
                <created>Wed, 17 Jun 2015 08:20:35 +0000</created>
                <updated>Wed, 15 Jul 2015 18:25:01 +0000</updated>
                            <resolved>Mon, 22 Jun 2015 17:04:14 +0000</resolved>
                                    <version>1.4.0</version>
                                    <fixVersion>1.4.1</fixVersion>
                    <fixVersion>1.5.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>2</votes>
                                    <watches>11</watches>
                                                                                                                <comments>
                            <comment id="14589498" author="nemccarthy" created="Wed, 17 Jun 2015 08:43:56 +0000"  >&lt;p&gt;This is hitting us hard. Let me know if there is anything we can do to help on this end with contributing a fix or testing. &lt;/p&gt;

&lt;p&gt;FYI heres details from the mailing list. &lt;/p&gt;

&lt;p&gt; &amp;#8212;&lt;/p&gt;

&lt;p&gt;When trying to save a data frame with 569610608 rows. &lt;/p&gt;

&lt;p&gt;  dfc.write.format(&quot;parquet&quot;).save(&#8220;/data/map_parquet_file&quot;)&lt;/p&gt;

&lt;p&gt;We get random results between runs. Caching the data frame in memory makes no difference. It looks like the write out misses some of the RDD partitions. We have an RDD with 6750 partitions. When we write out we get less files out than the number of partitions. When reading the data back in and running a count, we get smaller number of rows. &lt;/p&gt;

&lt;p&gt;I&#8217;ve tried counting the rows in all different ways. All return the same result, 560214031 rows, missing about 9.4 million rows (0.15%).&lt;/p&gt;

&lt;p&gt;  qc.read.parquet(&quot;/data/map_parquet_file&quot;).count&lt;br/&gt;
  qc.read.parquet(&quot;/data/map_parquet_file&quot;).rdd.count&lt;br/&gt;
  qc.read.parquet(&quot;/data/map_parquet_file&quot;).mapPartitions&lt;/p&gt;
{itr =&amp;gt; var c = 0; itr.foreach(_ =&amp;gt; c = c + 1); Seq(c).toIterator }
&lt;p&gt;.reduce(_ + _)&lt;/p&gt;

&lt;p&gt;Looking on HDFS the files, there are 6643 .parquet files. 107 missing partitions (about 0.15%). &lt;/p&gt;

&lt;p&gt;Then writing out the same cached DF again to a new file gives 6717 files on hdfs (about 33 files missing or 0.5%);&lt;/p&gt;

&lt;p&gt;  dfc.write.parquet(&#8220;/data/map_parquet_file_2&quot;)&lt;/p&gt;

&lt;p&gt;And we get 566670107 rows back (about 3million missing ~0.5%); &lt;/p&gt;

&lt;p&gt;  qc.read.parquet(&quot;/data/map_parquet_file_2&quot;).count&lt;/p&gt;

&lt;p&gt;Writing the same df out to json writes the expected number (6750) of parquet files and returns the right number of rows 569610608. &lt;/p&gt;

&lt;p&gt;  dfc.write.format(&quot;json&quot;).save(&quot;/data/map_parquet_file_3&quot;)&lt;br/&gt;
  qc.read.format(&quot;json&quot;).load(&quot;/data/map_parquet_file_3&quot;).count&lt;/p&gt;

&lt;p&gt;One thing to note is that the parquet part files on HDFS are not the normal sequential part numbers like for the json output and parquet output in Spark 1.3.&lt;/p&gt;

&lt;p&gt;part-r-06151.gz.parquet  part-r-118401.gz.parquet  part-r-146249.gz.parquet  part-r-196755.gz.parquet  part-r-35811.gz.parquet   part-r-55628.gz.parquet  part-r-73497.gz.parquet  part-r-97237.gz.parquet&lt;br/&gt;
part-r-06161.gz.parquet  part-r-118406.gz.parquet  part-r-146254.gz.parquet  part-r-196763.gz.parquet  part-r-35826.gz.parquet   part-r-55647.gz.parquet  part-r-73500.gz.parquet  _SUCCESS&lt;/p&gt;

&lt;p&gt;We are using MapR 4.0.2 for hdfs.&lt;/p&gt;</comment>
                            <comment id="14589592" author="lian cheng" created="Wed, 17 Jun 2015 10:23:05 +0000"  >&lt;p&gt;An example task execution order which causes overwriting:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;Writing a DataFrame with 4 RDD partitions to an empty directory.&lt;/li&gt;
	&lt;li&gt;Task 1 and task 2 get scheduled, while task 3 and task 4 are queued.  Both task 1 and task 2 find current max part number to be 0 (because destination directory is empty).&lt;/li&gt;
	&lt;li&gt;Task 1 finishes, generates &lt;tt&gt;part-r-00001.gz.parquet&lt;/tt&gt;. Current max part number becomes 1.&lt;/li&gt;
	&lt;li&gt;Task 4 gets scheduled, decides to write to &lt;tt&gt;part-r-00005.gz.parquet&lt;/tt&gt; (5 = current max part number + task ID), but hasn&apos;t start writing the file yet.&lt;/li&gt;
	&lt;li&gt;Task 2 finishes, generates &lt;tt&gt;part-r-00002.gz.parquet&lt;/tt&gt;. Current max part number becomes 2.&lt;/li&gt;
	&lt;li&gt;Task 3 gets scheduled, also decides to write to &lt;tt&gt;part-r-00005.gz.parquet&lt;/tt&gt; since task 4 hasn&apos;t start writing its output file, and task 3 finds current max part number is still 2.&lt;/li&gt;
	&lt;li&gt;Task 4 finishes writing &lt;tt&gt;part-r-00005.gz.parquet&lt;/tt&gt;&lt;/li&gt;
	&lt;li&gt;Task 3 finishes writing &lt;tt&gt;part-r-00005.gz.parquet&lt;/tt&gt;&lt;/li&gt;
	&lt;li&gt;Output of task 4 is overwritten.&lt;/li&gt;
&lt;/ol&gt;
</comment>
                            <comment id="14590315" author="marmbrus" created="Wed, 17 Jun 2015 18:48:43 +0000"  >&lt;p&gt;It seems to me that ORC is not free of this bug, but instead just more likely to avoid a problem, right?&lt;/p&gt;</comment>
                            <comment id="14590494" author="lian cheng" created="Wed, 17 Jun 2015 20:16:47 +0000"  >&lt;p&gt;Yeah, just updated the JIRA description.  ORC may hit this issue only when two tasks with the same task ID (which means they are in two concurrent jobs) are writing to the same location within the same millisecond.&lt;/p&gt;</comment>
                            <comment id="14590770" author="apachespark" created="Wed, 17 Jun 2015 22:43:03 +0000"  >&lt;p&gt;User &apos;liancheng&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/6864&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/6864&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14592093" author="lian cheng" created="Thu, 18 Jun 2015 16:55:35 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nemccarthy&quot; class=&quot;user-hover&quot; rel=&quot;nemccarthy&quot;&gt;nemccarthy&lt;/a&gt;, thanks again for the report.  &lt;a href=&quot;https://github.com/apache/spark/pull/6864#issuecomment-113024897&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Here&lt;/a&gt; is a summary for better understanding of this issue.&lt;/p&gt;</comment>
                            <comment id="14595474" author="apachespark" created="Mon, 22 Jun 2015 08:04:05 +0000"  >&lt;p&gt;User &apos;liancheng&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/6932&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/6932&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14596235" author="yhuai" created="Mon, 22 Jun 2015 17:04:14 +0000"  >&lt;p&gt;Issue resolved by pull request 6864&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/6864&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/6864&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14596241" author="yhuai" created="Mon, 22 Jun 2015 17:06:44 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nemccarthy&quot; class=&quot;user-hover&quot; rel=&quot;nemccarthy&quot;&gt;nemccarthy&lt;/a&gt; I have merged the fix to both master and branch 1.4.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12845347">SPARK-9072</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 22 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2g57r:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311620" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Shepherd</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>yhuai</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12332153">1.4.1</customfieldvalue>
    <customfieldvalue id="12332078">1.5.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>