<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 19:24:23 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-34167] Reading parquet with Decimal(8,2) written as a Decimal64 blows up</title>
                <link>https://issues.apache.org/jira/browse/SPARK-34167</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;When reading a parquet file written with Decimals with precision &amp;lt; 10 as a 64-bit representation, Spark tries to read it as an INT and fails. I generated this file using &lt;a href=&quot;https://github.com/rapidsai/cudf.&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/rapidsai/cudf.&lt;/a&gt; It allowed me to create a Decimal(8,2) backed by a 64-bit representation (LongDecimal). I have attached the files that can be read successfully using a 3rd party parquet reader (I used &lt;a href=&quot;https://hub.docker.com/r/nathanhowell/parquet-tools&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;nathonhowell/parquet-tools&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Steps to reproduce:&lt;/p&gt;

&lt;p&gt;Read the attached file that has a single Decimal(8,2) column with 10 values&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
scala&amp;gt; spark.read.parquet(&lt;span class=&quot;code-quote&quot;&gt;&quot;/tmp/pyspark_tests/936454/PARQUET_DATA&quot;&lt;/span&gt;).show

...
Caused by: java.lang.NullPointerException
  at org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.putLong(OnHeapColumnVector.java:327)
  at org.apache.spark.sql.execution.datasources.parquet.VectorizedRleValuesReader.readLongs(VectorizedRleValuesReader.java:370)
  at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readLongBatch(VectorizedColumnReader.java:514)
  at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:256)
  at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:273)
  at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:171)
  at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)
  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:173)
  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)
  at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:497)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:756)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)
  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
  at org.apache.spark.scheduler.Task.run(Task.scala:127)
  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:480)
  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1426)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:483)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:748)
...

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Here are my findings. The &lt;b&gt;&lt;tt&gt;VectorizedParquetRecordReader&lt;/tt&gt;&lt;/b&gt; starts to read in the long value from parquet file correctly because its basing the read on the &lt;a href=&quot;https://github.com/apache/spark/blob/e6f019836c099398542b443f7700f79de81da0d5/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase.java#L150&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;requestedSchema&lt;/a&gt; which is a &lt;b&gt;MessageType&lt;/b&gt; and has the underlying data stored correctly as &lt;tt&gt;INT64&lt;/tt&gt; where as the &lt;b&gt;WritableColumnVector&lt;/b&gt;&#160; is initialized based on the &lt;a href=&quot;https://github.com/apache/spark/blob/e6f019836c099398542b443f7700f79de81da0d5/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase.java#L151&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;batchSchema&lt;/a&gt; which is coming from &lt;tt&gt;org.apache.spark.sql.parquet.row.requested_schema&lt;/tt&gt; that is set by the reader which is a &lt;b&gt;&lt;tt&gt;StructType&lt;/tt&gt;&lt;/b&gt; and only has &lt;tt&gt;Decimal(_&lt;em&gt;,&lt;/em&gt;_)&lt;/tt&gt; in it.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/spark/blob/a44e008de3ae5aecad9e0f1a7af6a1e8b0d97f4e/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java#L224&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/a44e008de3ae5aecad9e0f1a7af6a1e8b0d97f4e/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java#L224&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;So we can see the problem above is the &lt;b&gt;WritableColumnVector&lt;/b&gt; is initialized to store an int array, while the &lt;b&gt;VectorizedParquetReader&lt;/b&gt; method calls the &lt;b&gt;readLongBatch&lt;/b&gt; method which in turn calls the &lt;b&gt;VectorizedRleValuesReader.readLongs&lt;/b&gt; which reads the long values and tries to call &lt;b&gt;WritableColumnVector.putLong&lt;/b&gt; which will throw a NPE because &lt;b&gt;WritableColumnVector&lt;/b&gt; wasn&apos;t initialized to store a long array.&lt;/p&gt;

&lt;p&gt;&#160;In the case where the file has a dictionaryPage a different exception is thrown&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Caused by: java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainLongDictionary
  at org.apache.parquet.column.Dictionary.decodeToInt(Dictionary.java:45)
  at org.apache.spark.sql.execution.datasources.parquet.ParquetDictionary.decodeToInt(ParquetDictionary.java:31)
  at org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.getInt(OnHeapColumnVector.java:298)
  at org.apache.spark.sql.execution.vectorized.WritableColumnVector.getDecimal(WritableColumnVector.java:353)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)
  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)
  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
  at org.apache.spark.scheduler.Task.run(Task.scala:127)
  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:748)

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;In this case we have to make sure the correct dictionary is initialized i.e. &lt;b&gt;PlainIntDictionary&lt;/b&gt; by setting the correct type in the &lt;b&gt;ColumnDescriptor&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Attached are two files, one with Decimal(8,2) ther other with Decimal(1,1) both written as Decimal backed by INT64. Decimal(1,1) results in a different exception but same for the same reason&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</description>
                <environment></environment>
        <key id="13353446">SPARK-34167</key>
            <summary>Reading parquet with Decimal(8,2) written as a Decimal64 blows up</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="razajafri">Raza Jafri</assignee>
                                    <reporter username="razajafri">Raza Jafri</reporter>
                        <labels>
                    </labels>
                <created>Wed, 20 Jan 2021 06:01:13 +0000</created>
                <updated>Mon, 22 Feb 2021 04:50:59 +0000</updated>
                            <resolved>Mon, 22 Feb 2021 04:49:15 +0000</resolved>
                                    <version>3.0.1</version>
                                    <fixVersion>3.2.0</fixVersion>
                                    <component>Input/Output</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="17269182" author="attilapiros" created="Thu, 21 Jan 2021 09:49:41 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=razajafri&quot; class=&quot;user-hover&quot; rel=&quot;razajafri&quot;&gt;razajafri&lt;/a&gt; could you please share with us how the parquet files are created?&lt;/p&gt;

&lt;p&gt;I tried to reproduce this issue in the following way but I had no luck:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Spark context Web UI available at http://192.168.0.17:4045
Spark context available as &apos;sc&apos; (master = local, app id = local-1611221568779).
Spark session available as &apos;spark&apos;.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  &apos;_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.0.1
      /_/

Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 1.8.0_242)
Type in expressions to have them evaluated.
Type :help for more information.

scala&amp;gt; import java.math.BigDecimal
import java.math.BigDecimal

scala&amp;gt; import org.apache.spark.sql.Row
import org.apache.spark.sql.Row

scala&amp;gt; import org.apache.spark.sql.types.{DecimalType, StructField, StructType}
import org.apache.spark.sql.types.{DecimalType, StructField, StructType}

scala&amp;gt; val schema = StructType(Array(StructField(&quot;num&quot;, DecimalType(8,2),true)))
schema: org.apache.spark.sql.types.StructType = StructType(StructField(num,DecimalType(8,2),true))

scala&amp;gt; val rdd = sc.parallelize((0 to 9).map(v =&amp;gt; new BigDecimal(s&quot;123456.7$v&quot;)))
rdd: org.apache.spark.rdd.RDD[java.math.BigDecimal] = ParallelCollectionRDD[0] at parallelize at &amp;lt;c
onsole&amp;gt;:27

scala&amp;gt; val df = spark.createDataFrame(rdd.map(Row(_)), schema)
df: org.apache.spark.sql.DataFrame = [num: decimal(8,2)]

scala&amp;gt; df.show()
+---------+
|      num|
+---------+
|123456.70|
|123456.71|
|123456.72|
|123456.73|
|123456.74|
|123456.75|
|123456.76|
|123456.77|
|123456.78|
|123456.79|
+---------+


scala&amp;gt; df.write.parquet(&quot;num.parquet&quot;)

scala&amp;gt; spark.read.parquet(&quot;num.parquet&quot;).show()
+---------+
|      num|
+---------+
|123456.70|
|123456.71|
|123456.72|
|123456.73|
|123456.74|
|123456.75|
|123456.76|
|123456.77|
|123456.78|
|123456.79|
+---------+

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
</comment>
                            <comment id="17269725" author="apachespark" created="Fri, 22 Jan 2021 00:11:24 +0000"  >&lt;p&gt;User &apos;razajafri&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/31284&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/31284&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17269726" author="apachespark" created="Fri, 22 Jan 2021 00:12:05 +0000"  >&lt;p&gt;User &apos;razajafri&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/31284&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/31284&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17270199" author="tgraves" created="Fri, 22 Jan 2021 15:07:24 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=razajafri&quot; class=&quot;user-hover&quot; rel=&quot;razajafri&quot;&gt;razajafri&lt;/a&gt;&#160;can you finish detailing your finding as to the flow of what breaks this?&#160; You state that spark tries to read it as an INT but the backtrace is in readLong.&#160; You started to detail the schema differences but can you followup with how that leads to reading as an INT even though its calling readLong?&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;For example you state &quot;The&#160;&lt;b&gt;&lt;tt&gt;VectorizedParquetRecordReader&lt;/tt&gt;&lt;/b&gt;&#160;reads in the parquet file correctly &quot; but that is where the null pointer exception is thrown. If you could detail it more it would help in understanding it.&lt;/p&gt;</comment>
                            <comment id="17270371" author="razajafri" created="Fri, 22 Jan 2021 19:19:01 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tgraves&quot; class=&quot;user-hover&quot; rel=&quot;tgraves&quot;&gt;tgraves&lt;/a&gt; I have updated the description to help trace the problem. &lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=attilapiros&quot; class=&quot;user-hover&quot; rel=&quot;attilapiros&quot;&gt;attilapiros&lt;/a&gt;&#160; I used &lt;a href=&quot;https://github.com/rapidsai/cudf.&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;rapidsai/cudf&lt;/a&gt; to create the file. I have updated the description with more details please let me know if you have any further questions&lt;/p&gt;</comment>
                            <comment id="17288176" author="cloud_fan" created="Mon, 22 Feb 2021 04:49:15 +0000"  >&lt;p&gt;Issue resolved by pull request 31284&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/31284&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/31284&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="13019038" name="part-00000-7fecd321-b247-4f7e-bff5-c2e4d8facaa0-c000.snappy.parquet" size="387" author="razajafri" created="Wed, 20 Jan 2021 06:01:40 +0000"/>
                            <attachment id="13019039" name="part-00000-940f44f1-f323-4a5e-b828-1e65d87895aa-c000.snappy.parquet" size="375" author="razajafri" created="Wed, 20 Jan 2021 06:04:59 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            4 years, 38 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z0mshc:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311620" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Shepherd</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>revans2</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12346518">3.1.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>