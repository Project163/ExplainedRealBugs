<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:34:42 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-10181] HiveContext is not used with keytab principal but with user principal/unix username</title>
                <link>https://issues.apache.org/jira/browse/SPARK-10181</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;`bin/spark-submit --num-executors 1 --executor-cores 5 --executor-memory 5G  --driver-java-options -XX:MaxPermSize=4G --driver-class-path lib/datanucleus-api-jdo-3.2.6.jar:lib/datanucleus-core-3.2.10.jar:lib/datanucleus-rdbms-3.2.9.jar:conf/hive-site.xml --files conf/hive-site.xml --master yarn --principal sparkjob --keytab /etc/security/keytabs/sparkjob.keytab --conf spark.yarn.executor.memoryOverhead=18000 --conf &quot;spark.executor.extraJavaOptions=-XX:MaxPermSize=4G&quot; --conf spark.eventLog.enabled=false ~/test.py`&lt;/p&gt;

&lt;p&gt;With:&lt;/p&gt;

&lt;p&gt;#!/usr/bin/python&lt;br/&gt;
from pyspark import SparkContext&lt;br/&gt;
from pyspark.sql import HiveContext&lt;/p&gt;

&lt;p&gt;sc = SparkContext()&lt;br/&gt;
sqlContext = HiveContext(sc)&lt;/p&gt;

&lt;p&gt;query = &quot;&quot;&quot; SELECT * FROM fm.sk_cluster &quot;&quot;&quot;&lt;br/&gt;
rdd = sqlContext.sql(query)&lt;/p&gt;

&lt;p&gt;rdd.registerTempTable(&quot;test&quot;)&lt;br/&gt;
sqlContext.sql(&quot;CREATE TABLE wcs.test LOCATION &apos;/tmp/test_gl&apos; AS SELECT * FROM test&quot;)&lt;/p&gt;

&lt;p&gt;Ends up with:&lt;/p&gt;

&lt;p&gt;Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denie&lt;br/&gt;
d: user=ua80tl, access=READ_EXECUTE, inode=&quot;/tmp/test_gl/.hive-staging_hive_2015-08-24_10-43-09_157_78057390024057878&lt;br/&gt;
34-1/&lt;del&gt;ext-10000&quot;:sparkjob:hdfs:drwxr-x&lt;/del&gt;--&lt;/p&gt;

&lt;p&gt;(Our umask denies read access to other by default)&lt;/p&gt;</description>
                <environment>&lt;p&gt;kerberos&lt;/p&gt;</environment>
        <key id="12858239">SPARK-10181</key>
            <summary>HiveContext is not used with keytab principal but with user principal/unix username</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="crystal_gaoyu">Yu Gao</assignee>
                                    <reporter username="bolke">Bolke de Bruin</reporter>
                        <labels>
                            <label>hive</label>
                            <label>hivecontext</label>
                            <label>kerberos</label>
                    </labels>
                <created>Mon, 24 Aug 2015 09:20:30 +0000</created>
                <updated>Wed, 9 May 2018 20:19:18 +0000</updated>
                            <resolved>Sun, 15 Nov 2015 22:54:25 +0000</resolved>
                                    <version>1.5.0</version>
                                    <fixVersion>1.5.3</fixVersion>
                    <fixVersion>1.6.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="14954123" author="crystal_gaoyu" created="Tue, 13 Oct 2015 00:26:20 +0000"  >&lt;p&gt;I&apos;m looking into this issue. &lt;/p&gt;

&lt;p&gt;Hi Bolke, I guess the program was run as unix user ua80tl, correct? Since principal sparkjob@YOUR_REALM was passed in and contained in keytab /etc/security/keytabs/sparkjob.keytab, all operations were supposed to be done as user sparkjob, but ua80tl was used to access table files.&lt;/p&gt;</comment>
                            <comment id="14954582" author="bolke" created="Tue, 13 Oct 2015 08:01:03 +0000"  >&lt;p&gt;Yes that is correct. &lt;/p&gt;</comment>
                            <comment id="14967643" author="crystal_gaoyu" created="Wed, 21 Oct 2015 18:48:09 +0000"  >&lt;p&gt;I reproduced this issue. Some hive metastore client threads try to read local ticket cache credentials instead of the specified principal and keytab. If there is no valid ticket cache for the unix user account running the code, following exception will be thrown -&lt;br/&gt;
ERROR transport.TSaslTransport: SASL negotiation failure&lt;br/&gt;
javax.security.sasl.SaslException: GSS initiate failed &lt;span class=&quot;error&quot;&gt;&amp;#91;Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:212)&lt;br/&gt;
        at org.apache.thrift.transport.TSaslClientTransport.handleSaslStartMessage(TSaslClientTransport.java:94)&lt;br/&gt;
        at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:271)&lt;br/&gt;
        at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)&lt;br/&gt;
        at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:52)&lt;br/&gt;
        at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:49)&lt;br/&gt;
        at java.security.AccessController.doPrivileged(Native Method)&lt;br/&gt;
        at javax.security.auth.Subject.doAs(Subject.java:415)&lt;br/&gt;
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)&lt;br/&gt;
        at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport.open(TUGIAssumingTransport.java:49)&lt;br/&gt;
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:420)&lt;br/&gt;
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&amp;lt;init&amp;gt;(HiveMetaStoreClient.java:236)&lt;br/&gt;
        at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&amp;lt;init&amp;gt;(SessionHiveMetaStoreClient.java:74)&lt;br/&gt;
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)&lt;br/&gt;
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)&lt;br/&gt;
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&lt;br/&gt;
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)&lt;br/&gt;
        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)&lt;br/&gt;
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&amp;lt;init&amp;gt;(RetryingMetaStoreClient.java:86)&lt;br/&gt;
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)&lt;br/&gt;
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)&lt;br/&gt;
        at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)&lt;br/&gt;
        at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)&lt;br/&gt;
        at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)&lt;br/&gt;
        at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)&lt;br/&gt;
        at org.apache.hadoop.hive.ql.metadata.Hive.&amp;lt;clinit&amp;gt;(Hive.java:166)&lt;br/&gt;
        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)&lt;br/&gt;
        at org.apache.spark.sql.hive.client.ClientWrapper.&amp;lt;init&amp;gt;(ClientWrapper.scala:171)&lt;br/&gt;
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)&lt;br/&gt;
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)&lt;br/&gt;
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&lt;br/&gt;
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)&lt;br/&gt;
        at org.apache.spark.sql.hive.client.IsolatedClientLoader.liftedTree1$1(IsolatedClientLoader.scala:183)&lt;br/&gt;
        at org.apache.spark.sql.hive.client.IsolatedClientLoader.&amp;lt;init&amp;gt;(IsolatedClientLoader.scala:179)&lt;br/&gt;
        at org.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:227)&lt;br/&gt;
        at org.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:186)&lt;br/&gt;
        at org.apache.spark.sql.hive.HiveContext.setConf(HiveContext.scala:393)&lt;br/&gt;
        at org.apache.spark.sql.hive.HiveContext.defaultOverrides(HiveContext.scala:175)&lt;br/&gt;
        at org.apache.spark.sql.hive.HiveContext.&amp;lt;init&amp;gt;(HiveContext.scala:178)&lt;br/&gt;
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)&lt;br/&gt;
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)&lt;br/&gt;
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&lt;br/&gt;
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)&lt;br/&gt;
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)&lt;br/&gt;
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)&lt;br/&gt;
        at py4j.Gateway.invoke(Gateway.java:214)&lt;br/&gt;
        at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)&lt;br/&gt;
        at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)&lt;br/&gt;
        at py4j.GatewayConnection.run(GatewayConnection.java:207)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:745)&lt;/p&gt;

&lt;p&gt;If there happens to have a valid ticket cache, the credentials there will be used. In my env, I ran the spark-submit command as unix user root, and specified ambari-qa as the kerberos principal, and put a different user&apos;s kerberos credentials  in root&apos;s ticket cache. Also set hdfs umask to 027.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;root@bdvs1401 spark-yu&amp;#93;&lt;/span&gt;# klist&lt;br/&gt;
Ticket cache: FILE:/tmp/krb5cc_0&lt;br/&gt;
Default principal: user4@BIADS.SVL.IBM.COM&lt;/p&gt;

&lt;p&gt;Valid starting     Expires            Service principal&lt;br/&gt;
10/21/15 11:08:13  10/21/15 21:08:36  krbtgt/BIADS.SVL.IBM.COM@BIADS.SVL.IBM.COM&lt;br/&gt;
        renew until 10/27/15 13:41:52&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;root@bdvs1401 spark-yu&amp;#93;&lt;/span&gt;# spark-submit --num-executors 1 --driver-class-path lib/datanucleus-api-jdo-3.2.6.jar:lib/datanucleus-core-3.2.10.jar:lib/datanucleus-rdbms-3.2.9.jar:conf/hive-site.xml --files conf/hive-site.xml --master yarn --principal ambari-qa --keytab /etc/security/keytabs/ambari-qa.keytab  --conf spark.eventLog.enabled=true ~/test.py&lt;/p&gt;

&lt;p&gt;Then ran into the exact file permission error:&lt;br/&gt;
org.apache.hadoop.hive.ql.metadata.HiveException: checkPaths: filesystem error in check phase. Permission denied: user=user4, access=READ_EXECUTE, inode=&quot;/tmp/test_gl/.hive-staging_hive_2015-10-14_23-10-57_135_5312925588384959525-1/&lt;del&gt;ext-10000&quot;:ambari-qa:hdfs:drwxr-x&lt;/del&gt;--&lt;br/&gt;
...&lt;br/&gt;
     at org.apache.hadoop.hive.ql.metadata.Hive.checkPaths(Hive.java:2504)&lt;br/&gt;
     at org.apache.hadoop.hive.ql.metadata.Hive.replaceFiles(Hive.java:2840)&lt;br/&gt;
     at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:1640)&lt;br/&gt;
     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)&lt;br/&gt;
     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
     at java.lang.reflect.Method.invoke(Method.java:606)&lt;br/&gt;
     at org.apache.spark.sql.hive.client.Shim_v0_14.loadTable(HiveShim.scala:441)&lt;br/&gt;
     at org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$loadTable$1.apply$mcV$sp(ClientWrapper.scala:489)&lt;br/&gt;
     at org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$loadTable$1.apply(ClientWrapper.scala:489)&lt;br/&gt;
     at org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$loadTable$1.apply(ClientWrapper.scala:489)&lt;br/&gt;
     at org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$withHiveState$1.apply(ClientWrapper.scala:256)&lt;br/&gt;
     at org.apache.spark.sql.hive.client.ClientWrapper.retryLocked(ClientWrapper.scala:211)&lt;br/&gt;
     at org.apache.spark.sql.hive.client.ClientWrapper.withHiveState(ClientWrapper.scala:248)&lt;br/&gt;
     at org.apache.spark.sql.hive.client.ClientWrapper.loadTable(ClientWrapper.scala:488)&lt;br/&gt;
     at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.sideEffectResult$lzycompute(InsertIntoHiveTable.scala:243)&lt;br/&gt;
     at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.sideEffectResult(InsertIntoHiveTable.scala:127)&lt;br/&gt;
     at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.doExecute(InsertIntoHiveTable.scala:263)&lt;br/&gt;
     at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:140)&lt;br/&gt;
     at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:138)&lt;br/&gt;
     at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)&lt;br/&gt;
     at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:138)&lt;br/&gt;
     at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:927)&lt;br/&gt;
     at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:927)&lt;br/&gt;
     at org.apache.spark.sql.hive.execution.CreateTableAsSelect.run(CreateTableAsSelect.scala:89)&lt;br/&gt;
     at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:57)&lt;br/&gt;
     at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:57)&lt;br/&gt;
     at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:69)&lt;br/&gt;
     at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:140)&lt;br/&gt;
     at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:138)&lt;br/&gt;
     at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)&lt;br/&gt;
     at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:138)&lt;br/&gt;
     at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:927)&lt;br/&gt;
     at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:927)&lt;br/&gt;
     at org.apache.spark.sql.DataFrame.&amp;lt;init&amp;gt;(DataFrame.scala:144)&lt;br/&gt;
     at org.apache.spark.sql.DataFrame.&amp;lt;init&amp;gt;(DataFrame.scala:129)&lt;br/&gt;
     at org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)&lt;br/&gt;
     at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:719)&lt;br/&gt;
     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)&lt;br/&gt;
     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
     at java.lang.reflect.Method.invoke(Method.java:606)&lt;br/&gt;
     at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)&lt;br/&gt;
     at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)&lt;br/&gt;
     at py4j.Gateway.invoke(Gateway.java:259)&lt;br/&gt;
     at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)&lt;br/&gt;
     at py4j.commands.CallCommand.execute(CallCommand.java:79)&lt;br/&gt;
     at py4j.GatewayConnection.run(GatewayConnection.java:207)&lt;br/&gt;
     at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
...&lt;/p&gt;</comment>
                            <comment id="14968238" author="crystal_gaoyu" created="Thu, 22 Oct 2015 00:16:46 +0000"  >&lt;p&gt;The exception was from the driver process on local machine, as yarn-client mode was used. On start up, UserGroupInformation.loginUserFromKeytab was indeed called with ambari-qa principal and keys, and thus static var UserGroupInfomation,loginUser was set to ambari-qa@BIADS.SVL.IBM.COM and all threads within the driver process were supposed to see and use this login credentials to authenticate with Hive and Hadoop. However, because of IsolatedClientLoader, UserGroupInformation class was not shared for hive metastore clients, and it was loaded separately and of course not able to see the prepared kerberos login credentials in the main thread.&lt;/p&gt;

&lt;p&gt;A fix would be adding UserGroupInformation and related class to spark.sql.hive.metastore.sharedPrefixes as default value. I tested it in my cluster and the issue was gone.&lt;/p&gt;</comment>
                            <comment id="14968239" author="apachespark" created="Thu, 22 Oct 2015 00:17:03 +0000"  >&lt;p&gt;User &apos;yolandagao&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/9208&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/9208&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14973423" author="apachespark" created="Sun, 25 Oct 2015 20:25:02 +0000"  >&lt;p&gt;User &apos;yolandagao&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/9272&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/9272&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14976990" author="bolke" created="Tue, 27 Oct 2015 19:15:53 +0000"  >&lt;p&gt;I will apply it too our install if it works against 1.5 and see if it works. Thanks btw!&lt;/p&gt;</comment>
                            <comment id="14977151" author="crystal_gaoyu" created="Tue, 27 Oct 2015 20:56:44 +0000"  >&lt;p&gt;No problem. Please try out the changes in the second pull request which does explicit kerberos login during hive client initialization to prepare the credentials, instead of making UserGroupInformation class shared which would introduce other class collision issues.&lt;/p&gt;</comment>
                            <comment id="15006078" author="yhuai" created="Sun, 15 Nov 2015 22:54:25 +0000"  >&lt;p&gt;Issue resolved by pull request 9272&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/9272&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/9272&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15006296" author="bolke" created="Mon, 16 Nov 2015 07:24:54 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yhuai&quot; class=&quot;user-hover&quot; rel=&quot;yhuai&quot;&gt;yhuai&lt;/a&gt; thanks for the fix which works (sorry for the late confirmation). Would it be possible to make it part of the 1.5 releases as a fix?&lt;/p&gt;</comment>
                            <comment id="15007071" author="yhuai" created="Mon, 16 Nov 2015 18:31:01 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bolke&quot; class=&quot;user-hover&quot; rel=&quot;bolke&quot;&gt;bolke&lt;/a&gt; I have merged it into branch-1.5. It will be release with 1.5.3.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="12914418">SPARK-11851</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 1 week, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2j9yf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>