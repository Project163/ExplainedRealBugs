<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:20:46 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-4516] Netty off-heap memory use causes executors to be killed by OS</title>
                <link>https://issues.apache.org/jira/browse/SPARK-4516</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;The netty block transfer manager has a race condition where it closes an active connection resulting in the error below. Switching to nio seems to alleviate the problem.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;14/11/20 18:53:43 INFO TransportClientFactory: Found inactive connection to i-974cd879.inst.aws.airbnb.com/10.154.228.43:57773, closing it.
14/11/20 18:53:43 ERROR RetryingBlockFetcher: Exception &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; beginning fetch of 1 outstanding blocks 
java.io.IOException: Failed to connect to i-974cd879.inst.aws.airbnb.com/10.154.228.43:57773
at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:141)
at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:78)
at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
at org.apache.spark.network.shuffle.RetryingBlockFetcher.start(RetryingBlockFetcher.java:120)
at org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:87)
at org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:148)
at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:288)
at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:52)
at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
at scala.collection.Iterator$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(Iterator.scala:727)
at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
at com.airbnb.common.ml.training.LinearRankerTrainer$$anonfun$7.apply(LinearRankerTrainer.scala:246)
at com.airbnb.common.ml.training.LinearRankerTrainer$$anonfun$7.apply(LinearRankerTrainer.scala:235)
at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:601)
at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:601)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
at org.apache.spark.scheduler.Task.run(Task.scala:56)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
Caused by: java.net.ConnectException: Connection refused: i-974cd879.inst.aws.airbnb.com/10.154.228.43:57773
at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:208)
at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:287)
at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment>&lt;p&gt;Linux, Mesos&lt;/p&gt;</environment>
        <key id="12756790">SPARK-4516</key>
            <summary>Netty off-heap memory use causes executors to be killed by OS</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="adav">Aaron Davidson</assignee>
                                    <reporter username="hector.yee">Hector Yee</reporter>
                        <labels>
                            <label>netty</label>
                            <label>shuffle</label>
                    </labels>
                <created>Thu, 20 Nov 2014 19:49:36 +0000</created>
                <updated>Sun, 17 May 2020 18:30:33 +0000</updated>
                            <resolved>Wed, 26 Nov 2014 04:58:20 +0000</resolved>
                                    <version>1.2.0</version>
                                    <fixVersion>1.2.0</fixVersion>
                                    <component>Shuffle</component>
                    <component>Spark Core</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>7</watches>
                                                                                                                <comments>
                            <comment id="14220014" author="andrewor14" created="Thu, 20 Nov 2014 21:18:14 +0000"  >&lt;p&gt;Hey &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hector.yee&quot; class=&quot;user-hover&quot; rel=&quot;hector.yee&quot;&gt;hector.yee&lt;/a&gt; so what is the race condition?&lt;/p&gt;</comment>
                            <comment id="14220025" author="hector.yee" created="Thu, 20 Nov 2014 21:24:26 +0000"  >&lt;p&gt;The channel is marked as inactive while it is being used I believe. I didn&apos;t dig into the code so I have no idea but according&lt;br/&gt;
to the logs that is what seems to be happening.&lt;/p&gt;</comment>
                            <comment id="14220106" author="ilikerps" created="Thu, 20 Nov 2014 22:11:40 +0000"  >&lt;p&gt;The code in question, despite the log message, does not actually close the socket, but just forgets about it. It seems at least somewhat likely that the client is in fact inactive because the port on the other side went down.&lt;/p&gt;

&lt;p&gt;Note that the second error is from us trying to create a new connection, not from reusing an old connection that was closed. This suggests that the server is in fact in a bad state. If you ever get the chance to reproduce this and look at the server&apos;s logs with netty on, that would be much appreciated.&lt;/p&gt;</comment>
                            <comment id="14220114" author="ilikerps" created="Thu, 20 Nov 2014 22:17:09 +0000"  >&lt;p&gt;Also ideally there would be some other exception earlier on the client that indicates when/why the connection became inactive, like a ClosedChannelException. If not, we may be silently ignoring an exception that should be logged.&lt;/p&gt;</comment>
                            <comment id="14220157" author="hector.yee" created="Thu, 20 Nov 2014 22:45:03 +0000"  >&lt;p&gt;Digging deeper it looks like you are right, the first machine fails silently with no reason in the log.&lt;br/&gt;
My guess is that it ran out of memory, when this kind of thing happens. The last time this happened was when the native snappy&lt;br/&gt;
library used too much ram. I upped the mesos overhead to 1G to fix those snappy errors: --conf spark.mesos.executor.memoryOverhead=1024 &lt;br/&gt;
Is it possible that netty uses something off java heap and is allocating too much?&lt;br/&gt;
Or maybe a silent failure somewhere that is not logged?&lt;/p&gt;

&lt;p&gt;Diagnostics follow:&lt;/p&gt;

&lt;p&gt;1st machine fails (f20aaa19) with nothing in the log. The last thing it says is starting 3 remote fetchers&lt;/p&gt;

&lt;p&gt;14/11/20 22:35:18 INFO MapOutputTrackerWorker: Don&apos;t have map outputs for shuffle 1, fetching them&lt;br/&gt;
14/11/20 22:35:18 INFO MapOutputTrackerWorker: Doing the fetch; tracker actor = Actor&lt;span class=&quot;error&quot;&gt;&amp;#91;akka.tcp://sparkDriver@i-62305989.inst.aws.airbnb.com:46605/user/MapOutputTracker#-1862215473&amp;#93;&lt;/span&gt;&lt;br/&gt;
14/11/20 22:35:18 INFO MapOutputTrackerWorker: Got the output locations&lt;br/&gt;
14/11/20 22:35:18 INFO ShuffleBlockFetcherIterator: Getting 498 non-empty blocks out of 724 blocks&lt;br/&gt;
14/11/20 22:35:18 INFO ShuffleBlockFetcherIterator: Started 3 remote fetches in 67 ms&lt;/p&gt;

&lt;p&gt;On the master it says&lt;br/&gt;
14/11/20 22:36:25 ERROR TaskSchedulerImpl: Lost executor 20141023-174642-3852091146-5050-41161-1224 on i-f20aaa19.inst.aws.airbnb.com: remote Akka client disassociated&lt;br/&gt;
14/11/20 22:36:25 WARN ReliableDeliverySupervisor: Association with remote system &lt;span class=&quot;error&quot;&gt;&amp;#91;akka.tcp://sparkExecutor@i-f20aaa19.inst.aws.airbnb.com:54417&amp;#93;&lt;/span&gt; has failed, address is now gated for &lt;span class=&quot;error&quot;&gt;&amp;#91;5000&amp;#93;&lt;/span&gt; ms. Reason is: &lt;span class=&quot;error&quot;&gt;&amp;#91;Disassociated&amp;#93;&lt;/span&gt;.&lt;br/&gt;
14/11/20 22:36:25 INFO TaskSetManager: Re-queueing tasks for 20141023-174642-3852091146-5050-41161-1224 from TaskSet 0.0&lt;/p&gt;

&lt;p&gt;14/11/20 22:36:25 WARN TaskSetManager: Lost task 7.0 in stage 1.0 (TID 898, i-f20aaa19.inst.aws.airbnb.com): ExecutorLostFailure (executor 20141023-174642-3852091146-5050-41161-1224 lost)&lt;br/&gt;
14/11/20 22:36:25 ERROR CoarseMesosSchedulerBackend: Asked to remove non-existent executor 20141023-174642-3852091146-5050-41161-1224&lt;/p&gt;

&lt;p&gt;2nd machine fails saying it could not connect&lt;/p&gt;

&lt;p&gt;14/11/20 22:36:36 INFO TransportClientFactory: Found inactive connection to i-f20aaa19.inst.aws.airbnb.com/10.225.139.181:51003, closing it.&lt;br/&gt;
14/11/20 22:36:36 ERROR RetryingBlockFetcher: Exception while beginning fetch of 1 outstanding blocks &lt;br/&gt;
java.io.IOException: Failed to connect to i-f20aaa19.inst.aws.airbnb.com/10.225.139.181:51003&lt;br/&gt;
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:141)&lt;br/&gt;
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:78)&lt;br/&gt;
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)&lt;br/&gt;
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.start(RetryingBlockFetcher.java:120)&lt;br/&gt;
	at org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:87)&lt;/p&gt;</comment>
                            <comment id="14220345" author="pwendell" created="Fri, 21 Nov 2014 01:05:36 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hector.yee&quot; class=&quot;user-hover&quot; rel=&quot;hector.yee&quot;&gt;hector.yee&lt;/a&gt; Yes - the netty transfer service will use off heap buffers whereas the NIO service doesn&apos;t. Is their any way you can verify that your executors are getting killed by the container manager (i.e. dmesg). That would likely narrow it down to this issue with 100% certainty.&lt;/p&gt;

&lt;p&gt;In terms of how to fix it - &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=adav&quot; class=&quot;user-hover&quot; rel=&quot;adav&quot;&gt;adav&lt;/a&gt; will know better than I. But one thing is I think you can ask netty not to use off heap buffers by setting &quot;spark.shuffle.io.preferDirectBufs&quot; to &quot;false&quot;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/spark/blob/master/network/common/src/main/java/org/apache/spark/network/util/TransportConf.java&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/master/network/common/src/main/java/org/apache/spark/network/util/TransportConf.java&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I thought we had other settings to limit the memory usage, but &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ilikerps&quot; class=&quot;user-hover&quot; rel=&quot;ilikerps&quot;&gt;ilikerps&lt;/a&gt; will know more.&lt;/p&gt;</comment>
                            <comment id="14220352" author="pwendell" created="Fri, 21 Nov 2014 01:12:17 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hector.yee&quot; class=&quot;user-hover&quot; rel=&quot;hector.yee&quot;&gt;hector.yee&lt;/a&gt; I updated the title, let me know if you decide this is not related to being killed by the OS but it seems like that is the case.&lt;/p&gt;</comment>
                            <comment id="14220362" author="hector.yee" created="Fri, 21 Nov 2014 01:29:00 +0000"  >&lt;p&gt;I checked the mesos log on the slave and it was an OOM kill&lt;/p&gt;

&lt;p&gt;I1120 22:36:20.193279 95373 slave.cpp:3321] Current usage 33.69%. Max allowed age: 1.126220739407303days&lt;br/&gt;
I1120 22:36:23.329488 95371 mem.cpp:532] OOM notifier is triggered for container def5caa2-c0f3-4175-9f5c-210735e6e009&lt;br/&gt;
I1120 22:36:23.329684 95371 mem.cpp:551] OOM detected for container def5caa2-c0f3-4175-9f5c-210735e6e009&lt;br/&gt;
I1120 22:36:23.330762 95371 mem.cpp:605] Memory limit exceeded: Requested: 26328MB Maximum Used: 26328MB&lt;/p&gt;

&lt;p&gt;MEMORY STATISTICS: &lt;br/&gt;
cache 126976&lt;br/&gt;
rss 27606781952&lt;br/&gt;
rss_huge 0&lt;br/&gt;
mapped_file 16384&lt;br/&gt;
writeback 0&lt;br/&gt;
swap 0&lt;br/&gt;
pgpgin 14435895&lt;br/&gt;
pgpgout 7695927&lt;br/&gt;
pgfault 63682623&lt;br/&gt;
pgmajfault 824&lt;br/&gt;
inactive_anon 0&lt;br/&gt;
active_anon 27606781952&lt;br/&gt;
inactive_file 126976&lt;br/&gt;
active_file 0&lt;br/&gt;
unevictable 0&lt;br/&gt;
hierarchical_memory_limit 27606908928&lt;br/&gt;
hierarchical_memsw_limit 18446744073709551615&lt;br/&gt;
total_cache 126976&lt;br/&gt;
total_rss 27606781952&lt;br/&gt;
total_rss_huge 0&lt;br/&gt;
total_mapped_file 16384&lt;br/&gt;
total_writeback 0&lt;br/&gt;
total_swap 0&lt;br/&gt;
total_pgpgin 14435895&lt;br/&gt;
total_pgpgout 7695927&lt;br/&gt;
total_pgfault 63682623&lt;br/&gt;
total_pgmajfault 824&lt;br/&gt;
total_inactive_anon 0&lt;br/&gt;
total_active_anon 27606781952&lt;br/&gt;
total_inactive_file 126976&lt;br/&gt;
total_active_file 0&lt;br/&gt;
total_unevictable 0&lt;br/&gt;
I1120 22:36:23.330862 95371 containerizer.cpp:1133] Container def5caa2-c0f3-4175-9f5c-210735e6e009 has reached its limit for resource mem&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/star_yellow.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;:26328 and will be terminated&lt;br/&gt;
I1120 22:36:23.330899 95371 containerizer.cpp:946] Destroying container &apos;def5caa2-c0f3-4175-9f5c-210735e6e009&apos;&lt;br/&gt;
I1120 22:36:23.332049 95367 cgroups.cpp:2207] Freezing cgroup /sys/fs/cgroup/freezer/mesos/def5caa2-c0f3-4175-9f5c-210735e6e009&lt;br/&gt;
I1120 22:36:23.434741 95371 cgroups.cpp:1374] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos/def5caa2-c0f3-4175-9f5c-210735e6e009 after 102.648064ms&lt;br/&gt;
I1120 22:36:23.436122 95391 cgroups.cpp:2224] Thawing cgroup /sys/fs/cgroup/freezer/mesos/def5caa2-c0f3-4175-9f5c-210735e6e009&lt;br/&gt;
I1120 22:36:23.437611 95391 cgroups.cpp:1403] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/def5caa2-c0f3-4175-9f5c-210735e6e009 after 1.39904ms&lt;br/&gt;
I1120 22:36:23.439303 95394 containerizer.cpp:1117] Executor for container &apos;def5caa2-c0f3-4175-9f5c-210735e6e009&apos; has exited&lt;br/&gt;
I1120 22:36:25.953094 95368 slave.cpp:2898] Executor &apos;33&apos; of framework 20141119-235105-1873488138-31272-108823-0041 terminated with signal Killed&lt;br/&gt;
I1120 22:36:25.953872 95368 slave.cpp:2215] Handling status update TASK_FAILED (UUID: 986cf483-d400-4edc-8423-dd0e51dfeeb8) for task 33 of framework 20141119-235105-1873488138-31272-108823-0041 from @0.0.0.0:0&lt;br/&gt;
I1120 22:36:25.953943 95368 slave.cpp:4305] Terminating task 33&lt;/p&gt;</comment>
                            <comment id="14220382" author="hector.yee" created="Fri, 21 Nov 2014 01:43:36 +0000"  >&lt;p&gt;Also the log was from  tmp  mesos  slaves  20141023-174642-3852091146-5050-41161-1224  frameworks  20141119-235105-1873488138-31272-108823-0041  executors  33  runs  def5caa2-c0f3-4175-9f5c-210735e6e009 so just to confirm the executor and container IDs match up&lt;/p&gt;</comment>
                            <comment id="14221519" author="pwendell" created="Fri, 21 Nov 2014 22:36:09 +0000"  >&lt;p&gt;Okay sounds good. Does changing the netty config help?&lt;/p&gt;</comment>
                            <comment id="14221758" author="hector.yee" created="Sat, 22 Nov 2014 02:26:00 +0000"  >&lt;p&gt;Yes turning off direct buffers worked with Netty&lt;/p&gt;</comment>
                            <comment id="14222172" author="pwendell" created="Sat, 22 Nov 2014 20:45:30 +0000"  >&lt;p&gt;Okay then I think this is just a documentation issue. We should add the documentation about direct buffers to the main configuration page and also mention it in the doc about network options.&lt;/p&gt;</comment>
                            <comment id="14225627" author="apachespark" created="Wed, 26 Nov 2014 02:49:32 +0000"  >&lt;p&gt;User &apos;aarondav&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/3465&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/3465&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14225700" author="ilikerps" created="Wed, 26 Nov 2014 04:13:56 +0000"  >&lt;p&gt;It turns out there was a real bug which caused us to allocate memory proportional to both number of cores and number of &lt;em&gt;executors&lt;/em&gt; in the cluster. PR &lt;a href=&quot;https://github.com/apache/spark/pull/3465&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;#3465&lt;/a&gt; should remove the latter factor, which should greatly decrease the amount of off-heap memory allocated.&lt;/p&gt;

&lt;p&gt;Do note that even with this patch, one key feature of the Netty transport service is that we do allocate and reuse significant off-heap buffer space rather than on-heap, which helps reduce GC pauses. So it&apos;s possible that certain environments which previously heavily constrained off-heap memory (by giving almost all of the container/cgroup&apos;s memory to the Spark heap) may have to be modified to ensure that at least 32 * (number of cores) MB is available to be allocated off-JVM heap.&lt;/p&gt;

&lt;p&gt;If this is not possible, you can either disable direct byte buffer usage via &quot;spark.shuffle.io.preferDirectBufs&quot; or set &quot;spark.shuffle.io.serverThreads&quot; and &quot;spark.shuffle.io.clientThreads&quot; to something smaller than the number of executor cores. Typically we find that 10GB/s network cannot saturate more than, say, 8 cores on a machine (in practice I&apos;ve never seen even that many required), so we would expect no performance degradation if you set these parameters such on beefier machines, and it should cap off-heap allocation to order of 256 MB.&lt;/p&gt;</comment>
                            <comment id="14225731" author="apachespark" created="Wed, 26 Nov 2014 04:42:24 +0000"  >&lt;p&gt;User &apos;aarondav&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/3469&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/3469&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14225743" author="ilikerps" created="Wed, 26 Nov 2014 04:58:22 +0000"  >&lt;p&gt;About my last point, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rxin&quot; class=&quot;user-hover&quot; rel=&quot;rxin&quot;&gt;rxin&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pwendell&quot; class=&quot;user-hover&quot; rel=&quot;pwendell&quot;&gt;pwendell&lt;/a&gt;, and I decided it may be better if we just cap the number of threads we use by default to 8, to try to avoid issues for people who use executors with very large number of cores and were on the edge of their off-heap limits already. #3469 implements this, which may cause a performance regression if we&apos;re wrong about the magic number 8 being an upper bound on the useful number of cores. It can be overridden via the serverThreads/clientThreads properties, but if anyone sees this as an issue, please let me know.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 51 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i22mcv:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311620" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Shepherd</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>rxin</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12327369">1.2.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>