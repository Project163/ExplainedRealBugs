<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:49:20 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-18050] spark 2.0.1 enable hive throw AlreadyExistsException(message:Database default already exists)</title>
                <link>https://issues.apache.org/jira/browse/SPARK-18050</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;in spark 2.0.1 ,I enable hive support and when init the sqlContext ,throw a AlreadyExistsException(message:Database default already exists),same as &lt;br/&gt;
&lt;a href=&quot;https://www.mail-archive.com/dev@spark.apache.org/msg15306.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://www.mail-archive.com/dev@spark.apache.org/msg15306.html&lt;/a&gt; ,my code is &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; val master = &lt;span class=&quot;code-quote&quot;&gt;&quot;local[*]&quot;&lt;/span&gt;
  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; val appName = &lt;span class=&quot;code-quote&quot;&gt;&quot;xqlServerSpark&quot;&lt;/span&gt;
  val fileSystem = FileSystem.get()
  val sparkConf = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; SparkConf().setMaster(master).
    setAppName(appName).set(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.sql.warehouse.dir&quot;&lt;/span&gt;, s&lt;span class=&quot;code-quote&quot;&gt;&quot;${fileSystem.getUri.toASCIIString}/user/hive/warehouse&quot;&lt;/span&gt;)
  val   hiveContext = SparkSession.builder().config(sparkConf).enableHiveSupport().getOrCreate().sqlContext
    print(sparkConf.get(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.sql.warehouse.dir&quot;&lt;/span&gt;))
    hiveContext.sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;show tables&quot;&lt;/span&gt;).show()
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;the result is correct,but a exception also throwBy the code&lt;/p&gt;</description>
                <environment>&lt;p&gt;jdk1.8, macOs,spark 2.0.1&lt;/p&gt;</environment>
        <key id="13014247">SPARK-18050</key>
            <summary>spark 2.0.1 enable hive throw AlreadyExistsException(message:Database default already exists)</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="cloud_fan">Wenchen Fan</assignee>
                                    <reporter username="cjuexuan">todd.chen</reporter>
                        <labels>
                    </labels>
                <created>Fri, 21 Oct 2016 13:53:33 +0000</created>
                <updated>Wed, 23 Nov 2016 17:56:02 +0000</updated>
                            <resolved>Wed, 23 Nov 2016 17:56:02 +0000</resolved>
                                                    <fixVersion>2.1.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="15653322" author="cloud_fan" created="Thu, 10 Nov 2016 07:42:12 +0000"  >&lt;p&gt;can you put the stacktrace here too?&lt;/p&gt;</comment>
                            <comment id="15663821" author="cjuexuan" created="Mon, 14 Nov 2016 12:41:23 +0000"  >&lt;p&gt;16/11/14 20:38:03 INFO hive.HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.&lt;br/&gt;
16/11/14 20:38:03 WARN conf.HiveConf: HiveConf of name hive.mapjoin.optimized.keys does not exist&lt;br/&gt;
16/11/14 20:38:03 WARN conf.HiveConf: HiveConf of name hive.optimize.multigroupby.common.distincts does not exist&lt;br/&gt;
16/11/14 20:38:03 WARN conf.HiveConf: HiveConf of name hive.mapjoin.lazy.hashtable does not exist&lt;br/&gt;
16/11/14 20:38:03 WARN conf.HiveConf: HiveConf of name hive.server2.thrift.http.min.worker.threads does not exist&lt;br/&gt;
16/11/14 20:38:03 WARN conf.HiveConf: HiveConf of name hive.server2.thrift.http.max.worker.threads does not exist&lt;br/&gt;
16/11/14 20:38:03 WARN conf.HiveConf: HiveConf of name hive.server2.logging.operation.verbose does not exist&lt;br/&gt;
16/11/14 20:38:04 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore&lt;br/&gt;
16/11/14 20:38:04 INFO metastore.ObjectStore: ObjectStore, initialize called&lt;br/&gt;
16/11/14 20:38:04 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored&lt;br/&gt;
16/11/14 20:38:04 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored&lt;br/&gt;
16/11/14 20:38:06 WARN conf.HiveConf: HiveConf of name hive.mapjoin.optimized.keys does not exist&lt;br/&gt;
16/11/14 20:38:06 WARN conf.HiveConf: HiveConf of name hive.optimize.multigroupby.common.distincts does not exist&lt;br/&gt;
16/11/14 20:38:06 WARN conf.HiveConf: HiveConf of name hive.mapjoin.lazy.hashtable does not exist&lt;br/&gt;
16/11/14 20:38:06 WARN conf.HiveConf: HiveConf of name hive.server2.thrift.http.min.worker.threads does not exist&lt;br/&gt;
16/11/14 20:38:06 WARN conf.HiveConf: HiveConf of name hive.server2.thrift.http.max.worker.threads does not exist&lt;br/&gt;
16/11/14 20:38:06 WARN conf.HiveConf: HiveConf of name hive.server2.logging.operation.verbose does not exist&lt;br/&gt;
16/11/14 20:38:06 INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=&quot;Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order&quot;&lt;br/&gt;
16/11/14 20:38:09 INFO DataNucleus.Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MFieldSchema&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.&lt;br/&gt;
16/11/14 20:38:09 INFO DataNucleus.Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MOrder&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.&lt;br/&gt;
16/11/14 20:38:11 INFO DataNucleus.Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MFieldSchema&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.&lt;br/&gt;
16/11/14 20:38:11 INFO DataNucleus.Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MOrder&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.&lt;br/&gt;
16/11/14 20:38:11 INFO DataNucleus.Query: Reading in results for query &quot;org.datanucleus.store.rdbms.query.SQLQuery@0&quot; since the connection used is closing&lt;br/&gt;
16/11/14 20:38:12 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is MYSQL&lt;br/&gt;
16/11/14 20:38:12 INFO metastore.ObjectStore: Initialized ObjectStore&lt;br/&gt;
16/11/14 20:38:13 INFO metastore.HiveMetaStore: Added admin role in metastore&lt;br/&gt;
16/11/14 20:38:13 INFO metastore.HiveMetaStore: Added public role in metastore&lt;br/&gt;
16/11/14 20:38:14 INFO metastore.HiveMetaStore: No user is added in admin role, since config is empty&lt;br/&gt;
16/11/14 20:38:14 INFO metastore.HiveMetaStore: 0: get_all_databases&lt;br/&gt;
16/11/14 20:38:14 INFO HiveMetaStore.audit: ugi=cjuexuan	ip=unknown-ip-addr	cmd=get_all_databases	&lt;br/&gt;
16/11/14 20:38:14 INFO metastore.HiveMetaStore: 0: get_functions: db=bi pat=*&lt;br/&gt;
16/11/14 20:38:14 INFO HiveMetaStore.audit: ugi=cjuexuan	ip=unknown-ip-addr	cmd=get_functions: db=bi pat=*	&lt;br/&gt;
16/11/14 20:38:14 INFO DataNucleus.Datastore: The class &quot;org.apache.hadoop.hive.metastore.model.MResourceUri&quot; is tagged as &quot;embedded-only&quot; so does not have its own datastore table.&lt;br/&gt;
16/11/14 20:38:15 INFO metastore.HiveMetaStore: 0: get_functions: db=default pat=*&lt;br/&gt;
16/11/14 20:38:15 INFO HiveMetaStore.audit: ugi=cjuexuan	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	&lt;br/&gt;
16/11/14 20:38:15 INFO metastore.HiveMetaStore: 0: get_functions: db=search pat=*&lt;br/&gt;
16/11/14 20:38:15 INFO HiveMetaStore.audit: ugi=cjuexuan	ip=unknown-ip-addr	cmd=get_functions: db=search pat=*	&lt;br/&gt;
16/11/14 20:38:15 INFO metastore.HiveMetaStore: 0: get_functions: db=test_randy pat=*&lt;br/&gt;
16/11/14 20:38:15 INFO HiveMetaStore.audit: ugi=cjuexuan	ip=unknown-ip-addr	cmd=get_functions: db=test_randy pat=*	&lt;br/&gt;
16/11/14 20:38:15 INFO metastore.HiveMetaStore: 0: get_functions: db=testaa pat=*&lt;br/&gt;
16/11/14 20:38:15 INFO HiveMetaStore.audit: ugi=cjuexuan	ip=unknown-ip-addr	cmd=get_functions: db=testaa pat=*	&lt;br/&gt;
16/11/14 20:38:15 INFO session.SessionState: Created local directory: /var/folders/0h/bdlvyj3j21d3t65dt8thq7500000gp/T/81f6f9b7-5e21-49ce-9dcc-48b5297e8d95_resources&lt;br/&gt;
16/11/14 20:38:15 INFO session.SessionState: Created HDFS directory: /tmp/hive-cjuexuan/cjuexuan/81f6f9b7-5e21-49ce-9dcc-48b5297e8d95&lt;br/&gt;
16/11/14 20:38:15 INFO session.SessionState: Created local directory: /tmp/cjuexuan/81f6f9b7-5e21-49ce-9dcc-48b5297e8d95&lt;br/&gt;
16/11/14 20:38:15 INFO session.SessionState: Created HDFS directory: /tmp/hive-cjuexuan/cjuexuan/81f6f9b7-5e21-49ce-9dcc-48b5297e8d95/_tmp_space.db&lt;br/&gt;
16/11/14 20:38:15 INFO client.HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse&lt;br/&gt;
16/11/14 20:38:15 WARN conf.HiveConf: HiveConf of name hive.mapjoin.optimized.keys does not exist&lt;br/&gt;
16/11/14 20:38:15 WARN conf.HiveConf: HiveConf of name hive.optimize.multigroupby.common.distincts does not exist&lt;br/&gt;
16/11/14 20:38:15 WARN conf.HiveConf: HiveConf of name hive.mapjoin.lazy.hashtable does not exist&lt;br/&gt;
16/11/14 20:38:15 WARN conf.HiveConf: HiveConf of name hive.server2.thrift.http.min.worker.threads does not exist&lt;br/&gt;
16/11/14 20:38:15 WARN conf.HiveConf: HiveConf of name hive.server2.thrift.http.max.worker.threads does not exist&lt;br/&gt;
16/11/14 20:38:15 WARN conf.HiveConf: HiveConf of name hive.server2.logging.operation.verbose does not exist&lt;br/&gt;
16/11/14 20:38:15 INFO session.SessionState: Created local directory: /var/folders/0h/bdlvyj3j21d3t65dt8thq7500000gp/T/59a86193-b20e-4b0e-8c74-ccc43e3f5203_resources&lt;br/&gt;
16/11/14 20:38:15 INFO session.SessionState: Created HDFS directory: /tmp/hive-cjuexuan/cjuexuan/59a86193-b20e-4b0e-8c74-ccc43e3f5203&lt;br/&gt;
16/11/14 20:38:15 INFO session.SessionState: Created local directory: /tmp/cjuexuan/59a86193-b20e-4b0e-8c74-ccc43e3f5203&lt;br/&gt;
16/11/14 20:38:15 INFO session.SessionState: Created HDFS directory: /tmp/hive-cjuexuan/cjuexuan/59a86193-b20e-4b0e-8c74-ccc43e3f5203/_tmp_space.db&lt;br/&gt;
16/11/14 20:38:15 INFO client.HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is /user/hive/warehouse&lt;br/&gt;
16/11/14 20:38:16 INFO metastore.HiveMetaStore: 0: create_database: Database(name:default, description:default database, locationUri:&lt;a href=&quot;file:/user/hive/warehouse&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;file:/user/hive/warehouse&lt;/a&gt;, parameters:{})&lt;br/&gt;
16/11/14 20:38:16 INFO HiveMetaStore.audit: ugi=cjuexuan	ip=unknown-ip-addr	cmd=create_database: Database(name:default, description:default database, locationUri:&lt;a href=&quot;file:/user/hive/warehouse&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;file:/user/hive/warehouse&lt;/a&gt;, parameters:{})	&lt;br/&gt;
16/11/14 20:38:16 ERROR metastore.RetryingHMSHandler: AlreadyExistsException(message:Database default already exists)&lt;br/&gt;
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:891)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:498)&lt;br/&gt;
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)&lt;br/&gt;
	at com.sun.proxy.$Proxy21.create_database(Unknown Source)&lt;br/&gt;
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:644)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:498)&lt;br/&gt;
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)&lt;br/&gt;
	at com.sun.proxy.$Proxy22.createDatabase(Unknown Source)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:306)&lt;br/&gt;
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply$mcV$sp(HiveClientImpl.scala:309)&lt;br/&gt;
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:309)&lt;br/&gt;
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:309)&lt;br/&gt;
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:280)&lt;br/&gt;
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:227)&lt;br/&gt;
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:226)&lt;br/&gt;
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:269)&lt;br/&gt;
	at org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:308)&lt;br/&gt;
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply$mcV$sp(HiveExternalCatalog.scala:99)&lt;br/&gt;
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)&lt;br/&gt;
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)&lt;br/&gt;
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:72)&lt;br/&gt;
	at org.apache.spark.sql.hive.HiveExternalCatalog.createDatabase(HiveExternalCatalog.scala:98)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:147)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.&amp;lt;init&amp;gt;(SessionCatalog.scala:89)&lt;br/&gt;
	at org.apache.spark.sql.hive.HiveSessionCatalog.&amp;lt;init&amp;gt;(HiveSessionCatalog.scala:51)&lt;br/&gt;
	at org.apache.spark.sql.hive.HiveSessionState.catalog$lzycompute(HiveSessionState.scala:49)&lt;br/&gt;
	at org.apache.spark.sql.hive.HiveSessionState.catalog(HiveSessionState.scala:48)&lt;br/&gt;
	at org.apache.spark.sql.hive.HiveSessionState.catalog(HiveSessionState.scala:31)&lt;br/&gt;
	at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:471)&lt;br/&gt;
	at com.ximalaya.xql.engine.exec.hive.HiveDataFrameReader.load(HiveDataFrameReader.scala:25)&lt;br/&gt;
	at com.ximalaya.xql.engine.exec.hive.HiveDataFrameReaderSuite$$anonfun$1.apply$mcV$sp(HiveDataFrameReaderSuite.scala:19)&lt;br/&gt;
	at com.ximalaya.xql.engine.exec.hive.HiveDataFrameReaderSuite$$anonfun$1.apply(HiveDataFrameReaderSuite.scala:15)&lt;br/&gt;
	at com.ximalaya.xql.engine.exec.hive.HiveDataFrameReaderSuite$$anonfun$1.apply(HiveDataFrameReaderSuite.scala:15)&lt;br/&gt;
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)&lt;br/&gt;
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)&lt;br/&gt;
	at org.scalatest.Transformer.apply(Transformer.scala:22)&lt;br/&gt;
	at org.scalatest.Transformer.apply(Transformer.scala:20)&lt;br/&gt;
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)&lt;br/&gt;
	at org.scalatest.TestSuite$class.withFixture(TestSuite.scala:196)&lt;br/&gt;
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1560)&lt;br/&gt;
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)&lt;br/&gt;
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)&lt;br/&gt;
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)&lt;br/&gt;
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)&lt;br/&gt;
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)&lt;br/&gt;
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1560)&lt;br/&gt;
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)&lt;br/&gt;
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)&lt;br/&gt;
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)&lt;br/&gt;
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)&lt;br/&gt;
	at scala.collection.immutable.List.foreach(List.scala:381)&lt;br/&gt;
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)&lt;br/&gt;
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)&lt;br/&gt;
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)&lt;br/&gt;
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)&lt;br/&gt;
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)&lt;br/&gt;
	at org.scalatest.Suite$class.run(Suite.scala:1147)&lt;br/&gt;
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)&lt;br/&gt;
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)&lt;br/&gt;
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)&lt;br/&gt;
	at org.scalatest.SuperEngine.runImpl(Engine.scala:521)&lt;br/&gt;
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)&lt;br/&gt;
	at com.ximalaya.xql.engine.exec.hive.HiveDataFrameReaderSuite.org$scalatest$BeforeAndAfterAll$$super$run(HiveDataFrameReaderSuite.scala:12)&lt;br/&gt;
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)&lt;br/&gt;
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)&lt;br/&gt;
	at com.ximalaya.xql.engine.exec.hive.HiveDataFrameReaderSuite.run(HiveDataFrameReaderSuite.scala:12)&lt;br/&gt;
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)&lt;br/&gt;
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1340)&lt;br/&gt;
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1334)&lt;br/&gt;
	at scala.collection.immutable.List.foreach(List.scala:381)&lt;br/&gt;
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1334)&lt;br/&gt;
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)&lt;br/&gt;
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)&lt;br/&gt;
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1500)&lt;br/&gt;
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)&lt;br/&gt;
	at org.scalatest.tools.Runner$.run(Runner.scala:850)&lt;br/&gt;
	at org.scalatest.tools.Runner.run(Runner.scala)&lt;br/&gt;
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:138)&lt;br/&gt;
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:28)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:498)&lt;br/&gt;
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)&lt;/p&gt;

&lt;p&gt;16/11/14 20:38:16 INFO execution.SparkSqlParser: Parsing command: track_liked&lt;br/&gt;
16/11/14 20:38:16 INFO metastore.HiveMetaStore: 0: get_table : db=default tbl=track_liked&lt;/p&gt;</comment>
                            <comment id="15663828" author="cjuexuan" created="Mon, 14 Nov 2016 12:45:13 +0000"  >&lt;p&gt;when I init sparkSession ,throw this error &lt;/p&gt;</comment>
                            <comment id="15665599" author="cloud_fan" created="Tue, 15 Nov 2016 00:59:03 +0000"  >&lt;p&gt;ah, it&apos;s not throwing an exception, but logging an error message. When we try to create the default database, we ask hive to do nothing if it already exists. However, Hive will log an error message instead of doing nothing. This should be fine and you can just ignore it.&lt;/p&gt;</comment>
                            <comment id="15665655" author="cjuexuan" created="Tue, 15 Nov 2016 01:22:21 +0000"  >&lt;p&gt;thanks&lt;/p&gt;</comment>
                            <comment id="15686930" author="alexliu68" created="Tue, 22 Nov 2016 14:48:04 +0000"  >&lt;p&gt;This error logging msg is very annoying. Is there any workaround to not display it at all?&lt;/p&gt;</comment>
                            <comment id="15690117" author="apachespark" created="Wed, 23 Nov 2016 13:36:05 +0000"  >&lt;p&gt;User &apos;cloud-fan&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/15993&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/15993&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 51 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i357vj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12335644">2.1.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>