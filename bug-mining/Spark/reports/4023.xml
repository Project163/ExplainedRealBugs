<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:47:22 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-17463] Serialization of accumulators in heartbeats is not thread-safe</title>
                <link>https://issues.apache.org/jira/browse/SPARK-17463</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Check out the following &lt;tt&gt;ConcurrentModificationException&lt;/tt&gt;:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
16/09/06 16:10:29 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(2,[Lscala.Tuple2;@66e7b6e7,BlockManagerId(2, HOST, 57743))] in 1 attempts
org.apache.spark.SparkException: Exception thrown in awaitResult
    at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:77)
    at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:75)
    at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
    at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
    at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
    at scala.PartialFunction$OrElse.apply(PartialFunction.scala:162)
    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:83)
    at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:102)
    at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:518)
    at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547)
    at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547)
    at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547)
    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1862)
    at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
Caused by: java.util.ConcurrentModificationException
    at java.util.ArrayList.writeObject(ArrayList.java:766)
    at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)
    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
    at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
    at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
    at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
    at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
    at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
    at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
    at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
    at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
    at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)
    at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
    at org.apache.spark.rpc.netty.NettyRpcEnv.serialize(NettyRpcEnv.scala:253)
    at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:227)
    at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:508)
    at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)
    ... 13 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Even though accumulators aren&apos;t thread-safe they can be concurrently read while serializing executor heartbeats and modified while tasks are running, leading to ConcurrentModificationException errors (thereby leading to missing heartbeats) or leading to inconsistent data (since individual fields of a multi-field object might be serialized at different points in time, leading to inconsistencies in accumulators like LongAccum).&lt;/p&gt;

&lt;p&gt;This seems like a pretty serious issue but I&apos;m not sure what&apos;s the best way to fix this. An obvious fix would be to properly synchronize all accesses to the fields of our accumulators and to synchronize the writeObject and writeKryo methods, but this may have an adverse performance impact&lt;/p&gt;</description>
                <environment></environment>
        <key id="13003731">SPARK-17463</key>
            <summary>Serialization of accumulators in heartbeats is not thread-safe</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="zsxwing">Shixiong Zhu</assignee>
                                    <reporter username="joshrosen">Josh Rosen</reporter>
                        <labels>
                    </labels>
                <created>Fri, 9 Sep 2016 01:15:39 +0000</created>
                <updated>Wed, 17 May 2017 23:23:55 +0000</updated>
                            <resolved>Thu, 6 Oct 2016 20:46:30 +0000</resolved>
                                    <version>2.0.0</version>
                                    <fixVersion>2.0.1</fixVersion>
                    <fixVersion>2.1.0</fixVersion>
                                    <component>Spark Core</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>9</watches>
                                                                                                                <comments>
                            <comment id="15475536" author="joshrosen" created="Fri, 9 Sep 2016 01:16:13 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zsxwing&quot; class=&quot;user-hover&quot; rel=&quot;zsxwing&quot;&gt;zsxwing&lt;/a&gt;, FYI, since you&apos;re good at these types of RPC thread-safety issues.&lt;/p&gt;</comment>
                            <comment id="15484720" author="zsxwing" created="Mon, 12 Sep 2016 17:33:08 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=joshrosen&quot; class=&quot;user-hover&quot; rel=&quot;joshrosen&quot;&gt;joshrosen&lt;/a&gt; I think we can just leave LongAccum as it is. It&apos;s no worse than Spark 1.6. In Spark 1.6, `sum` and `count` are different accumulators and have the same inconsistent issue.&lt;/p&gt;

&lt;p&gt;We definitely should fix CollectionAccumulator and SetAccumulator. I will submit a PR to add the necessary `synchronized` for them.&lt;/p&gt;

&lt;p&gt;By the way, I didn&apos;t notice that AccumulatorV2 sends the whole object back to the driver. Do you know any special reason? I remember previously we only send the values of accumulators back to driver.&lt;/p&gt;</comment>
                            <comment id="15484769" author="apachespark" created="Mon, 12 Sep 2016 17:49:11 +0000"  >&lt;p&gt;User &apos;zsxwing&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/15063&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/15063&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15485420" author="apachespark" created="Mon, 12 Sep 2016 21:59:05 +0000"  >&lt;p&gt;User &apos;zsxwing&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/15065&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/15065&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15491393" author="joshrosen" created="Wed, 14 Sep 2016 20:34:42 +0000"  >&lt;p&gt;Issue resolved by pull request 15063&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/15063&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/15063&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15550398" author="eseyfe" created="Thu, 6 Oct 2016 00:23:36 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zsxwing&quot; class=&quot;user-hover&quot; rel=&quot;zsxwing&quot;&gt;zsxwing&lt;/a&gt;,&lt;/p&gt;

&lt;p&gt;I think this bug is not fully fixed. I still get ConcurrentModificationException. &lt;/p&gt;

&lt;p&gt;JsonProtocol.scala:314 has below statement which is causing the problem:&lt;br/&gt;
          JArray(v.asInstanceOf[java.util.List&lt;span class=&quot;error&quot;&gt;&amp;#91;(BlockId, BlockStatus)&amp;#93;&lt;/span&gt;].asScala.toList.map {&lt;/p&gt;

&lt;p&gt;Adding synchronized keyword should fix the issue. What do you think?&lt;/p&gt;

&lt;p&gt;This is the stack trace:&lt;br/&gt;
java.util.ConcurrentModificationException&lt;br/&gt;
        at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:901)&lt;br/&gt;
        at java.util.ArrayList$Itr.next(ArrayList.java:851)&lt;br/&gt;
        at scala.collection.convert.Wrappers$JIteratorWrapper.next(Wrappers.scala:43)&lt;br/&gt;
        at scala.collection.Iterator$class.foreach(Iterator.scala:893)&lt;br/&gt;
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)&lt;br/&gt;
        at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&lt;br/&gt;
        at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&lt;br/&gt;
        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)&lt;br/&gt;
        at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:183)&lt;br/&gt;
        at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:45)&lt;br/&gt;
        at scala.collection.TraversableLike$class.to(TraversableLike.scala:590)&lt;br/&gt;
        at scala.collection.AbstractTraversable.to(Traversable.scala:104)&lt;br/&gt;
        at scala.collection.TraversableOnce$class.toList(TraversableOnce.scala:294)&lt;br/&gt;
        at scala.collection.AbstractTraversable.toList(Traversable.scala:104)&lt;br/&gt;
        at org.apache.spark.util.JsonProtocol$.accumValueToJson(JsonProtocol.scala:314)&lt;br/&gt;
        at org.apache.spark.util.JsonProtocol$$anonfun$accumulableInfoToJson$5.apply(JsonProtocol.scala:291)&lt;br/&gt;
        at org.apache.spark.util.JsonProtocol$$anonfun$accumulableInfoToJson$5.apply(JsonProtocol.scala:291)&lt;br/&gt;
        at scala.Option.map(Option.scala:146)&lt;br/&gt;
        at org.apache.spark.util.JsonProtocol$.accumulableInfoToJson(JsonProtocol.scala:291)&lt;br/&gt;
        at org.apache.spark.util.JsonProtocol$$anonfun$taskInfoToJson$12.apply(JsonProtocol.scala:283)&lt;br/&gt;
        at org.apache.spark.util.JsonProtocol$$anonfun$taskInfoToJson$12.apply(JsonProtocol.scala:283)&lt;br/&gt;
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&lt;br/&gt;
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&lt;br/&gt;
        at scala.collection.immutable.List.foreach(List.scala:381)&lt;br/&gt;
        at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35)&lt;br/&gt;
        at scala.collection.mutable.ListBuffer.foreach(ListBuffer.scala:45)&lt;br/&gt;
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&lt;br/&gt;
        at scala.collection.AbstractTraversable.map(Traversable.scala:104)&lt;br/&gt;
        at org.apache.spark.util.JsonProtocol$.taskInfoToJson(JsonProtocol.scala:283)&lt;br/&gt;
        at org.apache.spark.util.JsonProtocol$.taskEndToJson(JsonProtocol.scala:145)&lt;br/&gt;
        at org.apache.spark.util.JsonProtocol$.sparkEventToJson(JsonProtocol.scala:76)&lt;br/&gt;
        at org.apache.spark.scheduler.EventLoggingListener.logEvent(EventLoggingListener.scala:137)&lt;br/&gt;
        at org.apache.spark.scheduler.EventLoggingListener.onTaskEnd(EventLoggingListener.scala:157)&lt;br/&gt;
        at org.apache.spark.scheduler.SparkListenerBus$class.doPostEvent(SparkListenerBus.scala:45)&lt;br/&gt;
        at org.apache.spark.scheduler.LiveListenerBus.doPostEvent(LiveListenerBus.scala:35)&lt;br/&gt;
        at org.apache.spark.scheduler.LiveListenerBus.doPostEvent(LiveListenerBus.scala:35)&lt;br/&gt;
        at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:63)&lt;br/&gt;
        at org.apache.spark.scheduler.LiveListenerBus.postToAll(LiveListenerBus.scala:35)&lt;br/&gt;
        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(LiveListenerBus.scala:81)&lt;br/&gt;
        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:66)&lt;br/&gt;
        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:66)&lt;br/&gt;
        at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)&lt;br/&gt;
        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(LiveListenerBus.scala:65)&lt;br/&gt;
        at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1244)&lt;br/&gt;
        at org.apache.spark.scheduler.LiveListenerBus$$anon$1.run(LiveListenerBus.scala:64)&lt;/p&gt;</comment>
                            <comment id="15550558" author="eseyfe" created="Thu, 6 Oct 2016 01:39:47 +0000"  >&lt;p&gt;Reopening since it still  throw java.util.ConcurrentModificationException&lt;/p&gt;</comment>
                            <comment id="15550561" author="apachespark" created="Thu, 6 Oct 2016 01:41:05 +0000"  >&lt;p&gt;User &apos;seyfe&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/15371&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/15371&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15553134" author="zsxwing" created="Thu, 6 Oct 2016 20:45:50 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=eseyfe&quot; class=&quot;user-hover&quot; rel=&quot;eseyfe&quot;&gt;eseyfe&lt;/a&gt; It&apos;s a different issue. Could you create a new ticket, please?&lt;/p&gt;</comment>
                            <comment id="15553136" author="zsxwing" created="Thu, 6 Oct 2016 20:46:30 +0000"  >&lt;p&gt;It&apos;s already fixed. The issue in the comment is another one.&lt;/p&gt;</comment>
                            <comment id="15565493" author="harishk15" created="Tue, 11 Oct 2016 14:02:54 +0000"  >&lt;p&gt;It looks like a show stopper for my current project. Can you please let me know the 2.1.0 release date or do we have same issue in 1.6.0/1/2 ? So that i can revert back to 1.6.&lt;/p&gt;</comment>
                            <comment id="15566104" author="srowen" created="Tue, 11 Oct 2016 18:01:23 +0000"  >&lt;p&gt;What do you mean? this has been released already in 2.0.1.&lt;/p&gt;</comment>
                            <comment id="15566206" author="harishk15" created="Tue, 11 Oct 2016 18:37:34 +0000"  >&lt;p&gt;Is this fix is part of the &lt;a href=&quot;https://github.com/apache/spark/pull/15371&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/15371&lt;/a&gt; pull request?. I have 2.0.1 in my cluster but i am getting both the errors.&lt;/p&gt;

&lt;p&gt;16/10/11 00:53:42 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(2,&lt;span class=&quot;error&quot;&gt;&amp;#91;Lscala.Tuple2;@43f45f95,BlockManagerId(2, HOST, 43256))&amp;#93;&lt;/span&gt; in 1 attempts&lt;br/&gt;
org.apache.spark.SparkException: Exception thrown in awaitResult&lt;br/&gt;
	at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:77)&lt;br/&gt;
	at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:75)&lt;br/&gt;
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)&lt;br/&gt;
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)&lt;br/&gt;
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)&lt;br/&gt;
	at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167)&lt;br/&gt;
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:83)&lt;br/&gt;
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:102)&lt;br/&gt;
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:518)&lt;br/&gt;
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547)&lt;br/&gt;
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547)&lt;br/&gt;
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547)&lt;br/&gt;
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1877)&lt;br/&gt;
	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547)&lt;br/&gt;
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)&lt;br/&gt;
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)&lt;br/&gt;
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)&lt;br/&gt;
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
Caused by: java.util.ConcurrentModificationException&lt;br/&gt;
	at java.util.ArrayList.writeObject(ArrayList.java:766)&lt;br/&gt;
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:498)&lt;br/&gt;
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)&lt;br/&gt;
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)&lt;br/&gt;
	at java.io.ObjectOutputStream.defaultWriteObject(ObjectOutputStream.java:441)&lt;br/&gt;
	at java.util.Collections$SynchronizedCollection.writeObject(Collections.java:2081)&lt;br/&gt;
	at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:498)&lt;br/&gt;
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)&lt;br/&gt;
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)&lt;br/&gt;
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)&lt;br/&gt;
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)&lt;br/&gt;
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)&lt;br/&gt;
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)&lt;br/&gt;
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)&lt;br/&gt;
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)&lt;br/&gt;
	at org.apache.spark.rpc.netty.NettyRpcEnv.serialize(NettyRpcEnv.scala:253)&lt;br/&gt;
	at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:227)&lt;br/&gt;
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:508)&lt;br/&gt;
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)&lt;br/&gt;
	... 13 more&lt;/p&gt;</comment>
                            <comment id="15566299" author="srowen" created="Tue, 11 Oct 2016 19:09:58 +0000"  >&lt;p&gt;No, that change came after, and is part of a different JIRA that addresses another part of the same problem. It is not in 2.0.1&lt;/p&gt;</comment>
                            <comment id="15566300" author="srowen" created="Tue, 11 Oct 2016 19:10:06 +0000"  >&lt;p&gt;No, that change came after, and is part of a different JIRA that addresses another part of the same problem. It is not in 2.0.1&lt;/p&gt;</comment>
                            <comment id="15566391" author="zsxwing" created="Tue, 11 Oct 2016 19:48:46 +0000"  >&lt;p&gt;Do you have a reproducer? I saw `at java.util.Collections$SynchronizedCollection.writeObject(Collections.java:2081)` in the stack trace, so I think the internal ArrayList is accessed in some place. Did you use `collectionAccumulator` in your codes?&lt;/p&gt;

&lt;p&gt;FYI,  &lt;a href=&quot;https://github.com/apache/spark/pull/15371&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/15371&lt;/a&gt; is for &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-17816&quot; title=&quot;Json serialzation of accumulators are failing with ConcurrentModificationException&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-17816&quot;&gt;&lt;del&gt;SPARK-17816&lt;/del&gt;&lt;/a&gt; which fixes an issue in driver.&lt;/p&gt;</comment>
                            <comment id="15566413" author="harishk15" created="Tue, 11 Oct 2016 19:56:16 +0000"  >&lt;p&gt;Ok. thanks for the update. Do we have any work around for the second part of the issue? I tried this set(&quot;spark.rpc.netty.dispatcher.numThreads&quot;,&quot;2&quot;) but no luck&lt;/p&gt;</comment>
                            <comment id="15566427" author="harishk15" created="Tue, 11 Oct 2016 20:03:55 +0000"  >&lt;p&gt;No i dont have any code like that. I use pyspark .. Please find my code snippet&lt;br/&gt;
df1 with 60 columns (70M records)&lt;br/&gt;
df2  with 3000-7000 (varies) columns (10M)&lt;br/&gt;
join df1 and df2 with key columns (please note df1 is more granular data and df2 one level above. So data set will grow&lt;/p&gt;

&lt;p&gt;df3 = df1.join(df2, &lt;span class=&quot;error&quot;&gt;&amp;#91;keys&amp;#93;&lt;/span&gt;)&lt;br/&gt;
aggList = &lt;span class=&quot;error&quot;&gt;&amp;#91;func.mean(col).alias(col + &amp;#39;_m&amp;#39;) for col in df2.columns&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Last part is i do &amp;#8211; df4 = df3.groupBy(keys).agg(*aggList) --I applying mean to each column of the df3 data frame which might be 3000-10000 columns.&lt;br/&gt;
Let me know if you need entire stack trace of this issue.&lt;/p&gt;

&lt;p&gt;PS: We still have issue &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-16845&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/SPARK-16845&lt;/a&gt; &amp;#8211; So i have to break number of columns 500 chunks&lt;/p&gt;

</comment>
                            <comment id="15566515" author="harishk15" created="Tue, 11 Oct 2016 20:33:47 +0000"  >&lt;p&gt;My second approach was:&lt;br/&gt;
def testfunc(keys, vals, columnsToStandardize):&lt;br/&gt;
           df= pd.DataFrame(vals, columns = keys)&lt;br/&gt;
           df&lt;span class=&quot;error&quot;&gt;&amp;#91;columnsToStandardize&amp;#93;&lt;/span&gt; = df&lt;span class=&quot;error&quot;&gt;&amp;#91;columnsToStandardize&amp;#93;&lt;/span&gt; - df&lt;span class=&quot;error&quot;&gt;&amp;#91;columnsToStandardize&amp;#93;&lt;/span&gt;.mean() &lt;/p&gt;

&lt;p&gt;df3.rdd.map(keys).groupByKey().flatMap(lambda keyval: testfunc(keys&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;, keys&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;, columnsToStandardize))&lt;/p&gt;</comment>
                            <comment id="15589729" author="zsxwing" created="Wed, 19 Oct 2016 20:05:18 +0000"  >&lt;p&gt;I could not figure out the cause. Is it easy to reproduce in your query? Hope you can create a reproducer for us to debug.&lt;/p&gt;</comment>
                            <comment id="15646453" author="harishk15" created="Tue, 8 Nov 2016 04:26:10 +0000"  >&lt;p&gt;I was able to figure out the issue, its  not related to this bug. &lt;/p&gt;</comment>
                            <comment id="15787809" author="sunil.rangwani" created="Fri, 30 Dec 2016 14:52:12 +0000"  >&lt;p&gt;Hi &lt;br/&gt;
I get the same error with Spark 2.0.2 on EMR 5.2.0&lt;br/&gt;
I am using the CollectionAccumulator and I get the same stack trace. Hard to say from the stack trace what is the root cause. &lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;16/12/30 13:49:11 WARN NettyRpcEndpointRef: Error sending message [message = Heartbeat(1,[Lscala.Tuple2;@63f6e6d6,BlockManagerId(1, &amp;lt;Hostname snipped&amp;gt;, 45163))] in 3 attempts
org.apache.spark.SparkException: Exception thrown in awaitResult
	at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:77)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:75)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:83)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:102)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:518)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953)
	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.ConcurrentModificationException
	at java.util.ArrayList.writeObject(ArrayList.java:766)
	at sun.reflect.GeneratedMethodAccessor55.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.defaultWriteObject(ObjectOutputStream.java:441)
	at java.util.Collections$SynchronizedCollection.writeObject(Collections.java:2081)
	at sun.reflect.GeneratedMethodAccessor54.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.rpc.netty.NettyRpcEnv.serialize(NettyRpcEnv.scala:253)
	at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:227)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:508)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)
	... 13 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15788152" author="zsxwing" created="Fri, 30 Dec 2016 18:48:23 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sunil.rangwani&quot; class=&quot;user-hover&quot; rel=&quot;sunil.rangwani&quot;&gt;sunil.rangwani&lt;/a&gt; Could you provide the codes that use CollectionAccumulator?&lt;/p&gt;</comment>
                            <comment id="15791469" author="sunil.rangwani" created="Sun, 1 Jan 2017 18:20:32 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zsxwing&quot; class=&quot;user-hover&quot; rel=&quot;zsxwing&quot;&gt;zsxwing&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Below is how I am using the Collection accumulator. &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-comment&quot;&gt;// val spark: SparkSession
&lt;/span&gt;&lt;span class=&quot;code-comment&quot;&gt;// val dataFrame: DataFrame
&lt;/span&gt;val updatedRecordKeysAcc = spark.sparkContext.collectionAccumulator[&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;]

dataFrame.toJSON.foreach(processRecord(_, updatedRecordKeysAcc))

&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; collection.JavaConverters._
val updatedRecordKeys: &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; = updatedRecordKeysAcc.value.asScala.mkString(&lt;span class=&quot;code-quote&quot;&gt;&quot;\n&quot;&lt;/span&gt;)
&lt;span class=&quot;code-comment&quot;&gt;// write `updatedRecordKeys` to an S3 file using aws-sdk... 
&lt;/span&gt;
def processRecord(recordJSON: &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;, updatedRecordKeysAcc: CollectionAccumulator) {
	&lt;span class=&quot;code-comment&quot;&gt;// &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; Stuff with recordJSON... 
&lt;/span&gt;	&lt;span class=&quot;code-comment&quot;&gt;// val recordKey: &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; = getKeyFromJSON(recordJSON)
&lt;/span&gt;	updatedRecordKeysAcc.add(recordKey)
}

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It works when I am working with smaller datasets of 10s of 1000s but when I try to run it with a dataset of 10s of millions, it fails with the exception above. I tried to scale up the cluster to 5 to 6 nodes but it still gives the same error and the cluster remains underutilized.  &lt;br/&gt;
It also works alright when I comment out the line &lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;updatedRecordKeysAcc.add(recordKey)&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15800979" author="sunil.rangwani" created="Thu, 5 Jan 2017 10:11:56 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zsxwing&quot; class=&quot;user-hover&quot; rel=&quot;zsxwing&quot;&gt;zsxwing&lt;/a&gt; Do you agree this bug should be reopened?&lt;/p&gt;</comment>
                            <comment id="15813205" author="zsxwing" created="Mon, 9 Jan 2017 23:31:12 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sunil.rangwani&quot; class=&quot;user-hover&quot; rel=&quot;sunil.rangwani&quot;&gt;sunil.rangwani&lt;/a&gt; could you have a simple reproducer? I ran the following codes on both 2.0.2 and the latest master and could not reproduce it.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.spark.util.CollectionAccumulator

val updatedRecordKeysAcc = spark.sparkContext.collectionAccumulator[&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;]

def processRecord(recordJSON: &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;, updatedRecordKeysAcc: CollectionAccumulator[&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;]) {
	updatedRecordKeysAcc.add((recordJSON.toInt percent 100000).toString)
}

&lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (_ &amp;lt;- 0 until 100) {
  spark.range(1, 10000000).map(_.toString).foreach(processRecord(_, updatedRecordKeysAcc))
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This may be an EMR Spark only issue.&lt;/p&gt;</comment>
                            <comment id="15821106" author="sunil.rangwani" created="Thu, 12 Jan 2017 14:25:35 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zsxwing&quot; class=&quot;user-hover&quot; rel=&quot;zsxwing&quot;&gt;zsxwing&lt;/a&gt; &lt;br/&gt;
My recordKey that I add to updatedRecordKeysAcc is a string that is 102 characters long. Other than that its similar to you are doing in your code. I currently don&apos;t have a way to reproduce this on a non-EMR environment. Are you able to try this out please with a longer recordKey to see if you&apos;re able to reproduce it. &lt;/p&gt;</comment>
                            <comment id="16014564" author="allengeorge" created="Wed, 17 May 2017 18:31:21 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zsxwing&quot; class=&quot;user-hover&quot; rel=&quot;zsxwing&quot;&gt;zsxwing&lt;/a&gt; So, we&apos;re hitting this problem on spark 2.0.2. One thing I noticed while poking around &lt;tt&gt;AccumulatorsV2&lt;/tt&gt; is that not all accesses are locked. For example &lt;a href=&quot;https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala#L473&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;setValue&lt;/a&gt;&lt;br/&gt;
 and &lt;a href=&quot;https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala#L461&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;add&lt;/a&gt;. Would that be the cause at all? &lt;/p&gt;</comment>
                            <comment id="16014799" author="zsxwing" created="Wed, 17 May 2017 21:24:01 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=allengeorge&quot; class=&quot;user-hover&quot; rel=&quot;allengeorge&quot;&gt;allengeorge&lt;/a&gt; it&apos;s protected by `Collections.synchronizedList(new ArrayList&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;())`.&lt;/p&gt;</comment>
                            <comment id="16014930" author="allengeorge" created="Wed, 17 May 2017 23:23:55 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zsxwing&quot; class=&quot;user-hover&quot; rel=&quot;zsxwing&quot;&gt;zsxwing&lt;/a&gt; You&apos;re absolutely right: my apologies for the PEBCAK.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 26 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i33f6n:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>