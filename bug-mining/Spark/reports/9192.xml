<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 19:34:17 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-43781] IllegalStateException when cogrouping two datasets derived from the same source</title>
                <link>https://issues.apache.org/jira/browse/SPARK-43781</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Attempting to &lt;tt&gt;cogroup&lt;/tt&gt; two datasets derived from the same source dataset yields an &lt;tt&gt;IllegalStateException&lt;/tt&gt; when the query is executed.&lt;/p&gt;

&lt;p&gt;Minimal reproducer:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
StructType inputType = DataTypes.createStructType(
    &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; StructField[]{
        DataTypes.createStructField(&lt;span class=&quot;code-quote&quot;&gt;&quot;id&quot;&lt;/span&gt;, DataTypes.LongType, &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;),
        DataTypes.createStructField(&lt;span class=&quot;code-quote&quot;&gt;&quot;type&quot;&lt;/span&gt;, DataTypes.StringType, &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;)
    }
);

StructType keyType = DataTypes.createStructType(
    &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; StructField[]{
        DataTypes.createStructField(&lt;span class=&quot;code-quote&quot;&gt;&quot;id&quot;&lt;/span&gt;, DataTypes.LongType, &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;)
    }
);

List&amp;lt;Row&amp;gt; inputRows = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ArrayList&amp;lt;&amp;gt;();
inputRows.add(RowFactory.create(1L, &lt;span class=&quot;code-quote&quot;&gt;&quot;foo&quot;&lt;/span&gt;));
inputRows.add(RowFactory.create(1L, &lt;span class=&quot;code-quote&quot;&gt;&quot;bar&quot;&lt;/span&gt;));
inputRows.add(RowFactory.create(2L, &lt;span class=&quot;code-quote&quot;&gt;&quot;foo&quot;&lt;/span&gt;));
Dataset&amp;lt;Row&amp;gt; input = sparkSession.createDataFrame(inputRows, inputType);

KeyValueGroupedDataset&amp;lt;Row, Row&amp;gt; fooGroups = input
    .filter(&lt;span class=&quot;code-quote&quot;&gt;&quot;type = &lt;span class=&quot;code-quote&quot;&gt;&apos;foo&apos;&lt;/span&gt;&quot;&lt;/span&gt;)
    .groupBy(&lt;span class=&quot;code-quote&quot;&gt;&quot;id&quot;&lt;/span&gt;)
    .as(RowEncoder.apply(keyType), RowEncoder.apply(inputType));

KeyValueGroupedDataset&amp;lt;Row, Row&amp;gt; barGroups = input
    .filter(&lt;span class=&quot;code-quote&quot;&gt;&quot;type = &lt;span class=&quot;code-quote&quot;&gt;&apos;bar&apos;&lt;/span&gt;&quot;&lt;/span&gt;)
    .groupBy(&lt;span class=&quot;code-quote&quot;&gt;&quot;id&quot;&lt;/span&gt;)
    .as(RowEncoder.apply(keyType), RowEncoder.apply(inputType));

Dataset&amp;lt;Row&amp;gt; result = fooGroups.cogroup(
    barGroups,
    (CoGroupFunction&amp;lt;Row, Row, Row, Row&amp;gt;) (row, iterator, iterator1) -&amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ArrayList&amp;lt;Row&amp;gt;().iterator(),
    RowEncoder.apply(inputType));

result.explain();
result.show();&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Explain output (note mismatch in column IDs between Sort/Exchagne and LocalTableScan on the first input to the CoGroup):&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;
+- SerializeFromObject [validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;]), 0, id), LongType, &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;) AS id#37L, staticinvoke(&lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;]), 1, type), StringType, &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;), &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;, &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;) AS type#38]
&#160;&#160; +- CoGroup org.apache.spark.sql.KeyValueGroupedDataset$$Lambda$1478/1869116781@77856cc5, createexternalrow(id#16L, StructField(id,LongType,&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;)), createexternalrow(id#16L, type#17.toString, StructField(id,LongType,&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;), StructField(type,StringType,&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;)), createexternalrow(id#16L, type#17.toString, StructField(id,LongType,&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;), StructField(type,StringType,&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;)), [id#39L], [id#39L], [id#39L, type#40], [id#39L, type#40], obj#36: org.apache.spark.sql.Row
&#160; &#160; &#160; :- !Sort [id#39L ASC NULLS FIRST], &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;, 0
&#160; &#160; &#160; :&#160; +- !Exchange hashpartitioning(id#39L, 2), ENSURE_REQUIREMENTS, [plan_id=19]
&#160; &#160; &#160; : &#160; &#160; +- LocalTableScan [id#16L, type#17]
&#160; &#160; &#160; +- Sort [id#39L ASC NULLS FIRST], &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;, 0
&#160;&#160; &#160; &#160; &#160; +- Exchange hashpartitioning(id#39L, 2), ENSURE_REQUIREMENTS, [plan_id=20]
&#160; &#160; &#160; &#160; &#160; &#160; +- LocalTableScan [id#39L, type#40]&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Exception:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
java.lang.IllegalStateException: Couldn&apos;t find id#39L in [id#16L,type#17]
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:80)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:73)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:589)
&#160; &#160; &#160; &#160; at scala.collection.immutable.ArraySeq.map(ArraySeq.scala:75)
&#160; &#160; &#160; &#160; at scala.collection.immutable.ArraySeq.map(ArraySeq.scala:35)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:698)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:589)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:589)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1254)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1253)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.expressions.BinaryExpression.mapChildren(Expression.scala:608)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:589)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:528)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:73)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.expressions.BindReferences$.$anonfun$bindReferences$1(BoundAttribute.scala:94)
&#160; &#160; &#160; &#160; at scala.collection.immutable.List.map(List.scala:246)
&#160; &#160; &#160; &#160; at scala.collection.immutable.List.map(List.scala:79)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReferences(BoundAttribute.scala:94)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.expressions.UnsafeProjection$.create(Projection.scala:160)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.getPartitionKeyExtractor$1(ShuffleExchangeExec.scala:323)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$13(ShuffleExchangeExec.scala:391)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$13$adapted(ShuffleExchangeExec.scala:390)
&#160; &#160; &#160; &#160; at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:877)
&#160; &#160; &#160; &#160; at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:877)
&#160; &#160; &#160; &#160; at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
&#160; &#160; &#160; &#160; at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
&#160; &#160; &#160; &#160; at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
&#160; &#160; &#160; &#160; at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
&#160; &#160; &#160; &#160; at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
&#160; &#160; &#160; &#160; at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
&#160; &#160; &#160; &#160; at org.apache.spark.scheduler.Task.run(Task.scala:136)
&#160; &#160; &#160; &#160; at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
&#160; &#160; &#160; &#160; at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
&#160; &#160; &#160; &#160; at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
&#160; &#160; &#160; &#160; at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
&#160; &#160; &#160; &#160; at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
&#160; &#160; &#160; &#160; at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:748) &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Other observations:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;The same code works if I call &lt;tt&gt;createDataFrame()&lt;/tt&gt; twice and use two separate datasets as input to the cogroup.&lt;/li&gt;
	&lt;li&gt;The real code uses two different filters on the same cached dataset as the two inputs to the cogroup. However, this results in the same exception, and the same apparent error in the physical plan, which looks as follows:
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;
+- SerializeFromObject [validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;]), 0, id), LongType, &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;) AS id#47L, staticinvoke(&lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;]), 1, type), StringType, &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;), &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;, &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;) AS type#48]
&#160;&#160; +- CoGroup org.apache.spark.sql.KeyValueGroupedDataset$$Lambda$1526/693211959@7b2e931, createexternalrow(id#16L, StructField(id,LongType,&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;)), createexternalrow(id#16L, type#17.toString, StructField(id,LongType,&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;), StructField(type,StringType,&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;)), createexternalrow(id#16L, type#17.toString, StructField(id,LongType,&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;), StructField(type,StringType,&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;)), [id#49L], [id#49L], [id#49L, type#50], [id#49L, type#50], obj#46: org.apache.spark.sql.Row
&#160; &#160; &#160; :- !Sort [id#49L ASC NULLS FIRST], &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;, 0
&#160; &#160; &#160; :&#160; +- !Exchange hashpartitioning(id#49L, 2), ENSURE_REQUIREMENTS, [plan_id=26]
&#160; &#160; &#160; : &#160; &#160; +- Filter (type#17 = foo)
&#160; &#160; &#160; :&#160; &#160; &#160; &#160; +- InMemoryTableScan [id#16L, type#17], [(type#17 = foo)]
&#160; &#160; &#160; :&#160; &#160; &#160; &#160; &#160; &#160; &#160; +- InMemoryRelation [id#16L, type#17], StorageLevel(disk, memory, deserialized, 1 replicas)
&#160; &#160; &#160; :&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; +- LocalTableScan [id#16L, type#17]
&#160; &#160; &#160; +- Sort [id#49L ASC NULLS FIRST], &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;, 0
&#160;&#160; &#160; &#160; &#160; +- Exchange hashpartitioning(id#49L, 2), ENSURE_REQUIREMENTS, [plan_id=27]
&#160; &#160; &#160; &#160; &#160; &#160; +- Filter (type#50 = bar)
&#160;&#160; &#160; &#160; &#160; &#160; &#160; &#160; +- InMemoryTableScan [id#49L, type#50], [(type#50 = bar)]
&#160;&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; +- InMemoryRelation [id#49L, type#50], StorageLevel(disk, memory, deserialized, 1 replicas)
&#160;&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; +- LocalTableScan [id#16L, type#17] &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;The issue doesn&apos;t arise if I write the same code in PySpark, using&#160;&lt;tt&gt;FlatMapCoGroupsInPandas&lt;/tt&gt;.&lt;/li&gt;
&lt;/ul&gt;
</description>
                <environment>&lt;p&gt;Reproduces in a unit test, using Spark 3.3.1, the Java API, and a &lt;tt&gt;local&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;&lt;/tt&gt; SparkSession.&lt;/p&gt;</environment>
        <key id="13537559">SPARK-43781</key>
            <summary>IllegalStateException when cogrouping two datasets derived from the same source</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="fanjia">Jia Fan</assignee>
                                    <reporter username="mrry">Derek Murray</reporter>
                        <labels>
                    </labels>
                <created>Wed, 24 May 2023 22:58:29 +0000</created>
                <updated>Wed, 26 Jun 2024 01:46:08 +0000</updated>
                            <resolved>Fri, 11 Aug 2023 02:19:19 +0000</resolved>
                                    <version>3.3.1</version>
                    <version>3.4.0</version>
                                    <fixVersion>4.0.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="17742325" author="githubbot" created="Wed, 12 Jul 2023 09:04:06 +0000"  >&lt;p&gt;User &apos;Hisoka-X&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/41554&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/41554&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310660">
                    <name>Completes</name>
                                            <outwardlinks description="fixes">
                                        <issuelink>
            <issuekey id="13520424">SPARK-42132</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                            <outwardlinks description="duplicates">
                                        <issuelink>
            <issuekey id="13520424">SPARK-42132</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            2 years, 17 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z1i4io:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>