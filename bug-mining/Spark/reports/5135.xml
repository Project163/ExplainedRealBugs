<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:55:22 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-21418] NoSuchElementException: None.get in DataSourceScanExec with sun.io.serialization.extendedDebugInfo=true</title>
                <link>https://issues.apache.org/jira/browse/SPARK-21418</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;I don&apos;t have a minimal reproducible example yet, sorry. I have the following lines in a unit test for our Spark application:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;val df = mySparkSession.read.format(&lt;span class=&quot;code-quote&quot;&gt;&quot;jdbc&quot;&lt;/span&gt;)
  .options(Map(&lt;span class=&quot;code-quote&quot;&gt;&quot;url&quot;&lt;/span&gt; -&amp;gt; url, &lt;span class=&quot;code-quote&quot;&gt;&quot;dbtable&quot;&lt;/span&gt; -&amp;gt; &lt;span class=&quot;code-quote&quot;&gt;&quot;test_table&quot;&lt;/span&gt;))
  .load()
df.show
println(df.rdd.collect)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The output shows the DataFrame contents from &lt;tt&gt;df.show&lt;/tt&gt;. But the &lt;tt&gt;collect&lt;/tt&gt; fails:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.util.NoSuchElementException: None.get
java.util.NoSuchElementException: None.get
  at scala.None$.get(Option.scala:347)
  at scala.None$.get(Option.scala:345)
  at org.apache.spark.sql.execution.DataSourceScanExec$class.org$apache$spark$sql$execution$DataSourceScanExec$$redact(DataSourceScanExec.scala:70)
  at org.apache.spark.sql.execution.DataSourceScanExec$$anonfun$4.apply(DataSourceScanExec.scala:54)
  at org.apache.spark.sql.execution.DataSourceScanExec$$anonfun$4.apply(DataSourceScanExec.scala:52)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.AbstractTraversable.map(Traversable.scala:104)
  at org.apache.spark.sql.execution.DataSourceScanExec$class.simpleString(DataSourceScanExec.scala:52)
  at org.apache.spark.sql.execution.RowDataSourceScanExec.simpleString(DataSourceScanExec.scala:75)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.verboseString(QueryPlan.scala:349)
  at org.apache.spark.sql.execution.RowDataSourceScanExec.org$apache$spark$sql$execution$DataSourceScanExec$$super$verboseString(DataSourceScanExec.scala:75)
  at org.apache.spark.sql.execution.DataSourceScanExec$class.verboseString(DataSourceScanExec.scala:60)
  at org.apache.spark.sql.execution.RowDataSourceScanExec.verboseString(DataSourceScanExec.scala:75)
  at org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:556)
  at org.apache.spark.sql.execution.WholeStageCodegenExec.generateTreeString(WholeStageCodegenExec.scala:451)
  at org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:576)
  at org.apache.spark.sql.catalyst.trees.TreeNode.treeString(TreeNode.scala:480)
  at org.apache.spark.sql.catalyst.trees.TreeNode.treeString(TreeNode.scala:477)
  at org.apache.spark.sql.catalyst.trees.TreeNode.toString(TreeNode.scala:474)
  at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1421)
  at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
  at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
  at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
  at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
  at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
  at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
  at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
  at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
  at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
  at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
  at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
  at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
  at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
  at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
  at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
  at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
  at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
  at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
  at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
  at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
  at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
  at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
  at scala.collection.immutable.List$SerializationProxy.writeObject(List.scala:468)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:498)
  at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)
  at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
  at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
  at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
  at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
  at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
  at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
  at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
  at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
  at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
  at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
  at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
  at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
  at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)
  at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
  at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1003)
  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:930)
  at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:874)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1677)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)
  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Looks like this is due to &lt;a href=&quot;https://github.com/apache/spark/commit/91fa80fe8a2480d64c430bd10f97b3d44c007bcc#diff-2a91a9a59953aa82fa132aaf45bd731bR69&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/commit/91fa80fe8a2480d64c430bd10f97b3d44c007bcc#diff-2a91a9a59953aa82fa132aaf45bd731bR69&lt;/a&gt; from &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-20070&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/SPARK-20070&lt;/a&gt;. It tries to redact sensitive information from &lt;tt&gt;explain&lt;/tt&gt; output. (We are not trying to explain anything here, so I doubt it is meant to be running in this case.) When it needs to access some configuration, it tries to take it from the &quot;current&quot; Spark session, which it just reads from a thread-local variable. We appear to be on a thread where this variable is not set.&lt;/p&gt;

&lt;p&gt;My impression is that &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-20070&quot; title=&quot;Redact datasource explain output&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-20070&quot;&gt;&lt;del&gt;SPARK-20070&lt;/del&gt;&lt;/a&gt; unintentionally introduced a very hard constraint on multi-threaded Spark applications.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13087211">SPARK-21418</key>
            <summary>NoSuchElementException: None.get in DataSourceScanExec with sun.io.serialization.extendedDebugInfo=true</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Minor</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="srowen">Sean R. Owen</assignee>
                                    <reporter username="darabos">Daniel Darabos</reporter>
                        <labels>
                    </labels>
                <created>Fri, 14 Jul 2017 14:17:51 +0000</created>
                <updated>Thu, 21 Sep 2017 12:13:58 +0000</updated>
                            <resolved>Mon, 4 Sep 2017 21:12:41 +0000</resolved>
                                    <version>2.2.0</version>
                                    <fixVersion>2.2.1</fixVersion>
                    <fixVersion>2.3.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>7</watches>
                                                                                                                <comments>
                            <comment id="16088659" author="kiszk" created="Sat, 15 Jul 2017 16:43:01 +0000"  >&lt;p&gt;I am curious why &lt;tt&gt;java.io.ObjectOutputStream.writeOrdinaryObject&lt;/tt&gt; calls &lt;tt&gt;toString&lt;/tt&gt; method. Do you specify some option to run this program for JVM?&lt;/p&gt;</comment>
                            <comment id="16088952" author="darabos" created="Sun, 16 Jul 2017 15:08:00 +0000"  >&lt;p&gt;I&apos;m on holiday without a computer through the coming week, but I&apos;ll try to&lt;br/&gt;
dig deeper after that.&lt;/p&gt;

&lt;p&gt;I do recall that we enable a JVM flag for printing extra details on&lt;br/&gt;
serialization errors. Now I wonder if that flag collects string forms even&lt;br/&gt;
when no error happens. I guess I should not be surprised: if it did not,&lt;br/&gt;
there would be no reason to ever disable this feature.&lt;/p&gt;

&lt;p&gt;That already suggests an easy workaround &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;. Thanks!&lt;/p&gt;</comment>
                            <comment id="16152742" author="darabos" created="Mon, 4 Sep 2017 15:49:30 +0000"  >&lt;p&gt;Sorry for the delay. I can confirm that removing &lt;tt&gt;-Dsun.io.serialization.extendedDebugInfo=true&lt;/tt&gt; is the fix. We only use this flag when running unit tests, but it&apos;s very useful for debugging serialization issues. It happens often in Spark that you accidentally include something in a closure that cannot be serialized. It&apos;s hard to figure out without this flag what caused that.&lt;/p&gt;</comment>
                            <comment id="16152819" author="srowen" created="Mon, 4 Sep 2017 17:31:18 +0000"  >&lt;p&gt;I think we could easily make this code a little more defensive so that this doesn&apos;t result in an error. It&apos;s just trying to check if a config exists in SparkConf and there&apos;s no particular need for this to fail.&lt;/p&gt;</comment>
                            <comment id="16152821" author="apachespark" created="Mon, 4 Sep 2017 17:34:04 +0000"  >&lt;p&gt;User &apos;srowen&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/19123&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/19123&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16165448" author="tigerquoll" created="Wed, 13 Sep 2017 23:14:47 +0000"  >&lt;p&gt;I&apos;m getting the same stackdump in 2.2.0 while using apache-avro (current snapshot) in structured streaming with a globbed hdfs input path.  I do not get this error if I use a non-globbed input path.&lt;/p&gt;

&lt;p&gt;I should note that this is without setting sun.io.serialization.extendedDebugInfo at all.&lt;/p&gt;</comment>
                            <comment id="16165946" author="darabos" created="Thu, 14 Sep 2017 08:36:48 +0000"  >&lt;p&gt;Sean&apos;s fix should cover you no matter what triggers the unexpected &lt;tt&gt;toString&lt;/tt&gt; call. You could try building from &lt;tt&gt;master&lt;/tt&gt; (or taking a nightly from &lt;a href=&quot;https://spark.apache.org/developer-tools.html#nightly-builds&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://spark.apache.org/developer-tools.html#nightly-builds&lt;/a&gt;) to confirm that this is the case.&lt;/p&gt;</comment>
                            <comment id="16174642" author="koraseg" created="Thu, 21 Sep 2017 12:13:58 +0000"  >&lt;p&gt;There is still a place in FileSourceScanExec.scala where None.get error theoretically could appear.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;  val needsUnsafeRowConversion: &lt;span class=&quot;code-object&quot;&gt;Boolean&lt;/span&gt; = &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (relation.fileFormat.isInstanceOf[ParquetSource]) {
    SparkSession.getActiveSession.get.sessionState.conf.parquetVectorizedReaderEnabled
  } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
    &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I think it is worth defending this val initialization as well (setting a default configuration value in case of None). Although I didn&apos;t encounter any None.get errors so far after applying this patch.&lt;/p&gt;
</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 8 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3hj9r:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>