<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:27:16 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-2018] Big-Endian (IBM Power7)  Spark Serialization issue</title>
                <link>https://issues.apache.org/jira/browse/SPARK-2018</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;We have an application run on Spark on Power7 System .&lt;br/&gt;
But we meet an important issue about serialization.&lt;br/&gt;
The example HdfsWordCount can meet the problem.&lt;br/&gt;
./bin/run-example      org.apache.spark.examples.streaming.HdfsWordCount localdir&lt;br/&gt;
We used Power7 (Big-Endian arch) and Redhat  6.4.&lt;br/&gt;
Big-Endian  is the main cause since the example ran successfully in another Power-based Little Endian setup.&lt;/p&gt;

&lt;p&gt;here is the exception stack and log:&lt;/p&gt;

&lt;p&gt;Spark Executor Command: &quot;/opt/ibm/java-ppc64-70//bin/java&quot; &quot;-cp&quot; &quot;/home/test1/spark-1.0.0-bin-hadoop2/lib::/home/test1/src/spark-1.0.0-bin-hadoop2/conf:/home/test1/src/spark-1.0.0-bin-hadoop2/lib/spark-assembly-1.0.0-hadoop2.2.0.jar:/home/test1/src/spark-1.0.0-bin-hadoop2/lib/datanucleus-rdbms-3.2.1.jar:/home/test1/src/spark-1.0.0-bin-hadoop2/lib/datanucleus-api-jdo-3.2.1.jar:/home/test1/src/spark-1.0.0-bin-hadoop2/lib/datanucleus-core-3.2.2.jar:/home/test1/src/hadoop-2.3.0-cdh5.0.0/etc/hadoop/:/home/test1/src/hadoop-2.3.0-cdh5.0.0/etc/hadoop/&quot; &quot;-XX:MaxPermSize=128m&quot;  &quot;-Xdebug&quot; &quot;-Xrunjdwp:transport=dt_socket,address=99999,server=y,suspend=n&quot; &quot;-Xms512M&quot; &quot;-Xmx512M&quot; &quot;org.apache.spark.executor.CoarseGrainedExecutorBackend&quot; &quot;akka.tcp://spark@9.186.105.141:60253/user/CoarseGrainedScheduler&quot; &quot;2&quot; &quot;p7hvs7br16&quot; &quot;4&quot; &quot;akka.tcp://sparkWorker@p7hvs7br16:59240/user/Worker&quot; &quot;app-20140604023054-0000&quot;&lt;br/&gt;
========================================&lt;/p&gt;

&lt;p&gt;14/06/04 02:31:20 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable&lt;br/&gt;
14/06/04 02:31:21 INFO spark.SecurityManager: Changing view acls to: test1,yifeng&lt;br/&gt;
14/06/04 02:31:21 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(test1, yifeng)&lt;br/&gt;
14/06/04 02:31:22 INFO slf4j.Slf4jLogger: Slf4jLogger started&lt;br/&gt;
14/06/04 02:31:22 INFO Remoting: Starting remoting&lt;br/&gt;
14/06/04 02:31:22 INFO Remoting: Remoting started; listening on addresses :&lt;span class=&quot;error&quot;&gt;&amp;#91;akka.tcp://sparkExecutor@p7hvs7br16:39658&amp;#93;&lt;/span&gt;&lt;br/&gt;
14/06/04 02:31:22 INFO Remoting: Remoting now listens on addresses: &lt;span class=&quot;error&quot;&gt;&amp;#91;akka.tcp://sparkExecutor@p7hvs7br16:39658&amp;#93;&lt;/span&gt;&lt;br/&gt;
14/06/04 02:31:22 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: akka.tcp://spark@9.186.105.141:60253/user/CoarseGrainedScheduler&lt;br/&gt;
14/06/04 02:31:22 INFO worker.WorkerWatcher: Connecting to worker akka.tcp://sparkWorker@p7hvs7br16:59240/user/Worker&lt;br/&gt;
14/06/04 02:31:23 INFO worker.WorkerWatcher: Successfully connected to akka.tcp://sparkWorker@p7hvs7br16:59240/user/Worker&lt;br/&gt;
14/06/04 02:31:24 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver&lt;br/&gt;
14/06/04 02:31:24 INFO spark.SecurityManager: Changing view acls to: test1,yifeng&lt;br/&gt;
14/06/04 02:31:24 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(test1, yifeng)&lt;br/&gt;
14/06/04 02:31:24 INFO slf4j.Slf4jLogger: Slf4jLogger started&lt;br/&gt;
14/06/04 02:31:24 INFO Remoting: Starting remoting&lt;br/&gt;
14/06/04 02:31:24 INFO Remoting: Remoting started; listening on addresses :&lt;span class=&quot;error&quot;&gt;&amp;#91;akka.tcp://spark@p7hvs7br16:58990&amp;#93;&lt;/span&gt;&lt;br/&gt;
14/06/04 02:31:24 INFO Remoting: Remoting now listens on addresses: &lt;span class=&quot;error&quot;&gt;&amp;#91;akka.tcp://spark@p7hvs7br16:58990&amp;#93;&lt;/span&gt;&lt;br/&gt;
14/06/04 02:31:24 INFO spark.SparkEnv: Connecting to MapOutputTracker: akka.tcp://spark@9.186.105.141:60253/user/MapOutputTracker&lt;br/&gt;
14/06/04 02:31:25 INFO spark.SparkEnv: Connecting to BlockManagerMaster: akka.tcp://spark@9.186.105.141:60253/user/BlockManagerMaster&lt;br/&gt;
14/06/04 02:31:25 INFO storage.DiskBlockManager: Created local directory at /tmp/spark-local-20140604023125-3f61&lt;br/&gt;
14/06/04 02:31:25 INFO storage.MemoryStore: MemoryStore started with capacity 307.2 MB.&lt;br/&gt;
14/06/04 02:31:25 INFO network.ConnectionManager: Bound socket to port 39041 with id = ConnectionManagerId(p7hvs7br16,39041)&lt;br/&gt;
14/06/04 02:31:25 INFO storage.BlockManagerMaster: Trying to register BlockManager&lt;br/&gt;
14/06/04 02:31:25 INFO storage.BlockManagerMaster: Registered BlockManager&lt;br/&gt;
14/06/04 02:31:25 INFO spark.HttpFileServer: HTTP File server directory is /tmp/spark-7bce4e43-2833-4666-93af-bd97c327497b&lt;br/&gt;
14/06/04 02:31:25 INFO spark.HttpServer: Starting HTTP Server&lt;br/&gt;
14/06/04 02:31:25 INFO server.Server: jetty-8.y.z-SNAPSHOT&lt;br/&gt;
14/06/04 02:31:26 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:39958&lt;br/&gt;
14/06/04 02:31:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 2&lt;br/&gt;
14/06/04 02:31:26 INFO executor.Executor: Running task ID 2&lt;br/&gt;
14/06/04 02:31:26 ERROR executor.Executor: Exception in task ID 2&lt;br/&gt;
java.io.InvalidClassException: scala.reflect.ClassTag$$anon$1; local class incompatible: stream classdesc serialVersionUID = -8102093212602380348, local class serialVersionUID = -4937928798201944954&lt;br/&gt;
        at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:678)&lt;br/&gt;
        at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1678)&lt;br/&gt;
        at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1573)&lt;br/&gt;
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1827)&lt;br/&gt;
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)&lt;br/&gt;
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)&lt;br/&gt;
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)&lt;br/&gt;
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)&lt;br/&gt;
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)&lt;br/&gt;
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)&lt;br/&gt;
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)&lt;br/&gt;
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)&lt;br/&gt;
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)&lt;br/&gt;
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:409)&lt;br/&gt;
        at scala.collection.immutable.$colon$colon.readObject(List.scala:362)&lt;br/&gt;
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)&lt;br/&gt;
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
        at java.lang.reflect.Method.invoke(Method.java:607)&lt;br/&gt;
        at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1078)&lt;br/&gt;
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1949)&lt;br/&gt;
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)&lt;br/&gt;
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)&lt;br/&gt;
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)&lt;br/&gt;
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)&lt;/p&gt;

&lt;p&gt;at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)&lt;br/&gt;
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)&lt;br/&gt;
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:409)&lt;br/&gt;
        at scala.collection.immutable.$colon$colon.readObject(List.scala:362)&lt;br/&gt;
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)&lt;br/&gt;
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
        at java.lang.reflect.Method.invoke(Method.java:607)&lt;br/&gt;
        at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1078)&lt;br/&gt;
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1949)&lt;br/&gt;
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)&lt;br/&gt;
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)&lt;br/&gt;
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)&lt;br/&gt;
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)&lt;br/&gt;
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)&lt;br/&gt;
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)&lt;br/&gt;
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)&lt;br/&gt;
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)&lt;br/&gt;
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)&lt;br/&gt;
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)&lt;br/&gt;
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:409)&lt;br/&gt;
        at scala.collection.immutable.$colon$colon.readObject(List.scala:362)&lt;br/&gt;
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)&lt;br/&gt;
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
        at java.lang.reflect.Method.invoke(Method.java:607)&lt;br/&gt;
        at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1078)&lt;br/&gt;
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1949)&lt;br/&gt;
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)&lt;br/&gt;
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)&lt;br/&gt;
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)&lt;br/&gt;
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)&lt;br/&gt;
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)&lt;br/&gt;
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)&lt;br/&gt;
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)&lt;br/&gt;
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)&lt;br/&gt;
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)&lt;br/&gt;
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)&lt;br/&gt;
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:409)&lt;br/&gt;
        at scala.collection.immutable.$colon$colon.readObject(List.scala:362)&lt;br/&gt;
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)&lt;br/&gt;
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
        at java.lang.reflect.Method.invoke(Method.java:607)&lt;br/&gt;
        at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1078)&lt;br/&gt;
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1949)&lt;br/&gt;
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)&lt;br/&gt;
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)&lt;br/&gt;
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)&lt;br/&gt;
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)&lt;br/&gt;
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)&lt;br/&gt;
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)&lt;br/&gt;
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)&lt;br/&gt;
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)&lt;br/&gt;
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)&lt;br/&gt;
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)&lt;br/&gt;
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:409)&lt;br/&gt;
        at scala.collection.immutable.$colon$colon.readObject(List.scala:362)&lt;br/&gt;
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)&lt;br/&gt;
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
        at java.lang.reflect.Method.invoke(Method.java:607)&lt;/p&gt;

&lt;p&gt;   at java.lang.reflect.Method.invoke(Method.java:607)&lt;br/&gt;
        at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1078)&lt;br/&gt;
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1949)&lt;br/&gt;
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)&lt;br/&gt;
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)&lt;br/&gt;
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)&lt;br/&gt;
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)&lt;br/&gt;
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)&lt;br/&gt;
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)&lt;br/&gt;
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:409)&lt;br/&gt;
        at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:63)&lt;br/&gt;
        at org.apache.spark.scheduler.ResultTask$.deserializeInfo(ResultTask.scala:61)&lt;br/&gt;
        at org.apache.spark.scheduler.ResultTask.readExternal(ResultTask.scala:141)&lt;br/&gt;
        at java.io.ObjectInputStream.readExternalData(ObjectInputStream.java:1893)&lt;br/&gt;
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1852)&lt;br/&gt;
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)&lt;br/&gt;
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:409)&lt;br/&gt;
        at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:63)&lt;br/&gt;
        at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:85)&lt;br/&gt;
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:169)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:781)&lt;br/&gt;
14/06/04 02:31:26 ERROR executor.CoarseGrainedExecutorBackend: Driver Disassociated &lt;span class=&quot;error&quot;&gt;&amp;#91;akka.tcp://sparkExecutor@p7hvs7br16:39658&amp;#93;&lt;/span&gt; -&amp;gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;akka.tcp://spark@9.186.105.141:60253&amp;#93;&lt;/span&gt; disassociated! Shutting down.&lt;/p&gt;



</description>
                <environment>&lt;p&gt;hardware : IBM Power7&lt;br/&gt;
OS:Linux version 2.6.32-358.el6.ppc64 (mockbuild@ppc-017.build.eng.bos.redhat.com) (gcc version 4.4.7 20120313 (Red Hat 4.4.7-3) (GCC) ) #1 SMP Tue Jan 29 11:43:27 EST 2013&lt;/p&gt;


&lt;p&gt;JDK: Java(TM) SE Runtime Environment (build pxp6470sr5-20130619_01(SR5))&lt;br/&gt;
IBM J9 VM (build 2.6, JRE 1.7.0 Linux ppc64-64 Compressed References 20130617_152572 (JIT enabled, AOT enabled)&lt;/p&gt;


&lt;p&gt;Hadoop:Hadoop-0.2.3-CDH5.0&lt;br/&gt;
Spark:Spark-1.0.0 or Spark-0.9.1&lt;/p&gt;

&lt;p&gt;spark-env.sh:&lt;br/&gt;
export JAVA_HOME=/opt/ibm/java-ppc64-70/&lt;br/&gt;
export SPARK_MASTER_IP=9.114.34.69&lt;br/&gt;
export SPARK_WORKER_MEMORY=10000m&lt;br/&gt;
export SPARK_CLASSPATH=/home/test1/spark-1.0.0-bin-hadoop2/lib&lt;br/&gt;
export  STANDALONE_SPARK_MASTER_HOST=9.114.34.69&lt;br/&gt;
#export SPARK_JAVA_OPTS=&apos; -Xdebug -Xrunjdwp:transport=dt_socket,address=99999,server=y,suspend=n &apos;&lt;/p&gt;</environment>
        <key id="12718348">SPARK-2018</key>
            <summary>Big-Endian (IBM Power7)  Spark Serialization issue</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="tellison">Tim Ellison</assignee>
                                    <reporter username="gaoyanjie55">Yanjie Gao</reporter>
                        <labels>
                    </labels>
                <created>Wed, 4 Jun 2014 09:21:19 +0000</created>
                <updated>Wed, 13 May 2015 01:18:24 +0000</updated>
                            <resolved>Wed, 13 May 2015 01:18:24 +0000</resolved>
                                    <version>1.0.0</version>
                                    <fixVersion>1.3.2</fixVersion>
                    <fixVersion>1.4.0</fixVersion>
                                    <component>Deploy</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="14017528" author="srowen" created="Wed, 4 Jun 2014 09:38:35 +0000"  >&lt;p&gt;The meaning of the error is that Java thinks two serializable classes are not mutually compatible. This is because two different serialVersioUIDs get computed for two copies of what may be the same class. If I understand you correctly, you are communicating between different JVM versions, or reading one&apos;s output from the other? I don&apos;t think it&apos;s guaranteed that the auto-generated serialVersionUID will be the same. If so, it&apos;s nothing to do with big-endian-ness per se. Does it happen entirely within the same machine / JVM? &lt;/p&gt;</comment>
                            <comment id="14018409" author="gaoyanjie55" created="Thu, 5 Jun 2014 01:49:58 +0000"  >&lt;p&gt;Thanks for your quick reply!&lt;br/&gt;
I believe they  use the same jvm&lt;/p&gt;

&lt;p&gt;Do you think this may have another reason?&lt;/p&gt;

&lt;p&gt;How can I  debug it  to find the reason ?&lt;/p&gt;

&lt;p&gt;Best regards !&lt;br/&gt;
Yanjie Gao&lt;br/&gt;
here is the ps -aux |grep java log&lt;/p&gt;

&lt;p&gt; test1      349  0.5  3.7 2945280 195456 pts/7  Sl   02:30   0:22 /opt/ibm/java-ppc64-70//bin/java -cp /home/test1/spark-1.0.0-bin-hadoop2/lib::/home/test1/src/spark-1.0.0-bin-hadoop2/conf:/home/test1/src/spark-1.0.0-bin-hadoop2/lib/spark-assembly-1.0.0-hadoop2.2.0.jar:/home/test1/src/spark-1.0.0-bin-hadoop2/lib/datanucleus-rdbms-3.2.1.jar:/home/test1/src/spark-1.0.0-bin-hadoop2/lib/datanucleus-api-jdo-3.2.1.jar:/home/test1/src/spark-1.0.0-bin-hadoop2/lib/datanucleus-core-3.2.2.jar:/home/test1/src/hadoop-2.3.0-cdh5.0.0/etc/hadoop/:/home/test1/src/hadoop-2.3.0-cdh5.0.0/etc/hadoop/ -XX:MaxPermSize=128m -Dspark.akka.logLifecycleEvents=true -Xms512m -Xmx512m org.apache.spark.deploy.master.Master --ip 9.114.34.69 --port 7077 --webui-port 8080&lt;br/&gt;
test1      492  0.4  3.7 2946496 194432 ?      Sl   02:30   0:19 /opt/ibm/java-ppc64-70//bin/java -cp /home/test1/spark-1.0.0-bin-hadoop2/lib::/home/test1/src/spark-1.0.0-bin-hadoop2/conf:/home/test1/src/spark-1.0.0-bin-hadoop2/lib/spark-assembly-1.0.0-hadoop2.2.0.jar:/home/test1/src/spark-1.0.0-bin-hadoop2/lib/datanucleus-rdbms-3.2.1.jar:/home/test1/src/spark-1.0.0-bin-hadoop2/lib/datanucleus-api-jdo-3.2.1.jar:/home/test1/src/spark-1.0.0-bin-hadoop2/lib/datanucleus-core-3.2.2.jar:/home/test1/src/hadoop-2.3.0-cdh5.0.0/etc/hadoop/:/home/test1/src/hadoop-2.3.0-cdh5.0.0/etc/hadoop/ -XX:MaxPermSize=128m -Dspark.akka.logLifecycleEvents=true -Xms512m -Xmx512m org.apache.spark.deploy.worker.Worker spark://9.114.34.69:7077&lt;br/&gt;
test1     3160  0.0  0.0 104832  2816 pts/10   S+   03:40   0:00 grep java&lt;br/&gt;
test1    13163  0.1  2.7 1631232 144256 ?      Sl   Jun02   2:00 /opt/ibm/java-ppc64-70/bin/java -Dproc_namenode -Xmx1000m -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0 -Dhadoop.id.str=test1 -Dhadoop.root.logger=INFO,console -Djava.library.path=/home/test1/src/hadoop-2.3.0-cdh5.0.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0/logs -Dhadoop.log.file=hadoop-test1-namenode-p7hvs7br16.log -Dhadoop.home.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0 -Dhadoop.id.str=test1 -Dhadoop.root.logger=INFO,RFA -Djava.library.path=/home/test1/src/hadoop-2.3.0-cdh5.0.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.namenode.NameNode&lt;br/&gt;
test1    13328  0.0  2.1 1636160 113152 ?      Sl   Jun02   1:39 /opt/ibm/java-ppc64-70/bin/java -Dproc_datanode -Xmx1000m -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0 -Dhadoop.id.str=test1 -Dhadoop.root.logger=INFO,console -Djava.library.path=/home/test1/src/hadoop-2.3.0-cdh5.0.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0/logs -Dhadoop.log.file=hadoop-test1-datanode-p7hvs7br16.log -Dhadoop.home.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0 -Dhadoop.id.str=test1 -Dhadoop.root.logger=INFO,RFA -Djava.library.path=/home/test1/src/hadoop-2.3.0-cdh5.0.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -server -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.datanode.DataNode&lt;br/&gt;
test1    13474  0.0  2.1 1624960 113408 ?      Sl   Jun02   0:35 /opt/ibm/java-ppc64-70/bin/java -Dproc_secondarynamenode -Xmx1000m -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0 -Dhadoop.id.str=test1 -Dhadoop.root.logger=INFO,console -Djava.library.path=/home/test1/src/hadoop-2.3.0-cdh5.0.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0/logs -Dhadoop.log.file=hadoop-test1-secondarynamenode-p7hvs7br16.log -Dhadoop.home.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0 -Dhadoop.id.str=test1 -Dhadoop.root.logger=INFO,RFA -Djava.library.path=/home/test1/src/hadoop-2.3.0-cdh5.0.0/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode&lt;br/&gt;
test1    13702  0.3  2.4 1666112 124544 ?      Sl   Jun02   6:47 /opt/ibm/java-ppc64-70/bin/java -Dproc_resourcemanager -Xmx1000m -Dhadoop.log.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0/logs -Dyarn.log.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0/logs -Dhadoop.log.file=yarn-test1-resourcemanager-p7hvs7br16.log -Dyarn.log.file=yarn-test1-resourcemanager-p7hvs7br16.log -Dyarn.home.dir= -Dyarn.id.str=test1 -Dhadoop.root.logger=INFO,RFA -Dyarn.root.logger=INFO,RFA -Djava.library.path=/home/test1/src/hadoop-2.3.0-cdh5.0.0/lib/native -Dyarn.policy.file=hadoop-policy.xml -Dhadoop.log.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0/logs -Dyarn.log.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0/logs -Dhadoop.log.file=yarn-test1-resourcemanager-p7hvs7br16.log -Dyarn.log.file=yarn-test1-resourcemanager-p7hvs7br16.log -Dyarn.home.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0 -Dhadoop.home.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0 -Dhadoop.root.logger=INFO,RFA -Dyarn.root.logger=INFO,RFA -Djava.library.path=/home/test1/src/hadoop-2.3.0-cdh5.0.0/lib/native -classpath /home/test1/src/hadoop-2.3.0-cdh5.0.0/etc/hadoop/:/home/test1/src/hadoop-2.3.0-cdh5.0.0/etc/hadoop/:/home/test1/src/hadoop-2.3.0-cdh5.0.0/etc/hadoop/:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/common/lib/&lt;b&gt;:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/common/&lt;/b&gt;:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/hdfs:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/hdfs/lib/&lt;b&gt;:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/hdfs/&lt;/b&gt;:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/yarn/lib/&lt;b&gt;:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/yarn/&lt;/b&gt;:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/mapreduce/lib/&lt;b&gt;:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/mapreduce/&lt;/b&gt;:/home/test1/src/hadoop-2.3.0-cdh5.0.0/contrib/capacity-scheduler/&lt;b&gt;.jar:/home/test1/src/hadoop-2.3.0-cdh5.0.0/contrib/capacity-scheduler/&lt;/b&gt;.jar:/home/test1/src/hadoop-2.3.0-cdh5.0.0/contrib/capacity-scheduler/&lt;b&gt;.jar:/home/test1/src/hadoop-2.3.0-cdh5.0.0/contrib/capacity-scheduler/&lt;/b&gt;.jar:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/yarn/&lt;b&gt;:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/yarn/lib/&lt;/b&gt;:/home/test1/src/hadoop-2.3.0-cdh5.0.0/etc/hadoop//rm-config/log4j.properties org.apache.hadoop.yarn.server.resourcemanager.ResourceManager&lt;br/&gt;
test1    13800  0.1  1.9 1633664 98560 ?       Sl   Jun02   3:03 /opt/ibm/java-ppc64-70/bin/java -Dproc_nodemanager -Xmx1000m -Dhadoop.log.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0/logs -Dyarn.log.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0/logs -Dhadoop.log.file=yarn-test1-nodemanager-p7hvs7br16.log -Dyarn.log.file=yarn-test1-nodemanager-p7hvs7br16.log -Dyarn.home.dir= -Dyarn.id.str=test1 -Dhadoop.root.logger=INFO,RFA -Dyarn.root.logger=INFO,RFA -Djava.library.path=/home/test1/src/hadoop-2.3.0-cdh5.0.0/lib/native -Dyarn.policy.file=hadoop-policy.xml -server -Dhadoop.log.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0/logs -Dyarn.log.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0/logs -Dhadoop.log.file=yarn-test1-nodemanager-p7hvs7br16.log -Dyarn.log.file=yarn-test1-nodemanager-p7hvs7br16.log -Dyarn.home.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0 -Dhadoop.home.dir=/home/test1/src/hadoop-2.3.0-cdh5.0.0 -Dhadoop.root.logger=INFO,RFA -Dyarn.root.logger=INFO,RFA -Djava.library.path=/home/test1/src/hadoop-2.3.0-cdh5.0.0/lib/native -classpath /home/test1/src/hadoop-2.3.0-cdh5.0.0/etc/hadoop/:/home/test1/src/hadoop-2.3.0-cdh5.0.0/etc/hadoop/:/home/test1/src/hadoop-2.3.0-cdh5.0.0/etc/hadoop/:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/common/lib/&lt;b&gt;:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/common/&lt;/b&gt;:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/hdfs:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/hdfs/lib/&lt;b&gt;:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/hdfs/&lt;/b&gt;:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/yarn/lib/&lt;b&gt;:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/yarn/&lt;/b&gt;:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/mapreduce/lib/&lt;b&gt;:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/mapreduce/&lt;/b&gt;:/home/test1/src/hadoop-2.3.0-cdh5.0.0/contrib/capacity-scheduler/&lt;b&gt;.jar:/home/test1/src/hadoop-2.3.0-cdh5.0.0/contrib/capacity-scheduler/&lt;/b&gt;.jar:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/yarn/&lt;b&gt;:/home/test1/src/hadoop-2.3.0-cdh5.0.0/share/hadoop/yarn/lib/&lt;/b&gt;:/home/test1/src/hadoop-2.3.0-cdh5.0.0/etc/hadoop//nm-config/log4j.properties org.apache.hadoop.yarn.server.nodemanager.NodeManager&lt;br/&gt;
songdm   29650  0.4  5.1 3032704 264704 ?      Sl   May27  50:13 /opt/ibm/java-ppc64-70//bin/java -cp ::/home/songdm/spark/conf:/home/songdm/spark/assembly/target/scala-2.10/spark-assembly-1.0.0-SNAPSHOT-hadoop2.2.0.jar:/home/songdm/hadoop-2.2.0/etc/hadoop/:/home/songdm/hadoop-2.2.0/etc/hadoop/ -XX:MaxPermSize=128m -Djava.library.path= -Xms512m -Xmx512m org.apache.spark.deploy.SparkSubmit --master local&lt;span class=&quot;error&quot;&gt;&amp;#91;*&amp;#93;&lt;/span&gt; --class org.apache.spark.examples.streaming.HdfsWordCount /home/songdm/spark/examples/target/scala-2.10/spark-examples-1.0.0-SNAPSHOT-hadoop2.2.0.jar ./kk&lt;/p&gt;</comment>
                            <comment id="14027753" author="gireesh" created="Wed, 11 Jun 2014 13:31:20 +0000"  >&lt;p&gt;I face this same issue for a different class:&lt;/p&gt;

&lt;p&gt;org.apache.spark.SerializableWritable; local class incompatible: stream classdesc serialVersionUID = 6301214776158303468, local class serialVersionUID = -7785455416944904980&lt;/p&gt;

&lt;p&gt;All the entities are in the same system, using same JVM version.&lt;/p&gt;

&lt;p&gt;Bad  value in hex: 57726974 225BB4EC&lt;br/&gt;
Good value in hex: 93F4818C 225BB4EC&lt;/p&gt;

&lt;p&gt;Please note that the lower word (in big endian) is same while the higher word is different.&lt;/p&gt;

&lt;p&gt;With little debugging, I extracted the byte buffer from which the de-serialization was being performed, and it looks like below. Apparently, offsets 48,49,50,51 contain the right value for the lower word (in big endian) of the SUID, while offsets 41,42,43,44 seem to be containing portion of the class name (&apos;Writ&apos;, which is part of &apos;SerializableWritable&apos;). Looks like data corruption in the serializer?&lt;/p&gt;

&lt;p&gt;Is there any configurable option through which I can disable custom serializations, and fall back (compltely) to Java serialization?&lt;/p&gt;

&lt;p&gt;In a spark configuration which runs HdfsWordCount example, which process is responsible for serializing this object? With my little understanding of the whole framework, I can see a master, a worker, an execute runner (spawned by worker) and the client who issues the test case.&lt;/p&gt;

&lt;p&gt;The client reports the execption, but I believe it just prints whatever the execute runner reports to it. ExecutorRunner actually invokes a org.apache.spark.executor.CoarseGrainedExecutorBackend to  perform its task. This process never serialized any objects. is the serialization performed by the master? Or is the example exercising a pre-serialized disc file? How do we know that?&lt;/p&gt;

&lt;p&gt;Is there any debug option which reports messages on step-by-step serialization / de-serialization process? I have all the tools and debug capabilities at the JVM / native level, but lack of understanding of the spark topology makes it hard to move further.&lt;/p&gt;

&lt;p&gt;Finally, if I modify a scala file, how do I effect the changes into the class file and the parent jar file? when I performed sbt assembly or sbt package it indeed compiled the scala file, but the .class file ( org/apache/spark/SerializableWritable.class) or the .jar file (spark-core_2.10-0.9.1.jar) do not seem to have updated. Is there any other way of doing it?&lt;/p&gt;


&lt;p&gt;    &lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt; = 172, 0xac&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; = 116, 0x74, &apos;t&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; =  31, 0x1f&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;3&amp;#93;&lt;/span&gt; = 172, 0xac&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;4&amp;#93;&lt;/span&gt; = 237, 0xed&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;5&amp;#93;&lt;/span&gt; =   0, 0x00&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;6&amp;#93;&lt;/span&gt; =   5, 0x05&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;7&amp;#93;&lt;/span&gt; = 115, 0x73, &apos;s&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;8&amp;#93;&lt;/span&gt; = 114, 0x72, &apos;r&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;9&amp;#93;&lt;/span&gt; =   0, 0x00&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;10&amp;#93;&lt;/span&gt; =  37, 0x25, &apos;%&apos;  // indicates next 37 bytes are for the class name? then probably multiple over-writes happened: i) the classname over the SUID, ii) something else on the classname (bytes  45 through 48)&lt;/p&gt;

&lt;p&gt;    &lt;span class=&quot;error&quot;&gt;&amp;#91;11&amp;#93;&lt;/span&gt; = 111, 0x6f, &apos;o&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;12&amp;#93;&lt;/span&gt; = 114, 0x72, &apos;r&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;13&amp;#93;&lt;/span&gt; = 103, 0x67, &apos;g&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;14&amp;#93;&lt;/span&gt; =  46, 0x2e, &apos;.&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;15&amp;#93;&lt;/span&gt; =  97, 0x61, &apos;a&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;16&amp;#93;&lt;/span&gt; = 112, 0x70, &apos;p&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;17&amp;#93;&lt;/span&gt; =  97, 0x61, &apos;a&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;18&amp;#93;&lt;/span&gt; =  99, 0x63, &apos;c&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;19&amp;#93;&lt;/span&gt; = 104, 0x68, &apos;h&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;20&amp;#93;&lt;/span&gt; = 101, 0x65, &apos;e&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;21&amp;#93;&lt;/span&gt; =  46, 0x2e, &apos;.&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;22&amp;#93;&lt;/span&gt; = 115, 0x73, &apos;s&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;23&amp;#93;&lt;/span&gt; = 112, 0x70, &apos;p&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;24&amp;#93;&lt;/span&gt; =  97, 0x61, &apos;a&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;25&amp;#93;&lt;/span&gt; = 114, 0x72, &apos;r&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;26&amp;#93;&lt;/span&gt; = 107, 0x6b, &apos;k&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;27&amp;#93;&lt;/span&gt; =  46, 0x2e, &apos;.&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;28&amp;#93;&lt;/span&gt; =  83, 0x53, &apos;S&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;29&amp;#93;&lt;/span&gt; = 101, 0x65, &apos;e&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;30&amp;#93;&lt;/span&gt; = 114, 0x72, &apos;r&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;31&amp;#93;&lt;/span&gt; = 105, 0x69, &apos;i&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;32&amp;#93;&lt;/span&gt; =  97, 0x61, &apos;a&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;33&amp;#93;&lt;/span&gt; = 108, 0x6c, &apos;l&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;34&amp;#93;&lt;/span&gt; = 105, 0x69, &apos;i&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;35&amp;#93;&lt;/span&gt; =   8, 0x08&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;36&amp;#93;&lt;/span&gt; = 122, 0x7a, &apos;z&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;37&amp;#93;&lt;/span&gt; =  97, 0x61, &apos;a&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;38&amp;#93;&lt;/span&gt; =  98, 0x62, &apos;b&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;39&amp;#93;&lt;/span&gt; = 108, 0x6c, &apos;l&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;40&amp;#93;&lt;/span&gt; = 101, 0x65, &apos;e&apos;&lt;/p&gt;

&lt;p&gt;    &lt;span class=&quot;error&quot;&gt;&amp;#91;41&amp;#93;&lt;/span&gt; =  87, 0x57, &apos;W&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;42&amp;#93;&lt;/span&gt; = 114, 0x72, &apos;r&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;43&amp;#93;&lt;/span&gt; = 105, 0x69, &apos;i&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;44&amp;#93;&lt;/span&gt; = 116, 0x74, &apos;t&apos;&lt;/p&gt;

&lt;p&gt;    &lt;span class=&quot;error&quot;&gt;&amp;#91;45&amp;#93;&lt;/span&gt; = 192, 0xc0&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;46&amp;#93;&lt;/span&gt; =   7, 0x07&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;47&amp;#93;&lt;/span&gt; =  15, 0x0f&lt;/p&gt;

&lt;p&gt;    &lt;span class=&quot;error&quot;&gt;&amp;#91;48&amp;#93;&lt;/span&gt; =  34, 0x22, &apos;&quot;&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;49&amp;#93;&lt;/span&gt; =  91, 0x5b, &apos;[&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;50&amp;#93;&lt;/span&gt; = 180, 0xb4&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;51&amp;#93;&lt;/span&gt; = 236, 0xec&lt;/p&gt;

&lt;p&gt;    &lt;span class=&quot;error&quot;&gt;&amp;#91;52&amp;#93;&lt;/span&gt; =   3, 0x03&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;53&amp;#93;&lt;/span&gt; =   0, 0x00&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;54&amp;#93;&lt;/span&gt; =   0, 0x00&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;55&amp;#93;&lt;/span&gt; = 120, 0x78, &apos;x&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;56&amp;#93;&lt;/span&gt; = 112, 0x70, &apos;p&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;57&amp;#93;&lt;/span&gt; = 122, 0x7a, &apos;z&apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;58&amp;#93;&lt;/span&gt; =   0, 0x00&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;59&amp;#93;&lt;/span&gt; =   0, 0x00&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;60&amp;#93;&lt;/span&gt; =   4, 0x04&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;61&amp;#93;&lt;/span&gt; =   0, 0x00&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;62&amp;#93;&lt;/span&gt; =   0, 0x00&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;63&amp;#93;&lt;/span&gt; =  32, 0x20, &apos; &apos;&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;64&amp;#93;&lt;/span&gt; = 224, 0xe0&lt;br/&gt;
...&lt;/p&gt;</comment>
                            <comment id="14027808" author="mridulm80" created="Wed, 11 Jun 2014 14:25:26 +0000"  >&lt;p&gt;Ah ! This is an interesting bug.&lt;br/&gt;
Default spark uses java serialization ... so should not be an issue : but yet you are facing it ! (I am assuming you have not customized serialization).&lt;br/&gt;
Is it possible for you to dump data written and read at both ends ? The env vars and jvm details ?&lt;br/&gt;
Actually, spark does not do anything fancy for default serialization : so a simple example code without spark in picture could also be tried (write to file on master node, and read from the file in slave node - and see if it works)&lt;/p&gt;</comment>
                            <comment id="14027819" author="gireesh" created="Wed, 11 Jun 2014 14:37:14 +0000"  >&lt;p&gt;Mridul,&lt;/p&gt;

&lt;p&gt;Thanks for responding.&lt;/p&gt;

&lt;p&gt;1. As I explained earlier, I am just exercising the HdfsWordCount example, exactly following the documentation. So there is no specific JVM arguments in my case either.&lt;br/&gt;
2. Dumping data from either side: this is the problem - I don&apos;t know what is being written. I am able to capture only what is read (de-serialization end). My all efforts are to capture who writes this object.&lt;br/&gt;
3. so a simple example code without spark in picture: Can you please elaborate the steps involved? I am not familiar even with the basic operations on the hadoop platform. The current process I follow is as follows:&lt;br/&gt;
spark# sbin/start-all.sh&lt;br/&gt;
spark# ./bin/run-example org.apache.spark.streaming.examples.HdfsWordCount spark://&amp;lt;IP&amp;gt;:7077 localdir&lt;/p&gt;

&lt;p&gt;Thanks once again!&lt;/p&gt;</comment>
                            <comment id="14028649" author="gaoyanjie55" created="Thu, 12 Jun 2014 00:28:43 +0000"  >&lt;p&gt;Thanks a lot.&lt;br/&gt;
Now  I have some new detection.&lt;/p&gt;


&lt;p&gt;(1) Compare&lt;br/&gt;
        server                Spark exec mode          pass or not                                                                   jdk                                   &lt;br/&gt;
x86(Little Endian)    Local+cluster                 pass                                                                               x86&lt;br/&gt;
p8(Little Endian)     Local+cluster                 pass                                                                               IBM(little endian)&lt;br/&gt;
P7(Big Endian)      Local   mode                   pass(I change some jar classpath then can&apos;t pass)        IBM(Big  endian)&lt;br/&gt;
P7 (Big Endian)    Cluster mode                   not                                                                                  IBM(Big endian)&lt;/p&gt;


&lt;p&gt;(2) The Exception priciple&lt;br/&gt;
2.1 Main Error : Exception in thread &quot;main&quot; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1.0:0 failed 4 times, most recent failure: Exception failure in TID 3 on host arlab105.austin.ibm.com: java.io.InvalidClassException: org.apache.spark.SerializableWritable; local class incompatible: stream classdesc serialVersionUID = 6301214776158303468, local class serialVersionUID = -7785455416944904980&lt;/p&gt;

&lt;p&gt;(other  may has the same reason)&lt;br/&gt;
2.2 Exception in thread &quot;main&quot; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0:0 failed 1 times, most recent failure: Exception failure in TID 1 on host localhost: java.io.InvalidClassException: scala.Tuple2; invalid descriptor for field _1&lt;/p&gt;


&lt;p&gt;Now we analysis 2.1 Bug .&lt;br/&gt;
refer:&lt;br/&gt;
 serialVersionUID has two generate method&lt;br/&gt;
1   default 1Lprivate static final long serialVersionUID = 1L&lt;br/&gt;
2    Generated by  hash.       Class name ,interface name ,method ,attribute can affect the result.&lt;br/&gt;
Our error is not 1L .So it generated by method 2.&lt;/p&gt;

&lt;p&gt;UID is used  when the process deserialize the byte array .the   process read the local class file ,and find the class&apos;s UID.If it is diff with the array .Then throw the Exception. &lt;/p&gt;

&lt;p&gt;Let&apos;s  see the work flow of  Spark Serilization&lt;/p&gt;

&lt;p&gt;Local mode &lt;br/&gt;
once serialize &lt;br/&gt;
object ----&lt;del&gt;serialize(thread1 or thread2)&lt;/del&gt;--&lt;del&gt;&amp;gt;array&lt;/del&gt;----&lt;del&gt;deserialize(thread2 or process2)&lt;/del&gt;---&amp;gt;object&lt;/p&gt;

&lt;p&gt;Cluster mode&lt;br/&gt;
twice serialize&lt;br/&gt;
object ---&lt;del&gt;serialize(thread1 or thread2)&lt;/del&gt;--&lt;del&gt;&amp;gt;array&lt;/del&gt;-&lt;del&gt;Actor send message serialize ---&amp;gt;message&lt;/del&gt;--&lt;del&gt;&amp;gt;Actor receive and deserialize it -----&amp;gt;array  ------deserialize(thread2 or process2)&lt;/del&gt;---&amp;gt;object &lt;/p&gt;




&lt;p&gt;summary:&lt;br/&gt;
let&apos;t compare (1) &apos;s four situation.&lt;br/&gt;
I think the reason is that IBM jdk and (scala lib and akka lib)  may have some intersection of some class.  But they compile in diff platform use diff javac .They may generate diff UID. &lt;br/&gt;
In run time .jvm may load the same class from diff .class file. &lt;/p&gt;


&lt;p&gt;(3)Method to fix it.&lt;/p&gt;

&lt;p&gt;I think&lt;br/&gt;
The reason is the same class load diff class file.&lt;br/&gt;
There are two method  .May be there are other better method.&lt;br/&gt;
4.1 Let the two file has the same version UID:Compile  scala lib  and akka lib  in P7 platform&lt;br/&gt;
4.2 Let the two loader load the same Jar. Use some method like extend class loader or  OSGI .We force the jvm to load the same class file.(The difficult thing is that  classes is in jar and class num is too large .)&lt;/p&gt;

&lt;p&gt;Best Regards&lt;br/&gt;
Yanjie Gao&lt;/p&gt;
</comment>
                            <comment id="14032255" author="gireesh" created="Mon, 16 Jun 2014 09:23:43 +0000"  >&lt;p&gt;I was able to identify the root cause. Please see &lt;a href=&quot;https://github.com/ning/compress/issues/37&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/ning/compress/issues/37&lt;/a&gt; for details.&lt;/p&gt;</comment>
                            <comment id="14539529" author="apachespark" created="Tue, 12 May 2015 08:44:03 +0000"  >&lt;p&gt;User &apos;tellison&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/6077&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/6077&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14540579" author="srowen" created="Tue, 12 May 2015 19:49:25 +0000"  >&lt;p&gt;Issue resolved by pull request 6077&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/6077&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/6077&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12742721">SPARK-3603</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>396547</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 27 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1w9s7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>396668</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>