<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:55:09 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-21656] spark dynamic allocation should not idle timeout executors when there are enough tasks to run on them</title>
                <link>https://issues.apache.org/jira/browse/SPARK-21656</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Right now with dynamic allocation spark starts by getting the number of executors it needs to run all the tasks in parallel (or the configured maximum) for that stage.  After it gets that number it will never reacquire more unless either an executor dies, is explicitly killed by yarn or it goes to the next stage.  The dynamic allocation manager has the concept of idle timeout. Currently this says if a task hasn&apos;t been scheduled on that executor for a configurable amount of time (60 seconds by default), then let that executor go.  Note when it lets that executor go due to the idle timeout it never goes back to see if it should reacquire more.&lt;/p&gt;

&lt;p&gt;This is a problem for multiple reasons:&lt;br/&gt;
1 . Things can happen in the system that are not expected that can cause delays. Spark should be resilient to these. If the driver is GC&apos;ing, you have network delays, etc we could idle timeout executors even though there are tasks to run on them its just the scheduler hasn&apos;t had time to start those tasks.  Note that in the worst case this allows the number of executors to go to 0 and we have a deadlock.&lt;/p&gt;

&lt;p&gt;2. Internal Spark components have opposing requirements. The scheduler has a requirement to try to get locality, the dynamic allocation doesn&apos;t know about this and if it lets the executors go it hurts the scheduler from doing what it was designed to do.  For example the scheduler first tries to schedule node local, during this time it can skip scheduling on some executors.  After a while though the scheduler falls back from node local to scheduler on rack local, and then eventually on any node.  So during when the scheduler is doing node local scheduling, the other executors can idle timeout.  This means that when the scheduler does fall back to rack or any locality where it would have used those executors, we have already let them go and it can&apos;t scheduler all the tasks it could which can have a huge negative impact on job run time.&lt;/p&gt;

&lt;p&gt;In both of these cases when the executors idle timeout we never go back to check to see if we need more executors (until the next stage starts).  In the worst case you end up with 0 and deadlock, but generally this shows itself by just going down to very few executors when you could have 10&apos;s of thousands of tasks to run on them, which causes the job to take way more time (in my case I&apos;ve seen it should take minutes and it takes hours due to only been left a few executors).  &lt;/p&gt;

&lt;p&gt;We should handle these situations in Spark.   The most straight forward approach would be to not allow the executors to idle timeout when there are tasks that could run on those executors. This would allow the scheduler to do its job with locality scheduling.  In doing this it also fixes number 1 above because you never can go into a deadlock as it will keep enough executors to run all the tasks on. &lt;/p&gt;

&lt;p&gt;There are other approaches to fix this, like explicitly prevent it from going to 0 executors, that prevents a deadlock but can still cause the job to slowdown greatly.  We could also change it at some point to just re-check to see if we should get more executors, but this adds extra logic, we would have to decide when to check, its also just overhead in letting them go and then re-acquiring them again and this would cause some slowdown in the job as the executors aren&apos;t immediately there for the scheduler to place things on. &lt;/p&gt;</description>
                <environment></environment>
        <key id="13092964">SPARK-21656</key>
            <summary>spark dynamic allocation should not idle timeout executors when there are enough tasks to run on them</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="yoonlee95">Jong Yoon Lee</assignee>
                                    <reporter username="yoonlee95">Jong Yoon Lee</reporter>
                        <labels>
                    </labels>
                <created>Mon, 7 Aug 2017 18:12:25 +0000</created>
                <updated>Mon, 18 Dec 2017 17:21:53 +0000</updated>
                            <resolved>Wed, 16 Aug 2017 14:45:40 +0000</resolved>
                                    <version>2.1.1</version>
                                    <fixVersion>2.2.1</fixVersion>
                    <fixVersion>2.3.0</fixVersion>
                                    <component>Spark Core</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                    <workratio workratioPercent="0"/>
                                    <progress percentage="0">
                                    <originalProgress>
                                                    <row percentage="100" backgroundColor="#89afd7"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="0" backgroundColor="#51a825"/>
                                                    <row percentage="100" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </progress>
                                    <aggregateprogress percentage="0">
                                    <originalProgress>
                                                    <row percentage="100" backgroundColor="#89afd7"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="0" backgroundColor="#51a825"/>
                                                    <row percentage="100" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </aggregateprogress>
                                    <timeoriginalestimate seconds="86400">24h</timeoriginalestimate>
                            <timeestimate seconds="86400">24h</timeestimate>
                                        <comments>
                            <comment id="16116975" author="srowen" created="Mon, 7 Aug 2017 18:17:33 +0000"  >&lt;p&gt;I don&apos;t see how an executor would be idle if there is a task to run, unless of course you changed the locality settings a lot. There&apos;s no real detail here that would establish a problem in Spark. &lt;/p&gt;</comment>
                            <comment id="16117086" author="tgraves" created="Mon, 7 Aug 2017 19:11:51 +0000"  >&lt;p&gt;The executor can be idle if the scheduler doesn&apos;t put any tasks on it. The scheduler can skip executors due to the locality settings (spark.locality.wait.node).  We have seen this many times now where it gets in this harmonic where some executors get node locality and other don&apos;t.  The scheduler skips many of the executors that don&apos;t get locality and eventually they idle timeout when there are 10&apos;s of thousands of tasks left. &lt;br/&gt;
We generally see this with very large jobs that have like 1000 executors, 150000 map tasks.&lt;/p&gt;

&lt;p&gt;We shouldn&apos;t allow them to idle timeout if we still need them. &lt;/p&gt;</comment>
                            <comment id="16117122" author="srowen" created="Mon, 7 Aug 2017 19:34:18 +0000"  >&lt;p&gt;Good point. In that case, what&apos;s wrong with killing the executor? if the scheduler is consistently preferring locality enough to let those executors go idle &amp;#8211; either those settings are wrong or those executors aren&apos;t needed. What&apos;s the argument that the app needs them if no tasks are scheduling?&lt;/p&gt;</comment>
                            <comment id="16117159" author="tgraves" created="Mon, 7 Aug 2017 19:56:10 +0000"  >&lt;p&gt;If given more time the scheduler would have fallen back to use those for rack local or any locality.    Yes you can get around this by changing the locality settings (which is what the work around is) but I don&apos;t think that is what should happen.  Its 2 features that are conflicting with timeouts. And it is the defaults we ship with causing bad things to happen. I do think we should look at the locality logic in the scheduler more to see if there is anything to improve there but I haven&apos;t had time to do that.&lt;/p&gt;

&lt;p&gt;The thing is that dynamic allocation never gets more executors for the same stage once its  acquired them and let them idle timeout. So if you get some weird situations you end up just having very few executors to run thousands of tasks.  In my opinion its better to hold those executors and let the normal scheduler logic work.  &lt;/p&gt;

&lt;p&gt;We can add a config flag for this if needed if people would like this behavior but I think that conflict with the scheduler logic.&lt;/p&gt;</comment>
                            <comment id="16117167" author="srowen" created="Mon, 7 Aug 2017 20:02:00 +0000"  >&lt;p&gt;If the issue is &quot;given more time&quot; then increase the idle timeout? or indeed the locality settings. Why does this need another configuration? It sounds like it&apos;s at best a change to defaults, but, how about start by having the app care less about locality? It doesn&apos;t make sense to say that executors that are by definition not needed according to a user&apos;s config should not be reclaimed because the config is wrong.&lt;/p&gt;</comment>
                            <comment id="16117200" author="tgraves" created="Mon, 7 Aug 2017 20:22:50 +0000"  >&lt;p&gt;why not fix the bug in dynamic allocation?  changing configs is a work around.  like everything else what are the best configs for everyone&apos;s job.  &lt;/p&gt;

&lt;p&gt;dynamic allocation is supposed to get you enough executors to run all your tasks in parallel (up to your config limits).  This is not allowing that and its code within SPARK that is doing it, not user code. Thus a bug in my opinion.&lt;/p&gt;

&lt;p&gt;The documentation even hints at it. The problem is we just didn&apos;t catch this issue that in the initial code.&lt;/p&gt;

&lt;p&gt;From:&lt;br/&gt;
&lt;a href=&quot;http://spark.apache.org/docs/2.2.0/job-scheduling.html#remove-policy&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://spark.apache.org/docs/2.2.0/job-scheduling.html#remove-policy&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&quot;in that an executor should not be idle if there are still pending tasks to be scheduled&quot;&lt;/p&gt;

&lt;p&gt;One other option here would be to actually let them go and get new ones. This may or may not help depending on if it can get ones with better locality.  it might also just waste time releasing and reacquiring.&lt;/p&gt;

&lt;p&gt;I personally would also be ok with changing the locality wait for node to 0 which generally works around the problem, but I think this could happen in other cases and we should fix this bug too.  For instance say your driver does a full GC and can&apos;t schedule things within 60 seconds, you lose those executors and we never get them back.   What if you have temporary network congestion and your network timeout is plenty big to allow for, you could idle timeout.  yes we could increase the idle timeout, but in the normal working case the idle timeout is meant to be cases where you don&apos;t have any tasks to run on this executor.  Your stage has completed enough you can release some. This is not that case.&lt;/p&gt;</comment>
                            <comment id="16117204" author="tgraves" created="Mon, 7 Aug 2017 20:26:15 +0000"  >&lt;p&gt;Another option would be just to add logic for spark to look at some point to see if it should try reacquiring some. All of that though seems like more logic then just not letting them go.  To me Spark needs to be more resilient about this and should handle various possible conditions.  User shouldn&apos;t have to tune every single job to account for weird things happening.  Note that if dynamic allocation is off this doesn&apos;t happen. So why is user getting worse experience in this case.&lt;/p&gt;</comment>
                            <comment id="16117217" author="srowen" created="Mon, 7 Aug 2017 20:36:07 +0000"  >&lt;p&gt;I do not understand what the bug is. Configuration says an executor should go away if idle for X seconds. Configuration leads tasks to schedule on other executors for X seconds. It is correct that it is removed. You are claiming that it would help the application, but, the application is not scheduling anything on the executor. It does not help the app to keep it alive. Right? this seems obvious, so we must be talking about something different. You&apos;re talking about a bunch of other logic but what would it be based on? all of the data it has says the executor will be unused, indefinitely.&lt;/p&gt;</comment>
                            <comment id="16117241" author="tgraves" created="Mon, 7 Aug 2017 20:49:46 +0000"  >&lt;p&gt;As a said above it DOES help the application to keep them alive. the scheduler logic will fall back to them at some point when it goes to rack/any locality or when it finishes the tasks that are getting locality on those few nodes.  Thus why I&apos;m saying its a conflict within spark. &lt;/p&gt;

&lt;p&gt;SPARK should be resilient to any weird things happening.  In the cases I have described we could actually release all of our executors and never ask for more within a stage, that is a BUG.   We can change the configs to make it so that doesn&apos;t normally happen but a user could change them back and when they do that it shouldn&apos;t result in a deadlock.&lt;/p&gt;
</comment>
                            <comment id="16123358" author="tgraves" created="Fri, 11 Aug 2017 13:46:24 +0000"  >&lt;p&gt;example of test results with this.&lt;/p&gt;

&lt;p&gt;We have production job running 21600 tasks.  With default locality the job takes 3.1 hours due to this issue. With the fix proposed in the pull request the job takes 17 minutes.  The resource utilization of the fix does use more resource but every executor eventually has multiple tasks run on it, demonstrating that if we hold on to them for a while the scheduler will fall back and use them. &lt;/p&gt;</comment>
                            <comment id="16123373" author="srowen" created="Fri, 11 Aug 2017 13:55:14 +0000"  >&lt;p&gt;Is this the &apos;busy driver&apos; scenario that the PR contemplates? If not, then this may be true, but it&apos;s not the motivation of the PR, right? this is just a case where you need shorter locality timeout, or something. It&apos;s also not the 0-executor scenario that is the motivation of the PR either.&lt;/p&gt;

&lt;p&gt;If this is the &apos;busy driver&apos; scenario, then I also wonder what happens if you increase the locality timeout. That was one unfinished thread in the PR discussion; why do the other executors get tasks only so very eventually?&lt;/p&gt;

&lt;p&gt;I want to stay clear on what we&apos;re helping here, and also what the cost is: see the flip-side to this situation described in the PR, which could get worse.&lt;/p&gt;</comment>
                            <comment id="16123393" author="tgraves" created="Fri, 11 Aug 2017 14:12:09 +0000"  >&lt;p&gt;I don&apos;t know what you mean by busy driver.  The example of the tests results is showing this is fixing the issue.  The issue is as I&apos;ve describe in the description of the jira.  In this case its due to the scheduler and the fact it doesn&apos;t immediately use the executors due to the locality settings, as long as you keep those executors around (don&apos;t idle timeout them) they do get used and it has a huge impact on the run time.  the executors only eventually get tasks because of the scheduler locality delay.  &lt;/p&gt;

&lt;p&gt;I don&apos;t know what you mean by the flip-side of the situation and how this gets worse.&lt;/p&gt;

&lt;p&gt;If you want something to compare to go see how other frameworks due this same thing. TEZ for instance. This fix is changing it so it acts very similar to those.&lt;/p&gt;
</comment>
                            <comment id="16123411" author="srowen" created="Fri, 11 Aug 2017 14:24:24 +0000"  >&lt;p&gt;I&apos;m referring to the same issue you cite repeatedly, including:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/18874#issuecomment-321313616&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/18874#issuecomment-321313616&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-21656?focusedCommentId=16117200&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16117200&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/SPARK-21656?focusedCommentId=16117200&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16117200&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;Something like a driver busy in long GC pauses doesn&apos;t keep up with the fact that executors are non-idle and removes them. Its conclusion is incorrect and that&apos;s what we&apos;re trying to fix. All the more because going to 0 executors stops the stage.&lt;/p&gt;

&lt;p&gt;Right? I though we finally had it clear that this was the problem being fixed.&lt;/p&gt;

&lt;p&gt;Now you&apos;re just describing a job that needs a lower locality timeout. (Or else, describing a different problem with different solution, as in &lt;a href=&quot;https://github.com/apache/spark/pull/18874#issuecomment-321625808&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/18874#issuecomment-321625808&lt;/a&gt; &amp;#8211; why do they take so much longer than 3s to fall back to other executors?) That scenario is not a reason to make this change.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tgraves&quot; class=&quot;user-hover&quot; rel=&quot;tgraves&quot;&gt;tgraves&lt;/a&gt; please read &lt;a href=&quot;https://github.com/apache/spark/pull/18874#issuecomment-321683515&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/18874#issuecomment-321683515&lt;/a&gt; . You&apos;re saying there&apos;s no counterpart scenario that is actually harmed by this change a bit, and I think there is. We need to get on the same page.&lt;/p&gt;
</comment>
                            <comment id="16123435" author="tgraves" created="Fri, 11 Aug 2017 14:40:05 +0000"  >&lt;p&gt;Yes there is a trade off here, use some more resource or have your job run time be really really slow and possibly deadlock. I completely understand the scenario where some executors may stay up when they aren&apos;t being used, if you have a better solution to do both please state it.  As I&apos;ve stated changing config to me is a work around and not a solution. This case is handled by many other big data frameworks (pig, tez, mapreduce) and I believe spark should handle it as well.   &lt;/p&gt;

&lt;p&gt;I would much rather lean towards having as many jobs run as fast as possible without the user having to tune things even at the expense of possibly using more resources.  I&apos;ve describe 2 scenarios in which this problem can occur, there is also the extreme case where it goes to 0 that you keep mentioning. The fix provided is to address both of them.&lt;/p&gt;


</comment>
                            <comment id="16123439" author="tgraves" created="Fri, 11 Aug 2017 14:42:46 +0000"  >&lt;p&gt;Note, I&apos;ve never said there is no counter part scenario and if you read what I said in the pr you will see that:&lt;/p&gt;

&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; It doesn&apos;t hurt the common case, the common case is all your executors have tasks on them as long as there are tasks to run. Normally scheduler can fill up the executors. It will use more resources if the scheduler takes time to put tasks on them, but that versus the time wasted in jobs that don&apos;t have enough executors to run on is hard to quantify because its going to be so application dependent. yes it is a behavior change but a behavior change that is fixing an issue.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
</comment>
                            <comment id="16123443" author="srowen" created="Fri, 11 Aug 2017 14:48:07 +0000"  >&lt;p&gt;In the end I still don&apos;t quite agree with how you frame it here. It&apos;s making some jobs use more resource to let &lt;em&gt;other&lt;/em&gt; jobs move faster when bumping up against timeout limits. The downside of this change it no compelling just so that someone doesn&apos;t have to tune their job, so I&apos;d discard that argument. It is compelling to solve the &quot;busy driver&quot; and &quot;0 executor&quot; problems. I&apos;d have preferred to frame it that way from the get-go. This discussion isn&apos;t going to get farther, and agreeing on an outcome but disagreeing about why is close enough.&lt;/p&gt;</comment>
                            <comment id="16295282" author="codingcat" created="Mon, 18 Dec 2017 17:21:53 +0000"  >&lt;p&gt;NOTE: the issue fixed by &lt;a href=&quot;https://github.com/apache/spark/pull/18874&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/18874&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 48 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3ii7r:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>