<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:37:15 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-10086] Flaky StreamingKMeans test in PySpark</title>
                <link>https://issues.apache.org/jira/browse/SPARK-10086</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Here&apos;s a report on investigating test failures in StreamingKMeans in PySpark. (See Jenkins links below.)&lt;/p&gt;

&lt;p&gt;It is a StreamingKMeans test which trains on a DStream with 2 batches and then tests on those same 2 batches.  It fails here: &lt;a href=&quot;https://github.com/apache/spark/blob/1968276af0f681fe51328b7dd795bd21724a5441/python/pyspark/mllib/tests.py#L1144&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/1968276af0f681fe51328b7dd795bd21724a5441/python/pyspark/mllib/tests.py#L1144&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I recreated the same test, with variants training on: (1) the original 2 batches, (2) just the first batch, (3) just the second batch, and (4) neither batch.  Here is code which avoids Streaming altogether to identify what batches were processed.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;from pyspark.mllib.clustering &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; StreamingKMeans, StreamingKMeansModel

batches = [[[-0.5], [0.6], [0.8]], [[0.2], [-0.1], [0.3]]]
batches = [sc.parallelize(batch) &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; batch in batches]

stkm = StreamingKMeans(decayFactor=0.0, k=2)
stkm.setInitialCenters([[0.0], [1.0]], [1.0, 1.0])

# Train
def update(rdd):
    stkm._model.update(rdd, stkm._decayFactor, stkm._timeUnit)

# Remove one or both of these lines to test skipping batches.
update(batches[0])
update(batches[1])

# Test
def predict(rdd):
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; stkm._model.predict(rdd)

predict(batches[0]).collect()
predict(batches[1]).collect()
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;b&gt;Results&lt;/b&gt;:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;####################### EXPECTED

[0, 1, 1]                                                                       
[1, 0, 1]

####################### Skip batch 0

[1, 0, 0]
[0, 1, 0]

####################### Skip batch 1

[0, 1, 1]
[1, 0, 1]

####################### Skip both batches  (This is what we see in the test failures.)

[0, 1, 1]
[0, 0, 0]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Skipping both batches reproduces the failure.  There is no randomness in the StreamingKMeans algorithm (since initial centers are fixed, not randomized).&lt;/p&gt;

&lt;p&gt;CC: &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tdas&quot; class=&quot;user-hover&quot; rel=&quot;tdas&quot;&gt;tdas&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=freeman-lab&quot; class=&quot;user-hover&quot; rel=&quot;freeman-lab&quot;&gt;freeman-lab&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mengxr&quot; class=&quot;user-hover&quot; rel=&quot;mengxr&quot;&gt;mengxr&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Failure message:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;======================================================================
FAIL: test_trainOn_predictOn (__main__.StreamingKMeansTest)
Test that prediction happens on the updated model.
----------------------------------------------------------------------
Traceback (most recent call last):
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/home/jenkins/workspace/SparkPullRequestBuilder@3/python/pyspark/mllib/tests.py&quot;&lt;/span&gt;, line 1147, in test_trainOn_predictOn
    self._eventually(condition, catch_assertions=True)
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/home/jenkins/workspace/SparkPullRequestBuilder@3/python/pyspark/mllib/tests.py&quot;&lt;/span&gt;, line 123, in _eventually
    raise lastValue
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/home/jenkins/workspace/SparkPullRequestBuilder@3/python/pyspark/mllib/tests.py&quot;&lt;/span&gt;, line 114, in _eventually
    lastValue = condition()
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/home/jenkins/workspace/SparkPullRequestBuilder@3/python/pyspark/mllib/tests.py&quot;&lt;/span&gt;, line 1144, in condition
    self.assertEqual(predict_results, [[0, 1, 1], [1, 0, 1]])
AssertionError: Lists differ: [[0, 1, 1], [0, 0, 0]] != [[0, 1, 1], [1, 0, 1]]

First differing element 1:
[0, 0, 0]
[1, 0, 1]

- [[0, 1, 1], [0, 0, 0]]
?                 ^^^^

+ [[0, 1, 1], [1, 0, 1]]
?              +++   ^


----------------------------------------------------------------------
Ran 62 tests in 164.188s
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="12856979">SPARK-10086</key>
            <summary>Flaky StreamingKMeans test in PySpark</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="4">Incomplete</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="josephkb">Joseph K. Bradley</reporter>
                        <labels>
                            <label>bulk-closed</label>
                    </labels>
                <created>Tue, 18 Aug 2015 17:50:05 +0000</created>
                <updated>Tue, 21 May 2019 04:36:30 +0000</updated>
                            <resolved>Tue, 21 May 2019 04:36:30 +0000</resolved>
                                    <version>1.5.0</version>
                                                    <component>DStreams</component>
                    <component>MLlib</component>
                    <component>PySpark</component>
                    <component>Tests</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>7</watches>
                                                                                                                <comments>
                            <comment id="14905729" author="tdas" created="Thu, 24 Sep 2015 02:57:40 +0000"  >&lt;p&gt;You could use a maintain a counter for the number of batches completed. That is a foreachRDD can increment a counter. And the testing code should wait for the counter to reach 2 before checking for the model. Alternatively, the check should be in an eventually loop.&lt;/p&gt;</comment>
                            <comment id="14905731" author="tdas" created="Thu, 24 Sep 2015 02:59:44 +0000"  >&lt;p&gt;Actually never mind, its already in eventually. The default timeout is 30 seconds. Then I dont get why this is failing. Could it be a thread race condition visibility issue?&lt;/p&gt;</comment>
                            <comment id="14949615" author="josephkb" created="Thu, 8 Oct 2015 23:43:24 +0000"  >&lt;p&gt;CC: &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=davies&quot; class=&quot;user-hover&quot; rel=&quot;davies&quot;&gt;davies&lt;/a&gt;  When you have a chance, can you please take a look?  Thanks a lot!&lt;br/&gt;
It&apos;s unfortunately hard to reproduce...just happens on Jenkins sometimes.&lt;/p&gt;</comment>
                            <comment id="14994615" author="bryanc" created="Fri, 6 Nov 2015 22:49:28 +0000"  >&lt;p&gt;I&apos;ve been able to reproduce this locally, but haven&apos;t found the exact cause yet.  Here is what I know so far&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;It only happens in my env about 1 out of ~200 attempts, and seems to be pretty random&lt;/li&gt;
	&lt;li&gt;When it fails, the centroids still have the initial values while predicting, and they are later updated&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;My thought is that there is some kind of race condition as &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tdas&quot; class=&quot;user-hover&quot; rel=&quot;tdas&quot;&gt;tdas&lt;/a&gt; mentioned, so somehow &lt;tt&gt;predictOn&lt;/tt&gt; is occurring before &lt;tt&gt;trainOn&lt;/tt&gt;.  I can keep looking into it, but I&apos;d like to propose a slight change to the test (below).  &lt;/p&gt;

&lt;p&gt;It basically tests the same thing, but is just more flexible about &lt;em&gt;when&lt;/em&gt; the model update happens.  Prediction on the first batch should yield the same regardless if centroids are at initial state or updated, so we can check the first element in &lt;tt&gt;predict_results&lt;/tt&gt; for that.  Then the stream will repeat the second batch (as the default) and we just check the last element of &lt;tt&gt;predict_results&lt;/tt&gt; which should yeild the correct result only after the model has been updated at least once.  So far, this does not seem to fail in my local env.&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;  def test_trainOn_predictOn(self):
    &quot;&quot;&quot;Test that prediction happens on the updated model.&quot;&quot;&quot;
    stkm = StreamingKMeans(decayFactor=0.0, k=2)
    stkm.setInitialCenters([[0.0], [1.0]], [1.0, 1.0])

    # Since decay factor is set to zero, once the first batch
    # is passed the clusterCenters are updated to [-0.5, 0.7]
    # which causes 0.2 &amp;amp; 0.3 to be classified as 1, even though the
    # classification based in the initial model would have been 0
    # proving that the model is updated.
    batches = [[[-0.5], [0.6], [0.8]]]
    default_batch = [[0.2], [-0.1], [0.3]]
    batches = [sc.parallelize(batch) for batch in batches]
    default_batch = sc.parallelize(default_batch)
    input_stream = self.ssc.queueStream(batches, default=default_batch)
    predict_results = []

    def collect(rdd):
      rdd_collect = rdd.collect()
      if rdd_collect:
        predict_results.append(rdd_collect)

    stkm.trainOn(input_stream)
    predict_stream = stkm.predictOn(input_stream)
    predict_stream.foreachRDD(collect)

    self.ssc.start()

    def condition():
      self.assertTrue(predict_results)
      self.assertEqual(predict_results[0], [0, 1, 1])
      self.assertEqual(predict_results[-1], [1, 0, 1])
      return True

    self._eventually(condition, catch_assertions=True)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
</comment>
                            <comment id="15002620" author="apachespark" created="Thu, 12 Nov 2015 18:50:02 +0000"  >&lt;p&gt;User &apos;BryanCutler&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/9670&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/9670&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15116391" author="mengxr" created="Tue, 26 Jan 2016 00:44:02 +0000"  >&lt;p&gt;Saw more failures recently and changed the priority to critical.&lt;/p&gt;</comment>
                            <comment id="15116422" author="apachespark" created="Tue, 26 Jan 2016 01:00:04 +0000"  >&lt;p&gt;User &apos;mengxr&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/10909&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/10909&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15145625" author="bryanc" created="Sat, 13 Feb 2016 00:41:27 +0000"  >&lt;p&gt;I was able to track down the cause of these failures, so here is an update with what I found.  The test &lt;tt&gt;StreamingKMeansTest.test_trainOn_predictOn&lt;/tt&gt; has 2 &lt;tt&gt;DStream.foreachRDD&lt;/tt&gt; output operations, 1 in the call to &lt;tt&gt;StreamingKMeans.trainOn&lt;/tt&gt; and 1 with &lt;tt&gt;collect&lt;/tt&gt; which has a parent &lt;tt&gt;DStream&lt;/tt&gt; that is a &lt;tt&gt;PythonTransformedDStream&lt;/tt&gt; returned from &lt;tt&gt;StreamingKMeans.predictOn&lt;/tt&gt;, so 2 jobs are generated for each batch.  When the &lt;tt&gt;DStream&lt;/tt&gt; jobs are generated, there is nothing to compute for the first job, which updates the model.  For generating the second job, &lt;tt&gt;PythonTransformedDStream.compute&lt;/tt&gt; gets called which will then do a &lt;tt&gt;PythonTransformFunction&lt;/tt&gt; callback that creates a &lt;tt&gt;PythonRDD&lt;/tt&gt; and serializes the mapped predict function to a command, containing the current model.  &lt;/p&gt;

&lt;p&gt;Next, the 2 jobs are scheduled in order - first to update the model and then collect the predicted result.  At this point, there is a race condition between completing the model update and generating the next set of jobs, which is running in a different thread.  If there is enough of a delay in the update, then the next set of jobs will be generated and the old model will be serialized to the &lt;tt&gt;PythonRDD&lt;/tt&gt; command again.  Finally, the predict will be run against this old model causing the test failure.  &lt;/p&gt;

&lt;p&gt;To sum it up, the underlying issue is that a func can be serialized with a value before a job is run that updates this value.  This doesn&apos;t appear to be an issue in the Scala code as the closure cleaner is run just before the job is executed, and it will get the updated values.&lt;/p&gt;

&lt;p&gt;So far, the best solution I can think of would be to somehow delay the serialization of the model until it is needed, but I believe this would involve some big changes in &lt;tt&gt;PythonRDD&lt;/tt&gt; as would any other solutions I could think of.  Is something that would be worth doing to correct this, or might there be an easier fix that I am not seeing?  It&apos;s not just a &lt;tt&gt;StreamingKMeans&lt;/tt&gt; issue, so it would affect any PySpark streaming application with similar structure.  &lt;/p&gt;

&lt;p&gt;I am attaching some simplified code used to reproduce the issue.  I also have a similar Scala version that produces the expected results.&lt;/p&gt;</comment>
                            <comment id="15145628" author="bryanc" created="Sat, 13 Feb 2016 00:42:51 +0000"  >&lt;p&gt;Simple script &lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12787766/12787766_flakyRepro.py&quot; title=&quot;flakyRepro.py attached to SPARK-10086&quot;&gt;flakyRepro.py&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt; with similar operations to this StreamingKMeans test, used to reproduce the issue&lt;/p&gt;</comment>
                            <comment id="15180723" author="zsxwing" created="Fri, 4 Mar 2016 23:12:24 +0000"  >&lt;p&gt;I opened &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-13691&quot; title=&quot;Scala and Python generate inconsistent results&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-13691&quot;&gt;&lt;del&gt;SPARK-13691&lt;/del&gt;&lt;/a&gt; to describe the root issue in PySpark.&lt;/p&gt;</comment>
                            <comment id="15232777" author="josephkb" created="Fri, 8 Apr 2016 19:36:39 +0000"  >&lt;p&gt;I&apos;m removing the target version.  This is an important issue, but I don&apos;t see us getting a fix in for 2.0---and I have not heard of users encountering it (though the source of this bug might be hard to discern in practice).  Let&apos;s revisit for the next release.&lt;/p&gt;</comment>
                            <comment id="15232888" author="bryanc" created="Fri, 8 Apr 2016 20:46:08 +0000"  >&lt;p&gt;The changes to the test I proposed earlier are still valid, and now that the root cause has been described in &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-13691&quot; title=&quot;Scala and Python generate inconsistent results&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-13691&quot;&gt;&lt;del&gt;SPARK-13691&lt;/del&gt;&lt;/a&gt;, you could apply it and enable the test again to close this off.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310060">
                    <name>Container</name>
                                                                <inwardlinks description="Is contained by">
                                        <issuelink>
            <issuekey id="12895926">SPARK-10784</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12947124">SPARK-13691</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12787766" name="flakyRepro.py" size="987" author="bryanc" created="Sat, 13 Feb 2016 00:42:51 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 32 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2j2bz:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>