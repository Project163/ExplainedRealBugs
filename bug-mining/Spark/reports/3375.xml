<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:41:18 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-4452] Shuffle data structures can starve others on the same thread for memory </title>
                <link>https://issues.apache.org/jira/browse/SPARK-4452</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;When an Aggregator is used with ExternalSorter in a task, spark will create many small files and could cause too many files open error during merging.&lt;/p&gt;

&lt;p&gt;Currently, ShuffleMemoryManager does not work well when there are 2 spillable objects in a thread, which are ExternalSorter and ExternalAppendOnlyMap(used by Aggregator) in this case. Here is an example: Due to the usage of mapside aggregation, ExternalAppendOnlyMap is created first to read the RDD. It may ask as much memory as it can, which is totalMem/numberOfThreads. Then later on when ExternalSorter is created in the same thread, the ShuffleMemoryManager could refuse to allocate more memory to it, since the memory is already given to the previous requested object(ExternalAppendOnlyMap). That causes the ExternalSorter keeps spilling small files(due to the lack of memory)&lt;/p&gt;

&lt;p&gt;I&apos;m currently working on a PR to address these two issues. It will include following changes:&lt;/p&gt;

&lt;p&gt;1. The ShuffleMemoryManager should not only track the memory usage for each thread, but also the object who holds the memory&lt;br/&gt;
2. The ShuffleMemoryManager should be able to trigger the spilling of a spillable object. In this way, if a new object in a thread is requesting memory, the old occupant could be evicted/spilled. Previously the spillable objects trigger spilling by themselves. So one may not trigger spilling even if another object in the same thread needs more memory. After this change The ShuffleMemoryManager could trigger the spilling of an object if it needs to.&lt;br/&gt;
3. Make the iterator of ExternalAppendOnlyMap spillable. Previously ExternalAppendOnlyMap returns an destructive iterator and can not be spilled after the iterator is returned. This should be changed so that even after the iterator is returned, the ShuffleMemoryManager can still spill it.&lt;/p&gt;

&lt;p&gt;Currently, I have a working branch in progress: &lt;a href=&quot;https://github.com/tsdeng/spark/tree/enhance_memory_manager&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/tsdeng/spark/tree/enhance_memory_manager&lt;/a&gt;. Already made change 3 and have a prototype of change 1 and 2 to evict spillable from memory manager, still in progress. I will send a PR when it&apos;s done.&lt;/p&gt;

&lt;p&gt;Any feedback or thoughts on this change is highly appreciated !&lt;/p&gt;</description>
                <environment></environment>
        <key id="12755858">SPARK-4452</key>
            <summary>Shuffle data structures can starve others on the same thread for memory </summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="lianhuiwang">Lianhui Wang</assignee>
                                    <reporter username="tianshuo">Tim</reporter>
                        <labels>
                    </labels>
                <created>Mon, 17 Nov 2014 17:51:31 +0000</created>
                <updated>Mon, 20 Jun 2016 17:01:48 +0000</updated>
                            <resolved>Thu, 21 Apr 2016 17:02:38 +0000</resolved>
                                    <version>1.1.0</version>
                                    <fixVersion>2.0.0</fixVersion>
                                    <component>Spark Core</component>
                        <due></due>
                            <votes>8</votes>
                                    <watches>34</watches>
                                                                                                                <comments>
                            <comment id="14214903" author="apachespark" created="Mon, 17 Nov 2014 17:52:28 +0000"  >&lt;p&gt;User &apos;tsdeng&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/3302&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/3302&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14214962" author="tianshuo" created="Mon, 17 Nov 2014 18:32:23 +0000"  >&lt;p&gt;Originally, we found this problem by seeing Too Many Files Open exception when running SVD job on a big dataset. And it&apos;s caused by ExternalSorter dumping too many small files.&lt;/p&gt;

&lt;p&gt;With the fixes mentioned above(with the prototype), the problem is resolved. I&apos;m still working on finalizing the prototype.&lt;/p&gt;</comment>
                            <comment id="14215018" author="sandyr" created="Mon, 17 Nov 2014 19:21:09 +0000"  >&lt;p&gt;I haven&apos;t thought the implications out fully, but it worries me that data structures wouldn&apos;t be in charge of their own spilling.  It seems like for different data structures, different spilling patterns and sizes could be more efficient, and placing control in the hands of the shuffle memory manager negates this.&lt;/p&gt;

&lt;p&gt;Another fix (simpler but maybe less flexible) worth considering would be to define a static fraction of memory that goes to each of the aggregator and the external sorter, right?  One way that could be implemented is that each time the aggregator gets memory, it allocates some for the sorter.&lt;/p&gt;

&lt;p&gt;Also, for &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-2926&quot; title=&quot;Add MR-style (merge-sort) SortShuffleReader for sort-based shuffle&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-2926&quot;&gt;&lt;del&gt;SPARK-2926&lt;/del&gt;&lt;/a&gt;, we are working on a tiered merger that would avoid the &quot;Too many open files&quot; problem by only merging a limited number of files at once.&lt;/p&gt;</comment>
                            <comment id="14215047" author="sandyr" created="Mon, 17 Nov 2014 19:33:02 +0000"  >&lt;p&gt;A third possible fix would be to have the shuffle memory manager consider fairness in terms of spillable data structures instead of threads.&lt;/p&gt;</comment>
                            <comment id="14215222" author="tianshuo" created="Mon, 17 Nov 2014 21:39:24 +0000"  >&lt;p&gt;Hi, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sandyr&quot; class=&quot;user-hover&quot; rel=&quot;sandyr&quot;&gt;sandyr&lt;/a&gt;:&lt;br/&gt;
Your concern about data structures wouldn&apos; be in charge of their spilling is legit. That&apos;s why I&apos;m trying to make a incremental change:&lt;br/&gt;
1. The data structure still asks the ShuffleMemoryManager and decides if to spill itself.&lt;br/&gt;
2. But ShuffleMemoryManager can also trigger the spill of an object if the memory quota of a thread is used up.&lt;/p&gt;

&lt;p&gt;2 happens as a last resort when memory is not enough for the requesting object.&lt;br/&gt;
Also as you mentioned in the third solution, if the shuffle manager consider fairness among objs&#65292;it has to have a way to trigger the spilling of an object in a situation where current allocation is &quot;not fair&quot;. The memory manager has more of a global knowledge about memory allocation, so giving spilling ability to the manager could lead to more optimal memory allocation. If the spilling can only be triggered from the object itself, like currently, one obj may not be aware of the memory usage of other objs and keep holding the memory.&lt;/p&gt;

&lt;p&gt;My point is the data structure should be able to trigger spilling by itself, but it should also be able to handle when shuffleManager asks it to spill. I&apos;m also considering the obj can reject to spill itself do address the concern you mentioned.&lt;/p&gt;


</comment>
                            <comment id="14215233" author="tianshuo" created="Mon, 17 Nov 2014 21:45:22 +0000"  >&lt;p&gt;Currently, the two instances of Spillable, ExternalSorter and ExternalAppendOnlyMap does not seem to have complex logic to decide when to spill. They all ask as much memory as it can. I also consider this general strategy of asking as much memory as it can would apply to most cases. So I guess it would be better if the memory manager could trigger the spilling. Also make the spillable to reject a spilling request could possibly address your concern?&lt;/p&gt;</comment>
                            <comment id="14215269" author="sandyr" created="Mon, 17 Nov 2014 22:05:55 +0000"  >&lt;p&gt;Updated the title to reflect the specific problem.&lt;/p&gt;</comment>
                            <comment id="14215395" author="andrewor14" created="Mon, 17 Nov 2014 23:27:42 +0000"  >&lt;p&gt;Hey &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tianshuo&quot; class=&quot;user-hover&quot; rel=&quot;tianshuo&quot;&gt;tianshuo&lt;/a&gt; do you see this issue only for sort-based shuffle? Have you been able to reproduce it on hash-based shuffle?&lt;/p&gt;</comment>
                            <comment id="14215411" author="tianshuo" created="Mon, 17 Nov 2014 23:33:27 +0000"  >&lt;p&gt;Hi, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=andrewor14&quot; class=&quot;user-hover&quot; rel=&quot;andrewor14&quot;&gt;andrewor14&lt;/a&gt;:&lt;br/&gt;
Actually hash-based shuffle does not go as bad as sort-based shuffle on this particular problem. We were able to bypass this problem by using hash-based shuffle. This problem was so bad for me also because the elementsRead bug, so that could be also another reason why hash-based shuffle didn&apos;t break as badly.&lt;/p&gt;</comment>
                            <comment id="14215418" author="tianshuo" created="Mon, 17 Nov 2014 23:36:22 +0000"  >&lt;p&gt;Hi, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=andrewor14&quot; class=&quot;user-hover&quot; rel=&quot;andrewor14&quot;&gt;andrewor14&lt;/a&gt;:&lt;br/&gt;
The elementsRead bug that makes the situation so bad and went to &quot;too many files open&quot; is fixed here: &lt;a href=&quot;https://github.com/apache/spark/pull/3302&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/3302&lt;/a&gt;.&lt;br/&gt;
I will send another PR for the memory starving problem mentioned in this ticket soon.&lt;/p&gt;</comment>
                            <comment id="14215425" author="matei" created="Mon, 17 Nov 2014 23:44:05 +0000"  >&lt;p&gt;How much of this gets fixed if you fix the elementsRead bug in ExternalSorter?&lt;/p&gt;

&lt;p&gt;With forcing data structures to spill, the problem is that it will introduce complexity in every spillable data structure. I wonder if we can make it just give out memory in smaller increments, so that threads check whether they should spill more often. In addition, we can set a better minimum or maximum on each thread (e.g. always let it ramp up to, say, 5 MB, or some fraction of the memory space).&lt;/p&gt;

&lt;p&gt;I do like the idea of making the ShuffleMemoryManager track limits per object. I actually considered this when I wrote that and didn&apos;t do it, possibly because it would&apos;ve created more complexity in figuring out when an object is done. But it seems like it should be straightforward to add in, as long as you also track which objects come from which thread so that you can still releaseMemoryForThisThread() to clean up.&lt;/p&gt;</comment>
                            <comment id="14215427" author="andrewor14" created="Mon, 17 Nov 2014 23:46:00 +0000"  >&lt;p&gt;I see, in other words, there are two separate issues affecting sort-based shuffle:&lt;/p&gt;

&lt;p&gt;1. The `elementsRead` variable is not updated&lt;br/&gt;
2. External data structures starve each other if they&apos;re in the same thread&lt;/p&gt;

&lt;p&gt;where (2) is also common in hash-based shuffle. Your PR &lt;a href=&quot;https://github.com/apache/spark/pull/3302&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/3302&lt;/a&gt; fixes (1), but we still need to address (2) at some point. However, fixing (1) is important enough because we previously just unconditionally spilled every 32 records after a while.&lt;/p&gt;</comment>
                            <comment id="14215434" author="tianshuo" created="Mon, 17 Nov 2014 23:51:27 +0000"  >&lt;p&gt;Hi, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=andrewor14&quot; class=&quot;user-hover&quot; rel=&quot;andrewor14&quot;&gt;andrewor14&lt;/a&gt;,&lt;br/&gt;
Yeah exactly. Actually this ticket is more for addressing the (2) problem, I have a branch in progress for that: &lt;a href=&quot;https://github.com/tsdeng/spark/tree/enhance_memory_manager&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/tsdeng/spark/tree/enhance_memory_manager&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It&apos;s still a prototype, but greatly alleviate the problem for us. Just trying to finalize that.&lt;/p&gt;</comment>
                            <comment id="14215436" author="sandyr" created="Mon, 17 Nov 2014 23:54:34 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=andrewor14&quot; class=&quot;user-hover&quot; rel=&quot;andrewor14&quot;&gt;andrewor14&lt;/a&gt;, IIUC, (2) shouldn&apos;t happen in hash-based shuffle at all, because hash-based shuffle doesn&apos;t use multiple spillable data structures in each task.&lt;/p&gt;</comment>
                            <comment id="14215446" author="andrewor14" created="Mon, 17 Nov 2014 23:59:00 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sandyr&quot; class=&quot;user-hover&quot; rel=&quot;sandyr&quot;&gt;sandyr&lt;/a&gt; hash-based shuffle can still use two ExternalAppendOnlyMaps in 1 task if you have back-to-back shuffles where the second shuffle does a map-side combine.&lt;/p&gt;</comment>
                            <comment id="14215448" author="sandyr" created="Tue, 18 Nov 2014 00:01:32 +0000"  >&lt;p&gt;Ah, true.&lt;/p&gt;</comment>
                            <comment id="14215449" author="tianshuo" created="Tue, 18 Nov 2014 00:01:38 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=matei&quot; class=&quot;user-hover&quot; rel=&quot;matei&quot;&gt;matei&lt;/a&gt;:&lt;br/&gt;
You are right, it does add more complexity if we force the data structure to spill. But in my prototype branch I already made changes to ExternalSorter and ExternalAppendOnlyMap to make it support that. And it&apos;s not too hard and doable. &lt;br/&gt;
In terms of coding, it does add complexity, but the property we get from it is pretty nice: able to spill the object as we want to.&lt;br/&gt;
Also ExternalSorter and ExternalAppendOnlyMap are the only two that need to be changed.&lt;/p&gt;

&lt;p&gt;For your question, after fixing the elementsRead bug, we do not see the exception, but could still see tons of small files due to the memory starvation.&lt;/p&gt;</comment>
                            <comment id="14215491" author="apachespark" created="Tue, 18 Nov 2014 00:35:02 +0000"  >&lt;p&gt;User &apos;andrewor14&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/3330&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/3330&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14215556" author="matei" created="Tue, 18 Nov 2014 01:20:50 +0000"  >&lt;p&gt;Got it. It would be fine to do this if you found it to help, I was just wondering whether simpler fixes would get us far enough. For the forced spilling change, I&apos;d suggest writing a short design doc, or making sure that the comments in the code about it are very detailed (essentially having a design doc at the top of the class). This can have a lot of tricky cases due to concurrency so it&apos;s important to document the design.&lt;/p&gt;</comment>
                            <comment id="14215557" author="matei" created="Tue, 18 Nov 2014 01:21:26 +0000"  >&lt;p&gt;BTW we may also want to create a separate JIRA for the short-term fix for 1.1 and 1.2.&lt;/p&gt;</comment>
                            <comment id="14215617" author="andrewor14" created="Tue, 18 Nov 2014 02:08:57 +0000"  >&lt;p&gt;I have created &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-4467&quot; title=&quot;Number of elements read is never reset in ExternalSorter&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-4467&quot;&gt;&lt;del&gt;SPARK-4467&lt;/del&gt;&lt;/a&gt; for the `elementsRead` bug since the bigger issue here is distinct.&lt;/p&gt;</comment>
                            <comment id="14216691" author="matei" created="Tue, 18 Nov 2014 19:57:04 +0000"  >&lt;p&gt;BTW I&apos;ve thought about this more and here&apos;s what I&apos;d suggest: try a version where each object is allowed to ramp up to a certain size (say 5 MB) before being subject to the limit, and if that doesn&apos;t work, then maybe go for the forced-spilling one. The reason is that as soon as N objects are active, the ShuffleMemoryManager will not let any object ramp up to more than 1/N, so it just has to fill up its current quota and stop. This means that scenarios with very little free memory might only happen at the beginning (when tasks start up). If we can make this work, then we avoid a lot of concurrency problems that would happen with forced spilling. &lt;/p&gt;

&lt;p&gt;Another improvement would be to make the Spillables request less than 2x their current memory when they ramp up, e.g. 1.5x. They&apos;d then make more requests but it would lead to slower ramp-up and more of a chance for other threads to grab memory. But I think this will have less impact than simply increasing that free minimum amount.&lt;/p&gt;</comment>
                            <comment id="14216933" author="sandyr" created="Tue, 18 Nov 2014 22:29:05 +0000"  >&lt;p&gt;One issue with a limits-by-object approach is that it could result in extra wasted memory over the current approach for tasks that produce less shuffle data than they read.  E.g. consider a rdd.reduceByKey(...).map(...).reduceByKey(...)...&lt;/p&gt;

&lt;p&gt;The object aggregating inputs used to have access to the full memory allotted to the task, but now it only gets half the memory.  In situations where the object aggregating outputs doesn&apos;t need as much memory (because there is less output data), some of the memory that previously would have been used is unused.&lt;/p&gt;

&lt;p&gt;A forced spilling approach seems like it could give some of the advantages that preemption provides in cluster scheduling - better utilization through enabling objects to use more than their &quot;fair&quot; amount until it turns out other objects need those resources.&lt;/p&gt;</comment>
                            <comment id="14217253" author="andrewor14" created="Wed, 19 Nov 2014 01:54:36 +0000"  >&lt;p&gt;I have opened a JIRA that targets on fixing this on a smaller scope: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-4480&quot; title=&quot;Avoid many small spills in external data structures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-4480&quot;&gt;&lt;del&gt;SPARK-4480&lt;/del&gt;&lt;/a&gt;. I intend to pull this smaller fix into 1.1.1, and maybe it&apos;s sufficient for 1.2.0. This particular JIRA (&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-4452&quot; title=&quot;Shuffle data structures can starve others on the same thread for memory &quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-4452&quot;&gt;&lt;del&gt;SPARK-4452&lt;/del&gt;&lt;/a&gt;) likely involves a much bigger change that is too ambitious for either release at the moment.&lt;/p&gt;</comment>
                            <comment id="14217274" author="andrewor14" created="Wed, 19 Nov 2014 02:09:39 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=matei&quot; class=&quot;user-hover&quot; rel=&quot;matei&quot;&gt;matei&lt;/a&gt; I have implemented your first suggestion here: &lt;a href=&quot;https://github.com/apache/spark/pull/3353&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/3353&lt;/a&gt;. In my particular workload, I&apos;ve noticed at least an order of magnitude reduction in the number of shuffle files written. More details provided in the PR.&lt;/p&gt;</comment>
                            <comment id="14217331" author="matei" created="Wed, 19 Nov 2014 03:12:12 +0000"  >&lt;p&gt;Forced spilling is orthogonal to how you set the limits actually. For example, if there are N objects, one way to set limits is to reserve at least 1/N of memory for each one. But another way would be to group them by thread, and use a different algorithm for allocation within a thread (e.g. set each object&apos;s cap to more if other objects in their thread are using less). Whether you force spilling or not, you&apos;ll have to decide what the right limit for each thing is.&lt;/p&gt;</comment>
                            <comment id="14217340" author="sandyr" created="Wed, 19 Nov 2014 03:27:33 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=matei&quot; class=&quot;user-hover&quot; rel=&quot;matei&quot;&gt;matei&lt;/a&gt; my point is not that forced spilling allows us to avoid setting limits, but that it allows those limits to be soft: if an entity (thread or object) is not requesting the 1/N memory reserved for it, that memory can be given to other entities that need it.  Then, if the entity later requests the memory reserved to it, the other entities above their fair allocation can be forced to spill.&lt;/p&gt;

&lt;p&gt;(I don&apos;t necessarily mean to argue that this advantage is worth the added complexity.)&lt;/p&gt;</comment>
                            <comment id="14218289" author="tianshuo" created="Wed, 19 Nov 2014 18:35:13 +0000"  >&lt;p&gt;Hi,&lt;br/&gt;
While I&apos;m working on this ticket, I have an related question:&lt;br/&gt;
I noticed an extra constraint in the usage of ExternalAppendOnlyMap. &lt;br/&gt;
Even in the current implementation(master), If an ExternalAppendOnlyMap exported a iterator(spilled), you can not get the iterator again, since the memory iterator is destructive.&lt;br/&gt;
But in our unit tests, the constraint seems to be ignored... many tests are calling iterator multiple times. It works because the data is small and does not trigger the spilling in unit test.&lt;/p&gt;

&lt;p&gt;But I just want to confirm, if it&apos;s ok I explicitly adding this constraint to the code and unit test: Iterator of an ExternalAppendOnlyMap can only be exported once&lt;/p&gt;</comment>
                            <comment id="14218318" author="andrewor14" created="Wed, 19 Nov 2014 19:05:16 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tianshuo&quot; class=&quot;user-hover&quot; rel=&quot;tianshuo&quot;&gt;tianshuo&lt;/a&gt; That is a correct assumption for ExternalAppendOnlyMap: once it has spilled and we called `iterator`, which destroyed the underlying map, we should not be able to call `iterator` again or insert any items into the map. We should really document that clearly, but your understanding is correct.&lt;/p&gt;</comment>
                            <comment id="14219795" author="tianshuo" created="Thu, 20 Nov 2014 18:57:24 +0000"  >&lt;p&gt;Hi, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=matei&quot; class=&quot;user-hover&quot; rel=&quot;matei&quot;&gt;matei&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;My way of implementing it is more like the 2nd way you suggested. I will put up a design doc. But I would like to give a preview of my implementation first&lt;/p&gt;

&lt;p&gt; I already implemented following and seems work for me&lt;/p&gt;

&lt;p&gt;1. Memory Allocation and spilling is divided into two levels. SpillableTaskMemoryManager for memory allocation and spilling of current thread/task. ShuffleMemoryManager coordinates memory allocation among threads/tasks&lt;/p&gt;

&lt;p&gt;2. SpillableTaskMemoryManager: objects are grouped by threads, each STMM maps to one thread/task. If an object requires more memory, it asks STMM for it. STMM will ask ShuffleMemoryManager for more memory for current thread. if the returned memory does not satisfy the request, it will tries to spill objs in current thread to give up memory. Notice the objects it may spill are thread-local, so there is no contention&lt;/p&gt;

&lt;p&gt;3. ShuffleMemoryManager: The algorithm in thread memory allocation is basically unchanged. Only thing is that spillables do not ask SMM directly for more memory, instead STMM asks for memory for the thread.&lt;/p&gt;

&lt;p&gt;By making this change, spilling is triggered from STMM. This design has following properties in mind:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Incremental change, thread memory allocation algorithm is unchanged. This way each task/thread get a fair share of memory.&lt;/li&gt;
	&lt;li&gt;Spiling is thread local and is triggered from STMM to avoid unnecessary locking and contention.&lt;/li&gt;
	&lt;li&gt;Two levels of memory allocation makes a distinction between allocating memory for tasks and allocating memory/spilling objs in the current task. This distinction makes contention management more clear and easier&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14220070" author="tianshuo" created="Thu, 20 Nov 2014 21:48:52 +0000"  >&lt;p&gt;Here is a link of the diff:&lt;br/&gt;
&lt;a href=&quot;https://github.com/tsdeng/spark/compare/fix_memory_starvation?expand=1&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/tsdeng/spark/compare/fix_memory_starvation?expand=1&lt;/a&gt;&lt;br/&gt;
It&apos;s still in progress and needs lots of cleanups, but could show the idea of the design.&lt;br/&gt;
Hope can get some early feedback on this route&lt;/p&gt;</comment>
                            <comment id="14222572" author="sandyr" created="Mon, 24 Nov 2014 01:33:59 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tianshuo&quot; class=&quot;user-hover&quot; rel=&quot;tianshuo&quot;&gt;tianshuo&lt;/a&gt;, I took a look at the patch, and the general approach looks reasonable to me.&lt;/p&gt;

&lt;p&gt;A couple additional thoughts that apply both to the current approach and Tianshuo&apos;s patch:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;When we chain an ExternalAppendOnlyMap to an ExternalSorter for processing combined map outputs in sort based shuffle, we end up double counting, no? Both data structures will be holding references to the same objects and estimating their size based on these objects.&lt;/li&gt;
	&lt;li&gt;We could make the in-memory iterators destructive as well right?  I.e. if the data structures can release references to objects as they yield them, then we can give memory back to the shuffle memory manager and make it available to other data structures in the same thread.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;If we can avoid double and holding on to unneeded objects, it would obviate some of the need for intra-thread limits / forced spilling.&lt;/p&gt;</comment>
                            <comment id="14224996" author="tianshuo" created="Tue, 25 Nov 2014 18:55:16 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sandyr&quot; class=&quot;user-hover&quot; rel=&quot;sandyr&quot;&gt;sandyr&lt;/a&gt;:&lt;br/&gt;
Thanks for the feedback!&lt;/p&gt;

&lt;p&gt;For double counting, yes, the external data structure may results to double counting. But it only applies to the in-memory portion of the data. In my PR, in ExternalOnlyMap, once the in-memory portion is spilled, the memory is recycled(by giving an empty iterator and empty map).&lt;/p&gt;

&lt;p&gt; So there are two approaches I can do&lt;br/&gt;
1. Minor change based on my current change: also recycle the memory when memory iterator is drained&lt;br/&gt;
2. A little bigger change: Make the memory iterator destructive by nulling out the underlying element in the array when the element is returned, this also requires spillable data structure to report back the memory occupied when the iterator is being consumed, while currently it only reports the memory usage when new data is being inserted.&lt;/p&gt;

&lt;p&gt;So change 1 seems adding less constraints to the spillable data structure, what do you think?&lt;/p&gt;
</comment>
                            <comment id="14226969" author="sandyr" created="Wed, 26 Nov 2014 23:04:59 +0000"  >&lt;p&gt;Thinking about the current change a little more, an issue is that it will spill all the in-memory data to disk in situations where this is probably overkill.  E.g. consider the typical situation of shuffle data slightly exceeding memory.  We end up spilling the entire data structure if a downstream data structure needs even a small amount of memory.&lt;/p&gt;

&lt;p&gt;I think that your proposed change 2 is probably worthwhile.&lt;/p&gt;</comment>
                            <comment id="14290874" author="srowen" created="Sat, 24 Jan 2015 23:27:48 +0000"  >&lt;p&gt;Can this JIRA be resolved now that its children are resolved, or is the more to this one?&lt;/p&gt;</comment>
                            <comment id="14290885" author="sandyr" created="Sat, 24 Jan 2015 23:41:43 +0000"  >&lt;p&gt;I think there&apos;s more to this one, the subtasks solved the most egregious issues, but shuffle data structures can still hog memory in detrimental ways described in some of the comments above.&lt;/p&gt;</comment>
                            <comment id="14608618" author="apachespark" created="Tue, 30 Jun 2015 16:30:19 +0000"  >&lt;p&gt;User &apos;lianhuiwang&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/7130&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/7130&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14609144" author="joshrosen" created="Tue, 30 Jun 2015 21:55:09 +0000"  >&lt;p&gt;I&apos;ve linked this to the Project Tungsten JIRA epic, since the increased uses of spillable collections in the Tunsgten code will magnify this issue.&lt;/p&gt;</comment>
                            <comment id="15030450" author="apachespark" created="Sat, 28 Nov 2015 10:14:03 +0000"  >&lt;p&gt;User &apos;lianhuiwang&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/10024&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/10024&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15252205" author="davies" created="Thu, 21 Apr 2016 17:02:38 +0000"  >&lt;p&gt;Issue resolved by pull request 10024&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/10024&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/10024&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15255658" author="romi-totango" created="Sun, 24 Apr 2016 17:34:31 +0000"  >&lt;p&gt;Hi, what&apos;s the reason this will only be available in Spark 2.0.0, and not 1.6.4 or 1.7.0?&lt;/p&gt;</comment>
                            <comment id="15256602" author="davies" created="Mon, 25 Apr 2016 17:09:19 +0000"  >&lt;p&gt;We only backport critical bug fix into released branch.&lt;/p&gt;

&lt;p&gt;There is no 1.7.0, 2.0 will released around June 2016.&lt;/p&gt;</comment>
                            <comment id="15276109" author="xhao1" created="Mon, 9 May 2016 08:35:23 +0000"  >&lt;p&gt;Since this is an old issue which impact Spark since 1.1.0, can the patch be merged to Spark 1.6.X ? This will be very helpful for Spark 1.6.X users. Thanks.&lt;/p&gt;</comment>
                            <comment id="15277583" author="apachespark" created="Tue, 10 May 2016 04:16:03 +0000"  >&lt;p&gt;User &apos;lianhuiwang&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/13020&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/13020&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15278147" author="apachespark" created="Tue, 10 May 2016 14:13:04 +0000"  >&lt;p&gt;User &apos;lianhuiwang&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/13027&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/13027&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="12958122">SPARK-14560</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="12310010">
                    <name>Incorporates</name>
                                            <outwardlinks description="incorporates">
                                        <issuelink>
            <issuekey id="12755995">SPARK-4467</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12743081">SPARK-3633</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="12310050">
                    <name>Regression</name>
                                                                <inwardlinks description="is broken by">
                                        <issuelink>
            <issuekey id="12730103">SPARK-2711</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12311120" key="com.pyxis.greenhopper.jira:gh-epic-link">
                        <customfieldname>Epic Link</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>SPARK-9697</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 28 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i22grz:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>