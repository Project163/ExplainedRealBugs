<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:16:13 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-2898] Failed to connect to daemon</title>
                <link>https://issues.apache.org/jira/browse/SPARK-2898</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;There is a deadlock  in handle_sigchld() because of logging&lt;/p&gt;

&lt;p&gt;--------------------------------------------------------------------&lt;br/&gt;
Java options: -Dspark.storage.memoryFraction=0.66 -Dspark.serializer=org.apache.spark.serializer.JavaSerializer -Dspark.executor.memory=3g -Dspark.locality.wait=60000000&lt;br/&gt;
Options: SchedulerThroughputTest --num-tasks=10000 --num-trials=4 --inter-trial-wait=1&lt;br/&gt;
--------------------------------------------------------------------&lt;br/&gt;
14/08/06 22:09:41 WARN JettyUtils: Failed to create UI on port 4040. Trying again on port 4041. - Failure(java.net.BindException: Address already in use)&lt;br/&gt;
worker 50114 crashed abruptly with exit status 1&lt;br/&gt;
14/08/06 22:10:37 ERROR Executor: Exception in task 1476.0 in stage 1.0 (TID 11476)&lt;br/&gt;
org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)&lt;br/&gt;
	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:150)&lt;br/&gt;
	at org.apache.spark.api.python.PythonRDD$$anon$1.&amp;lt;init&amp;gt;(PythonRDD.scala:154)&lt;br/&gt;
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:87)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)&lt;br/&gt;
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)&lt;br/&gt;
	at org.apache.spark.scheduler.Task.run(Task.scala:54)&lt;br/&gt;
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
Caused by: java.io.EOFException&lt;br/&gt;
	at java.io.DataInputStream.readInt(DataInputStream.java:392)&lt;br/&gt;
	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:101)&lt;br/&gt;
	... 10 more&lt;br/&gt;
14/08/06 22:10:37 WARN PythonWorkerFactory: Failed to open socket to Python daemon:&lt;br/&gt;
java.net.ConnectException: Connection refused&lt;br/&gt;
	at java.net.PlainSocketImpl.socketConnect(Native Method)&lt;br/&gt;
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)&lt;br/&gt;
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)&lt;br/&gt;
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)&lt;br/&gt;
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)&lt;br/&gt;
	at java.net.Socket.connect(Socket.java:579)&lt;br/&gt;
	at java.net.Socket.connect(Socket.java:528)&lt;br/&gt;
	at java.net.Socket.&amp;lt;init&amp;gt;(Socket.java:425)&lt;br/&gt;
	at java.net.Socket.&amp;lt;init&amp;gt;(Socket.java:241)&lt;br/&gt;
	at org.apache.spark.api.python.PythonWorkerFactory.createSocket$1(PythonWorkerFactory.scala:68)&lt;br/&gt;
	at org.apache.spark.api.python.PythonWorkerFactory.liftedTree1$1(PythonWorkerFactory.scala:83)&lt;br/&gt;
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:82)&lt;br/&gt;
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:55)&lt;br/&gt;
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:101)&lt;br/&gt;
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:66)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)&lt;br/&gt;
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)&lt;br/&gt;
	at org.apache.spark.scheduler.Task.run(Task.scala:54)&lt;br/&gt;
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
14/08/06 22:10:37 ERROR Executor: Exception in task 1478.0 in stage 1.0 (TID 11478)&lt;br/&gt;
java.io.EOFException&lt;br/&gt;
	at java.io.DataInputStream.readInt(DataInputStream.java:392)&lt;br/&gt;
	at org.apache.spark.api.python.PythonWorkerFactory.createSocket$1(PythonWorkerFactory.scala:69)&lt;br/&gt;
	at org.apache.spark.api.python.PythonWorkerFactory.liftedTree1$1(PythonWorkerFactory.scala:83)&lt;br/&gt;
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:82)&lt;br/&gt;
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:55)&lt;br/&gt;
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:101)&lt;br/&gt;
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:66)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)&lt;br/&gt;
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)&lt;br/&gt;
	at org.apache.spark.scheduler.Task.run(Task.scala:54)&lt;br/&gt;
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
14/08/06 22:10:37 WARN PythonWorkerFactory: Assuming that daemon unexpectedly quit, attempting to restart&lt;br/&gt;
14/08/06 22:10:37 WARN TaskSetManager: Lost task 1476.0 in stage 1.0 (TID 11476, localhost): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)&lt;br/&gt;
        org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:150)&lt;br/&gt;
        org.apache.spark.api.python.PythonRDD$$anon$1.&amp;lt;init&amp;gt;(PythonRDD.scala:154)&lt;br/&gt;
        org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:87)&lt;br/&gt;
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)&lt;br/&gt;
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)&lt;br/&gt;
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)&lt;br/&gt;
        org.apache.spark.scheduler.Task.run(Task.scala:54)&lt;br/&gt;
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)&lt;br/&gt;
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br/&gt;
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
        java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
14/08/06 22:10:37 ERROR TaskSetManager: Task 1476 in stage 1.0 failed 1 times; aborting job&lt;br/&gt;
14/08/06 22:10:37 WARN TaskSetManager: Lost task 1478.0 in stage 1.0 (TID 11478, localhost): java.io.EOFException: &lt;br/&gt;
        java.io.DataInputStream.readInt(DataInputStream.java:392)&lt;br/&gt;
        org.apache.spark.api.python.PythonWorkerFactory.createSocket$1(PythonWorkerFactory.scala:69)&lt;br/&gt;
        org.apache.spark.api.python.PythonWorkerFactory.liftedTree1$1(PythonWorkerFactory.scala:83)&lt;br/&gt;
        org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:82)&lt;br/&gt;
        org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:55)&lt;br/&gt;
        org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:101)&lt;br/&gt;
        org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:66)&lt;br/&gt;
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)&lt;br/&gt;
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)&lt;br/&gt;
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)&lt;br/&gt;
        org.apache.spark.scheduler.Task.run(Task.scala:54)&lt;br/&gt;
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)&lt;br/&gt;
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br/&gt;
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
        java.lang.Thread.run(Thread.java:745)&lt;/p&gt;


&lt;p&gt;Another one:&lt;/p&gt;

&lt;p&gt;Daemon failed to fork PySpark worker: &lt;span class=&quot;error&quot;&gt;&amp;#91;Errno 35&amp;#93;&lt;/span&gt; Resource temporarily unavailable&lt;br/&gt;
14/08/07 12:04:37 ERROR Executor: Exception in task 15579.0 in stage 0.0 (TID 15579)&lt;br/&gt;
java.lang.IllegalStateException: Python daemon failed to launch worker&lt;br/&gt;
	at org.apache.spark.api.python.PythonWorkerFactory.createSocket$1(PythonWorkerFactory.scala:71)&lt;br/&gt;
	at org.apache.spark.api.python.PythonWorkerFactory.liftedTree1$1(PythonWorkerFactory.scala:83)&lt;br/&gt;
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:82)&lt;br/&gt;
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:55)&lt;br/&gt;
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:101)&lt;br/&gt;
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:66)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)&lt;br/&gt;
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)&lt;br/&gt;
	at org.apache.spark.scheduler.Task.run(Task.scala:54)&lt;br/&gt;
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
14/08/07 12:04:37 WARN TaskSetManager: Lost task 15579.0 in stage 0.0 (TID 15579, localhost): java.lang.IllegalStateException: Python daemon failed to launch worker&lt;br/&gt;
        org.apache.spark.api.python.PythonWorkerFactory.createSocket$1(PythonWorkerFactory.scala:71)&lt;br/&gt;
        org.apache.spark.api.python.PythonWorkerFactory.liftedTree1$1(PythonWorkerFactory.scala:83)&lt;br/&gt;
        org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:82)&lt;br/&gt;
        org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:55)&lt;br/&gt;
        org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:101)&lt;br/&gt;
        org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:66)&lt;br/&gt;
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)&lt;br/&gt;
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)&lt;br/&gt;
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)&lt;br/&gt;
        org.apache.spark.scheduler.Task.run(Task.scala:54)&lt;br/&gt;
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)&lt;br/&gt;
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br/&gt;
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
        java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
14/08/07 12:04:37 ERROR TaskSetManager: Task 15579 in stage 0.0 failed 1 times; aborting job&lt;/p&gt;



&lt;p&gt;worker 17037 crashed abruptly with exit status 1&lt;br/&gt;
14/08/07 12:06:34 ERROR Executor: Exception in task 19607.0 in stage 0.0 (TID 19607)&lt;br/&gt;
java.io.EOFException&lt;br/&gt;
	at java.io.DataInputStream.readInt(DataInputStream.java:392)&lt;br/&gt;
	at org.apache.spark.api.python.PythonWorkerFactory.createSocket$1(PythonWorkerFactory.scala:69)&lt;br/&gt;
	at org.apache.spark.api.python.PythonWorkerFactory.liftedTree1$1(PythonWorkerFactory.scala:83)&lt;br/&gt;
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:82)&lt;br/&gt;
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:55)&lt;br/&gt;
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:101)&lt;br/&gt;
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:66)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)&lt;br/&gt;
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)&lt;br/&gt;
	at org.apache.spark.scheduler.Task.run(Task.scala:54)&lt;br/&gt;
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
14/08/07 12:06:34 WARN PythonWorkerFactory: Failed to open socket to Python daemon:&lt;br/&gt;
java.net.ConnectException: Connection refused&lt;br/&gt;
	at java.net.PlainSocketImpl.socketConnect(Native Method)&lt;br/&gt;
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)&lt;br/&gt;
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)&lt;br/&gt;
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)&lt;br/&gt;
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)&lt;br/&gt;
	at java.net.Socket.connect(Socket.java:579)&lt;br/&gt;
	at java.net.Socket.connect(Socket.java:528)&lt;br/&gt;
	at java.net.Socket.&amp;lt;init&amp;gt;(Socket.java:425)&lt;br/&gt;
	at java.net.Socket.&amp;lt;init&amp;gt;(Socket.java:241)&lt;br/&gt;
	at org.apache.spark.api.python.PythonWorkerFactory.createSocket$1(PythonWorkerFactory.scala:68)&lt;br/&gt;
	at org.apache.spark.api.python.PythonWorkerFactory.liftedTree1$1(PythonWorkerFactory.scala:83)&lt;br/&gt;
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:82)&lt;br/&gt;
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:55)&lt;br/&gt;
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:101)&lt;br/&gt;
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:66)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)&lt;br/&gt;
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)&lt;br/&gt;
	at org.apache.spark.scheduler.Task.run(Task.scala:54)&lt;br/&gt;
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
4/08/07 12:06:34 ERROR Executor: Exception in task 19604.0 in stage 0.0 (TID 19604)&lt;br/&gt;
org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)&lt;br/&gt;
	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:150)&lt;br/&gt;
	at org.apache.spark.api.python.PythonRDD$$anon$1.&amp;lt;init&amp;gt;(PythonRDD.scala:154)&lt;br/&gt;
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:87)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)&lt;br/&gt;
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)&lt;br/&gt;
	at org.apache.spark.scheduler.Task.run(Task.scala:54)&lt;br/&gt;
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
Caused by: java.io.EOFException&lt;br/&gt;
	at java.io.DataInputStream.readInt(DataInputStream.java:392)&lt;br/&gt;
	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:101)&lt;br/&gt;
	... 10 more&lt;br/&gt;
14/08/07 12:06:34 WARN PythonWorkerFactory: Assuming that daemon unexpectedly quit, attempting to restart&lt;br/&gt;
14/08/07 12:06:34 WARN TaskSetManager: Lost task 19604.0 in stage 0.0 (TID 19604, localhost): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)&lt;br/&gt;
        org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:150)&lt;br/&gt;
        org.apache.spark.api.python.PythonRDD$$anon$1.&amp;lt;init&amp;gt;(PythonRDD.scala:154)&lt;br/&gt;
        org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:87)&lt;br/&gt;
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)&lt;br/&gt;
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)&lt;br/&gt;
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)&lt;br/&gt;
        org.apache.spark.scheduler.Task.run(Task.scala:54)&lt;br/&gt;
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)&lt;br/&gt;
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br/&gt;
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
        java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
14/08/07 12:06:34 ERROR TaskSetManager: Task 19604 in stage 0.0 failed 1 times; aborting job&lt;br/&gt;
14/08/07 12:06:34 WARN TaskSetManager: Lost task 19607.0 in stage 0.0 (TID 19607, localhost): java.io.EOFException:&lt;br/&gt;
        java.io.DataInputStream.readInt(DataInputStream.java:392)&lt;br/&gt;
        org.apache.spark.api.python.PythonWorkerFactory.createSocket$1(PythonWorkerFactory.scala:69)&lt;br/&gt;
        org.apache.spark.api.python.PythonWorkerFactory.liftedTree1$1(PythonWorkerFactory.scala:83)&lt;br/&gt;
        org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:82)&lt;br/&gt;
        org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:55)&lt;br/&gt;
        org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:101)&lt;br/&gt;
        org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:66)&lt;br/&gt;
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)&lt;br/&gt;
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)&lt;br/&gt;
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)&lt;br/&gt;
        org.apache.spark.scheduler.Task.run(Task.scala:54)&lt;br/&gt;
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)&lt;br/&gt;
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br/&gt;
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
        java.lang.Thread.run(Thread.java:745)&lt;/p&gt;


&lt;p&gt;14/08/07 13:29:01 WARN PythonWorkerFactory: Assuming that daemon unexpectedly quit, attempting to restart&lt;br/&gt;
14/08/07 13:29:01 ERROR Executor: Exception in task 9085.0 in stage 0.0 (TID 9085)&lt;br/&gt;
java.io.IOException: Cannot run program &quot;python&quot;: error=2, No such file or directory&lt;br/&gt;
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1041)&lt;br/&gt;
	at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:149)&lt;br/&gt;
	at org.apache.spark.api.python.PythonWorkerFactory.liftedTree1$1(PythonWorkerFactory.scala:89)&lt;br/&gt;
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:82)&lt;br/&gt;
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:55)&lt;br/&gt;
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:101)&lt;br/&gt;
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:66)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)&lt;br/&gt;
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)&lt;br/&gt;
	at org.apache.spark.scheduler.Task.run(Task.scala:54)&lt;br/&gt;
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
Caused by: java.io.IOException: error=2, No such file or directory&lt;br/&gt;
	at java.lang.UNIXProcess.forkAndExec(Native Method)&lt;br/&gt;
	at java.lang.UNIXProcess.&amp;lt;init&amp;gt;(UNIXProcess.java:184)&lt;br/&gt;
	at java.lang.ProcessImpl.start(ProcessImpl.java:130)&lt;br/&gt;
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1022)&lt;br/&gt;
	... 14 more&lt;br/&gt;
14/08/07 13:29:01 ERROR Executor: Exception in task 9084.0 in stage 0.0 (TID 9084)&lt;br/&gt;
java.io.IOException: Cannot run program &quot;python&quot;: error=316, Unknown error: 316&lt;br/&gt;
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1041)&lt;br/&gt;
	at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:149)&lt;br/&gt;
	at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:79)&lt;br/&gt;
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:55)&lt;br/&gt;
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:101)&lt;br/&gt;
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:66)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)&lt;br/&gt;
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)&lt;br/&gt;
	at org.apache.spark.scheduler.Task.run(Task.scala:54)&lt;br/&gt;
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
Caused by: java.io.IOException: error=316, Unknown error: 316&lt;br/&gt;
	at java.lang.UNIXProcess.forkAndExec(Native Method)&lt;br/&gt;
	at java.lang.UNIXProcess.&amp;lt;init&amp;gt;(UNIXProcess.java:184)&lt;br/&gt;
	at java.lang.ProcessImpl.start(ProcessImpl.java:130)&lt;br/&gt;
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1022)&lt;br/&gt;
	... 13 more&lt;/p&gt;

</description>
                <environment></environment>
        <key id="12732466">SPARK-2898</key>
            <summary>Failed to connect to daemon</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="davies">Davies Liu</assignee>
                                    <reporter username="davies">Davies Liu</reporter>
                        <labels>
                    </labels>
                <created>Thu, 7 Aug 2014 05:19:31 +0000</created>
                <updated>Mon, 15 Jun 2015 11:34:09 +0000</updated>
                            <resolved>Sun, 10 Aug 2014 20:02:00 +0000</resolved>
                                    <version>1.1.0</version>
                                    <fixVersion>1.1.0</fixVersion>
                                    <component>PySpark</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="14089915" author="apachespark" created="Thu, 7 Aug 2014 21:57:23 +0000"  >&lt;p&gt;User &apos;davies&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/1842&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/1842&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14585809" author="taylor_tails" created="Mon, 15 Jun 2015 11:34:09 +0000"  >&lt;p&gt;FYI &lt;/p&gt;

&lt;p&gt;java.io.IOException: Cannot run program &quot;python&quot;: error=316, Unknown error: 316&lt;/p&gt;

&lt;p&gt;I have seen this error to occur on mac because lib/jspawnhelper is missing execute permissions in your jre.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>410494</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 23 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1ymi7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>410488</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>