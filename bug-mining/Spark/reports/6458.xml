<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 19:05:13 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-23961] pyspark toLocalIterator throws an exception</title>
                <link>https://issues.apache.org/jira/browse/SPARK-23961</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Given a dataframe and use toLocalIterator. If we do not consume all records, it will throw:&#160;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;ERROR PythonRDD: Error while sending iterator&lt;br/&gt;
 java.net.SocketException: Connection reset by peer: socket write error&lt;br/&gt;
 at java.net.SocketOutputStream.socketWrite0(Native Method)&lt;br/&gt;
 at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)&lt;br/&gt;
 at java.net.SocketOutputStream.write(SocketOutputStream.java:155)&lt;br/&gt;
 at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)&lt;br/&gt;
 at java.io.DataOutputStream.write(DataOutputStream.java:107)&lt;br/&gt;
 at java.io.FilterOutputStream.write(FilterOutputStream.java:97)&lt;br/&gt;
 at org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:497)&lt;br/&gt;
 at org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:509)&lt;br/&gt;
 at org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:509)&lt;br/&gt;
 at scala.collection.Iterator$class.foreach(Iterator.scala:893)&lt;br/&gt;
 at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)&lt;br/&gt;
 at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:509)&lt;br/&gt;
 at org.apache.spark.api.python.PythonRDD$$anon$2$$anonfun$run$1.apply$mcV$sp(PythonRDD.scala:705)&lt;br/&gt;
 at org.apache.spark.api.python.PythonRDD$$anon$2$$anonfun$run$1.apply(PythonRDD.scala:705)&lt;br/&gt;
 at org.apache.spark.api.python.PythonRDD$$anon$2$$anonfun$run$1.apply(PythonRDD.scala:705)&lt;br/&gt;
 at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)&lt;br/&gt;
 at org.apache.spark.api.python.PythonRDD$$anon$2.run(PythonRDD.scala:706)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;To reproduce, here is a simple pyspark shell script that show the error:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;import itertools&lt;br/&gt;
 df = spark.read.parquet(&quot;large parquet folder&quot;).cache()&lt;br/&gt;
print(df.count())&lt;br/&gt;
 b = df.toLocalIterator()&lt;br/&gt;
 print(len(list(itertools.islice(b, 20))))&lt;br/&gt;
 b = None # Make the iterator goes out of scope.&#160; Throws here.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Consuming all records do not throw.&#160; Taking only a subset of the partitions create the error.&lt;/li&gt;
	&lt;li&gt;In another experiment, doing the same on a regular RDD works if we cache/materialize it. If we do not cache the RDD, it throws similarly.&lt;/li&gt;
	&lt;li&gt;It works in scala shell&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&#160;&lt;/p&gt;</description>
                <environment></environment>
        <key id="13151638">SPARK-23961</key>
            <summary>pyspark toLocalIterator throws an exception</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Minor</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="bryanc">Bryan Cutler</assignee>
                                    <reporter username="FlamingMike">Michel Lemay</reporter>
                        <labels>
                            <label>DataFrame</label>
                            <label>pyspark</label>
                    </labels>
                <created>Wed, 11 Apr 2018 12:38:35 +0000</created>
                <updated>Mon, 12 Dec 2022 18:10:34 +0000</updated>
                            <resolved>Tue, 7 May 2019 21:48:45 +0000</resolved>
                                    <version>2.0.2</version>
                    <version>2.1.2</version>
                    <version>2.2.1</version>
                    <version>2.3.0</version>
                                    <fixVersion>3.0.0</fixVersion>
                                    <component>PySpark</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="16434799" author="gurwls223" created="Thu, 12 Apr 2018 01:20:36 +0000"  >&lt;p&gt;FWIW, I met this issue a while ago too (and I gave up with using this at that time and forget to debug it ahead).&lt;/p&gt;</comment>
                            <comment id="16448884" author="dongjoon" created="Mon, 23 Apr 2018 21:25:41 +0000"  >&lt;p&gt;Yep. I&apos;m also able to observe this for all Spark 2.X (2.0 ~ 2.3). `toLocalIterator` is introduced at Spark 2.0.&lt;/p&gt;</comment>
                            <comment id="16784843" author="bryanc" created="Tue, 5 Mar 2019 20:00:57 +0000"  >&lt;p&gt;I could also reproduce with a nearly identical error using the following&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; time
from pyspark.sql &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; SparkSession
from pyspark.sql.functions &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; rand, udf
from pyspark.sql.types &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; *

spark = SparkSession\
        .builder\
        .appName(&lt;span class=&quot;code-quote&quot;&gt;&quot;toLocalIterator_Test&quot;&lt;/span&gt;)\
        .getOrCreate()

df = spark.range(1 &amp;lt;&amp;lt; 16).select(rand())

it = df.toLocalIterator()

print(next(it))
it = None

time.sleep(5)
spark.stop()
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I think there are a couple issues with the way this is currently working. When toLocalIterator is called in Python, the Scala side also creates a local iterator which immediately starts a loop to consume the entire iterator and write it all to Python without any synchronization with the Python iterator. Blocking the write operation only happens when the socket receive buffer is full.  Small examples work fine if the data all fits in the read buffer, but the above code fails because the writing becomes blocked, then the Python iterator stops reading and closes the connection, which the Scala side sees as an error.  I can work on a fix for this.&lt;/p&gt;</comment>
                            <comment id="16835142" author="bryanc" created="Tue, 7 May 2019 21:48:45 +0000"  >&lt;p&gt;Issue resolved by pull request 24070&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/24070&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/24070&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="13191534">SPARK-25733</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="13229685">SPARK-27548</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13232289">SPARK-27659</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 27 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3sexz:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>