<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:12:28 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-1019] pyspark RDD take() throws NPE</title>
                <link>https://issues.apache.org/jira/browse/SPARK-1019</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;I&apos;m getting sporadic NPEs from pyspark that I can&apos;t narrow down, but I&apos;m able to reproduce it consistently from the attached data file.  If I delete any single line from the file, it works; but the file as is or larger (it&apos;s a snippet of a much larger log file) causes the problem.&lt;/p&gt;

&lt;p&gt;Printing the lines read works fine...but afterwards, I get the exception.&lt;/p&gt;

&lt;p&gt;Problem occurs with vanilla pyspark and IPython, but NOT with the scala spark shell.&lt;/p&gt;

&lt;p&gt;sc.textFile(&quot;testlog13&quot;).take(5) (does not seem to matter how many lines I take).&lt;/p&gt;

&lt;p&gt;In &lt;span class=&quot;error&quot;&gt;&amp;#91;32&amp;#93;&lt;/span&gt;: sc.textFile(&quot;testlog16&quot;).take(5)&lt;br/&gt;
14/01/09 14:16:45 INFO MemoryStore: ensureFreeSpace(69808) called with curMem=1185875, maxMem=342526525&lt;br/&gt;
14/01/09 14:16:45 INFO MemoryStore: Block broadcast_16 stored as values to memory (estimated size 68.2 KB, free 325.5 MB)&lt;br/&gt;
14/01/09 14:16:45 INFO FileInputFormat: Total input paths to process : 1&lt;br/&gt;
14/01/09 14:16:45 INFO SparkContext: Starting job: runJob at PythonRDD.scala:288&lt;br/&gt;
14/01/09 14:16:45 INFO DAGScheduler: Got job 23 (runJob at PythonRDD.scala:288) with 1 output partitions (allowLocal=true)&lt;br/&gt;
14/01/09 14:16:45 INFO DAGScheduler: Final stage: Stage 23 (runJob at PythonRDD.scala:288)&lt;br/&gt;
14/01/09 14:16:45 INFO DAGScheduler: Parents of final stage: List()&lt;br/&gt;
14/01/09 14:16:45 INFO DAGScheduler: Missing parents: List()&lt;br/&gt;
14/01/09 14:16:45 INFO DAGScheduler: Computing the requested partition locally&lt;br/&gt;
14/01/09 14:16:45 INFO HadoopRDD: Input split: &lt;a href=&quot;file:/home/training/testlog16:0+61124&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;file:/home/training/testlog16:0+61124&lt;/a&gt;&lt;br/&gt;
14/01/09 14:16:45 INFO PythonRDD: Times: total = 14, boot = 1, init = 3, finish = 10&lt;br/&gt;
14/01/09 14:16:45 INFO SparkContext: Job finished: runJob at PythonRDD.scala:288, took 0.021146108 s&lt;br/&gt;
Out&lt;span class=&quot;error&quot;&gt;&amp;#91;32&amp;#93;&lt;/span&gt;: &lt;br/&gt;
[u&apos;100.219.90.44 - 102 &lt;span class=&quot;error&quot;&gt;&amp;#91;15/Sep/2013:23:58:51 +0100&amp;#93;&lt;/span&gt; &quot;GET KBDOC-00087.html HTTP/1.0&quot; 200 8681 &quot;http://www.loudacre.com&quot;  &quot;Loudacre CSR Browser&quot; &apos;,&lt;br/&gt;
 u&apos;100.219.90.44 - 102 &lt;span class=&quot;error&quot;&gt;&amp;#91;15/Sep/2013:23:58:51 +0100&amp;#93;&lt;/span&gt; &quot;GET KBDOC-00087.html HTTP/1.0&quot; 200 8681 &quot;http://www.loudacre.com&quot;  &quot;Loudacre CSR Browser&quot; &apos;,&lt;br/&gt;
 u&apos;100.219.90.44 - 102 &lt;span class=&quot;error&quot;&gt;&amp;#91;15/Sep/2013:23:58:51 +0100&amp;#93;&lt;/span&gt; &quot;GET theme.css HTTP/1.0&quot; 200 8681 &quot;http://www.loudacre.com&quot;  &quot;Loudacre CSR Browser&quot; &apos;,&lt;br/&gt;
 u&apos;182.4.148.56 - 173 &lt;span class=&quot;error&quot;&gt;&amp;#91;15/Sep/2013:23:58:30 +0100&amp;#93;&lt;/span&gt; &quot;GET KBDOC-00076.html HTTP/1.0&quot; 200 17546 &quot;http://www.loudacre.com&quot;  &quot;Loudacre CSR Browser&quot; &apos;,&lt;br/&gt;
 u&apos;182.4.148.56 - 173 &lt;span class=&quot;error&quot;&gt;&amp;#91;15/Sep/2013:23:58:30 +0100&amp;#93;&lt;/span&gt; &quot;GET theme.css HTTP/1.0&quot; 200 17546 &quot;http://www.loudacre.com&quot;  &quot;Loudacre CSR Browser&quot; &apos;]&lt;/p&gt;

&lt;p&gt;In &lt;span class=&quot;error&quot;&gt;&amp;#91;33&amp;#93;&lt;/span&gt;: Exception in thread &quot;stdin writer for python&quot; java.lang.NullPointerException&lt;br/&gt;
	at org.apache.hadoop.fs.BufferedFSInputStream.getPos(BufferedFSInputStream.java:54)&lt;br/&gt;
	at org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:60)&lt;br/&gt;
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:246)&lt;br/&gt;
	at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:275)&lt;br/&gt;
	at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:227)&lt;br/&gt;
	at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:195)&lt;br/&gt;
	at java.io.DataInputStream.read(DataInputStream.java:100)&lt;br/&gt;
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:211)&lt;br/&gt;
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)&lt;br/&gt;
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:206)&lt;br/&gt;
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:45)&lt;br/&gt;
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:167)&lt;br/&gt;
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:150)&lt;br/&gt;
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)&lt;br/&gt;
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:27)&lt;br/&gt;
	at scala.collection.Iterator$$anon$19.hasNext(Iterator.scala:400)&lt;br/&gt;
	at scala.collection.Iterator$class.foreach(Iterator.scala:772)&lt;br/&gt;
	at scala.collection.Iterator$$anon$19.foreach(Iterator.scala:399)&lt;br/&gt;
	at org.apache.spark.api.python.PythonRDD$$anon$2.run(PythonRDD.scala:98)&lt;/p&gt;

</description>
                <environment></environment>
        <key id="12704550">SPARK-1019</key>
            <summary>pyspark RDD take() throws NPE</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="joshrosen">Josh Rosen</assignee>
                                    <reporter username="dcarroll@cloudera.com">Diana Carroll</reporter>
                        <labels>
                    </labels>
                <created>Thu, 9 Jan 2014 14:20:36 +0000</created>
                <updated>Fri, 12 Sep 2014 20:26:06 +0000</updated>
                            <resolved>Wed, 12 Mar 2014 23:18:35 +0000</resolved>
                                    <version>0.8.1</version>
                    <version>0.9.0</version>
                                    <fixVersion>0.9.1</fixVersion>
                    <fixVersion>1.0.0</fixVersion>
                                    <component>PySpark</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="13953654" author="dcarroll@cloudera.com" created="Thu, 30 Jan 2014 06:25:41 +0000"  >&lt;p&gt;I&apos;m getting this reliably again using the .9 rc build, using a different dataset. Will attach for ease of reproduction. (zip codes with latitude and longitude)&lt;/p&gt;</comment>
                            <comment id="13953657" author="patrick" created="Thu, 30 Jan 2014 11:09:56 +0000"  >&lt;p&gt;Hey Diana,&lt;/p&gt;

&lt;p&gt;I just tried to reproduce this from a fresh spark build and couldn&apos;t. If you do a clean checkout of spark like I did below do you get the error?&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;cd /tmp
git clone https:&lt;span class=&quot;code-comment&quot;&gt;//git-wip-us.apache.org/repos/asf/incubator-spark.git -b branch-0.9
&lt;/span&gt;cd incubator-spark
sbt/sbt assembly
wget https:&lt;span class=&quot;code-comment&quot;&gt;//spark-project.atlassian.net/secure/attachment/12402/testlog13
&lt;/span&gt;./bin/pyspark
&amp;gt;&amp;gt;&amp;gt; sc.textFile(&lt;span class=&quot;code-quote&quot;&gt;&quot;testlog13&quot;&lt;/span&gt;).take(5)
[u&lt;span class=&quot;code-quote&quot;&gt;&apos;165.32.101.206 - 8 [15/Sep/2013:23:59:50 +0100] &lt;span class=&quot;code-quote&quot;&gt;&quot;GET theme.css HTTP/1.0&quot;&lt;/span&gt; 200 17446 &lt;span class=&quot;code-quote&quot;&gt;&quot;http:&lt;span class=&quot;code-comment&quot;&gt;//www.loudacre.com&quot;&lt;/span&gt;  &lt;span class=&quot;code-quote&quot;&gt;&quot;Loudacre CSR Browser&quot;&lt;/span&gt; &apos;&lt;/span&gt;, u&lt;span class=&quot;code-quote&quot;&gt;&apos;100.219.90.44 - 102 [15/Sep/2013:23:58:51 +0100] &lt;span class=&quot;code-quote&quot;&gt;&quot;GET KBDOC-00087.html HTTP/1.0&quot;&lt;/span&gt; 200 8681 &lt;span class=&quot;code-quote&quot;&gt;&quot;http://www.loudacre.com&quot;&lt;/span&gt;  &lt;span class=&quot;code-quote&quot;&gt;&quot;Loudacre CSR Browser&quot;&lt;/span&gt; &apos;&lt;/span&gt;, u&lt;span class=&quot;code-quote&quot;&gt;&apos;100.219.90.44 - 102 [15/Sep/2013:23:58:51 +0100] &lt;span class=&quot;code-quote&quot;&gt;&quot;GET theme.css HTTP/1.0&quot;&lt;/span&gt; 200 8681 &lt;span class=&quot;code-quote&quot;&gt;&quot;http://www.loudacre.com&quot;&lt;/span&gt;  &lt;span class=&quot;code-quote&quot;&gt;&quot;Loudacre CSR Browser&quot;&lt;/span&gt; &apos;&lt;/span&gt;, u&lt;span class=&quot;code-quote&quot;&gt;&apos;182.4.148.56 - 173 [15/Sep/2013:23:58:30 +0100] &lt;span class=&quot;code-quote&quot;&gt;&quot;GET KBDOC-00076.html HTTP/1.0&quot;&lt;/span&gt; 200 17546 &lt;span class=&quot;code-quote&quot;&gt;&quot;http://www.loudacre.com&quot;&lt;/span&gt;  &lt;span class=&quot;code-quote&quot;&gt;&quot;Loudacre CSR Browser&quot;&lt;/span&gt; &apos;&lt;/span&gt;, u&lt;span class=&quot;code-quote&quot;&gt;&apos;182.4.148.56 - 173 [15/Sep/2013:23:58:30 +0100] &lt;span class=&quot;code-quote&quot;&gt;&quot;GET theme.css HTTP/1.0&quot;&lt;/span&gt; 200 17546 &lt;span class=&quot;code-quote&quot;&gt;&quot;http://www.loudacre.com&quot;&lt;/span&gt;  &lt;span class=&quot;code-quote&quot;&gt;&quot;Loudacre CSR Browser&quot;&lt;/span&gt; &apos;&lt;/span&gt;]&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
</comment>
                            <comment id="13953658" author="dcarroll@cloudera.com" created="Thu, 30 Jan 2014 16:20:40 +0000"  >&lt;p&gt;I&apos;m not getting the problem from that sample file &amp;#8211; it was sporadic when I was testing.  But I&apos;m now getting it consistently from the latlon.tsv file I just uploaded (a tab-delimited list of zip codes with latitude and longitude).  &lt;/p&gt;

&lt;p&gt;I just did a fresh build from your instructions and am still getting the problem  In case it is helpful I will attach a log file with debugging enabled.  (Doesn&apos;t seem to give much more info I&apos;m afraid.)&lt;/p&gt;

&lt;p&gt;I notice that the actual exception is occurring in Hadoop LineRecordReader.  Maybe my issue has something to do with the version of Hadoop I&apos;m using?  I&apos;ve tested this in CDH5b1 and the (yet to be released) CDH5b2.&lt;/p&gt;

</comment>
                            <comment id="13953659" author="dcarroll@cloudera.com" created="Thu, 30 Jan 2014 16:21:42 +0000"  >&lt;p&gt;debug log&lt;/p&gt;</comment>
                            <comment id="13953660" author="dcarroll@cloudera.com" created="Thu, 30 Jan 2014 17:04:15 +0000"  >&lt;p&gt;I just thought to test the latlon.tsv file using HDFS instead of the local file system.  That works fine.  It is only loading it from the local file system that is giving me problems.&lt;/p&gt;

&lt;p&gt;I also note that the take() is successful...it displays the requested records and &lt;b&gt;then&lt;/b&gt; throws NPE for whatever that&apos;s worth.&lt;/p&gt;</comment>
                            <comment id="13953667" author="joshrosen" created="Thu, 30 Jan 2014 18:56:05 +0000"  >&lt;p&gt;Thanks for reporting this and for including sample data.  I was able to reproduce this bug using the latest master branch, Hadoop 1.0.4, and a local pyspark shell:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&amp;gt;&amp;gt;&amp;gt; sc.textFile(&lt;span class=&quot;code-quote&quot;&gt;&quot;latlon.tsv&quot;&lt;/span&gt;).take(1)
14/01/30 18:35:00 INFO MemoryStore: ensureFreeSpace(34262) called with curMem=34238, maxMem=311387750
14/01/30 18:35:00 INFO MemoryStore: Block broadcast_1 stored as values to memory (estimated size 33.5 KB, free 296.9 MB)
14/01/30 18:35:00 INFO FileInputFormat: Total input paths to process : 1
14/01/30 18:35:00 INFO SparkContext: Starting job: take at &amp;lt;stdin&amp;gt;:1
14/01/30 18:35:00 INFO DAGScheduler: Got job 1 (take at &amp;lt;stdin&amp;gt;:1) with 1 output partitions (allowLocal=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)
14/01/30 18:35:00 INFO DAGScheduler: Final stage: Stage 1 (take at &amp;lt;stdin&amp;gt;:1)
14/01/30 18:35:00 INFO DAGScheduler: Parents of &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; stage: List()
14/01/30 18:35:00 INFO DAGScheduler: Missing parents: List()
14/01/30 18:35:00 INFO DAGScheduler: Computing the requested partition locally
14/01/30 18:35:00 INFO HadoopRDD: Input split: file:/Users/joshrosen/Documents/spark/spark/latlon.tsv:0+1127316
14/01/30 18:35:00 INFO PythonRDD: Times: total = 39, boot = 2, init = 37, finish = 0
14/01/30 18:35:00 INFO SparkContext: Job finished: take at &amp;lt;stdin&amp;gt;:1, took 0.043407 s
[u&lt;span class=&quot;code-quote&quot;&gt;&apos;00210\t43.005895\t-71.013202&apos;&lt;/span&gt;]
&amp;gt;&amp;gt;&amp;gt; Exception in thread &lt;span class=&quot;code-quote&quot;&gt;&quot;stdin writer &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; python&quot;&lt;/span&gt; java.lang.NullPointerException
	at org.apache.hadoop.fs.BufferedFSInputStream.getPos(BufferedFSInputStream.java:48)
	at org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:41)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:214)
	at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:237)
	at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:189)
	at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:158)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:134)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:133)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:38)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:164)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:149)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:27)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:350)
	at scala.collection.Iterator$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:232)
	at org.apache.spark.api.python.PythonRDD$$anon$2.run(PythonRDD.scala:85)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I used this stacktrace with &lt;a href=&quot;http://brainleg.com/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;BrainLeg&lt;/a&gt;, a structural stacktrace search engine, and found a few other projects that have run into similar NPEs:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HCATALOG-626&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HCATALOG-626&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://github.com/facebook/presto/pull/950&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/facebook/presto/pull/950&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The exception seems to be dependent on the size of the input file, since the job runs without any exceptions if I run on a small sample of lines from the original input.  I&apos;ll check out those links to see how those other projects may have fixed this issue.  I&apos;ll also try running under a debugger to see if I can track down where the null is being introduced.&lt;/p&gt;</comment>
                            <comment id="13953661" author="joshrosen" created="Thu, 30 Jan 2014 20:19:37 +0000"  >&lt;p&gt;Interestingly, I can only reproduce the NPE when calling an action that only reads a portion of the input file, like first() or take().&lt;/p&gt;

&lt;p&gt;If I call&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;sc.textFile(&lt;span class=&quot;code-quote&quot;&gt;&quot;latlon.tsv&quot;&lt;/span&gt;).collect()[0]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;which forces the entire file to be read, then I don&apos;t see any exception.&lt;/p&gt;

&lt;p&gt;My current theory is that there&apos;s a race condition between the Python daemon&apos;s stdin writer consuming its iterator and the job completion callbacks closing that iterator.&lt;/p&gt;

&lt;p&gt;Imagine that I run &lt;tt&gt;sc.textFile().take(n )&lt;/tt&gt; in Scala Spark.  The data flow here is entirely pull-based and we should only read as much of the file as is necessary to return the first &lt;tt&gt;n&lt;/tt&gt; records.  When the job completes, the Hadoop iterator is closed using a job completion callback.&lt;/p&gt;

&lt;p&gt;Now, consider what happens when I run this locally using PySpark.  PySpark&apos;s data flow is a mixture of push and pull with backpressure: the stdin writer thread pulls records from Hadoop or cached RDDs and pushes them into the Python worker process (where network buffer sizes provide backpressure), which pushes its output records back to Java.  If the job finishes before the Python worker has consumed all of its input, the job completion callbacks might close the Hadoop iterator while the PySpark stdin writer thread is still attempting to consume it, leading to a NullPointerException because it calls a closed data stream.&lt;/p&gt;

&lt;p&gt;This theory explains why the above &lt;tt&gt;collect()&lt;/tt&gt; call didn&apos;t throw a NPE: since the entire input needed to be consumed before the job could complete, the stdin writer thread would never attempt to read from a closed stream.  It also explains why you didn&apos;t see NPEs for small inputs: if the input is small enough, the Python worker will have enough time and network buffer space to receive the entire input before the job completes, even if it only iterates over its first few input records.&lt;/p&gt;

&lt;p&gt;I&apos;ll try tracing through the execution in more detail to verify if this is what&apos;s causing the exception.&lt;/p&gt;

&lt;p&gt;When fixing this, we&apos;ll have to be careful to avoid introducing race conditions.  If we simply added job completion callbacks to stop the PythonRDD stdin writer thread, we&apos;d have to worry about a race between that callback and the Hadoop iterator callback.  Maybe we could guarantee that completion callbacks are called in reverse order of registration.&lt;/p&gt;

&lt;p&gt;It looks like the underlying bug might have been introduced in &lt;a href=&quot;https://github.com/apache/incubator-spark/commit/b9d6783f36d527f5082bf13a4ee6fd108e97795c&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/incubator-spark/commit/b9d6783f36d527f5082bf13a4ee6fd108e97795c&lt;/a&gt;, which optimized PySpark&apos;s &lt;tt&gt;take()&lt;/tt&gt; to push the limit into &lt;tt&gt;mapPartitions()&lt;/tt&gt; calls to avoid computing the entire first partition.  That commit added a try-catch block that seems to imply that IOExceptions could occur if the Python worker didn&apos;t consume its entire input:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;} &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; {
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; e: IOException =&amp;gt;
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&lt;span class=&quot;code-comment&quot;&gt;// This can happen &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; legitimate reasons &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the Python code stops returning data before we are done
&lt;/span&gt;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&lt;span class=&quot;code-comment&quot;&gt;// passing elements through, e.g., &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; take(). Just log a message to say it happened.
&lt;/span&gt;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;logInfo(&lt;span class=&quot;code-quote&quot;&gt;&quot;stdin writer to Python finished early&quot;&lt;/span&gt;)
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;logDebug(&lt;span class=&quot;code-quote&quot;&gt;&quot;stdin writer to Python finished early&quot;&lt;/span&gt;, e)
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;}
&#160;&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;br/&gt;
&#160;This seems a little fishy, since it&apos;s implicitly detecting the Python worker process being finished rather than using an explicit signal from the worker to PythonRDD.  Also, IOException is too broad of an exception, which was the root cause of &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-1025&quot; title=&quot;pyspark hangs when parent base file is unavailable&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-1025&quot;&gt;&lt;del&gt;SPARK-1025&lt;/del&gt;&lt;/a&gt;, so we should remove this catch.&lt;/p&gt;</comment>
                            <comment id="13954013" author="patrick" created="Sun, 9 Mar 2014 21:07:26 +0000"  >&lt;p&gt;Okay I spent some time today playing with this. Josh I confirmed that the issue is what you suspected - the callback for the HadoopRDD is getting called and invalidating the input stream. I&apos;ve proposed a patch here that I found fixed the issue for me locally. I think my patch still has a potential race, but it least decreases the odds of this exception substantially so I&apos;d say it&apos;s strictly better than what&apos;s there now.&lt;/p&gt;</comment>
                            <comment id="13954014" author="patrick" created="Sun, 9 Mar 2014 21:08:34 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dcarroll%40cloudera.com&quot; class=&quot;user-hover&quot; rel=&quot;dcarroll@cloudera.com&quot;&gt;dcarroll@cloudera.com&lt;/a&gt; If you could test this patch that would be great.&lt;/p&gt;</comment>
                            <comment id="13954021" author="dcarroll@cloudera.com" created="Mon, 10 Mar 2014 08:32:28 +0000"  >&lt;p&gt;Will do.  (Just back from vacation so probably not today!)&lt;/p&gt;


&lt;p&gt;On Mon, Mar 10, 2014 at 12:09 AM, Patrick Wendell (JIRA) &amp;lt;&lt;/p&gt;
</comment>
                            <comment id="13954025" author="patrick" created="Mon, 10 Mar 2014 14:42:48 +0000"  >&lt;p&gt;Forgot to link the associated pull request:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/112&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/112&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13954031" author="dcarroll@cloudera.com" created="Tue, 11 Mar 2014 07:15:38 +0000"  >&lt;p&gt;I tested the patch and it works!  I tested it side by side with unpatched 0.9.  Unpatched = NPE in the latlon.tsv test file; patched = no NPE.&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;</comment>
                            <comment id="13954051" author="patrick" created="Wed, 12 Mar 2014 23:18:36 +0000"  >&lt;p&gt;I think the fixed proposed here is the best we can do given the current cancellation support in Spark. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=joshrosen&quot; class=&quot;user-hover&quot; rel=&quot;joshrosen&quot;&gt;joshrosen&lt;/a&gt; please feel free to re-open this if you see a nicer solution. There may very well be one.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/spark/pull/112/files&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/112/files&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13954089" author="joshrosen" created="Sun, 16 Mar 2014 11:17:18 +0000"  >&lt;p&gt;I&apos;m still skeptical of the &lt;tt&gt;catch IOException&lt;/tt&gt; code, since it&apos;s been involved in two bugs now, so we should probably revisit that at some point, but this seems like an okay fix for now for the NPE issues.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12709924">SPARK-1579</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>383003</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            11 years, 36 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1tzg7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>383271</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>