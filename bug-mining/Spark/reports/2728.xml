<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:34:47 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-11617] MEMORY LEAK: ByteBuf.release() was not called before it&apos;s garbage-collected</title>
                <link>https://issues.apache.org/jira/browse/SPARK-11617</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;The problem may be related to&lt;br/&gt;
 &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-11235&quot; title=&quot;Support streaming data using network library&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-11235&quot;&gt;&lt;del&gt;SPARK-11235&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;NETWORK&amp;#93;&lt;/span&gt; Add ability to stream data using network lib.&lt;/p&gt;

&lt;p&gt;while running on yarn-client mode, there are error messages:&lt;/p&gt;

&lt;p&gt;15/11/09 10:23:55 ERROR util.ResourceLeakDetector: LEAK: ByteBuf.release() was not called before it&apos;s garbage-collected. Enable advanced leak reporting to find out where the leak occurred. To enable advanced leak reporting, specify the JVM option &apos;-Dio.netty.leakDetectionLevel=advanced&apos; or call ResourceLeakDetector.setLevel() See &lt;a href=&quot;http://netty.io/wiki/reference-counted-objects.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://netty.io/wiki/reference-counted-objects.html&lt;/a&gt; for more information.&lt;/p&gt;

&lt;p&gt;and then it will cause &lt;br/&gt;
cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Container killed by YARN for exceeding memory limits. 9.0 GB of 9 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.&lt;/p&gt;

&lt;p&gt;and WARN scheduler.TaskSetManager: Lost task 105.0 in stage 1.0 (TID 2616, gsr489): java.lang.IndexOutOfBoundsException: index: 130828, length: 16833 (expected: range(0, 524288)).&lt;/p&gt;</description>
                <environment></environment>
        <key id="12911759">SPARK-11617</key>
            <summary>MEMORY LEAK: ByteBuf.release() was not called before it&apos;s garbage-collected</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="vanzin">Marcelo Masiero Vanzin</assignee>
                                    <reporter username="raynow">Ling Zhou</reporter>
                        <labels>
                    </labels>
                <created>Tue, 10 Nov 2015 05:17:34 +0000</created>
                <updated>Fri, 20 Nov 2015 13:34:08 +0000</updated>
                            <resolved>Tue, 17 Nov 2015 01:28:51 +0000</resolved>
                                    <version>1.6.0</version>
                                    <fixVersion>1.6.0</fixVersion>
                                    <component>Spark Core</component>
                    <component>YARN</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="14999587" author="joshrosen" created="Tue, 10 Nov 2015 23:31:28 +0000"  >&lt;p&gt;/cc &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vanzin&quot; class=&quot;user-hover&quot; rel=&quot;vanzin&quot;&gt;vanzin&lt;/a&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zsxwing&quot; class=&quot;user-hover&quot; rel=&quot;zsxwing&quot;&gt;zsxwing&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14999613" author="vanzin" created="Tue, 10 Nov 2015 23:47:31 +0000"  >&lt;p&gt;I&apos;ve seen this, just haven&apos;t been able to track it down... it shouldn&apos;t be a real leak (probably happens when shutting down an RPC connection, maybe one small buffer gets leaked) but the log message is scary, so we should clean it up.&lt;/p&gt;</comment>
                            <comment id="14999980" author="apachespark" created="Wed, 11 Nov 2015 06:03:05 +0000"  >&lt;p&gt;User &apos;vanzin&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/9619&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/9619&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15001615" author="raynow" created="Thu, 12 Nov 2015 03:03:09 +0000"  >&lt;p&gt;After testing this pull request, there is no ERROR: LEAK. But it will cause&lt;/p&gt;

&lt;p&gt;        15/11/12 10:39:57 WARN scheduler.TaskSetManager: Lost task 159.0 in stage 1.0 (TID 2670, gsr488): io.netty.util.IllegalReferenceCountException: refCnt: 0&lt;/p&gt;

&lt;p&gt;        15/11/12 10:46:37 WARN scheduler.TaskSetManager: Lost task 173.0 in stage 3.0 (TID 19373, gsr492): java.lang.IndexOutOfBoundsException: index: 41, length: 1077 (expected: range(0, 1139))&lt;/p&gt;

&lt;p&gt;        It seems problem has not been solved.&lt;/p&gt;

</comment>
                            <comment id="15001692" author="vanzin" created="Thu, 12 Nov 2015 04:48:49 +0000"  >&lt;p&gt;Do you have the complete exception? I don&apos;t remember hitting this.&lt;/p&gt;</comment>
                            <comment id="15001721" author="raynow" created="Thu, 12 Nov 2015 05:41:02 +0000"  >&lt;p&gt;Ok, it&apos;s in two programs.&lt;br/&gt;
In one programs, it&apos;s like&lt;/p&gt;

&lt;p&gt;15/11/12 13:22:51 INFO scheduler.TaskSetManager: Starting task 270.0 in stage 1.0 (TID 2781, gsr490, partition 270,PROCESS_LOCAL, 1994 bytes)&lt;br/&gt;
15/11/12 13:22:51 INFO scheduler.TaskSetManager: Starting task 271.0 in stage 1.0 (TID 2782, gsr492, partition 271,PROCESS_LOCAL, 1994 bytes)&lt;br/&gt;
15/11/12 13:22:51 WARN scheduler.TaskSetManager: Lost task 132.0 in stage 1.0 (TID 2643, gsr490): io.netty.util.IllegalReferenceCountException: refCnt: 0&lt;br/&gt;
        at io.netty.buffer.AbstractByteBuf.ensureAccessible(AbstractByteBuf.java:1178)&lt;br/&gt;
        at io.netty.buffer.AbstractByteBuf.checkIndex(AbstractByteBuf.java:1129)&lt;br/&gt;
        at io.netty.buffer.PooledUnsafeDirectByteBuf.getBytes(PooledUnsafeDirectByteBuf.java:138)&lt;br/&gt;
        at io.netty.buffer.CompositeByteBuf.getBytes(CompositeByteBuf.java:687)&lt;br/&gt;
        at io.netty.buffer.CompositeByteBuf.getBytes(CompositeByteBuf.java:42)&lt;br/&gt;
        at io.netty.buffer.SlicedByteBuf.getBytes(SlicedByteBuf.java:181)&lt;br/&gt;
        at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:677)&lt;br/&gt;
        at io.netty.buffer.ByteBufInputStream.read(ByteBufInputStream.java:120)&lt;br/&gt;
        at org.apache.spark.storage.BufferReleasingInputStream.read(ShuffleBlockFetcherIterator.scala:360)&lt;br/&gt;
        at org.xerial.snappy.SnappyInputStream.readNext(SnappyInputStream.java:361)&lt;br/&gt;
        at org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:381)&lt;br/&gt;
        at org.xerial.snappy.SnappyInputStream.rawRead(SnappyInputStream.java:182)&lt;br/&gt;
        at org.xerial.snappy.SnappyInputStream.read(SnappyInputStream.java:163)&lt;br/&gt;
        at com.esotericsoftware.kryo.io.Input.fill(Input.java:140)&lt;br/&gt;
        at com.esotericsoftware.kryo.io.Input.require(Input.java:169)&lt;br/&gt;
        at com.esotericsoftware.kryo.io.Input.readUtf8_slow(Input.java:524)&lt;br/&gt;
        at com.esotericsoftware.kryo.io.Input.readUtf8(Input.java:517)&lt;br/&gt;
        at com.esotericsoftware.kryo.io.Input.readString(Input.java:447)&lt;br/&gt;
        at com.esotericsoftware.kryo.serializers.DefaultSerializers$StringSerializer.read(DefaultSerializers.java:157)&lt;br/&gt;
        at com.esotericsoftware.kryo.serializers.DefaultSerializers$StringSerializer.read(DefaultSerializers.java:146)&lt;br/&gt;
        at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)&lt;br/&gt;
        at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:224)&lt;br/&gt;
        at org.apache.spark.serializer.DeserializationStream.readKey(Serializer.scala:169)&lt;br/&gt;
        at org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:201)&lt;br/&gt;
        at org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:198)&lt;br/&gt;
        at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)&lt;br/&gt;
        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)&lt;br/&gt;
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)&lt;br/&gt;
        at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)&lt;br/&gt;
        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)&lt;br/&gt;
        at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)&lt;br/&gt;
        at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:103)&lt;br/&gt;
        at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)&lt;br/&gt;
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)&lt;br/&gt;
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)&lt;br/&gt;
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)&lt;br/&gt;
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)&lt;br/&gt;
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)&lt;br/&gt;
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)&lt;br/&gt;
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)&lt;br/&gt;
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)&lt;br/&gt;
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)&lt;br/&gt;
        at org.apache.spark.scheduler.Task.run(Task.scala:88)&lt;br/&gt;
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:745)&lt;/p&gt;

&lt;p&gt;in another program it&apos;s like&lt;/p&gt;

&lt;p&gt;15/11/12 13:35:06 INFO scheduler.TaskSetManager: Starting task 268.2 in stage 3.0 (TID 20032, gsr489, partition 268,PROCESS_LOCAL, 1961 bytes)&lt;br/&gt;
15/11/12 13:35:06 INFO scheduler.TaskSetManager: Lost task 281.1 in stage 3.0 (TID 19895) on executor gsr489: io.netty.util.IllegalReferenceCountException (refCnt: 0) &lt;span class=&quot;error&quot;&gt;&amp;#91;duplicate 274&amp;#93;&lt;/span&gt;&lt;br/&gt;
15/11/12 13:35:06 INFO scheduler.TaskSetManager: Starting task 281.2 in stage 3.0 (TID 20033, gsr491, partition 281,PROCESS_LOCAL, 1961 bytes)&lt;br/&gt;
15/11/12 13:35:06 INFO scheduler.TaskSetManager: Lost task 113.2 in stage 3.0 (TID 19982) on executor gsr491: io.netty.util.IllegalReferenceCountException (refCnt: 0) &lt;span class=&quot;error&quot;&gt;&amp;#91;duplicate 275&amp;#93;&lt;/span&gt;&lt;br/&gt;
15/11/12 13:35:06 INFO scheduler.TaskSetManager: Starting task 113.3 in stage 3.0 (TID 20034, gsr490, partition 113,PROCESS_LOCAL, 1961 bytes)&lt;br/&gt;
15/11/12 13:35:06 INFO scheduler.TaskSetManager: Lost task 412.1 in stage 3.0 (TID 19830) on executor gsr490: io.netty.util.IllegalReferenceCountException (refCnt: 0) &lt;span class=&quot;error&quot;&gt;&amp;#91;duplicate 276&amp;#93;&lt;/span&gt;&lt;br/&gt;
15/11/12 13:35:06 INFO scheduler.TaskSetManager: Starting task 412.2 in stage 3.0 (TID 20035, gsr491, partition 412,PROCESS_LOCAL, 1961 bytes)&lt;br/&gt;
15/11/12 13:35:06 INFO scheduler.TaskSetManager: Lost task 211.0 in stage 3.0 (TID 19411) on executor gsr491: io.netty.util.IllegalReferenceCountException (refCnt: 0) &lt;span class=&quot;error&quot;&gt;&amp;#91;duplicate 277&amp;#93;&lt;/span&gt;&lt;br/&gt;
15/11/12 13:35:06 INFO scheduler.TaskSetManager: Starting task 211.1 in stage 3.0 (TID 20036, gsr487, partition 211,PROCESS_LOCAL, 1961 bytes)&lt;br/&gt;
15/11/12 13:35:06 INFO scheduler.TaskSetManager: Lost task 22.1 in stage 3.0 (TID 19745) on executor gsr487: io.netty.util.IllegalReferenceCountException (refCnt: 0) &lt;span class=&quot;error&quot;&gt;&amp;#91;duplicate 278&amp;#93;&lt;/span&gt;&lt;br/&gt;
15/11/12 13:35:07 INFO scheduler.TaskSetManager: Starting task 22.2 in stage 3.0 (TID 20037, gsr492, partition 22,PROCESS_LOCAL, 1961 bytes)&lt;br/&gt;
15/11/12 13:35:07 INFO scheduler.TaskSetManager: Lost task 343.1 in stage 3.0 (TID 19759) on executor gsr492: io.netty.util.IllegalReferenceCountException (refCnt: 0) &lt;span class=&quot;error&quot;&gt;&amp;#91;duplicate 279&amp;#93;&lt;/span&gt;&lt;br/&gt;
15/11/12 13:35:07 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on gsr493:57074 in memory (size: 1456.0 B, free: 125.8 MB)&lt;br/&gt;
15/11/12 13:35:07 INFO scheduler.TaskSetManager: Starting task 343.2 in stage 3.0 (TID 20038, gsr489, partition 343,PROCESS_LOCAL, 1961 bytes)&lt;br/&gt;
15/11/12 13:35:07 INFO scheduler.TaskSetManager: Lost task 377.2 in stage 3.0 (TID 19927) on executor gsr489: io.netty.util.IllegalReferenceCountException (refCnt: 0) &lt;span class=&quot;error&quot;&gt;&amp;#91;duplicate 280&amp;#93;&lt;/span&gt;&lt;br/&gt;
15/11/12 13:35:07 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on gsr491:41858 in memory (size: 1456.0 B, free: 51.5 MB)&lt;br/&gt;
15/11/12 13:35:07 INFO scheduler.TaskSetManager: Starting task 377.3 in stage 3.0 (TID 20039, gsr488, partition 377,PROCESS_LOCAL, 1961 bytes)&lt;br/&gt;
15/11/12 13:35:07 WARN scheduler.TaskSetManager: Lost task 237.1 in stage 3.0 (TID 19851, gsr488): java.lang.IndexOutOfBoundsException: index: 39501, length: 4 (expected: range(0, 0))&lt;br/&gt;
	at io.netty.buffer.AbstractByteBuf.checkIndex(AbstractByteBuf.java:1134)&lt;br/&gt;
	at io.netty.buffer.AbstractByteBuf.checkDstIndex(AbstractByteBuf.java:1148)&lt;br/&gt;
	at io.netty.buffer.CompositeByteBuf.getBytes(CompositeByteBuf.java:676)&lt;br/&gt;
	at io.netty.buffer.CompositeByteBuf.getBytes(CompositeByteBuf.java:42)&lt;br/&gt;
	at io.netty.buffer.SlicedByteBuf.getBytes(SlicedByteBuf.java:181)&lt;br/&gt;
	at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:677)&lt;br/&gt;
	at io.netty.buffer.ByteBufInputStream.read(ByteBufInputStream.java:120)&lt;br/&gt;
	at org.apache.spark.storage.BufferReleasingInputStream.read(ShuffleBlockFetcherIterator.scala:360)&lt;br/&gt;
	at org.xerial.snappy.SnappyInputStream.readNext(SnappyInputStream.java:361)&lt;br/&gt;
	at org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:381)&lt;br/&gt;
	at org.xerial.snappy.SnappyInputStream.rawRead(SnappyInputStream.java:182)&lt;br/&gt;
	at org.xerial.snappy.SnappyInputStream.read(SnappyInputStream.java:163)&lt;br/&gt;
	at java.io.ObjectInputStream$PeekInputStream.read(ObjectInputStream.java:2313)&lt;br/&gt;
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2326)&lt;br/&gt;
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2797)&lt;br/&gt;
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:802)&lt;br/&gt;
	at java.io.ObjectInputStream.&amp;lt;init&amp;gt;(ObjectInputStream.java:299)&lt;br/&gt;
	at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.&amp;lt;init&amp;gt;(JavaSerializer.scala:64)&lt;br/&gt;
	at org.apache.spark.serializer.JavaDeserializationStream.&amp;lt;init&amp;gt;(JavaSerializer.scala:64)&lt;br/&gt;
	at org.apache.spark.serializer.JavaSerializerInstance.deserializeStream(JavaSerializer.scala:123)&lt;br/&gt;
	at org.apache.spark.shuffle.BlockStoreShuffleReader$$anonfun$3.apply(BlockStoreShuffleReader.scala:64)&lt;br/&gt;
	at org.apache.spark.shuffle.BlockStoreShuffleReader$$anonfun$3.apply(BlockStoreShuffleReader.scala:60)&lt;br/&gt;
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)&lt;br/&gt;
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)&lt;br/&gt;
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)&lt;br/&gt;
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)&lt;br/&gt;
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:197)&lt;br/&gt;
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:103)&lt;br/&gt;
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)&lt;br/&gt;
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)&lt;br/&gt;
	at org.apache.spark.scheduler.Task.run(Task.scala:88)&lt;br/&gt;
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:745)&lt;/p&gt;</comment>
                            <comment id="15001815" author="nadenf" created="Thu, 12 Nov 2015 08:01:28 +0000"  >&lt;p&gt;Can confirm both of these issues. Could it be related to the recent Snappy library upgrade ?&lt;/p&gt;

&lt;p&gt;Also seeing lots of java.lang.OutOfMemoryError: Direct buffer memory coming from the Netty library.&lt;/p&gt;

&lt;p&gt;We are executing a number of SQL statements in parallel and the code continues to work fine in 1.4.1/1.5.0 so it&apos;s a recent regression.&lt;/p&gt;</comment>
                            <comment id="15002988" author="vanzin" created="Thu, 12 Nov 2015 21:41:52 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=raynow&quot; class=&quot;user-hover&quot; rel=&quot;raynow&quot;&gt;raynow&lt;/a&gt; I don&apos;t know your github alias, but I updated the change with a fix to hopefully address that issue; could you test it? Thanks!&lt;/p&gt;</comment>
                            <comment id="15003879" author="jlewandowski" created="Fri, 13 Nov 2015 11:35:53 +0000"  >&lt;p&gt;This also happens in standalone mode, Netty based RPC - I&apos;ve seen this in Master and Worker logs.&lt;/p&gt;</comment>
                            <comment id="15006153" author="raynow" created="Mon, 16 Nov 2015 02:15:10 +0000"  >&lt;p&gt;Hi Vanzin, I tested this fix. While there is no reference problem again, there are still task failure and executor lost. I am not sure where the problem is, the log message shows some RPC errors and exceptions. &lt;/p&gt;</comment>
                            <comment id="15006998" author="vanzin" created="Mon, 16 Nov 2015 17:53:51 +0000"  >&lt;p&gt;Can you post the exceptions if they&apos;re different than the ones you posted before?&lt;/p&gt;</comment>
                            <comment id="15007315" author="vanzin" created="Mon, 16 Nov 2015 21:05:30 +0000"  >&lt;p&gt;BTW, I updated the PR with a test case that fails with the exceptions you saw if I disable the fix; they pass consistently with the fix applied. I also ran several jobs that do a lot of shuffles and didn&apos;t see any problems with the latest fix applied.&lt;/p&gt;</comment>
                            <comment id="15007981" author="raynow" created="Tue, 17 Nov 2015 04:10:25 +0000"  >&lt;p&gt;While running sort in spark-perf there are still exceeding memory limit errors . I am running it in yarn-client mode.&lt;/p&gt;

&lt;p&gt;15/11/17 10:48:43 INFO scheduler.TaskSetManager: Finished task 339.0 in stage 3.0 (TID 19539) in 8887 ms on gsr493 (3/6400)&lt;br/&gt;
15/11/17 10:48:43 INFO scheduler.TaskSetManager: Starting task 503.0 in stage 3.0 (TID 19703, gsr493, partition 503,PROCESS_LOCAL, 1961 bytes)&lt;br/&gt;
15/11/17 10:48:43 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 3.0 (TID 19211) in 8974 ms on gsr493 (4/6400)&lt;br/&gt;
15/11/17 10:48:43 INFO scheduler.TaskSetManager: Starting task 504.0 in stage 3.0 (TID 19704, gsr493, partition 504,PROCESS_LOCAL, 1961 bytes)&lt;br/&gt;
15/11/17 10:48:43 INFO scheduler.TaskSetManager: Finished task 211.0 in stage 3.0 (TID 19411) in 8967 ms on gsr493 (5/6400)&lt;br/&gt;
15/11/17 10:48:43 INFO cluster.YarnClientSchedulerBackend: Disabling executor 92.&lt;br/&gt;
15/11/17 10:48:43 ERROR cluster.YarnScheduler: Lost executor 92 on gsr489: Pending loss reason.&lt;br/&gt;
15/11/17 10:48:43 INFO scheduler.DAGScheduler: Executor lost: 92 (epoch 1)&lt;br/&gt;
15/11/17 10:48:43 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 92 from BlockManagerMaster.&lt;br/&gt;
15/11/17 10:48:43 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(92, gsr489, 45506)&lt;br/&gt;
15/11/17 10:48:43 INFO storage.BlockManagerMaster: Removed 92 successfully in removeExecutor&lt;br/&gt;
15/11/17 10:48:43 INFO scheduler.ShuffleMapStage: ShuffleMapStage 2 is now unavailable on executor 92 (6336/6400, false)&lt;br/&gt;
15/11/17 10:48:43 ERROR cluster.YarnScheduler: Actual reason for lost executor 92: Container killed by YARN for exceeding memory limits. 9.1 GB of 9 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.&lt;br/&gt;
15/11/17 10:48:43 WARN scheduler.TaskSetManager: Lost task 68.0 in stage 3.0 (TID 19268, gsr489): ExecutorLostFailure (executor 92 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 9.1 GB of 9 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.&lt;br/&gt;
15/11/17 10:48:43 WARN scheduler.TaskSetManager: Lost task 268.0 in stage 3.0 (TID 19468, gsr489): ExecutorLostFailure (executor 92 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 9.1 GB of 9 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.&lt;br/&gt;
15/11/17 10:48:43 WARN scheduler.TaskSetManager: Lost task 468.0 in stage 3.0 (TID 19668, gsr489): ExecutorLostFailure (executor 92 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 9.1 GB of 9 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.&lt;br/&gt;
15/11/17 10:48:43 WARN scheduler.TaskSetManager: Lost task 168.0 in stage 3.0 (TID 19368, gsr489): ExecutorLostFailure (executor 92 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 9.1 GB of 9 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.&lt;br/&gt;
15/11/17 10:48:43 WARN scheduler.TaskSetManager: Lost task 368.0 in stage 3.0 (TID 19568, gsr489): ExecutorLostFailure (executor 92 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 9.1 GB of 9 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.&lt;br/&gt;
15/11/17 10:48:43 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Container killed by YARN for exceeding memory limits. 9.1 GB of 9 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.&lt;br/&gt;
15/11/17 10:48:43 INFO cluster.YarnClientSchedulerBackend: Asked to remove non-existent executor 92&lt;br/&gt;
15/11/17 10:48:44 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on gsr444:54110 in memory (size: 1462.0 B, free: 125.8 MB)&lt;br/&gt;
15/11/17 10:48:44 INFO scheduler.TaskSetManager: Starting task 368.1 in stage 3.0 (TID 19705, gsr493, partition 368,PROCESS_LOCAL, 1961 bytes)&lt;br/&gt;
15/11/17 10:48:44 INFO scheduler.TaskSetManager: Finished task 439.0 in stage 3.0 (TID 19639) in 9696 ms on gsr493 (6/6400)&lt;br/&gt;
15/11/17 10:48:44 INFO cluster.YarnClientSchedulerBackend: Disabling executor 84.&lt;br/&gt;
15/11/17 10:48:44 ERROR cluster.YarnScheduler: Lost executor 84 on gsr491: Pending loss reason.&lt;br/&gt;
15/11/17 10:48:44 INFO scheduler.DAGScheduler: Executor lost: 84 (epoch 2)&lt;br/&gt;
15/11/17 10:48:44 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 84 from BlockManagerMaster.&lt;br/&gt;
15/11/17 10:48:44 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(84, gsr491, 38469)&lt;br/&gt;
15/11/17 10:48:44 INFO storage.BlockManagerMaster: Removed 84 successfully in removeExecutor&lt;br/&gt;
15/11/17 10:48:44 INFO scheduler.ShuffleMapStage: ShuffleMapStage 2 is now unavailable on executor 84 (6271/6400, false)&lt;br/&gt;
15/11/17 10:48:44 ERROR cluster.YarnScheduler: Actual reason for lost executor 84: Container killed by YARN for exceeding memory limits. 9.0 GB of 9 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.&lt;br/&gt;
15/11/17 10:48:44 WARN scheduler.TaskSetManager: Lost task 155.0 in stage 3.0 (TID 19355, gsr491): ExecutorLostFailure (executor 84 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 9.0 GB of 9 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.&lt;br/&gt;
15/11/17 10:48:44 WARN scheduler.TaskSetManager: Lost task 355.0 in stage 3.0 (TID 19555, gsr491): ExecutorLostFailure (executor 84 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 9.0 GB of 9 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.&lt;br/&gt;
15/11/17 10:48:44 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Container killed by YARN for exceeding memory limits. 9.0 GB of 9 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.&lt;/p&gt;</comment>
                            <comment id="15009055" author="vanzin" created="Tue, 17 Nov 2015 17:17:32 +0000"  >&lt;p&gt;Those errors are not necessarily caused by the changed code here; you need to increase &lt;tt&gt;spark.yarn.executor.memoryOverhead&lt;/tt&gt; as the message says. If even after increasing the memory still keeps growing, then there might be a problem. Somewhere.&lt;/p&gt;</comment>
                            <comment id="15010355" author="raynow" created="Wed, 18 Nov 2015 06:38:20 +0000"  >&lt;p&gt;You are right.  After increasing spark.yarn.executor.memoryOverhead, the ERROR disappeared. Thank you.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="12912095">SPARK-11648</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12912095">SPARK-11648</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2o6ef:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>