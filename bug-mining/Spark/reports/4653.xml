<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:52:01 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-13450] SortMergeJoin will OOM when join rows have lot of same keys</title>
                <link>https://issues.apache.org/jira/browse/SPARK-13450</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;  When I run a sql with join, task throw  java.lang.OutOfMemoryError and sql failed. I have set spark.executor.memory  4096m.&lt;br/&gt;
  SortMergeJoin use a ArrayBuffer&lt;span class=&quot;error&quot;&gt;&amp;#91;InternalRow&amp;#93;&lt;/span&gt; to store bufferedMatches, if the join rows have a lot of same key, it will throw OutOfMemoryError.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;  &lt;span class=&quot;code-comment&quot;&gt;/** Buffered rows from the buffered side of the join. This is empty &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; there are no matches. */&lt;/span&gt;
  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt;[&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;] val bufferedMatches: ArrayBuffer[InternalRow] = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ArrayBuffer[InternalRow]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;  Here is the stackTrace:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;org.xerial.snappy.SnappyNative.arrayCopy(Native Method)
org.xerial.snappy.Snappy.arrayCopy(Snappy.java:84)
org.xerial.snappy.SnappyInputStream.rawRead(SnappyInputStream.java:190)
org.xerial.snappy.SnappyInputStream.read(SnappyInputStream.java:163)
java.io.DataInputStream.readFully(DataInputStream.java:195)
java.io.DataInputStream.readLong(DataInputStream.java:416)
org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.loadNext(UnsafeSorterSpillReader.java:71)
org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillMerger$2.loadNext(UnsafeSorterSpillMerger.java:79)
org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.next(UnsafeExternalRowSorter.java:136)
org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.next(UnsafeExternalRowSorter.java:123)
org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:84)
org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedBufferedToRowWithNullFreeJoinKey(SortMergeJoin.scala:300)
org.apache.spark.sql.execution.joins.SortMergeJoinScanner.bufferMatchingRows(SortMergeJoin.scala:329)
org.apache.spark.sql.execution.joins.SortMergeJoinScanner.findNextInnerJoinRows(SortMergeJoin.scala:229)
org.apache.spark.sql.execution.joins.SortMergeJoin$$anonfun$doExecute$1$$anon$1.advanceNext(SortMergeJoin.scala:105)
org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:68)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:88)
org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:86)
org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:741)
org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:741)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:337)
org.apache.spark.rdd.RDD.iterator(RDD.scala:301)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:337)
org.apache.spark.rdd.RDD.iterator(RDD.scala:301)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
org.apache.spark.scheduler.Task.run(Task.scala:89)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:215)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:744)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="12941327">SPARK-13450</key>
            <summary>SortMergeJoin will OOM when join rows have lot of same keys</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="tejasp">Tejas Patil</assignee>
                                    <reporter username="shenhong">shenh062326</reporter>
                        <labels>
                    </labels>
                <created>Tue, 23 Feb 2016 08:44:00 +0000</created>
                <updated>Wed, 15 Mar 2017 19:27:32 +0000</updated>
                            <resolved>Wed, 15 Mar 2017 19:27:32 +0000</resolved>
                                    <version>1.6.0</version>
                    <version>2.0.2</version>
                    <version>2.1.0</version>
                                    <fixVersion>2.2.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>9</watches>
                                                                                                                <comments>
                            <comment id="15158566" author="shenhong" created="Tue, 23 Feb 2016 08:50:28 +0000"  >&lt;p&gt;I think we should add a ExternalAppendOnlyArrayBuffer to replace ArrayBuffer.&lt;br/&gt;
I will add in my own branch first.&lt;/p&gt;</comment>
                            <comment id="15158626" author="srowen" created="Tue, 23 Feb 2016 09:41:30 +0000"  >&lt;p&gt;This isn&apos;t a helpful JIRA since you didn&apos;t say how you reproduce this at all. &lt;/p&gt;</comment>
                            <comment id="15158824" author="hvanhovell" created="Tue, 23 Feb 2016 12:54:27 +0000"  >&lt;p&gt;+1 on reproduceable bugs.&lt;/p&gt;

&lt;p&gt;So you are basically doing a Cartesian Product. Sort-Merge join will cache the right side of the join, it is likely OOM when there are a lot of rows with the same key. You can try to mitigate this problem by actually using a Cartesian Join (will perform horrible), do a broadcast join (if one of the sides fits in memory), or do some sort of a co-group.&lt;/p&gt;

&lt;p&gt;If you want to go ahead and want to improve this, I&apos;d suggest you take a look at spark/sql/core/src/main/scala/org/apache/spark/sql/execution/Window.scala in which we also spill to disk when the number of rows in a partition becomes to large. &lt;/p&gt;</comment>
                            <comment id="15160271" author="shenhong" created="Wed, 24 Feb 2016 06:41:05 +0000"  >&lt;p&gt;A join has a lot of rows with the same key.&lt;/p&gt;</comment>
                            <comment id="15162858" author="shenhong" created="Wed, 24 Feb 2016 11:36:23 +0000"  >&lt;p&gt;Thanks, I will add in my own branch first.&lt;/p&gt;</comment>
                            <comment id="15168595" author="apachespark" created="Fri, 26 Feb 2016 07:39:03 +0000"  >&lt;p&gt;User &apos;shenh062326&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/11386&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/11386&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15812508" author="tejasp" created="Mon, 9 Jan 2017 18:46:19 +0000"  >&lt;p&gt;I have seen this problem a couple times in prod while trying out jobs over Spark. There have been some discussions in the jira and here are my comments on those:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;How to reproduce ? As &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=shenhong&quot; class=&quot;user-hover&quot; rel=&quot;shenhong&quot;&gt;shenhong&lt;/a&gt; said, even in my case there were keys in the joined relation which were skewed. I was able to grab a heap dump which shows the array buffer grown more than a GB : &lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12846382/heap-dump-analysis.png&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12846382/heap-dump-analysis.png&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hvanhovell&quot; class=&quot;user-hover&quot; rel=&quot;hvanhovell&quot;&gt;hvanhovell&lt;/a&gt; had some suggestions about using Cartesian Join OR co-group. In my case, our users are running Hive SQL queries as-is over Spark. If there are one-off such cases, we could have done that but with automated migration of several jobs, we want the query to just work. OOMs lead to un-reliable behavior and affects users.&lt;/li&gt;
	&lt;li&gt;I looked at `Window.scala` but it only works for unsafe rows. In sort merge join, we may or may not have unsafe rows.&lt;/li&gt;
	&lt;li&gt;There was a PR associated with this jira (&lt;a href=&quot;https://github.com/apache/spark/pull/11386&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/11386&lt;/a&gt;) but its inactive. I tried to pick it up but it does not apply. Its basically copying ExternalAppendOnlyMap code and introducing a `Buffer` version of it for lists. Instead of taking care of merge conflicts, I was able to implement buffer version by reusing `ExternalAppendOnlyMap` code. Will submit a fresh PR after testing it.&lt;/li&gt;
&lt;/ul&gt;


&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;java.lang.OutOfMemoryError: Java heap space
at org.apache.spark.sql.catalyst.expressions.UnsafeRow.copy(UnsafeRow.java:503)
at org.apache.spark.sql.catalyst.expressions.UnsafeRow.copy(UnsafeRow.java:61)
at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.bufferMatchingRows(SortMergeJoinExec.scala:756)
at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.findNextInnerJoinRows(SortMergeJoinExec.scala:660)
at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1$$anon$1.advanceNext(SortMergeJoinExec.scala:137)
at org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:68)
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:186)
at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.&amp;lt;init&amp;gt;(TungstenAggregationIterator.scala:355)
at org.apache.spark.sql.execution.aggregate.HashAggregateExec$$anonfun$doExecute$1$$anonfun$4.apply(HashAggregateExec.scala:103)
at org.apache.spark.sql.execution.aggregate.HashAggregateExec$$anonfun$doExecute$1$$anonfun$4.apply(HashAggregateExec.scala:94)
at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785)
at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
at org.apache.spark.scheduler.Task.run(Task.scala:86)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15812550" author="zhzhan" created="Mon, 9 Jan 2017 19:02:31 +0000"  >&lt;p&gt;ExternalAppendOnlyMap estimate the size of the data saved. In SortMergeJoin, I think we can leverage UnsafeExternalSorter to get more accurate and controllable behavior.&lt;/p&gt;</comment>
                            <comment id="15863275" author="apachespark" created="Mon, 13 Feb 2017 07:14:03 +0000"  >&lt;p&gt;User &apos;tejasapatil&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/16909&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/16909&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12846382" name="heap-dump-analysis.png" size="77356" author="tejasp" created="Mon, 9 Jan 2017 18:47:36 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 40 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2t7cn:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>