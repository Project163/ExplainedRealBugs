<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:51:42 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-19737] New analysis rule for reporting unregistered functions without relying on relation resolution</title>
                <link>https://issues.apache.org/jira/browse/SPARK-19737</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Let&apos;s consider the following simple SQL query that reference an undefined function &lt;tt&gt;foo&lt;/tt&gt; that is never registered in the function registry:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-sql&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;SELECT&lt;/span&gt; foo(&lt;span class=&quot;code-keyword&quot;&gt;a&lt;/span&gt;) &lt;span class=&quot;code-keyword&quot;&gt;FROM&lt;/span&gt; t
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Assuming table &lt;tt&gt;t&lt;/tt&gt; is a partitioned  temporary view consisting of a large number of files stored on S3, it may take the analyzer a long time before realizing that &lt;tt&gt;foo&lt;/tt&gt; is not registered yet.&lt;/p&gt;

&lt;p&gt;The reason is that the existing analysis rule &lt;tt&gt;ResolveFunctions&lt;/tt&gt; requires all child expressions to be resolved first. Therefore, &lt;tt&gt;ResolveRelations&lt;/tt&gt; has to be executed first to resolve all columns referenced by the unresolved function invocation. This further leads to partition discovery for &lt;tt&gt;t&lt;/tt&gt;, which may take a long time.&lt;/p&gt;

&lt;p&gt;To address this case, we propose a new lightweight analysis rule &lt;tt&gt;LookupFunctions&lt;/tt&gt; that&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Matches all unresolved function invocations&lt;/li&gt;
	&lt;li&gt;Look up the function names from the function registry&lt;/li&gt;
	&lt;li&gt;Report analysis error for any unregistered functions&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Since this rule doesn&apos;t actually try to resolve the unresolved functions, it doesn&apos;t rely on &lt;tt&gt;ResolveRelations&lt;/tt&gt; and therefore doesn&apos;t trigger partition discovery.&lt;/p&gt;

&lt;p&gt;We may put this analysis rule in a separate &lt;tt&gt;Once&lt;/tt&gt; rule batch that sits between the &quot;Substitution&quot; batch and the &quot;Resolution&quot; batch to avoid running it repeatedly and make sure it gets executed before &lt;tt&gt;ResolveRelations&lt;/tt&gt;.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13046254">SPARK-19737</key>
            <summary>New analysis rule for reporting unregistered functions without relying on relation resolution</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="lian cheng">Cheng Lian</assignee>
                                    <reporter username="lian cheng">Cheng Lian</reporter>
                        <labels>
                    </labels>
                <created>Sat, 25 Feb 2017 06:06:28 +0000</created>
                <updated>Sun, 11 Mar 2018 15:54:16 +0000</updated>
                            <resolved>Mon, 6 Mar 2017 18:37:23 +0000</resolved>
                                    <version>2.2.0</version>
                                    <fixVersion>2.2.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="15896127" author="apachespark" created="Sun, 5 Mar 2017 09:03:03 +0000"  >&lt;p&gt;User &apos;liancheng&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/17168&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/17168&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15897794" author="lian cheng" created="Mon, 6 Mar 2017 18:37:23 +0000"  >&lt;p&gt;Issue resolved by pull request 17168&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/17168&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/17168&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16371377" author="landais christophe" created="Wed, 21 Feb 2018 13:02:44 +0000"  >&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;Migrating our application from spark 2.1.1 to spark 2.2.1, we see a major degradation in spark-SQL timing. One insert takes 5 seconds in 2.1.1 and 75 seconds in spark 2.2.1. Looking in executor traces (I force configuration to one executor) , we see it takes time between spark.sql(&#8220;insert into&#8221;) is done and task is submitted to executor&lt;/p&gt;

&lt;p&gt;My application traces&#160;:&lt;/p&gt;

&lt;p&gt;2018-02-21 06:30:53 - Executor&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; Going to execute request &#8230;&lt;/p&gt;

&lt;p&gt;2018-02-21 06:32:08 - Executor&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; request executed (tag: NO_TAG) (table: ca4mn.sys_4g_pcmd_mme_15min) (date: 20180221061500) - duration (s)&#160; 74.846&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Executor trace&#160;:&lt;/p&gt;

&lt;p&gt;18/02/21 06:30:52 INFO Executor: Finished task 0.0 in stage 3.0 (TID 1). 4675 bytes result sent to driver&#160; (landais note: this is the previous task that is terminated)&lt;/p&gt;

&lt;p&gt;18/02/21 06:32:06 INFO CoarseGrainedExecutorBackend: Got assigned task 2&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;What is doing spark between 06:30:53 and 06:32:06 ? I have taken several thread dump in the container while execution was in progress, with a delay of 2 seconds between thread dump. They are identical. Thread dump is put at the end of this comment.&lt;/p&gt;

&lt;p&gt;Thread dump shows time is taken while verifying function exists: it is &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-19737&quot; title=&quot;New analysis rule for reporting unregistered functions without relying on relation resolution&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-19737&quot;&gt;&lt;del&gt;SPARK-19737&lt;/del&gt;&lt;/a&gt; modification.&lt;/p&gt;

&lt;p&gt;My SQL request contains 1000 functions because we are doing aggregation on many columns. Functions are like MAX, MIN, etc &#8230;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Please, can you perform a modification that improves this check ? For example: doing only one check for each different function ? Or why not introducing a spark parameter to bypass this check ?&lt;/p&gt;

&lt;p&gt;----------------&lt;/p&gt;

&lt;p&gt;Thread dump&lt;/p&gt;

&lt;p&gt;178 &quot;Executor&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;&quot; #95 prio=5 os_prio=0 tid=0x00007f587f355800 nid=0x7c runnable &lt;span class=&quot;error&quot;&gt;&amp;#91;0x00007f57549f7000&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;179&#160;&#160;&#160; java.lang.Thread.State: RUNNABLE&lt;/p&gt;

&lt;p&gt;180&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at java.net.SocketInputStream.socketRead0(Native Method)&lt;/p&gt;

&lt;p&gt;181&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)&lt;/p&gt;

&lt;p&gt;182&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at java.net.SocketInputStream.read(SocketInputStream.java:171)&lt;/p&gt;

&lt;p&gt;183&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at java.net.SocketInputStream.read(SocketInputStream.java:141)&lt;/p&gt;

&lt;p&gt;184&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)&lt;/p&gt;

&lt;p&gt;185&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)&lt;/p&gt;

&lt;p&gt;186&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at java.io.BufferedInputStream.read(BufferedInputStream.java:345)&lt;/p&gt;

&lt;p&gt;187&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; - locked &amp;lt;0x000000008913b110&amp;gt; (a java.io.BufferedInputStream)&lt;/p&gt;

&lt;p&gt;188&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:127)&lt;/p&gt;

&lt;p&gt;189&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)&lt;/p&gt;

&lt;p&gt;190&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)&lt;/p&gt;

&lt;p&gt;191&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)&lt;/p&gt;

&lt;p&gt;192&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)&lt;/p&gt;

&lt;p&gt;193&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:77)&lt;/p&gt;

&lt;p&gt;194&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_database(ThriftHiveMetastore.java:654)&lt;/p&gt;

&lt;p&gt;195&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_database(ThriftHiveMetastore.java:641)&lt;/p&gt;

&lt;p&gt;196&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabase(HiveMetaStoreClient.java:1158)&lt;/p&gt;

&lt;p&gt;197&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at sun.reflect.GeneratedMethodAccessor43.invoke(Unknown Source)&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; **&lt;/p&gt;

&lt;p&gt;198&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#160;&#160;&#160;&#160;&#160;&lt;/p&gt;

&lt;p&gt;199&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at java.lang.reflect.Method.invoke(Method.java:498)&lt;/p&gt;

&lt;p&gt;200&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)&lt;/p&gt;

&lt;p&gt;201&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at com.sun.proxy.$Proxy31.getDatabase(Unknown Source)&lt;/p&gt;

&lt;p&gt;202&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1301)&lt;/p&gt;

&lt;p&gt;203&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1290)&#160;&#160;&#160;&#160;&#160; **&lt;/p&gt;

&lt;p&gt;204&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$databaseExists$1.apply$mcZ$sp(HiveClientImpl.scala:358)&lt;/p&gt;

&lt;p&gt;205&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$databaseExists$1.apply(HiveClientImpl.scala:358)&lt;/p&gt;

&lt;p&gt;206&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$databaseExists$1.apply(HiveClientImpl.scala:358)&lt;/p&gt;

&lt;p&gt;207&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:290)&lt;/p&gt;

&lt;p&gt;208&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:231)&lt;/p&gt;

&lt;p&gt;209&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:230)&lt;/p&gt;

&lt;p&gt;210&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; - locked &amp;lt;0x000000008900dd88&amp;gt; (a org.apache.spark.sql.hive.client.IsolatedClientLoader)&lt;/p&gt;

&lt;p&gt;211&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:273)&lt;/p&gt;

&lt;p&gt;212&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:357)&lt;/p&gt;

&lt;p&gt;213&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)&lt;/p&gt;

&lt;p&gt;214&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)&lt;/p&gt;

&lt;p&gt;215&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)&lt;/p&gt;

&lt;p&gt;216&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)&lt;/p&gt;

&lt;p&gt;217&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; - locked &amp;lt;0x0000000089037ae0&amp;gt; (a org.apache.spark.sql.hive.HiveExternalCatalog)&lt;/p&gt;

&lt;p&gt;218&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)&lt;/p&gt;

&lt;p&gt;219&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.catalog.SessionCatalog.databaseExists(SessionCatalog.scala:246)&lt;/p&gt;

&lt;p&gt;220&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.catalog.SessionCatalog.org$apache$spark$sql$catalyst$catalog$SessionCatalog$$requireDbExists(SessionCatalog.scala:172)&lt;/p&gt;

&lt;p&gt;221&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.catalog.SessionCatalog.functionExists(SessionCatalog.scala:1044)&lt;/p&gt;

&lt;p&gt;222&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.hive.HiveSessionCatalog.functionExists(HiveSessionCatalog.scala:173)&lt;/p&gt;

&lt;p&gt;223&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$15.applyOrElse(Analyzer.scala:1196)&lt;/p&gt;

&lt;p&gt;224&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$15.applyOrElse(Analyzer.scala:1195)&lt;/p&gt;

&lt;p&gt;225&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)&lt;/p&gt;

&lt;p&gt;226&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)&lt;/p&gt;

&lt;p&gt;227&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)&lt;/p&gt;

&lt;p&gt;228&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:266)&lt;/p&gt;

&lt;p&gt;229&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)&lt;/p&gt;

&lt;p&gt;230&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)&lt;/p&gt;

&lt;p&gt;231&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)&lt;/p&gt;

&lt;p&gt;232&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)&lt;/p&gt;

&lt;p&gt;233&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)&lt;/p&gt;

&lt;p&gt;234&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)&lt;/p&gt;

&lt;p&gt;235&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsDown$1.apply(QueryPlan.scala:258)&lt;/p&gt;

&lt;p&gt;236&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsDown$1.apply(QueryPlan.scala:258)&lt;/p&gt;

&lt;p&gt;237&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)&lt;/p&gt;

&lt;p&gt;238&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)&lt;/p&gt;

&lt;p&gt;239&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:293)&lt;/p&gt;

&lt;p&gt;240&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&lt;/p&gt;

&lt;p&gt;241&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&lt;/p&gt;

&lt;p&gt;242&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.collection.immutable.List.foreach(List.scala:381)&lt;/p&gt;

&lt;p&gt;243&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&lt;/p&gt;

&lt;p&gt;244&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.collection.immutable.List.map(List.scala:285)&lt;/p&gt;

&lt;p&gt;245&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:293)&lt;/p&gt;

&lt;p&gt;246&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)&lt;/p&gt;

&lt;p&gt;247&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)&lt;/p&gt;

&lt;p&gt;248&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)&lt;/p&gt;

&lt;p&gt;249&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDown(QueryPlan.scala:258)&lt;/p&gt;

&lt;p&gt;250&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressions(QueryPlan.scala:249)&lt;/p&gt;

&lt;p&gt;251&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformAllExpressions$1.applyOrElse(QueryPlan.scala:309)&lt;/p&gt;

&lt;p&gt;252&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformAllExpressions$1.applyOrElse(QueryPlan.scala:308)&lt;/p&gt;

&lt;p&gt;253&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)&lt;/p&gt;

&lt;p&gt;254&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)&lt;/p&gt;

&lt;p&gt;255&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)&lt;/p&gt;

&lt;p&gt;256&#160;&#160;&#160; &#160;&#160;&#160;&#160;&#160;at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:266)&lt;/p&gt;

&lt;p&gt;257&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)&lt;/p&gt;

&lt;p&gt;258&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)&lt;/p&gt;

&lt;p&gt;259&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$11.apply(TreeNode.scala:335)&lt;/p&gt;

&lt;p&gt;260&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&lt;/p&gt;

&lt;p&gt;261&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&lt;/p&gt;

&lt;p&gt;262&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.collection.immutable.List.foreach(List.scala:381)&lt;/p&gt;

&lt;p&gt;263&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&lt;/p&gt;

&lt;p&gt;264&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.collection.immutable.List.map(List.scala:285)&lt;/p&gt;

&lt;p&gt;265&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:333)&lt;/p&gt;

&lt;p&gt;266&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)&lt;/p&gt;

&lt;p&gt;267&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)&lt;/p&gt;

&lt;p&gt;268&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)&lt;/p&gt;

&lt;p&gt;269&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)&lt;/p&gt;

&lt;p&gt;270&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)&lt;/p&gt;

&lt;p&gt;271&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)&lt;/p&gt;

&lt;p&gt;272&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)&lt;/p&gt;

&lt;p&gt;273&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)&lt;/p&gt;

&lt;p&gt;274&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)&lt;/p&gt;

&lt;p&gt;275&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)&lt;/p&gt;

&lt;p&gt;276&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)&lt;/p&gt;

&lt;p&gt;277&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)&lt;/p&gt;

&lt;p&gt;278&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)&lt;/p&gt;

&lt;p&gt;279&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)&lt;/p&gt;

&lt;p&gt;280&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)&lt;/p&gt;

&lt;p&gt;281&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)&lt;/p&gt;

&lt;p&gt;282&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)&lt;/p&gt;

&lt;p&gt;283&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)&lt;/p&gt;

&lt;p&gt;284&#160;&#160;&#160;&#160;&#160; &#160;&#160;&#160;at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)&lt;/p&gt;

&lt;p&gt;285&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)&lt;/p&gt;

&lt;p&gt;286&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)&lt;/p&gt;

&lt;p&gt;287&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:256)&lt;/p&gt;

&lt;p&gt;288&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.plans.QueryPlan.transformAllExpressions(QueryPlan.scala:308)&lt;/p&gt;

&lt;p&gt;289&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:1195)&lt;/p&gt;

&lt;p&gt;290&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:1194)&lt;/p&gt;

&lt;p&gt;291&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)&lt;/p&gt;

&lt;p&gt;292&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)&lt;/p&gt;

&lt;p&gt;293&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)&lt;/p&gt;

&lt;p&gt;294&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)&lt;/p&gt;

&lt;p&gt;295&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)&lt;/p&gt;

&lt;p&gt;296&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)&lt;/p&gt;

&lt;p&gt;297&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)&lt;/p&gt;

&lt;p&gt;298&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at scala.collection.immutable.List.foreach(List.scala:381)&lt;/p&gt;

&lt;p&gt;299&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)&lt;/p&gt;

&lt;p&gt;300&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:69)&lt;/p&gt;

&lt;p&gt;301&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; - locked &amp;lt;0x00000000f9c68cd8&amp;gt; (a org.apache.spark.sql.execution.QueryExecution)&lt;/p&gt;

&lt;p&gt;302&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:67)&lt;/p&gt;

&lt;p&gt;303&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:50)&lt;/p&gt;

&lt;p&gt;304&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)&lt;/p&gt;

&lt;p&gt;305&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:632)&lt;/p&gt;

&lt;p&gt;306&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at com.nokia.rtna.sumrz.engine.JobExecutorTask.run(JobExecutorTask.java:181)&lt;/p&gt;

&lt;p&gt;307 &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)&lt;/p&gt;

&lt;p&gt;308&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at java.util.concurrent.FutureTask.run(FutureTask.java:266)&lt;/p&gt;

&lt;p&gt;309&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)&lt;/p&gt;

&lt;p&gt;310&#160;&#160; &#160;&#160;&#160;&#160;&#160;&#160;at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)&lt;/p&gt;

&lt;p&gt;311&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; at java.lang.Thread.run(Thread.java:748)&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Thanks and BR,&lt;/p&gt;

&lt;p&gt;Christophe&lt;/p&gt;</comment>
                            <comment id="16372289" author="lian cheng" created="Thu, 22 Feb 2018 01:21:24 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=LANDAIS+Christophe&quot; class=&quot;user-hover&quot; rel=&quot;LANDAIS Christophe&quot;&gt;LANDAIS Christophe&lt;/a&gt;, I filed &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-23486&quot; title=&quot;LookupFunctions should not check the same function name more than once&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-23486&quot;&gt;&lt;del&gt;SPARK-23486&lt;/del&gt;&lt;/a&gt; for this. Should be relatively straightforward to fix and I&apos;d like to have a new contributor to try it as a starter task. Thanks for reporting!&lt;/p&gt;</comment>
                            <comment id="16394550" author="kevinyu98" created="Sun, 11 Mar 2018 15:54:16 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=LANDAIS+Christophe&quot; class=&quot;user-hover&quot; rel=&quot;LANDAIS Christophe&quot;&gt;LANDAIS Christophe&lt;/a&gt;, I&#160;submit a PR under&#160;&#160;&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-23486&quot; title=&quot;LookupFunctions should not check the same function name more than once&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-23486&quot;&gt;&lt;del&gt;SPARK-23486&lt;/del&gt;&lt;/a&gt;, can you try and to see if it helps ?&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310560">
                    <name>Problem/Incident</name>
                                            <outwardlinks description="causes">
                                        <issuelink>
            <issuekey id="13140082">SPARK-23486</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 36 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3ambz:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>