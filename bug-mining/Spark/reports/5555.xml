<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:58:45 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-23053] taskBinarySerialization and task partitions calculate in DagScheduler.submitMissingTasks should keep the same RDD checkpoint status</title>
                <link>https://issues.apache.org/jira/browse/SPARK-23053</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;When we run concurrent jobs using the same rdd which is marked to do checkpoint. If one job has finished running the job, and start the process of RDD.doCheckpoint, while another job is submitted, then submitStage and submitMissingTasks will be called. In &lt;a href=&quot;https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L961&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;submitMissingTasks&lt;/a&gt;, will serialize taskBinaryBytes and calculate task partitions which are both affected by the status of checkpoint, if the former is calculated before doCheckpoint finished, while the latter is calculated after doCheckpoint finished, when run task, rdd.compute will be called, for some rdds with particular partition type such as &lt;a href=&quot;https://github.com/apache/spark/blob/master/streaming/src/main/scala/org/apache/spark/streaming/rdd/MapWithStateRDD.scala&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;MapWithStateRDD&lt;/a&gt; who will do partition type cast, will get a ClassCastException because the part params is actually a CheckpointRDDPartition.&lt;br/&gt;
This error occurs because rdd.doCheckpoint occurs in the same thread that called sc.runJob, while the task serialization occurs in the DAGSchedulers event loop.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13130411">SPARK-23053</key>
            <summary>taskBinarySerialization and task partitions calculate in DagScheduler.submitMissingTasks should keep the same RDD checkpoint status</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="ivoson">Tengfei Huang</assignee>
                                    <reporter username="ivoson">Tengfei Huang</reporter>
                        <labels>
                    </labels>
                <created>Fri, 12 Jan 2018 04:43:51 +0000</created>
                <updated>Fri, 23 Feb 2018 06:32:47 +0000</updated>
                            <resolved>Tue, 13 Feb 2018 16:07:08 +0000</resolved>
                                    <version>2.1.0</version>
                                    <fixVersion>2.1.3</fixVersion>
                    <fixVersion>2.2.2</fixVersion>
                    <fixVersion>2.3.0</fixVersion>
                                    <component>Scheduler</component>
                    <component>Spark Core</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="16323541" author="apachespark" created="Fri, 12 Jan 2018 04:52:04 +0000"  >&lt;p&gt;User &apos;ivoson&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/20244&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/20244&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16353761" author="ivoson" created="Tue, 6 Feb 2018 11:47:32 +0000"  >&lt;p&gt;here is the stack trace of exception.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
java.lang.ClassCastException: org.apache.spark.rdd.CheckpointRDDPartition cannot be &lt;span class=&quot;code-keyword&quot;&gt;cast&lt;/span&gt; to org.apache.spark.streaming.rdd.MapWithStateRDDPartition
at org.apache.spark.streaming.rdd.MapWithStateRDD.compute(MapWithStateRDD.scala:152)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:957)
at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)
at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)
at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)
at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)
at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="16357952" author="ivoson" created="Fri, 9 Feb 2018 05:48:07 +0000"  >&lt;p&gt;the following is a repro case, for clarity&#160;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-comment&quot;&gt;/** Wrapped rdd partition. */&lt;/span&gt;
&lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;WrappedPartition(val partition: Partition) &lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; Partition {
  def index: Int = partition.index
}

/**
 * An RDD with a particular defined Partition which is WrappedPartition.
 * The compute method will &lt;span class=&quot;code-keyword&quot;&gt;cast&lt;/span&gt; the split to WrappedPartition. The &lt;span class=&quot;code-keyword&quot;&gt;cast&lt;/span&gt; operation will be
 * used in &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; test suite.
 */
&lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;WrappedRDD(parent: RDD[Int]) &lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; RDD[Int](parent) {
  &lt;span class=&quot;code-keyword&quot;&gt;protected&lt;/span&gt; def getPartitions: Array[Partition] = {
    parent.partitions.map(p =&amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; WrappedPartition(p))
  }

  def compute(split: Partition, context: TaskContext): Iterator[Int] = {
    parent.compute(split.asInstanceOf[WrappedPartition].partition, context)
  }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
/**
 * In &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; repro, we simulate the scene in concurrent jobs using the same
 * rdd which is marked to &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; checkpoint:
 * Job one has already finished the spark job, and start the process of doCheckpoint;
 * Job two is submitted, and submitMissingTasks is called.
 * In submitMissingTasks, &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; taskSerialization is called before doCheckpoint is done,
 * &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; part calculates from stage.rdd.partitions is called after doCheckpoint is done,
 * we may get a ClassCastException when execute the task because of some rdd will &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt;
 * Partition &lt;span class=&quot;code-keyword&quot;&gt;cast&lt;/span&gt;.
 *
 * With &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; test &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt;, just want to indicate that we should &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; taskSerialization and
 * part calculate in submitMissingTasks with the same rdd checkpoint status.
 */
repro(&lt;span class=&quot;code-quote&quot;&gt;&quot;SPARK-23053: avoid ClassCastException in concurrent execution with checkpoint&quot;&lt;/span&gt;) {
  &lt;span class=&quot;code-comment&quot;&gt;// set checkpointDir.
&lt;/span&gt;  val checkpointDir = Utils.createTempDir()
  sc.setCheckpointDir(checkpointDir.toString)

  &lt;span class=&quot;code-comment&quot;&gt;// Semaphores to control the process sequence &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; the two threads below.
&lt;/span&gt;  val doCheckpointStarted = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Semaphore(0)
  val taskBinaryBytesFinished = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Semaphore(0)
  val checkpointStateUpdated = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Semaphore(0)

  val rdd = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; WrappedRDD(sc.makeRDD(1 to 100, 4))
  rdd.checkpoint()

  val checkpointRunnable = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;Runnable&lt;/span&gt; {
    override def run() = {
      &lt;span class=&quot;code-comment&quot;&gt;// Simulate what RDD.doCheckpoint() does here.
&lt;/span&gt;      rdd.doCheckpointCalled = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;
      val checkpointData = rdd.checkpointData.get
      RDDCheckpointData.&lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; {
        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (checkpointData.cpState == CheckpointState.Initialized) {
          checkpointData.cpState = CheckpointState.CheckpointingInProgress
        }
      }

      val newRDD = checkpointData.doCheckpoint()

      &lt;span class=&quot;code-comment&quot;&gt;// Release doCheckpointStarted after job triggered in checkpoint finished, so
&lt;/span&gt;      &lt;span class=&quot;code-comment&quot;&gt;// that taskBinary serialization can start.
&lt;/span&gt;      doCheckpointStarted.release()
      &lt;span class=&quot;code-comment&quot;&gt;// Wait until taskBinary serialization finished in submitMissingTasksThread.
&lt;/span&gt;      taskBinaryBytesFinished.acquire()

      &lt;span class=&quot;code-comment&quot;&gt;// Update our state and truncate the RDD lineage.
&lt;/span&gt;      RDDCheckpointData.&lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; {
        checkpointData.cpRDD = Some(newRDD)
        checkpointData.cpState = CheckpointState.Checkpointed
        rdd.markCheckpointed()
      }
      checkpointStateUpdated.release()
    }
  }

  val submitMissingTasksRunnable = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;Runnable&lt;/span&gt; {
    override def run() = {
      &lt;span class=&quot;code-comment&quot;&gt;// Simulate the process of submitMissingTasks.
&lt;/span&gt;      &lt;span class=&quot;code-comment&quot;&gt;// Wait until doCheckpoint job running finished, but checkpoint status not changed.
&lt;/span&gt;      doCheckpointStarted.acquire()

      val ser = SparkEnv.get.closureSerializer.newInstance()

      &lt;span class=&quot;code-comment&quot;&gt;// Simulate task serialization &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; submitMissingTasks.
&lt;/span&gt;      &lt;span class=&quot;code-comment&quot;&gt;// Task serialized with rdd checkpoint not finished.
&lt;/span&gt;      val cleanedFunc = sc.clean(Utils.getIteratorSize _)
      val func = (ctx: TaskContext, it: Iterator[Int]) =&amp;gt; cleanedFunc(it)
      val taskBinaryBytes = JavaUtils.bufferToArray(
        ser.serialize((rdd, func): AnyRef))
      &lt;span class=&quot;code-comment&quot;&gt;// Because partition calculate is in a &lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; block, so in the fixed code
&lt;/span&gt;      &lt;span class=&quot;code-comment&quot;&gt;// partition is calculated here.
&lt;/span&gt;      val correctPart = rdd.partitions(0)

      &lt;span class=&quot;code-comment&quot;&gt;// Release taskBinaryBytesFinished so changing checkpoint status to Checkpointed will
&lt;/span&gt;      &lt;span class=&quot;code-comment&quot;&gt;// be done in checkpointThread.
&lt;/span&gt;      taskBinaryBytesFinished.release()
      &lt;span class=&quot;code-comment&quot;&gt;// Wait until checkpoint status changed to Checkpointed in checkpointThread.
&lt;/span&gt;      checkpointStateUpdated.acquire()

      &lt;span class=&quot;code-comment&quot;&gt;// Now we&apos;re done simulating the interleaving that might happen within the scheduler,
&lt;/span&gt;      &lt;span class=&quot;code-comment&quot;&gt;// we&apos;ll check to make sure the &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; state is OK by simulating a couple steps that
&lt;/span&gt;      &lt;span class=&quot;code-comment&quot;&gt;// normally happen on the executor.
&lt;/span&gt;      &lt;span class=&quot;code-comment&quot;&gt;// Part calculated with rdd checkpoint already finished.
&lt;/span&gt;      val errPart = rdd.partitions(0)

      &lt;span class=&quot;code-comment&quot;&gt;// TaskBinary will be deserialized when run task in executor.
&lt;/span&gt;      val (taskRdd, taskFunc) = ser.deserialize[(RDD[Int], (TaskContext, Iterator[Int]) =&amp;gt; Unit)](
        ByteBuffer.wrap(taskBinaryBytes), &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.currentThread.getContextClassLoader)

      val taskContext = mock(classOf[TaskContext])
      doNothing().when(taskContext).killTaskIfInterrupted()

      &lt;span class=&quot;code-comment&quot;&gt;// Make sure our test &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; is setup correctly -- we expect a ClassCastException here
&lt;/span&gt;      &lt;span class=&quot;code-comment&quot;&gt;// &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; we use the rdd.partitions after checkpointing was done, but our binary bytes is
&lt;/span&gt;      &lt;span class=&quot;code-comment&quot;&gt;// from before it finished.
&lt;/span&gt;      intercept[ClassCastException] {
        &lt;span class=&quot;code-comment&quot;&gt;// Triggered when runTask in executor.
&lt;/span&gt;        taskRdd.iterator(errPart, taskContext)
      }

      &lt;span class=&quot;code-comment&quot;&gt;// Execute successfully with correctPart.
&lt;/span&gt;      taskRdd.iterator(correctPart, taskContext)
    }
  }

  &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
    &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;(checkpointRunnable).start()
    val submitMissingTasksThread = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;(submitMissingTasksRunnable)
    submitMissingTasksThread.start()
    submitMissingTasksThread.join()
  } &lt;span class=&quot;code-keyword&quot;&gt;finally&lt;/span&gt; {
    Utils.deleteRecursively(checkpointDir)
  }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16362556" author="irashid" created="Tue, 13 Feb 2018 16:03:46 +0000"  >&lt;p&gt;Fixed by &lt;a href=&quot;https://github.com/apache/spark/pull/20244&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/20244&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I set the fix version to 2.3.1, because we&apos;re in the middle of voting for RC3.  If we cut another RC this would actually be fixed in 2.3.0.&lt;/p&gt;</comment>
                            <comment id="16368560" author="apachespark" created="Sun, 18 Feb 2018 15:21:05 +0000"  >&lt;p&gt;User &apos;ivoson&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/20635&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/20635&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 39 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3ou1j:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>