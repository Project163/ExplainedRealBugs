<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 19:26:27 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-35602] Job crashes with java.io.UTFDataFormatException: encoded string too long</title>
                <link>https://issues.apache.org/jira/browse/SPARK-35602</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Running stafeful structured streaming app using java. When running on Spark 3.1.1 app is crashing with java.io.UTFDataFormatException: encoded string too long. I am not getting this problem when running on Spark 3.0.1&lt;/p&gt;

&lt;p&gt;21/06/01 17:50:35 WARN DAGScheduler: Broadcasting large task binary with size 1986.3 KiB21/06/01 17:50:35 WARN DAGScheduler: Broadcasting large task binary with size 1986.3 KiB21/06/01 17:50:37 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 0) (ip-10-64-12-189.eu-west-1.compute.internal executor 1): java.io.UTFDataFormatException: encoded string too long: 156449 bytes at java.io.DataOutputStream.writeUTF(DataOutputStream.java:364) at java.io.DataOutputStream.writeUTF(DataOutputStream.java:323) at org.apache.spark.sql.execution.streaming.state.StateSchemaCompatibilityChecker.createSchemaFile(StateSchemaCompatibilityChecker.scala:102) at org.apache.spark.sql.execution.streaming.state.StateSchemaCompatibilityChecker.check(StateSchemaCompatibilityChecker.scala:67) at org.apache.spark.sql.execution.streaming.state.StateStore$.$anonfun$getStateStoreProvider$2(StateStore.scala:487) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) at scala.util.Try$.apply(Try.scala:213) at org.apache.spark.sql.execution.streaming.state.StateStore$.$anonfun$getStateStoreProvider$1(StateStore.scala:487) at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86) at org.apache.spark.sql.execution.streaming.state.StateStore$.getStateStoreProvider(StateStore.scala:483) at org.apache.spark.sql.execution.streaming.state.StateStore$.get(StateStore.scala:468) at org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:125) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373) at org.apache.spark.rdd.RDD.iterator(RDD.scala:337) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373) at org.apache.spark.rdd.RDD.iterator(RDD.scala:337) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373) at org.apache.spark.rdd.RDD.iterator(RDD.scala:337) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:131) at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)&lt;br/&gt;
21/06/01 17:50:40 ERROR TaskSetManager: Task 0 in stage 1.0 failed 4 times; aborting job21/06/01 17:50:40 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@2745b41f is aborting.21/06/01 17:50:40 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@2745b41f aborted.21/06/01 17:50:40 ERROR MicroBatchExecution: Query &lt;span class=&quot;error&quot;&gt;&amp;#91;id = adcf4f93-8c51-4a14-9d9d-1e7a858c8a8c, runId = 86b6c41c-a32f-485d-bbf3-24b844c27739&amp;#93;&lt;/span&gt; terminated with errororg.apache.spark.SparkException: Writing job aborted. at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:388) at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:336) at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:297) at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:304) at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:40) at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:40) at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:46) at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3733) at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3005) at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3724) at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107) at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232) at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135) at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107) at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135) at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68) at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3722) at org.apache.spark.sql.Dataset.collect(Dataset.scala:3005) at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:589) at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107) at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232) at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135) at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107) at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135) at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68) at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$15(MicroBatchExecution.scala:584) at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:357) at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:355) at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68) at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:584) at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:226) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:357) at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:355) at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68) at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:194) at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57) at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:188) at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:333) at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)&lt;/p&gt;</description>
                <environment>&lt;p&gt;AWS emr-6.3.0&lt;/p&gt;</environment>
        <key id="13381529">SPARK-35602</key>
            <summary>Job crashes with java.io.UTFDataFormatException: encoded string too long</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="sarutak">Kousuke Saruta</assignee>
                                    <reporter username="dejanm@gmail.com">dejan miljkovic</reporter>
                        <labels>
                    </labels>
                <created>Tue, 1 Jun 2021 18:05:55 +0000</created>
                <updated>Wed, 9 Jun 2021 03:46:18 +0000</updated>
                            <resolved>Wed, 9 Jun 2021 01:36:41 +0000</resolved>
                                    <version>3.1.1</version>
                                    <fixVersion>3.2.0</fixVersion>
                    <fixVersion>3.1.3</fixVersion>
                                    <component>Structured Streaming</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="17356088" author="sarutak" created="Thu, 3 Jun 2021 02:25:45 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dejanm%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;dejanm@gmail.com&quot;&gt;dejanm@gmail.com&lt;/a&gt; , could you provide reproducible code?&lt;/p&gt;</comment>
                            <comment id="17356138" author="dejanm@gmail.com" created="Thu, 3 Jun 2021 03:43:32 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sarutak&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;Kousuke&lt;/a&gt;,&lt;/p&gt;

&lt;p&gt;Thanks for the quick response.&lt;/p&gt;

&lt;p&gt;Not sure if I can create a simple example. This is a huge app with complex logic. The key thing is stateful part. State class is multi-level deep and can be quite big. My estimate is up to 30Mb.&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Note that code was running on Spark 2.4 and 3.0.1. So this is probably regression introduced by Spark 3.1.1&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;I will try to make an example that I can share.&lt;/p&gt;

&lt;p&gt;Dejan&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17356159" author="dejanm@gmail.com" created="Thu, 3 Jun 2021 04:57:29 +0000"  >&lt;p&gt;The problem is in &lt;br/&gt;
StateSchemaCompatibilityChecker.createSchemaFile &lt;br/&gt;
which is calling &lt;br/&gt;
DataOutputStream.writeUTF&lt;br/&gt;
for json schema string&lt;/p&gt;

&lt;p&gt;wtiteUTF is imiting sring to 65535&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;#000080&quot;&gt;if &lt;/font&gt;(utflen &amp;gt; &lt;font color=&quot;#0000ff&quot;&gt;65535&lt;/font&gt;)&lt;br/&gt;
 &lt;font color=&quot;#000080&quot;&gt;throw new &lt;/font&gt;UTFDataFormatException(&lt;br/&gt;
 &lt;font color=&quot;#008000&quot;&gt;&quot;encoded string too long: &quot; &lt;/font&gt;+ utflen + &lt;font color=&quot;#008000&quot;&gt;&quot; bytes&quot;&lt;/font&gt;);&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Basically code is limiting complexity of the schema that can be used in stateful&#160; streaming by json&lt;/b&gt; &lt;b&gt;schema&lt;/b&gt; &lt;b&gt;string size.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;I think that DataOutputStream.writeUTF should not be used in StateSchemaCompatibilityChecker.createSchemaFile&#160;&lt;/p&gt;

&lt;p&gt;Best,&lt;/p&gt;

&lt;p&gt;Dejan&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;#0000ff&quot;&gt;&#160;&lt;/font&gt;&lt;/p&gt;</comment>
                            <comment id="17356465" author="dejanm@gmail.com" created="Thu, 3 Jun 2021 14:14:04 +0000"  >&lt;p&gt;StateSchemaCompatibilityChecker.scala is added to the project in&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-27237&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/SPARK-27237&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Best,&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Dejan&lt;/p&gt;</comment>
                            <comment id="17356614" author="sarutak" created="Thu, 3 Jun 2021 17:33:28 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dejanm%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;dejanm@gmail.com&quot;&gt;dejanm@gmail.com&lt;/a&gt;&lt;br/&gt;
Thank you for reporting. I understand the reason.&lt;br/&gt;
You can set spark.sql.streaming.stateStore.stateSchemaCheck to false for a workaround.&lt;/p&gt;</comment>
                            <comment id="17357757" author="apachespark" created="Sat, 5 Jun 2021 05:23:13 +0000"  >&lt;p&gt;User &apos;sarutak&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/32788&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/32788&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17357758" author="apachespark" created="Sat, 5 Jun 2021 05:24:38 +0000"  >&lt;p&gt;User &apos;sarutak&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/32788&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/32788&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17359687" author="kabhwan" created="Wed, 9 Jun 2021 01:36:41 +0000"  >&lt;p&gt;Issue resolved by pull request 32788&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/32788&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/32788&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17359693" author="kabhwan" created="Wed, 9 Jun 2021 01:49:48 +0000"  >&lt;p&gt;Very interested that the json representation of the schema exceeds 64k.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dejanm%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;dejanm@gmail.com&quot;&gt;dejanm@gmail.com&lt;/a&gt;&lt;br/&gt;
Would you mind sharing the schema, with redaction on the field name? Like redacting the field name &quot;hello&quot; to &quot;xxxxx&quot; (same length). Never mind if the schema cannot be shared.&lt;/p&gt;</comment>
                            <comment id="17359741" author="dejanm@gmail.com" created="Wed, 9 Jun 2021 03:46:18 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=kabhwan&quot; class=&quot;user-hover&quot; rel=&quot;kabhwan&quot;&gt;kabhwan&lt;/a&gt;,&lt;/p&gt;

&lt;p&gt;Sorry can not share the schema. I am using class (encoded by&#160;Encoders.bean) that has many levels. Some time variable names are quite long. Application is using stateful streaming api and it is calculating user statistics for one network protocol.&#160;The protocol is quite rich.&lt;/p&gt;

&lt;p&gt;Dejan&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            4 years, 22 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z0rk4o:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>