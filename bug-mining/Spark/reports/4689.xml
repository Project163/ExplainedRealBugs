<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:52:19 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-20086] issue with pyspark 2.1.0 window function</title>
                <link>https://issues.apache.org/jira/browse/SPARK-20086</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;original  post at&lt;br/&gt;
&lt;a href=&quot;http://stackoverflow.com/questions/43007433/pyspark-2-1-0-error-when-working-with-window-function&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;stackoverflow &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I get error when working with pyspark window function. here is some example code:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;&quot;&gt;&lt;b&gt;borderStyle=solid&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; pyspark
    &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; pyspark.sql.functions as sf
    &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; pyspark.sql.types as sparktypes
    from pyspark.sql &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; window
    
    sc = pyspark.SparkContext()
    sqlc = pyspark.SQLContext(sc)
    rdd = sc.parallelize([(1, 2.0), (1, 3.0), (1, 1.), (1, -2.), (1, -1.)])
    df = sqlc.createDataFrame(rdd, [&lt;span class=&quot;code-quote&quot;&gt;&quot;x&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;AmtPaid&quot;&lt;/span&gt;])
    df.show()

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;gives:&lt;/p&gt;


&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  x&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;AmtPaid&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;    2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;    3.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;    1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;   -2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;   -1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;



&lt;p&gt;next, compute cumulative sum&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-style: solid;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;border-bottom-style: solid;&quot;&gt;&lt;b&gt;test.py&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    win_spec_max = (window.Window
                    .partitionBy([&lt;span class=&quot;code-quote&quot;&gt;&apos;x&apos;&lt;/span&gt;])
                    .rowsBetween(window.Window.unboundedPreceding, 0)))
    df = df.withColumn(&lt;span class=&quot;code-quote&quot;&gt;&apos;AmtPaidCumSum&apos;&lt;/span&gt;,
                       sf.sum(sf.col(&lt;span class=&quot;code-quote&quot;&gt;&apos;AmtPaid&apos;&lt;/span&gt;)).over(win_spec_max))
    df.show()
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;gives,&lt;/p&gt;


&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  x&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;AmtPaid&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;AmtPaidCumSum&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;    2.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;          2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;    3.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;          5.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;    1.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;          6.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;   -2.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;          4.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;   -1.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;          3.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;


&lt;p&gt;next, compute cumulative max,&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    df = df.withColumn(&lt;span class=&quot;code-quote&quot;&gt;&apos;AmtPaidCumSumMax&apos;&lt;/span&gt;, sf.max(sf.col(&lt;span class=&quot;code-quote&quot;&gt;&apos;AmtPaidCumSum&apos;&lt;/span&gt;)).over(win_spec_max))

    df.show()
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;gives error log&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;     Py4JJavaError: An error occurred while calling o2609.showString.


with traceback:


    Py4JJavaErrorTraceback (most recent call last)
    &amp;lt;ipython-input-215-3106d06b6e49&amp;gt; in &amp;lt;module&amp;gt;()
    ----&amp;gt; 1 df.show()

    /Users/&amp;lt;&amp;gt;/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/dataframe.pyc in show(self, n, truncate)
        316         &quot;&quot;&quot;
        317         if isinstance(truncate, bool) and truncate:
    --&amp;gt; 318             print(self._jdf.showString(n, 20))
        319         else:
        320             print(self._jdf.showString(n, int(truncate)))

    /Users/&amp;lt;&amp;gt;/.virtualenvs/&amp;lt;&amp;gt;/lib/python2.7/site-packages/py4j/java_gateway.pyc in __call__(self, *args)
       1131         answer = self.gateway_client.send_command(command)
       1132         return_value = get_return_value(
    -&amp;gt; 1133             answer, self.gateway_client, self.target_id, self.name)
       1134 
       1135         for temp_arg in temp_args:

    /Users/&amp;lt;&amp;gt;/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw)
         61     def deco(*a, **kw):
         62         try:
    ---&amp;gt; 63             return f(*a, **kw)
         64         except py4j.protocol.Py4JJavaError as e:
         65             s = e.java_exception.toString()

    /Users/&amp;lt;&amp;gt;/.virtualenvs/&amp;lt;&amp;gt;/lib/python2.7/site-packages/py4j/protocol.pyc in get_return_value(answer, gateway_client, target_id, name)
        317                 raise Py4JJavaError(
        318                     &quot;An error occurred while calling {0}{1}{2}.\n&quot;.
    --&amp;gt; 319                     format(target_id, &quot;.&quot;, name), value)
        320             else:
        321                 raise Py4JError(
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;but interestingly enough, if i introduce another change before sencond window operation, say inserting a column then it does not give that error:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    df = df.withColumn(&lt;span class=&quot;code-quote&quot;&gt;&apos;MaxBound&apos;&lt;/span&gt;, sf.lit(6.))
    df.show()
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  x&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;AmtPaid&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;AmtPaidCumSum&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;MaxBound&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;    2.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;          2.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;     6.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;    3.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;          5.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;     6.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;    1.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;          6.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;     6.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;   -2.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;          4.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;     6.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;   -1.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;          3.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;     6.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;


&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    #then apply the second window operations
    df = df.withColumn(&lt;span class=&quot;code-quote&quot;&gt;&apos;AmtPaidCumSumMax&apos;&lt;/span&gt;, sf.max(sf.col(&lt;span class=&quot;code-quote&quot;&gt;&apos;AmtPaidCumSum&apos;&lt;/span&gt;)).over(win_spec_max))
    df.show()
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  x&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;AmtPaid&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;AmtPaidCumSum&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;MaxBound&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;AmtPaidCumSumMax&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;    2.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;          2.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;     6.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;             2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;    3.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;          5.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;     6.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;             5.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;    1.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;          6.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;     6.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;             6.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;   -2.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;          4.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;     6.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;             6.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;   -1.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;          3.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;     6.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;             6.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;


&lt;p&gt;I do not understand this behaviour&lt;/p&gt;

&lt;p&gt;well, so far so good, but then I try another operation then again get similar error:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    def _udf_compare_cumsum_sll(x):
        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; x[&lt;span class=&quot;code-quote&quot;&gt;&apos;AmtPaidCumSumMax&apos;&lt;/span&gt;] &amp;gt;= x[&lt;span class=&quot;code-quote&quot;&gt;&apos;MaxBound&apos;&lt;/span&gt;]:
            output = 0
        &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt;:
            output = x[&lt;span class=&quot;code-quote&quot;&gt;&apos;AmtPaid&apos;&lt;/span&gt;]
        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; output

    udf_compare_cumsum_sll = sf.udf(_udf_compare_cumsum_sll, sparktypes.FloatType())
    df = df.withColumn(&lt;span class=&quot;code-quote&quot;&gt;&apos;AmtPaidAdjusted&apos;&lt;/span&gt;, udf_compare_cumsum_sll(sf.struct([df[x] &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; x in df.columns])))
    df.show()
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;gives,&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;    Py4JJavaErrorTraceback (most recent call last)
    &amp;lt;ipython-input-18-3106d06b6e49&amp;gt; in &amp;lt;module&amp;gt;()
    ----&amp;gt; 1 df.show()

    /Users/&amp;lt;&amp;gt;/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/dataframe.pyc in show(self, n, truncate)
        316         &quot;&quot;&quot;
        317         if isinstance(truncate, bool) and truncate:
    --&amp;gt; 318             print(self._jdf.showString(n, 20))
        319         else:
        320             print(self._jdf.showString(n, int(truncate)))

    /Users/&amp;lt;&amp;gt;/.virtualenvs/&amp;lt;&amp;gt;/lib/python2.7/site-packages/py4j/java_gateway.pyc in __call__(self, *args)
       1131         answer = self.gateway_client.send_command(command)
       1132         return_value = get_return_value(
    -&amp;gt; 1133             answer, self.gateway_client, self.target_id, self.name)
       1134 
       1135         for temp_arg in temp_args:

    /Users/&amp;lt;&amp;gt;/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw)
         61     def deco(*a, **kw):
         62         try:
    ---&amp;gt; 63             return f(*a, **kw)
         64         except py4j.protocol.Py4JJavaError as e:
         65             s = e.java_exception.toString()

    /Users/&amp;lt;&amp;gt;/.virtualenvs/&amp;lt;&amp;gt;/lib/python2.7/site-packages/py4j/protocol.pyc in get_return_value(answer, gateway_client, target_id, name)
        317                 raise Py4JJavaError(
        318                     &quot;An error occurred while calling {0}{1}{2}.\n&quot;.
    --&amp;gt; 319                     format(target_id, &quot;.&quot;, name), value)
        320             else:
        321                 raise Py4JError(

    Py4JJavaError: An error occurred while calling o91.showString.
    : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 36.0 failed 1 times, most recent failure: Lost task 0.0 in stage 36.0 (TID 645, localhost, executor driver): org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Binding attribute, tree: AmtPaidCumSum#10
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;    	

&lt;p&gt;I wonder if someone could reproduce this behaviour ...&lt;/p&gt;


&lt;p&gt;here is complete log ..&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;    Py4JJavaErrorTraceback (most recent call last)
    &amp;lt;ipython-input-69-3106d06b6e49&amp;gt; in &amp;lt;module&amp;gt;()
    ----&amp;gt; 1 df.show()

    /Users/&amp;lt;&amp;gt;/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/dataframe.pyc in show(self, n, truncate)
        316         &quot;&quot;&quot;
        317         if isinstance(truncate, bool) and truncate:
    --&amp;gt; 318             print(self._jdf.showString(n, 20))
        319         else:
        320             print(self._jdf.showString(n, int(truncate)))

    /Users/&amp;lt;&amp;gt;/.virtualenvs/&amp;lt;&amp;gt;/lib/python2.7/site-packages/py4j/java_gateway.pyc in __call__(self, *args)
       1131         answer = self.gateway_client.send_command(command)
       1132         return_value = get_return_value(
    -&amp;gt; 1133             answer, self.gateway_client, self.target_id, self.name)
       1134
       1135         for temp_arg in temp_args:

    /Users/&amp;lt;&amp;gt;/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw)
         61     def deco(*a, **kw):
         62         try:
    ---&amp;gt; 63             return f(*a, **kw)
         64         except py4j.protocol.Py4JJavaError as e:
         65             s = e.java_exception.toString()

    /Users/&amp;lt;&amp;gt;/.virtualenvs/&amp;lt;&amp;gt;/lib/python2.7/site-packages/py4j/protocol.pyc in get_return_value(answer, gateway_client, target_id, name)
        317                 raise Py4JJavaError(
        318                     &quot;An error occurred while calling {0}{1}{2}.\n&quot;.
    --&amp;gt; 319                     format(target_id, &quot;.&quot;, name), value)
        320             else:
        321                 raise Py4JError(

    Py4JJavaError: An error occurred while calling o703.showString.
    : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 119.0 failed 1 times, most recent failure: Lost task 0.0 in stage 119.0 (TID 1817, localhost, executor driver): org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Binding attribute, tree: AmtPaidCumSum#2076
    	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
    	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:88)
    	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:87)
    	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:288)
    	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:288)
    	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:287)
    	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
    	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
    	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5$$anonfun$apply$11.apply(TreeNode.scala:360)
    	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
    	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
    	at scala.collection.immutable.List.foreach(List.scala:381)
    	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
    	at scala.collection.immutable.List.map(List.scala:285)
    	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:358)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:277)
    	at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:87)
    	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$$anonfun$bind$1.apply(GenerateMutableProjection.scala:38)
    	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$$anonfun$bind$1.apply(GenerateMutableProjection.scala:38)
    	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
    	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
    	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
    	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
    	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
    	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.bind(GenerateMutableProjection.scala:38)
    	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.generate(GenerateMutableProjection.scala:44)
    	at org.apache.spark.sql.execution.SparkPlan.newMutableProjection(SparkPlan.scala:353)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$windowFrameExpressionFactoryPairs$2$$anonfun$org$apache$spark$sql$execution$window$WindowExec$$anonfun$$processor$1$1.apply(WindowExec.scala:203)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$windowFrameExpressionFactoryPairs$2$$anonfun$org$apache$spark$sql$execution$window$WindowExec$$anonfun$$processor$1$1.apply(WindowExec.scala:202)
    	at org.apache.spark.sql.execution.window.AggregateProcessor$.apply(AggregateProcessor.scala:98)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$windowFrameExpressionFactoryPairs$2.org$apache$spark$sql$execution$window$WindowExec$$anonfun$$processor$1(WindowExec.scala:198)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$windowFrameExpressionFactoryPairs$2$$anonfun$6.apply(WindowExec.scala:225)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$windowFrameExpressionFactoryPairs$2$$anonfun$6.apply(WindowExec.scala:222)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$14$$anon$1$$anonfun$16.apply(WindowExec.scala:318)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$14$$anon$1$$anonfun$16.apply(WindowExec.scala:318)
    	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
    	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
    	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
    	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
    	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
    	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$14$$anon$1.&amp;lt;init&amp;gt;(WindowExec.scala:318)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$14.apply(WindowExec.scala:290)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$14.apply(WindowExec.scala:289)
    	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)
    	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)
    	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
    	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
    	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
    	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
    	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
    	at org.apache.spark.scheduler.Task.run(Task.scala:99)
    	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
    	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    	at java.lang.Thread.run(Thread.java:745)
    Caused by: java.lang.RuntimeException: Couldn&apos;t find AmtPaidCumSum#2076 in [sum#2299,max#2300,x#2066L,AmtPaid#2067]
    	at scala.sys.package$.error(package.scala:27)
    	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1$$anonfun$applyOrElse$1.apply(BoundAttribute.scala:94)
    	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1$$anonfun$applyOrElse$1.apply(BoundAttribute.scala:88)
    	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
    	... 62 more

    Driver stacktrace:
    	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
    	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
    	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
    	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
    	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
    	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
    	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
    	at scala.Option.foreach(Option.scala:257)
    	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
    	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
    	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
    	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
    	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
    	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
    	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)
    	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)
    	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)
    	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:333)
    	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
    	at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2371)
    	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
    	at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765)
    	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2370)
    	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2377)
    	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2113)
    	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2112)
    	at org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2795)
    	at org.apache.spark.sql.Dataset.head(Dataset.scala:2112)
    	at org.apache.spark.sql.Dataset.take(Dataset.scala:2327)
    	at org.apache.spark.sql.Dataset.showString(Dataset.scala:248)
    	at sun.reflect.GeneratedMethodAccessor83.invoke(Unknown Source)
    	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    	at java.lang.reflect.Method.invoke(Method.java:498)
    	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    	at py4j.Gateway.invoke(Gateway.java:280)
    	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    	at py4j.commands.CallCommand.execute(CallCommand.java:79)
    	at py4j.GatewayConnection.run(GatewayConnection.java:214)
    	at java.lang.Thread.run(Thread.java:745)
    Caused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Binding attribute, tree: null
    	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
    	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:88)
    	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:87)
    	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:288)
    	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:288)
    	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:287)
    	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
    	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
    	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5$$anonfun$apply$11.apply(TreeNode.scala:360)
    	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
    	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
    	at scala.collection.immutable.List.foreach(List.scala:381)
    	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
    	at scala.collection.immutable.List.map(List.scala:285)
    	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:358)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:277)
    	at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:87)
    	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$$anonfun$bind$1.apply(GenerateMutableProjection.scala:38)
    	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$$anonfun$bind$1.apply(GenerateMutableProjection.scala:38)
    	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
    	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
    	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
    	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
    	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
    	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.bind(GenerateMutableProjection.scala:38)
    	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.generate(GenerateMutableProjection.scala:44)
    	at org.apache.spark.sql.execution.SparkPlan.newMutableProjection(SparkPlan.scala:353)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$windowFrameExpressionFactoryPairs$2$$anonfun$org$apache$spark$sql$execution$window$WindowExec$$anonfun$$processor$1$1.apply(WindowExec.scala:203)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$windowFrameExpressionFactoryPairs$2$$anonfun$org$apache$spark$sql$execution$window$WindowExec$$anonfun$$processor$1$1.apply(WindowExec.scala:202)
    	at org.apache.spark.sql.execution.window.AggregateProcessor$.apply(AggregateProcessor.scala:98)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$windowFrameExpressionFactoryPairs$2.org$apache$spark$sql$execution$window$WindowExec$$anonfun$$processor$1(WindowExec.scala:198)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$windowFrameExpressionFactoryPairs$2$$anonfun$6.apply(WindowExec.scala:225)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$windowFrameExpressionFactoryPairs$2$$anonfun$6.apply(WindowExec.scala:222)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$14$$anon$1$$anonfun$16.apply(WindowExec.scala:318)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$14$$anon$1$$anonfun$16.apply(WindowExec.scala:318)
    	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
    	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
    	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
    	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
    	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
    	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$14$$anon$1.&amp;lt;init&amp;gt;(WindowExec.scala:318)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$14.apply(WindowExec.scala:290)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$14.apply(WindowExec.scala:289)
    	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)
    	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)
    	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
    	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
    	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
    	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
    	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
    	at org.apache.spark.scheduler.Task.run(Task.scala:99)
    	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
    	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    	... 1 more
    Caused by: java.lang.RuntimeException: Couldn&apos;t find AmtPaidCumSum#2076 in [sum#2299,max#2300,x#2066L,AmtPaid#2067]
    	at scala.sys.package$.error(package.scala:27)
    	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1$$anonfun$applyOrElse$1.apply(BoundAttribute.scala:94)
    	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1$$anonfun$applyOrElse$1.apply(BoundAttribute.scala:88)
    	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
    	... 62 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="13059042">SPARK-20086</key>
            <summary>issue with pyspark 2.1.0 window function</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="hvanhovell">Herman van H&#246;vell</assignee>
                                    <reporter username="mandarup">mandar upadhye</reporter>
                        <labels>
                    </labels>
                <created>Fri, 24 Mar 2017 21:31:29 +0000</created>
                <updated>Tue, 13 Nov 2018 18:30:24 +0000</updated>
                            <resolved>Sun, 26 Mar 2017 20:48:57 +0000</resolved>
                                    <version>2.1.0</version>
                                    <fixVersion>2.1.1</fixVersion>
                    <fixVersion>2.2.0</fixVersion>
                                    <component>PySpark</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="15942190" author="apachespark" created="Sun, 26 Mar 2017 07:58:03 +0000"  >&lt;p&gt;User &apos;hvanhovell&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/17432&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/17432&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16087862" author="mscheifer" created="Fri, 14 Jul 2017 19:31:32 +0000"  >&lt;p&gt;Is there a way I can work around this if I&apos;m stuck on Spark 2.1.0 ?&lt;/p&gt;

&lt;p&gt;Thanks,&lt;br/&gt;
Matthew&lt;/p&gt;</comment>
                            <comment id="16111838" author="jacshen" created="Wed, 2 Aug 2017 22:23:40 +0000"  >&lt;p&gt;will there be a fix for 2.1.0?&lt;/p&gt;

&lt;p&gt;Regards,&lt;br/&gt;
Jacky&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13198118">SPARK-26041</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 15 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3crxj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>