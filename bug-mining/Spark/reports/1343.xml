<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:23:02 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-4520] SparkSQL exception when reading certain columns from a parquet file</title>
                <link>https://issues.apache.org/jira/browse/SPARK-4520</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;I am seeing this issue with spark sql throwing an exception when trying to read selective columns from a thrift parquet file and also when caching them.&lt;br/&gt;
On some further digging, I was able to narrow it down to at-least one particular column type: map&amp;lt;string, set&amp;lt;string&amp;gt;&amp;gt; to be causing this issue. To reproduce this I created a test thrift file with a very basic schema and stored some sample data in a parquet file:&lt;/p&gt;

&lt;p&gt;Test.thrift&lt;br/&gt;
===========&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;typedef binary SomeId

&lt;span class=&quot;code-keyword&quot;&gt;enum&lt;/span&gt; SomeExclusionCause {
  WHITELIST = 1,
  HAS_PURCHASE = 2,
}

struct SampleThriftObject {
  10: string col_a;
  20: string col_b;
  30: string col_c;
  40: optional map&amp;lt;SomeExclusionCause, set&amp;lt;SomeId&amp;gt;&amp;gt; col_d;
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;=============&lt;/p&gt;

&lt;p&gt;And loading the data in spark through schemaRDD:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.spark.sql.SchemaRDD
val sqlContext = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; org.apache.spark.sql.SQLContext(sc);
val parquetFile = &lt;span class=&quot;code-quote&quot;&gt;&quot;/path/to/generated/parquet/file&quot;&lt;/span&gt;
val parquetFileRDD = sqlContext.parquetFile(parquetFile)
parquetFileRDD.printSchema
root
 |-- col_a: string (nullable = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)
 |-- col_b: string (nullable = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)
 |-- col_c: string (nullable = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)
 |-- col_d: map (nullable = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)
 |    |-- key: string
 |    |-- value: array (valueContainsNull = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)
 |    |    |-- element: string (containsNull = &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;)

parquetFileRDD.registerTempTable(&lt;span class=&quot;code-quote&quot;&gt;&quot;test&quot;&lt;/span&gt;)
sqlContext.cacheTable(&lt;span class=&quot;code-quote&quot;&gt;&quot;test&quot;&lt;/span&gt;)
sqlContext.sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;select col_a from test&quot;&lt;/span&gt;).collect() &amp;lt;-- see the exception stack here 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): parquet.io.ParquetDecodingException: Can not read value at 0 in block -1 in file file:/tmp/xyz/part-r-00000.parquet
	at parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:213)
	at parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:204)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:145)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.&lt;span class=&quot;code-keyword&quot;&gt;generic&lt;/span&gt;.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$16.apply(RDD.scala:780)
	at org.apache.spark.rdd.RDD$$anonfun$16.apply(RDD.scala:780)
	at org.apache.spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:1223)
	at org.apache.spark.SparkContext$$anonfun$runJob$3.apply(SparkContext.scala:1223)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:195)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)

Caused by: java.lang.ArrayIndexOutOfBoundsException: -1
	at java.util.ArrayList.elementData(ArrayList.java:418)
	at java.util.ArrayList.get(ArrayList.java:431)
	at parquet.io.GroupColumnIO.getLast(GroupColumnIO.java:95)
	at parquet.io.GroupColumnIO.getLast(GroupColumnIO.java:95)
	at parquet.io.PrimitiveColumnIO.getLast(PrimitiveColumnIO.java:80)
	at parquet.io.PrimitiveColumnIO.isLast(PrimitiveColumnIO.java:74)
	at parquet.io.RecordReaderImplementation.&amp;lt;init&amp;gt;(RecordReaderImplementation.java:282)
	at parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:131)
	at parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:96)
	at parquet.filter2.compat.FilterCompat$NoOpFilter.accept(FilterCompat.java:136)
	at parquet.io.MessageColumnIO.getRecordReader(MessageColumnIO.java:96)
	at parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:126)
	at parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:193)
	... 27 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you take out the col_d from the thrift file, the problem goes away. The problem also shows up when trying to read the particular column without caching the table first. The same file can be dumped/read using parquet-tools just fine. Here is the file dump using parquet-tools:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;row group 0 
--------------------------------------------------------------------------------
col_a:           BINARY UNCOMPRESSED DO:0 FPO:4 SZ:89/89/1.00 VC:9 ENC [more]...
col_b:           BINARY UNCOMPRESSED DO:0 FPO:93 SZ:89/89/1.00 VC:9 EN [more]...
col_c:           BINARY UNCOMPRESSED DO:0 FPO:182 SZ:89/89/1.00 VC:9 E [more]...
col_d:          
.map:           
..key:           BINARY UNCOMPRESSED DO:0 FPO:271 SZ:29/29/1.00 VC:9 E [more]...
..value:        
...value_tuple:  BINARY UNCOMPRESSED DO:0 FPO:300 SZ:29/29/1.00 VC:9 E [more]...

    col_a TV=9 RL=0 DL=1
    ----------------------------------------------------------------------------
    page 0:  DLE:RLE RLE:BIT_PACKED VLE:PLAIN SZ:60 VC:9

    col_b TV=9 RL=0 DL=1
    ----------------------------------------------------------------------------
    page 0:  DLE:RLE RLE:BIT_PACKED VLE:PLAIN SZ:60 VC:9

    col_c TV=9 RL=0 DL=1
    ----------------------------------------------------------------------------
    page 0:  DLE:RLE RLE:BIT_PACKED VLE:PLAIN SZ:60 VC:9

    col_d.map.key TV=9 RL=1 DL=2
    ----------------------------------------------------------------------------
    page 0:  DLE:RLE RLE:RLE VLE:PLAIN SZ:12 VC:9

    col_d.map.value.value_tuple TV=9 RL=2 DL=4
    ----------------------------------------------------------------------------
    page 0:  DLE:RLE RLE:RLE VLE:PLAIN SZ:12 VC:9

BINARY col_a 
--------------------------------------------------------------------------------
*** row group 1 of 1, values 1 to 9 *** 
value 1: R:1 D:1 V:a1
value 2: R:1 D:1 V:a2
value 3: R:1 D:1 V:a3
value 4: R:1 D:1 V:a4
value 5: R:1 D:1 V:a5
value 6: R:1 D:1 V:a6
value 7: R:1 D:1 V:a7
value 8: R:1 D:1 V:a8
value 9: R:1 D:1 V:a9

BINARY col_b 
--------------------------------------------------------------------------------
*** row group 1 of 1, values 1 to 9 *** 
value 1: R:1 D:1 V:b1
value 2: R:1 D:1 V:b2
value 3: R:1 D:1 V:b3
value 4: R:1 D:1 V:b4
value 5: R:1 D:1 V:b5
value 6: R:1 D:1 V:b6
value 7: R:1 D:1 V:b7
value 8: R:1 D:1 V:b8
value 9: R:1 D:1 V:b9

BINARY col_c 
--------------------------------------------------------------------------------
*** row group 1 of 1, values 1 to 9 *** 
value 1: R:1 D:1 V:c1
value 2: R:1 D:1 V:c2
value 3: R:1 D:1 V:c3
value 4: R:1 D:1 V:c4
value 5: R:1 D:1 V:c5
value 6: R:1 D:1 V:c6
value 7: R:1 D:1 V:c7
value 8: R:1 D:1 V:c8
value 9: R:1 D:1 V:c9

BINARY col_d.map.key 
--------------------------------------------------------------------------------
*** row group 1 of 1, values 1 to 9 *** 
value 1: R:0 D:0 V:&amp;lt;&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;&amp;gt;
value 2: R:0 D:0 V:&amp;lt;&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;&amp;gt;
value 3: R:0 D:0 V:&amp;lt;&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;&amp;gt;
value 4: R:0 D:0 V:&amp;lt;&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;&amp;gt;
value 5: R:0 D:0 V:&amp;lt;&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;&amp;gt;
value 6: R:0 D:0 V:&amp;lt;&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;&amp;gt;
value 7: R:0 D:0 V:&amp;lt;&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;&amp;gt;
value 8: R:0 D:0 V:&amp;lt;&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;&amp;gt;
value 9: R:0 D:0 V:&amp;lt;&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;&amp;gt;

BINARY col_d.map.value.value_tuple 
--------------------------------------------------------------------------------
*** row group 1 of 1, values 1 to 9 *** 
value 1: R:0 D:0 V:&amp;lt;&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;&amp;gt;
value 2: R:0 D:0 V:&amp;lt;&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;&amp;gt;
value 3: R:0 D:0 V:&amp;lt;&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;&amp;gt;
value 4: R:0 D:0 V:&amp;lt;&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;&amp;gt;
value 5: R:0 D:0 V:&amp;lt;&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;&amp;gt;
value 6: R:0 D:0 V:&amp;lt;&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;&amp;gt;
value 7: R:0 D:0 V:&amp;lt;&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;&amp;gt;
value 8: R:0 D:0 V:&amp;lt;&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;&amp;gt;
value 9: R:0 D:0 V:&amp;lt;&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="12756808">SPARK-4520</key>
            <summary>SparkSQL exception when reading certain columns from a parquet file</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="sadhan">sadhan sood</assignee>
                                    <reporter username="sadhan">sadhan sood</reporter>
                        <labels>
                    </labels>
                <created>Thu, 20 Nov 2014 20:16:11 +0000</created>
                <updated>Wed, 3 Jun 2015 15:59:52 +0000</updated>
                            <resolved>Fri, 24 Apr 2015 17:59:17 +0000</resolved>
                                    <version>1.2.0</version>
                                    <fixVersion>1.3.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>7</watches>
                                                                                                                <comments>
                            <comment id="14219929" author="sadhan" created="Thu, 20 Nov 2014 20:17:48 +0000"  >&lt;p&gt;Attaching a sample parquet file which can be used to reproduce the issue with the given schema in the description.&lt;/p&gt;</comment>
                            <comment id="14254093" author="sadhan" created="Fri, 19 Dec 2014 22:04:38 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=marmbrus&quot; class=&quot;user-hover&quot; rel=&quot;marmbrus&quot;&gt;marmbrus&lt;/a&gt;, can you assign this to me - we fixed this issue internally and would be happy to submit a fix for it.&lt;/p&gt;</comment>
                            <comment id="14269900" author="alexlevenson" created="Thu, 8 Jan 2015 19:23:09 +0000"  >&lt;p&gt;Awesome! What did the issue turn out to be? Is it a parquet bug?&lt;/p&gt;</comment>
                            <comment id="14276196" author="trrichard" created="Tue, 13 Jan 2015 23:34:34 +0000"  >&lt;p&gt;I&apos;m also interested in the solution to this. I&apos;m having a similar problem with reading parquet data fields with repeating elements. Sadhan thanks for your research into this.&lt;/p&gt;

&lt;p&gt;parquet.io.ParquetDecodingException: Can not read value at 0 in block -1 in file &lt;a href=&quot;file:/tmp/parquetSample/committed_2015-01-13_15-03-40.915-08-00/part-m-00014.gz.parquet&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;file:/tmp/parquetSample/committed_2015-01-13_15-03-40.915-08-00/part-m-00014.gz.parquet&lt;/a&gt;&lt;br/&gt;
	at parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:213)&lt;br/&gt;
	at parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:204)&lt;br/&gt;
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.hasNext(NewHadoopRDD.scala:145)&lt;br/&gt;
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)&lt;br/&gt;
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)&lt;br/&gt;
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)&lt;br/&gt;
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)&lt;br/&gt;
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)&lt;br/&gt;
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)&lt;br/&gt;
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)&lt;br/&gt;
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)&lt;br/&gt;
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)&lt;br/&gt;
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)&lt;br/&gt;
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)&lt;br/&gt;
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)&lt;br/&gt;
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)&lt;br/&gt;
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)&lt;br/&gt;
	at org.apache.spark.rdd.RDD$$anonfun$16.apply(RDD.scala:780)&lt;br/&gt;
	at org.apache.spark.rdd.RDD$$anonfun$16.apply(RDD.scala:780)&lt;br/&gt;
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)&lt;br/&gt;
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)&lt;br/&gt;
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)&lt;br/&gt;
	at org.apache.spark.scheduler.Task.run(Task.scala:56)&lt;br/&gt;
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:724)&lt;br/&gt;
Caused by: java.lang.ArrayIndexOutOfBoundsException: -1&lt;br/&gt;
	at java.util.ArrayList.elementData(ArrayList.java:371)&lt;br/&gt;
	at java.util.ArrayList.get(ArrayList.java:384)&lt;br/&gt;
	at parquet.io.GroupColumnIO.getLast(GroupColumnIO.java:95)&lt;br/&gt;
	at parquet.io.GroupColumnIO.getLast(GroupColumnIO.java:95)&lt;br/&gt;
	at parquet.io.GroupColumnIO.getLast(GroupColumnIO.java:95)&lt;br/&gt;
	at parquet.io.GroupColumnIO.getLast(GroupColumnIO.java:95)&lt;br/&gt;
	at parquet.io.PrimitiveColumnIO.getLast(PrimitiveColumnIO.java:80)&lt;br/&gt;
	at parquet.io.PrimitiveColumnIO.isLast(PrimitiveColumnIO.java:74)&lt;br/&gt;
	at parquet.io.RecordReaderImplementation.&amp;lt;init&amp;gt;(RecordReaderImplementation.java:290)&lt;br/&gt;
	at parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:131)&lt;br/&gt;
	at parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:96)&lt;br/&gt;
	at parquet.filter2.compat.FilterCompat$NoOpFilter.accept(FilterCompat.java:136)&lt;br/&gt;
	at parquet.io.MessageColumnIO.getRecordReader(MessageColumnIO.java:96)&lt;br/&gt;
	at parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:126)&lt;br/&gt;
	at parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:193)&lt;br/&gt;
	... 26 more&lt;/p&gt;</comment>
                            <comment id="14276202" author="trrichard" created="Tue, 13 Jan 2015 23:36:51 +0000"  >&lt;p&gt;Schema:&lt;br/&gt;
requestedSchema: message root {&lt;br/&gt;
  required group key &lt;/p&gt;
{
    required int64 marketplaceId;
    required binary iaId (UTF8);
  }
&lt;p&gt;  required group value {&lt;br/&gt;
    required group time &lt;/p&gt;
{
      required int32 grain;
      required int64 milliseconds;
    }
&lt;p&gt;    required group series {&lt;br/&gt;
      required group values (LIST) &lt;/p&gt;
{
        repeated double array;
      }
&lt;p&gt;    }&lt;br/&gt;
  }&lt;br/&gt;
}&lt;/p&gt;


&lt;p&gt;Just like Sadhan I can read the data with parquet-tools but not with spark.&lt;/p&gt;</comment>
                            <comment id="14280698" author="sadhan" created="Fri, 16 Jan 2015 19:23:35 +0000"  >&lt;p&gt;Tyler, Alex - the problem is not with parquet but how we are reading the parquet columns.  Just wanted to make sure that you are seeing this problem with thrift generated parquet files as well? I am going to submit my fix this weekend now that I have some availability, my apologies for the delay.&lt;/p&gt;</comment>
                            <comment id="14280705" author="trrichard" created="Fri, 16 Jan 2015 19:29:42 +0000"  >&lt;p&gt;No rush. Just interested. I figured my problem was something along the lines of :&lt;br/&gt;
schema !=  my custom serializer != the spark deserializer&lt;/p&gt;

&lt;p&gt;But it looks like the problems may lie with the spark deserializer more than my own serialization.&lt;/p&gt;</comment>
                            <comment id="14286578" author="apachespark" created="Wed, 21 Jan 2015 23:51:15 +0000"  >&lt;p&gt;User &apos;sadhan&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/4148&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/4148&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14511436" author="yhuai" created="Fri, 24 Apr 2015 17:59:17 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lian+cheng&quot; class=&quot;user-hover&quot; rel=&quot;lian cheng&quot;&gt;lian cheng&lt;/a&gt; I am resolving this one.&lt;/p&gt;</comment>
                            <comment id="14571248" author="lian cheng" created="Wed, 3 Jun 2015 15:59:52 +0000"  >&lt;p&gt;This issue is related to Parquet backwards compatibility. The column name with &lt;tt&gt;_tuple&lt;/tt&gt; postfix is a historical issue of parquet-thrift. Related logic &lt;a href=&quot;https://github.com/apache/parquet-mr/blob/apache-parquet-1.7.0/parquet-thrift/src/main/java/org/apache/parquet/thrift/ThriftSchemaConvertVisitor.java#L114-L145&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;still exists&lt;/a&gt; in the most recent Parquet 1.7.0 release.&lt;/p&gt;

&lt;p&gt;The most recent Parquet format spec (not released yet up until writing) handles this situation via &lt;a href=&quot;https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#backward-compatibility-rules&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;LIST backwards compatibility rules&lt;/a&gt;. IIRC, at least these rules have been implemented properly in parquet-avro, not quite sure about situations of other Parquet submodules.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-6774&quot; title=&quot;Implement Parquet complex types backwards-compatiblity rules&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-6774&quot;&gt;&lt;del&gt;SPARK-6774&lt;/del&gt;&lt;/a&gt; aims to fix these stuff for Spark SQL Parquet support.&lt;/p&gt;

&lt;p&gt;Was reviewing Parquet backwards compatibility related issues. Just leave a comment here for future reference.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12682731" name="part-r-00000.parquet" size="2450" author="sadhan" created="Thu, 20 Nov 2014 20:17:48 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 24 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i22mgv:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12327642">1.3.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>