<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:55:21 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-17321] YARN shuffle service should use good disk from yarn.nodemanager.local-dirs</title>
                <link>https://issues.apache.org/jira/browse/SPARK-17321</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;We run spark on yarn, after enabled spark dynamic allocation, we notice some spark application failed randomly due to YarnShuffleService.&lt;br/&gt;
From log I found&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;2016-08-29 11:33:03,450 ERROR org.apache.spark.network.TransportContext: Error while initializing Netty pipeline&lt;br/&gt;
java.lang.NullPointerException&lt;br/&gt;
        at org.apache.spark.network.server.TransportRequestHandler.&amp;lt;init&amp;gt;(TransportRequestHandler.java:77)&lt;br/&gt;
        at org.apache.spark.network.TransportContext.createChannelHandler(TransportContext.java:159)&lt;br/&gt;
        at org.apache.spark.network.TransportContext.initializePipeline(TransportContext.java:135)&lt;br/&gt;
        at org.apache.spark.network.server.TransportServer$1.initChannel(TransportServer.java:123)&lt;br/&gt;
        at org.apache.spark.network.server.TransportServer$1.initChannel(TransportServer.java:116)&lt;br/&gt;
        at io.netty.channel.ChannelInitializer.channelRegistered(ChannelInitializer.java:69)&lt;br/&gt;
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRegistered(AbstractChannelHandlerContext.java:133)&lt;br/&gt;
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRegistered(AbstractChannelHandlerContext.java:119)&lt;br/&gt;
        at io.netty.channel.DefaultChannelPipeline.fireChannelRegistered(DefaultChannelPipeline.java:733)&lt;br/&gt;
        at io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:450)&lt;br/&gt;
        at io.netty.channel.AbstractChannel$AbstractUnsafe.access$100(AbstractChannel.java:378)&lt;br/&gt;
        at io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:424)&lt;br/&gt;
        at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)&lt;br/&gt;
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)&lt;br/&gt;
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:745)&lt;/p&gt;&lt;/blockquote&gt; 
&lt;p&gt;Which caused by the first disk in yarn.nodemanager.local-dirs was broken.&lt;br/&gt;
If we enabled spark.yarn.shuffle.stopOnFailure(&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-16505&quot; title=&quot;YARN shuffle service should throw errors when it fails to start&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-16505&quot;&gt;&lt;del&gt;SPARK-16505&lt;/del&gt;&lt;/a&gt;) we might lost hundred nodes which is unacceptable.&lt;br/&gt;
We have 12 disks in yarn.nodemanager.local-dirs, so why not use other good disks if the first one is broken?&lt;/p&gt;</description>
                <environment></environment>
        <key id="13001498">SPARK-17321</key>
            <summary>YARN shuffle service should use good disk from yarn.nodemanager.local-dirs</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="jerryshao">Saisai Shao</assignee>
                                    <reporter username="zhaoyunjiong">yunjiong zhao</reporter>
                        <labels>
                    </labels>
                <created>Tue, 30 Aug 2016 22:38:07 +0000</created>
                <updated>Sun, 17 May 2020 18:13:14 +0000</updated>
                            <resolved>Thu, 31 Aug 2017 01:27:22 +0000</resolved>
                                    <version>1.6.2</version>
                    <version>2.0.0</version>
                    <version>2.1.1</version>
                                    <fixVersion>2.3.0</fixVersion>
                                    <component>Spark Core</component>
                    <component>YARN</component>
                        <due></due>
                            <votes>2</votes>
                                    <watches>10</watches>
                                                                                                                <comments>
                            <comment id="15450424" author="apachespark" created="Tue, 30 Aug 2016 23:09:07 +0000"  >&lt;p&gt;User &apos;zhaoyunjiong&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/14887&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/14887&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15451742" author="srowen" created="Wed, 31 Aug 2016 09:37:53 +0000"  >&lt;p&gt;Duplicate of &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-14963&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/SPARK-14963&lt;/a&gt; ?&lt;/p&gt;</comment>
                            <comment id="15451925" author="zhaoyunjiong" created="Wed, 31 Aug 2016 11:05:19 +0000"  >&lt;p&gt;Below logs shows that NodeManager already detect the disk failure, but ExternalShuffleBlockResolver still use the failure disk.&lt;/p&gt;

&lt;p&gt;2016-08-30 10:16:24,982 INFO org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService: Disk(s) failed. 3/12 local-dirs turned bad: /hadoop/1/scratch/local,/hadoop/7/scratch/local,/hadoop/10/scratch/local;3/12 log-dirs turned bad: /hadoop/1/scratch/logs,/hadoop/7/scratch/logs,/hadoop/10/scratch/logs&lt;br/&gt;
...&lt;br/&gt;
2016-08-30 10:16:38,008 INFO org.apache.spark.network.yarn.YarnShuffleService: Initializing YARN shuffle service for Spark&lt;br/&gt;
2016-08-30 10:16:38,008 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Adding auxiliary service spark_shuffle, &quot;spark_shuffle&quot;&lt;br/&gt;
2016-08-30 10:16:38,260 ERROR org.apache.spark.network.shuffle.ExternalShuffleBlockResolver: error opening leveldb file /hadoop/1/scratch/local/registeredExecutors.ldb. Creating new file, will not be able to recover state for existing applications&lt;br/&gt;
org.fusesource.leveldbjni.internal.NativeDB$DBException: IO error: /hadoop/1/scratch/local/registeredExecutors.ldb/LOCK: No such file or directory&lt;br/&gt;
at org.fusesource.leveldbjni.internal.NativeDB.checkStatus(NativeDB.java:200)&lt;br/&gt;
at org.fusesource.leveldbjni.internal.NativeDB.open(NativeDB.java:218)&lt;br/&gt;
at org.fusesource.leveldbjni.JniDBFactory.open(JniDBFactory.java:168)&lt;br/&gt;
at org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.(ExternalShuffleBlockResolver.java:100)&lt;br/&gt;
at org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.(ExternalShuffleBlockResolver.java:81)&lt;br/&gt;
at org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.(ExternalShuffleBlockHandler.java:56)&lt;br/&gt;
at org.apache.spark.network.yarn.YarnShuffleService.serviceInit(YarnShuffleService.java:129)&lt;br/&gt;
at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)&lt;br/&gt;
at org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices.serviceInit(AuxServices.java:122)&lt;br/&gt;
at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)&lt;br/&gt;
at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)&lt;br/&gt;
at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:220)&lt;br/&gt;
at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)&lt;br/&gt;
at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)&lt;br/&gt;
at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:186)&lt;br/&gt;
at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)&lt;br/&gt;
at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:357)&lt;br/&gt;
at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:404)&lt;br/&gt;
2016-08-30 10:16:38,262 WARN org.apache.spark.network.shuffle.ExternalShuffleBlockResolver: error deleting /hadoop/1/scratch/local/registeredExecutors.ldb&lt;br/&gt;
2016-08-30 10:16:38,262 ERROR org.apache.spark.network.yarn.YarnShuffleService: Failed to initialize external shuffle service&lt;br/&gt;
java.io.IOException: Unable to create state store&lt;br/&gt;
at org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.(ExternalShuffleBlockResolver.java:129)&lt;br/&gt;
at org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.(ExternalShuffleBlockResolver.java:81)&lt;br/&gt;
at org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.(ExternalShuffleBlockHandler.java:56)&lt;br/&gt;
at org.apache.spark.network.yarn.YarnShuffleService.serviceInit(YarnShuffleService.java:129)&lt;br/&gt;
at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)&lt;br/&gt;
at org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices.serviceInit(AuxServices.java:122)&lt;br/&gt;
at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)&lt;br/&gt;
at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)&lt;br/&gt;
at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:220)&lt;br/&gt;
at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)&lt;br/&gt;
at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)&lt;br/&gt;
at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:186)&lt;br/&gt;
at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)&lt;br/&gt;
at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:357)&lt;br/&gt;
at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:404)&lt;br/&gt;
Caused by: org.fusesource.leveldbjni.internal.NativeDB$DBException: IO error: /hadoop/1/scratch/local/registeredExecutors.ldb/LOCK: No such file or directory&lt;br/&gt;
at org.fusesource.leveldbjni.internal.NativeDB.checkStatus(NativeDB.java:200)&lt;br/&gt;
at org.fusesource.leveldbjni.internal.NativeDB.open(NativeDB.java:218)&lt;br/&gt;
at org.fusesource.leveldbjni.JniDBFactory.open(JniDBFactory.java:168)&lt;br/&gt;
at org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.(ExternalShuffleBlockResolver.java:127)&lt;br/&gt;
... 14 more&lt;br/&gt;
2016-08-30 10:16:38,456 INFO org.apache.spark.network.yarn.YarnShuffleService: Started YARN shuffle service for Spark on port 7337. Authentication is not enabled. Registered executor file is /hadoop/1/scratch/local/registeredExecutors.ldb&lt;/p&gt;</comment>
                            <comment id="15451942" author="zhaoyunjiong" created="Wed, 31 Aug 2016 11:13:37 +0000"  >&lt;p&gt;If yarn.nodemanager.recovery.enabled = false &amp;amp; the first disk was broken, yarn shuffle service still will fail.&lt;/p&gt;</comment>
                            <comment id="15474342" author="anlei" created="Thu, 8 Sep 2016 16:34:21 +0000"  >&lt;p&gt;We discovered the same issue. It seems the shuffle service has no way of being notified about the failed disk. Only option then is probably to restart the node manager.&lt;/p&gt;</comment>
                            <comment id="15474557" author="tgraves" created="Thu, 8 Sep 2016 17:53:09 +0000"  >&lt;p&gt;so there are 2 possible things here:&lt;/p&gt;

&lt;p&gt;1) You are using YARN NM recovery.  If this is the case &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-14963&quot; title=&quot;YarnShuffleService should use YARN getRecoveryPath() for leveldb location&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-14963&quot;&gt;&lt;del&gt;SPARK-14963&lt;/del&gt;&lt;/a&gt; should prevent this problem as the recovery path is supposed to be critical to NM and NM should not start if its bad&lt;/p&gt;

&lt;p&gt;2) You aren&apos;t using NM recovery. If this is the case then you probably don&apos;t really care about the levelDB being saved because you aren&apos;t expecting things to live across Nm restarts.  In this case if people are having issues like this perhaps we should change the code to be conditionalized on NM recovery or a spark config.&lt;/p&gt;

&lt;p&gt;Which case are you running?&lt;/p&gt;</comment>
                            <comment id="15476355" author="anlei" created="Fri, 9 Sep 2016 08:39:39 +0000"  >&lt;p&gt;But NM recovery only kicks in if the NM goes down, right? As I see it, a failing disk will not necessarily trigger a NM restart, but the NM will still know that the disk is not usable and keep working. At this point you can have a working NM with a non-working shuffle service which tries to access its ldb file on a broken disk. Or am I missing something here?&lt;/p&gt;</comment>
                            <comment id="15477028" author="tgraves" created="Fri, 9 Sep 2016 13:12:39 +0000"  >&lt;p&gt;it is possible but the point of the recovery dir is it something that is supposed to be critical to the NM or its supposed to be special storage to handle that situation.  NM writes its recovery data there too in addition to the shuffle services so if it goes bad more then likely the nm is going to get the same exception and crash.&lt;/p&gt;</comment>
                            <comment id="15477061" author="tgraves" created="Fri, 9 Sep 2016 13:26:36 +0000"  >&lt;p&gt;Note that if we want to fix this without yarn NM recovery enabled I think we should do as mentioned in the Pr and in this jira above.  change the shuffle handler so that if nm recovery/spark config aren&apos;t enabled then don&apos;t save the data to the DB at all. Then it doesn&apos;t rely on the disk at all.&lt;/p&gt;</comment>
                            <comment id="15483413" author="anlei" created="Mon, 12 Sep 2016 07:56:17 +0000"  >&lt;p&gt;I guess then we encountered the 1% where the NM handles the broken disk just fine, but the shuffle service did not.&lt;/p&gt;</comment>
                            <comment id="15484748" author="tgraves" created="Mon, 12 Sep 2016 17:42:27 +0000"  >&lt;p&gt;Not sure I follow this comment.  So you are using NM recovery with the recovery path specified?    &lt;br/&gt;
And you saw an error in the spark shuffle creating or writing to the DB but the NM stayed up ok writing its recovery data to the same disk?&lt;/p&gt;</comment>
                            <comment id="15487166" author="anlei" created="Tue, 13 Sep 2016 13:10:24 +0000"  >&lt;p&gt;No, we&apos;re not using NM recovery. What we observed is the following:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;NM runs with a list of local dirs on various disks&lt;/li&gt;
	&lt;li&gt;Shuffle service puts its data into one of these local dirs&lt;/li&gt;
	&lt;li&gt;One disk fails making that local dir unuseable, which is incidentally the one where shuffle service put its data&lt;/li&gt;
	&lt;li&gt;NM recognizes the disk failure but keeps running happily (did not have any critical data there? no idea...)&lt;/li&gt;
	&lt;li&gt;Shuffle service can&apos;t access its data and is not working anymore on this node&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;So in essence, the NM has some way of recognizing that a local dir is not useable anymore and can keep operating. The shuffle service lacks this functionality and becomes unuseable. Does that make sense?&lt;/p&gt;</comment>
                            <comment id="15487203" author="tgraves" created="Tue, 13 Sep 2016 13:31:06 +0000"  >&lt;p&gt;yes that makes sense and as I stated I think the fix for this should be that the Spark shuffle services doesn&apos;t use the backup database at all if NM recovery (and spark config) aren&apos;t enabled.  Thus you wouldn&apos;t have any disk errors.  If NM recovery isn&apos;t enabled the spark DB isn&apos;t going to do you any good because NM is going to shoot any running containers on restart.&lt;/p&gt;

&lt;p&gt;If you are up for making those changes please go ahead and put up patch.&lt;/p&gt;</comment>
                            <comment id="16104373" author="roncenzhao" created="Fri, 28 Jul 2017 03:05:08 +0000"  >&lt;p&gt;Hi, I encounter this problem too. Any process about this bug? Thanks~&lt;/p&gt;</comment>
                            <comment id="16105164" author="tgraves" created="Fri, 28 Jul 2017 16:04:18 +0000"  >&lt;p&gt;Can you clarify?   as stated above you should not be using nodemanager.local-dirs.  If you are you should look at reconfiguring yarn to use the proper NM recovery dirs.  see &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-14963&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/SPARK-14963&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;if you aren&apos;t using NM recovery then yes we should fix this so spark doesn&apos;t use backup db at all.&lt;/p&gt;
</comment>
                            <comment id="16138320" author="jerryshao" created="Wed, 23 Aug 2017 13:12:58 +0000"  >&lt;p&gt;We&apos;re facing the same issue. I think YARN shuffle service should be like:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;If NM recovery is not enabled, then Spark will not persist data into leveldb, in that case yarn shuffle service can still be served but lose the ability for recovery, (it is fine because the failure of NM will kill the containers as well as applications).&lt;/li&gt;
	&lt;li&gt;If NM recovery is enabled, then user or yarn should guarantee recovery path is reliable. Because recovery path is also crucial for NM to recover.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;What do you think &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tgraves&quot; class=&quot;user-hover&quot; rel=&quot;tgraves&quot;&gt;tgraves&lt;/a&gt; ? &lt;/p&gt;

&lt;p&gt;I&apos;m currently working on the 1st thing to avoid persisting data into leveldb, to see if this is a feasible solution.&lt;/p&gt;</comment>
                            <comment id="16138347" author="tgraves" created="Wed, 23 Aug 2017 13:36:31 +0000"  >&lt;p&gt;Yes that sounds good.  It wouldn&apos;t hurt to verify the second point. The NM should throw an exception on container launch because it itself can&apos;t record the container start thus can&apos;t recover.&lt;/p&gt;</comment>
                            <comment id="16139488" author="lishuming" created="Thu, 24 Aug 2017 02:55:04 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jerryshao&quot; class=&quot;user-hover&quot; rel=&quot;jerryshao&quot;&gt;jerryshao&lt;/a&gt; I agree with what you said, however there are some questions in my mind:&lt;/p&gt;

&lt;p&gt;1. The current chosen strategy is puzzling somehow, because both `yarn.nodemanager.local-dirs` and `NM recovery path` are available to choose to store leveldb now, so we can always pick an available disk to store, avoiding the disk problem(&lt;a href=&quot;https://github.com/apache/spark/pull/18905#issuecomment-323287272&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/18905#issuecomment-323287272&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;2. If as &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jerryshao&quot; class=&quot;user-hover&quot; rel=&quot;jerryshao&quot;&gt;jerryshao&lt;/a&gt; said, `yarn.nodemanager.local-dirs` should not be used whenever NM recovery is enabled or not, am I right ?&lt;/p&gt;

&lt;p&gt;3. Can someone check that If we don&apos;t use leveldb, ShuffleService which uses `Map` will affect NM&apos;s memory or something else?&lt;/p&gt;</comment>
                            <comment id="16139502" author="jerryshao" created="Thu, 24 Aug 2017 03:18:24 +0000"  >&lt;p&gt;1. if NM recovery is enabled, then yarn will provide a recovery path, this recovery path will be used for any aux-service running on yarn (tez, mr, spark...) and NM itself to store state. So user/yarn should guarantee the availability of this path, if not then NM itself will be failed to restart. So as a conclusion if NM recovery is enabled, then we should always use recovery path.&lt;/p&gt;

&lt;p&gt;2. Yes we will never use NM local dirs whether NM recovery is enabled or not. Previously we need to support Hadoop 2.6- which has no recovery path, so we choose a local dir instead. Since now we only support 2.6+, so there&apos;s no meaning to still use NM local dir.&lt;/p&gt;

&lt;p&gt;3. The memory overhead should not be large, since it only stores some application/executor information. Also when you use external shuffle service in standalone and Mesos, it always use memory, so I don&apos;t think it is a big problem.&lt;/p&gt;</comment>
                            <comment id="16142227" author="apachespark" created="Fri, 25 Aug 2017 21:26:09 +0000"  >&lt;p&gt;User &apos;jerryshao&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/19032&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/19032&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16142229" author="apachespark" created="Fri, 25 Aug 2017 21:26:37 +0000"  >&lt;p&gt;User &apos;jerryshao&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/19032&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/19032&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16148318" author="jerryshao" created="Thu, 31 Aug 2017 01:27:22 +0000"  >&lt;p&gt;Issue resolved by pull request 19032&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/19032&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/19032&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="13093069">SPARK-21660</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 11 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i331fj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>