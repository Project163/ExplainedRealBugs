<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:40:27 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-13710] Spark shell shows ERROR when launching on Windows</title>
                <link>https://issues.apache.org/jira/browse/SPARK-13710</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;On Windows, when we launch &lt;tt&gt;bin\spark-shell.cmd&lt;/tt&gt;, it shows ERROR message and stacktrace.&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;C:\Users\tsudukim\Documents\workspace\spark-dev3&amp;gt;bin\spark-shell
[ERROR] Terminal initialization failed; falling back to unsupported
java.lang.NoClassDefFoundError: Could not initialize class scala.tools.fusesource_embedded.jansi.internal.Kernel32
        at scala.tools.fusesource_embedded.jansi.internal.WindowsSupport.getConsoleMode(WindowsSupport.java:50)
        at scala.tools.jline_embedded.WindowsTerminal.getConsoleMode(WindowsTerminal.java:204)
        at scala.tools.jline_embedded.WindowsTerminal.init(WindowsTerminal.java:82)
        at scala.tools.jline_embedded.TerminalFactory.create(TerminalFactory.java:101)
        at scala.tools.jline_embedded.TerminalFactory.get(TerminalFactory.java:158)
        at scala.tools.jline_embedded.console.ConsoleReader.&amp;lt;init&amp;gt;(ConsoleReader.java:229)
        at scala.tools.jline_embedded.console.ConsoleReader.&amp;lt;init&amp;gt;(ConsoleReader.java:221)
        at scala.tools.jline_embedded.console.ConsoleReader.&amp;lt;init&amp;gt;(ConsoleReader.java:209)
        at scala.tools.nsc.interpreter.jline_embedded.JLineConsoleReader.&amp;lt;init&amp;gt;(JLineReader.scala:61)
        at scala.tools.nsc.interpreter.jline_embedded.InteractiveReader.&amp;lt;init&amp;gt;(JLineReader.scala:33)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
        at scala.tools.nsc.interpreter.ILoop$$anonfun$scala$tools$nsc$interpreter$ILoop$$instantiate$1$1.apply(ILoop.scala:865)
        at scala.tools.nsc.interpreter.ILoop$$anonfun$scala$tools$nsc$interpreter$ILoop$$instantiate$1$1.apply(ILoop.scala:862)
        at scala.tools.nsc.interpreter.ILoop.scala$tools$nsc$interpreter$ILoop$$mkReader$1(ILoop.scala:871)
        at scala.tools.nsc.interpreter.ILoop$$anonfun$15$$anonfun$apply$8.apply(ILoop.scala:875)
        at scala.tools.nsc.interpreter.ILoop$$anonfun$15$$anonfun$apply$8.apply(ILoop.scala:875)
        at scala.util.Try$.apply(Try.scala:192)
        at scala.tools.nsc.interpreter.ILoop$$anonfun$15.apply(ILoop.scala:875)
        at scala.tools.nsc.interpreter.ILoop$$anonfun$15.apply(ILoop.scala:875)
        at scala.collection.immutable.Stream$$anonfun$map$1.apply(Stream.scala:418)
        at scala.collection.immutable.Stream$$anonfun$map$1.apply(Stream.scala:418)
        at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)
        at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)
        at scala.collection.immutable.Stream.collect(Stream.scala:435)
        at scala.tools.nsc.interpreter.ILoop.chooseReader(ILoop.scala:877)
        at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$2.apply(ILoop.scala:916)
        at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply$mcZ$sp(ILoop.scala:916)
        at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:911)
        at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:911)
        at scala.reflect.internal.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:97)
        at scala.tools.nsc.interpreter.ILoop.process(ILoop.scala:911)
        at org.apache.spark.repl.Main$.doMain(Main.scala:64)
        at org.apache.spark.repl.Main$.main(Main.scala:47)
        at org.apache.spark.repl.Main.main(Main.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:737)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:183)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:208)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:122)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

Using Spark&apos;s default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to &quot;WARN&quot;.
To adjust logging level use sc.setLogLevel(newLevel).
16/03/07 13:05:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Spark context available as sc (master = local[*], app id = local-1457323533704).
SQL context available as sqlContext.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  &apos;_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.0.0-SNAPSHOT
      /_/

Using Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)
Type in expressions to have them evaluated.
Type :help for more information.

scala&amp;gt; sc.textFile(&quot;README.md&quot;)
res0: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[1] at textFile at &amp;lt;console&amp;gt;:25

scala&amp;gt; sc.textFile(&quot;README.md&quot;).count()
res1: Long = 97
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Spark-shell itself seems to work file during my simple operation check.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12947607">SPARK-13710</key>
            <summary>Spark shell shows ERROR when launching on Windows</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Minor</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="FlamingMike">Michel Lemay</assignee>
                                    <reporter username="tsudukim">Masayoshi Tsuzuki</reporter>
                        <labels>
                    </labels>
                <created>Mon, 7 Mar 2016 04:19:18 +0000</created>
                <updated>Thu, 4 Aug 2016 03:52:41 +0000</updated>
                            <resolved>Thu, 31 Mar 2016 19:14:28 +0000</resolved>
                                                    <fixVersion>2.0.0</fixVersion>
                                    <component>Spark Shell</component>
                    <component>Windows</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="15182565" author="tsudukim" created="Mon, 7 Mar 2016 04:22:21 +0000"  >&lt;p&gt;It shows the similar ERROR message and stacktrace also when exit from spark shell.&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;scala&amp;gt; :quit
16/03/07 13:06:31 INFO SparkUI: Stopped Spark web UI at http://192.168.33.129:4040
16/03/07 13:06:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/03/07 13:06:31 INFO MemoryStore: MemoryStore cleared
16/03/07 13:06:31 INFO BlockManager: BlockManager stopped
16/03/07 13:06:31 INFO BlockManagerMaster: BlockManagerMaster stopped
16/03/07 13:06:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/03/07 13:06:31 INFO SparkContext: Successfully stopped SparkContext
[WARN] Task failed
java.lang.NoClassDefFoundError: Could not initialize class scala.tools.fusesource_embedded.jansi.internal.Kernel32
        at scala.tools.fusesource_embedded.jansi.internal.WindowsSupport.setConsoleMode(WindowsSupport.java:60)
        at scala.tools.jline_embedded.WindowsTerminal.setConsoleMode(WindowsTerminal.java:208)
        at scala.tools.jline_embedded.WindowsTerminal.restore(WindowsTerminal.java:95)
        at scala.tools.jline_embedded.TerminalSupport$1.run(TerminalSupport.java:52)
        at scala.tools.jline_embedded.internal.ShutdownHooks.runTasks(ShutdownHooks.java:66)
        at scala.tools.jline_embedded.internal.ShutdownHooks.access$000(ShutdownHooks.java:22)
        at scala.tools.jline_embedded.internal.ShutdownHooks$1.run(ShutdownHooks.java:47)

16/03/07 13:06:31 INFO ShutdownHookManager: Shutdown hook called
16/03/07 13:06:31 INFO ShutdownHookManager: Deleting directory C:\Users\tsudukim\AppData\Local\Temp\spark-d9077a51-fc78-4852-ad45-2b7085d72940\repl-f753505f-69cf-4593-bb21-f5aa2683bcca
16/03/07 13:06:31 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\tsudukim\AppData\Local\Temp\spark-d9077a51-fc78-4852-ad45-2b7085d72940\repl-f753505f-69cf-4593-bb21-f5aa2683bcca
java.io.IOException: Failed to delete: C:\Users\tsudukim\AppData\Local\Temp\spark-d9077a51-fc78-4852-ad45-2b7085d72940\repl-f753505f-69cf-4593-bb21-f5aa2683bcca
        at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:935)
        at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:64)
        at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:61)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
        at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:61)
        at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:217)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:189)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:189)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:189)
        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1788)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:189)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:189)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:189)
        at scala.util.Try$.apply(Try.scala:192)
        at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:189)
        at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:179)
        at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
16/03/07 13:06:31 INFO ShutdownHookManager: Deleting directory C:\Users\tsudukim\AppData\Local\Temp\spark-d9077a51-fc78-4852-ad45-2b7085d72940
16/03/07 13:06:31 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\tsudukim\AppData\Local\Temp\spark-d9077a51-fc78-4852-ad45-2b7085d72940
java.io.IOException: Failed to delete: C:\Users\tsudukim\AppData\Local\Temp\spark-d9077a51-fc78-4852-ad45-2b7085d72940
        at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:935)
        at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:64)
        at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:61)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
        at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:61)
        at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:217)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:189)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:189)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:189)
        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1788)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:189)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:189)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:189)
        at scala.util.Try$.apply(Try.scala:192)
        at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:189)
        at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:179)
        at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15208529" author="flamingmike" created="Wed, 23 Mar 2016 14:54:55 +0000"  >&lt;p&gt;I&apos;m having the same issue. It&apos;s worse than that though..&lt;/p&gt;

&lt;p&gt;Pasting code in the shell does random things like the following:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;scala&amp;gt; (1 to 10).foreach(x =&amp;gt; {
     |     println(x)
     | })
&amp;lt;console&amp;gt;:3: error: &lt;span class=&quot;code-quote&quot;&gt;&apos;;&apos;&lt;/span&gt; expected but &lt;span class=&quot;code-quote&quot;&gt;&apos;)&apos;&lt;/span&gt; found.
j)
 ^
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note how there is a magic &lt;b&gt;&lt;em&gt;j&lt;/em&gt;&lt;/b&gt; inserted in the code evaluated by the shell that wasn&apos;t present in pasted data!!&lt;/p&gt;</comment>
                            <comment id="15208720" author="srowen" created="Wed, 23 Mar 2016 16:36:44 +0000"  >&lt;p&gt;Something is clearly funky with jline here. It didn&apos;t init and who knows what that means about how lines are parsed in the shell. I don&apos;t know of any resolution; looks like a jline + Windows thing.&lt;/p&gt;</comment>
                            <comment id="15208849" author="flamingmike" created="Wed, 23 Mar 2016 17:44:18 +0000"  >&lt;p&gt;Googling around, it might be a conflict between jline2 and jline 0.9x&lt;/p&gt;

&lt;p&gt;Scala 2.10.x was compiling a local copy of jline and as of 2.11.x, is using jline 2.12.1&lt;/p&gt;

&lt;p&gt;Spark dependency tree shows: &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;[INFO] +- org.apache.curator:curator-recipes:jar:2.4.0:compile
[INFO] |  +- org.apache.curator:curator-framework:jar:2.4.0:compile
[INFO] |  |  \- org.apache.curator:curator-client:jar:2.4.0:compile
[INFO] |  \- org.apache.zookeeper:zookeeper:jar:3.4.5:compile
[INFO] |     \- jline:jline:jar:0.9.94:compile
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15208949" author="flamingmike" created="Wed, 23 Mar 2016 18:54:53 +0000"  >&lt;p&gt;I&apos;m not sure it broke something but seems to be working on my local build using a modified apache curator dependency like this:  &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;      &amp;lt;dependency&amp;gt;
        &amp;lt;groupId&amp;gt;org.apache.curator&amp;lt;/groupId&amp;gt;
        &amp;lt;artifactId&amp;gt;curator-recipes&amp;lt;/artifactId&amp;gt;
        &amp;lt;version&amp;gt;${curator.version}&amp;lt;/version&amp;gt;
        &amp;lt;scope&amp;gt;${hadoop.deps.scope}&amp;lt;/scope&amp;gt;
        &amp;lt;exclusions&amp;gt;
          &amp;lt;exclusion&amp;gt;
            &amp;lt;groupId&amp;gt;org.jboss.netty&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;netty&amp;lt;/artifactId&amp;gt;
          &amp;lt;/exclusion&amp;gt;
          &amp;lt;exclusion&amp;gt;
            &amp;lt;groupId&amp;gt;jline&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;jline&amp;lt;/artifactId&amp;gt;
          &amp;lt;/exclusion&amp;gt;
      &amp;lt;/exclusions&amp;gt;
      &amp;lt;/dependency&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I only excluded jline.&lt;/p&gt;</comment>
                            <comment id="15209114" author="srowen" created="Wed, 23 Mar 2016 20:34:22 +0000"  >&lt;p&gt;I agree that&apos;s a likely explanation. Maybe the conflict only surfaces in Windows. Curator is still used by some ZK-using parts of the code like the standalone Master, but, I can&apos;t quite figure out why it would need jline. It may only be needed if, say, you&apos;re running its command line tools. This exclusion may be a good idea then.&lt;/p&gt;

&lt;p&gt;I wonder if this has consequences for the 2.10 build &amp;#8211; I&apos;d suspect less of a problem if it was using the old jline?&lt;/p&gt;</comment>
                            <comment id="15209127" author="flamingmike" created="Wed, 23 Mar 2016 20:43:07 +0000"  >&lt;p&gt;Looks like it was renamed to a different package.  Therefore no conflict possible.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt; scala.tools.jline
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15209134" author="srowen" created="Wed, 23 Mar 2016 20:47:37 +0000"  >&lt;p&gt;Hm. The maven coordinates didn&apos;t change. I wonder if there is somehow still an issue where the new version contains code in the old namespace for backwards compatibility or whatever. That is I wonder why the exclusion above fixed it for you?&lt;/p&gt;</comment>
                            <comment id="15209484" author="flamingmike" created="Thu, 24 Mar 2016 00:17:36 +0000"  >&lt;p&gt;Spark coordinates for curator did not change..  It still depends on Curator 2.4.0 that later depends on ZK and jline 0.9.94.&lt;/p&gt;

&lt;p&gt;The problem is in Scala itself. It went from a local copy of jline sources in 2.10.x: &lt;a href=&quot;https://github.com/scala/scala/tree/2.10.x/src/jline&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/scala/scala/tree/2.10.x/src/jline&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;to a standard maven import in 2.11.x:&lt;br/&gt;
&lt;a href=&quot;https://github.com/scala/scala/blob/2.11.x/build.sbt#L74&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/scala/scala/blob/2.11.x/build.sbt#L74&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;However, I&apos;m not sure exactly how it plays out at runtime but I guess it&apos;s something in the lines of:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Spark jar is loaded in the JVM&lt;/li&gt;
	&lt;li&gt;Scala runtime is loaded in the JVM but jline resources are discarded because one copy is already there (this part wasn&apos;t happening in scala 2.10.x since resources were names or located somewhere else)&lt;/li&gt;
	&lt;li&gt;Spark tries to initialize the scala REPL but that fails because it loads the wrong jline/jansi&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Since I&apos;m no expert in JVM classloading and the likes so your guess is probably better than mine..&lt;/p&gt;</comment>
                            <comment id="15210120" author="srowen" created="Thu, 24 Mar 2016 11:48:52 +0000"  >&lt;p&gt;I think your exclusion is legitimate for curator-recipes. It shouldn&apos;t affect 2.10 in any event because it embeds its own copy. For 2.11, we&apos;re going to have conflicts if the assembly somehow resolves jline 0.9.x, which can happen with the closest-first dependency rules. Feel free to make a PR.&lt;/p&gt;</comment>
                            <comment id="15211963" author="flamingmike" created="Fri, 25 Mar 2016 15:52:38 +0000"  >&lt;p&gt;I&apos;d gladly do that but the current state of the master branch is somewhat unstable..  Tens of tests fail OOTB from my vanilla fork before any modifications! Is that expected?&lt;/p&gt;</comment>
                            <comment id="15212029" author="srowen" created="Fri, 25 Mar 2016 16:32:26 +0000"  >&lt;p&gt;Although a couple of the thousands of tests are flaky and spurious failures are fairly common, I don&apos;t think you&apos;d normally get lots of failures. It may be due to something more fundamental about the env or what you&apos;re building. You can post some errors here for a look. You can try opening a &lt;span class=&quot;error&quot;&gt;&amp;#91;WIP&amp;#93;&lt;/span&gt; PR to see how it tests versus Jenkins too .&lt;/p&gt;</comment>
                            <comment id="15216659" author="apachespark" created="Tue, 29 Mar 2016 19:08:04 +0000"  >&lt;p&gt;User &apos;michellemay&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/12043&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/12043&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15218405" author="flamingmike" created="Wed, 30 Mar 2016 17:31:34 +0000"  >&lt;p&gt;Tests passes on Jenkins which is a good thing..  However, my local run did not bode so well:&lt;/p&gt;

&lt;p&gt;&amp;gt; Tests: succeeded 1402, failed 23, canceled 0, ignored 12, pending 0&lt;br/&gt;
Failed tests:&lt;br/&gt;
   org.apache.spark.sql.SQLQuerySuite&lt;br/&gt;
   org.apache.spark.sql.DateFunctionsSuite&lt;br/&gt;
   org.apache.spark.sql.jdbc.JDBCSuite&lt;br/&gt;
   org.apache.spark.sql.sources.TableScanSuite&lt;br/&gt;
   org.apache.spark.sql.ScalaReflectionRelationSuite&lt;br/&gt;
   org.apache.spark.sql.execution.datasources.json.JsonSuite&lt;br/&gt;
   org.apache.spark.sql.execution.datasources.parquet.ParquetPartitionDiscoverySuite&lt;/p&gt;

&lt;p&gt;I&apos;m running on a standard AWS linux instance (r3.2xlarge), with Java 1.8 with latest master branch..  What should I do with that ?&lt;/p&gt;</comment>
                            <comment id="15220467" author="srowen" created="Thu, 31 Mar 2016 19:00:27 +0000"  >&lt;p&gt;It depends on why it fails,  from the error logs earlier. I would guess there is some common cause, and it&apos;s something to do with the env, but don&apos;t know what.&lt;/p&gt;</comment>
                            <comment id="15220499" author="srowen" created="Thu, 31 Mar 2016 19:14:28 +0000"  >&lt;p&gt;Issue resolved by pull request 12043&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/12043&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/12043&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15406744" author="arsenvlad" created="Wed, 3 Aug 2016 22:37:40 +0000"  >&lt;p&gt;I noticed that when using the binary package  &quot;spark-2.0.0-bin-without-hadoop.tgz&quot; (i.e. with user-provided Hadoop pointed to via &quot;export SPARK_DIST_CLASSPATH=$(hadoop classpath)&quot;) the same error still happens.&lt;/p&gt;

&lt;p&gt;java.lang.NoClassDefFoundError: Could not initialize class scala.tools.fusesource_embedded.jansi.internal.Kernel32&lt;br/&gt;
        at scala.tools.fusesource_embedded.jansi.internal.WindowsSupport.getConsoleMode(WindowsSupport.java:50)&lt;/p&gt;

&lt;p&gt;I compared the jars provided with spark-2.0.0-bin-with-hadoop-2.7 to the ones provided with spark-2.0.0-bin-without-hadoop and noticed that jline-2.12.jar is present in the &quot;with-hadoop&quot; but is missing from the &quot;without-hadoop&quot; binary package.&lt;/p&gt;

&lt;p&gt;When I copy the jline-2.12.jar to the jars folder of &quot;withou-hadoop&quot;, I can start bin\spark-shell  without getting this error.&lt;/p&gt;

&lt;p&gt;Is there a reason jline-2.12.jar is not part of the &quot;without-hadoop&quot; package?&lt;/p&gt;</comment>
                            <comment id="15407096" author="srowen" created="Thu, 4 Aug 2016 03:52:41 +0000"  >&lt;p&gt;Possibly related to &lt;a href=&quot;https://github.com/apache/spark/commit/4775eb414fa8285cfdc301e52dac52a2ef64c9e1&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/commit/4775eb414fa8285cfdc301e52dac52a2ef64c9e1&lt;/a&gt; / &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-16770&quot; title=&quot;Spark shell not usable with german keyboard due to JLine version&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-16770&quot;&gt;&lt;del&gt;SPARK-16770&lt;/del&gt;&lt;/a&gt; ?&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 15 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2u9k7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>