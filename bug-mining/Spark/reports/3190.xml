<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 02:11:20 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92">
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-13747] Concurrent execution in SQL doesn't work with Scala ForkJoinPool</title>
                <link>https://issues.apache.org/jira/browse/SPARK-13747</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Run the following codes may fail&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;(1 to 100).par.foreach { _ =&amp;gt;
  println(sc.parallelize(1 to 5).map { i =&amp;gt; (i, i) }.toDF(&lt;span class="code-quote"&gt;"a"&lt;/span&gt;, &lt;span class="code-quote"&gt;"b"&lt;/span&gt;).count())
}

java.lang.IllegalArgumentException: spark.sql.execution.id is already set 
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:87) 
        at org.apache.spark.sql.DataFrame.withNewExecutionId(DataFrame.scala:1904) 
        at org.apache.spark.sql.DataFrame.collect(DataFrame.scala:1385) 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is because SparkContext.runJob can be suspended when using a ForkJoinPool (e.g.,scala.concurrent.ExecutionContext.Implicits.global) as it calls Await.ready (introduced by &lt;a href="https://github.com/apache/spark/pull/9264" class="external-link" target="_blank" rel="nofollow noopener"&gt;https://github.com/apache/spark/pull/9264&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;So when SparkContext.runJob is suspended, ForkJoinPool will run another task in the same thread, however, the local properties has been polluted.&lt;/p&gt;</description>
                <environment/>
        <key id="12948114">SPARK-13747</key>
            <summary>Concurrent execution in SQL doesn't work with Scala ForkJoinPool</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="zsxwing">Shixiong Zhu</assignee>
                                    <reporter username="zsxwing">Shixiong Zhu</reporter>
                        <labels>
                    </labels>
                <created>Tue, 8 Mar 2016 19:07:09 +0000</created>
                <updated>Thu, 18 May 2017 00:22:15 +0000</updated>
                            <resolved>Thu, 18 May 2017 00:22:15 +0000</resolved>
                                    <version>2.0.0</version>
                    <version>2.0.1</version>
                                    <fixVersion>2.2.0</fixVersion>
                                    <component>SQL</component>
                        <due/>
                            <votes>1</votes>
                                    <watches>14</watches>
                                                                                                                <comments>
                            <comment id="15185533" author="zsxwing" created="Tue, 8 Mar 2016 19:09:50 +0000">&lt;p&gt;FYI, I switched to branch-1.6, and ran the same example. It will fail with StackOverflow because it submits too many blocking tasks to ForkJoinPool.&lt;/p&gt;

&lt;p&gt;Therefore, I'm not sure if it's worth to fix it. In general, the user should not submit many blocking tasks to ForkJoinPool otherwise StackOverflow will happen.&lt;/p&gt;</comment>
                            <comment id="15185743" author="apachespark" created="Tue, 8 Mar 2016 20:39:06 +0000">&lt;p&gt;User 'andrewor14' has created a pull request for this issue:&lt;br/&gt;
&lt;a href="https://github.com/apache/spark/pull/11586" class="external-link" target="_blank" rel="nofollow noopener"&gt;https://github.com/apache/spark/pull/11586&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15525924" author="venki_t@yahoo.com" created="Tue, 27 Sep 2016 12:11:23 +0000">&lt;p&gt;Looks like it still exists in 2.0.0? Will try to add a relevant test-case, soon&lt;/p&gt;</comment>
                            <comment id="15574502" author="chinwei" created="Fri, 14 Oct 2016 07:16:11 +0000">&lt;p&gt;I encounter this in 2.0.1, is there any workaround like having separate SparkSession will help?&lt;/p&gt;</comment>
                            <comment id="15579284" author="zsxwing" created="Sun, 16 Oct 2016 04:16:45 +0000">&lt;p&gt;&lt;a href="https://issues.apache.org/jira/secure/ViewProfile.jspa?name=chinwei" class="user-hover" rel="chinwei"&gt;chinwei&lt;/a&gt; could you post the stack trace here?&lt;/p&gt;</comment>
                            <comment id="15580989" author="chinwei" created="Mon, 17 Oct 2016 02:33:21 +0000">&lt;p&gt;java.lang.IllegalArgumentException: spark.sql.execution.id is already set&lt;br/&gt;
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:81) ~&lt;span class="error"&gt;&amp;#91;spark-sql_2.11-2.0.1.jar:2.0.1&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2546) ~&lt;span class="error"&gt;&amp;#91;spark-sql_2.11-2.0.1.jar:2.0.1&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2192) ~&lt;span class="error"&gt;&amp;#91;spark-sql_2.11-2.0.1.jar:2.0.1&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2199) ~&lt;span class="error"&gt;&amp;#91;spark-sql_2.11-2.0.1.jar:2.0.1&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2227) ~&lt;span class="error"&gt;&amp;#91;spark-sql_2.11-2.0.1.jar:2.0.1&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2226) ~&lt;span class="error"&gt;&amp;#91;spark-sql_2.11-2.0.1.jar:2.0.1&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2559) ~&lt;span class="error"&gt;&amp;#91;spark-sql_2.11-2.0.1.jar:2.0.1&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.spark.sql.Dataset.count(Dataset.scala:2226) ~&lt;span class="error"&gt;&amp;#91;spark-sql_2.11-2.0.1.jar:2.0.1&amp;#93;&lt;/span&gt;&lt;/p&gt;</comment>
                            <comment id="15581094" author="zsxwing" created="Mon, 17 Oct 2016 03:38:30 +0000">&lt;p&gt;Could you post the full stack track, please? It would be helpful to know who calls `count`.&lt;/p&gt;</comment>
                            <comment id="15581113" author="chinwei" created="Mon, 17 Oct 2016 03:48:02 +0000">&lt;p&gt;It is running on Akka, with forkjoin dispatcher. There are 2 actors running concurrently doing different Spark job using the same SparkSession. I can't give the full stack, but here is the outline:&lt;/p&gt;

&lt;p&gt;java.lang.IllegalArgumentException: spark.sql.execution.id is already set&lt;br/&gt;
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:81) ~&lt;span class="error"&gt;&amp;#91;spark-sql_2.11-2.0.1.jar:2.0.1&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2546) ~&lt;span class="error"&gt;&amp;#91;spark-sql_2.11-2.0.1.jar:2.0.1&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2192) ~&lt;span class="error"&gt;&amp;#91;spark-sql_2.11-2.0.1.jar:2.0.1&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2199) ~&lt;span class="error"&gt;&amp;#91;spark-sql_2.11-2.0.1.jar:2.0.1&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2227) ~&lt;span class="error"&gt;&amp;#91;spark-sql_2.11-2.0.1.jar:2.0.1&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2226) ~&lt;span class="error"&gt;&amp;#91;spark-sql_2.11-2.0.1.jar:2.0.1&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2559) ~&lt;span class="error"&gt;&amp;#91;spark-sql_2.11-2.0.1.jar:2.0.1&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.spark.sql.Dataset.count(Dataset.scala:2226) ~&lt;span class="error"&gt;&amp;#91;spark-sql_2.11-2.0.1.jar:2.0.1&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&amp;lt;-- Here is the code that call the df.count  --&amp;gt;&lt;/p&gt;

&lt;p&gt;at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526) &lt;span class="error"&gt;&amp;#91;akka-actor_2.11-2.4.8.jar:na&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at akka.actor.ActorCell.invoke(ActorCell.scala:495) &lt;span class="error"&gt;&amp;#91;akka-actor_2.11-2.4.8.jar:na&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257) &lt;span class="error"&gt;&amp;#91;akka-actor_2.11-2.4.8.jar:na&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at akka.dispatch.Mailbox.run(Mailbox.scala:224) &lt;span class="error"&gt;&amp;#91;akka-actor_2.11-2.4.8.jar:na&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at akka.dispatch.Mailbox.exec(Mailbox.scala:234) &lt;span class="error"&gt;&amp;#91;akka-actor_2.11-2.4.8.jar:na&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) &lt;span class="error"&gt;&amp;#91;scala-library-2.11.8.jar:na&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) &lt;span class="error"&gt;&amp;#91;scala-library-2.11.8.jar:na&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) &lt;span class="error"&gt;&amp;#91;scala-library-2.11.8.jar:na&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) &lt;span class="error"&gt;&amp;#91;scala-library-2.11.8.jar:na&amp;#93;&lt;/span&gt;&lt;/p&gt;

</comment>
                            <comment id="15583440" author="zsxwing" created="Mon, 17 Oct 2016 20:57:54 +0000">&lt;p&gt;There are other places need to be fixed.&lt;/p&gt;</comment>
                            <comment id="15583453" author="apachespark" created="Mon, 17 Oct 2016 21:03:04 +0000">&lt;p&gt;User 'zsxwing' has created a pull request for this issue:&lt;br/&gt;
&lt;a href="https://github.com/apache/spark/pull/15520" class="external-link" target="_blank" rel="nofollow noopener"&gt;https://github.com/apache/spark/pull/15520&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15583454" author="zsxwing" created="Mon, 17 Oct 2016 21:03:09 +0000">&lt;p&gt;&lt;a href="https://issues.apache.org/jira/secure/ViewProfile.jspa?name=chinwei" class="user-hover" rel="chinwei"&gt;chinwei&lt;/a&gt; Could you test &lt;a href="https://github.com/apache/spark/pull/15520" class="external-link" target="_blank" rel="nofollow noopener"&gt;https://github.com/apache/spark/pull/15520&lt;/a&gt; and see if the error is gone?&lt;/p&gt;</comment>
                            <comment id="15609137" author="apachespark" created="Wed, 26 Oct 2016 17:44:06 +0000">&lt;p&gt;User 'zsxwing' has created a pull request for this issue:&lt;br/&gt;
&lt;a href="https://github.com/apache/spark/pull/15646" class="external-link" target="_blank" rel="nofollow noopener"&gt;https://github.com/apache/spark/pull/15646&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15734575" author="zsxwing" created="Fri, 9 Dec 2016 07:23:47 +0000">&lt;p&gt;This issue still exists. Reopened it.&lt;/p&gt;</comment>
                            <comment id="15734585" author="apachespark" created="Fri, 9 Dec 2016 07:32:08 +0000">&lt;p&gt;User 'zsxwing' has created a pull request for this issue:&lt;br/&gt;
&lt;a href="https://github.com/apache/spark/pull/16230" class="external-link" target="_blank" rel="nofollow noopener"&gt;https://github.com/apache/spark/pull/16230&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15745766" author="yhuai" created="Tue, 13 Dec 2016 17:54:03 +0000">&lt;p&gt;Issue resolved by pull request 16230&lt;br/&gt;
&lt;a href="https://github.com/apache/spark/pull/16230" class="external-link" target="_blank" rel="nofollow noopener"&gt;https://github.com/apache/spark/pull/16230&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15936997" author="barrybecker4" created="Wed, 22 Mar 2017 19:49:31 +0000">&lt;p&gt;We have hit this on rare instances in our production environment when calling tableNames on SQLContext. We are using Spark 2.1.0. Are there any possible workarounds that we might try? What is the ETA of spark 2.2?&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;spark.sql.execution.id is already set
org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:81)
org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765)
org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2370)
org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2375)
org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2375)
org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2778)
org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2375)
org.apache.spark.sql.Dataset.collect(Dataset.scala:2351)
org.apache.spark.sql.SQLContext.tableNames(SQLContext.scala:750)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15976580" author="mousa" created="Thu, 20 Apr 2017 12:17:22 +0000">&lt;p&gt;I am also running into this issue &lt;b&gt;sporadically&lt;/b&gt; when collecting the results of joining two dataframes.&lt;/p&gt;

&lt;p&gt;The code that &lt;b&gt;sporadically&lt;/b&gt; generates this issue is:&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;val spark = SparkSession.builder().appName(&lt;span class="code-quote"&gt;"application"&lt;/span&gt;).master(&lt;span class="code-quote"&gt;"local[*]"&lt;/span&gt;).getOrCreate()
val itemCountry = spark.read.format(&lt;span class="code-quote"&gt;"csv"&lt;/span&gt;)
  .option(&lt;span class="code-quote"&gt;"header"&lt;/span&gt;, &lt;span class="code-quote"&gt;"&lt;span class="code-keyword"&gt;true&lt;/span&gt;"&lt;/span&gt;)
  .schema(StructType(Array(
    StructField(&lt;span class="code-quote"&gt;"itemId"&lt;/span&gt;, IntegerType, &lt;span class="code-keyword"&gt;false&lt;/span&gt;),
    StructField(&lt;span class="code-quote"&gt;"countryId"&lt;/span&gt;, IntegerType, &lt;span class="code-keyword"&gt;false&lt;/span&gt;))))
  .csv(&lt;span class="code-quote"&gt;"/item_country.csv"&lt;/span&gt;) &lt;span class="code-comment"&gt;// This file matches the schema provided
&lt;/span&gt;val itemPerformance = spark.read.format(&lt;span class="code-quote"&gt;"csv"&lt;/span&gt;)
  .option(&lt;span class="code-quote"&gt;"header"&lt;/span&gt;, &lt;span class="code-quote"&gt;"&lt;span class="code-keyword"&gt;true&lt;/span&gt;"&lt;/span&gt;)
  .schema(StructType(Array(
    StructField(&lt;span class="code-quote"&gt;"itemId"&lt;/span&gt;, IntegerType, &lt;span class="code-keyword"&gt;false&lt;/span&gt;),
    StructField(&lt;span class="code-quote"&gt;"date"&lt;/span&gt;, TimestampType, &lt;span class="code-keyword"&gt;false&lt;/span&gt;),
    StructField(&lt;span class="code-quote"&gt;"performance"&lt;/span&gt;, IntegerType, &lt;span class="code-keyword"&gt;false&lt;/span&gt;))))
  .csv(&lt;span class="code-quote"&gt;"/item_performance.csv"&lt;/span&gt;) &lt;span class="code-comment"&gt;// This file matches the schema provided
&lt;/span&gt;
itemCountry.join(itemPerformance, itemCountry(&lt;span class="code-quote"&gt;"itemId"&lt;/span&gt;) === itemPerformance(&lt;span class="code-quote"&gt;"itemId"&lt;/span&gt;))
  .groupBy(&lt;span class="code-quote"&gt;"countryId"&lt;/span&gt;)
  .agg(sum(when(to_date(itemPerformance(&lt;span class="code-quote"&gt;"date"&lt;/span&gt;)) &amp;gt; to_date(lit(&lt;span class="code-quote"&gt;"2017-01-01"&lt;/span&gt;)), itemPerformance(&lt;span class="code-quote"&gt;"performance"&lt;/span&gt;)).otherwise(0)).alias(&lt;span class="code-quote"&gt;"performance"&lt;/span&gt;)).show()
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The stack trace is:&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;java.lang.IllegalArgumentException: spark.sql.execution.id is already set
at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:81)
at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765)
at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2370)
at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2375)
at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2375)
at org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2778)
at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2375)
at org.apache.spark.sql.Dataset.collect(Dataset.scala:2351)
at .... [Custom caller functions]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15977132" author="zsxwing" created="Thu, 20 Apr 2017 17:45:41 +0000">&lt;p&gt;&lt;a href="https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mousa" class="user-hover" rel="mousa"&gt;mousa&lt;/a&gt; could you try the master branch? This issue will be fixed in Spark 2.2.0.&lt;/p&gt;</comment>
                            <comment id="15982949" author="dnaumenko" created="Tue, 25 Apr 2017 14:24:49 +0000">&lt;p&gt;&lt;a href="https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zsxwing" class="user-hover" rel="zsxwing"&gt;zsxwing&lt;/a&gt; I did a similar test with join and have the same error in 2.2.0 (actual query here - &lt;a href="https://github.com/dnaumenko/spark-realtime-analytics-sample/blob/master/samples/src/main/scala/com/github/sparksample/httpapp/SimpleServer.scala" class="external-link" target="_blank" rel="nofollow noopener"&gt;https://github.com/dnaumenko/spark-realtime-analytics-sample/blob/master/samples/src/main/scala/com/github/sparksample/httpapp/SimpleServer.scala&lt;/a&gt;). &lt;/p&gt;

&lt;p&gt;My test setup is a simple akka-http long-running application and separate Gatling script that spawns multiple requests for join query (&lt;a href="https://github.com/dnaumenko/spark-realtime-analytics-sample/blob/master/loadtool/src/main/scala/com/github/sparksample/SimpleServerSimulation.scala" class="external-link" target="_blank" rel="nofollow noopener"&gt;https://github.com/dnaumenko/spark-realtime-analytics-sample/blob/master/loadtool/src/main/scala/com/github/sparksample/SimpleServerSimulation.scala&lt;/a&gt; is test simulation script).&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/secure/ViewProfile.jspa?name=barrybecker4" class="user-hover" rel="barrybecker4"&gt;barrybecker4&lt;/a&gt; I've managed to fix the problem by switching akka's default executor to thread-pool. But I guess the root cause is that Spark is relying on ThreadLocal variables and manages them incorrectly.&lt;/p&gt;</comment>
                            <comment id="15983612" author="apachespark" created="Tue, 25 Apr 2017 21:07:03 +0000">&lt;p&gt;User 'zsxwing' has created a pull request for this issue:&lt;br/&gt;
&lt;a href="https://github.com/apache/spark/pull/17763" class="external-link" target="_blank" rel="nofollow noopener"&gt;https://github.com/apache/spark/pull/17763&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15983615" author="zsxwing" created="Tue, 25 Apr 2017 21:08:32 +0000">&lt;p&gt;&lt;a href="https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dnaumenko" class="user-hover" rel="dnaumenko"&gt;dnaumenko&lt;/a&gt; Unfortunately, Spark uses ThreadLocal variables a lot but ForkJoinPool doesn't support that very well (It's easy to leak ThreadLocal variables to other tasks). Could you check if &lt;a href="https://github.com/apache/spark/pull/17763" class="external-link" target="_blank" rel="nofollow noopener"&gt;https://github.com/apache/spark/pull/17763&lt;/a&gt; can fix your issue?&lt;/p&gt;</comment>
                            <comment id="16000788" author="mousa" created="Mon, 8 May 2017 14:14:58 +0000">&lt;p&gt;Thanks to &lt;a href="https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dnaumenko" class="user-hover" rel="dnaumenko"&gt;dnaumenko&lt;/a&gt;, we also managed to avoid running into this issue by switching akka’s default executor to “thread-pool-executor”. We are also using akka a lot in our system that is creating Spark’s jobs.&lt;br/&gt;
What I need to make sure of that, is this issue really a Spark issue or a fork-join framework issue? As far as I understood, a thread can be reused by its (or another) process before completely finishing its work to perform another work which would pollute its locals. If this is the case then the solution proposed in the pull request &lt;a href="https://github.com/apache/spark/pull/17763" class="external-link" target="_blank" rel="nofollow noopener"&gt;https://github.com/apache/spark/pull/17763&lt;/a&gt; won’t resolve this issue as it simply, if I fully got the proposed solution, prevents timeout exceptions from being thrown when calling "Await.ready".&lt;/p&gt;</comment>
                            <comment id="16000830" author="barrybecker4" created="Mon, 8 May 2017 14:31:10 +0000">&lt;p&gt;There seems to be some related discussion here&lt;br/&gt;
&lt;a href="http://apache-spark-developers-list.1001551.n3.nabble.com/IllegalArgumentException-spark-sql-execution-id-is-already-set-td19124.html" class="external-link" target="_blank" rel="nofollow noopener"&gt;http://apache-spark-developers-list.1001551.n3.nabble.com/IllegalArgumentException-spark-sql-execution-id-is-already-set-td19124.html&lt;/a&gt;&lt;br/&gt;
We use job-server (2.0-preview branch version) which uses akka. I believe spark does not. Maybe that is why we periodically see this issue (not sure). How can I switch akka's default executor to be "thread-pool-executor"? Is it a config option somewhere?&lt;/p&gt;</comment>
                            <comment id="16000863" author="mousa" created="Mon, 8 May 2017 14:59:48 +0000">&lt;p&gt;You can override akka's default executor for your application by adding the following configuration into an "application.conf" file that should be in the root of the class path.&lt;/p&gt;
&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;akka {
  actor {
    &lt;span class="code-keyword"&gt;default&lt;/span&gt;-dispatcher {
      # Which kind of ExecutorService to use &lt;span class="code-keyword"&gt;for&lt;/span&gt; &lt;span class="code-keyword"&gt;this&lt;/span&gt; dispatcher
      # Valid options:
      #  - &lt;span class="code-quote"&gt;"&lt;span class="code-keyword"&gt;default&lt;/span&gt;-executor"&lt;/span&gt; requires a &lt;span class="code-quote"&gt;"&lt;span class="code-keyword"&gt;default&lt;/span&gt;-executor"&lt;/span&gt; section
      #  - &lt;span class="code-quote"&gt;"fork-join-executor"&lt;/span&gt; requires a &lt;span class="code-quote"&gt;"fork-join-executor"&lt;/span&gt; section
      #  - &lt;span class="code-quote"&gt;"thread-pool-executor"&lt;/span&gt; requires a &lt;span class="code-quote"&gt;"thread-pool-executor"&lt;/span&gt; section
      #  - A FQCN of a &lt;span class="code-keyword"&gt;class &lt;/span&gt;extending ExecutorServiceConfigurator
      executor = &lt;span class="code-quote"&gt;"thread-pool-executor"&lt;/span&gt;
    }
  }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Further information can be found at: &lt;a href="http://doc.akka.io/docs/akka/current/general/configuration.html" class="external-link" target="_blank" rel="nofollow noopener"&gt;http://doc.akka.io/docs/akka/current/general/configuration.html&lt;/a&gt; and &lt;a href="http://doc.akka.io/docs/akka/current/scala/dispatchers.html" class="external-link" target="_blank" rel="nofollow noopener"&gt;http://doc.akka.io/docs/akka/current/scala/dispatchers.html&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16001117" author="revolucion09" created="Mon, 8 May 2017 17:14:09 +0000">&lt;p&gt;The above solution did help my program to start some additional threads, but it is still failing in some of them.&lt;/p&gt;

&lt;p&gt;java.lang.IllegalArgumentException: spark.sql.execution.id is already set&lt;br/&gt;
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:81)&lt;/p&gt;

&lt;p&gt;Although I am using akka-http to start a server, I am just executing spark actions inside traditional Futures.&lt;/p&gt;

&lt;p&gt;Spark data calls are called within a DSL Routing 'get' requests.&lt;/p&gt;

&lt;p&gt;using 2.1.0 in scala 2.11&lt;/p&gt;</comment>
                            <comment id="16001186" author="barrybecker4" created="Mon, 8 May 2017 17:47:01 +0000">&lt;p&gt;I also tried the "thread-pool-executor" workaround suggested above, but adding the suggested json to the top level of spark job-servers local.conf file. I still saw the error. The error is difficult to reproduce reliably, but I did see it once after making the change.&lt;/p&gt;</comment>
                            <comment id="16001303" author="barrybecker4" created="Mon, 8 May 2017 18:39:04 +0000">&lt;p&gt;@saif1988, just to clarify, did you add the following?&lt;br/&gt;
default-dispatcher {&lt;br/&gt;
      executor = "default-executor"&lt;br/&gt;
}&lt;br/&gt;
How do you know for sure that it fixes the problem? Did you have a case that reliably reproduced it? My problem is that it is very rare. I can know if it still happens, but absence of the error does not tell me that its truly fixed.&lt;/p&gt;</comment>
                            <comment id="16001356" author="revolucion09" created="Mon, 8 May 2017 19:10:36 +0000">&lt;p&gt;Sorry for the confusion. No, it doesnt work. I am currently trying out with using different execution contexts.&lt;/p&gt;

&lt;p&gt;My issue happens always, it is 100% reproducible.&lt;/p&gt;

&lt;p&gt;To simplify what I am doing:&lt;/p&gt;

&lt;p&gt;1. akka-http server is started and REST DSL is setup&lt;br/&gt;
2. Inside a get dsl, I call a Spark dataframe which calls collect action from within a Future&lt;br/&gt;
3. the object containing the future calls await.result, as I need the dataframe to respond a 200 to http&lt;br/&gt;
4. the collect method is passed through as an annonymous function. runtime exception poinst at such annonymous function as the callback starter of my exception&lt;br/&gt;
5. The future call is handled by a thread pool manged by spark pool. Uses FAIR scheduling.&lt;/p&gt;

&lt;p&gt;When my website starts, 4 collects are called simoultaneously. Only one get call returns 200. The others are internal server errors.&lt;/p&gt;</comment>
                            <comment id="16001365" author="zsxwing" created="Mon, 8 May 2017 19:15:49 +0000">&lt;p&gt;&lt;a href="https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mousa" class="user-hover" rel="mousa"&gt;mousa&lt;/a&gt; This is is because Spark uses ThreadLocal in a fork-join pool. Let me try to clarify the issue.&lt;/p&gt;

&lt;p&gt;A fork-join pool allows to run another pending task in the same thread when a running task is calling Await.ready/result. The magic is when someone calls Await.ready/result, it will first check if there is any pending task submitted to the pool, if so, it will call the pending task instead of waiting. (See scala.concurrent#blocking)&lt;/p&gt;

&lt;p&gt;In Spark, the codes hitting this issue have the following pattern.&lt;/p&gt;

&lt;div class="code panel" style="border-width: 1px;"&gt;&lt;div class="codeContent panelContent"&gt;
&lt;pre class="code-java"&gt;&lt;span class="code-keyword"&gt;try&lt;/span&gt; {
    check &lt;span class="code-keyword"&gt;if&lt;/span&gt; a thread local is set
    &lt;span class="code-keyword"&gt;if&lt;/span&gt; so, &lt;span class="code-keyword"&gt;throw&lt;/span&gt; an exception
    &lt;span class="code-keyword"&gt;else&lt;/span&gt;
        set the thread local value
        &lt;span class="code-keyword"&gt;do&lt;/span&gt; some work
        Call Await.ready/result to wait &lt;span class="code-keyword"&gt;for&lt;/span&gt; a result &lt;span class="code-comment"&gt;// This doesn't clear the thread local value. 
&lt;/span&gt;                                                     &lt;span class="code-comment"&gt;// If the fork-join pool schedules a pending task here,
&lt;/span&gt;                                                     &lt;span class="code-comment"&gt;// it will see the thread local value.
&lt;/span&gt;        &lt;span class="code-keyword"&gt;do&lt;/span&gt; some work
} &lt;span class="code-keyword"&gt;finally&lt;/span&gt; {
    clear the thread local value.
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;My PR is basically just not calling `scala.concurrent#blocking`. It just makes a fork-join pool become a normal thread pool executor.&lt;/p&gt;</comment>
                            <comment id="16001371" author="revolucion09" created="Mon, 8 May 2017 19:18:16 +0000">&lt;p&gt;I did fix it now 100% sure.&lt;/p&gt;

&lt;p&gt;1. Used thread-pool-executor&lt;br/&gt;
2. Replaced implicit global execution context with a FixedThreadPool executor (of size 16 on my case).&lt;/p&gt;

&lt;p&gt;I have no idea whether I am doing things properly, but it works flawlessly now.&lt;/p&gt;</comment>
                            <comment id="16001388" author="barrybecker4" created="Mon, 8 May 2017 19:26:58 +0000">&lt;p&gt;Good to hear that your workaround was successful. How did you do step 2? (Replaced implicit global execution context with a FixedThreadPool executor) Is that in the json configuration or in the code?&lt;/p&gt;</comment>
                            <comment id="16001389" author="zsxwing" created="Mon, 8 May 2017 19:27:20 +0000">&lt;p&gt;&lt;a href="https://issues.apache.org/jira/secure/ViewProfile.jspa?name=revolucion09" class="user-hover" rel="revolucion09"&gt;revolucion09&lt;/a&gt; If you are not using ForkJoinPool, I'm 100% sure you won't hit this issue.&lt;/p&gt;</comment>
                            <comment id="16001405" author="revolucion09" created="Mon, 8 May 2017 19:32:43 +0000">&lt;p&gt;&lt;a href="https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zsxwing" class="user-hover" rel="zsxwing"&gt;zsxwing&lt;/a&gt; I may be confused then. Which issue I am hitting if not this one? You are correct though, I am not explicitly using ForkJoinPool anywhere on my code.&lt;/p&gt;


&lt;p&gt;&lt;span class="error"&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; &lt;span class="error"&gt;&amp;#91;05/08/2017 13:25:27.735&amp;#93;&lt;/span&gt; &lt;span class="error"&gt;&amp;#91;Sake-akka.actor.default-dispatcher-3&amp;#93;&lt;/span&gt; &lt;span class="error"&gt;&amp;#91;akka.actor.ActorSystemImpl(Sake)&amp;#93;&lt;/span&gt; Error during processing of request: 'spark.sql.execution.id is already set'. Completing with 500 Internal Server Error response. To change default exception handling behavior, provide a custom ExceptionHandler.&lt;br/&gt;
java.lang.IllegalArgumentException: spark.sql.execution.id is already set&lt;br/&gt;
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:81)&lt;br/&gt;
        at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765)&lt;br/&gt;
        at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2370)&lt;br/&gt;
        at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2375)&lt;br/&gt;
        at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2375)&lt;br/&gt;
        at org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2778)&lt;br/&gt;
        at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2375)&lt;br/&gt;
        at org.apache.spark.sql.Dataset.collect(Dataset.scala:2351)&lt;br/&gt;
        at org.cmortech.sake.publisher.Descriptor$$anonfun$11.apply(Descriptor.scala:202)&lt;br/&gt;
        at org.cmortech.sake.publisher.Descriptor$$anonfun$11.apply(Descriptor.scala:202)&lt;br/&gt;
        at org.cmortech.sake.water.entities.orphanage.Orphan$$anonfun$start$1.apply(Orphan.scala:56)&lt;br/&gt;
        at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)&lt;br/&gt;
        at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)&lt;br/&gt;
        at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121)&lt;br/&gt;
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&lt;br/&gt;
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&lt;br/&gt;
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&lt;br/&gt;
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&lt;/p&gt;</comment>
                            <comment id="16001409" author="zsxwing" created="Mon, 8 May 2017 19:35:52 +0000">&lt;p&gt;&lt;a href="https://issues.apache.org/jira/secure/ViewProfile.jspa?name=revolucion09" class="user-hover" rel="revolucion09"&gt;revolucion09&lt;/a&gt; I don't know who created ForkJoinPool but your stack trace says it's inside ForkJoinPool.&lt;/p&gt;</comment>
                            <comment id="16001415" author="zsxwing" created="Mon, 8 May 2017 19:39:17 +0000">&lt;p&gt;Okey, I see, the thread name is "Sake-akka.actor.default-dispatcher-3", so I think it's just Akka default-dispatcher.&lt;/p&gt;</comment>
                            <comment id="16001419" author="revolucion09" created="Mon, 8 May 2017 19:42:31 +0000">&lt;p&gt;&lt;a href="https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zsxwing" class="user-hover" rel="zsxwing"&gt;zsxwing&lt;/a&gt; Thanks. My program is pretty much default. Could it be that because I am running inside akka-http dsl, and just by using ExecutionContext.implicits.Global, it does spawn everything on a ForkJoinPool? &lt;/p&gt;

&lt;p&gt;Good thing is that I could resolve it by creating my own Executors.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://issues.apache.org/jira/secure/ViewProfile.jspa?name=barrybecker4" class="user-hover" rel="barrybecker4"&gt;barrybecker4&lt;/a&gt;&lt;br/&gt;
  private val pool = Executors.newFixedThreadPool(16)&lt;br/&gt;
  implicit private val ec = ExecutionContext.fromExecutorService(pool)&lt;/p&gt;</comment>
                            <comment id="16001426" author="zsxwing" created="Mon, 8 May 2017 19:45:18 +0000">&lt;p&gt;&lt;a href="https://issues.apache.org/jira/secure/ViewProfile.jspa?name=revolucion09" class="user-hover" rel="revolucion09"&gt;revolucion09&lt;/a&gt; The default dispatcher uses ForkJoinPool. See &lt;a href="http://doc.akka.io/docs/akka/current/scala/dispatchers.html#Default_dispatcher" class="external-link" target="_blank" rel="nofollow noopener"&gt;http://doc.akka.io/docs/akka/current/scala/dispatchers.html#Default_dispatcher&lt;/a&gt; &lt;/p&gt;</comment>
                            <comment id="16002674" author="dnaumenko" created="Tue, 9 May 2017 13:34:02 +0000">&lt;p&gt;&lt;a href="https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zsxwing" class="user-hover" rel="zsxwing"&gt;zsxwing&lt;/a&gt; I've tried to build a Spark from a branch in pull request. Didn't manage to make a complete build (had some problems with R dependencies), so I've replaced only spark-core.jar and it seems like the issue still occurs. Could you please provide a jar/dist for re-test? &lt;/p&gt;

&lt;p&gt;As a side note, the fixed-thread-pool solution works well for us. We will probably stick with it.&lt;/p&gt;</comment>
                            <comment id="16002814" author="revolucion09" created="Tue, 9 May 2017 14:50:25 +0000">&lt;p&gt;&lt;a href="https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dnaumenko" class="user-hover" rel="dnaumenko"&gt;dnaumenko&lt;/a&gt;&lt;br/&gt;
Nonetheless, if I am not mistaken, there are proofs that fork join pools provide significant performance boost in scalable environments, that is why akka uses them by default. Fixed or Cached pool threads are considered dangerous for production environments.&lt;/p&gt;</comment>
                            <comment id="16002852" author="dnaumenko" created="Tue, 9 May 2017 15:17:02 +0000">&lt;p&gt;&lt;a href="https://issues.apache.org/jira/secure/ViewProfile.jspa?name=revolucion09" class="user-hover" rel="revolucion09"&gt;revolucion09&lt;/a&gt; It's a bit an off-topic discussion. My 50 cents to it - from my understanding, fork-join pool in Akka helps to keep all processors busy, so you can archive a high message throughput per second (as long as you don't use blocking operations). But in Spark driver program, it's not a critical, cause the most time it will be waiting for worker nodes anyway. &lt;/p&gt;</comment>
                            <comment id="16008526" author="kagan" created="Fri, 12 May 2017 18:26:25 +0000">&lt;p&gt;I am having the same exception.&lt;/p&gt;

&lt;p&gt;I am creating a new data source that processes reading batch files asynchronously into a temp folder and then returns them as a data frame.&lt;/p&gt;

&lt;p&gt;Within the buildScan(): RDD&lt;span class="error"&gt;&amp;#91;Row&amp;#93;&lt;/span&gt;  method  I have a loop that saves the results of each batch in a parquet file:&lt;/p&gt;

&lt;p&gt; val df = spark.sparkContext.parallelize(batchResult.records, 200).toDF()     &lt;br/&gt;
 df.write.mode(SaveMode.Overwrite).save(tempFile)}&lt;/p&gt;

&lt;p&gt;Then once the temp files are all written, buildScan method returns &lt;br/&gt;
I will load all those temp files in parallel and return the union in an RDD like this:&lt;br/&gt;
sqlContext.read&lt;br/&gt;
      .schema(schema)&lt;br/&gt;
      .load(files: _*)     &lt;br/&gt;
      .queryExecution.executedPlan. execute().asInstanceOf[RDD&lt;span class="error"&gt;&amp;#91;Row&amp;#93;&lt;/span&gt;]&lt;/p&gt;

&lt;p&gt;I can see the concurrency issue as I am trying to write the temp files at same time I am trying to construct a return RDD.  &lt;br/&gt;
Is there a better way of doing this?&lt;br/&gt;
To work around, I can save the temp files as regular CSV to work around the issue but I prefer to save these files as Parquet files using Spark API.&lt;br/&gt;
Would upgrading to 2.12 fix this issue in my case?&lt;/p&gt;
</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12933317">SPARK-12964</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12863179">SPARK-10548</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 27 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2ucon:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12338301">2.0.2</customfieldvalue>
    <customfieldvalue id="12335644">2.1.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>