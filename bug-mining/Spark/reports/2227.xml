<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:30:27 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-8297] Scheduler backend is not notified in case node fails in YARN</title>
                <link>https://issues.apache.org/jira/browse/SPARK-8297</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;When a node crashes, yarn detects the failure and notifies spark - but this information is not propagated to scheduler backend (unlike in mesos mode, for example).&lt;/p&gt;

&lt;p&gt;It results in repeated re-execution of stages (due to FetchFailedException on shuffle side), resulting finally in application failure.&lt;/p&gt;</description>
                <environment>&lt;p&gt;Spark on yarn - both client and cluster mode.&lt;/p&gt;</environment>
        <key id="12836941">SPARK-8297</key>
            <summary>Scheduler backend is not notified in case node fails in YARN</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="mridulm80">Mridul Muralidharan</assignee>
                                    <reporter username="mridulm80">Mridul Muralidharan</reporter>
                        <labels>
                    </labels>
                <created>Wed, 10 Jun 2015 20:24:04 +0000</created>
                <updated>Thu, 30 Jul 2015 17:38:39 +0000</updated>
                            <resolved>Thu, 30 Jul 2015 17:38:38 +0000</resolved>
                                    <version>1.2.2</version>
                    <version>1.3.1</version>
                    <version>1.4.1</version>
                    <version>1.5.0</version>
                                    <fixVersion>1.5.0</fixVersion>
                                    <component>YARN</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="14581038" author="mridulm80" created="Wed, 10 Jun 2015 20:32:19 +0000"  >&lt;p&gt;Spark on mesos handles this situation by calling removeExecutor() on the scheduler backend - yarn module does not.&lt;br/&gt;
I have this fixed locally, but unfortunately, I do not have the bandwidth to shepherd a patch.&lt;/p&gt;

&lt;p&gt;The fix is simple - replicate something similar to what is done in CoarseMesosSchedulerBackend.slaveLost().&lt;br/&gt;
Essentially :&lt;br/&gt;
a) maintain a mapping from container-id to executor-id in YarnAllocator (consistent with and inverse of executorIdToContainer)&lt;br/&gt;
b) propagate the scheduler backend to YarnAllocator when YarnClusterScheduler.postCommitHook is called, &lt;br/&gt;
c) In processCompletedContainers, if the container is not in releasedContainers, invoke backend.removeExecutor(executorId, msg) to notify backend that the executor has not exit&apos;ed gracefully/expectedly.&lt;br/&gt;
d) Remove mapping from containerIdToExecutorId and executorIdToContainer in processCompletedContainers (The latter also fixes a memory leak in YarnAllocator btw).&lt;/p&gt;


&lt;p&gt;In case no one is picking this one up, I can fix it later in 1.5 release cycle.&lt;/p&gt;</comment>
                            <comment id="14581383" author="jerryshao" created="Thu, 11 Jun 2015 03:15:08 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mridulm80&quot; class=&quot;user-hover&quot; rel=&quot;mridulm80&quot;&gt;mridulm80&lt;/a&gt;, I tried with latest master branch with spark-shell under yarn-client mode. I simulated executor crash by kill it with &quot;kill -9&quot;. From my observation, the scheduler backend is notified when executor is lost, here is the log:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;scala&amp;gt; 15/06/11 11:08:38 ERROR cluster.YarnScheduler: Lost executor 1 on jerryshao-desktop: remote Rpc client disassociated
15/06/11 11:08:38 WARN remote.ReliableDeliverySupervisor: Association with remote system [akka.tcp://sparkExecutor@jerryshao-desktop:50766] has failed, address is now gated for [5000] ms. Reason: [Disassociated] 
15/06/11 11:08:38 INFO scheduler.DAGScheduler: Executor lost: 1 (epoch 0)
15/06/11 11:08:38 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
15/06/11 11:08:38 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(1, jerryshao-desktop, 48633)
15/06/11 11:08:38 INFO storage.BlockManagerMaster: Removed 1 successfully in removeExecutor
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is the driver log, also YarnAllocator is correctly remove the metadata in &lt;tt&gt;processCompletedContainers()&lt;/tt&gt; I cannot fully catch your meaning, would you please describe a little specifically?&lt;/p&gt;</comment>
                            <comment id="14581713" author="mridulm80" created="Thu, 11 Jun 2015 09:24:36 +0000"  >&lt;p&gt;kill -9 is not sufficient - sockets will get closed, which will cause akka to notify master.&lt;br/&gt;
To test, pulling the ethernet cable to the node should suffice.&lt;/p&gt;

&lt;p&gt;Essentially, when the node goes completely MIA, spark-yarn does not handle it : example exceptions while shuffle fetch would be something like NoRouteToHost, etc.&lt;/p&gt;</comment>
                            <comment id="14582796" author="jerryshao" created="Fri, 12 Jun 2015 01:31:57 +0000"  >&lt;p&gt;OK, thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mridulm80&quot; class=&quot;user-hover&quot; rel=&quot;mridulm80&quot;&gt;mridulm80&lt;/a&gt;, I will take a try, from my understanding, akka will also get notified if connection is abruptly lost, I didn&apos;t test it, will take a try.&lt;/p&gt;</comment>
                            <comment id="14583115" author="jerryshao" created="Fri, 12 Jun 2015 07:54:34 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mridulm80&quot; class=&quot;user-hover&quot; rel=&quot;mridulm80&quot;&gt;mridulm80&lt;/a&gt;, I&apos;m trying to simulate your scenario by pulling the ethernet cable while running the application, still I cannot reproduce like what you mentioned. When I pulled the cable, the Spark application is hung silent without any message, after some time the HeatbeatReceiver detects that one executor is lost, it will send kill request to ApplicationMaster, ApplicationMaster will call YarnAllocator to release the container and related metadata. From what I could observed, in YarnSchedulerBackend and YarnAllocator, the status is expected.&lt;/p&gt;

&lt;p&gt;I&apos;m not sure if it can exactly simulate what you mentioned above&lt;/p&gt;</comment>
                            <comment id="14583560" author="mridulm80" created="Fri, 12 Jun 2015 15:21:45 +0000"  >
&lt;p&gt;I am not sure why the spark application would hang - if you pull the cable for a worker. What exactly was the behavior you observed ?&lt;/p&gt;

&lt;p&gt;Note that we observe yarn detecting the missing node and deallocating all containers for all applications on the node - and notifies the corresponding application master&apos;s.&lt;br/&gt;
In spark-yarn, we clean up the yarn specific state for that. We just do not propagate that to scheduler backend (which, for example, spark-mesos scheduler does).&lt;/p&gt;

&lt;p&gt;To elaborate, the exact scenario where we fairly regularly (about once a month) encounter is like this :&lt;/p&gt;

&lt;p&gt;We run the spark application on about 600+ nodes on a much larger cluster, and during the course of the job, one or more nodes will fail &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;.&lt;br/&gt;
The job typically is a cascade of maps followed by reduces - and so other than initial task, everything else pretty much runs on process local locality level (for maps).&lt;br/&gt;
When an executor goes MIA (does not respond to ping, etc &lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;), shuffle fetches will fail - causing repeated attempts at reexecution, and eventual application hang &lt;span class=&quot;error&quot;&gt;&amp;#91;3&amp;#93;&lt;/span&gt;.&lt;/p&gt;



&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; Not all node failures trigger this issue - which makes reproducing this unpredictable - hence needing to rely on logs.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; In our specific app, the timeout&apos;s for heartbeat is increased due to gc issues we see in spark - when executors are repeatedly killed just cos they were slower in responding to heartbeat.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;3&amp;#93;&lt;/span&gt; We have high threshold for task and application failures, since it is a long running job and there are usually frequent transient failures (particularly due to yarn aggresively police&apos;ing the resource limits). &lt;/p&gt;
</comment>
                            <comment id="14615522" author="apachespark" created="Mon, 6 Jul 2015 19:44:08 +0000"  >&lt;p&gt;User &apos;mridulm&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/7243&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/7243&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14628800" author="apachespark" created="Wed, 15 Jul 2015 21:43:07 +0000"  >&lt;p&gt;User &apos;vanzin&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/7431&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/7431&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 18 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2fwcf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>