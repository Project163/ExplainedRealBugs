<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 19:07:04 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-29322] History server is stuck reading incomplete event log file compressed with zstd</title>
                <link>https://issues.apache.org/jira/browse/SPARK-29322</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;While working on &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-28869&quot; title=&quot;Roll over event log files&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-28869&quot;&gt;&lt;del&gt;SPARK-28869&lt;/del&gt;&lt;/a&gt;, I&apos;ve discovered the issue that reading inprogress event log file on zstd compression could lead the thread being stuck. I just experimented with Spark History Server and observed same issue. I&apos;ll attach the jstack files.&lt;/p&gt;

&lt;p&gt;This is very easy to reproduce: setting configuration as below&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;spark.eventLog.enabled=true&lt;/li&gt;
	&lt;li&gt;spark.eventLog.compress=true&lt;/li&gt;
	&lt;li&gt;spark.eventLog.compression.codec=zstd&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;and start Spark application. While the application is running, load the application in SHS webpage. It may succeed to replay the event log, but high likely it will be stuck and loading page will be also stuck.&lt;/p&gt;

&lt;p&gt;Only listing the thread stack trace being stuck across jstack files:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2019-10-02 11:32:36
Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.191-b12 mixed mode):

...

&lt;span class=&quot;code-quote&quot;&gt;&quot;qtp2072313080-30&quot;&lt;/span&gt; #30 daemon prio=5 os_prio=31 tid=0x00007ff5b90e7800 nid=0x9703 runnable [0x000070000f220000]
   java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.State: RUNNABLE
	at java.io.FileInputStream.readBytes(Native Method)
	at java.io.FileInputStream.read(FileInputStream.java:255)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:156)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	- locked &amp;lt;0x00000007b5f97c60&amp;gt; (a org.apache.hadoop.fs.BufferedFSInputStream)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:436)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:257)
	at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:276)
	at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:228)
	at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:196)
	- locked &amp;lt;0x00000007b5f97b58&amp;gt; (a org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	- locked &amp;lt;0x00000007b5f97af8&amp;gt; (a java.io.BufferedInputStream)
	at com.github.luben.zstd.ZstdInputStream.readInternal(ZstdInputStream.java:129)
	at com.github.luben.zstd.ZstdInputStream.read(ZstdInputStream.java:107)
	- locked &amp;lt;0x00000007b5f97ac0&amp;gt; (a com.github.luben.zstd.ZstdInputStream)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	- locked &amp;lt;0x00000007b5cd3bd0&amp;gt; (a java.io.BufferedInputStream)
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	- locked &amp;lt;0x00000007b5f94a00&amp;gt; (a java.io.InputStreamReader)
	at java.io.InputStreamReader.read(InputStreamReader.java:184)
	at java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.io.BufferedReader.readLine(BufferedReader.java:324)
	- locked &amp;lt;0x00000007b5f94a00&amp;gt; (a java.io.InputStreamReader)
	at java.io.BufferedReader.readLine(BufferedReader.java:389)
	at scala.io.BufferedSource$BufferedLineIterator.hasNext(BufferedSource.scala:74)
	at scala.collection.Iterator$$anon$20.hasNext(Iterator.scala:884)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:511)
	at org.apache.spark.scheduler.ReplayListenerBus.replay(ReplayListenerBus.scala:80)
	at org.apache.spark.scheduler.ReplayListenerBus.replay(ReplayListenerBus.scala:58)
	at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$rebuildAppStore$5(FsHistoryProvider.scala:976)
	at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$rebuildAppStore$5$adapted(FsHistoryProvider.scala:975)
	at org.apache.spark.deploy.history.FsHistoryProvider$$Lambda$662/1267867461.apply(Unknown Source)
	at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2567)
	at org.apache.spark.deploy.history.FsHistoryProvider.rebuildAppStore(FsHistoryProvider.scala:975)
	at org.apache.spark.deploy.history.FsHistoryProvider.createInMemoryStore(FsHistoryProvider.scala:1093)
	at org.apache.spark.deploy.history.FsHistoryProvider.getAppUI(FsHistoryProvider.scala:346)
	at org.apache.spark.deploy.history.HistoryServer.getAppUI(HistoryServer.scala:188)
	at org.apache.spark.deploy.history.ApplicationCache.$anonfun$loadApplicationEntry$2(ApplicationCache.scala:163)
	at org.apache.spark.deploy.history.ApplicationCache$$Lambda$592/2060065989.apply(Unknown Source)
	at org.apache.spark.deploy.history.ApplicationCache.time(ApplicationCache.scala:135)
	at org.apache.spark.deploy.history.ApplicationCache.org$apache$spark$deploy$history$ApplicationCache$$loadApplicationEntry(ApplicationCache.scala:161)
	at org.apache.spark.deploy.history.ApplicationCache$$anon$1.load(ApplicationCache.scala:56)
	at org.apache.spark.deploy.history.ApplicationCache$$anon$1.load(ApplicationCache.scala:52)
	at org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	at org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	- locked &amp;lt;0x00000007b5cd3de0&amp;gt; (a org.sparkproject.guava.cache.LocalCache$StrongAccessEntry)
	at org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.sparkproject.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.sparkproject.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.deploy.history.ApplicationCache.get(ApplicationCache.scala:89)
	at org.apache.spark.deploy.history.ApplicationCache.withSparkUI(ApplicationCache.scala:101)
	at org.apache.spark.deploy.history.HistoryServer.org$apache$spark$deploy$history$HistoryServer$$loadAppUi(HistoryServer.scala:245)
	at org.apache.spark.deploy.history.HistoryServer$$anon$1.doGet(HistoryServer.scala:98)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:873)
	at org.sparkproject.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1623)
	at org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)
	at org.sparkproject.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1610)
	at org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:540)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)
	at org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1345)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)
	at org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)
	at org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1247)
	at org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
	at org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:753)
	at org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:220)
	at org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.sparkproject.jetty.server.Server.handle(Server.java:505)
	at org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:370)
	at org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:267)
	at org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
	at org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.sparkproject.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
	at org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
	at org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:698)
	at org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:804)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:748)


2019-10-02 11:33:13
Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.191-b12 mixed mode):

...

&lt;span class=&quot;code-quote&quot;&gt;&quot;qtp2072313080-30&quot;&lt;/span&gt; #30 daemon prio=5 os_prio=31 tid=0x00007ff5b90e7800 nid=0x9703 runnable [0x000070000f220000]
   java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.State: RUNNABLE
	at java.io.FileInputStream.readBytes(Native Method)
	at java.io.FileInputStream.read(FileInputStream.java:255)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:156)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	- locked &amp;lt;0x00000007b5f97c60&amp;gt; (a org.apache.hadoop.fs.BufferedFSInputStream)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:436)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:257)
	at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:276)
	at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:228)
	at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:196)
	- locked &amp;lt;0x00000007b5f97b58&amp;gt; (a org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	- locked &amp;lt;0x00000007b5f97af8&amp;gt; (a java.io.BufferedInputStream)
	at com.github.luben.zstd.ZstdInputStream.readInternal(ZstdInputStream.java:129)
	at com.github.luben.zstd.ZstdInputStream.read(ZstdInputStream.java:107)
	- locked &amp;lt;0x00000007b5f97ac0&amp;gt; (a com.github.luben.zstd.ZstdInputStream)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	- locked &amp;lt;0x00000007b5cd3bd0&amp;gt; (a java.io.BufferedInputStream)
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	- locked &amp;lt;0x00000007b5f94a00&amp;gt; (a java.io.InputStreamReader)
	at java.io.InputStreamReader.read(InputStreamReader.java:184)
	at java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.io.BufferedReader.readLine(BufferedReader.java:324)
	- locked &amp;lt;0x00000007b5f94a00&amp;gt; (a java.io.InputStreamReader)
	at java.io.BufferedReader.readLine(BufferedReader.java:389)
	at scala.io.BufferedSource$BufferedLineIterator.hasNext(BufferedSource.scala:74)
	at scala.collection.Iterator$$anon$20.hasNext(Iterator.scala:884)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:511)
	at org.apache.spark.scheduler.ReplayListenerBus.replay(ReplayListenerBus.scala:80)
	at org.apache.spark.scheduler.ReplayListenerBus.replay(ReplayListenerBus.scala:58)
	at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$rebuildAppStore$5(FsHistoryProvider.scala:976)
	at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$rebuildAppStore$5$adapted(FsHistoryProvider.scala:975)
	at org.apache.spark.deploy.history.FsHistoryProvider$$Lambda$662/1267867461.apply(Unknown Source)
	at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2567)
	at org.apache.spark.deploy.history.FsHistoryProvider.rebuildAppStore(FsHistoryProvider.scala:975)
	at org.apache.spark.deploy.history.FsHistoryProvider.createInMemoryStore(FsHistoryProvider.scala:1093)
	at org.apache.spark.deploy.history.FsHistoryProvider.getAppUI(FsHistoryProvider.scala:346)
	at org.apache.spark.deploy.history.HistoryServer.getAppUI(HistoryServer.scala:188)
	at org.apache.spark.deploy.history.ApplicationCache.$anonfun$loadApplicationEntry$2(ApplicationCache.scala:163)
	at org.apache.spark.deploy.history.ApplicationCache$$Lambda$592/2060065989.apply(Unknown Source)
	at org.apache.spark.deploy.history.ApplicationCache.time(ApplicationCache.scala:135)
	at org.apache.spark.deploy.history.ApplicationCache.org$apache$spark$deploy$history$ApplicationCache$$loadApplicationEntry(ApplicationCache.scala:161)
	at org.apache.spark.deploy.history.ApplicationCache$$anon$1.load(ApplicationCache.scala:56)
	at org.apache.spark.deploy.history.ApplicationCache$$anon$1.load(ApplicationCache.scala:52)
	at org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	at org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	- locked &amp;lt;0x00000007b5cd3de0&amp;gt; (a org.sparkproject.guava.cache.LocalCache$StrongAccessEntry)
	at org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.sparkproject.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.sparkproject.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.deploy.history.ApplicationCache.get(ApplicationCache.scala:89)
	at org.apache.spark.deploy.history.ApplicationCache.withSparkUI(ApplicationCache.scala:101)
	at org.apache.spark.deploy.history.HistoryServer.org$apache$spark$deploy$history$HistoryServer$$loadAppUi(HistoryServer.scala:245)
	at org.apache.spark.deploy.history.HistoryServer$$anon$1.doGet(HistoryServer.scala:98)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:873)
	at org.sparkproject.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1623)
	at org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)
	at org.sparkproject.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1610)
	at org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:540)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)
	at org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1345)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)
	at org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)
	at org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1247)
	at org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
	at org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:753)
	at org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:220)
	at org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.sparkproject.jetty.server.Server.handle(Server.java:505)
	at org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:370)
	at org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:267)
	at org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
	at org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.sparkproject.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
	at org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
	at org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:698)
	at org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:804)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:748)


2019-10-02 11:34:00
Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.191-b12 mixed mode):

...

&lt;span class=&quot;code-quote&quot;&gt;&quot;qtp2072313080-30&quot;&lt;/span&gt; #30 daemon prio=5 os_prio=31 tid=0x00007ff5b90e7800 nid=0x9703 runnable [0x000070000f220000]
   java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.State: RUNNABLE
	at java.io.FileInputStream.readBytes(Native Method)
	at java.io.FileInputStream.read(FileInputStream.java:255)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:156)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	- locked &amp;lt;0x00000007b5f97c60&amp;gt; (a org.apache.hadoop.fs.BufferedFSInputStream)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:436)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:257)
	at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:276)
	at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:228)
	at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:196)
	- locked &amp;lt;0x00000007b5f97b58&amp;gt; (a org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	- locked &amp;lt;0x00000007b5f97af8&amp;gt; (a java.io.BufferedInputStream)
	at com.github.luben.zstd.ZstdInputStream.readInternal(ZstdInputStream.java:129)
	at com.github.luben.zstd.ZstdInputStream.read(ZstdInputStream.java:107)
	- locked &amp;lt;0x00000007b5f97ac0&amp;gt; (a com.github.luben.zstd.ZstdInputStream)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	- locked &amp;lt;0x00000007b5cd3bd0&amp;gt; (a java.io.BufferedInputStream)
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	- locked &amp;lt;0x00000007b5f94a00&amp;gt; (a java.io.InputStreamReader)
	at java.io.InputStreamReader.read(InputStreamReader.java:184)
	at java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.io.BufferedReader.readLine(BufferedReader.java:324)
	- locked &amp;lt;0x00000007b5f94a00&amp;gt; (a java.io.InputStreamReader)
	at java.io.BufferedReader.readLine(BufferedReader.java:389)
	at scala.io.BufferedSource$BufferedLineIterator.hasNext(BufferedSource.scala:74)
	at scala.collection.Iterator$$anon$20.hasNext(Iterator.scala:884)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:511)
	at org.apache.spark.scheduler.ReplayListenerBus.replay(ReplayListenerBus.scala:80)
	at org.apache.spark.scheduler.ReplayListenerBus.replay(ReplayListenerBus.scala:58)
	at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$rebuildAppStore$5(FsHistoryProvider.scala:976)
	at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$rebuildAppStore$5$adapted(FsHistoryProvider.scala:975)
	at org.apache.spark.deploy.history.FsHistoryProvider$$Lambda$662/1267867461.apply(Unknown Source)
	at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2567)
	at org.apache.spark.deploy.history.FsHistoryProvider.rebuildAppStore(FsHistoryProvider.scala:975)
	at org.apache.spark.deploy.history.FsHistoryProvider.createInMemoryStore(FsHistoryProvider.scala:1093)
	at org.apache.spark.deploy.history.FsHistoryProvider.getAppUI(FsHistoryProvider.scala:346)
	at org.apache.spark.deploy.history.HistoryServer.getAppUI(HistoryServer.scala:188)
	at org.apache.spark.deploy.history.ApplicationCache.$anonfun$loadApplicationEntry$2(ApplicationCache.scala:163)
	at org.apache.spark.deploy.history.ApplicationCache$$Lambda$592/2060065989.apply(Unknown Source)
	at org.apache.spark.deploy.history.ApplicationCache.time(ApplicationCache.scala:135)
	at org.apache.spark.deploy.history.ApplicationCache.org$apache$spark$deploy$history$ApplicationCache$$loadApplicationEntry(ApplicationCache.scala:161)
	at org.apache.spark.deploy.history.ApplicationCache$$anon$1.load(ApplicationCache.scala:56)
	at org.apache.spark.deploy.history.ApplicationCache$$anon$1.load(ApplicationCache.scala:52)
	at org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	at org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	- locked &amp;lt;0x00000007b5cd3de0&amp;gt; (a org.sparkproject.guava.cache.LocalCache$StrongAccessEntry)
	at org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.sparkproject.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.sparkproject.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.deploy.history.ApplicationCache.get(ApplicationCache.scala:89)
	at org.apache.spark.deploy.history.ApplicationCache.withSparkUI(ApplicationCache.scala:101)
	at org.apache.spark.deploy.history.HistoryServer.org$apache$spark$deploy$history$HistoryServer$$loadAppUi(HistoryServer.scala:245)
	at org.apache.spark.deploy.history.HistoryServer$$anon$1.doGet(HistoryServer.scala:98)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:873)
	at org.sparkproject.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1623)
	at org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)
	at org.sparkproject.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1610)
	at org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:540)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)
	at org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1345)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)
	at org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)
	at org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1247)
	at org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
	at org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:753)
	at org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:220)
	at org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.sparkproject.jetty.server.Server.handle(Server.java:505)
	at org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:370)
	at org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:267)
	at org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
	at org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.sparkproject.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
	at org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
	at org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:698)
	at org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:804)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:748)


2019-10-02 11:38:33
Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.191-b12 mixed mode):

...

&lt;span class=&quot;code-quote&quot;&gt;&quot;qtp2072313080-30&quot;&lt;/span&gt; #30 daemon prio=5 os_prio=31 tid=0x00007ff5b90e7800 nid=0x9703 runnable [0x000070000f220000]
   java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.State: RUNNABLE
	at java.io.FileInputStream.readBytes(Native Method)
	at java.io.FileInputStream.read(FileInputStream.java:255)
	at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:156)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	- locked &amp;lt;0x00000007b5f97c60&amp;gt; (a org.apache.hadoop.fs.BufferedFSInputStream)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:436)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:257)
	at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:276)
	at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:228)
	at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:196)
	- locked &amp;lt;0x00000007b5f97b58&amp;gt; (a org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	- locked &amp;lt;0x00000007b5f97af8&amp;gt; (a java.io.BufferedInputStream)
	at com.github.luben.zstd.ZstdInputStream.readInternal(ZstdInputStream.java:129)
	at com.github.luben.zstd.ZstdInputStream.read(ZstdInputStream.java:107)
	- locked &amp;lt;0x00000007b5f97ac0&amp;gt; (a com.github.luben.zstd.ZstdInputStream)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	- locked &amp;lt;0x00000007b5cd3bd0&amp;gt; (a java.io.BufferedInputStream)
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	- locked &amp;lt;0x00000007b5f94a00&amp;gt; (a java.io.InputStreamReader)
	at java.io.InputStreamReader.read(InputStreamReader.java:184)
	at java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.io.BufferedReader.readLine(BufferedReader.java:324)
	- locked &amp;lt;0x00000007b5f94a00&amp;gt; (a java.io.InputStreamReader)
	at java.io.BufferedReader.readLine(BufferedReader.java:389)
	at scala.io.BufferedSource$BufferedLineIterator.hasNext(BufferedSource.scala:74)
	at scala.collection.Iterator$$anon$20.hasNext(Iterator.scala:884)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:511)
	at org.apache.spark.scheduler.ReplayListenerBus.replay(ReplayListenerBus.scala:80)
	at org.apache.spark.scheduler.ReplayListenerBus.replay(ReplayListenerBus.scala:58)
	at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$rebuildAppStore$5(FsHistoryProvider.scala:976)
	at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$rebuildAppStore$5$adapted(FsHistoryProvider.scala:975)
	at org.apache.spark.deploy.history.FsHistoryProvider$$Lambda$662/1267867461.apply(Unknown Source)
	at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2567)
	at org.apache.spark.deploy.history.FsHistoryProvider.rebuildAppStore(FsHistoryProvider.scala:975)
	at org.apache.spark.deploy.history.FsHistoryProvider.createInMemoryStore(FsHistoryProvider.scala:1093)
	at org.apache.spark.deploy.history.FsHistoryProvider.getAppUI(FsHistoryProvider.scala:346)
	at org.apache.spark.deploy.history.HistoryServer.getAppUI(HistoryServer.scala:188)
	at org.apache.spark.deploy.history.ApplicationCache.$anonfun$loadApplicationEntry$2(ApplicationCache.scala:163)
	at org.apache.spark.deploy.history.ApplicationCache$$Lambda$592/2060065989.apply(Unknown Source)
	at org.apache.spark.deploy.history.ApplicationCache.time(ApplicationCache.scala:135)
	at org.apache.spark.deploy.history.ApplicationCache.org$apache$spark$deploy$history$ApplicationCache$$loadApplicationEntry(ApplicationCache.scala:161)
	at org.apache.spark.deploy.history.ApplicationCache$$anon$1.load(ApplicationCache.scala:56)
	at org.apache.spark.deploy.history.ApplicationCache$$anon$1.load(ApplicationCache.scala:52)
	at org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	at org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	- locked &amp;lt;0x00000007b5cd3de0&amp;gt; (a org.sparkproject.guava.cache.LocalCache$StrongAccessEntry)
	at org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.sparkproject.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.sparkproject.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.deploy.history.ApplicationCache.get(ApplicationCache.scala:89)
	at org.apache.spark.deploy.history.ApplicationCache.withSparkUI(ApplicationCache.scala:101)
	at org.apache.spark.deploy.history.HistoryServer.org$apache$spark$deploy$history$HistoryServer$$loadAppUi(HistoryServer.scala:245)
	at org.apache.spark.deploy.history.HistoryServer$$anon$1.doGet(HistoryServer.scala:98)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:873)
	at org.sparkproject.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1623)
	at org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)
	at org.sparkproject.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1610)
	at org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:540)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)
	at org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1345)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)
	at org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)
	at org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)
	at org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1247)
	at org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
	at org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:753)
	at org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:220)
	at org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
	at org.sparkproject.jetty.server.Server.handle(Server.java:505)
	at org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:370)
	at org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:267)
	at org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
	at org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.sparkproject.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
	at org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
	at org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
	at org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:698)
	at org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:804)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:748)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="13260029">SPARK-29322</key>
            <summary>History server is stuck reading incomplete event log file compressed with zstd</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="kabhwan">Jungtaek Lim</assignee>
                                    <reporter username="kabhwan">Jungtaek Lim</reporter>
                        <labels>
                    </labels>
                <created>Wed, 2 Oct 2019 02:49:11 +0000</created>
                <updated>Thu, 3 Oct 2019 13:31:57 +0000</updated>
                            <resolved>Thu, 3 Oct 2019 03:49:08 +0000</resolved>
                                    <version>3.0.0</version>
                                    <fixVersion>3.0.0</fixVersion>
                                    <component>Spark Core</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>2</watches>
                                                                                                                <comments>
                            <comment id="16942446" author="kabhwan" created="Wed, 2 Oct 2019 02:54:19 +0000"  >&lt;p&gt;FYI, thread being stuck was finished immediately when I stopped the application.&lt;/p&gt;</comment>
                            <comment id="16942456" author="kabhwan" created="Wed, 2 Oct 2019 03:22:52 +0000"  >&lt;p&gt;Just initiated discussion on dev. mailing list to see which approach is preferred to deal with this.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://lists.apache.org/thread.html/24f2c371a5859f2a31f305ef42c8aaed1b1c6add2239d6cf97f57203@%3Cdev.spark.apache.org%3E&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://lists.apache.org/thread.html/24f2c371a5859f2a31f305ef42c8aaed1b1c6add2239d6cf97f57203@%3Cdev.spark.apache.org%3E&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16942483" author="dongjoon" created="Wed, 2 Oct 2019 04:51:29 +0000"  >&lt;p&gt;Can we file and link an upstream issue Hadoop or ZSTD here? BTW, thanks!&lt;/p&gt;</comment>
                            <comment id="16942488" author="dongjoon" created="Wed, 2 Oct 2019 04:57:30 +0000"  >&lt;p&gt;Since this is ZSTD, you are using `hadoop-3.2` profile on the master branch with Hadoop 3.2.0, right? In addition to that, please update the reproducible procedure into the JIRA description. The content on the mailing list would be a good candidate.&lt;/p&gt;</comment>
                            <comment id="16942489" author="dongjoon" created="Wed, 2 Oct 2019 05:00:33 +0000"  >&lt;p&gt;Also, cc &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dbtsai&quot; class=&quot;user-hover&quot; rel=&quot;dbtsai&quot;&gt;dbtsai&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16942493" author="kabhwan" created="Wed, 2 Oct 2019 05:09:43 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Since this is ZSTD, you are using `hadoop-3.2` profile on the master branch with Hadoop 3.2.0, right?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Spark 2.x also has support for zstd, as well as I have been working without enabling Hadoop-3.2 for &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-28869&quot; title=&quot;Roll over event log files&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-28869&quot;&gt;&lt;del&gt;SPARK-28869&lt;/del&gt;&lt;/a&gt; so if I&apos;m not missing anything, I don&apos;t think it&apos;s related to `hadoop-3.2` profile.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;please update the reproducible procedure into the JIRA description.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Thanks for guiding! I&apos;ll update the description accordingly.&lt;/p&gt;</comment>
                            <comment id="16942502" author="dongjoon" created="Wed, 2 Oct 2019 05:34:38 +0000"  >&lt;p&gt;&lt;del&gt;AFAIK, we are using Hadoop ZStandardCodec which is added by &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-13578&quot; title=&quot;Add Codec for ZStandard Compression&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-13578&quot;&gt;&lt;del&gt;HADOOP-13578&lt;/del&gt;&lt;/a&gt; at 2.9.0 and 3.0.0.&lt;/del&gt; My bad. I checked the code path. You&apos;re right. I was confused with Parquet in SQL module.&lt;/p&gt;</comment>
                            <comment id="16943305" author="dongjoon" created="Thu, 3 Oct 2019 03:49:08 +0000"  >&lt;p&gt;Issue resolved by pull request 25996&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/25996&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/25996&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13202608">SPARK-26283</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12981940" name="history-server-1.jstack" size="29334" author="kabhwan" created="Wed, 2 Oct 2019 02:50:15 +0000"/>
                            <attachment id="12981941" name="history-server-2.jstack" size="28073" author="kabhwan" created="Wed, 2 Oct 2019 02:50:15 +0000"/>
                            <attachment id="12981942" name="history-server-3.jstack" size="29469" author="kabhwan" created="Wed, 2 Oct 2019 02:50:15 +0000"/>
                            <attachment id="12981943" name="history-server-4.jstack" size="29093" author="kabhwan" created="Wed, 2 Oct 2019 02:50:15 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>4.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 6 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z077sw:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>