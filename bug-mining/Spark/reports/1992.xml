<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:28:39 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-6411] PySpark DataFrames can&apos;t be created if any datetimes have timezones</title>
                <link>https://issues.apache.org/jira/browse/SPARK-6411</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;I am unable to create a DataFrame with PySpark if any of the &lt;tt&gt;datetime&lt;/tt&gt; objects that pass through the conversion process have a &lt;tt&gt;tzinfo&lt;/tt&gt; property set. &lt;/p&gt;

&lt;p&gt;This works fine:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;In [9]: sc.parallelize([(datetime.datetime(2014, 7, 8, 11, 10),)]).toDF().collect()
Out[9]: [Row(_1=datetime.datetime(2014, 7, 8, 11, 10))]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;as expected, the tuple&apos;s schema is inferred as having one anonymous column with a datetime field, and the datetime roundtrips through to the Java side python deserialization and then back into python land upon &lt;tt&gt;collect&lt;/tt&gt;. This however:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;In [5]: from dateutil.tz &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; tzutc

In [10]: sc.parallelize([(datetime.datetime(2014, 7, 8, 11, 10, tzinfo=tzutc()),)]).toDF().collect()
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;explodes with&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Py4JJavaError: An error occurred &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 12, localhost): net.razorvine.pickle.PickleException: invalid pickle data &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; datetime; expected 1 or 7 args, got 2
	at net.razorvine.pickle.objects.DateTimeConstructor.createDateTime(DateTimeConstructor.java:69)
	at net.razorvine.pickle.objects.DateTimeConstructor.construct(DateTimeConstructor.java:32)
	at net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:617)
	at net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:170)
	at net.razorvine.pickle.Unpickler.load(Unpickler.java:84)
	at net.razorvine.pickle.Unpickler.loads(Unpickler.java:97)
	at org.apache.spark.api.python.SerDeUtil$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(SerDeUtil.scala:154)
	at org.apache.spark.api.python.SerDeUtil$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(SerDeUtil.scala:153)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:119)
	at scala.collection.Iterator$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(Iterator.scala:727)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:114)
	at scala.collection.&lt;span class=&quot;code-keyword&quot;&gt;generic&lt;/span&gt;.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;to(TraversableOnce.scala:273)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.to(SerDeUtil.scala:114)
	at scala.collection.TraversableOnce$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;toBuffer(TraversableOnce.scala:265)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.toBuffer(SerDeUtil.scala:114)
	at scala.collection.TraversableOnce$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;toArray(TraversableOnce.scala:252)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.toArray(SerDeUtil.scala:114)
	at org.apache.spark.rdd.RDD$$anonfun$17.apply(RDD.scala:813)
	at org.apache.spark.rdd.RDD$$anonfun$17.apply(RDD.scala:813)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1520)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1520)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:744)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1211)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1200)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1199)
	at scala.collection.mutable.ResizableArray$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1199)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1401)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1362)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;By the looks of the error, it would appear as though the java depickler isn&apos;t expecting the pickle stream to provide that extra timezone constructor argument.&lt;/p&gt;

&lt;p&gt;Here&apos;s the disassembled pickle stream for a timezone-less datetime:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&amp;gt;&amp;gt;&amp;gt; object = datetime.datetime(2014, 7, 8, 11, 10)
&amp;gt;&amp;gt;&amp;gt; stream = pickle.dumps(object)
&amp;gt;&amp;gt;&amp;gt; pickletools.dis(stream)
    0: c    GLOBAL     &lt;span class=&quot;code-quote&quot;&gt;&apos;datetime datetime&apos;&lt;/span&gt;
   19: p    PUT        0
   22: (    MARK
   23: S        STRING     &lt;span class=&quot;code-quote&quot;&gt;&apos;\x07\xde\x07\x08\x0b\n\x00\x00\x00\x00&apos;&lt;/span&gt;
   65: p        PUT        1
   68: t        TUPLE      (MARK at 22)
   69: p    PUT        2
   72: R    REDUCE
   73: p    PUT        3
   76: .    STOP
highest protocol among opcodes = 0
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and then for one with a timezone:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&amp;gt;&amp;gt;&amp;gt; object = datetime.datetime(2014, 7, 8, 11, 10, tzinfo=tzutc())
&amp;gt;&amp;gt;&amp;gt; stream = pickle.dumps(object)
&amp;gt;&amp;gt;&amp;gt; pickletools.dis(stream)
    0: c    GLOBAL     &lt;span class=&quot;code-quote&quot;&gt;&apos;datetime datetime&apos;&lt;/span&gt;
   19: p    PUT        0
   22: (    MARK
   23: S        STRING     &lt;span class=&quot;code-quote&quot;&gt;&apos;\x07\xde\x07\x08\x0b\n\x00\x00\x00\x00&apos;&lt;/span&gt;
   65: p        PUT        1
   68: c        GLOBAL     &lt;span class=&quot;code-quote&quot;&gt;&apos;copy_reg _reconstructor&apos;&lt;/span&gt;
   93: p        PUT        2
   96: (        MARK
   97: c            GLOBAL     &lt;span class=&quot;code-quote&quot;&gt;&apos;dateutil.tz tzutc&apos;&lt;/span&gt;
  116: p            PUT        3
  119: c            GLOBAL     &lt;span class=&quot;code-quote&quot;&gt;&apos;datetime tzinfo&apos;&lt;/span&gt;
  136: p            PUT        4
  139: g            GET        4
  142: (            MARK
  143: t                TUPLE      (MARK at 142)
  144: R            REDUCE
  145: p            PUT        5
  148: t            TUPLE      (MARK at 96)
  149: p        PUT        6
  152: R        REDUCE
  153: p        PUT        7
  156: t        TUPLE      (MARK at 22)
  157: p    PUT        8
  160: R    REDUCE
  161: p    PUT        9
  164: .    STOP
highest protocol among opcodes = 0
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I would bet that the Pyrolite library is missing support for that nested object as a second tuple member in the reconstruction of the datetime object. Has anyone hit this before? Any more information I can provide?&lt;/p&gt;</description>
                <environment></environment>
        <key id="12783199">SPARK-6411</key>
            <summary>PySpark DataFrames can&apos;t be created if any datetimes have timezones</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="davies">Davies Liu</assignee>
                                    <reporter username="airhorns">Harry Brundage</reporter>
                        <labels>
                    </labels>
                <created>Thu, 19 Mar 2015 11:49:07 +0000</created>
                <updated>Thu, 11 Jun 2015 08:00:47 +0000</updated>
                            <resolved>Thu, 11 Jun 2015 08:00:47 +0000</resolved>
                                    <version>1.3.0</version>
                                    <fixVersion>1.5.0</fixVersion>
                                    <component>PySpark</component>
                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="14379947" author="airhorns" created="Wed, 25 Mar 2015 14:33:04 +0000"  >&lt;p&gt;I&apos;ve opened and issue on the upstream Pyrolite library which I think is causing this problem here: &lt;a href=&quot;https://github.com/irmen/Pyrolite/issues/19&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/irmen/Pyrolite/issues/19&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14526244" author="apachespark" created="Mon, 4 May 2015 05:04:02 +0000"  >&lt;p&gt;User &apos;mengxr&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/5850&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/5850&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14526291" author="mengxr" created="Mon, 4 May 2015 05:56:59 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=airhorns&quot; class=&quot;user-hover&quot; rel=&quot;airhorns&quot;&gt;airhorns&lt;/a&gt; I&apos;m testing Spark with Pyrolite master branch. With your patch, it is possible to create DFs with datetime that contains tzinfo. However, I found two issues:&lt;/p&gt;

&lt;p&gt;1. A tz-unaware date (or maybe datetime) object becomes tz-aware after a round trip, which makes `test_apply_schema` in `sql/tests.py` fail.&lt;br/&gt;
2. The tzinfo does not remain the same after a round trip.&lt;/p&gt;

&lt;p&gt;Test code:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    def test_datetime_with_timezone(self):
        &quot;&quot;&quot;
        SPARK-6411
        &quot;&quot;&quot;
        &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt;:
            &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; pytz
            has_pytz = True
        except:
            has_pytz = False

        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; has_pytz:
            tz = pytz.timezone(&lt;span class=&quot;code-quote&quot;&gt;&apos;America/Los_Angeles&apos;&lt;/span&gt;)
            date = datetime.datetime(2014, 7, 8, 11, 10, tzinfo=tz)
            first = self.sqlCtx.createDataFrame([(date,)]).first()[0]
            self.assertEqual(date, first)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Output:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;======================================================================
FAIL: test_datetime_with_timezone (__main__.SQLTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/Users/meng/src/spark/python/pyspark/sql/tests.py&quot;&lt;/span&gt;, line 536, in test_datetime_with_timezone
    self.assertEqual(date, first)
AssertionError: datetime.datetime(2014, 7, 8, 11, 10, tzinfo=&amp;lt;DstTzInfo &lt;span class=&quot;code-quote&quot;&gt;&apos;America/Los_Angeles&apos;&lt;/span&gt; PST-1 day, 16:00:00 STD&amp;gt;) != datetime.datetime(2014, 7, 8, 11, 10, tzinfo=&amp;lt;DstTzInfo &lt;span class=&quot;code-quote&quot;&gt;&apos;America/Los_Angeles&apos;&lt;/span&gt; PDT-1 day, 17:00:00 DST&amp;gt;)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I will check the conversion for date/datetime. It would be really helpful if you could provide insights.&lt;/p&gt;</comment>
                            <comment id="14546463" author="davies" created="Sat, 16 May 2015 01:01:55 +0000"  >&lt;p&gt;Since TimestampType in Spark SQL does not support timezone, there is no way to get the timezone back after a round trip. So we should drop the timezone for datetime before serializing (convert to UTC).&lt;/p&gt;

&lt;p&gt;I will send out a PR soon.&lt;/p&gt;</comment>
                            <comment id="14549576" author="apachespark" created="Tue, 19 May 2015 00:41:05 +0000"  >&lt;p&gt;User &apos;davies&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/6250&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/6250&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12826659">SPARK-7314</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 27 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i26z1j:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12332078">1.5.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>