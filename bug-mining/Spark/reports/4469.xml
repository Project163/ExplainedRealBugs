<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:50:40 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-18589] persist() resolves &quot;java.lang.RuntimeException: Invalid PythonUDF &lt;lambda&gt;(...), requires attributes from more than one child&quot;</title>
                <link>https://issues.apache.org/jira/browse/SPARK-18589</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Smells like another optimizer bug that&apos;s similar to &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-17100&quot; title=&quot;pyspark filter on a udf column after join gives java.lang.UnsupportedOperationException&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-17100&quot;&gt;&lt;del&gt;SPARK-17100&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-18254&quot; title=&quot;UDFs don&amp;#39;t see aliased column names&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-18254&quot;&gt;&lt;del&gt;SPARK-18254&lt;/del&gt;&lt;/a&gt;. I&apos;m seeing this on 2.0.2 and on master at commit &lt;tt&gt;fb07bbe575aabe68422fd3a31865101fb7fa1722&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;I don&apos;t have a minimal repro for this yet, but the error I&apos;m seeing is:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;py4j.protocol.Py4JJavaError: An error occurred &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; calling o247.count.
: java.lang.RuntimeException: Invalid PythonUDF &amp;lt;...&amp;gt;(...), requires attributes from more than one child.
    at scala.sys.&lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt;$.error(&lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt;.scala:27)
    at org.apache.spark.sql.execution.python.ExtractPythonUDFs$$anonfun$org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract$2.apply(ExtractPythonUDFs.scala:150)
    at org.apache.spark.sql.execution.python.ExtractPythonUDFs$$anonfun$org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract$2.apply(ExtractPythonUDFs.scala:149)
    at scala.collection.immutable.Stream.foreach(Stream.scala:594)
    at org.apache.spark.sql.execution.python.ExtractPythonUDFs$.org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract(ExtractPythonUDFs.scala:149)
    at org.apache.spark.sql.execution.python.ExtractPythonUDFs$$anonfun$apply$2.applyOrElse(ExtractPythonUDFs.scala:114)
    at org.apache.spark.sql.execution.python.ExtractPythonUDFs$$anonfun$apply$2.applyOrElse(ExtractPythonUDFs.scala:113)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$2.apply(TreeNode.scala:312)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$2.apply(TreeNode.scala:312)
    at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:311)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)
    at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:305)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)
    at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:305)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:305)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:328)
    at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:326)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:305)
    at org.apache.spark.sql.execution.python.ExtractPythonUDFs$.apply(ExtractPythonUDFs.scala:113)
    at org.apache.spark.sql.execution.python.ExtractPythonUDFs$.apply(ExtractPythonUDFs.scala:93)
    at org.apache.spark.sql.execution.QueryExecution$$anonfun$prepareForExecution$1.apply(QueryExecution.scala:93)
    at org.apache.spark.sql.execution.QueryExecution$$anonfun$prepareForExecution$1.apply(QueryExecution.scala:93)
    at scala.collection.LinearSeqOptimized$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foldLeft(LinearSeqOptimized.scala:124)
    at scala.collection.immutable.List.foldLeft(List.scala:84)
    at org.apache.spark.sql.execution.QueryExecution.prepareForExecution(QueryExecution.scala:93)
    at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:83)
    at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:83)
    at org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2555)
    at org.apache.spark.sql.Dataset.count(Dataset.scala:2226)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at py4j.Gateway.invoke(Gateway.java:280)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:214)
    at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The extended plan (cleaned of field names) is as follows:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;== Parsed Logical Plan ==
&lt;span class=&quot;code-quote&quot;&gt;&apos;Filter NOT (&apos;&lt;/span&gt;expected_prediction = &apos;prediction)
+- Project [p1, p2, pair_features, rawPrediction, probability, prediction, &lt;span class=&quot;code-keyword&quot;&gt;cast&lt;/span&gt;((p1._testing_universal_key = p2._testing_universal_key) as &lt;span class=&quot;code-object&quot;&gt;float&lt;/span&gt;) AS expected_prediction]
   +- Project [p1, p2, pair_features, rawPrediction, probability, UDF(rawPrediction) AS prediction]
      +- Project [p1, p2, pair_features, rawPrediction, UDF(rawPrediction) AS probability]
         +- Project [p1, p2, pair_features, UDF(pair_features) AS rawPrediction]
            +- Project [p1, p2, &amp;lt;lambda&amp;gt;(p1.person, p2.person) AS pair_features]
               +- Project [struct(...) AS p1, struct(...) AS p2]
                  +- Project [_blocking_key, ..., ...]
                     +- Join Inner, (_blocking_key = _blocking_key)
                        :- SubqueryAlias p1
                        :  +- Project [..., &amp;lt;lambda&amp;gt;(dataset_name, primary_key, person) AS _blocking_key]
                        :     +- Project [...]
                        :        +- Project [primary_key, universal_key, _testing_universal_key, struct(...) AS person]
                        :           +- Project [...]
                        :              +- Project [_testing_universal_key, primary_key, struct(...) AS person]
                        :                 +- LogicalRDD [...]
                        +- SubqueryAlias p2
                           +- Project [..., &amp;lt;lambda&amp;gt;(dataset_name, primary_key, person) AS _blocking_key]
                              +- Project [...]
                                 +- Project [primary_key, universal_key, _testing_universal_key, struct(...) AS person]
                                    +- Project [...]
                                       +- Project [_testing_universal_key, primary_key, struct(...) AS person]
                                          +- LogicalRDD [...]

== Analyzed Logical Plan ==
p1: struct&amp;lt;...&amp;gt;, p2: struct&amp;lt;...&amp;gt;, pair_features: vector, rawPrediction: vector, probability: vector, prediction: &lt;span class=&quot;code-object&quot;&gt;double&lt;/span&gt;, expected_prediction: &lt;span class=&quot;code-object&quot;&gt;float&lt;/span&gt;
Filter NOT (&lt;span class=&quot;code-keyword&quot;&gt;cast&lt;/span&gt;(expected_prediction as &lt;span class=&quot;code-object&quot;&gt;double&lt;/span&gt;) = prediction)
+- Project [p1, p2, pair_features, rawPrediction, probability, prediction, &lt;span class=&quot;code-keyword&quot;&gt;cast&lt;/span&gt;((p1._testing_universal_key = p2._testing_universal_key) as &lt;span class=&quot;code-object&quot;&gt;float&lt;/span&gt;) AS expected_prediction]
   +- Project [p1, p2, pair_features, rawPrediction, probability, UDF(rawPrediction) AS prediction]
      +- Project [p1, p2, pair_features, rawPrediction, UDF(rawPrediction) AS probability]
         +- Project [p1, p2, pair_features, UDF(pair_features) AS rawPrediction]
            +- Project [p1, p2, &amp;lt;lambda&amp;gt;(p1.person, p2.person) AS pair_features]
               +- Project [struct(...) AS p1, struct(...) AS p2]
                  +- Project [_blocking_key, ..., ...]
                     +- Join Inner, (_blocking_key = _blocking_key)
                        :- SubqueryAlias p1
                        :  +- Project [..., &amp;lt;lambda&amp;gt;(dataset_name, primary_key, person) AS _blocking_key]
                        :     +- Project [...]
                        :        +- Project [primary_key, universal_key, _testing_universal_key, struct(...) AS person]
                        :           +- Project [...]
                        :              +- Project [_testing_universal_key, primary_key, struct(...) AS person]
                        :                 +- LogicalRDD [...]
                        +- SubqueryAlias p2
                           +- Project [..., &amp;lt;lambda&amp;gt;(dataset_name, primary_key, person) AS _blocking_key]
                              +- Project [...]
                                 +- Project [primary_key, universal_key, _testing_universal_key, struct(...) AS person]
                                    +- Project [...]
                                       +- Project [_testing_universal_key, primary_key, struct(...) AS person]
                                          +- LogicalRDD [...]

== Optimized Logical Plan ==
Project [struct(...) AS p1, struct(...) AS p2, &amp;lt;lambda&amp;gt;(struct(...).person, struct(...).person) AS pair_features, UDF(&amp;lt;lambda&amp;gt;(struct(...).person, struct(...).person)) AS rawPrediction, UDF(UDF(&amp;lt;lambda&amp;gt;(struct(...).person, struct(...).person))) AS probability, UDF(UDF(&amp;lt;lambda&amp;gt;(struct(...).person, struct(...).person))) AS prediction, &lt;span class=&quot;code-keyword&quot;&gt;cast&lt;/span&gt;((struct(...)._testing_universal_key = struct(...)._testing_universal_key) as &lt;span class=&quot;code-object&quot;&gt;float&lt;/span&gt;) AS expected_prediction]
+- Join Inner, (NOT (&lt;span class=&quot;code-keyword&quot;&gt;cast&lt;/span&gt;(&lt;span class=&quot;code-keyword&quot;&gt;cast&lt;/span&gt;((struct(...)._testing_universal_key = struct(...)._testing_universal_key) as &lt;span class=&quot;code-object&quot;&gt;float&lt;/span&gt;) as &lt;span class=&quot;code-object&quot;&gt;double&lt;/span&gt;) = UDF(UDF(&amp;lt;lambda&amp;gt;(struct(...).person, struct(...).person)))) &amp;amp;&amp;amp; (_blocking_key = _blocking_key))
   :- Project [..., &amp;lt;lambda&amp;gt;(dataset_name, primary_key, person) AS _blocking_key]
   :  +- Filter isnotnull(&amp;lt;lambda&amp;gt;(dataset_name, primary_key, person))
   :     +- InMemoryRelation [...], &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, 10000, StorageLevel(memory, 1 replicas)
   :        :  +- *Project [primary_key, struct(...) AS person, test_people AS dataset_name]
   :        :     +- Scan ExistingRDD[...]
   +- Project [..., &amp;lt;lambda&amp;gt;(dataset_name, primary_key, person) AS _blocking_key]
      +- Filter isnotnull(&amp;lt;lambda&amp;gt;(dataset_name, primary_key, person))
         +- InMemoryRelation [...], &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, 10000, StorageLevel(memory, 1 replicas)
            :  +- *Project [primary_key, struct(...) AS person, test_people AS dataset_name]
            :     +- Scan ExistingRDD[...]

== Physical Plan ==
java.lang.RuntimeException: Invalid PythonUDF &amp;lt;lambda&amp;gt;(struct(...).person, struct(...).person), requires attributes from more than one child.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note the error at the end when Spark tries to print the physical plan. I&apos;ve scrubbed some Project fields from the plan to simplify the display, but if I&apos;ve scrubbed anything you think is important let me know.&lt;/p&gt;

&lt;p&gt;I can get around this problem by adding a &lt;tt&gt;persist()&lt;/tt&gt; right before the operation that fails. The failing operation is a filter.&lt;/p&gt;

&lt;p&gt;Any clues on how I can boil this down to a minimal repro? Any clues about where the problem is?&lt;/p&gt;</description>
                <environment>&lt;p&gt;Python 3.5, Java 8&lt;/p&gt;</environment>
        <key id="13023447">SPARK-18589</key>
            <summary>persist() resolves &quot;java.lang.RuntimeException: Invalid PythonUDF &lt;lambda&gt;(...), requires attributes from more than one child&quot;</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="davies">Davies Liu</assignee>
                                    <reporter username="nchammas">Nicholas Chammas</reporter>
                        <labels>
                    </labels>
                <created>Fri, 25 Nov 2016 21:27:13 +0000</created>
                <updated>Thu, 23 Mar 2017 14:06:01 +0000</updated>
                            <resolved>Sat, 21 Jan 2017 00:12:53 +0000</resolved>
                                    <version>2.0.2</version>
                    <version>2.1.0</version>
                                    <fixVersion>2.1.1</fixVersion>
                    <fixVersion>2.2.0</fixVersion>
                                    <component>PySpark</component>
                    <component>SQL</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="15696717" author="nchammas" created="Fri, 25 Nov 2016 21:28:31 +0000"  >&lt;p&gt;cc &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=davies&quot; class=&quot;user-hover&quot; rel=&quot;davies&quot;&gt;davies&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hvanhovell&quot; class=&quot;user-hover&quot; rel=&quot;hvanhovell&quot;&gt;hvanhovell&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15761741" author="franklyndsouza" created="Mon, 19 Dec 2016 17:20:38 +0000"  >&lt;p&gt;The sequence of steps that causes this are:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;join two dataframes A and B &amp;gt; make a udf that uses one column from A and another from B &amp;gt; filter on column produced by udf &amp;gt; java.lang.RuntimeException: Invalid PythonUDF &amp;lt;lambda&amp;gt;(b#1L, c#6L), requires attributes from more than one child.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here are some minimum steps to reproduce this issue in pyspark&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;from pyspark.sql &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; types
from pyspark.sql &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; functions as F
df1 = sqlCtx.createDataFrame([types.Row(a=1, b=2), types.Row(a=1, b=4)])
df2 = sqlCtx.createDataFrame([types.Row(a=1, c=12)])
joined = df1.join(df2, df1[&lt;span class=&quot;code-quote&quot;&gt;&apos;a&apos;&lt;/span&gt;] == df2[&lt;span class=&quot;code-quote&quot;&gt;&apos;a&apos;&lt;/span&gt;])
extra = joined.withColumn(&lt;span class=&quot;code-quote&quot;&gt;&apos;sum&apos;&lt;/span&gt;, F.udf(lambda a,b : a+b, types.IntegerType())(joined[&lt;span class=&quot;code-quote&quot;&gt;&apos;b&apos;&lt;/span&gt;], joined[&lt;span class=&quot;code-quote&quot;&gt;&apos;c&apos;&lt;/span&gt;]))
filtered = extra.where(extra[&lt;span class=&quot;code-quote&quot;&gt;&apos;sum&apos;&lt;/span&gt;] &amp;lt; F.lit(10)).collect()
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;b&gt;doing extra.cache() before the filtering will fix the issue&lt;/b&gt; but obviously isn&apos;t a solution.&lt;/p&gt;</comment>
                            <comment id="15822278" author="apachespark" created="Fri, 13 Jan 2017 20:05:01 +0000"  >&lt;p&gt;User &apos;davies&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/16581&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/16581&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="13046036">SPARK-19728</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12997803">SPARK-17100</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13017698">SPARK-18254</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 44 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i36smv:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>