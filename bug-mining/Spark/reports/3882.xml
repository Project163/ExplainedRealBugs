<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:46:22 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-16409] regexp_extract with optional groups causes NPE</title>
                <link>https://issues.apache.org/jira/browse/SPARK-16409</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;df = sqlContext.createDataFrame([&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;aaaac&amp;#39;&amp;#93;&lt;/span&gt;], &lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;s&amp;#39;&amp;#93;&lt;/span&gt;)&lt;br/&gt;
df.select(F.regexp_extract(&apos;s&apos;, r&apos;(a+)(b)?(c)&apos;, 2)).collect()&lt;/p&gt;

&lt;p&gt;causes NPE. Worse, in a large program it doesn&apos;t cause NPE instantly; it actually works fine, until some unpredictable (and inconsistent) moment in the future when (presumably) the invalid memory access occurs, and then it fails. For this reason, it took several hours to debug this.&lt;/p&gt;

&lt;p&gt;Suggestion: either fill the group with null; or raise exception immediately after examining the argument with a message that optional groups are not allowed.&lt;/p&gt;

&lt;p&gt;Traceback:&lt;/p&gt;

&lt;p&gt;---------------------------------------------------------------------------&lt;br/&gt;
Py4JJavaError                             Traceback (most recent call last)&lt;br/&gt;
&amp;lt;ipython-input-8-825292b569fc&amp;gt; in &amp;lt;module&amp;gt;()&lt;br/&gt;
----&amp;gt; 1 df.select(F.regexp_extract(&apos;s&apos;, r&apos;(a+)(b)?(c)&apos;, 2)).collect()&lt;/p&gt;

&lt;p&gt;C:\Users\me\Downloads\spark-2.0.0-preview-bin-hadoop2.7\python\pyspark\sql\dataframe.py in collect(self)&lt;br/&gt;
    294         &quot;&quot;&quot;&lt;br/&gt;
    295         with SCCallSiteSync(self._sc) as css:&lt;br/&gt;
--&amp;gt; 296             port = self._jdf.collectToPython()&lt;br/&gt;
    297         return list(_load_from_socket(port, BatchedSerializer(PickleSerializer())))&lt;br/&gt;
    298 &lt;/p&gt;

&lt;p&gt;C:\Users\me\Downloads\spark-2.0.0-preview-bin-hadoop2.7\python\lib\py4j-0.10.1-src.zip\py4j\java_gateway.py in _&lt;em&gt;call&lt;/em&gt;_(self, *args)&lt;br/&gt;
    931         answer = self.gateway_client.send_command(command)&lt;br/&gt;
    932         return_value = get_return_value(&lt;br/&gt;
--&amp;gt; 933             answer, self.gateway_client, self.target_id, self.name)&lt;br/&gt;
    934 &lt;br/&gt;
    935         for temp_arg in temp_args:&lt;/p&gt;

&lt;p&gt;C:\Users\me\Downloads\spark-2.0.0-preview-bin-hadoop2.7\python\pyspark\sql\utils.py in deco(*a, **kw)&lt;br/&gt;
     55     def deco(*a, **kw):&lt;br/&gt;
     56         try:&lt;br/&gt;
---&amp;gt; 57             return f(*a, **kw)&lt;br/&gt;
     58         except py4j.protocol.Py4JJavaError as e:&lt;br/&gt;
     59             s = e.java_exception.toString()&lt;/p&gt;

&lt;p&gt;C:\Users\me\Downloads\spark-2.0.0-preview-bin-hadoop2.7\python\lib\py4j-0.10.1-src.zip\py4j\protocol.py in get_return_value(answer, gateway_client, target_id, name)&lt;br/&gt;
    310                 raise Py4JJavaError(&lt;br/&gt;
    311                     &quot;An error occurred while calling &lt;/p&gt;
{0}
{1}
{2}
&lt;p&gt;.\n&quot;.&lt;br/&gt;
--&amp;gt; 312                     format(target_id, &quot;.&quot;, name), value)&lt;br/&gt;
    313             else:&lt;br/&gt;
    314                 raise Py4JError(&lt;/p&gt;

&lt;p&gt;Py4JJavaError: An error occurred while calling o51.collectToPython.&lt;br/&gt;
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.NullPointerException&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter.write(UnsafeRowWriter.java:210)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)&lt;br/&gt;
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)&lt;br/&gt;
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$7$$anon$1.hasNext(WholeStageCodegenExec.scala:357)&lt;br/&gt;
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)&lt;br/&gt;
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)&lt;br/&gt;
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)&lt;br/&gt;
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)&lt;br/&gt;
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)&lt;br/&gt;
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)&lt;br/&gt;
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)&lt;br/&gt;
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)&lt;br/&gt;
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.to(SerDeUtil.scala:112)&lt;br/&gt;
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)&lt;br/&gt;
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.toBuffer(SerDeUtil.scala:112)&lt;br/&gt;
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)&lt;br/&gt;
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.toArray(SerDeUtil.scala:112)&lt;br/&gt;
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:883)&lt;br/&gt;
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:883)&lt;br/&gt;
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1889)&lt;br/&gt;
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1889)&lt;br/&gt;
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)&lt;br/&gt;
	at org.apache.spark.scheduler.Task.run(Task.scala:85)&lt;br/&gt;
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:745)&lt;/p&gt;

&lt;p&gt;Driver stacktrace:&lt;br/&gt;
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)&lt;br/&gt;
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)&lt;br/&gt;
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)&lt;br/&gt;
	at scala.Option.foreach(Option.scala:257)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)&lt;br/&gt;
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)&lt;br/&gt;
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1863)&lt;br/&gt;
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1876)&lt;br/&gt;
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1889)&lt;br/&gt;
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1903)&lt;br/&gt;
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:883)&lt;br/&gt;
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;br/&gt;
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:357)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.collect(RDD.scala:882)&lt;br/&gt;
	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)&lt;br/&gt;
	at org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply$mcI$sp(Dataset.scala:2417)&lt;br/&gt;
	at org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2417)&lt;br/&gt;
	at org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2417)&lt;br/&gt;
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)&lt;br/&gt;
	at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2436)&lt;br/&gt;
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:2416)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:498)&lt;br/&gt;
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)&lt;br/&gt;
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)&lt;br/&gt;
	at py4j.Gateway.invoke(Gateway.java:280)&lt;br/&gt;
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)&lt;br/&gt;
	at py4j.commands.CallCommand.execute(CallCommand.java:79)&lt;br/&gt;
	at py4j.GatewayConnection.run(GatewayConnection.java:211)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
Caused by: java.lang.NullPointerException&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter.write(UnsafeRowWriter.java:210)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)&lt;br/&gt;
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)&lt;br/&gt;
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$7$$anon$1.hasNext(WholeStageCodegenExec.scala:357)&lt;br/&gt;
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)&lt;br/&gt;
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:117)&lt;br/&gt;
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)&lt;br/&gt;
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)&lt;br/&gt;
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)&lt;br/&gt;
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)&lt;br/&gt;
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)&lt;br/&gt;
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)&lt;br/&gt;
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.to(SerDeUtil.scala:112)&lt;br/&gt;
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)&lt;br/&gt;
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.toBuffer(SerDeUtil.scala:112)&lt;br/&gt;
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)&lt;br/&gt;
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.toArray(SerDeUtil.scala:112)&lt;br/&gt;
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:883)&lt;br/&gt;
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:883)&lt;br/&gt;
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1889)&lt;br/&gt;
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1889)&lt;br/&gt;
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)&lt;br/&gt;
	at org.apache.spark.scheduler.Task.run(Task.scala:85)&lt;br/&gt;
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
	... 1 more&lt;/p&gt;</description>
                <environment></environment>
        <key id="12987386">SPARK-16409</key>
            <summary>regexp_extract with optional groups causes NPE</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="srowen">Sean R. Owen</assignee>
                                    <reporter username="mmoroz">Max Moroz</reporter>
                        <labels>
                    </labels>
                <created>Thu, 7 Jul 2016 06:51:20 +0000</created>
                <updated>Sun, 7 Aug 2016 11:20:48 +0000</updated>
                            <resolved>Sun, 7 Aug 2016 11:20:35 +0000</resolved>
                                    <version>2.0.0</version>
                                    <fixVersion>1.6.3</fixVersion>
                    <fixVersion>2.0.1</fixVersion>
                    <fixVersion>2.1.0</fixVersion>
                                    <component>Spark Core</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="15365764" author="srowen" created="Thu, 7 Jul 2016 07:49:36 +0000"  >&lt;p&gt;We&apos;re missing the stack trace here? that&apos;s an important piece of info. Also what is your data like that triggers this, if possible?&lt;/p&gt;</comment>
                            <comment id="15368059" author="mmoroz" created="Fri, 8 Jul 2016 17:49:52 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=srowen&quot; class=&quot;user-hover&quot; rel=&quot;srowen&quot;&gt;srowen&lt;/a&gt; So sorry I was sure I copied the entire code. I&apos;m gonna update the issue with the full details.&lt;/p&gt;</comment>
                            <comment id="15407972" author="mmoroz" created="Thu, 4 Aug 2016 15:45:24 +0000"  >&lt;p&gt;Still causes NPE on the newly released Spark 2.0.0.&lt;/p&gt;</comment>
                            <comment id="15408607" author="rxin" created="Thu, 4 Aug 2016 22:47:57 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=srowen&quot; class=&quot;user-hover&quot; rel=&quot;srowen&quot;&gt;srowen&lt;/a&gt; do you have time to fix this one?&lt;/p&gt;</comment>
                            <comment id="15408894" author="srowen" created="Fri, 5 Aug 2016 05:23:31 +0000"  >&lt;p&gt;It&apos;s not quite my area, but I might know the answer. As to occurring &quot;randomly&quot;, I suspect you mean that your example causes it immediately but in a large program, where an action isn&apos;t invoked until much later, it would execute and manifest when this statement was executed. That&apos;s normal for Spark programs, given the architecture.&lt;/p&gt;

&lt;p&gt;In this case the matching group is missing. Looking at this implementation, it seems like this could be a problem already in &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;  override def nullSafeEval(s: Any, p: Any, r: Any): Any = {
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!p.equals(lastRegex)) {
      &lt;span class=&quot;code-comment&quot;&gt;// regex value changed
&lt;/span&gt;      lastRegex = p.asInstanceOf[UTF8String].clone()
      pattern = Pattern.compile(lastRegex.toString)
    }
    val m = pattern.matcher(s.toString)
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (m.find) {
      val mr: MatchResult = m.toMatchResult
      UTF8String.fromString(mr.group(r.asInstanceOf[Int]))
    } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
      UTF8String.EMPTY_UTF8
    }
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;mr.group() returns null in this case and so the whole method does. Seems like it&apos;s not supposed to do that. I&apos;ll table a PR?&lt;/p&gt;</comment>
                            <comment id="15408961" author="apachespark" created="Fri, 5 Aug 2016 06:23:06 +0000"  >&lt;p&gt;User &apos;srowen&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/14504&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/14504&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15410916" author="srowen" created="Sun, 7 Aug 2016 11:20:35 +0000"  >&lt;p&gt;Issue resolved by pull request 14504&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/14504&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/14504&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12985667">SPARK-16324</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 15 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i30mwf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>