<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:15:04 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-2403] Spark stuck when class is not registered with Kryo</title>
                <link>https://issues.apache.org/jira/browse/SPARK-2403</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;We are using Kryo and require registering classes. When trying to serialize something containing an unregistered class, Kryo will raise an exception.&lt;/p&gt;

&lt;p&gt;DAGScheduler.submitMissingTasks runs in the scheduler thread and checks if the contents of the task can be serialized by trying to serialize it:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/spark/blob/v1.0.0/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L767&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/v1.0.0/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L767&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It catches NotSerializableException and aborts the task with an error when this happens.&lt;/p&gt;

&lt;p&gt;The problem is, Kryo does not raise NotSerializableException for unregistered classes. It raises IllegalArgumentException instead. This exception is not caught and kills the scheduler thread. The application then hangs, waiting indefinitely for the job to finish.&lt;/p&gt;

&lt;p&gt;Catching IllegalArgumentException also is a quick fix. I&apos;ll send a pull request for it if you agree. Thanks!&lt;/p&gt;</description>
                <environment></environment>
        <key id="12726064">SPARK-2403</key>
            <summary>Spark stuck when class is not registered with Kryo</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="darabos">Daniel Darabos</reporter>
                        <labels>
                    </labels>
                <created>Tue, 8 Jul 2014 12:41:37 +0000</created>
                <updated>Mon, 10 Nov 2014 02:07:22 +0000</updated>
                            <resolved>Tue, 8 Jul 2014 17:45:42 +0000</resolved>
                                    <version>1.0.0</version>
                                    <fixVersion>1.0.2</fixVersion>
                    <fixVersion>1.1.0</fixVersion>
                                    <component>Spark Core</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="14054907" author="darabos" created="Tue, 8 Jul 2014 12:58:18 +0000"  >&lt;p&gt;I think DAGSchedulerActorSupervisor is supposed to kill the system when things like this happen. I am not sure why that does not happen in this case.&lt;/p&gt;

&lt;p&gt;I forgot to include the stack trace:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;java.lang.IllegalArgumentException: Class is not registered: scala.collection.immutable.Range
Note: To register this class use: kryo.register(scala.collection.immutable.Range.class);
        at com.esotericsoftware.kryo.Kryo.getRegistration(Kryo.java:442) ~[kryo-2.21.jar:na]
        at com.esotericsoftware.kryo.util.DefaultClassResolver.writeClass(DefaultClassResolver.java:79) ~[kryo-2.21.jar:na]
        at com.esotericsoftware.kryo.Kryo.writeClass(Kryo.java:472) ~[kryo-2.21.jar:na]
        at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:565) ~[kryo-2.21.jar:na]
        at org.apache.spark.serializer.KryoSerializationStream.writeObject(KryoSerializer.scala:101) ~[spark-core_2.10-1.0.0.jar:1.0.0]
        at org.apache.spark.rdd.ParallelCollectionPartition$$anonfun$writeObject$1.apply(ParallelCollectionRDD.scala:65) ~[spark-core_2.10-1.0.0.jar:1.0.0]
        at org.apache.spark.rdd.ParallelCollectionPartition$$anonfun$writeObject$1.apply(ParallelCollectionRDD.scala:65) ~[spark-core_2.10-1.0.0.jar:1.0.0]
        at org.apache.spark.util.Utils$.serializeViaNestedStream(Utils.scala:105) ~[spark-core_2.10-1.0.0.jar:1.0.0]
        at org.apache.spark.rdd.ParallelCollectionPartition.writeObject(ParallelCollectionRDD.scala:65) ~[spark-core_2.10-1.0.0.jar:1.0.0]
        at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source) ~[na:na]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.7.0_55]
        at java.lang.reflect.Method.invoke(Method.java:606) ~[na:1.7.0_55]
        at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988) ~[na:1.7.0_55]
        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495) ~[na:1.7.0_55]
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431) ~[na:1.7.0_55]
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177) ~[na:1.7.0_55]
        at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347) ~[na:1.7.0_55]
        at org.apache.spark.scheduler.ShuffleMapTask.writeExternal(ShuffleMapTask.scala:126) ~[spark-core_2.10-1.0.0.jar:1.0.0]
        at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1458) ~[na:1.7.0_55]
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1429) ~[na:1.7.0_55]
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177) ~[na:1.7.0_55]
        at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347) ~[na:1.7.0_55]
        at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:42) ~[spark-core_2.10-1.0.0.jar:1.0.0]
        at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:71) ~[spark-core_2.10-1.0.0.jar:1.0.0]
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitMissingTasks(DAGScheduler.scala:767) ~[spark-core_2.10-1.0.0.jar:1.0.0]
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:713) ~[spark-core_2.10-1.0.0.jar:1.0.0]
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$submitStage$4.apply(DAGScheduler.scala:717) ~[spark-core_2.10-1.0.0.jar:1.0.0]
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$submitStage$4.apply(DAGScheduler.scala:716) ~[spark-core_2.10-1.0.0.jar:1.0.0]
        at scala.collection.immutable.List.foreach(List.scala:318) ~[scala-library-2.10.4.jar:na]
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:716) ~[spark-core_2.10-1.0.0.jar:1.0.0]
        at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:697) ~[spark-core_2.10-1.0.0.jar:1.0.0]
        at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1176) ~[spark-core_2.10-1.0.0.jar:1.0.0]
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498) [akka-actor_2.10-2.2.3.jar:2.2.3]
        at akka.actor.ActorCell.invoke(ActorCell.scala:456) [akka-actor_2.10-2.2.3.jar:2.2.3]
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237) [akka-actor_2.10-2.2.3.jar:2.2.3]
        at akka.dispatch.Mailbox.run(Mailbox.scala:219) [akka-actor_2.10-2.2.3.jar:2.2.3]
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386) [akka-actor_2.10-2.2.3.jar:2.2.3]
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.10.4.jar:na]
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.10.4.jar:na]
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.10.4.jar:na]
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.10.4.jar:na]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="14204237" author="kadiyalakc" created="Mon, 10 Nov 2014 02:07:22 +0000"  >&lt;p&gt;I still get this error (deploying on YARN cluster). Built from source (date git repo cloned - 11/9/2014). &lt;/p&gt;

&lt;p&gt;mvn -Pyarn -Pdeb -Phadoop-2.4 -Dhadoop.version=2.4.1 -DskipTests clean package install&lt;/p&gt;

&lt;p&gt;Here is the error trace:&lt;/p&gt;

&lt;p&gt;2014-11-08 14:58:21,134 INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;sparkDriver-akka.actor.default-dispatcher-3&amp;#93;&lt;/span&gt; spark.SparkContext (Logging.scala:logInfo(59)) - Created broadcast 0 from broadcast at DAGScheduler.scala:838&lt;br/&gt;
2014-11-08 14:58:21,158 INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;sparkDriver-akka.actor.default-dispatcher-3&amp;#93;&lt;/span&gt; cluster.YarnClusterScheduler (Logging.scala:logInfo(59)) - Cancelling stage 0&lt;br/&gt;
2014-11-08 14:58:21,163 INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;Driver&amp;#93;&lt;/span&gt; scheduler.DAGScheduler (Logging.scala:logInfo(59)) - Job 0 failed: reduce at SparkPi.scala:35, took 0.425176 s&lt;br/&gt;
2014-11-08 14:58:21,170 INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;Driver&amp;#93;&lt;/span&gt; yarn.ApplicationMaster (Logging.scala:logInfo(59)) - Final app status: FAILED, exitCode: 15, (reason: User class threw exception: Job aborted due to stage failure: Task serialization failed: java.io.IOException: java.lang.IllegalArgumentException: Class is not registered: scala.collection.immutable.Range&lt;br/&gt;
Note: To register this class use: kryo.register(scala.collection.immutable.Range.class);&lt;br/&gt;
org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:988)&lt;br/&gt;
org.apache.spark.rdd.ParallelCollectionPartition.writeObject(ParallelCollectionRDD.scala:51)&lt;br/&gt;
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)&lt;br/&gt;
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
java.lang.reflect.Method.invoke(Method.java:606)&lt;br/&gt;
java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)&lt;br/&gt;
java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)&lt;br/&gt;
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)&lt;br/&gt;
java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)&lt;br/&gt;
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)&lt;br/&gt;
java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)&lt;br/&gt;
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)&lt;br/&gt;
java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)&lt;br/&gt;
java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)&lt;br/&gt;
org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:42)&lt;br/&gt;
org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:73)&lt;br/&gt;
org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitMissingTasks(DAGScheduler.scala:876)&lt;br/&gt;
org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:778)&lt;br/&gt;
org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:762)&lt;br/&gt;
org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1381)&lt;br/&gt;
akka.actor.Actor$class.aroundReceive(Actor.scala:465)&lt;br/&gt;
org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DAGScheduler.scala:1367)&lt;br/&gt;
akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)&lt;br/&gt;
akka.actor.ActorCell.invoke(ActorCell.scala:487)&lt;br/&gt;
akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)&lt;br/&gt;
akka.dispatch.Mailbox.run(Mailbox.scala:220)&lt;br/&gt;
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)&lt;br/&gt;
scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)&lt;br/&gt;
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)&lt;br/&gt;
scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)&lt;br/&gt;
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)&lt;br/&gt;
)&lt;br/&gt;
2014-11-08 14:58:31,722 INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; ipc.Client (Client.java:handleConnectionFailure(841)) - Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)&lt;br/&gt;
2014-11-08 14:58:32,724 INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; ipc.Client (Client.java:handleConnectionFailure(841)) - Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)&lt;br/&gt;
2014-11-08 14:58:33,726 INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; ipc.Client (Client.java:handleConnectionFailure(841)) - Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>404171</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            11 years, 2 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1xk5r:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>404211</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>