<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:43:28 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-15417] Failed to enable hive support in PySpark shell</title>
                <link>https://issues.apache.org/jira/browse/SPARK-15417</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Unable to use Hive meta-store in pyspark shell. Tried both HiveContext and SparkSession. Both failed. It always uses in-memory catalog.&lt;/p&gt;

&lt;p&gt;Method 1: Using SparkSession&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; from pyspark.sql import SparkSession

&amp;gt;&amp;gt;&amp;gt; spark = SparkSession.builder.enableHiveSupport().getOrCreate()

&amp;gt;&amp;gt;&amp;gt; spark.sql(&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING)&quot;)

DataFrame[]

&amp;gt;&amp;gt;&amp;gt; spark.sql(&quot;LOAD DATA LOCAL INPATH &apos;examples/src/main/resources/kv1.txt&apos; INTO TABLE src&quot;)

Traceback (most recent call last):

  File &quot;&amp;lt;stdin&amp;gt;&quot;, line 1, in &amp;lt;module&amp;gt;

  File &quot;/Users/xiaoli/IdeaProjects/sparkDelivery/python/pyspark/sql/session.py&quot;, line 494, in sql

    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)

  File &quot;/Users/xiaoli/IdeaProjects/sparkDelivery/python/lib/py4j-0.10.1-src.zip/py4j/java_gateway.py&quot;, line 933, in __call__

  File &quot;/Users/xiaoli/IdeaProjects/sparkDelivery/python/pyspark/sql/utils.py&quot;, line 57, in deco

    return f(*a, **kw)

  File &quot;/Users/xiaoli/IdeaProjects/sparkDelivery/python/lib/py4j-0.10.1-src.zip/py4j/protocol.py&quot;, line 312, in get_return_value

py4j.protocol.Py4JJavaError: An error occurred while calling o21.sql.

: java.lang.UnsupportedOperationException: loadTable is not implemented

at org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.loadTable(InMemoryCatalog.scala:297)

at org.apache.spark.sql.catalyst.catalog.SessionCatalog.loadTable(SessionCatalog.scala:280)

at org.apache.spark.sql.execution.command.LoadData.run(tables.scala:263)

at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:57)

at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:55)

at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:69)

at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)

at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)

at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)

at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)

at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)

at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)

at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:85)

at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:85)

at org.apache.spark.sql.Dataset.&amp;lt;init&amp;gt;(Dataset.scala:187)

at org.apache.spark.sql.Dataset.&amp;lt;init&amp;gt;(Dataset.scala:168)

at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:63)

at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:541)

at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)

at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

at java.lang.reflect.Method.invoke(Method.java:606)

at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)

at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)

at py4j.Gateway.invoke(Gateway.java:280)

at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)

at py4j.commands.CallCommand.execute(CallCommand.java:79)

at py4j.GatewayConnection.run(GatewayConnection.java:211)

at java.lang.Thread.run(Thread.java:745)

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;Method 2: Using HiveContext: &lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;&amp;gt;&amp;gt;&amp;gt; from pyspark.sql import HiveContext

&amp;gt;&amp;gt;&amp;gt; sqlContext = HiveContext(sc)

&amp;gt;&amp;gt;&amp;gt; sqlContext.sql(&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING)&quot;)

DataFrame[]

&amp;gt;&amp;gt;&amp;gt; sqlContext.sql(&quot;LOAD DATA LOCAL INPATH &apos;examples/src/main/resources/kv1.txt&apos; INTO TABLE src&quot;)

Traceback (most recent call last):

  File &quot;&amp;lt;stdin&amp;gt;&quot;, line 1, in &amp;lt;module&amp;gt;

  File &quot;/Users/xiaoli/IdeaProjects/sparkDelivery/python/pyspark/sql/context.py&quot;, line 346, in sql

    return self.sparkSession.sql(sqlQuery)

  File &quot;/Users/xiaoli/IdeaProjects/sparkDelivery/python/pyspark/sql/session.py&quot;, line 494, in sql

    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)

  File &quot;/Users/xiaoli/IdeaProjects/sparkDelivery/python/lib/py4j-0.10.1-src.zip/py4j/java_gateway.py&quot;, line 933, in __call__

  File &quot;/Users/xiaoli/IdeaProjects/sparkDelivery/python/pyspark/sql/utils.py&quot;, line 57, in deco

    return f(*a, **kw)

  File &quot;/Users/xiaoli/IdeaProjects/sparkDelivery/python/lib/py4j-0.10.1-src.zip/py4j/protocol.py&quot;, line 312, in get_return_value

py4j.protocol.Py4JJavaError: An error occurred while calling o21.sql.

: java.lang.UnsupportedOperationException: loadTable is not implemented

at org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.loadTable(InMemoryCatalog.scala:297)

at org.apache.spark.sql.catalyst.catalog.SessionCatalog.loadTable(SessionCatalog.scala:280)

at org.apache.spark.sql.execution.command.LoadData.run(tables.scala:263)

at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:57)

at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:55)

at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:69)

at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)

at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)

at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)

at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)

at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)

at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)

at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:85)

at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:85)

at org.apache.spark.sql.Dataset.&amp;lt;init&amp;gt;(Dataset.scala:187)

at org.apache.spark.sql.Dataset.&amp;lt;init&amp;gt;(Dataset.scala:168)

at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:63)

at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:541)

at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)

at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

at java.lang.reflect.Method.invoke(Method.java:606)

at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)

at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)

at py4j.Gateway.invoke(Gateway.java:280)

at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)

at py4j.commands.CallCommand.execute(CallCommand.java:79)

at py4j.GatewayConnection.run(GatewayConnection.java:211)

at java.lang.Thread.run(Thread.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="12971293">SPARK-15417</key>
            <summary>Failed to enable hive support in PySpark shell</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="andrewor14">Andrew Or</assignee>
                                    <reporter username="smilegator">Xiao Li</reporter>
                        <labels>
                    </labels>
                <created>Thu, 19 May 2016 20:35:04 +0000</created>
                <updated>Fri, 20 May 2016 18:16:53 +0000</updated>
                            <resolved>Fri, 20 May 2016 06:44:31 +0000</resolved>
                                    <version>2.0.0</version>
                                    <fixVersion>2.0.0</fixVersion>
                                    <component>PySpark</component>
                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="15292159" author="andrewor14" created="Thu, 19 May 2016 21:19:23 +0000"  >&lt;p&gt;Good catch, I have a patch to fix this.&lt;/p&gt;</comment>
                            <comment id="15292167" author="apachespark" created="Thu, 19 May 2016 21:22:04 +0000"  >&lt;p&gt;User &apos;andrewor14&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/13203&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/13203&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                            <outwardlinks description="blocks">
                                        <issuelink>
            <issuekey id="12970973">SPARK-15396</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12969989">SPARK-15345</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 26 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2y8jj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12329449">2.0.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>