<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:31:39 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-10143] Parquet changed the behavior of calculating splits</title>
                <link>https://issues.apache.org/jira/browse/SPARK-10143</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;When Parquet&apos;s task side metadata is enabled (by default it is enabled and it needs to be enabled to deal with tables with many files), Parquet delegates the work of calculating initial splits to FileInputFormat (see &lt;a href=&quot;https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetInputFormat.java#L301-L311&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetInputFormat.java#L301-L311&lt;/a&gt;). If filesystem&apos;s block size is smaller than the row group size and users do not set min split size, splits in the initial split list will have lots of dummy splits and they contribute to empty tasks (because the starting point and ending point of a split does not cover the starting point of a row group). &lt;/p&gt;</description>
                <environment></environment>
        <key id="12857694">SPARK-10143</key>
            <summary>Parquet changed the behavior of calculating splits</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="yhuai">Yin Huai</assignee>
                                    <reporter username="yhuai">Yin Huai</reporter>
                        <labels>
                    </labels>
                <created>Thu, 20 Aug 2015 23:41:29 +0000</created>
                <updated>Fri, 21 Aug 2015 21:33:24 +0000</updated>
                            <resolved>Fri, 21 Aug 2015 21:30:27 +0000</resolved>
                                    <version>1.5.0</version>
                                    <fixVersion>1.5.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="14705988" author="yhuai" created="Thu, 20 Aug 2015 23:47:13 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rdblue&quot; class=&quot;user-hover&quot; rel=&quot;rdblue&quot;&gt;rdblue&lt;/a&gt; Can you confirm the behavior change of Parquet? Looks like we are just asking FileInputFormat to give us the initial splits. I am thinking to use the current setting of parquet row group size as the fs min split size for the job. What do you think? Thanks &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="14706045" author="apachespark" created="Fri, 21 Aug 2015 01:07:10 +0000"  >&lt;p&gt;User &apos;yhuai&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/8346&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/8346&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14706875" author="rdblue" created="Fri, 21 Aug 2015 15:34:00 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yhuai&quot; class=&quot;user-hover&quot; rel=&quot;yhuai&quot;&gt;yhuai&lt;/a&gt;, you&apos;re right that the input format now delegates to FileInputFormat to calculate splits. The main goal of &lt;a href=&quot;https://issues.apache.org/jira/browse/PARQUET-139&quot; title=&quot;Avoid reading file footers in parquet-avro InputFormat&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PARQUET-139&quot;&gt;&lt;del&gt;PARQUET-139&lt;/del&gt;&lt;/a&gt; was to be able to calculate splits without information from the file footer because the creates a performance problem: reading all of the file footers before submitting a job.&lt;/p&gt;

&lt;p&gt;What you&apos;re suggesting, to ensure min split size is at least the row group size will require reading the footers, so you will end up trading a minor problem (empty tasks) that affects bad files for a big problem (reading footers) that affects all files. I suggest not taking any action here and recommending the user rewrite the data. The row group size should never be larger than the HDFS block size.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/PARQUET-308&quot; title=&quot;Add accessor to ParquetWriter to get current data size&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PARQUET-308&quot;&gt;&lt;del&gt;PARQUET-308&lt;/del&gt;&lt;/a&gt; actually bumps up the block size to the row group size if it is less. It also adds padding to avoid row groups that span HDFS blocks, though you should set the max padding size to something reasonable to use it, like 8MB.&lt;/p&gt;</comment>
                            <comment id="14707067" author="yhuai" created="Fri, 21 Aug 2015 17:19:40 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rdblue&quot; class=&quot;user-hover&quot; rel=&quot;rdblue&quot;&gt;rdblue&lt;/a&gt; Thank you for the detailed info! One thing I did not explain clearly is for now I will use &lt;tt&gt;ParquetOutputFormat.getLongBlockSize&lt;/tt&gt; to get the block size setting from the conf to avoid touching parquet footers. What do you think? &lt;/p&gt;

&lt;p&gt;The main motivation of this workaround is for native S3 file system, we only have the concept of HDFS block size but we do not really break a file larger than this block size to multiple physical S3 files. So, for a large parquet file in S3, it is still a single S3 object. Since bumping up the default block size may affect the parallelism of workloads using other file formats, I am bumping the min split size only for parquet input format.&lt;/p&gt;</comment>
                            <comment id="14707112" author="yhuai" created="Fri, 21 Aug 2015 17:39:50 +0000"  >&lt;p&gt;I did a test yesterday that scans a table with 1824 files (file size&apos;s range is probably 80MB to 280MB and row group size is 128MB) in S3. To expose the overhead of those empty tasks, I did not read any column. My cluster had 16 cores in total. Without changing the min split setting, my scan job got 5023 tasks and the job took 102s to finish on average. After I changed the min split size to 400MB, I got one task per file and the job took 42s to finish on average (btw, one task per file was the behavior I got when I used parquet 1.6.0rc3 in Spark 1.4). I will test &lt;a href=&quot;https://github.com/apache/spark/pull/8346&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/8346&lt;/a&gt; with the same setting later.&lt;/p&gt;</comment>
                            <comment id="14707148" author="rdblue" created="Fri, 21 Aug 2015 17:57:14 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yhuai&quot; class=&quot;user-hover&quot; rel=&quot;yhuai&quot;&gt;yhuai&lt;/a&gt; if you do that, you will get the current value for the configuration, not what was used to write the file. If you want to know what the value was when the file was written, you have to read its footer.&lt;/p&gt;

&lt;p&gt;As far as solving the challenge of S3 input splits, if you&apos;re running in S3, why not split the files based on total length? Example:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;2 files: 500 MB and 700MB&lt;/li&gt;
	&lt;li&gt;Want 5 reducers&lt;/li&gt;
	&lt;li&gt;Splits: file 1:0-250MB, file 1:250-500MB, file 2:0-250MB, file 2:250-500MB, file 2:500-700MB&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Even without knowing the block size, you can control parallelism. If there are lots of small blocks (say 64MB block size), then you get approximately what you wanted. If there are big blocks (256MB) then you are still okay. If you have gigantic blocks (500MB) then you waste a couple tasks and get as much parallelism as possible anyway.&lt;/p&gt;</comment>
                            <comment id="14707194" author="yhuai" created="Fri, 21 Aug 2015 18:19:09 +0000"  >&lt;p&gt;oh, I meant the current value for the configuration is a much better heuristic to determine the number of mappers than the default HDFS block size when HDFS block size is small. &lt;/p&gt;</comment>
                            <comment id="14707222" author="rdblue" created="Fri, 21 Aug 2015 18:27:08 +0000"  >&lt;p&gt;I think you&apos;re going to end up assuming every row group is 128MB then. That&apos;s not terrible, but you may as well allocate the number of tasks that you want and divide up the input evenly by size. In the worst case, you get fewer real tasks because that&apos;s the way the data is laid out (fewer row groups than tasks). For other cases, your executors are responsible for continuous chunks of files from S3. If you do it based on the default row group size I think you&apos;re going to hit the case where you create too many tasks fairly often.&lt;/p&gt;</comment>
                            <comment id="14707363" author="yhuai" created="Fri, 21 Aug 2015 19:51:05 +0000"  >&lt;p&gt;Yeah, the setting is not the real row group size and we probably will always assume a row group is 128MB. But I feel in generally, end users do not really tweak this setting often. With our current master, we are creating too many tasks, which cause significant regression in some cases.&lt;/p&gt;

&lt;p&gt;My thought on splitting the files based on total length is that it requires us to determine the number of tasks first, right?&lt;/p&gt;</comment>
                            <comment id="14707414" author="rdblue" created="Fri, 21 Aug 2015 20:32:38 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yhuai&quot; class=&quot;user-hover&quot; rel=&quot;yhuai&quot;&gt;yhuai&lt;/a&gt;, yes, you&apos;d want to determine the number of tasks first. Or, if you don&apos;t care then you can choose a reasonable task size like 256MB. That shouldn&apos;t be less than the default row group size, but could easily be larger.&lt;/p&gt;</comment>
                            <comment id="14707497" author="yhuai" created="Fri, 21 Aug 2015 21:30:27 +0000"  >&lt;p&gt;Issue resolved by pull request 8346&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/8346&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/8346&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14707500" author="yhuai" created="Fri, 21 Aug 2015 21:33:24 +0000"  >&lt;p&gt;Just a note about this change. If the parallelism is not enough because of this change, users can decrease the &lt;tt&gt;parquet.block.size&lt;/tt&gt; set in the hadoop conf or through &lt;tt&gt;org.apache.spark.deploy.SparkHadoopUtil.get.conf.set(&quot;parquet.block.size&quot;, &quot;new value&quot;)&lt;/tt&gt;, and/or also set &lt;tt&gt;mapred.min.split.size&lt;/tt&gt; to a lower number.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 13 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2j6nj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12332078">1.5.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>