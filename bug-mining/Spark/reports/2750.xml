<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:34:58 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-11195] Exception thrown on executor throws ClassNotFoundException on driver</title>
                <link>https://issues.apache.org/jira/browse/SPARK-11195</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;I have a minimal repro job&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;&quot;&gt;&lt;b&gt;Repro.scala&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt; repro

&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.spark.SparkContext
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.spark.SparkConf
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.spark.SparkException

&lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;MyException(message: &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;) &lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; Exception(message: &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;)

object Repro {
  def main(args: Array[&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;]) {
    val conf = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; SparkConf().setAppName(&lt;span class=&quot;code-quote&quot;&gt;&quot;MyException ClassNotFound Repro&quot;&lt;/span&gt;)
    val sc = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; SparkContext(conf)

    sc.parallelize(List(1)).map { x =&amp;gt;
      &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; repro.MyException(&lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; is a failure&quot;&lt;/span&gt;)
      &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;
    }.collect()
  }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;On Spark 1.4.1, I get a task failure with the reason correctly set to MyException.&lt;br/&gt;
On Spark 1.5.1, I &lt;em&gt;expect&lt;/em&gt; the same behavior, but instead I get a task failure with an UnknownReason caused by ClassNotFoundException.&lt;/p&gt;

&lt;p&gt;here is the job on vanilla Spark 1.4.1:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;&quot;&gt;&lt;b&gt;spark_1.5.1_log&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;$ ./bin/spark-submit --master local --deploy-mode client --&lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;repro.Repro /home/nix/repro/target/scala-2.10/repro-assembly-0.0.1.jar
Using Spark&apos;s &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt; log4j profile: org/apache/spark/log4j-defaults.properties
15/10/19 11:55:20 INFO SparkContext: Running Spark version 1.4.1
15/10/19 11:55:21 WARN NativeCodeLoader: Unable to load &lt;span class=&quot;code-keyword&quot;&gt;native&lt;/span&gt;-hadoop library &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; your platform... using builtin-java classes where applicable
15/10/19 11:55:22 WARN Utils: Your hostname, choochootrain resolves to a loopback address: 127.0.1.1; using 10.0.1.97 instead (on &lt;span class=&quot;code-keyword&quot;&gt;interface&lt;/span&gt; wlan0)
15/10/19 11:55:22 WARN Utils: Set SPARK_LOCAL_IP &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; you need to bind to another address
15/10/19 11:55:22 INFO &lt;span class=&quot;code-object&quot;&gt;SecurityManager&lt;/span&gt;: Changing view acls to: root
15/10/19 11:55:22 INFO &lt;span class=&quot;code-object&quot;&gt;SecurityManager&lt;/span&gt;: Changing modify acls to: root
15/10/19 11:55:22 INFO &lt;span class=&quot;code-object&quot;&gt;SecurityManager&lt;/span&gt;: &lt;span class=&quot;code-object&quot;&gt;SecurityManager&lt;/span&gt;: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)
15/10/19 11:55:24 INFO Slf4jLogger: Slf4jLogger started
15/10/19 11:55:24 INFO Remoting: Starting remoting
15/10/19 11:55:24 INFO Remoting: Remoting started; listening on addresses :[akka.tcp:&lt;span class=&quot;code-comment&quot;&gt;//sparkDriver@10.0.1.97:46683]
&lt;/span&gt;15/10/19 11:55:24 INFO Utils: Successfully started service &lt;span class=&quot;code-quote&quot;&gt;&apos;sparkDriver&apos;&lt;/span&gt; on port 46683.
15/10/19 11:55:24 INFO SparkEnv: Registering MapOutputTracker
15/10/19 11:55:24 INFO SparkEnv: Registering BlockManagerMaster
15/10/19 11:55:24 INFO DiskBlockManager: Created local directory at /tmp/spark-0348a320-0ca3-4528-9ab5-9ba37d3c2e07/blockmgr-08496143-1d9d-41c8-a581-b6220edf00d5
15/10/19 11:55:24 INFO MemoryStore: MemoryStore started with capacity 265.4 MB
15/10/19 11:55:25 INFO HttpFileServer: HTTP File server directory is /tmp/spark-0348a320-0ca3-4528-9ab5-9ba37d3c2e07/httpd-52c396d2-b47f-45a5-bb76-d10aa864e6d5
15/10/19 11:55:25 INFO HttpServer: Starting HTTP Server
15/10/19 11:55:25 INFO Utils: Successfully started service &lt;span class=&quot;code-quote&quot;&gt;&apos;HTTP file server&apos;&lt;/span&gt; on port 47915.
15/10/19 11:55:25 INFO SparkEnv: Registering OutputCommitCoordinator
15/10/19 11:55:25 INFO Utils: Successfully started service &lt;span class=&quot;code-quote&quot;&gt;&apos;SparkUI&apos;&lt;/span&gt; on port 4040.
15/10/19 11:55:25 INFO SparkUI: Started SparkUI at http:&lt;span class=&quot;code-comment&quot;&gt;//10.0.1.97:4040
&lt;/span&gt;15/10/19 11:55:25 INFO SparkContext: Added JAR file:/home/nix/repro/target/scala-2.10/repro-assembly-0.0.1.jar at http:&lt;span class=&quot;code-comment&quot;&gt;//10.0.1.97:47915/jars/repro-assembly-0.0.1.jar with timestamp 1445280925969
&lt;/span&gt;15/10/19 11:55:26 INFO Executor: Starting executor ID driver on host localhost
15/10/19 11:55:26 INFO Utils: Successfully started service &lt;span class=&quot;code-quote&quot;&gt;&apos;org.apache.spark.network.netty.NettyBlockTransferService&apos;&lt;/span&gt; on port 46569.
15/10/19 11:55:26 INFO NettyBlockTransferService: Server created on 46569
15/10/19 11:55:26 INFO BlockManagerMaster: Trying to register BlockManager
15/10/19 11:55:26 INFO BlockManagerMasterEndpoint: Registering block manager localhost:46569 with 265.4 MB RAM, BlockManagerId(driver, localhost, 46569)
15/10/19 11:55:26 INFO BlockManagerMaster: Registered BlockManager
15/10/19 11:55:27 INFO SparkContext: Starting job: collect at repro.scala:18
15/10/19 11:55:27 INFO DAGScheduler: Got job 0 (collect at repro.scala:18) with 1 output partitions (allowLocal=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;)
15/10/19 11:55:27 INFO DAGScheduler: Final stage: ResultStage 0(collect at repro.scala:18)
15/10/19 11:55:27 INFO DAGScheduler: Parents of &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; stage: List()
15/10/19 11:55:27 INFO DAGScheduler: Missing parents: List()
15/10/19 11:55:27 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at repro.scala:15), which has no missing parents
15/10/19 11:55:28 INFO MemoryStore: ensureFreeSpace(1984) called with curMem=0, maxMem=278302556
15/10/19 11:55:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1984.0 B, free 265.4 MB)
15/10/19 11:55:28 INFO MemoryStore: ensureFreeSpace(1248) called with curMem=1984, maxMem=278302556
15/10/19 11:55:28 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1248.0 B, free 265.4 MB)
15/10/19 11:55:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:46569 (size: 1248.0 B, free: 265.4 MB)
15/10/19 11:55:28 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:874
15/10/19 11:55:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at repro.scala:15)
15/10/19 11:55:28 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
15/10/19 11:55:28 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1375 bytes)
15/10/19 11:55:28 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
15/10/19 11:55:28 INFO Executor: Fetching http:&lt;span class=&quot;code-comment&quot;&gt;//10.0.1.97:47915/jars/repro-assembly-0.0.1.jar with timestamp 1445280925969
&lt;/span&gt;15/10/19 11:55:28 INFO Utils: Fetching http:&lt;span class=&quot;code-comment&quot;&gt;//10.0.1.97:47915/jars/repro-assembly-0.0.1.jar to /tmp/spark-0348a320-0ca3-4528-9ab5-9ba37d3c2e07/userFiles-08fd567b-f708-49c8-a41b-83994436ef4f/fetchFileTemp4791648304973175221.tmp
&lt;/span&gt;15/10/19 11:55:28 INFO Executor: Adding file:/tmp/spark-0348a320-0ca3-4528-9ab5-9ba37d3c2e07/userFiles-08fd567b-f708-49c8-a41b-83994436ef4f/repro-assembly-0.0.1.jar to &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;loader
15/10/19 11:55:28 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
repro.MyException: &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; is a failure
        at repro.Repro$$anonfun$main$1.apply$mcZI$sp(repro.scala:16)
        at repro.Repro$$anonfun$main$1.apply(repro.scala:15)
        at repro.Repro$$anonfun$main$1.apply(repro.scala:15)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at scala.collection.Iterator$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(Iterator.scala:727)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        at scala.collection.&lt;span class=&quot;code-keyword&quot;&gt;generic&lt;/span&gt;.Growable$class.$plus$plus$eq(Growable.scala:48)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
        at scala.collection.TraversableOnce$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;to(TraversableOnce.scala:273)
        at scala.collection.AbstractIterator.to(Iterator.scala:1157)
        at scala.collection.TraversableOnce$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;toBuffer(TraversableOnce.scala:265)
        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
        at scala.collection.TraversableOnce$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;toArray(TraversableOnce.scala:252)
        at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:885)
        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:885)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)
        at org.apache.spark.scheduler.Task.run(Task.scala:70)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
15/10/19 11:55:28 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost): repro.MyException: &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; is a failure
        at repro.Repro$$anonfun$main$1.apply$mcZI$sp(repro.scala:16)
        at repro.Repro$$anonfun$main$1.apply(repro.scala:15)
        at repro.Repro$$anonfun$main$1.apply(repro.scala:15)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at scala.collection.Iterator$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(Iterator.scala:727)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        at scala.collection.&lt;span class=&quot;code-keyword&quot;&gt;generic&lt;/span&gt;.Growable$class.$plus$plus$eq(Growable.scala:48)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
        at scala.collection.TraversableOnce$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;to(TraversableOnce.scala:273)
        at scala.collection.AbstractIterator.to(Iterator.scala:1157)
        at scala.collection.TraversableOnce$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;toBuffer(TraversableOnce.scala:265)
        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
        at scala.collection.TraversableOnce$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;toArray(TraversableOnce.scala:252)
        at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:885)
        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:885)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)
        at org.apache.spark.scheduler.Task.run(Task.scala:70)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
 
15/10/19 11:55:28 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
15/10/19 11:55:28 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
15/10/19 11:55:28 INFO TaskSchedulerImpl: Cancelling stage 0
15/10/19 11:55:28 INFO DAGScheduler: ResultStage 0 (collect at repro.scala:18) failed in 0.542 s
15/10/19 11:55:28 INFO DAGScheduler: Job 0 failed: collect at repro.scala:18, took 0.972468 s
Exception in thread &lt;span class=&quot;code-quote&quot;&gt;&quot;main&quot;&lt;/span&gt; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): repro.MyException: &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; is a failure
        at repro.Repro$$anonfun$main$1.apply$mcZI$sp(repro.scala:16)
        at repro.Repro$$anonfun$main$1.apply(repro.scala:15)
        at repro.Repro$$anonfun$main$1.apply(repro.scala:15)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at scala.collection.Iterator$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(Iterator.scala:727)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        at scala.collection.&lt;span class=&quot;code-keyword&quot;&gt;generic&lt;/span&gt;.Growable$class.$plus$plus$eq(Growable.scala:48)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
        at scala.collection.TraversableOnce$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;to(TraversableOnce.scala:273)
        at scala.collection.AbstractIterator.to(Iterator.scala:1157)
        at scala.collection.TraversableOnce$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;toBuffer(TraversableOnce.scala:265)
        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
        at scala.collection.TraversableOnce$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;toArray(TraversableOnce.scala:252)
        at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:885)
        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:885)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)
        at org.apache.spark.scheduler.Task.run(Task.scala:70)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
 
Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263)
        at scala.collection.mutable.ResizableArray$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
        at scala.Option.foreach(Option.scala:236)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
15/10/19 11:55:28 INFO SparkContext: Invoking stop() from shutdown hook
15/10/19 11:55:28 INFO SparkUI: Stopped Spark web UI at http:&lt;span class=&quot;code-comment&quot;&gt;//10.0.1.97:4040
&lt;/span&gt;15/10/19 11:55:28 INFO DAGScheduler: Stopping DAGScheduler
15/10/19 11:55:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
15/10/19 11:55:29 INFO Utils: path = /tmp/spark-0348a320-0ca3-4528-9ab5-9ba37d3c2e07/blockmgr-08496143-1d9d-41c8-a581-b6220edf00d5, already present as root &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; deletion.
15/10/19 11:55:29 INFO MemoryStore: MemoryStore cleared
15/10/19 11:55:29 INFO BlockManager: BlockManager stopped
15/10/19 11:55:29 INFO BlockManagerMaster: BlockManagerMaster stopped
15/10/19 11:55:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
15/10/19 11:55:29 INFO SparkContext: Successfully stopped SparkContext
15/10/19 11:55:29 INFO Utils: Shutdown hook called
15/10/19 11:55:29 INFO Utils: Deleting directory /tmp/spark-0348a320-0ca3-4528-9ab5-9ba37d3c2e07
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and here is the job on vanilla Spark 1.5.1&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;&quot;&gt;&lt;b&gt;spark_1.5.1_log&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;$ ./bin/spark-submit --master local --deploy-mode client --&lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;repro.Repro /home/nix/repro/target/scala-2.10/repro-assembly-0.0.1.jar
Using Spark&apos;s &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt; log4j profile: org/apache/spark/log4j-defaults.properties
15/10/19 11:53:30 INFO SparkContext: Running Spark version 1.5.1
15/10/19 11:53:31 WARN NativeCodeLoader: Unable to load &lt;span class=&quot;code-keyword&quot;&gt;native&lt;/span&gt;-hadoop library &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; your platform... using builtin-java classes where applicable
15/10/19 11:53:32 WARN Utils: Your hostname, choochootrain resolves to a loopback address: 127.0.1.1; using 10.0.1.97 instead (on &lt;span class=&quot;code-keyword&quot;&gt;interface&lt;/span&gt; wlan0)
15/10/19 11:53:32 WARN Utils: Set SPARK_LOCAL_IP &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; you need to bind to another address
15/10/19 11:53:32 INFO &lt;span class=&quot;code-object&quot;&gt;SecurityManager&lt;/span&gt;: Changing view acls to: root
15/10/19 11:53:32 INFO &lt;span class=&quot;code-object&quot;&gt;SecurityManager&lt;/span&gt;: Changing modify acls to: root
15/10/19 11:53:32 INFO &lt;span class=&quot;code-object&quot;&gt;SecurityManager&lt;/span&gt;: &lt;span class=&quot;code-object&quot;&gt;SecurityManager&lt;/span&gt;: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)
15/10/19 11:53:34 INFO Slf4jLogger: Slf4jLogger started
15/10/19 11:53:34 INFO Remoting: Starting remoting
15/10/19 11:53:34 INFO Remoting: Remoting started; listening on addresses :[akka.tcp:&lt;span class=&quot;code-comment&quot;&gt;//sparkDriver@10.0.1.97:47096]
&lt;/span&gt;15/10/19 11:53:34 INFO Utils: Successfully started service &lt;span class=&quot;code-quote&quot;&gt;&apos;sparkDriver&apos;&lt;/span&gt; on port 47096.
15/10/19 11:53:34 INFO SparkEnv: Registering MapOutputTracker
15/10/19 11:53:34 INFO SparkEnv: Registering BlockManagerMaster
15/10/19 11:53:34 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-be9b3111-6640-4bc8-bbd0-054cccfa474f
15/10/19 11:53:34 INFO MemoryStore: MemoryStore started with capacity 530.3 MB
15/10/19 11:53:35 INFO HttpFileServer: HTTP File server directory is /tmp/spark-e2aeb6af-3b15-4d36-8f1c-abd1ae494be2/httpd-e1fcfd42-4521-4cee-96d2-eac83d0a89ea
15/10/19 11:53:35 INFO HttpServer: Starting HTTP Server
15/10/19 11:53:35 INFO Utils: Successfully started service &lt;span class=&quot;code-quote&quot;&gt;&apos;HTTP file server&apos;&lt;/span&gt; on port 59017.
15/10/19 11:53:35 INFO SparkEnv: Registering OutputCommitCoordinator
15/10/19 11:53:35 INFO Utils: Successfully started service &lt;span class=&quot;code-quote&quot;&gt;&apos;SparkUI&apos;&lt;/span&gt; on port 4040.
15/10/19 11:53:35 INFO SparkUI: Started SparkUI at http:&lt;span class=&quot;code-comment&quot;&gt;//10.0.1.97:4040
&lt;/span&gt;15/10/19 11:53:35 INFO SparkContext: Added JAR file:/home/nix/repro/target/scala-2.10/repro-assembly-0.0.1.jar at http:&lt;span class=&quot;code-comment&quot;&gt;//10.0.1.97:59017/jars/repro-assembly-0.0.1.jar with timestamp 1445280815913
&lt;/span&gt;15/10/19 11:53:36 WARN MetricsSystem: Using &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt; name DAGScheduler &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; source because spark.app.id is not set.
15/10/19 11:53:36 INFO Executor: Starting executor ID driver on host localhost
15/10/19 11:53:36 INFO Utils: Successfully started service &lt;span class=&quot;code-quote&quot;&gt;&apos;org.apache.spark.network.netty.NettyBlockTransferService&apos;&lt;/span&gt; on port 40701.
15/10/19 11:53:36 INFO NettyBlockTransferService: Server created on 40701
15/10/19 11:53:36 INFO BlockManagerMaster: Trying to register BlockManager
15/10/19 11:53:36 INFO BlockManagerMasterEndpoint: Registering block manager localhost:40701 with 530.3 MB RAM, BlockManagerId(driver, localhost, 40701)
15/10/19 11:53:36 INFO BlockManagerMaster: Registered BlockManager
15/10/19 11:53:38 INFO SparkContext: Starting job: collect at repro.scala:18
15/10/19 11:53:38 INFO DAGScheduler: Got job 0 (collect at repro.scala:18) with 1 output partitions
15/10/19 11:53:38 INFO DAGScheduler: Final stage: ResultStage 0(collect at repro.scala:18)
15/10/19 11:53:38 INFO DAGScheduler: Parents of &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; stage: List()
15/10/19 11:53:38 INFO DAGScheduler: Missing parents: List()
15/10/19 11:53:38 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at repro.scala:15), which has no missing parents
15/10/19 11:53:38 INFO MemoryStore: ensureFreeSpace(1984) called with curMem=0, maxMem=556038881
15/10/19 11:53:38 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1984.0 B, free 530.3 MB)
15/10/19 11:53:38 INFO MemoryStore: ensureFreeSpace(1248) called with curMem=1984, maxMem=556038881
15/10/19 11:53:38 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1248.0 B, free 530.3 MB)
15/10/19 11:53:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:40701 (size: 1248.0 B, free: 530.3 MB)
15/10/19 11:53:38 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:861
15/10/19 11:53:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at repro.scala:15)
15/10/19 11:53:38 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
15/10/19 11:53:38 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 2091 bytes)
15/10/19 11:53:38 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
15/10/19 11:53:38 INFO Executor: Fetching http:&lt;span class=&quot;code-comment&quot;&gt;//10.0.1.97:59017/jars/repro-assembly-0.0.1.jar with timestamp 1445280815913
&lt;/span&gt;15/10/19 11:53:38 INFO Utils: Fetching http:&lt;span class=&quot;code-comment&quot;&gt;//10.0.1.97:59017/jars/repro-assembly-0.0.1.jar to /tmp/spark-e2aeb6af-3b15-4d36-8f1c-abd1ae494be2/userFiles-94c6cfc6-59a3-4a6f-ab06-b8b41956c9c0/fetchFileTemp5258469143029308872.tmp
&lt;/span&gt;15/10/19 11:53:38 INFO Executor: Adding file:/tmp/spark-e2aeb6af-3b15-4d36-8f1c-abd1ae494be2/userFiles-94c6cfc6-59a3-4a6f-ab06-b8b41956c9c0/repro-assembly-0.0.1.jar to &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;loader
15/10/19 11:53:39 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
repro.MyException: &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; is a failure
        at repro.Repro$$anonfun$main$1.apply$mcZI$sp(repro.scala:16)
        at repro.Repro$$anonfun$main$1.apply(repro.scala:15)
        at repro.Repro$$anonfun$main$1.apply(repro.scala:15)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at scala.collection.Iterator$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(Iterator.scala:727)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        at scala.collection.&lt;span class=&quot;code-keyword&quot;&gt;generic&lt;/span&gt;.Growable$class.$plus$plus$eq(Growable.scala:48)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
        at scala.collection.TraversableOnce$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;to(TraversableOnce.scala:273)
        at scala.collection.AbstractIterator.to(Iterator.scala:1157)
        at scala.collection.TraversableOnce$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;toBuffer(TraversableOnce.scala:265)
        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
        at scala.collection.TraversableOnce$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;toArray(TraversableOnce.scala:252)
        at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:905)
        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:905)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
        at org.apache.spark.scheduler.Task.run(Task.scala:88)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
15/10/19 11:53:39 WARN ThrowableSerializationWrapper: Task exception could not be deserialized
java.lang.ClassNotFoundException: repro.MyException
        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.loadClass(&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.java:425)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.loadClass(&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.java:358)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;.forName0(Native Method)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;.forName(&lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;.java:274)
        at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:67)
        at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1612)
        at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
        at org.apache.spark.ThrowableSerializationWrapper.readObject(TaskEndReason.scala:167)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1897)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1997)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1921)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1997)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1921)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
        at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:72)
        at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:98)
        at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:108)
        at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:105)
        at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:105)
        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1699)
        at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:105)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
15/10/19 11:53:39 ERROR TaskResultGetter: Could not deserialize TaskEndReason: ClassNotFound with classloader org.apache.spark.util.MutableURLClassLoader@7f08a6b1
15/10/19 11:53:39 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost): UnknownReason
15/10/19 11:53:39 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
15/10/19 11:53:39 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
15/10/19 11:53:39 INFO TaskSchedulerImpl: Cancelling stage 0
15/10/19 11:53:39 INFO DAGScheduler: ResultStage 0 (collect at repro.scala:18) failed in 0.567 s
15/10/19 11:53:39 INFO DAGScheduler: Job 0 failed: collect at repro.scala:18, took 1.049437 s
Exception in thread &lt;span class=&quot;code-quote&quot;&gt;&quot;main&quot;&lt;/span&gt; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): UnknownReason
Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)
        at scala.collection.mutable.ResizableArray$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
        at scala.Option.foreach(Option.scala:236)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)
        at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:905)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
        at org.apache.spark.rdd.RDD.collect(RDD.scala:904)
        at repro.Repro$.main(repro.scala:18)
        at repro.Repro.main(repro.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/10/19 11:53:39 INFO SparkContext: Invoking stop() from shutdown hook
15/10/19 11:53:39 INFO SparkUI: Stopped Spark web UI at http:&lt;span class=&quot;code-comment&quot;&gt;//10.0.1.97:4040
&lt;/span&gt;15/10/19 11:53:39 INFO DAGScheduler: Stopping DAGScheduler
15/10/19 11:53:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
15/10/19 11:53:39 INFO MemoryStore: MemoryStore cleared
15/10/19 11:53:39 INFO BlockManager: BlockManager stopped
15/10/19 11:53:39 INFO BlockManagerMaster: BlockManagerMaster stopped
15/10/19 11:53:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
15/10/19 11:53:39 INFO SparkContext: Successfully stopped SparkContext
15/10/19 11:53:39 INFO ShutdownHookManager: Shutdown hook called
15/10/19 11:53:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-e2aeb6af-3b15-4d36-8f1c-abd1ae494be2
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;See &lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/incubator-spark-user/201510.mbox/browser&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://mail-archives.apache.org/mod_mbox/incubator-spark-user/201510.mbox/browser&lt;/a&gt; for full context&lt;/p&gt;</description>
                <environment></environment>
        <key id="12906098">SPARK-11195</key>
            <summary>Exception thrown on executor throws ClassNotFoundException on driver</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="choochootrain">Hurshal Patel</assignee>
                                    <reporter username="choochootrain">Hurshal Patel</reporter>
                        <labels>
                    </labels>
                <created>Mon, 19 Oct 2015 22:32:19 +0000</created>
                <updated>Wed, 18 Nov 2015 17:39:53 +0000</updated>
                            <resolved>Wed, 18 Nov 2015 17:37:03 +0000</resolved>
                                    <version>1.5.1</version>
                                    <fixVersion>1.5.3</fixVersion>
                    <fixVersion>1.6.0</fixVersion>
                                    <component>Spark Core</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>9</watches>
                                                                                                                <comments>
                            <comment id="14965044" author="srowen" created="Tue, 20 Oct 2015 12:41:50 +0000"  >&lt;p&gt;What is in your assembly jar? I assume you&apos;re sure it has MyException? what other third-party classes? I wonder if you have some class that Spark uses that isn&apos;t shaded that is interfering.&lt;/p&gt;</comment>
                            <comment id="14965642" author="choochootrain" created="Tue, 20 Oct 2015 19:49:46 +0000"  >&lt;p&gt;MyException is in Repro.scala - the assembly jar only has the scala runtime classes and my repro classes so that shouldn&apos;t be an issue. &lt;/p&gt;</comment>
                            <comment id="14969184" author="invkrh" created="Thu, 22 Oct 2015 13:53:52 +0000"  >&lt;p&gt;Has this issue been resolved ? We have the same problem with Kafka exception:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;15/10/22 12:34:35 WARN ThrowableSerializationWrapper: WARN ThrowableSerializationWrapper: Task exception could not be deserialized
java.lang.ClassNotFoundException: org.apache.kafka.common.errors.TimeoutException
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.loadClass(&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.loadClass(&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.java:357)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;.forName0(Native Method)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;.forName(&lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;.java:348)
	at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:67)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1613)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1518)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1774)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
	at org.apache.spark.ThrowableSerializationWrapper.readObject(TaskEndReason.scala:167)
	at sun.reflect.GeneratedMethodAccessor39.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1900)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:72)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:98)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:108)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:105)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:105)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1699)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:105)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
15/10/22 12:34:35 ERROR TaskResultGetter: Could not deserialize TaskEndReason: ClassNotFound with classloader org.apache.spark.util.MutableURLClassLoader@12cdcf4
15/10/22 12:34:35 WARN TaskSetManager: Lost task 22.2 in stage 14.0 (TID 3048, 172.17.1.13): UnknownReason
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
</comment>
                            <comment id="14969201" author="srowen" created="Thu, 22 Oct 2015 14:06:06 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=invkrh&quot; class=&quot;user-hover&quot; rel=&quot;invkrh&quot;&gt;invkrh&lt;/a&gt; that&apos;s unrelated. You haven&apos;t included Kafka classes in your app.&lt;/p&gt;</comment>
                            <comment id="14969271" author="invkrh" created="Thu, 22 Oct 2015 15:04:11 +0000"  >&lt;p&gt;Thank you for the quick reply.&lt;/p&gt;

&lt;p&gt;In fact, we have Kafka classes in our app with &quot;--jars /opt/spark/lib/kafka_2.10-0.8.2.2.jar,/opt/spark/lib/kafka-clients-0.8.2.2.jar&quot;.&lt;/p&gt;

&lt;p&gt;It seems that the kafka exception I mentioned was created on slaves, however, when they were sent back to driver, the driver can not deserialize the exception object since kafka deps are not in the driver&apos;s classpath.&lt;/p&gt;

&lt;p&gt;Normally, &quot;--jars&quot; should include the list of local jars on the driver and executor classpaths. &lt;/p&gt;

&lt;p&gt;But it doesn&apos;t, that&apos;s why we think there may be a bug here, .&lt;/p&gt;

&lt;p&gt;The workaround is just to make the driver&apos;s classpath contain kafka deps by adding &quot;--conf spark.driver.extraClassPath=/opt/spark/lib/kafka_2.10-0.8.2.2.jar:/opt/spark/lib/kafka-clients-0.8.2.2.jar&quot; to spark-sumbit.&lt;/p&gt;

&lt;p&gt;And it works.&lt;/p&gt;

&lt;p&gt;(Maybe I should create another issue on this)&lt;/p&gt;
</comment>
                            <comment id="14969535" author="choochootrain" created="Thu, 22 Oct 2015 17:46:47 +0000"  >&lt;p&gt;this is very likely the same issue. the only difference between my repro and yours is that I have a fatjar with all my classes and you are providing the deps with --jars but in either case the driver doesn&apos;t have the correct classpath.&lt;/p&gt;</comment>
                            <comment id="14971783" author="cody@koeninger.org" created="Fri, 23 Oct 2015 20:26:09 +0000"  >&lt;p&gt;I&apos;m seeing this on 1.5.1 as well&lt;/p&gt;</comment>
                            <comment id="14981678" author="apachespark" created="Fri, 30 Oct 2015 01:09:04 +0000"  >&lt;p&gt;User &apos;choochootrain&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/9367&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/9367&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15009638" author="apachespark" created="Tue, 17 Nov 2015 22:01:02 +0000"  >&lt;p&gt;User &apos;choochootrain&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/9779&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/9779&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15011499" author="yhuai" created="Wed, 18 Nov 2015 17:37:03 +0000"  >&lt;p&gt;Issue resolved by pull request 9779&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/9779&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/9779&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2n7of:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12333083">1.6.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>