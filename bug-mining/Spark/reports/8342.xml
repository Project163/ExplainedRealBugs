<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 19:28:51 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-37958] Pyspark SparkContext.AddFile() does not respect spark.files.overwrite</title>
                <link>https://issues.apache.org/jira/browse/SPARK-37958</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;I am currently running apache spark 3.1.1. on kubernetes.&lt;/p&gt;

&lt;p&gt;When I try to re-add a file that has already been added I see that the updated file is not actually loaded into the cluster. I see the following warning when calling the addFile() function.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
22/01/18 19:05:50 WARN SparkContext: The path http:&lt;span class=&quot;code-comment&quot;&gt;//15.4.12.12:80/demo_data.csv has been added already. Overwriting of added paths is not supported in the current version. &lt;/span&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;When I display the dataframe that was loaded I see that the old data is loaded. If I log into the worker pods and delete the file, the same results or observed.&lt;/p&gt;

&lt;p&gt;My SparkConf has the following configurations&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
(&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.master&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;k8s:&lt;span class=&quot;code-comment&quot;&gt;//https://15.4.7.11:6443&apos;&lt;/span&gt;)
&lt;/span&gt;(&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.app.name&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;spark-jupyter-mlib&apos;&lt;/span&gt;)
(&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.submit.deploy.mode&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;cluster&apos;&lt;/span&gt;)
(&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.kubernetes.container.image&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;tschneider/apache-spark-k8:v7&apos;&lt;/span&gt;)
(&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.kubernetes.namespace&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;spark&apos;&lt;/span&gt;)
(&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.kubernetes.pyspark.pythonVersion&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;3&apos;&lt;/span&gt;)
(&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.kubernetes.authenticate.driver.serviceAccountName&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;spark-sa&apos;&lt;/span&gt;)
(&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.kubernetes.authenticate.serviceAccountName&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;spark-sa&apos;&lt;/span&gt;)
(&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.executor.instances&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;3&apos;&lt;/span&gt;)
(&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.executor.cores&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;2&apos;&lt;/span&gt;)
(&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.executor.memory&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;4096m&apos;&lt;/span&gt;)
(&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.executor.memoryOverhead&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;1024m&apos;&lt;/span&gt;)
(&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.driver.memory&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;1024m&apos;&lt;/span&gt;)
(&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.driver.host&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;15.4.12.12&apos;&lt;/span&gt;)
(&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.files.overwrite&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&apos;&lt;/span&gt;)
(&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.files.useFetchCache&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&apos;&lt;/span&gt;) &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;According to the documentation for 3.1.1. The spark.files.overwrite parameter should in fact load the updated files. The documentation can be found here: &lt;a href=&quot;https://spark.apache.org/docs/3.1.1/configuration.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://spark.apache.org/docs/3.1.1/configuration.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The only workaround is to use a python function to manually delete and re-download the file. Calling addFile still shows the warning in this case. My code for the delete and redownload is as follows:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
def os_remove(file_path):
&#160; &#160; &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; socket
&#160; &#160; hostname = socket.gethostname()&#160; &#160; action = None
&#160; &#160; &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; os
&#160; &#160; &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; os.path.exists(file_path):
&#160; &#160; &#160; &#160; action = &lt;span class=&quot;code-quote&quot;&gt;&quot;delete&quot;&lt;/span&gt;
&#160; &#160; &#160; &#160; os.remove(file_path)
&#160; &#160; &#160; &#160;&#160;
&#160; &#160; &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; (hostname, action)worker_file_path = u&lt;span class=&quot;code-quote&quot;&gt;&quot;file:&lt;span class=&quot;code-comment&quot;&gt;///{0}&quot;&lt;/span&gt;.format(csv_file_name)
&lt;/span&gt;

worker_count = &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;(spark_session.conf.get(&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.executor.instances&apos;&lt;/span&gt;))
rdd = sc.parallelize(range(worker_count)).map(lambda &lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;: os_remove(worker_file_path))
rdd.collect()


def download_updated_file(file_url):
&#160; &#160; &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; urllib.parse as parse
&#160; &#160; file_name = os.path.basename(parse.urlparse(csv_file_url).path)
&#160; &#160; local_file_path = &lt;span class=&quot;code-quote&quot;&gt;&quot;/{0}&quot;&lt;/span&gt;.format(file_name)
&#160; &#160;&#160;
&#160; &#160; &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; urllib.request as urllib
&#160; &#160; urllib.urlretrieve(file_url, local_file_path)
&#160; &#160;&#160;

rdd = sc.parallelize(range(worker_count)).map(lambda &lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;: download_updated_file(csv_file_url))
rdd.collect()&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;I believe this is either a bug or a documentation mistake. Perhaps the configuration parameter has a misleading description?&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</description>
                <environment></environment>
        <key id="13423351">SPARK-37958</key>
            <summary>Pyspark SparkContext.AddFile() does not respect spark.files.overwrite</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="yoda-mon">Leona Yoda</assignee>
                                    <reporter username="tschneider_live">taylor schneider</reporter>
                        <labels>
                    </labels>
                <created>Tue, 18 Jan 2022 19:45:11 +0000</created>
                <updated>Mon, 12 Dec 2022 18:10:40 +0000</updated>
                            <resolved>Thu, 3 Feb 2022 00:42:02 +0000</resolved>
                                    <version>3.1.1</version>
                                    <fixVersion>3.3.0</fixVersion>
                                    <component>Documentation</component>
                    <component>Input/Output</component>
                    <component>Java API</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="17485032" author="yoda-mon" created="Tue, 1 Feb 2022 03:10:24 +0000"  >&lt;p&gt;This is probably default behavior not only on k8s or pyspark environment.&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;spark.files.overwrite&lt;/tt&gt; was introduced in 1.0. Then at the PR &lt;a href=&quot;https://github.com/apache/spark/pull/14396&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/14396&lt;/a&gt; , calling addFile twice on the same name file is not allowed on drivers side.&lt;/p&gt;

&lt;p&gt;By setting this true users can overwrite the files that already existed at startup by calling addFile or addJar, which is prohibited at default configuration.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-comment&quot;&gt;// code placeholder
&lt;/span&gt;$ export K8S_ENDPOINT=&lt;span class=&quot;code-quote&quot;&gt;&quot;https:&lt;span class=&quot;code-comment&quot;&gt;//192.168.49.2:8443&quot;&lt;/span&gt; # minikube
&lt;/span&gt;$ export SPARK_IMAGE=&lt;span class=&quot;code-quote&quot;&gt;&quot;spark:add-file-in-advance&quot;&lt;/span&gt; # containes /opt/spark/work-dir/test.text
$ docker run -i $SPARK_IMAGE cat /opt/spark/work-dir/test.txt
...
This is original file

# put a &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; file on driver&apos;s side
$ cat work-dir/test.txt
Hello
spark
on
k8


# spark.files.overwrite=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;
$ ./bin/spark-shell \
&#160; --master k8s:&lt;span class=&quot;code-comment&quot;&gt;//${K8S_ENDPOINT} \
&lt;/span&gt;&#160; --deploy-mode client \
&#160; --name spark-shell \
&#160; --conf spark.executor.instances=1 \
&#160; --conf spark.kubernetes.container.image=${SPARK_IMAGE} \
&#160; --conf spark.kubernetes.container.image.pullPolicy=Never \
&#160; --conf spark.files.overwrite=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;
scala&amp;gt; spark.read.format(&lt;span class=&quot;code-quote&quot;&gt;&quot;text&quot;&lt;/span&gt;).load(&lt;span class=&quot;code-quote&quot;&gt;&quot;work-dir/test.txt&quot;&lt;/span&gt;).show()
[Stage 0:&amp;gt; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; +--------------------+
| &#160; &#160; &#160; &#160; &#160; &#160; &#160; value|
+--------------------+
|This is original ...|
+--------------------+
scala&amp;gt; sc.addFile(&lt;span class=&quot;code-quote&quot;&gt;&quot;work-dir/test.txt&quot;&lt;/span&gt;)
scala&amp;gt; spark.read.format(&lt;span class=&quot;code-quote&quot;&gt;&quot;text&quot;&lt;/span&gt;).load(&lt;span class=&quot;code-quote&quot;&gt;&quot;work-dir/test.txt&quot;&lt;/span&gt;).show()
22/02/01 02:48:02 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (172.17.0.4 executor 1): org.apache.spark.SparkException: File ./test.txt exists and does not match contents of spark:&lt;span class=&quot;code-comment&quot;&gt;//...
&lt;/span&gt;

# spark.files.overwrite=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;
$ ./bin/spark-shell \
&#160; --master k8s:&lt;span class=&quot;code-comment&quot;&gt;//${K8S_ENDPOINT} \
&lt;/span&gt;&#160; --deploy-mode client \
&#160; --name spark-shell \
&#160; --conf spark.executor.instances=1 \
&#160; --conf spark.kubernetes.container.image=${SPARK_IMAGE} \
&#160; --conf spark.kubernetes.container.image.pullPolicy=Never \
&#160; --conf spark.files.overwrite=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;

scala&amp;gt; sc.addFile(&lt;span class=&quot;code-quote&quot;&gt;&quot;work-dir/test.txt&quot;&lt;/span&gt;)
scala&amp;gt; spark.read.format(&lt;span class=&quot;code-quote&quot;&gt;&quot;text&quot;&lt;/span&gt;).load(&lt;span class=&quot;code-quote&quot;&gt;&quot;work-dir/test.txt&quot;&lt;/span&gt;).show() 
[Stage 0:&amp;gt; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; +-----+
|value|
+-----+
|Hello|
|spark|
| &#160; on|
| &#160;k8s|
+-----+
scala&amp;gt; sc.addFile(&lt;span class=&quot;code-quote&quot;&gt;&quot;work-dir/test.txt&quot;&lt;/span&gt;)
22/02/01 02:54:08 WARN SparkContext: The path work-dir/test.txt has been added already. Overwriting of added paths is not supported in the current version&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;We might be better to consider deleting this option because the PR aimed to avoid concurrency problem, but the behavior above might cause the problem.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Anyway, it is misleading description so I would like to open update PR.&lt;/p&gt;</comment>
                            <comment id="17485046" author="apachespark" created="Tue, 1 Feb 2022 05:35:24 +0000"  >&lt;p&gt;User &apos;yoda-mon&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/35377&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/35377&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17485047" author="apachespark" created="Tue, 1 Feb 2022 05:35:28 +0000"  >&lt;p&gt;User &apos;yoda-mon&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/35377&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/35377&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17486174" author="gurwls223" created="Thu, 3 Feb 2022 00:42:02 +0000"  >&lt;p&gt;Issue resolved by pull request 35377&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/35377&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/35377&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310250" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10431"><![CDATA[Important]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            3 years, 40 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z0ypb4:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>