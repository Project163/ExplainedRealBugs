<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:18:59 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-2546] Configuration object thread safety issue</title>
                <link>https://issues.apache.org/jira/browse/SPARK-2546</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;// observed in 0.9.1 but expected to exist in 1.0.1 as well&lt;/p&gt;

&lt;p&gt;This ticket is copy-pasted from a thread on the dev@ list:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;We discovered a very interesting bug in Spark at work last week in Spark 0.9.1 &#8212; that the way Spark uses the Hadoop Configuration object is prone to thread safety issues.  I believe it still applies in Spark 1.0.1 as well.  Let me explain:&lt;/p&gt;

&lt;p&gt;Observations&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Was running a relatively simple job (read from Avro files, do a map, do another map, write back to Avro files)&lt;/li&gt;
	&lt;li&gt;412 of 413 tasks completed, but the last task was hung in RUNNING state&lt;/li&gt;
	&lt;li&gt;The 412 successful tasks completed in median time 3.4s&lt;/li&gt;
	&lt;li&gt;The last hung task didn&apos;t finish even in 20 hours&lt;/li&gt;
	&lt;li&gt;The executor with the hung task was responsible for 100% of one core of CPU usage&lt;/li&gt;
	&lt;li&gt;Jstack of the executor attached (relevant thread pasted below)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Diagnosis&lt;/p&gt;

&lt;p&gt;After doing some code spelunking, we determined the issue was concurrent use of a Configuration object for each task on an executor.  In Hadoop each task runs in its own JVM, but in Spark multiple tasks can run in the same JVM, so the single-threaded access assumptions of the Configuration object no longer hold in Spark.&lt;/p&gt;

&lt;p&gt;The specific issue is that the AvroRecordReader actually &lt;em&gt;modifies&lt;/em&gt; the JobConf it&apos;s given when it&apos;s instantiated!  It adds a key for the RPC protocol engine in the process of connecting to the Hadoop FileSystem.  When many tasks start at the same time (like at the start of a job), many tasks are adding this configuration item to the one Configuration object at once.  Internally Configuration uses a java.lang.HashMap, which isn&apos;t threadsafe&#8230; The below post is an excellent explanation of what happens in the situation where multiple threads insert into a HashMap at the same time.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://mailinator.blogspot.com/2009/06/beautiful-race-condition.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://mailinator.blogspot.com/2009/06/beautiful-race-condition.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The gist is that you have a thread following a cycle of linked list nodes indefinitely.  This exactly matches our observations of the 100% CPU core and also the final location in the stack trace.&lt;/p&gt;

&lt;p&gt;So it seems the way Spark shares a Configuration object between task threads in an executor is incorrect.  We need some way to prevent concurrent access to a single Configuration object.&lt;/p&gt;


&lt;p&gt;Proposed fix&lt;/p&gt;

&lt;p&gt;We can clone the JobConf object in HadoopRDD.getJobConf() so each task gets its own JobConf object (and thus Configuration object).  The optimization of broadcasting the Configuration object across the cluster can remain, but on the other side I think it needs to be cloned for each task to allow for concurrent access.  I&apos;m not sure the performance implications, but the comments suggest that the Configuration object is ~10KB so I would expect a clone on the object to be relatively speedy.&lt;/p&gt;

&lt;p&gt;Has this been observed before?  Does my suggested fix make sense?  I&apos;d be happy to file a Jira ticket and continue discussion there for the right way to fix.&lt;/p&gt;


&lt;p&gt;Thanks!&lt;br/&gt;
Andrew&lt;/p&gt;


&lt;p&gt;P.S.  For others seeing this issue, our temporary workaround is to enable spark.speculation, which retries failed (or hung) tasks on other machines.&lt;/p&gt;


&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;&quot;Executor task launch worker-6&quot; daemon prio=10 tid=0x00007f91f01fe000 nid=0x54b1 runnable [0x00007f92d74f1000]
   java.lang.Thread.State: RUNNABLE
    at java.util.HashMap.transfer(HashMap.java:601)
    at java.util.HashMap.resize(HashMap.java:581)
    at java.util.HashMap.addEntry(HashMap.java:879)
    at java.util.HashMap.put(HashMap.java:505)
    at org.apache.hadoop.conf.Configuration.set(Configuration.java:803)
    at org.apache.hadoop.conf.Configuration.set(Configuration.java:783)
    at org.apache.hadoop.conf.Configuration.setClass(Configuration.java:1662)
    at org.apache.hadoop.ipc.RPC.setProtocolEngine(RPC.java:193)
    at org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol(NameNodeProxies.java:343)
    at org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:168)
    at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:129)
    at org.apache.hadoop.hdfs.DFSClient.&amp;lt;init&amp;gt;(DFSClient.java:436)
    at org.apache.hadoop.hdfs.DFSClient.&amp;lt;init&amp;gt;(DFSClient.java:403)
    at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:125)
    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2262)
    at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:86)
    at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2296)
    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2278)
    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:316)
    at org.apache.hadoop.fs.Path.getFileSystem(Path.java:194)
    at org.apache.avro.mapred.FsInput.&amp;lt;init&amp;gt;(FsInput.java:37)
    at org.apache.avro.mapred.AvroRecordReader.&amp;lt;init&amp;gt;(AvroRecordReader.java:43)
    at org.apache.avro.mapred.AvroInputFormat.getRecordReader(AvroInputFormat.java:52)
    at org.apache.spark.rdd.HadoopRDD$$anon$1.&amp;lt;init&amp;gt;(HadoopRDD.scala:156)
    at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:149)
    at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:64)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
    at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
    at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
    at org.apache.spark.scheduler.Task.run(Task.scala:53)
    at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)
    at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
    at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:415)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
    at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;</description>
                <environment></environment>
        <key id="12727850">SPARK-2546</key>
            <summary>Configuration object thread safety issue</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="joshrosen">Josh Rosen</assignee>
                                    <reporter username="aash">Andrew Ash</reporter>
                        <labels>
                    </labels>
                <created>Thu, 17 Jul 2014 05:20:44 +0000</created>
                <updated>Tue, 15 Sep 2015 07:21:18 +0000</updated>
                            <resolved>Sun, 19 Oct 2014 07:40:21 +0000</resolved>
                                    <version>0.9.1</version>
                    <version>1.0.2</version>
                    <version>1.1.0</version>
                    <version>1.2.0</version>
                                    <fixVersion>1.0.3</fixVersion>
                    <fixVersion>1.1.1</fixVersion>
                    <fixVersion>1.2.0</fixVersion>
                                    <component>Spark Core</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>11</watches>
                                                                                                                <comments>
                            <comment id="14064625" author="aash" created="Thu, 17 Jul 2014 06:19:08 +0000"  >&lt;p&gt;On the thread:&lt;/p&gt;

&lt;p&gt;Me:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Reynold&apos;s recent announcement of the broadcast RDD object patch may also have implications of the right path forward here.  I&apos;m not sure I fully understand the implications though: &lt;a href=&quot;https://github.com/apache/spark/pull/1452&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/1452&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&quot;Once this is committed, we can also remove the JobConf broadcast in HadoopRDD.&quot;&lt;/p&gt;&lt;/blockquote&gt;


&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pwendell&quot; class=&quot;user-hover&quot; rel=&quot;pwendell&quot;&gt;pwendell&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I think you are correct and a follow up to &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-2521&quot; title=&quot;Broadcast RDD object once per TaskSet (instead of sending it for every task)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-2521&quot;&gt;&lt;del&gt;SPARK-2521&lt;/del&gt;&lt;/a&gt; will end up&lt;br/&gt;
fixing this. The desing of &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-2521&quot; title=&quot;Broadcast RDD object once per TaskSet (instead of sending it for every task)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-2521&quot;&gt;&lt;del&gt;SPARK-2521&lt;/del&gt;&lt;/a&gt; automatically broadcasts RDD&lt;br/&gt;
data in tasks and the approach creates a new copy of the RDD and&lt;br/&gt;
associated data for each task. A natural follow-up to that patch is to&lt;br/&gt;
stop handling the jobConf separately (since we will now broadcast all&lt;br/&gt;
referents of the RDD itself) and just have it broadcasted with the&lt;br/&gt;
RDD. I&apos;m not sure if Reynold plans to include this in &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-2521&quot; title=&quot;Broadcast RDD object once per TaskSet (instead of sending it for every task)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-2521&quot;&gt;&lt;del&gt;SPARK-2521&lt;/del&gt;&lt;/a&gt; or&lt;br/&gt;
afterwards, but it&apos;s likely we&apos;d do that soon.&lt;/p&gt;&lt;/blockquote&gt;</comment>
                            <comment id="14080531" author="pwendell" created="Thu, 31 Jul 2014 05:48:31 +0000"  >&lt;p&gt;Ideally we should merge either this or &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-2585&quot; title=&quot;Remove special handling of Hadoop JobConf&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-2585&quot;&gt;&lt;del&gt;SPARK-2585&lt;/del&gt;&lt;/a&gt; in the 1.1 release.&lt;/p&gt;</comment>
                            <comment id="14086660" author="joshrosen" created="Tue, 5 Aug 2014 19:57:40 +0000"  >&lt;p&gt;Hi Andrew,&lt;/p&gt;

&lt;p&gt;Do you have any way to reliably reproduce this issue?  I&apos;m considering implementing a clone()-based approach and I&apos;d like to have a way to test whether I&apos;ve fixed this bug.&lt;/p&gt;</comment>
                            <comment id="14086830" author="ash211" created="Tue, 5 Aug 2014 21:37:19 +0000"  >&lt;p&gt;I don&apos;t have a reliable repro that&apos;s in a unit test format.  On my prod&lt;br/&gt;
cluster though it reproduces quite reliably!  I&apos;d suggest using the&lt;br/&gt;
AvroInputFormat on sizable files and a large number of partitions &amp;#8211; I had&lt;br/&gt;
O(400) partitions and O(15GB) of data in that dataset.&lt;/p&gt;

&lt;p&gt;Sidenote &amp;#8211; the trouble with unit testing race conditions is that you have&lt;br/&gt;
to run them for a long time in an error-prone situation and hope that the&lt;br/&gt;
behavior is triggered.  You could verify that the Configuration objects&lt;br/&gt;
each partition gets are equal() but not reference equal, but that&apos;s not&lt;br/&gt;
directly testing for the race condition.&lt;/p&gt;


</comment>
                            <comment id="14099239" author="pwendell" created="Fri, 15 Aug 2014 22:02:53 +0000"  >&lt;p&gt;Hey Andrew I think due to us cutting &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-2585&quot; title=&quot;Remove special handling of Hadoop JobConf&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-2585&quot;&gt;&lt;del&gt;SPARK-2585&lt;/del&gt;&lt;/a&gt; from this release it will remain broken in Spark 1.1. We could look into a solution based on clone()&apos;ing the conf for future patch releases in the 1.1 branch.&lt;/p&gt;</comment>
                            <comment id="14099251" author="ash211" created="Fri, 15 Aug 2014 22:11:20 +0000"  >&lt;p&gt;Ok I&apos;ll stay on the lookout for this bug and ping here again if we observe&lt;br/&gt;
this.  Luckily we haven&apos;t seen this particular issue since, but that&apos;s&lt;br/&gt;
mostly been because other things are causing problems.&lt;/p&gt;

&lt;p&gt;We have a few bugs now that are nondeterministically broken in Spark and&lt;br/&gt;
cause jobs to fail/hang, but if we retry the job several times (and&lt;br/&gt;
spark.speculation helps somewhat) we can usually eventually get a job to&lt;br/&gt;
complete.  I can share that list if you&apos;re interested of what&apos;s highest on&lt;br/&gt;
our minds right now.&lt;/p&gt;


&lt;p&gt;On Fri, Aug 15, 2014 at 6:03 PM, Patrick Wendell (JIRA) &amp;lt;jira@apache.org&amp;gt;&lt;/p&gt;
</comment>
                            <comment id="14148244" author="aash" created="Thu, 25 Sep 2014 20:18:36 +0000"  >&lt;p&gt;Another proposed fix: extend JobConf as a shim and replace the Hadoop one with one that&apos;s threadsafe&lt;/p&gt;</comment>
                            <comment id="14148645" author="joshrosen" created="Fri, 26 Sep 2014 02:36:49 +0000"  >&lt;p&gt;JobConf has a &lt;em&gt;ton&lt;/em&gt; of methods and it&apos;s not clear whether we can get away with synchronizing only some of them.&lt;/p&gt;

&lt;p&gt;I&apos;m going to look into using Scala macro annotations (&lt;a href=&quot;http://docs.scala-lang.org/overviews/macros/annotations.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://docs.scala-lang.org/overviews/macros/annotations.html&lt;/a&gt;) to create a &lt;tt&gt;@synchronizeAll&lt;/tt&gt; macro for adding synchronization to all methods of a class.&lt;/p&gt;</comment>
                            <comment id="14148657" author="joshrosen" created="Fri, 26 Sep 2014 02:56:30 +0000"  >&lt;p&gt;A synchronization wrapper (whether written by hand or generated using macros) might introduce an unwanted runtime dependency on the exact compile-time version of Hadoop that we used.  For example, say we compile against Hadoop 1.x and run on Hadoop 1.y (where y &amp;gt; x) and the runtime version of JobConf contains methods that were not present in the version that we wrapped at compile-time.  What happens in this case?&lt;/p&gt;

&lt;p&gt;Before we explore this option, I should probably re-visit &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-2585&quot; title=&quot;Remove special handling of Hadoop JobConf&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-2585&quot;&gt;&lt;del&gt;SPARK-2585&lt;/del&gt;&lt;/a&gt; to see if I can understand why the patch seemed to introduce a performance regression, since that approach is Hadoop version agnostic.&lt;/p&gt;</comment>
                            <comment id="14160790" author="joshrosen" created="Mon, 6 Oct 2014 19:28:42 +0000"  >&lt;p&gt;I&apos;ve decided to go with the cloning approach, since this seems simplest and safest.&lt;/p&gt;

&lt;p&gt;It looks like SparkContext has a public &lt;tt&gt;hadoopConfiguration&lt;/tt&gt; &lt;tt&gt;val&lt;/tt&gt; that holds a re-used Configuration object.  It looks like this may have been purposely exposed to allow users to set Hadoop configuration properties (see how it&apos;s mentioned in docs/storage-openstack-swift.md; the Spark EC2 instructions also mention using this attribute to set S3 credentials).  This object is used as the default Hadoop configuration in the &lt;tt&gt;newAPIHadoopRDD&lt;/tt&gt; and &lt;tt&gt;saveAsHadoop*&lt;/tt&gt; methods; it&apos;s also read in many other places inside of Spark.&lt;/p&gt;

&lt;p&gt;While &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-2585&quot; title=&quot;Remove special handling of Hadoop JobConf&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-2585&quot;&gt;&lt;del&gt;SPARK-2585&lt;/del&gt;&lt;/a&gt; addressed sharing of the Configuration objects in executors, it seems that we still might face races in the driver if multiple threads are sharing a SparkContext and one thread mutates the shared configuration while another thread submits a job that reads it.&lt;/p&gt;

&lt;p&gt;This seems like a tricky problem to fix.  I don&apos;t think that we can change &lt;tt&gt;SparkContext.hadoopConfiguration&lt;/tt&gt; to return a copy of the configuration object, since it seems that the shared / mutating semantics are required by some existing code.  At the same time, we can&apos;t simply clone the return value before using it in our internal driver-side code since a) we can&apos;t lock out writers/mutators while performing the clone() and b) the change in semantics might break existing user-code.  Essentially, I don&apos;t think that there&apos;s anything that we can do that&apos;s guaranteed to be safe once a Configuration has been exposed to multiple threads; we need to perform the cloning before the object has been shared.&lt;/p&gt;</comment>
                            <comment id="14160842" author="joshrosen" created="Mon, 6 Oct 2014 20:00:11 +0000"  >&lt;p&gt;Here are a few &quot;in the wild&quot; examples of how &lt;tt&gt;sc.hadoopConfiguration&lt;/tt&gt; is currently used, to give a sense of the impact of any changes that we might make here.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/barnybug/spark-elasticsearch-blogpost/blob/master/Main.scala#L23&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Setting elasticserach configuration properties&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;sc.hadoopConfiguration.set(&lt;span class=&quot;code-quote&quot;&gt;&quot;es.resource&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;syslog/entry&quot;&lt;/span&gt;)
output.saveAsHadoopFile[ESOutputFormat](&lt;span class=&quot;code-quote&quot;&gt;&quot;-&quot;&lt;/span&gt;)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;http://stackoverflow.com/a/26156429/590203&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Setting S3 credentials&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;val conf = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; SparkConf().setAppName(&lt;span class=&quot;code-quote&quot;&gt;&quot;Simple Application&quot;&lt;/span&gt;).setMaster(&lt;span class=&quot;code-quote&quot;&gt;&quot;local&quot;&lt;/span&gt;)      
val sc = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; SparkContext(conf)
val hadoopConf=sc.hadoopConfiguration;
hadoopConf.set(&lt;span class=&quot;code-quote&quot;&gt;&quot;fs.s3.impl&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.hadoop.fs.s3native.NativeS3FileSystem&quot;&lt;/span&gt;)
hadoopConf.set(&lt;span class=&quot;code-quote&quot;&gt;&quot;fs.s3.awsAccessKeyId&quot;&lt;/span&gt;,myAccessKey)
hadoopConf.set(&lt;span class=&quot;code-quote&quot;&gt;&quot;fs.s3.awsSecretAccessKey&quot;&lt;/span&gt;,mySecretKey)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There&apos;s a lot more examples here: &lt;a href=&quot;https://github.com/search?utf8=%E2%9C%93&amp;amp;q=%22sc.hadoopconfiguration%22&amp;amp;type=Code&amp;amp;ref=searchresults&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/search?utf8=%E2%9C%93&amp;amp;q=%22sc.hadoopconfiguration%22&amp;amp;type=Code&amp;amp;ref=searchresults&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The most common use-case seems to be setting S3 credentials.  One option would be to slowly deprecate the existing &lt;tt&gt;hadoopConfiguration&lt;/tt&gt; field in favor of methods for setting S3 credentials.  Currently, you can set these options in SparkConf before creating the SparkContext; unfortunately, this isn&apos;t an option for users that want to set configurations after starting SparkContext (e.g. IPython notebook users).  I suppose that these users could work with a clone of the configuration object and manually pass that object into Spark methods.&lt;/p&gt;

&lt;p&gt;If we did add a SparkContext-wide setting for changing Hadoop configurations, then in multi-user shared-SparkContext environments we run the risk of users overwriting each others&apos; S3 credentials.&lt;/p&gt;</comment>
                            <comment id="14160892" author="joshrosen" created="Mon, 6 Oct 2014 20:33:46 +0000"  >&lt;p&gt;For now, let&apos;s ignore the design issue of whether the current API is confusing in multi-user shared-SparkContext environments.  If we want to keep the current API without any driver-side thread-safety issues, is there anything that we can do?&lt;/p&gt;

&lt;p&gt;Maybe we can add a very limited amount of synchronization to Configuration.  &lt;a href=&quot;https://github.com/apache/hadoop/blob/d989ac04449dc33da5e2c32a7f24d59cc92de536/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java#L666&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Looking at a recent version of Configuration.java&lt;/a&gt;, it seems that the private &lt;tt&gt;updatingResource&lt;/tt&gt; HashMap and &lt;tt&gt;finalParameters&lt;/tt&gt; HashSet fields the only non-thread-safe collections in Configuration (Java&apos;s &lt;tt&gt;Properties&lt;/tt&gt; class is thread-safe).&lt;/p&gt;

&lt;p&gt;My hunch is that the &lt;tt&gt;updatingResource&lt;/tt&gt; HashMap was the map referred to by the stacktrace posted in this issue.  We might be able to use reflection to find this field and inject a synchronized HashMap instead.&lt;/p&gt;</comment>
                            <comment id="14161013" author="aash" created="Mon, 6 Oct 2014 21:41:23 +0000"  >&lt;p&gt;Excellent research Josh!&lt;/p&gt;

&lt;p&gt;I agree that we should pass for now on the driver-side thread-safety issues.  All the issues I&apos;ve encountered so far have been in multiple accesses on the executor side, which the cloning on access approach seems to take care of.&lt;/p&gt;</comment>
                            <comment id="14161214" author="apachespark" created="Mon, 6 Oct 2014 23:35:47 +0000"  >&lt;p&gt;User &apos;JoshRosen&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/2684&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/2684&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14175608" author="aash" created="Fri, 17 Oct 2014 21:59:20 +0000"  >&lt;p&gt;We tested Josh&apos;s patch, confirming the fix and measuring the perf regression at ~8%&lt;/p&gt;</comment>
                            <comment id="14176243" author="joshrosen" created="Sun, 19 Oct 2014 07:40:21 +0000"  >&lt;p&gt;Issue resolved by pull request 2684&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/2684&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/2684&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14176245" author="joshrosen" created="Sun, 19 Oct 2014 07:45:19 +0000"  >&lt;p&gt;I&apos;ve fixed this in HadoopRDD and applied my fix to all branches.  Note that the fix is currently guarded by a configuration option, &lt;tt&gt;spark.hadoop.cloneConf&lt;/tt&gt;.  This is in order to avoid unexpected performance regressions when users who were unaffected by this issue choose to upgrade to 1.1.1 or 1.0.3.  We&apos;ll probably make cloning the default in 1.2.0 and may spend some more time trying to understand its performance implications.&lt;/p&gt;

&lt;p&gt;Note that this does not address the potential for thread-safety issues due to Configuration-sharing on the driver.  As described upthread, this is a much harder issue to fix.  Since I&apos;m not aware of any cases where this has caused issues on the driver, I&apos;m inclined to wait things out and address that if it&apos;s discovered to be an issue.  &lt;/p&gt;

&lt;p&gt;I&apos;ve opened &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-11209&quot; title=&quot;Configuration#updatingResource/finalParameters are not thread-safe&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-11209&quot;&gt;&lt;del&gt;HADOOP-11209&lt;/del&gt;&lt;/a&gt; to try to fix the Configuration thread-safety issues upstream, so hopefully this won&apos;t be a problem in the future.&lt;/p&gt;</comment>
                            <comment id="14286971" author="ozawa" created="Thu, 22 Jan 2015 05:22:31 +0000"  >&lt;p&gt;Now &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-11209&quot; title=&quot;Configuration#updatingResource/finalParameters are not thread-safe&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-11209&quot;&gt;&lt;del&gt;HADOOP-11209&lt;/del&gt;&lt;/a&gt;, the problem reported by &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=joshrosen&quot; class=&quot;user-hover&quot; rel=&quot;joshrosen&quot;&gt;joshrosen&lt;/a&gt;, is resolved by &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=varun_saxena&quot; class=&quot;user-hover&quot; rel=&quot;varun_saxena&quot;&gt;varun_saxena&lt;/a&gt;&apos;s contribution. Thanks for your reporting.&lt;/p&gt;</comment>
                            <comment id="14627863" author="ankurmitujjain" created="Wed, 15 Jul 2015 10:29:07 +0000"  >&lt;p&gt;This exists in SPARK 1.4 too...&lt;/p&gt;</comment>
                            <comment id="14627909" author="ozawa" created="Wed, 15 Jul 2015 11:29:12 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=anknai&quot; class=&quot;user-hover&quot; rel=&quot;anknai&quot;&gt;anknai&lt;/a&gt; cc: &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=joshrosen&quot; class=&quot;user-hover&quot; rel=&quot;joshrosen&quot;&gt;joshrosen&lt;/a&gt; the problem is fixed in Hadoop 2.7. Could you build spark with hadoop.version=2.7.1? I&apos;ll also backport the patch to 2.6.x, but it takes a bit time to release.&lt;/p&gt;</comment>
                            <comment id="14627914" author="ankurmitujjain" created="Wed, 15 Jul 2015 11:33:29 +0000"  >&lt;p&gt;Thanks Tsuyoshi...&lt;br/&gt;
I thought fix is already done for version 1.0.3, 1.1.1, 1.2.0. &lt;br/&gt;
So 1.4.0 should have this fix with it....&lt;/p&gt;

&lt;p&gt;Anyways this means that on EMR we will face this issue as they are using Hadoop 2.4.0&lt;/p&gt;</comment>
                            <comment id="14631465" author="joshrosen" created="Fri, 17 Jul 2015 15:24:58 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ankurmitujjain&quot; class=&quot;user-hover&quot; rel=&quot;ankurmitujjain&quot;&gt;ankurmitujjain&lt;/a&gt;, you can try setting &lt;tt&gt;spark.hadoop.cloneConf=true&lt;/tt&gt; in your SparkConf in order to enable additional defensive copying that is designed to guard against this issue. This setting is off by default because this cloning is actually fairly expensive because new &lt;tt&gt;Configuration&lt;/tt&gt; objects are costly to create.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12864158">SPARK-10611</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12704582">SPARK-1097</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12748986">HADOOP-11209</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12728262">SPARK-2585</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="12310040">
                    <name>Required</name>
                                            <outwardlinks description="requires">
                                        <issuelink>
            <issuekey id="12727603">SPARK-2521</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>405955</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 18 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1xuy7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>405975</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12327369">1.2.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>