<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:14:44 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-1394] calling system.platform on worker raises IOError</title>
                <link>https://issues.apache.org/jira/browse/SPARK-1394</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;A simple program that calls system.platform() on the worker fails most of the time (it works some times but very rarely).&lt;br/&gt;
This is critical since many libraries call that method (e.g. boto).&lt;/p&gt;

&lt;p&gt;Here is the trace of the attempt to call that method:&lt;/p&gt;



&lt;p&gt;$ /usr/local/spark/bin/pyspark&lt;br/&gt;
Python 2.7.3 (default, Feb 27 2014, 20:00:17)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;GCC 4.6.3&amp;#93;&lt;/span&gt; on linux2&lt;br/&gt;
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&lt;br/&gt;
14/04/02 18:18:37 INFO Utils: Using Spark&apos;s default log4j profile: org/apache/spark/log4j-defaults.properties&lt;br/&gt;
14/04/02 18:18:37 WARN Utils: Your hostname, qlika-dev resolves to a loopback address: 127.0.1.1; using 10.33.102.46 instead (on interface eth1)&lt;br/&gt;
14/04/02 18:18:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address&lt;br/&gt;
14/04/02 18:18:38 INFO Slf4jLogger: Slf4jLogger started&lt;br/&gt;
14/04/02 18:18:38 INFO Remoting: Starting remoting&lt;br/&gt;
14/04/02 18:18:39 INFO Remoting: Remoting started; listening on addresses :&lt;span class=&quot;error&quot;&gt;&amp;#91;akka.tcp://spark@10.33.102.46:36640&amp;#93;&lt;/span&gt;&lt;br/&gt;
14/04/02 18:18:39 INFO Remoting: Remoting now listens on addresses: &lt;span class=&quot;error&quot;&gt;&amp;#91;akka.tcp://spark@10.33.102.46:36640&amp;#93;&lt;/span&gt;&lt;br/&gt;
14/04/02 18:18:39 INFO SparkEnv: Registering BlockManagerMaster&lt;br/&gt;
14/04/02 18:18:39 INFO DiskBlockManager: Created local directory at /tmp/spark-local-20140402181839-919f&lt;br/&gt;
14/04/02 18:18:39 INFO MemoryStore: MemoryStore started with capacity 294.6 MB.&lt;br/&gt;
14/04/02 18:18:39 INFO ConnectionManager: Bound socket to port 43357 with id = ConnectionManagerId(10.33.102.46,43357)&lt;br/&gt;
14/04/02 18:18:39 INFO BlockManagerMaster: Trying to register BlockManager&lt;br/&gt;
14/04/02 18:18:39 INFO BlockManagerMasterActor$BlockManagerInfo: Registering block manager 10.33.102.46:43357 with 294.6 MB RAM&lt;br/&gt;
14/04/02 18:18:39 INFO BlockManagerMaster: Registered BlockManager&lt;br/&gt;
14/04/02 18:18:39 INFO HttpServer: Starting HTTP Server&lt;br/&gt;
14/04/02 18:18:39 INFO HttpBroadcast: Broadcast server started at &lt;a href=&quot;http://10.33.102.46:51803&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://10.33.102.46:51803&lt;/a&gt;&lt;br/&gt;
14/04/02 18:18:39 INFO SparkEnv: Registering MapOutputTracker&lt;br/&gt;
14/04/02 18:18:39 INFO HttpFileServer: HTTP File server directory is /tmp/spark-9b38acb0-7b01-4463-b0a6-602bfed05a2b&lt;br/&gt;
14/04/02 18:18:39 INFO HttpServer: Starting HTTP Server&lt;br/&gt;
14/04/02 18:18:40 INFO SparkUI: Started Spark Web UI at &lt;a href=&quot;http://10.33.102.46:4040&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://10.33.102.46:4040&lt;/a&gt;&lt;br/&gt;
14/04/02 18:18:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable&lt;br/&gt;
Welcome to&lt;br/&gt;
      ____              __&lt;br/&gt;
     / _&lt;em&gt;/&lt;/em&gt;_  ___ ____&lt;em&gt;/ /&lt;/em&gt;_&lt;br/&gt;
    &lt;em&gt;\ \/ _ \/ _ `/ __/  &apos;&lt;/em&gt;/&lt;br/&gt;
   /__ / ._&lt;em&gt;/&amp;#95;,&lt;/em&gt;/&lt;em&gt;/ /&lt;/em&gt;/&amp;#95;\   version 0.9.0&lt;br/&gt;
      /_/&lt;/p&gt;

&lt;p&gt;Using Python version 2.7.3 (default, Feb 27 2014 20:00:17)&lt;br/&gt;
Spark context available as sc.&lt;br/&gt;
&amp;gt;&amp;gt;&amp;gt; import platform&lt;br/&gt;
&amp;gt;&amp;gt;&amp;gt; sc.parallelize(&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;).map(lambda x : platform.system()).collect()&lt;br/&gt;
14/04/02 18:19:17 INFO SparkContext: Starting job: collect at &amp;lt;stdin&amp;gt;:1&lt;br/&gt;
14/04/02 18:19:17 INFO DAGScheduler: Got job 0 (collect at &amp;lt;stdin&amp;gt;:1) with 1 output partitions (allowLocal=false)&lt;br/&gt;
14/04/02 18:19:17 INFO DAGScheduler: Final stage: Stage 0 (collect at &amp;lt;stdin&amp;gt;:1)&lt;br/&gt;
14/04/02 18:19:17 INFO DAGScheduler: Parents of final stage: List()&lt;br/&gt;
14/04/02 18:19:17 INFO DAGScheduler: Missing parents: List()&lt;br/&gt;
14/04/02 18:19:17 INFO DAGScheduler: Submitting Stage 0 (PythonRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; at collect at &amp;lt;stdin&amp;gt;:1), which has no missing parents&lt;br/&gt;
14/04/02 18:19:17 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (PythonRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; at collect at &amp;lt;stdin&amp;gt;:1)&lt;br/&gt;
14/04/02 18:19:17 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks&lt;br/&gt;
14/04/02 18:19:17 INFO TaskSetManager: Starting task 0.0:0 as TID 0 on executor localhost: localhost (PROCESS_LOCAL)&lt;br/&gt;
14/04/02 18:19:17 INFO TaskSetManager: Serialized task 0.0:0 as 2152 bytes in 12 ms&lt;br/&gt;
14/04/02 18:19:17 INFO Executor: Running task ID 0&lt;br/&gt;
PySpark worker failed with exception:&lt;br/&gt;
Traceback (most recent call last):&lt;br/&gt;
  File &quot;/usr/local/spark/python/pyspark/worker.py&quot;, line 77, in main&lt;br/&gt;
    serializer.dump_stream(func(split_index, iterator), outfile)&lt;br/&gt;
  File &quot;/usr/local/spark/python/pyspark/serializers.py&quot;, line 182, in dump_stream&lt;br/&gt;
    self.serializer.dump_stream(self._batched(iterator), stream)&lt;br/&gt;
  File &quot;/usr/local/spark/python/pyspark/serializers.py&quot;, line 117, in dump_stream&lt;br/&gt;
    for obj in iterator:&lt;br/&gt;
  File &quot;/usr/local/spark/python/pyspark/serializers.py&quot;, line 171, in _batched&lt;br/&gt;
    for item in iterator:&lt;br/&gt;
  File &quot;&amp;lt;stdin&amp;gt;&quot;, line 1, in &amp;lt;lambda&amp;gt;&lt;br/&gt;
  File &quot;/usr/lib/python2.7/platform.py&quot;, line 1306, in system&lt;br/&gt;
    return uname()&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;&lt;br/&gt;
  File &quot;/usr/lib/python2.7/platform.py&quot;, line 1273, in uname&lt;br/&gt;
    processor = _syscmd_uname(&apos;-p&apos;,&apos;&apos;)&lt;br/&gt;
  File &quot;/usr/lib/python2.7/platform.py&quot;, line 1030, in _syscmd_uname&lt;br/&gt;
    rc = f.close()&lt;br/&gt;
IOError: &lt;span class=&quot;error&quot;&gt;&amp;#91;Errno 10&amp;#93;&lt;/span&gt; No child processes&lt;/p&gt;

&lt;p&gt;14/04/02 18:19:17 ERROR Executor: Exception in task ID 0&lt;br/&gt;
org.apache.spark.api.python.PythonException: Traceback (most recent call last):&lt;br/&gt;
  File &quot;/usr/local/spark/python/pyspark/worker.py&quot;, line 77, in main&lt;br/&gt;
    serializer.dump_stream(func(split_index, iterator), outfile)&lt;br/&gt;
  File &quot;/usr/local/spark/python/pyspark/serializers.py&quot;, line 182, in dump_stream&lt;br/&gt;
    self.serializer.dump_stream(self._batched(iterator), stream)&lt;br/&gt;
  File &quot;/usr/local/spark/python/pyspark/serializers.py&quot;, line 117, in dump_stream&lt;br/&gt;
    for obj in iterator:&lt;br/&gt;
  File &quot;/usr/local/spark/python/pyspark/serializers.py&quot;, line 171, in _batched&lt;br/&gt;
    for item in iterator:&lt;br/&gt;
  File &quot;&amp;lt;stdin&amp;gt;&quot;, line 1, in &amp;lt;lambda&amp;gt;&lt;br/&gt;
  File &quot;/usr/lib/python2.7/platform.py&quot;, line 1306, in system&lt;br/&gt;
    return uname()&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;&lt;br/&gt;
  File &quot;/usr/lib/python2.7/platform.py&quot;, line 1273, in uname&lt;br/&gt;
    processor = _syscmd_uname(&apos;-p&apos;,&apos;&apos;)&lt;br/&gt;
  File &quot;/usr/lib/python2.7/platform.py&quot;, line 1030, in _syscmd_uname&lt;br/&gt;
    rc = f.close()&lt;br/&gt;
IOError: &lt;span class=&quot;error&quot;&gt;&amp;#91;Errno 10&amp;#93;&lt;/span&gt; No child processes&lt;/p&gt;

&lt;p&gt;        at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:131)&lt;br/&gt;
        at org.apache.spark.api.python.PythonRDD$$anon$1.&amp;lt;init&amp;gt;(PythonRDD.scala:153)&lt;br/&gt;
        at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:96)&lt;br/&gt;
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)&lt;br/&gt;
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)&lt;br/&gt;
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)&lt;br/&gt;
        at org.apache.spark.scheduler.Task.run(Task.scala:53)&lt;br/&gt;
        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)&lt;br/&gt;
        at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)&lt;br/&gt;
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:744)&lt;/p&gt;</description>
                <environment>&lt;p&gt;Tested on Ubuntu and Linux, local and remote master, python 2.7.*&lt;/p&gt;</environment>
        <key id="12706259">SPARK-1394</key>
            <summary>calling system.platform on worker raises IOError</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="idanzalz">Idan Zalzberg</reporter>
                        <labels>
                            <label>pyspark</label>
                    </labels>
                <created>Wed, 2 Apr 2014 18:29:07 +0000</created>
                <updated>Fri, 25 Jul 2014 21:36:09 +0000</updated>
                            <resolved>Fri, 25 Jul 2014 21:36:09 +0000</resolved>
                                    <version>0.9.0</version>
                                    <fixVersion>1.0.1</fixVersion>
                                    <component>PySpark</component>
                        <due></due>
                            <votes>2</votes>
                                    <watches>7</watches>
                                                                                                                <comments>
                            <comment id="13959735" author="idanzalz" created="Fri, 4 Apr 2014 07:51:17 +0000"  >&lt;p&gt;This seems to be related to the way the handle_sigchld method in daemon.py works.&lt;br/&gt;
In order to kill the zombie processes the worker calls os.waitpid on SIGCHLD. however. since using Popen also tries to do that eventually, you get a closed handle.&lt;/p&gt;

&lt;p&gt;Since platform.py is a native library, I would guess we should find a solution in pyspark (i.e. change the way handle_sigchld works, or maybe limit the processes it waits on)&lt;/p&gt;</comment>
                            <comment id="13984066" author="frol" created="Tue, 29 Apr 2014 06:23:42 +0000"  >&lt;p&gt;I have the same issue with Spark 0.9.1. Is there any workaround?&lt;/p&gt;</comment>
                            <comment id="13984070" author="idanzalz" created="Tue, 29 Apr 2014 06:32:21 +0000"  >&lt;p&gt;If you have an _&lt;em&gt;init&lt;/em&gt;_.py  you are sure to go through, you can add the following code to your file:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;# PySpark adds a SIGCHLD signal handler, but that breaks other packages, so we remove it
&lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt;:
    &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; signal
    signal.signal(signal.SIGCHLD, signal.SIG_DFL)
except: pass
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It&apos;s a work around, it would be better to have a &quot;smart&quot; signal handler that only handles the processes that are direct descendants of the daemon. I might try to get something like that out. &lt;/p&gt;</comment>
                            <comment id="13985007" author="frol" created="Wed, 30 Apr 2014 00:31:14 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=idanzalz&quot; class=&quot;user-hover&quot; rel=&quot;idanzalz&quot;&gt;idanzalz&lt;/a&gt; unfortunately, it had helped to avoid only one exception, so I commented signal binding in PySpark and these crashes went away. I hope it will be fixed somehow in next Spark release.&lt;/p&gt;</comment>
                            <comment id="13992920" author="sgottipa" created="Thu, 8 May 2014 16:57:23 +0000"  >&lt;p&gt;We also bumping into the same issue. My I know, how and where can we comment the signal binding in pyspark?&lt;/p&gt;</comment>
                            <comment id="13993339" author="frol" created="Fri, 9 May 2014 02:53:03 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sgottipa&quot; class=&quot;user-hover&quot; rel=&quot;sgottipa&quot;&gt;sgottipa&lt;/a&gt; spark/python/pyspark/daemon.py:75     #signal.signal(SIGCHLD, handle_sigchld)&lt;/p&gt;</comment>
                            <comment id="14004032" author="mrocklin" created="Tue, 20 May 2014 21:54:45 +0000"  >&lt;p&gt;For what it&apos;s worth the platform library is unfortunately called by a number of numerical libraries associated to machine learning.  In particular&lt;/p&gt;

&lt;p&gt;Theano calls on platform&lt;br/&gt;
NumExpr calls on platform&lt;br/&gt;
Pandas calls on NumExpr&lt;/p&gt;

&lt;p&gt;These libraries are very popular in the numerics / machine learning space.&lt;/p&gt;</comment>
                            <comment id="14004047" author="frol" created="Tue, 20 May 2014 22:06:32 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mrocklin&quot; class=&quot;user-hover&quot; rel=&quot;mrocklin&quot;&gt;mrocklin&lt;/a&gt; What? What is your reply for?&lt;/p&gt;</comment>
                            <comment id="14004050" author="mrocklin" created="Tue, 20 May 2014 22:09:17 +0000"  >&lt;p&gt;Just trying to lend weight to this issue by adding context.  &lt;br/&gt;
Let me know if this is an inappropriate use of this forum.&lt;/p&gt;</comment>
                            <comment id="14046216" author="farrellee" created="Fri, 27 Jun 2014 18:06:18 +0000"  >&lt;p&gt;i&apos;m taking a look at this&lt;/p&gt;</comment>
                            <comment id="14046301" author="farrellee" created="Fri, 27 Jun 2014 19:15:35 +0000"  >&lt;p&gt;fyi, this certainly looks like the waitpid(0,...) cleanup handler is cleaning up more than it should be&lt;/p&gt;

&lt;p&gt;also, fyi, if you comment it out you&apos;ll start accumulating defunct worker processes, which is not good&lt;/p&gt;</comment>
                            <comment id="14046349" author="farrellee" created="Fri, 27 Jun 2014 20:14:01 +0000"  >&lt;p&gt;the python daemon does two levels of forking. the manager forks workers and each worker forks sub-workers, who then handle connections.&lt;/p&gt;

&lt;p&gt;the master has a sigchld handler to cleanup workers. the workers also have sigchld handlers to clean up sub-workers.&lt;/p&gt;

&lt;p&gt;the problem here is the worker&apos;s handler is also installed on the sub-worker, which interferes with calls like platform.system() (but not os.uname, btw!)&lt;/p&gt;

&lt;p&gt;i assert that the sub-workers, who do not intentionally fork, should not be responsible for cleaning up unexpected sub-processes, and as such should not have a sigchld handler.&lt;/p&gt;

&lt;p&gt;if it&apos;s desired to have tighter control of process cleanup, namespaces should be used and it should be the manager&apos;s responsibility.&lt;/p&gt;

&lt;p&gt;pull request - &lt;a href=&quot;https://github.com/apache/spark/pull/1247&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/1247&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14047085" author="farrellee" created="Sun, 29 Jun 2014 09:34:22 +0000"  >&lt;p&gt;this should be resolved, @aarondav&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>384582</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            11 years, 21 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1u96f:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>384850</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>