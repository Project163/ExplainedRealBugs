<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 19:32:09 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-18011] SparkR serialize &quot;NA&quot; throws exception</title>
                <link>https://issues.apache.org/jira/browse/SPARK-18011</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;For some versions of R, if Date has &quot;NA&quot; field, backend will throw negative index exception.&lt;br/&gt;
To reproduce the problem:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&amp;gt; a &amp;lt;- as.Date(c(&lt;span class=&quot;code-quote&quot;&gt;&quot;2016-11-11&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;NA&quot;&lt;/span&gt;))
&amp;gt; b &amp;lt;- as.data.frame(a)
&amp;gt; c &amp;lt;- createDataFrame(b)
&amp;gt; dim(c)

16/10/19 10:31:24 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)
java.lang.NegativeArraySizeException
	at org.apache.spark.api.r.SerDe$.readStringBytes(SerDe.scala:110)
	at org.apache.spark.api.r.SerDe$.readString(SerDe.scala:119)
	at org.apache.spark.api.r.SerDe$.readDate(SerDe.scala:128)
	at org.apache.spark.api.r.SerDe$.readTypedObject(SerDe.scala:77)
	at org.apache.spark.api.r.SerDe$.readObject(SerDe.scala:61)
	at org.apache.spark.sql.api.r.SQLUtils$$anonfun$bytesToRow$1.apply(SQLUtils.scala:161)
	at org.apache.spark.sql.api.r.SQLUtils$$anonfun$bytesToRow$1.apply(SQLUtils.scala:160)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.Range.foreach(Range.scala:160)
	at scala.collection.TraversableLike$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.api.r.SQLUtils$.bytesToRow(SQLUtils.scala:160)
	at org.apache.spark.sql.api.r.SQLUtils$$anonfun$5.apply(SQLUtils.scala:138)
	at org.apache.spark.sql.api.r.SQLUtils$$anonfun$5.apply(SQLUtils.scala:138)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:372)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="13013588">SPARK-18011</key>
            <summary>SparkR serialize &quot;NA&quot; throws exception</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="4">Incomplete</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="wm624">Miao Wang</reporter>
                        <labels>
                            <label>bulk-closed</label>
                    </labels>
                <created>Wed, 19 Oct 2016 17:31:45 +0000</created>
                <updated>Tue, 31 Jan 2023 05:05:19 +0000</updated>
                            <resolved>Tue, 21 May 2019 04:15:37 +0000</resolved>
                                                                    <component>SparkR</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="15589331" author="wm624" created="Wed, 19 Oct 2016 17:32:41 +0000"  >&lt;p&gt;We have detailed discussions on PR &lt;a href=&quot;https://github.com/apache/spark/pull/15421&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/15421&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Now, we are trying to find a solution.&lt;/p&gt;</comment>
                            <comment id="15810660" author="felixcheung" created="Mon, 9 Jan 2017 04:33:25 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=wangmiao1981&quot; class=&quot;user-hover&quot; rel=&quot;wangmiao1981&quot;&gt;wangmiao1981&lt;/a&gt;do you remember this one? I thought at one point you said you were close to a fix? Would you be interested in addressing this?&lt;/p&gt;</comment>
                            <comment id="15812306" author="wm624" created="Mon, 9 Jan 2017 17:22:51 +0000"  >&lt;p&gt;The problem is for some R version (e.g., the version on my mac), serializer doesn&apos;t encode length of `NA`, but only value `NA` as a string. &lt;/p&gt;

&lt;p&gt;Let me comment out the exception handling in #15421 and debug it again. I think I am close to find the right fix. I will work on this one. Thanks!&lt;/p&gt;

&lt;p&gt;Miao&lt;/p&gt;</comment>
                            <comment id="15827032" author="wm624" created="Tue, 17 Jan 2017 23:26:22 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=felixcheung&quot; class=&quot;user-hover&quot; rel=&quot;felixcheung&quot;&gt;felixcheung&lt;/a&gt; I did intensive debug in both R side and scala side.&lt;/p&gt;

&lt;p&gt;On R side, I debugged `createDataFrame.default` and `parallelize`, which converts the data.frame into RDD and DataFrame. The code of turning the data into RDD is done in `parallelize`:&lt;br/&gt;
 sliceLen &amp;lt;- ceiling(length(coll) / numSlices)&lt;br/&gt;
 slices &amp;lt;- split(coll, rep(1: (numSlices + 1), each = sliceLen)&lt;span class=&quot;error&quot;&gt;&amp;#91;1:length(coll)&amp;#93;&lt;/span&gt;)&lt;br/&gt;
 serializedSlices &amp;lt;- lapply(slices, serialize, connection = NULL)&lt;/p&gt;

&lt;p&gt;I add debug message after the `serialize`:&lt;br/&gt;
lapply(serializedSlices, function(`x`) &lt;/p&gt;
{message(paste(&quot;unserialized &quot;, unserialize(x)))}
&lt;p&gt;)&lt;/p&gt;

&lt;p&gt;The data `NA` is unserialized successfully. &lt;/p&gt;

&lt;p&gt;Then, the serialized data is transferred to Scala side by jrdd &amp;lt;- callJStatic(&quot;org.apache.spark.api.r.RRDD&quot;, &quot;createRDDFromArray&quot;, sc, serializedSlices) and returns a handle of the RDD in `jrdd`, which is later used by `createDataFrame.default`.&lt;/p&gt;

&lt;p&gt;I did not find anything wrong here.&lt;/p&gt;

&lt;p&gt;On the Scala side, the problem happens in &lt;/p&gt;

&lt;p&gt;def readString(in: DataInputStream): String = &lt;/p&gt;
{
    val len = in.readInt() &amp;lt;=== it encounters the problem when reading `NA` as a string.
    readStringBytes(in, len)
  }

&lt;p&gt;Then, I changed the logic as follows:&lt;br/&gt;
 def readString(in: DataInputStream): String = {&lt;br/&gt;
    var len = in.readInt()&lt;br/&gt;
    if (len &amp;lt; 0) &lt;/p&gt;
{
      len = 3&amp;lt;===== I enforce reading 3 bytes in this case, because I believe that it is the case of `NA`
    }
&lt;p&gt;    readStringBytes(in, len)&lt;br/&gt;
  }&lt;/p&gt;

&lt;p&gt;Then I run the following commands in sparkR:&lt;/p&gt;

&lt;p&gt;&amp;gt; a &amp;lt;- as.Date(NA)&lt;br/&gt;
&amp;gt; b &amp;lt;- as.data.frame(a)&lt;br/&gt;
&amp;gt; c &amp;lt;- collect(select(createDataFrame(b), &quot;*&quot;))&lt;br/&gt;
&amp;gt; c&lt;br/&gt;
   a&lt;br/&gt;
1 NA&lt;br/&gt;
It executes correctly without hitting the exception handling (I add debug information in the handling logic. If it is hit, error message will be print on the console and I verified that it is print out without the above logic).&lt;/p&gt;

&lt;p&gt;So, we can conclude that the problem is caused by `serialize` function with my local R installation, which serialize `NA` as string without packing its length before the actual value. Since `unserialize` can decode the seralized data, this protocol should be by R design when handling `NA` as `Date` type. I don&apos;t find the source code of `serialize` in R source code, which calls Internal(serialize(object, connection, type, version, refhook))&lt;/p&gt;

&lt;p&gt;For the fix, we can either leave it as it is by an exception handling or explicitly add a handling in readString when index is negative.&lt;/p&gt;

&lt;p&gt;What do you think? Thanks!  &lt;/p&gt;</comment>
                            <comment id="15827478" author="felixcheung" created="Wed, 18 Jan 2017 06:06:26 +0000"  >&lt;p&gt;very cool, thanks for all the investigation. What version of R you are running on? Is this on Mac or Linux?&lt;/p&gt;</comment>
                            <comment id="15828426" author="wm624" created="Wed, 18 Jan 2017 17:21:00 +0000"  >&lt;p&gt;OS and R information:&lt;br/&gt;
R version 3.3.0 (2016-05-03) &amp;#8211; &quot;Supposedly Educational&quot;&lt;br/&gt;
Copyright (C) 2016 The R Foundation for Statistical Computing&lt;br/&gt;
Platform: x86_64-apple-darwin13.4.0 (64-bit)&lt;/p&gt;

&lt;p&gt;We may check the .Internal(unserialize) logic to see how it handles NA for Date type and port it to readDate method in scala. &lt;/p&gt;</comment>
                            <comment id="17679385" author="apachespark" created="Sat, 21 Jan 2023 02:32:49 +0000"  >&lt;p&gt;User &apos;joveyuan-db&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/39681&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/39681&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="13517728">SPARK-42005</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            2 years, 42 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i353t3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>