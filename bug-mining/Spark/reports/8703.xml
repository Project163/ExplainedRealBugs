<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 19:31:13 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-40703] Performance regression for joins in Spark 3.3 vs Spark 3.2</title>
                <link>https://issues.apache.org/jira/browse/SPARK-40703</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;When running the TPC-DS benchmarks using a DSv2 datasource in Spark 3.3, a performance regression vs Spark 3.2 was discovered. More specifically, it appears as if &lt;em&gt;EnsureRequirements.ensureDistributionAndOrdering&lt;/em&gt;() no longer enforces a minimum number of partitions for a join distribution in some cases. This impacts DSv2 datasources, because if a scan has only a single read partition &lt;em&gt;DataSourceV2ScanExecBase.outputPartitioning&lt;/em&gt;() returns a &lt;em&gt;SinglePartition&lt;/em&gt; instance. The &lt;em&gt;SinglePartition&lt;/em&gt; creates a &lt;em&gt;SinglePartitionShuffleSpec&lt;/em&gt;, and &lt;em&gt;SinglePartitionShuffleSpec.canCreatePartitioning&lt;/em&gt;() returns true.&lt;/p&gt;

&lt;p&gt;Because &lt;em&gt;canCreatePartitioning&lt;/em&gt;() returns true in this case, &lt;em&gt;EnsureRequirements.ensureDistributionAndOrdering&lt;/em&gt;() won&apos;t enforce minimum parallelism and also will favor the single partition when considering the best distribution candidate. Ultimately this results in a single partition being selected for the join distribution, even if the other side of the join is a large table with many partitions. This can seriously impact performance of the join.&lt;/p&gt;

&lt;p&gt;Spark 3.2 enforces minimum parallelism differently in &lt;em&gt;ensureDistributionAndOrdering&lt;/em&gt;() and thus does not suffer from this issue. It will shuffle both sides of the join to enforce parallelism.&lt;/p&gt;

&lt;p&gt;In the TPC-DS benchmark, some queries affected include 14a and 14b. This can also be demonstrated using a simple query, for example:&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;select ics.i_item_sk from catalog_sales cs join item ics on cs.cs_item_sk = ics.i_item_sk&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;...where &lt;em&gt;item&lt;/em&gt; is a small table that is read into one partition, and &lt;em&gt;catalog_sales&lt;/em&gt; is a large table. These tables are part of the TPC-DS but you can create your own. Also, to demonstrate the issue, you may need to turn off broadcast joins though that is not required for this issue to occur, it happens when running the TPC-DS with broadcast setting at default.&lt;/p&gt;

&lt;p&gt;Attached is the plan for this query in Spark 3.2 and in Spark 3.3. The plan shows how in Spark 3.2, the join parallelism of 200 is reached by inserting an exchange after the item table scan. In Spark 3.3, no such exchange is inserted and the join parallelism is 1.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13485166">SPARK-40703</key>
            <summary>Performance regression for joins in Spark 3.3 vs Spark 3.2</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="csun">Chao Sun</assignee>
                                    <reporter username="bryanck">Bryan Keller</reporter>
                        <labels>
                    </labels>
                <created>Fri, 7 Oct 2022 14:38:22 +0000</created>
                <updated>Sat, 15 Oct 2022 02:22:45 +0000</updated>
                            <resolved>Sat, 15 Oct 2022 02:22:45 +0000</resolved>
                                    <version>3.3.0</version>
                                    <fixVersion>3.3.1</fixVersion>
                    <fixVersion>3.4.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>10</watches>
                                                                                                                <comments>
                            <comment id="17614199" author="sunchao" created="Fri, 7 Oct 2022 18:08:39 +0000"  >&lt;p&gt;cc &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yumwang&quot; class=&quot;user-hover&quot; rel=&quot;yumwang&quot;&gt;yumwang&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17614200" author="sunchao" created="Fri, 7 Oct 2022 18:12:14 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bryanck&quot; class=&quot;user-hover&quot; rel=&quot;bryanck&quot;&gt;bryanck&lt;/a&gt; In the above case, what is the partition spec for the other side of the join? Normally Spark should pick the partition spec with the most number of partitions and use that to shuffle the other side.&#160;&lt;/p&gt;</comment>
                            <comment id="17614205" author="bryanck" created="Fri, 7 Oct 2022 18:34:14 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sunchao&quot; class=&quot;user-hover&quot; rel=&quot;sunchao&quot;&gt;sunchao&lt;/a&gt;&#160;, the other side is HashPartitioning with num partitions of 200. There is a check shouldConsiderMinParallelism which ends up being false, because the SinglePartition returns true for canCreatePartitioing. The canCreatePartitioning also results in the SinglePartition being selected as the best candidate.&lt;/p&gt;</comment>
                            <comment id="17614207" author="bryanck" created="Fri, 7 Oct 2022 18:38:39 +0000"  >&lt;p&gt;I tried a fix/hack that returns&#160;an UnknownPartitioning(0) instead of SinglePartition, and that fixes this issue, but not sure if that has adverse effects elsewhere.&lt;/p&gt;</comment>
                            <comment id="17614210" author="bryanck" created="Fri, 7 Oct 2022 18:44:50 +0000"  >&lt;p&gt;DSv1 DataSourceScanExec returns UnknownPartitioning(0) for non-bucketed scans, and I believe this is why DSv1 isn&apos;t impacted&lt;/p&gt;</comment>
                            <comment id="17614289" author="csun" created="Fri, 7 Oct 2022 22:00:38 +0000"  >&lt;p&gt;I see. The reason HashPartitioning is not picked as the best candidate in this case could be that &lt;tt&gt;spark.sql.requireAllClusterKeysForCoPartition&lt;/tt&gt; is turned on by default, so its &lt;tt&gt;canCreatePartitioing&lt;/tt&gt; returns false. One idea is to try setting &lt;tt&gt;spark.sql.requireAllClusterKeysForCoPartition&lt;/tt&gt; to false.&lt;/p&gt;</comment>
                            <comment id="17614291" author="bryanck" created="Fri, 7 Oct 2022 22:06:48 +0000"  >&lt;p&gt;I gave that a try, but setting &lt;tt&gt;spark.sql.requireAllClusterKeysForCoPartition=false&lt;/tt&gt; doesn&apos;t seem to help, I still am seeing a single partition selected for the join&lt;/p&gt;</comment>
                            <comment id="17614296" author="csun" created="Fri, 7 Oct 2022 22:28:06 +0000"  >&lt;p&gt;Hmm interesting. Let me try to come up with a unit test and check what happened underneath.&#160;&lt;/p&gt;</comment>
                            <comment id="17614298" author="csun" created="Fri, 7 Oct 2022 22:29:24 +0000"  >&lt;p&gt;(one idea is that &lt;tt&gt;SinglePartitionSpec#canCreatePartitioing&lt;/tt&gt; should also consider &lt;tt&gt;spark.sql.requireAllClusterKeysForCoPartition&lt;/tt&gt; instead of always returning true)&lt;/p&gt;</comment>
                            <comment id="17614302" author="bryanck" created="Fri, 7 Oct 2022 22:53:39 +0000"  >&lt;p&gt;Wouldn&apos;t that have the same problem if &lt;tt&gt;spark.sql.requireAllClusterKeysForCoPartition&lt;/tt&gt; is set?&lt;/p&gt;</comment>
                            <comment id="17614303" author="csun" created="Fri, 7 Oct 2022 23:08:51 +0000"  >&lt;p&gt;Hmm somehow in the unit test I was able to see that changing &lt;tt&gt;spark.sql.requireAllClusterKeysForCoPartition&lt;/tt&gt; to false fixed the issue:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  test(&lt;span class=&quot;code-quote&quot;&gt;&quot;EnsureRequirements should respect spark.sql.shuffle.partitions with SinglePartition&quot;&lt;/span&gt;) {
    withSQLConf(SQLConf.SHUFFLE_PARTITIONS.key -&amp;gt; 10.toString) {
      val plan1: SparkPlan = DummySparkPlan(
        outputPartitioning = HashPartitioning(exprA :: Nil, 15))
      val plan2 = DummySparkPlan(outputPartitioning = SinglePartition)
      val smjExec = SortMergeJoinExec(
        exprA :: exprB :: Nil, exprC :: exprD :: Nil, Inner, None, plan1, plan2)
      applyEnsureRequirementsWithSubsetKeys(smjExec) match {
        &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; SortMergeJoinExec(_, _, _, _,
        SortExec(_, _, DummySparkPlan(_, _, left: HashPartitioning, _, _), _),
        SortExec(_, _, ShuffleExchangeExec(right: HashPartitioning, _, _), _), _) =&amp;gt;
          &lt;span class=&quot;code-keyword&quot;&gt;assert&lt;/span&gt;(left.expressions === Seq(exprA))
          &lt;span class=&quot;code-keyword&quot;&gt;assert&lt;/span&gt;(right.expressions === Seq(exprC))
          &lt;span class=&quot;code-keyword&quot;&gt;assert&lt;/span&gt;(left.numPartitions == 15)
          &lt;span class=&quot;code-keyword&quot;&gt;assert&lt;/span&gt;(right.numPartitions == 15)
        &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; other =&amp;gt; fail(other.toString)
      }
    }
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;(This was added in &lt;tt&gt;EnsureRequirementsSuite&lt;/tt&gt;.&lt;/p&gt;</comment>
                            <comment id="17614738" author="bryanck" created="Sun, 9 Oct 2022 16:12:05 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=csun&quot; class=&quot;user-hover&quot; rel=&quot;csun&quot;&gt;csun&lt;/a&gt;&#160;I added a pyspark script that shows the difference between DSv1 and DSv2, in case that helps.&lt;/p&gt;</comment>
                            <comment id="17615255" author="csun" created="Mon, 10 Oct 2022 18:25:08 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bryanck&quot; class=&quot;user-hover&quot; rel=&quot;bryanck&quot;&gt;bryanck&lt;/a&gt;&#160;. Now I see where the issue is.&lt;/p&gt;

&lt;p&gt;In your pyspark example, one side reports &lt;tt&gt;UnknownPartitioning&lt;/tt&gt; while another side reports &lt;tt&gt;SinglePartition&lt;/tt&gt;. Later on, Spark will insert shuffle for &lt;tt&gt;UnknownPartitioning&lt;/tt&gt; so it becomes &lt;tt&gt;HashPartitioning&lt;/tt&gt;. In this particular case, when Spark is deciding which side to insert shuffle, it&apos;ll pick the &lt;tt&gt;HashPartitioning&lt;/tt&gt; again and convert it into the same &lt;tt&gt;HashPartitioning&lt;/tt&gt; but&#160;with &lt;tt&gt;numPartitions = 1&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;Before:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
 ShuffleExchange(HashPartition(200))  &amp;lt;--&amp;gt;  SinglePartition
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;(suppose &lt;tt&gt;spark.sql.shuffle.partitions&lt;/tt&gt; is 200)&lt;/p&gt;

&lt;p&gt;After:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
 ShuffleExchange(HashPartition(1))  &amp;lt;--&amp;gt;  SinglePartition
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;br/&gt;
The reason Spark chooses to do in this way is because there is a&#160;trade-off between shuffle cost and parallelism. At the moment, when Spark sees that one side of the join has &lt;tt&gt;ShuffleExchange&lt;/tt&gt; (meaning it needs to be shuffled anyways), and the other side doesn&apos;t, it&apos;ll try to avoid shuffling the other side. &lt;/p&gt;

&lt;p&gt;This makes more sense if we have:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
ShuffleExchange(HashPartition(200)) &amp;lt;-&amp;gt; HashPartition(150)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;as in this case, Spark will avoid shuffle the right hand side and instead just change the number of shuffle partitions on the left:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
ShuffleExchange(HashPartition(150) &amp;lt;-&amp;gt; HashPartition(150)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I feel we can treat the &lt;tt&gt;SinglePartition&lt;/tt&gt; as a special case here. Let me see if I can come up with a PR.&lt;/p&gt;</comment>
                            <comment id="17615264" author="bryanck" created="Mon, 10 Oct 2022 18:32:39 +0000"  >&lt;p&gt;Sounds good, thanks.&lt;/p&gt;</comment>
                            <comment id="17615379" author="apachespark" created="Mon, 10 Oct 2022 22:28:59 +0000"  >&lt;p&gt;User &apos;sunchao&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/38196&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/38196&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17618004" author="q79969786" created="Sat, 15 Oct 2022 02:22:45 +0000"  >&lt;p&gt;Issue resolved by pull request 38196&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/38196&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/38196&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310560">
                    <name>Problem/Incident</name>
                                                                <inwardlinks description="is caused by">
                                        <issuelink>
            <issuekey id="13383029">SPARK-35703</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="13050142" name="spark32-plan.txt" size="1299" author="bryanck" created="Fri, 7 Oct 2022 14:51:12 +0000"/>
                            <attachment id="13050143" name="spark33-plan.txt" size="1175" author="bryanck" created="Fri, 7 Oct 2022 14:51:16 +0000"/>
                            <attachment id="13050196" name="test.py" size="844" author="bryanck" created="Sun, 9 Oct 2022 16:16:31 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            3 years, 4 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z196co:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12351710">3.3.1</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>