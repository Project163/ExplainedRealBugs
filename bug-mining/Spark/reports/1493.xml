<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:24:23 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-4300] Race condition during SparkWorker shutdown</title>
                <link>https://issues.apache.org/jira/browse/SPARK-4300</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;When a shark job is done. there are some error message as following show in the log&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;INFO 22:10:41,635 SparkMaster: akka.tcp:&lt;span class=&quot;code-comment&quot;&gt;//sparkDriver@ip-172-31-11-204.us-west-1.compute.internal:57641 got disassociated, removing it.
&lt;/span&gt; INFO 22:10:41,640 SparkMaster: Removing app app-20141106221014-0000
 INFO 22:10:41,687 SparkMaster: Removing application Shark::ip-172-31-11-204.us-west-1.compute.internal
 INFO 22:10:41,710 SparkWorker: Asked to kill executor app-20141106221014-0000/0
 INFO 22:10:41,712 SparkWorker: Runner thread &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; executor app-20141106221014-0000/0 interrupted
 INFO 22:10:41,714 SparkWorker: Killing process!
ERROR 22:10:41,738 SparkWorker: Error writing stream to file /&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/spark/work/app-20141106221014-0000/0/stdout
ERROR 22:10:41,739 SparkWorker: java.io.IOException: Stream closed
ERROR 22:10:41,739 SparkWorker: 	at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:162)
ERROR 22:10:41,740 SparkWorker: 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:272)
ERROR 22:10:41,740 SparkWorker: 	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
ERROR 22:10:41,740 SparkWorker: 	at java.io.FilterInputStream.read(FilterInputStream.java:107)
ERROR 22:10:41,741 SparkWorker: 	at org.apache.spark.util.logging.FileAppender.appendStreamToFile(FileAppender.scala:70)
ERROR 22:10:41,741 SparkWorker: 	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply$mcV$sp(FileAppender.scala:39)
ERROR 22:10:41,741 SparkWorker: 	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)
ERROR 22:10:41,742 SparkWorker: 	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)
ERROR 22:10:41,742 SparkWorker: 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1311)
ERROR 22:10:41,742 SparkWorker: 	at org.apache.spark.util.logging.FileAppender$$anon$1.run(FileAppender.scala:38)
 INFO 22:10:41,838 SparkMaster: Connected to Cassandra cluster: 4299
 INFO 22:10:41,839 SparkMaster: Adding host 172.31.11.204 (Analytics)
 INFO 22:10:41,840 SparkMaster: New Cassandra host /172.31.11.204:9042 added
 INFO 22:10:41,841 SparkMaster: Adding host 172.31.11.204 (Analytics)
 INFO 22:10:41,842 SparkMaster: Adding host 172.31.11.204 (Analytics)
 INFO 22:10:41,852 SparkMaster: akka.tcp:&lt;span class=&quot;code-comment&quot;&gt;//sparkDriver@ip-172-31-11-204.us-west-1.compute.internal:57641 got disassociated, removing it.
&lt;/span&gt; INFO 22:10:41,853 SparkMaster: akka.tcp:&lt;span class=&quot;code-comment&quot;&gt;//sparkDriver@ip-172-31-11-204.us-west-1.compute.internal:57641 got disassociated, removing it.
&lt;/span&gt; INFO 22:10:41,853 SparkMaster: akka.tcp:&lt;span class=&quot;code-comment&quot;&gt;//sparkDriver@ip-172-31-11-204.us-west-1.compute.internal:57641 got disassociated, removing it.
&lt;/span&gt; INFO 22:10:41,857 SparkMaster: akka.tcp:&lt;span class=&quot;code-comment&quot;&gt;//sparkDriver@ip-172-31-11-204.us-west-1.compute.internal:57641 got disassociated, removing it.
&lt;/span&gt; INFO 22:10:41,862 SparkMaster: Adding host 172.31.11.204 (Analytics)
 WARN 22:10:42,200 SparkMaster: Got status update &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; unknown executor app-20141106221014-0000/0
 INFO 22:10:42,211 SparkWorker: Executor app-20141106221014-0000/0 finished with state KILLED exitStatus 143
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;/var/lib/spark/work/app-20141106221014-0000/0/stdout is on the disk. It is trying to write to a close IO stream. &lt;/p&gt;

&lt;p&gt;Spark worker shuts down by &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt; &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; def killProcess(message: Option[&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;]) {
    &lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt; exitCode: Option[Int] = None
    logInfo(&lt;span class=&quot;code-quote&quot;&gt;&quot;Killing process!&quot;&lt;/span&gt;)
    process.destroy()
    process.waitFor()
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (stdoutAppender != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
      stdoutAppender.stop()
    }
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (stderrAppender != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
      stderrAppender.stop()
    }
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (process != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
    exitCode = Some(process.waitFor())
    }
    worker ! ExecutorStateChanged(appId, execId, state, message, exitCode)
 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;But stdoutAppender concurrently writes to output log file, which creates race condition. &lt;/p&gt;</description>
                <environment></environment>
        <key id="12753741">SPARK-4300</key>
            <summary>Race condition during SparkWorker shutdown</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Minor</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="srowen">Sean R. Owen</assignee>
                                    <reporter username="alexliu68">Alex Liu</reporter>
                        <labels>
                    </labels>
                <created>Fri, 7 Nov 2014 17:18:27 +0000</created>
                <updated>Wed, 1 Aug 2018 10:35:19 +0000</updated>
                            <resolved>Fri, 13 Mar 2015 17:55:10 +0000</resolved>
                                    <version>1.1.0</version>
                                    <fixVersion>1.2.2</fixVersion>
                    <fixVersion>1.3.1</fixVersion>
                    <fixVersion>1.4.0</fixVersion>
                                    <component>Spark Shell</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="14338223" author="srowen" created="Thu, 26 Feb 2015 10:49:51 +0000"  >&lt;p&gt;Is this specific to shark? I don&apos;t imagine so. It seems like the appender has to stop before the process is destroyed, since the appender can&apos;t keep reading its stdout/stderr after it is destroyed. I&apos;ll try a PR.&lt;/p&gt;</comment>
                            <comment id="14338229" author="apachespark" created="Thu, 26 Feb 2015 10:56:44 +0000"  >&lt;p&gt;User &apos;srowen&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/4787&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/4787&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14645331" author="alexliu68" created="Wed, 29 Jul 2015 01:23:54 +0000"  >&lt;p&gt;We find this issue still exists in 1.3.1&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;ERROR [&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;-6] 2015-07-28 22:49:57,653 SparkWorker-0 ExternalLogger.java:96 - Error writing stream to file /&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/spark/worker/worker-0/app-20150728224954-0003/0/stderr
ERROR [&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;-6] 2015-07-28 22:49:57,653 SparkWorker-0 ExternalLogger.java:96 - java.io.IOException: Stream closed
ERROR [&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;-6] 2015-07-28 22:49:57,654 SparkWorker-0 ExternalLogger.java:96 - 	at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:170) ~[na:1.8.0_40]
ERROR [&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;-6] 2015-07-28 22:49:57,654 SparkWorker-0 ExternalLogger.java:96 - 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:283) ~[na:1.8.0_40]
ERROR [&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;-6] 2015-07-28 22:49:57,654 SparkWorker-0 ExternalLogger.java:96 - 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[na:1.8.0_40]
ERROR [&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;-6] 2015-07-28 22:49:57,654 SparkWorker-0 ExternalLogger.java:96 - 	at java.io.FilterInputStream.read(FilterInputStream.java:107) ~[na:1.8.0_40]
ERROR [&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;-6] 2015-07-28 22:49:57,655 SparkWorker-0 ExternalLogger.java:96 - 	at org.apache.spark.util.logging.FileAppender.appendStreamToFile(FileAppender.scala:70) ~[spark-core_2.10-1.3.1.1.jar:1.3.1.1]
ERROR [&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;-6] 2015-07-28 22:49:57,655 SparkWorker-0 ExternalLogger.java:96 - 	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply$mcV$sp(FileAppender.scala:39) [spark-core_2.10-1.3.1.1.jar:1.3.1.1]
ERROR [&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;-6] 2015-07-28 22:49:57,655 SparkWorker-0 ExternalLogger.java:96 - 	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39) [spark-core_2.10-1.3.1.1.jar:1.3.1.1]
ERROR [&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;-6] 2015-07-28 22:49:57,655 SparkWorker-0 ExternalLogger.java:96 - 	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39) [spark-core_2.10-1.3.1.1.jar:1.3.1.1]
ERROR [&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;-6] 2015-07-28 22:49:57,655 SparkWorker-0 ExternalLogger.java:96 - 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618) [spark-core_2.10-1.3.1.1.jar:1.3.1.1]
ERROR [&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;-6] 2015-07-28 22:49:57,656 SparkWorker-0 ExternalLogger.java:96 - 	at org.apache.spark.util.logging.FileAppender$$anon$1.run(FileAppender.scala:38) [spark-core_2.10-1.3.1.1.jar:1.3.1.1]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;at  &lt;a href=&quot;https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala#L159&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala#L159&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The process auto shuts down, but the log appenders are still running, which causes the error log messages.&lt;/p&gt;</comment>
                            <comment id="14680154" author="lior.c@taboola.com" created="Mon, 10 Aug 2015 14:06:23 +0000"  >&lt;p&gt;Also exists in spark 1.4:&lt;/p&gt;

&lt;div class=&quot;panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;panelContent&quot;&gt;
&lt;p&gt;12:31:10.821 &lt;span class=&quot;error&quot;&gt;&amp;#91;File appending thread for /var/lib/spark/data/disk1/app-20150809122638-0000/13/stdout&amp;#93;&lt;/span&gt; ERROR org.apache.spark.util.logging.FileAppender - Error writing stream to file /var/lib/spark/data/disk1/app-2015080&lt;br/&gt;
9122638-0000/13/stdout&lt;br/&gt;
java.io.IOException: Stream closed&lt;br/&gt;
        at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:145) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;?:1.6.0_41&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:255) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;?:1.6.0_41&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.io.BufferedInputStream.read(BufferedInputStream.java:317) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;?:1.6.0_41&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.io.FilterInputStream.read(FilterInputStream.java:90) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;?:1.6.0_41&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.spark.util.logging.FileAppender.appendStreamToFile(FileAppender.scala:70) &lt;span class=&quot;error&quot;&gt;&amp;#91;spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply$mcV$sp(FileAppender.scala:39) &lt;span class=&quot;error&quot;&gt;&amp;#91;spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39) &lt;span class=&quot;error&quot;&gt;&amp;#91;spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39) &lt;span class=&quot;error&quot;&gt;&amp;#91;spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772) &lt;span class=&quot;error&quot;&gt;&amp;#91;spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.spark.util.logging.FileAppender$$anon$1.run(FileAppender.scala:38) &lt;span class=&quot;error&quot;&gt;&amp;#91;spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0&amp;#93;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And later on I see:&lt;/p&gt;
&lt;div class=&quot;panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;panelContent&quot;&gt;
&lt;p&gt;12:22:30.861 &lt;span class=&quot;error&quot;&gt;&amp;#91;sparkWorker-akka.actor.default-dispatcher-2&amp;#93;&lt;/span&gt; ERROR akka.actor.ActorSystemImpl - Uncaught fatal error from thread &lt;span class=&quot;error&quot;&gt;&amp;#91;sparkWorker-akka.remote.default-remote-dispatcher-5&amp;#93;&lt;/span&gt; shutting down ActorSystem &lt;span class=&quot;error&quot;&gt;&amp;#91;sparkWorker&amp;#93;&lt;/span&gt;&lt;br/&gt;
java.lang.OutOfMemoryError: GC overhead limit exceeded&lt;br/&gt;
        at org.spark_project.protobuf.ByteString.copyFrom(ByteString.java:192) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.spark_project.protobuf.ByteString.copyFrom(ByteString.java:204) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at akka.remote.serialization.MessageContainerSerializer.serializeSelection(MessageContainerSerializer.scala:36) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at akka.remote.serialization.MessageContainerSerializer.toBinary(MessageContainerSerializer.scala:25) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at akka.remote.MessageSerializer$.serialize(MessageSerializer.scala:36) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at akka.remote.EndpointWriter$$anonfun$serializeMessage$1.apply(Endpoint.scala:845) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at akka.remote.EndpointWriter$$anonfun$serializeMessage$1.apply(Endpoint.scala:845) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at akka.remote.EndpointWriter.serializeMessage(Endpoint.scala:844) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at akka.remote.EndpointWriter.writeSend(Endpoint.scala:747) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at akka.remote.EndpointWriter$$anonfun$2.applyOrElse(Endpoint.scala:722) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at akka.actor.Actor$class.aroundReceive(Actor.scala:465) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at akka.remote.EndpointActor.aroundReceive(Endpoint.scala:415) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) &lt;span class=&quot;error&quot;&gt;&amp;#91;spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at akka.actor.ActorCell.invoke(ActorCell.scala:487) &lt;span class=&quot;error&quot;&gt;&amp;#91;spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238) &lt;span class=&quot;error&quot;&gt;&amp;#91;spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at akka.dispatch.Mailbox.run(Mailbox.scala:220) &lt;span class=&quot;error&quot;&gt;&amp;#91;spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393) &lt;span class=&quot;error&quot;&gt;&amp;#91;spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) &lt;span class=&quot;error&quot;&gt;&amp;#91;spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) &lt;span class=&quot;error&quot;&gt;&amp;#91;spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) &lt;span class=&quot;error&quot;&gt;&amp;#91;spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) &lt;span class=&quot;error&quot;&gt;&amp;#91;spark-assembly-1.4.0-hadoop2.2.0.jar:1.4.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
Exception in thread &quot;qtp1853216600-31&quot; java.lang.OutOfMemoryError: GC overhead limit exceeded&lt;br/&gt;
        at org.spark-project.jetty.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:708)&lt;br/&gt;
        at org.spark-project.jetty.io.nio.SelectorManager$1.run(SelectorManager.java:290)&lt;br/&gt;
        at org.spark-project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)&lt;br/&gt;
        at org.spark-project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:662)Exception in thread &quot;qtp1853216600-37&quot; java.lang.OutOfMemoryError: GC overhead limit exceeded&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In application log I see:&lt;/p&gt;
&lt;div class=&quot;panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;panelContent&quot;&gt;
&lt;p&gt;2015-08-10 12:41:33,761 WARN  &lt;span class=&quot;error&quot;&gt;&amp;#91;task-result-getter-0&amp;#93;&lt;/span&gt; TaskSetManager - Lost task 165.2 in stage 207.7 (TID 141815, 10.10.0.83): java.io.IOException: Failed to connect to /10.10.0.67:42846&lt;br/&gt;
        at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:193)&lt;br/&gt;
        at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:156)&lt;br/&gt;
        at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:88)&lt;br/&gt;
        at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)&lt;br/&gt;
        at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)&lt;br/&gt;
        at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)&lt;br/&gt;
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)&lt;br/&gt;
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)&lt;br/&gt;
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:662)&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="14682472" author="alexliu68" created="Tue, 11 Aug 2015 20:47:20 +0000"  >&lt;p&gt;I created &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-9844&quot; title=&quot;File appender race condition during SparkWorker shutdown&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-9844&quot;&gt;&lt;del&gt;SPARK-9844&lt;/del&gt;&lt;/a&gt; for the unsolved issue.&lt;/p&gt;</comment>
                            <comment id="16565104" author="liqingan" created="Wed, 1 Aug 2018 10:26:42 +0000"  >&lt;p&gt;i feel upset for this issue !&#160;&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/thumbs_down.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;
&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;Uncaught fatal error from thread &lt;span class=&quot;error&quot;&gt;&amp;#91;sparkWorker-akka.actor.default-dispatcher-56&amp;#93;&lt;/span&gt; shutting down ActorSystem &lt;span class=&quot;error&quot;&gt;&amp;#91;sparkWorker&amp;#93;&lt;/span&gt;&lt;br/&gt;
java.lang.OutOfMemoryError: GC overhead limit exceeded&lt;br/&gt;
	at java.util.Arrays.copyOfRange(Arrays.java:2694)&lt;br/&gt;
	at java.lang.String.&amp;lt;init&amp;gt;(String.java:203)&lt;br/&gt;
	at java.lang.StringBuilder.toString(StringBuilder.java:405)&lt;br/&gt;
	at java.io.ObjectInputStream$BlockDataInputStream.readUTFBody(ObjectInputStream.java:3068)&lt;br/&gt;
	at java.io.ObjectInputStream$BlockDataInputStream.readUTF(ObjectInputStream.java:2864)&lt;br/&gt;
	at java.io.ObjectInputStream.readString(ObjectInputStream.java:1638)&lt;br/&gt;
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1341)&lt;br/&gt;
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)&lt;br/&gt;
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)&lt;br/&gt;
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)&lt;br/&gt;
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)&lt;br/&gt;
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)&lt;br/&gt;
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)&lt;br/&gt;
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)&lt;br/&gt;
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)&lt;br/&gt;
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)&lt;br/&gt;
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)&lt;br/&gt;
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)&lt;br/&gt;
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)&lt;br/&gt;
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)&lt;br/&gt;
	at akka.serialization.JavaSerializer$$anonfun$1.apply(Serializer.scala:136)&lt;br/&gt;
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)&lt;br/&gt;
	at akka.serialization.JavaSerializer.fromBinary(Serializer.scala:136)&lt;br/&gt;
	at akka.serialization.Serialization$$anonfun$deserialize$1.apply(Serialization.scala:104)&lt;br/&gt;
	at scala.util.Try$.apply(Try.scala:161)&lt;br/&gt;
	at akka.serialization.Serialization.deserialize(Serialization.scala:98)&lt;br/&gt;
	at akka.remote.MessageSerializer$.deserialize(MessageSerializer.scala:23)&lt;br/&gt;
	at akka.remote.DefaultMessageDispatcher.payload$lzycompute$1(Endpoint.scala:55)&lt;br/&gt;
	at akka.remote.DefaultMessageDispatcher.payload$1(Endpoint.scala:55)&lt;br/&gt;
	at akka.remote.DefaultMessageDispatcher.dispatch(Endpoint.scala:73)&lt;br/&gt;
	at akka.remote.EndpointReader$$anonfun$receive$2.applyOrElse(Endpoint.scala:764)&lt;br/&gt;
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&#26089;&#19978;2&#28857;12:12.751&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;ERROR&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;org.apache.spark.util.logging.FileAppender&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;Error writing stream to file /hadoop/var/run/spark/work/app-20180727141925-0019/38075/stderr&lt;br/&gt;
java.io.IOException: Stream closed&lt;br/&gt;
	at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:162)&lt;br/&gt;
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:272)&lt;br/&gt;
	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)&lt;br/&gt;
	at java.io.FilterInputStream.read(FilterInputStream.java:107)&lt;br/&gt;
	at org.apache.spark.util.logging.FileAppender.appendStreamToFile(FileAppender.scala:70)&lt;br/&gt;
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply$mcV$sp(FileAppender.scala:39)&lt;br/&gt;
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)&lt;br/&gt;
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)&lt;br/&gt;
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1468)&lt;br/&gt;
	at org.apache.spark.util.logging.FileAppender$$anon$1.run(FileAppender.scala:38)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&#26089;&#19978;2&#28857;12:12.752&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;ERROR&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;org.apache.spark.util.logging.FileAppender&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;Error writing stream to file /hadoop/var/run/spark/work/app-20180727142159-0032/30823/stderr&lt;br/&gt;
java.io.IOException: Stream closed&lt;br/&gt;
	at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:162)&lt;br/&gt;
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:272)&lt;br/&gt;
	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)&lt;br/&gt;
	at java.io.FilterInputStream.read(FilterInputStream.java:107)&lt;br/&gt;
	at org.apache.spark.util.logging.FileAppender.appendStreamToFile(FileAppender.scala:70)&lt;br/&gt;
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply$mcV$sp(FileAppender.scala:39)&lt;br/&gt;
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)&lt;br/&gt;
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)&lt;br/&gt;
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1468)&lt;br/&gt;
	at org.apache.spark.util.logging.FileAppender$$anon$1.run(FileAppender.scala:38)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12854269">SPARK-9844</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12854269">SPARK-9844</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12933898" name="B{~TP2PW~}1TYA2AG{CA41H.png" size="127772" author="liqingan" created="Wed, 1 Aug 2018 10:34:42 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 15 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2244f:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>