<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:57:19 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-22618] RDD.unpersist can cause fatal exception when used with dynamic allocation</title>
                <link>https://issues.apache.org/jira/browse/SPARK-22618</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;If you use rdd.unpersist() with dynamic allocation, then an executor can be deallocated while your rdd is being removed, which will throw an uncaught exception killing your job. &lt;/p&gt;

&lt;p&gt;I looked into different ways of preventing this error from occurring but couldn&apos;t come up with anything that wouldn&apos;t require a big change. I propose the best fix is just to catch and log IOExceptions in unpersist() so they don&apos;t kill your job. This will match the effective behavior when executors are lost from dynamic allocation in other parts of the code.&lt;/p&gt;

&lt;p&gt;In the worst case scenario I think this could lead to RDD partitions getting left on executors after they were unpersisted, but this is probably better than the whole job failing. I think in most cases the IOException would be due to the executor dieing for some reason, which is effectively the same result as unpersisting the rdd from that executor anyway.&lt;/p&gt;

&lt;p&gt;I noticed this exception in a job that loads a 100GB dataset on a cluster where we use dynamic allocation heavily. Here is the relevant stack trace&lt;/p&gt;

&lt;p&gt;java.io.IOException: Connection reset by peer&lt;br/&gt;
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method)&lt;br/&gt;
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)&lt;br/&gt;
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)&lt;br/&gt;
        at sun.nio.ch.IOUtil.read(IOUtil.java:192)&lt;br/&gt;
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)&lt;br/&gt;
        at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:221)&lt;br/&gt;
        at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:899)&lt;br/&gt;
        at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:276)&lt;br/&gt;
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:119)&lt;br/&gt;
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)&lt;br/&gt;
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)&lt;br/&gt;
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)&lt;br/&gt;
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)&lt;br/&gt;
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)&lt;br/&gt;
        at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:748)&lt;br/&gt;
Exception in thread &quot;main&quot; org.apache.spark.SparkException: Exception thrown in awaitResult:&lt;br/&gt;
        at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:205)&lt;br/&gt;
        at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)&lt;br/&gt;
        at org.apache.spark.storage.BlockManagerMaster.removeRdd(BlockManagerMaster.scala:131)&lt;br/&gt;
        at org.apache.spark.SparkContext.unpersistRDD(SparkContext.scala:1806)&lt;br/&gt;
        at org.apache.spark.rdd.RDD.unpersist(RDD.scala:217)&lt;br/&gt;
        at com.ibm.sparktc.sparkbench.workload.exercise.CacheTest.doWorkload(CacheTest.scala:62)&lt;br/&gt;
        at com.ibm.sparktc.sparkbench.workload.Workload$class.run(Workload.scala:40)&lt;br/&gt;
        at com.ibm.sparktc.sparkbench.workload.exercise.CacheTest.run(CacheTest.scala:33)&lt;br/&gt;
        at com.ibm.sparktc.sparkbench.workload.SuiteKickoff$$anonfun$com$ibm$sparktc$sparkbench$workload$SuiteKickoff$$runSerially$1.apply(SuiteKickoff.scala:78)&lt;br/&gt;
        at com.ibm.sparktc.sparkbench.workload.SuiteKickoff$$anonfun$com$ibm$sparktc$sparkbench$workload$SuiteKickoff$$runSerially$1.apply(SuiteKickoff.scala:78)&lt;br/&gt;
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&lt;br/&gt;
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&lt;br/&gt;
        at scala.collection.immutable.List.foreach(List.scala:381)&lt;br/&gt;
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&lt;br/&gt;
        at scala.collection.immutable.List.map(List.scala:285)&lt;br/&gt;
        at com.ibm.sparktc.sparkbench.workload.SuiteKickoff$.com$ibm$sparktc$sparkbench$workload$SuiteKickoff$$runSerially(SuiteKickoff.scala:78)&lt;br/&gt;
        at com.ibm.sparktc.sparkbench.workload.SuiteKickoff$$anonfun$2.apply(SuiteKickoff.scala:52)&lt;br/&gt;
        at com.ibm.sparktc.sparkbench.workload.SuiteKickoff$$anonfun$2.apply(SuiteKickoff.scala:47)&lt;br/&gt;
        at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)&lt;br/&gt;
        at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)&lt;br/&gt;
        at scala.collection.immutable.Range.foreach(Range.scala:160)&lt;br/&gt;
        at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)&lt;br/&gt;
        at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)&lt;br/&gt;
        at com.ibm.sparktc.sparkbench.workload.SuiteKickoff$.run(SuiteKickoff.scala:47)&lt;br/&gt;
        at com.ibm.sparktc.sparkbench.workload.MultipleSuiteKickoff$$anonfun$com$ibm$sparktc$sparkbench$workload$MultipleSuiteKickoff$$runSuitesSerially$1.apply(MultipleSuiteKickoff.scala:24)&lt;br/&gt;
        at com.ibm.sparktc.sparkbench.workload.MultipleSuiteKickoff$$anonfun$com$ibm$sparktc$sparkbench$workload$MultipleSuiteKickoff$$runSuitesSerially$1.apply(MultipleSuiteKickoff.scala:24)&lt;br/&gt;
        at scala.collection.immutable.List.foreach(List.scala:381)&lt;br/&gt;
        at com.ibm.sparktc.sparkbench.workload.MultipleSuiteKickoff$.com$ibm$sparktc$sparkbench$workload$MultipleSuiteKickoff$$runSuitesSerially(MultipleSuiteKickoff.scala:24)&lt;br/&gt;
        at com.ibm.sparktc.sparkbench.workload.MultipleSuiteKickoff$$anonfun$run$1.apply(MultipleSuiteKickoff.scala:13)&lt;br/&gt;
        at com.ibm.sparktc.sparkbench.workload.MultipleSuiteKickoff$$anonfun$run$1.apply(MultipleSuiteKickoff.scala:10)&lt;br/&gt;
        at scala.collection.immutable.List.foreach(List.scala:381)&lt;br/&gt;
        at com.ibm.sparktc.sparkbench.workload.MultipleSuiteKickoff$.run(MultipleSuiteKickoff.scala:10)&lt;br/&gt;
        at com.ibm.sparktc.sparkbench.cli.CLIKickoff$.main(CLIKickoff.scala:16)&lt;br/&gt;
        at com.ibm.sparktc.sparkbench.cli.CLIKickoff.main(CLIKickoff.scala)&lt;br/&gt;
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&lt;br/&gt;
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
        at java.lang.reflect.Method.invoke(Method.java:498)&lt;br/&gt;
        at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)&lt;br/&gt;
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:843)&lt;br/&gt;
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:188)&lt;br/&gt;
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:218)&lt;br/&gt;
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:127)&lt;br/&gt;
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)&lt;br/&gt;
Caused by: java.io.IOException: Connection reset by peer&lt;br/&gt;
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method)&lt;br/&gt;
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)&lt;br/&gt;
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)&lt;br/&gt;
        at sun.nio.ch.IOUtil.read(IOUtil.java:192)&lt;br/&gt;
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)&lt;br/&gt;
        at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:221)&lt;br/&gt;
        at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:899)&lt;br/&gt;
        at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:276)&lt;br/&gt;
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:119)&lt;br/&gt;
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)&lt;br/&gt;
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)&lt;br/&gt;
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)&lt;br/&gt;
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)&lt;br/&gt;
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)&lt;br/&gt;
        at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:748)&lt;/p&gt;</description>
                <environment></environment>
        <key id="13121055">SPARK-22618</key>
            <summary>RDD.unpersist can cause fatal exception when used with dynamic allocation</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Minor</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="bradkaiser">Brad</assignee>
                                    <reporter username="bradkaiser">Brad</reporter>
                        <labels>
                    </labels>
                <created>Mon, 27 Nov 2017 18:39:26 +0000</created>
                <updated>Thu, 29 Mar 2018 14:11:17 +0000</updated>
                            <resolved>Thu, 7 Dec 2017 13:05:09 +0000</resolved>
                                    <version>2.3.0</version>
                                    <fixVersion>2.3.0</fixVersion>
                                    <component>Spark Core</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="16267223" author="bradkaiser" created="Mon, 27 Nov 2017 18:40:11 +0000"  >&lt;p&gt;I have a PR for this forthcoming.&lt;/p&gt;</comment>
                            <comment id="16269374" author="apachespark" created="Tue, 28 Nov 2017 19:58:04 +0000"  >&lt;p&gt;User &apos;brad-kaiser&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/19836&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/19836&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16281816" author="cloud_fan" created="Thu, 7 Dec 2017 13:05:09 +0000"  >&lt;p&gt;Issue resolved by pull request 19836&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/19836&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/19836&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16416309" author="tgraves" created="Tue, 27 Mar 2018 21:58:25 +0000"  >&lt;p&gt;thanks for fixing this, hitting it now in spark 2.2, I think this same issue can happen with broadcast variables if its told to wait, did you happen to look at that at the same time?&#160;&#160;&lt;/p&gt;</comment>
                            <comment id="16416821" author="cloud_fan" created="Wed, 28 Mar 2018 04:58:12 +0000"  >&lt;p&gt;Looks like we can apply the same fix to `Broadcast.unpersist`. Do you want to send a PR to fix? thanks!&lt;/p&gt;</comment>
                            <comment id="16417466" author="tgraves" created="Wed, 28 Mar 2018 14:54:00 +0000"  >&lt;p&gt;I&apos;ll file a separate Jira for it and put up a pr&lt;/p&gt;</comment>
                            <comment id="16419062" author="bradkaiser" created="Thu, 29 Mar 2018 14:11:17 +0000"  >&lt;p&gt;Yeah the fix for broadcaset unpersist should be basically the same. Thanks Thomas.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13148614">SPARK-23806</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 33 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3n8nr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>