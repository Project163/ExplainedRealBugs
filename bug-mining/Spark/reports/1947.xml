<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:28:13 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-8020] Spark SQL conf in spark-defaults.conf make metadataHive get constructed too early</title>
                <link>https://issues.apache.org/jira/browse/SPARK-8020</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;To correctly construct a &lt;tt&gt;metadataHive&lt;/tt&gt; object, we need two settings, &lt;tt&gt;spark.sql.hive.metastore.version&lt;/tt&gt; and &lt;tt&gt;spark.sql.hive.metastore.jars&lt;/tt&gt;. If users want to use Hive 0.12&apos;s metastore, they need to set &lt;tt&gt;spark.sql.hive.metastore.version&lt;/tt&gt; to &lt;tt&gt;0.12.0&lt;/tt&gt; and set &lt;tt&gt;spark.sql.hive.metastore.jars&lt;/tt&gt; to &lt;tt&gt;maven&lt;/tt&gt; or a classpath containing Hive and Hadoop&apos;s jars. However, any spark sql setting in the &lt;tt&gt;spark-defaults.conf&lt;/tt&gt; will trigger the construction of &lt;tt&gt;metadataHive&lt;/tt&gt; and cause Spark SQL connect to the wrong metastore (e.g. connect to the local derby metastore instead of a remove mysql Hive 0.12 metastore). Also, if &lt;tt&gt;spark.sql.hive.metastore.version 0.12.0&lt;/tt&gt; is the first conf set to SQL conf, we will get&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Exception in thread &lt;span class=&quot;code-quote&quot;&gt;&quot;main&quot;&lt;/span&gt; java.lang.IllegalArgumentException: Builtin jars can only be used when hive execution version == hive metastore version. Execution: 0.13.1 != Metastore: 0.12.0. Specify a vaild path to the correct hive jars using $HIVE_METASTORE_JARS or change spark.sql.hive.metastore.version to 0.13.1.
	at org.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:186)
	at org.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:175)
	at org.apache.spark.sql.hive.HiveContext.setConf(HiveContext.scala:358)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:186)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:185)
	at scala.collection.IndexedSeqOptimized$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.sql.SQLContext.&amp;lt;init&amp;gt;(SQLContext.scala:185)
	at org.apache.spark.sql.hive.HiveContext.&amp;lt;init&amp;gt;(HiveContext.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLEnv$.init(SparkSQLEnv.scala:53)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.&amp;lt;init&amp;gt;(SparkSQLCLIDriver.scala:248)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:136)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:664)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:169)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:192)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:111)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="12834388">SPARK-8020</key>
            <summary>Spark SQL conf in spark-defaults.conf make metadataHive get constructed too early</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="yhuai">Yin Huai</assignee>
                                    <reporter username="yhuai">Yin Huai</reporter>
                        <labels>
                    </labels>
                <created>Mon, 1 Jun 2015 21:42:16 +0000</created>
                <updated>Mon, 18 Apr 2016 15:19:11 +0000</updated>
                            <resolved>Tue, 2 Jun 2015 07:17:31 +0000</resolved>
                                    <version>1.4.0</version>
                                    <fixVersion>1.4.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="14568248" author="apachespark" created="Mon, 1 Jun 2015 23:55:02 +0000"  >&lt;p&gt;User &apos;yhuai&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/6563&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/6563&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14568313" author="yhuai" created="Tue, 2 Jun 2015 00:47:34 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cheolsoo&quot; class=&quot;user-hover&quot; rel=&quot;cheolsoo&quot;&gt;cheolsoo&lt;/a&gt; Can you try &lt;a href=&quot;https://github.com/apache/spark/pull/6563?&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/6563?&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14568427" author="jeanlyn" created="Tue, 2 Jun 2015 02:46:02 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yhuai&quot; class=&quot;user-hover&quot; rel=&quot;yhuai&quot;&gt;yhuai&lt;/a&gt;,I set &lt;b&gt;spark.sql.hive.metastore.jars&lt;/b&gt; in spark-defaults.conf i got errors like yours.But when i set &lt;b&gt;spark.sql.hive.metastore.jars&lt;/b&gt; in &lt;b&gt;hive-site.xml&lt;/b&gt; i got&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;5/06/02 10:42:04 INFO storage.BlockManagerMaster: Trying to register BlockManager
15/06/02 10:42:04 INFO storage.BlockManagerMasterEndpoint: Registering block manager localhost:41416 with 706.6 MB RAM, BlockManagerId(driver, localhost, 41416)
15/06/02 10:42:04 INFO storage.BlockManagerMaster: Registered BlockManager
SET spark.sql.hive.metastore.version=0.12.0
15/06/02 10:42:04 WARN conf.HiveConf: DEPRECATED: Configuration property hive.metastore.local no longer has any effect. Make sure to provide a valid value &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; hive.metastore.u
ris &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; you are connecting to a remote metastore.
15/06/02 10:42:04 WARN conf.HiveConf: DEPRECATED: hive.metastore.ds.retry.* no longer has any effect.  Use hive.hmshandler.retry.* instead
15/06/02 10:42:04 INFO hive.HiveContext: Initializing HiveMetastoreConnection version 0.12.0 using maven.
Ivy Default Cache set to: /home/dd_edw/.ivy2/cache
The jars &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; the packages stored in: /home/dd_edw/.ivy2/jars
http:&lt;span class=&quot;code-comment&quot;&gt;//www.datanucleus.org/downloads/maven2 added as a remote repository with the name: repo-1
&lt;/span&gt;:: loading settings :: url = jar:file:/data0/spark-1.3.0-bin-2.2.0/lib/spark-assembly-1.4.0-SNAPSHOT-hadoop2.2.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
org.apache.hive#hive-metastore added as a dependency
org.apache.hive#hive-exec added as a dependency
org.apache.hive#hive-common added as a dependency
org.apache.hive#hive-serde added as a dependency
com.google.guava#guava added as a dependency
org.apache.hadoop#hadoop-client added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0
       confs: [&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;]
       found org.apache.hive#hive-metastore;0.12.0 in central
       found org.antlr#antlr;3.4 in central
       found org.antlr#antlr-runtime;3.4 in central
....
xception in thread &lt;span class=&quot;code-quote&quot;&gt;&quot;main&quot;&lt;/span&gt; java.lang.ClassNotFoundException: java.lang.NoClassDefFoundError: com/google/common/base/Preconditions when creating Hive client using classpath: fi
le:/tmp/hive3795822184995995241vv12/aopalliance_aopalliance-1.0.jar, file:/tmp/hive3795822184995995241vv12/org.apache.hive_hive-exec-0.12.0.jar, file:/tmp/hive3795822184995995
241vv12/org.apache.thrift_libfb303-0.9.0.jar, file:/tmp/hive3795822184995995241vv12/commons-digester_commons-digester-1.8.jar, file:/tmp/hive3795822184995995241vv12/com.sun.je
rsey_jersey-client-1.9.jar, file:/tmp/hive3795822184995995241vv12/org.apache.httpcomponents_httpclient-4.2.5.jar, file:/tmp/hive3795822184995995241vv12/org.antlr_stringtemplat
e-3.2.1.jar, file:/tmp/hive3795822184995995241vv12/commons-logging_commons-logging-1.1.3.jar, file:/tmp/hive3795822184995995241vv12/org.antlr_antlr-runtime-3.4.jar, file:/tmp/
hive3795822184995995241vv12/org.mockito_mockito-all-1.8.2.jar, file:/tmp/hive3795822184995995241vv12/org.apache.derby_derby-10.4.2.0.jar, file:/tmp/hive3795822184995995241vv12
/antlr_antlr-2.7.7.jar, file:/tmp/hive3795822184995995241vv12/commons-net_commons-net-3.1.jar, file:/tmp/hive3795822184995995241vv12/org.slf4j_slf4j-log4j12-1.7.5.jar, file:/t
mp/hive3795822184995995241vv12/junit_junit-3.8.1.jar, file:/tmp/hive3795822184995995241vv12/org.codehaus.jackson_jackson-jaxrs-1.8.8.jar, file:/tmp/hive3795822184995995241vv12
/commons-cli_commons-cli-1.2.jar, file:/tmp/hive3795822184995995241vv12/org.apache.hive_hive-serde-0.12.0.jar, file:/tmp/hive3795822184995995241vv12/org.codehaus.jettison_jett
ison-1.1.jar, file:/tmp/hive3795822184995995241vv12/javax.xml.stream_stax-api-1.0-2.jar, file:/tmp/hive3795822184995995241vv12/org.apache.avro_avro-1.7.4.jar, file:/tmp/hive37
95822184995995241vv12/org.apache.hadoop_hadoop-mapreduce-client-app-2.4.0.jar, file:/tmp/hive3795822184995995241vv12/org.apache.hadoop_hadoop-mapreduce-client-common-2.4.0.jar
, file:/tmp/hive3795822184995995241vv12/org.codehaus.jackson_jackson-xc-1.8.8.jar, file:/tmp/hive3795822184995995241vv12/org.apache.hadoop_hadoop-annotations-2.4.0.jar, file:/
tmp/hive3795822184995995241vv12/org.mortbay.jetty_jetty-util-6.1.26.jar, file:/tmp/hive3795822184995995241vv12/org.apache.commons_commons-math3-3.1.1.jar, file:/tmp/hive379582
2184995995241vv12/javax.transaction_jta-1.1.jar, file:/tmp/hive3795822184995995241vv12/commons-httpclient_commons-httpclient-3.1.jar, file:/tmp/hive3795822184995995241vv12/xml
enc_xmlenc-0.52.jar, file:/tmp/hive3795822184995995241vv12/org.sonatype.sisu.inject_cglib-2.2.1-v20090111.jar, file:/tmp/hive3795822184995995241vv12/com.google.code.findbugs_j
sr305-1.3.9.jar, file:/tmp/hive3795822184995995241vv12/commons-codec_commons-codec-1.4.jar, file:/tmp/hive3795822184995995241vv12/com.google.guava_guava-14.0.1.jar, file:/tmp/
hive3795822184995995241vv12/org.apache.hadoop_hadoop-mapreduce-client-shuffle-2.4.0.jar, file:/tmp/hive3795822184995995241vv12/org.jboss.netty_netty-3.2.2.Final.jar, file:/tmp
/hive3795822184995995241vv12/org.apache.commons_commons-compress-1.4.1.jar, file:/tmp/hive3795822184995995241vv12/org.apache.avro_avro-mapred-1.7.1.jar, file:/tmp/hive37958221
84995995241vv12/org.slf4j_slf4j-api-1.7.5.jar, file:/tmp/hive3795822184995995241vv12/javolution_javolution-5.5.1.jar, file:/tmp/hive3795822184995995241vv12/com.sun.xml.bind_ja
xb-impl-2.2.3-1.jar, file:/tmp/hive3795822184995995241vv12/org.iq80.snappy_snappy-0.2.jar, file:/tmp/hive3795822184995995241vv12/org.apache.hadoop_hadoop-yarn-client-2.4.0.jar
, file:/tmp/hive3795822184995995241vv12/log4j_log4j-1.2.17.jar, file:/tmp/hive3795822184995995241vv12/commons-pool_commons-pool-1.5.4.jar, file:/tmp/hive3795822184995995241vv1
2/io.netty_netty-3.4.0.Final.jar, file:/tmp/hive3795822184995995241vv12/org.apache.avro_avro-ipc-1.7.1.jar, file:/tmp/hive3795822184995995241vv12/org.apache.zookeeper_zookeepe
r-3.4.3.jar, file:/tmp/hive3795822184995995241vv12/org.json_json-20090211.jar, file:/tmp/hive3795822184995995241vv12/org.apache.hive_hive-metastore-0.12.0.jar, file:/tmp/hive3
795822184995995241vv12/org.datanucleus_datanucleus-api-jdo-3.2.1.jar, file:/tmp/hive3795822184995995241vv12/org.mortbay.jetty_servlet-api-2.5-20081211.jar, file:/tmp/hive37958
22184995995241vv12/org.apache.hadoop_hadoop-auth-2.4.0.jar, file:/tmp/hive3795822184995995241vv12/javax.xml.bind_jaxb-api-2.2.2.jar, file:/tmp/hive3795822184995995241vv12/com.
sun.jersey_jersey-server-1.9.jar, file:/tmp/hive3795822184995995241vv12/asm_asm-3.2.jar, file:/tmp/hive3795822184995995241vv12/javax.activation_activation-1.1.jar, file:/tmp/h
ive3795822184995995241vv12/org.datanucleus_datanucleus-core-3.2.2.jar, file:/tmp/hive3795822184995995241vv12/com.jolbox_bonecp-0.7.1.RELEASE.jar, file:/tmp/hive379582218499599
5241vv12/org.tukaani_xz-1.0.jar, file:/tmp/hive3795822184995995241vv12/org.mortbay.jetty_jetty-6.1.26.jar, file:/tmp/hive3795822184995995241vv12/com.sun.jersey.contribs_jersey
-guice-1.9.jar, file:/tmp/hive3795822184995995241vv12/org.apache.hadoop_hadoop-hdfs-2.4.0.jar, file:/tmp/hive3795822184995995241vv12/commons-collections_commons-collections-3.
2.1.jar, file:/tmp/hive3795822184995995241vv12/commons-beanutils_commons-beanutils-1.7.0.jar, file:/tmp/hive3795822184995995241vv12/org.apache.hadoop_hadoop-mapreduce-client-c
ore-2.4.0.jar, file:/tmp/hive3795822184995995241vv12/org.apache.hadoop_hadoop-common-2.4.0.jar, file:/tmp/hive3795822184995995241vv12/com.googlecode.javaewah_JavaEWAH-0.3.2.ja
r, file:/tmp/hive3795822184995995241vv12/com.sun.jersey_jersey-json-1.9.jar, file:/tmp/hive3795822184995995241vv12/org.apache.hadoop_hadoop-mapreduce-client-jobclient-2.4.0.ja
r, file:/tmp/hive3795822184995995241vv12/com.sun.jersey_jersey-core-1.9.jar, file:/tmp/hive3795822184995995241vv12/org.datanucleus_datanucleus-rdbms-3.2.1.jar, file:/tmp/hive3
795822184995995241vv12/javax.jdo_jdo-api-3.0.1.jar, file:/tmp/hive3795822184995995241vv12/org.apache.hive_hive-common-0.12.0.jar, file:/tmp/hive3795822184995995241vv12/commons
-beanutils_commons-beanutils-core-1.8.0.jar, file:/tmp/hive3795822184995995241vv12/org.codehaus.jackson_jackson-mapper-asl-1.8.8.jar, file:/tmp/hive3795822184995995241vv12/com
.thoughtworks.paranamer_paranamer-2.3.jar, file:/tmp/hive3795822184995995241vv12/com.google.protobuf_protobuf-java-2.5.0.jar, file:/tmp/hive3795822184995995241vv12/javax.servl
et_servlet-api-2.5.jar, file:/tmp/hive3795822184995995241vv12/org.apache.velocity_velocity-1.7.jar, file:/tmp/hive3795822184995995241vv12/org.apache.thrift_libthrift-0.9.0.jar
, file:/tmp/hive3795822184995995241vv12/org.apache.hadoop_hadoop-yarn-server-common-2.4.0.jar, file:/tmp/hive3795822184995995241vv12/jline_jline-0.9.94.jar, file:/tmp/hive3795
822184995995241vv12/commons-logging_commons-logging-api-1.0.4.jar, file:/tmp/hive3795822184995995241vv12/org.apache.hive_hive-shims-0.12.0.jar, file:/tmp/hive37958221849959952
41vv12/org.apache.hadoop_hadoop-yarn-api-2.4.0.jar, file:/tmp/hive3795822184995995241vv12/commons-io_commons-io-2.4.jar, file:/tmp/hive3795822184995995241vv12/org.antlr_ST4-4.
0.4.jar, file:/tmp/hive3795822184995995241vv12/org.codehaus.jackson_jackson-core-asl-1.8.8.jar, file:/tmp/hive3795822184995995241vv12/commons-lang_commons-lang-2.6.jar, file:/
tmp/hive3795822184995995241vv12/org.apache.hadoop_hadoop-yarn-common-2.4.0.jar, file:/tmp/hive3795822184995995241vv12/org.apache.httpcomponents_httpcore-4.2.5.jar, file:/tmp/h
ive3795822184995995241vv12/org.antlr_antlr-3.4.jar, file:/tmp/hive3795822184995995241vv12/commons-configuration_commons-configuration-1.6.jar, file:/tmp/hive379582218499599524
1vv12/com.google.inject_guice-3.0.jar, file:/tmp/hive3795822184995995241vv12/javax.inject_javax.inject-1.jar, file:/tmp/hive3795822184995995241vv12/org.xerial.snappy_snappy-ja
va-1.0.4.1.jar, file:/tmp/hive3795822184995995241vv12/org.apache.hadoop_hadoop-client-2.4.0.jar
Please make sure that jars &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; your version of hive and hadoop are included in the paths passed to spark.sql.hive.metastore.jars.
       at org.apache.spark.sql.hive.client.IsolatedClientLoader.liftedTree1$1(IsolatedClientLoader.scala:174)
       at org.apache.spark.sql.hive.client.IsolatedClientLoader.&amp;lt;init&amp;gt;(IsolatedClientLoader.scala:166)
       at org.apache.spark.sql.hive.client.IsolatedClientLoader$.forVersion(IsolatedClientLoader.scala:45)
       at org.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:213)
       at org.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:174)
       at org.apache.spark.sql.hive.HiveContext.setConf(HiveContext.scala:349)
       at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:188)
       at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:187)
       at scala.collection.IndexedSeqOptimized$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(IndexedSeqOptimized.scala:33)
       at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
       at org.apache.spark.sql.SQLContext.&amp;lt;init&amp;gt;(SQLContext.scala:187)
       at org.apache.spark.sql.hive.HiveContext.&amp;lt;init&amp;gt;(HiveContext.scala:70)
       at org.apache.spark.sql.hive.thriftserver.SparkSQLEnv$.init(SparkSQLEnv.scala:53)
       at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.&amp;lt;init&amp;gt;(SparkSQLCLIDriver.scala:248)
       at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:136)
       at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
       at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
       at java.lang.reflect.Method.invoke(Method.java:597)
       at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:664)
       at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:169)
       at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:192)
       at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:111)
       at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/06/02 10:42:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt; as i show in &lt;a href=&quot;https://github.com/apache/spark/pull/5876&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/5876&lt;/a&gt;&lt;br/&gt;
shall we move this discusstion to user mail list?&lt;/p&gt;</comment>
                            <comment id="14568444" author="cheolsoo" created="Tue, 2 Jun 2015 03:14:09 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yhuai&quot; class=&quot;user-hover&quot; rel=&quot;yhuai&quot;&gt;yhuai&lt;/a&gt;, the patch seems to fix the original error. Thank you!&lt;/p&gt;

&lt;p&gt;But it doesn&apos;t make it easy for me to use Hive 0.12 metastore. Now the challenge is that I set &lt;tt&gt;spark.sql.hive.metastore.jars&lt;/tt&gt; to &lt;tt&gt;/home/cheolsoop/hive-0.12.0-bin/lib/*:$(hadoop classpath)&lt;/tt&gt;, and that brings in all sorts of class conflicts that I didn&apos;t have when using the built-in Hive metastore. For now, I&apos;ll probably continue to use my workaround (i.e. commenting out &lt;a href=&quot;https://github.com/apache/spark/blob/master/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLCLIDriver.scala#L174&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;this code&lt;/a&gt;) and use the built-in Hive metastore. Btw, Hive 0.13 client is almost compatible with Hive 0.12 metastore server except one introduced by &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-6330&quot; title=&quot;Metastore support for permanent UDFs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-6330&quot;&gt;&lt;del&gt;HIVE-6330&lt;/del&gt;&lt;/a&gt;. It is not too bad as long as users can build their own jars.&lt;/p&gt;</comment>
                            <comment id="14568482" author="yhuai" created="Tue, 2 Jun 2015 04:03:04 +0000"  >&lt;p&gt;Is guava causing the problem?&lt;/p&gt;</comment>
                            <comment id="14568533" author="apachespark" created="Tue, 2 Jun 2015 05:07:09 +0000"  >&lt;p&gt;User &apos;yhuai&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/6571&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/6571&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14568599" author="yhuai" created="Tue, 2 Jun 2015 06:09:08 +0000"  >&lt;p&gt;Can you put those settings back to spark-defaults.conf and try again once this one is resolved? I feel putting them in hive-site is not the right way.&lt;/p&gt;</comment>
                            <comment id="14568691" author="yhuai" created="Tue, 2 Jun 2015 07:17:31 +0000"  >&lt;p&gt;Issue resolved by pull request 6571&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/6571&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/6571&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14570303" author="jeanlyn" created="Wed, 3 Jun 2015 05:37:12 +0000"  >&lt;p&gt;I had tried to put the settings back to &lt;b&gt;spark-defaults.conf&lt;/b&gt; just now,and i builded spark with rc4.I still got the same &lt;b&gt;ClassNotFoundException&lt;/b&gt; excption as i mentioned about&lt;/p&gt;</comment>
                            <comment id="15245838" author="apachespark" created="Mon, 18 Apr 2016 15:19:11 +0000"  >&lt;p&gt;User &apos;yhuai&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/12471&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/12471&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                            <outwardlinks description="duplicates">
                                        <issuelink>
            <issuekey id="12832452">SPARK-7851</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 31 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2fhan:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12329359">1.4.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>