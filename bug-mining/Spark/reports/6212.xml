<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 19:03:16 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-26267] Kafka source may reprocess data</title>
                <link>https://issues.apache.org/jira/browse/SPARK-26267</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Due to &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7703&quot; title=&quot;KafkaConsumer.position may return a wrong offset after &amp;quot;seekToEnd&amp;quot; is called&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7703&quot;&gt;&lt;del&gt;KAFKA-7703&lt;/del&gt;&lt;/a&gt;, when the Kafka source tries to get the latest offset, it may get an earliest offset, and then it will reprocess messages that have been processed when it gets the correct latest offset in the next batch.&lt;/p&gt;

&lt;p&gt;This usually happens when restarting a streaming query.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13202353">SPARK-26267</key>
            <summary>Kafka source may reprocess data</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="zsxwing">Shixiong Zhu</assignee>
                                    <reporter username="zsxwing">Shixiong Zhu</reporter>
                        <labels>
                            <label>correctness</label>
                    </labels>
                <created>Tue, 4 Dec 2018 19:25:22 +0000</created>
                <updated>Wed, 4 Sep 2019 11:11:38 +0000</updated>
                            <resolved>Tue, 8 Jan 2019 00:54:12 +0000</resolved>
                                    <version>2.4.0</version>
                                    <fixVersion>2.4.1</fixVersion>
                    <fixVersion>3.0.0</fixVersion>
                                    <component>Structured Streaming</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="16709169" author="zsxwing" created="Tue, 4 Dec 2018 19:27:11 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7703&quot; title=&quot;KafkaConsumer.position may return a wrong offset after &amp;quot;seekToEnd&amp;quot; is called&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7703&quot;&gt;&lt;del&gt;KAFKA-7703&lt;/del&gt;&lt;/a&gt; only exists in Kafka 1.1.0 and above, so a possible workaround is using an old version that doesn&apos;t have this issue. This doesn&apos;t impact Spark 2.3.x and below as we use Kafka 0.10.0.1 by default.&lt;/p&gt;</comment>
                            <comment id="16721869" author="githubbot" created="Fri, 14 Dec 2018 23:19:40 +0000"  >&lt;p&gt;zsxwing opened a new pull request #23324: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26267&quot; title=&quot;Kafka source may reprocess data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-26267&quot;&gt;&lt;del&gt;SPARK-26267&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;SS&amp;#93;&lt;/span&gt;Retry when detecting incorrect offsets from Kafka&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/23324&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/23324&lt;/a&gt;&lt;/p&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;What changes were proposed in this pull request?&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;   Due to &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7703&quot; title=&quot;KafkaConsumer.position may return a wrong offset after &amp;quot;seekToEnd&amp;quot; is called&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7703&quot;&gt;&lt;del&gt;KAFKA-7703&lt;/del&gt;&lt;/a&gt;(&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7703&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-7703&lt;/a&gt;), Kafka may return an earliest offset when we are request a latest offset. This will cause Spark to reprocess data.&lt;/p&gt;

&lt;p&gt;   To reduce the impact of &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7703&quot; title=&quot;KafkaConsumer.position may return a wrong offset after &amp;quot;seekToEnd&amp;quot; is called&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7703&quot;&gt;&lt;del&gt;KAFKA-7703&lt;/del&gt;&lt;/a&gt;, this PR will use the previous offsets we get to audit the result from Kafka. If we find any incorrect offset, we will retry at most `maxOffsetFetchAttempts` times. For the first batch of a new query, as we don&apos;t have any previous offsets, we simply fetch offsets twice. This should reduce the chance to hit &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7703&quot; title=&quot;KafkaConsumer.position may return a wrong offset after &amp;quot;seekToEnd&amp;quot; is called&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7703&quot;&gt;&lt;del&gt;KAFKA-7703&lt;/del&gt;&lt;/a&gt; a lot.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;How was this patch tested?&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;   Jenkins&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16726970" author="githubbot" created="Fri, 21 Dec 2018 18:45:00 +0000"  >&lt;p&gt;zsxwing closed pull request #23324: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26267&quot; title=&quot;Kafka source may reprocess data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-26267&quot;&gt;&lt;del&gt;SPARK-26267&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;SS&amp;#93;&lt;/span&gt;Retry when detecting incorrect offsets from Kafka&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/23324&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/23324&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaContinuousReadSupport.scala b/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaContinuousReadSupport.scala&lt;br/&gt;
index 1753a28fba2fb..02dfb9ca2b95a 100644&lt;br/&gt;
&amp;#8212; a/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaContinuousReadSupport.scala&lt;br/&gt;
+++ b/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaContinuousReadSupport.scala&lt;br/&gt;
@@ -60,7 +60,7 @@ class KafkaContinuousReadSupport(&lt;br/&gt;
   override def initialOffset(): Offset = {&lt;br/&gt;
     val offsets = initialOffsets match &lt;/p&gt;
{
       case EarliestOffsetRangeLimit =&amp;gt; KafkaSourceOffset(offsetReader.fetchEarliestOffsets())
-      case LatestOffsetRangeLimit =&amp;gt; KafkaSourceOffset(offsetReader.fetchLatestOffsets())
+      case LatestOffsetRangeLimit =&amp;gt; KafkaSourceOffset(offsetReader.fetchLatestOffsets(None))
       case SpecificOffsetRangeLimit(p) =&amp;gt; offsetReader.fetchSpecificOffsets(p, reportDataLoss)
     }
&lt;p&gt;     logInfo(s&quot;Initial offsets: $offsets&quot;)&lt;br/&gt;
@@ -107,7 +107,7 @@ class KafkaContinuousReadSupport(&lt;/p&gt;

&lt;p&gt;   override def needsReconfiguration(config: ScanConfig): Boolean = &lt;/p&gt;
{
     val knownPartitions = config.asInstanceOf[KafkaContinuousScanConfig].knownPartitions
-    offsetReader.fetchLatestOffsets().keySet != knownPartitions
+    offsetReader.fetchLatestOffsets(None).keySet != knownPartitions
   }

&lt;p&gt;   override def toString(): String = s&quot;KafkaSource&lt;span class=&quot;error&quot;&gt;&amp;#91;$offsetReader&amp;#93;&lt;/span&gt;&quot;&lt;br/&gt;
diff --git a/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchReadSupport.scala b/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchReadSupport.scala&lt;br/&gt;
index bb4de674c3c72..b4f042e93a5da 100644&lt;br/&gt;
&amp;#8212; a/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchReadSupport.scala&lt;br/&gt;
+++ b/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchReadSupport.scala&lt;br/&gt;
@@ -84,7 +84,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka010&amp;#93;&lt;/span&gt; class KafkaMicroBatchReadSupport(&lt;/p&gt;

&lt;p&gt;   override def latestOffset(start: Offset): Offset = {&lt;br/&gt;
     val startPartitionOffsets = start.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;KafkaSourceOffset&amp;#93;&lt;/span&gt;.partitionToOffsets&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val latestPartitionOffsets = kafkaOffsetReader.fetchLatestOffsets()&lt;br/&gt;
+    val latestPartitionOffsets = kafkaOffsetReader.fetchLatestOffsets(Some(startPartitionOffsets))&lt;br/&gt;
     endPartitionOffsets = KafkaSourceOffset(maxOffsetsPerTrigger.map 
{ maxOffsets =&amp;gt;
       rateLimit(maxOffsets, startPartitionOffsets, latestPartitionOffsets)
     }
&lt;p&gt;.getOrElse &lt;/p&gt;
{
@@ -133,10 +133,21 @@ private[kafka010] class KafkaMicroBatchReadSupport(
     }
&lt;p&gt;.toSeq&lt;br/&gt;
     logDebug(&quot;TopicPartitions: &quot; + topicPartitions.mkString(&quot;, &quot;))&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    val fromOffsets = startPartitionOffsets ++ newPartitionInitialOffsets&lt;br/&gt;
+    val untilOffsets = endPartitionOffsets&lt;br/&gt;
+    untilOffsets.foreach { case (tp, untilOffset) =&amp;gt;&lt;br/&gt;
+      fromOffsets.get(tp).foreach { fromOffset =&amp;gt;&lt;br/&gt;
+        if (untilOffset &amp;lt; fromOffset) &lt;/p&gt;
{
+          reportDataLoss(s&quot;Partition $tp&apos;s offset was changed from &quot; +
+            s&quot;$fromOffset to $untilOffset, some data may have been missed&quot;)
+        }
&lt;p&gt;+      }&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
     // Calculate offset ranges&lt;br/&gt;
     val offsetRanges = rangeCalculator.getRanges(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;fromOffsets = startPartitionOffsets ++ newPartitionInitialOffsets,&lt;/li&gt;
	&lt;li&gt;untilOffsets = endPartitionOffsets,&lt;br/&gt;
+      fromOffsets = fromOffsets,&lt;br/&gt;
+      untilOffsets = untilOffsets,&lt;br/&gt;
       executorLocations = getSortedExecutorList())&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // Reuse Kafka consumers only when all the offset ranges have distinct TopicPartitions,&lt;br/&gt;
@@ -186,7 +197,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka010&amp;#93;&lt;/span&gt; class KafkaMicroBatchReadSupport(&lt;br/&gt;
         case EarliestOffsetRangeLimit =&amp;gt;&lt;br/&gt;
           KafkaSourceOffset(kafkaOffsetReader.fetchEarliestOffsets())&lt;br/&gt;
         case LatestOffsetRangeLimit =&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;KafkaSourceOffset(kafkaOffsetReader.fetchLatestOffsets())&lt;br/&gt;
+          KafkaSourceOffset(kafkaOffsetReader.fetchLatestOffsets(None))&lt;br/&gt;
         case SpecificOffsetRangeLimit(p) =&amp;gt;&lt;br/&gt;
           kafkaOffsetReader.fetchSpecificOffsets(p, reportDataLoss)&lt;br/&gt;
       }&lt;br/&gt;
diff --git a/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaOffsetRangeCalculator.scala b/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaOffsetRangeCalculator.scala&lt;br/&gt;
index fb209c724afba..6008794924052 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaOffsetRangeCalculator.scala&lt;br/&gt;
+++ b/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaOffsetRangeCalculator.scala&lt;br/&gt;
@@ -37,6 +37,8 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka010&amp;#93;&lt;/span&gt; class KafkaOffsetRangeCalculator(val minPartitions: Option[Int&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;the read tasks of the skewed partitions to multiple Spark tasks.&lt;/li&gt;
	&lt;li&gt;The number of Spark tasks will be &lt;b&gt;approximately&lt;/b&gt; `numPartitions`. It can be less or more&lt;/li&gt;
	&lt;li&gt;depending on rounding errors or Kafka partitions that didn&apos;t receive any new data.&lt;br/&gt;
+   *&lt;br/&gt;
+   * Empty ranges (`KafkaOffsetRange.size &amp;lt;= 0`) will be dropped.&lt;br/&gt;
    */&lt;br/&gt;
   def getRanges(&lt;br/&gt;
       fromOffsets: PartitionOffsetMap,&lt;br/&gt;
diff --git a/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaOffsetReader.scala b/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaOffsetReader.scala&lt;br/&gt;
index 82066697cb95a..fc443d22bf5a2 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaOffsetReader.scala&lt;br/&gt;
+++ b/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaOffsetReader.scala&lt;br/&gt;
@@ -21,6 +21,7 @@ import java.
{util =&amp;gt; ju}
&lt;p&gt; import java.util.concurrent.&lt;/p&gt;
{Executors, ThreadFactory}&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import scala.collection.JavaConverters._&lt;br/&gt;
+import scala.collection.mutable.ArrayBuffer&lt;br/&gt;
 import scala.concurrent.&lt;/p&gt;
{ExecutionContext, Future}
&lt;p&gt; import scala.concurrent.duration.Duration&lt;br/&gt;
 import scala.util.control.NonFatal&lt;br/&gt;
@@ -137,6 +138,12 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka010&amp;#93;&lt;/span&gt; class KafkaOffsetReader(&lt;br/&gt;
         // Poll to get the latest assigned partitions&lt;br/&gt;
         consumer.poll(0)&lt;br/&gt;
         val partitions = consumer.assignment()&lt;br/&gt;
+&lt;br/&gt;
+        // Call `position` to wait until the potential offset request triggered by `poll(0)` is&lt;br/&gt;
+        // done. This is a workaround for &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7703&quot; title=&quot;KafkaConsumer.position may return a wrong offset after &amp;quot;seekToEnd&amp;quot; is called&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7703&quot;&gt;&lt;del&gt;KAFKA-7703&lt;/del&gt;&lt;/a&gt;, which an async `seekToBeginning` triggered by&lt;br/&gt;
+        // `poll(0)` may reset offsets that should have been set by another request.&lt;br/&gt;
+        partitions.asScala.map(p =&amp;gt; p -&amp;gt; consumer.position(p)).foreach(_ =&amp;gt; {})&lt;br/&gt;
+&lt;br/&gt;
         consumer.pause(partitions)&lt;br/&gt;
         assert(partitions.asScala == partitionOffsets.keySet,&lt;br/&gt;
           &quot;If startingOffsets contains specific offsets, you must specify all TopicPartitions.\n&quot; +&lt;br/&gt;
@@ -192,19 +199,82 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka010&amp;#93;&lt;/span&gt; class KafkaOffsetReader(&lt;br/&gt;
   /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Fetch the latest offsets for the topic partitions that are indicated&lt;/li&gt;
	&lt;li&gt;in the [&lt;span class=&quot;error&quot;&gt;&amp;#91;ConsumerStrategy&amp;#93;&lt;/span&gt;].&lt;br/&gt;
+   *&lt;br/&gt;
+   * Kafka may return earliest offsets when we are requesting latest offsets if `poll` is called&lt;br/&gt;
+   * right before `seekToEnd` (&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7703&quot; title=&quot;KafkaConsumer.position may return a wrong offset after &amp;quot;seekToEnd&amp;quot; is called&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7703&quot;&gt;&lt;del&gt;KAFKA-7703&lt;/del&gt;&lt;/a&gt;). As a workaround, we will call `position` right after&lt;br/&gt;
+   * `poll` to wait until the potential offset request triggered by `poll(0)` is done.&lt;br/&gt;
+   *&lt;br/&gt;
+   * In addition, to avoid other unknown issues, we also use the given `knownOffsets` to audit the&lt;br/&gt;
+   * latest offsets returned by Kafka. If we find some incorrect offsets (a latest offset is less&lt;br/&gt;
+   * than an offset in `knownOffsets`), we will retry at most `maxOffsetFetchAttempts` times. When&lt;br/&gt;
+   * a topic is recreated, the latest offsets may be less than offsets in `knownOffsets`. We cannot&lt;br/&gt;
+   * distinguish this with &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7703&quot; title=&quot;KafkaConsumer.position may return a wrong offset after &amp;quot;seekToEnd&amp;quot; is called&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7703&quot;&gt;&lt;del&gt;KAFKA-7703&lt;/del&gt;&lt;/a&gt;, so we just return whatever we get from Kafka after retrying.&lt;br/&gt;
    */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def fetchLatestOffsets(): Map&lt;span class=&quot;error&quot;&gt;&amp;#91;TopicPartition, Long&amp;#93;&lt;/span&gt; = runUninterruptibly {&lt;br/&gt;
+  def fetchLatestOffsets(&lt;br/&gt;
+      knownOffsets: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;PartitionOffsetMap&amp;#93;&lt;/span&gt;): PartitionOffsetMap = runUninterruptibly {&lt;br/&gt;
     withRetriesWithoutInterrupt {&lt;br/&gt;
       // Poll to get the latest assigned partitions&lt;br/&gt;
       consumer.poll(0)&lt;br/&gt;
       val partitions = consumer.assignment()&lt;br/&gt;
+&lt;br/&gt;
+      // Call `position` to wait until the potential offset request triggered by `poll(0)` is&lt;br/&gt;
+      // done. This is a workaround for &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7703&quot; title=&quot;KafkaConsumer.position may return a wrong offset after &amp;quot;seekToEnd&amp;quot; is called&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7703&quot;&gt;&lt;del&gt;KAFKA-7703&lt;/del&gt;&lt;/a&gt;, which an async `seekToBeginning` triggered by&lt;br/&gt;
+      // `poll(0)` may reset offsets that should have been set by another request.&lt;br/&gt;
+      partitions.asScala.map(p =&amp;gt; p -&amp;gt; consumer.position(p)).foreach(_ =&amp;gt; {})&lt;br/&gt;
+&lt;br/&gt;
       consumer.pause(partitions)&lt;br/&gt;
       logDebug(s&quot;Partitions assigned to consumer: $partitions. Seeking to the end.&quot;)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;consumer.seekToEnd(partitions)&lt;/li&gt;
	&lt;li&gt;val partitionOffsets = partitions.asScala.map(p =&amp;gt; p -&amp;gt; consumer.position(p)).toMap&lt;/li&gt;
	&lt;li&gt;logDebug(s&quot;Got latest offsets for partition : $partitionOffsets&quot;)&lt;/li&gt;
	&lt;li&gt;partitionOffsets&lt;br/&gt;
+      if (knownOffsets.isEmpty) 
{
+        consumer.seekToEnd(partitions)
+        partitions.asScala.map(p =&amp;gt; p -&amp;gt; consumer.position(p)).toMap
+      }
&lt;p&gt; else {&lt;br/&gt;
+        var partitionOffsets: PartitionOffsetMap = Map.empty&lt;br/&gt;
+&lt;br/&gt;
+        /**&lt;br/&gt;
+         * Compare `knownOffsets` and `partitionOffsets`. Returns all partitions that have incorrect&lt;br/&gt;
+         * latest offset (offset in `knownOffsets` is great than the one in `partitionOffsets`).&lt;br/&gt;
+         */&lt;br/&gt;
+        def findIncorrectOffsets(): Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;(TopicPartition, Long, Long)&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+          var incorrectOffsets = ArrayBuffer&lt;span class=&quot;error&quot;&gt;&amp;#91;(TopicPartition, Long, Long)&amp;#93;&lt;/span&gt;()&lt;br/&gt;
+          partitionOffsets.foreach { case (tp, offset) =&amp;gt;&lt;br/&gt;
+            knownOffsets.foreach(_.get(tp).foreach &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: { knownOffset =&amp;gt;+              if (knownOffset &amp;gt; offset) {
+                val incorrectOffset = (tp, knownOffset, offset)
+                incorrectOffsets += incorrectOffset
+              }+            }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;)&lt;br/&gt;
+          }&lt;br/&gt;
+          incorrectOffsets&lt;br/&gt;
+        }&lt;br/&gt;
+&lt;br/&gt;
+        // Retry to fetch latest offsets when detecting incorrect offsets. We don&apos;t use&lt;br/&gt;
+        // `withRetriesWithoutInterrupt` to retry because:&lt;br/&gt;
+        //&lt;br/&gt;
+        // - `withRetriesWithoutInterrupt` will reset the consumer for each attempt but a fresh&lt;br/&gt;
+        //    consumer has a much bigger chance to hit &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7703&quot; title=&quot;KafkaConsumer.position may return a wrong offset after &amp;quot;seekToEnd&amp;quot; is called&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7703&quot;&gt;&lt;del&gt;KAFKA-7703&lt;/del&gt;&lt;/a&gt;.&lt;br/&gt;
+        // - Avoid calling `consumer.poll(0)` which may cause &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7703&quot; title=&quot;KafkaConsumer.position may return a wrong offset after &amp;quot;seekToEnd&amp;quot; is called&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7703&quot;&gt;&lt;del&gt;KAFKA-7703&lt;/del&gt;&lt;/a&gt;.&lt;br/&gt;
+        var incorrectOffsets: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;(TopicPartition, Long, Long)&amp;#93;&lt;/span&gt; = Nil&lt;br/&gt;
+        var attempt = 0&lt;br/&gt;
+        do {&lt;br/&gt;
+          consumer.seekToEnd(partitions)&lt;br/&gt;
+          partitionOffsets = partitions.asScala.map(p =&amp;gt; p -&amp;gt; consumer.position(p)).toMap&lt;br/&gt;
+          attempt += 1&lt;br/&gt;
+&lt;br/&gt;
+          incorrectOffsets = findIncorrectOffsets()&lt;br/&gt;
+          if (incorrectOffsets.nonEmpty) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            logWarning(&amp;quot;Found incorrect offsets in some partitions &amp;quot; ++              s&amp;quot;(partition, previous offset, fetched offset)}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+        } while (incorrectOffsets.nonEmpty &amp;amp;&amp;amp; attempt &amp;lt; maxOffsetFetchAttempts)&lt;br/&gt;
+&lt;br/&gt;
+        logDebug(s&quot;Got latest offsets for partition : $partitionOffsets&quot;)&lt;br/&gt;
+        partitionOffsets&lt;br/&gt;
+      }&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaSource.scala b/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaSource.scala&lt;br/&gt;
index 66ec7e0cd084a..d65b3cea632c4 100644&lt;br/&gt;
&amp;#8212; a/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaSource.scala&lt;br/&gt;
+++ b/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaSource.scala&lt;br/&gt;
@@ -130,7 +130,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka010&amp;#93;&lt;/span&gt; class KafkaSource(&lt;br/&gt;
     metadataLog.get(0).getOrElse {&lt;br/&gt;
       val offsets = startingOffsets match &lt;/p&gt;
{
         case EarliestOffsetRangeLimit =&amp;gt; KafkaSourceOffset(kafkaReader.fetchEarliestOffsets())
-        case LatestOffsetRangeLimit =&amp;gt; KafkaSourceOffset(kafkaReader.fetchLatestOffsets())
+        case LatestOffsetRangeLimit =&amp;gt; KafkaSourceOffset(kafkaReader.fetchLatestOffsets(None))
         case SpecificOffsetRangeLimit(p) =&amp;gt; kafkaReader.fetchSpecificOffsets(p, reportDataLoss)
       }
&lt;p&gt;       metadataLog.add(0, offsets)&lt;br/&gt;
@@ -148,7 +148,8 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka010&amp;#93;&lt;/span&gt; class KafkaSource(&lt;br/&gt;
     // Make sure initialPartitionOffsets is initialized&lt;br/&gt;
     initialPartitionOffsets&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val latest = kafkaReader.fetchLatestOffsets()&lt;br/&gt;
+    val latest = kafkaReader.fetchLatestOffsets(&lt;br/&gt;
+      currentPartitionOffsets.orElse(Some(initialPartitionOffsets)))&lt;br/&gt;
     val offsets = maxOffsetsPerTrigger match {&lt;br/&gt;
       case None =&amp;gt;&lt;br/&gt;
         latest&lt;br/&gt;
diff --git a/external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchSourceSuite.scala b/external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchSourceSuite.scala&lt;br/&gt;
index 5ee76990b54f4..61cbb3285a4f0 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchSourceSuite.scala&lt;br/&gt;
+++ b/external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchSourceSuite.scala&lt;br/&gt;
@@ -329,6 +329,54 @@ abstract class KafkaMicroBatchSourceSuiteBase extends KafkaSourceSuiteBase 
{
     )
   }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+  test(&quot;subscribe topic by pattern with topic recreation between batches&quot;) {&lt;br/&gt;
+    val topicPrefix = newTopic()&lt;br/&gt;
+    val topic = topicPrefix + &quot;-good&quot;&lt;br/&gt;
+    val topic2 = topicPrefix + &quot;-bad&quot;&lt;br/&gt;
+    testUtils.createTopic(topic, partitions = 1)&lt;br/&gt;
+    testUtils.sendMessages(topic, Array(&quot;1&quot;, &quot;3&quot;))&lt;br/&gt;
+    testUtils.createTopic(topic2, partitions = 1)&lt;br/&gt;
+    testUtils.sendMessages(topic2, Array(&quot;2&quot;, &quot;4&quot;))&lt;br/&gt;
+&lt;br/&gt;
+    val reader = spark&lt;br/&gt;
+      .readStream&lt;br/&gt;
+      .format(&quot;kafka&quot;)&lt;br/&gt;
+      .option(&quot;kafka.bootstrap.servers&quot;, testUtils.brokerAddress)&lt;br/&gt;
+      .option(&quot;kafka.metadata.max.age.ms&quot;, &quot;1&quot;)&lt;br/&gt;
+      .option(&quot;kafka.default.api.timeout.ms&quot;, &quot;3000&quot;)&lt;br/&gt;
+      .option(&quot;startingOffsets&quot;, &quot;earliest&quot;)&lt;br/&gt;
+      .option(&quot;subscribePattern&quot;, s&quot;$topicPrefix-.*&quot;)&lt;br/&gt;
+&lt;br/&gt;
+    val ds = reader.load()&lt;br/&gt;
+      .selectExpr(&quot;CAST(key AS STRING)&quot;, &quot;CAST(value AS STRING)&quot;)&lt;br/&gt;
+      .as&lt;span class=&quot;error&quot;&gt;&amp;#91;(String, String)&amp;#93;&lt;/span&gt;&lt;br/&gt;
+      .map(kv =&amp;gt; kv._2.toInt)&lt;br/&gt;
+&lt;br/&gt;
+    testStream(ds)(&lt;br/&gt;
+      StartStream(),&lt;br/&gt;
+      AssertOnQuery &lt;/p&gt;
{ q =&amp;gt;
+        q.processAllAvailable()
+        true
+      }
&lt;p&gt;,&lt;br/&gt;
+      CheckAnswer(1, 2, 3, 4),&lt;br/&gt;
+      // Restart the stream in this test to make the test stable. When recreating a topic when a&lt;br/&gt;
+      // consumer is alive, it may not be able to see the recreated topic even if a fresh consumer&lt;br/&gt;
+      // has seen it.&lt;br/&gt;
+      StopStream,&lt;br/&gt;
+      // Recreate `topic2` and wait until it&apos;s available&lt;br/&gt;
+      WithOffsetSync(new TopicPartition(topic2, 0), expectedOffset = 1) &lt;/p&gt;
{ () =&amp;gt;
+        testUtils.deleteTopic(topic2)
+        testUtils.createTopic(topic2)
+        testUtils.sendMessages(topic2, Array(&quot;6&quot;))
+      }
&lt;p&gt;,&lt;br/&gt;
+      StartStream(),&lt;br/&gt;
+      ExpectFailure&lt;span class=&quot;error&quot;&gt;&amp;#91;IllegalStateException&amp;#93;&lt;/span&gt;(e =&amp;gt; &lt;/p&gt;
{
+        // The offset of `topic2` should be changed from 2 to 1
+        assert(e.getMessage.contains(&quot;was changed from 2 to 1&quot;))
+      }
&lt;p&gt;)&lt;br/&gt;
+    )&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
   test(&quot;ensure that initial offset are written with an extra byte in the beginning (&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-19517&quot; title=&quot;KafkaSource fails to initialize partition offsets&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-19517&quot;&gt;&lt;del&gt;SPARK-19517&lt;/del&gt;&lt;/a&gt;)&quot;) {&lt;br/&gt;
     withTempDir { metadataPath =&amp;gt;&lt;br/&gt;
       val topic = &quot;kafka-initial-offset-current&quot;&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16726973" author="githubbot" created="Fri, 21 Dec 2018 18:53:41 +0000"  >&lt;p&gt;zsxwing opened a new pull request #23365: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26267&quot; title=&quot;Kafka source may reprocess data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-26267&quot;&gt;&lt;del&gt;SPARK-26267&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;SS&amp;#93;&lt;/span&gt; Retry when detecting incorrect offsets from Kafka (2.4)&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/23365&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/23365&lt;/a&gt;&lt;/p&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;What changes were proposed in this pull request?&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;   Backport #23324 to branch-2.4.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;How was this patch tested?&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;   Jenkins&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="13249303">SPARK-28641</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="12310560">
                    <name>Problem/Incident</name>
                                                                <inwardlinks description="is caused by">
                                        <issuelink>
            <issuekey id="13202134">KAFKA-7703</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13241783">SPARK-28174</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 47 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|s015z4:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>