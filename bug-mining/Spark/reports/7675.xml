<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 19:23:22 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-33635] Performance regression in Kafka read</title>
                <link>https://issues.apache.org/jira/browse/SPARK-33635</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;I have observed a slowdown in the reading of data from kafka on all of our systems when migrating from spark 2.4.5 to Spark 3.0.0 (and Spark 3.0.1)&lt;/p&gt;

&lt;p&gt;I have created a sample project to isolate the problem as much as possible, with just a read all data from a kafka topic (see &lt;a href=&quot;https://github.com/codegorillauk/spark-kafka-read&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/codegorillauk/spark-kafka-read&lt;/a&gt;&#160;).&lt;/p&gt;

&lt;p&gt;With 2.4.5, across multiple runs, &lt;br/&gt;
 I get a stable read rate of 1,120,000 (1.12 mill) rows per second&lt;/p&gt;

&lt;p&gt;With 3.0.0 or 3.0.1, across multiple runs,&lt;br/&gt;
 I get a stable read rate of 632,000 (0.632 mil) rows per second&lt;/p&gt;

&lt;p&gt;The represents a &lt;b&gt;44% loss in performance&lt;/b&gt;. Which is, a lot.&lt;/p&gt;

&lt;p&gt;I have been working though the spark-sql-kafka-0-10 code base, but change for spark 3 have been ongoing for over a year and its difficult to pin point an exact change or reason for the degradation.&lt;/p&gt;

&lt;p&gt;I am happy to help fix this problem, but will need some assitance as I am unfamiliar with the&#160;spark-sql-kafka-0-10 project.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;A sample of the data my test reads (note: its not parsing csv - this is just test data)&lt;br/&gt;
 1606921800000,001e0610e532,lightsense,tsl250rd,intensity,21853,53.262,acceleration_z,651,ep,290,commit,913,pressure,138,pm1,799,uv_intensity,823,idletime,-372,count,-72,ir_intensity,185,concentration,-61,flags,-532,tx,694.36,ep_heatsink,-556.92,acceleration_x,-221.40,fw,910.53,sample_flow_rate,-959.60,uptime,-515.15,pm10,-768.03,powersupply,214.72,magnetic_field_y,-616.04,alphasense,606.73,AoT_Chicago,053,Racine Ave &amp;amp; 18th St Chicago IL,41.857959,-87.65642700000002,AoT Chicago (S) &lt;span class=&quot;error&quot;&gt;&amp;#91;C&amp;#93;&lt;/span&gt;,2017/12/15 00:00:00,&lt;/p&gt;</description>
                <environment>&lt;p&gt;A simple 5 node system. A simple data row of csv data in kafka, evenly distributed between the partitions.&lt;/p&gt;

&lt;p&gt;Open JDK 1.8.0.252&lt;/p&gt;

&lt;p&gt;Spark in stand alone - 5 nodes, 10 workers (2 worker per node, each locked to a distinct NUMA group)&lt;br/&gt;
kafka (v 2.3.1) cluster - 5 nodes (1 broker per node).&lt;br/&gt;
Centos 7.7.1908&lt;/p&gt;

&lt;p&gt;1 topic, 10 partiions, 1 hour queue life&lt;/p&gt;

&lt;p&gt;(this is just one of clusters we have, I have tested on all of them and theyall exhibit the same performance degredation)&lt;/p&gt;</environment>
        <key id="13343823">SPARK-33635</key>
            <summary>Performance regression in Kafka read</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="kabhwan">Jungtaek Lim</assignee>
                                    <reporter username="david.wyles">David Wyles</reporter>
                        <labels>
                    </labels>
                <created>Wed, 2 Dec 2020 16:54:15 +0000</created>
                <updated>Thu, 7 Jan 2021 03:13:25 +0000</updated>
                            <resolved>Wed, 6 Jan 2021 06:00:43 +0000</resolved>
                                    <version>3.0.0</version>
                    <version>3.0.1</version>
                                    <fixVersion>3.0.2</fixVersion>
                    <fixVersion>3.1.1</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>7</watches>
                                                                                                                <comments>
                            <comment id="17245277" author="srowen" created="Mon, 7 Dec 2020 15:32:51 +0000"  >&lt;p&gt;I don&apos;t think this is actionable by others unless you can help narrow down why this changed, and why it&apos;s attributable to Spark.&lt;/p&gt;</comment>
                            <comment id="17245304" author="david.wyles" created="Mon, 7 Dec 2020 15:52:38 +0000"  >&lt;p&gt;Fair point, the library I suspect is&#160;spark-sql-kafka-0-10, is that covered on these issues.&lt;/p&gt;</comment>
                            <comment id="17245306" author="david.wyles" created="Mon, 7 Dec 2020 16:00:24 +0000"  >&lt;p&gt;Is anyone even able to confirm my results are not just unique to me?&lt;/p&gt;</comment>
                            <comment id="17245329" author="david.wyles" created="Mon, 7 Dec 2020 16:30:27 +0000"  >&lt;p&gt;Apart from all the cached consumer changes and other things on kafka-010-sql there was the kafka client change, it went from 2.0.0 to 2.4.1.&lt;/p&gt;

&lt;p&gt;This I can test, using just kafka libraries. I&apos;ll give it a go and see what the outcome is.&lt;/p&gt;</comment>
                            <comment id="17246430" author="david.wyles" created="Wed, 9 Dec 2020 10:33:53 +0000"  >&lt;p&gt;Having performed my tests I can conclude that the kafka client versions behave reasonable identically.&lt;/p&gt;

&lt;p&gt;I ran my test on a local partition, a single thread reading just one partition of the same data.&lt;/p&gt;

&lt;p&gt;But the curious thing was that It would max out around 660,000 rows per second which is much more in line with the row rate provided by Spark 3.0.0/1 (of 632,000 per second)&lt;/p&gt;

&lt;p&gt;So that leads to believe that 2.4.5 was not single threaded (per partition), as the time and numbers I measured before were from the driver - so they are correct.&lt;/p&gt;

&lt;p&gt;Not once on any of the reads directly from kafka on this test system did I get anywhere near the 1.1 mil rows/second.&lt;/p&gt;

&lt;p&gt;I no longer believe this is a true regression in performance, I now think that 2.4.5 was &quot;cheating&quot;.&lt;/p&gt;

&lt;p&gt;Should you wish to close this as not a bug, then I&apos;m happy for that to happen - but I would like your thoughts on how 2.4.5 was cheating.&lt;/p&gt;</comment>
                            <comment id="17246625" author="gsomogyi" created="Wed, 9 Dec 2020 15:58:13 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=david.wyles&quot; class=&quot;user-hover&quot; rel=&quot;david.wyles&quot;&gt;david.wyles&lt;/a&gt; try to turn off Kafka consumer caching. Apart from that there were no super significant changes which could cause this.&lt;/p&gt;

&lt;p&gt;I&apos;ve taken a look at your application and it does groupby and stuff like that. This is not related to Kafka read performance since Spark SQL engine contains huge amount of changes.&lt;br/&gt;
I suggest to create an application which just moves simple data from one topic into another and please use the exact same broker version.&lt;br/&gt;
If it&apos;s still slow we can measure further things.&lt;/p&gt;</comment>
                            <comment id="17246631" author="gsomogyi" created="Wed, 9 Dec 2020 16:00:44 +0000"  >&lt;p&gt;BTW, I&apos;m sure you know but using collect gathers all the data on the driver side which is not really suggested under any circumstances.&lt;/p&gt;</comment>
                            <comment id="17246633" author="gsomogyi" created="Wed, 9 Dec 2020 16:03:39 +0000"  >&lt;p&gt;Since you&apos;re measuring speed I&apos;ve ported the Kafka source from DSv1 to DSv2. DSv1 is the default but the DSv2 can be tried out by setting &quot;spark.sql.sources.useV1SourceList&quot; properly. If you can try it out I would appreciate it.&lt;/p&gt;</comment>
                            <comment id="17246640" author="gsomogyi" created="Wed, 9 Dec 2020 16:11:14 +0000"  >&lt;p&gt;I&apos;ve changed to SQL because you&apos;re not executing a Structured Streaming query but an SQL batch.&lt;/p&gt;</comment>
                            <comment id="17246643" author="gsomogyi" created="Wed, 9 Dec 2020 16:16:24 +0000"  >&lt;blockquote&gt;&lt;p&gt;I no longer believe this is a true regression in performance, I now think that 2.4.5 was &quot;cheating&quot;.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;If you mean by cheating Spark uses one consumer from multiple threads then the answer is no. Kafka consumer is strictly forbidden to use from multiple threads.&lt;br/&gt;
 If such thing happens then Kafka realizes it and exception will be thrown which will stop the query immediately.&lt;/p&gt;</comment>
                            <comment id="17247875" author="david.wyles" created="Fri, 11 Dec 2020 12:10:13 +0000"  >&lt;p&gt;I&apos;ll give all those a go and get back to you.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;The collect in this test case is only 13 items of data after the group by - so I know thats not going to impact it.&lt;/p&gt;

&lt;p&gt;But I can modify it to just read and write to kafka.&lt;/p&gt;</comment>
                            <comment id="17247878" author="david.wyles" created="Fri, 11 Dec 2020 12:13:29 +0000"  >&lt;p&gt;&quot;Since you&apos;re measuring speed I&apos;ve ported the Kafka source from DSv1 to DSv2. DSv1 is the default but the DSv2 can be tried out by setting &quot;spark.sql.sources.useV1SourceList&quot; properly. If you can try it out I would appreciate it.&quot;&lt;/p&gt;

&lt;p&gt;Is that availble already in the 3.0.1 build, or do I need to pull and build it myself?&lt;/p&gt;</comment>
                            <comment id="17247879" author="david.wyles" created="Fri, 11 Dec 2020 12:14:28 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gsomogyi&quot; class=&quot;user-hover&quot; rel=&quot;gsomogyi&quot;&gt;gsomogyi&lt;/a&gt;&#160;&lt;br/&gt;
&quot;try to turn off Kafka consumer caching. Apart from that there were no super significant changes which could cause this.&quot;&lt;/p&gt;

&lt;p&gt;Whats the option for that?&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17248549" author="gsomogyi" created="Sun, 13 Dec 2020 10:08:34 +0000"  >&lt;p&gt;Mixed up with DStreams, in Strutured Streaming and SQL there is no turn off flag.&lt;/p&gt;</comment>
                            <comment id="17248550" author="gsomogyi" created="Sun, 13 Dec 2020 10:13:34 +0000"  >&lt;blockquote&gt;&lt;p&gt;The collect in this test case is only 13 items of data after the group by - so I know thats not going to impact it.&lt;br/&gt;
 But I can modify it to just read and write to kafka.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yeah, we need to reduce the use-case to the most minimal app to measure only what we need. Aggregations and all those stuff don&apos;t belong to Kafka read and write performance.&lt;/p&gt;</comment>
                            <comment id="17251911" author="david.wyles" created="Fri, 18 Dec 2020 17:34:10 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gsomogyi&quot; class=&quot;user-hover&quot; rel=&quot;gsomogyi&quot;&gt;gsomogyi&lt;/a&gt;&#160;Just so you know, I&apos;m still doing this.&lt;/p&gt;

&lt;p&gt;I&apos;ve simplied the test case to kafka to kafka, also tried with v2 sources (by removing kafka from the v1 sources list).&lt;/p&gt;

&lt;p&gt;On 3.0.1 My data rate is still on the order of 650 - 700k rows (10 partitions, 700 million rows, 1 core per executors - of which there are 10), there is no noticable change when using v1 or v2 sources (v2 may have been every so slightly faster, but I&apos;d need a lot more test runs to make that a concrete fact, faster by maybe 4%).&lt;/p&gt;

&lt;p&gt;I&apos;ll get back to you next week when I&apos;ve run this on 2.4.5&lt;/p&gt;

&lt;p&gt;Remember, based on all my testing, and raw kafka reads on my system - the 3.0.1 spark is performing in line with expectations. The odd one here is 2.4.5, and maybe when I run it with these test setup we will get numbers more in line with kafka consumer behaviour.&lt;/p&gt;</comment>
                            <comment id="17252153" author="gsomogyi" created="Sat, 19 Dec 2020 10:20:34 +0000"  >&lt;blockquote&gt;&lt;p&gt;Remember, based on all my testing, and raw kafka reads on my system - the 3.0.1 spark is performing in line with expectations.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Good to hear. You don&apos;t have to hurry since I&apos;m on vacation this year unless a breaking issue appears in the upcoming Spark release.&lt;/p&gt;</comment>
                            <comment id="17256046" author="david.wyles" created="Tue, 29 Dec 2020 16:26:52 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gsomogyi&quot; class=&quot;user-hover&quot; rel=&quot;gsomogyi&quot;&gt;gsomogyi&lt;/a&gt;&#160;I now have my results.&lt;br/&gt;
 I was so unhappy about these results I ran all the tests again, the only thing that changed between them is the version of spark running on the cluster, everything else was static - the data input from kafka was an unchanging static set of data.&lt;/p&gt;

&lt;p&gt;Input-&amp;gt;&#160;&lt;b&gt;672733262&lt;/b&gt; rows&lt;/p&gt;

&lt;p&gt;&lt;ins&gt;&lt;b&gt;Spark 2.4.5&lt;/b&gt;:&lt;/ins&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;440&lt;/b&gt; seconds - &lt;b&gt;1,528,939&lt;/b&gt; rows per second.&lt;/p&gt;

&lt;p&gt;&lt;ins&gt;&lt;b&gt;Spark 3.0.1&lt;/b&gt;:&lt;/ins&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;990&lt;/b&gt; seconds - &lt;b&gt;679,528&lt;/b&gt; rows per seconds.&lt;/p&gt;

&lt;p&gt;These are multiple runs (I even took the best from spark 3.0.1)&lt;/p&gt;

&lt;p&gt;I also captured the event logs between these two versions of spark - should anyone find them useful.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://drive.google.com/drive/folders/1aElmzVWmJqRALQimdOYxdJu559_3EX_9?usp=sharing&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;event logs&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So, no matter what I do, I can only conclude that Spark 2.4.5 was a lot faster in this test case.&lt;/p&gt;

&lt;p&gt;Is Spark SQL reading the source data twice, just as it would if there was a &quot;order by&quot; in the query?&lt;/p&gt;

&lt;p&gt;Sample code used:&lt;/p&gt;

&lt;p&gt;val spark =&lt;br/&gt;
 &#160; SparkSession.builder.appName(&quot;Kafka Read Performance&quot;)&lt;br/&gt;
 &#160; &#160; .config(&quot;spark.executor.memory&quot;,&quot;16g&quot;)&lt;br/&gt;
 &#160; &#160; .config(&quot;spark.cores.max&quot;, &quot;10&quot;)&lt;br/&gt;
 &#160; &#160; .config(&quot;spark.eventLog.enabled&quot;,&quot;true&quot;)&lt;br/&gt;
 &#160; &#160; .config(&quot;spark.eventLog.dir&quot;,&quot;file:///tmp/spark-events&quot;)&lt;br/&gt;
 &#160; &#160; .config(&quot;spark.eventLog.overwrite&quot;,&quot;true&quot;)&lt;br/&gt;
 &#160; &#160;.getOrCreate()&lt;/p&gt;

&lt;p&gt;import spark.implicits._&lt;/p&gt;

&lt;p&gt;val &lt;b&gt;startTime&lt;/b&gt; = System.nanoTime()&lt;/p&gt;

&lt;p&gt;val df = &lt;br/&gt;
 &#160; spark&lt;br/&gt;
 &#160; &#160; .read&lt;br/&gt;
 &#160; &#160; .format(&quot;kafka&quot;)&lt;br/&gt;
 &#160; &#160; .option(&quot;kafka.bootstrap.servers&quot;, config.brokers)&lt;br/&gt;
 &#160; &#160; .option(&quot;subscribe&quot;, config.inTopic)&lt;br/&gt;
 &#160; &#160; .option(&quot;startingOffsets&quot;, &quot;earliest&quot;)&lt;br/&gt;
 &#160; &#160; .option(&quot;endingOffsets&quot;, &quot;latest&quot;)&lt;br/&gt;
 &#160; &#160; .option(&quot;failOnDataLoss&quot;,&quot;false&quot;)&lt;br/&gt;
 &#160; &#160; .load()&lt;/p&gt;

&lt;p&gt;df&lt;br/&gt;
 &#160; .write&lt;br/&gt;
 &#160; .format(&quot;kafka&quot;)&lt;br/&gt;
 &#160; .option(&quot;kafka.bootstrap.servers&quot;, config.brokers)&lt;br/&gt;
 &#160; .option(&quot;topic&quot;, config.outTopic)&lt;br/&gt;
 &#160; .mode(SaveMode.Append)&lt;br/&gt;
 &#160; .save()&lt;/p&gt;

&lt;p&gt;val &lt;b&gt;endTime&lt;/b&gt; = System.nanoTime()&lt;/p&gt;

&lt;p&gt;val elapsedSecs = (endTime - startTime) / 1E9&lt;/p&gt;

&lt;p&gt;// static input sample was used, fixed row count.&lt;/p&gt;

&lt;p&gt;println(s&quot;Took $elapsedSecs secs&quot;)&lt;br/&gt;
 spark.stop()&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17257962" author="yukihito" created="Mon, 4 Jan 2021 05:09:45 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=david.wyles&quot; class=&quot;user-hover&quot; rel=&quot;david.wyles&quot;&gt;david.wyles&lt;/a&gt;, I tried your sample code in my local dev environment.&#160;&lt;/p&gt;

&lt;p&gt;Kafka: 2.7.0&lt;/p&gt;

&lt;p&gt;both the input topic and the output topic have a single partition&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;There are fewer messages in the test topics. However, the read speed of Spark3 is still noticeably slower. In my case, Spark2.4 took around 7s while Spark3.1 took 11s. I have tested a few times, the results are quite consistent.&#160;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17259399" author="kabhwan" created="Wed, 6 Jan 2021 04:17:28 +0000"  >&lt;p&gt;I&apos;ve spent some time to trace the issue, and noticed &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-29054&quot; title=&quot;Invalidate Kafka consumer when new delegation token available&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-29054&quot;&gt;&lt;del&gt;SPARK-29054&lt;/del&gt;&lt;/a&gt; (+&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-30495&quot; title=&quot;How to disable &amp;#39;spark.security.credentials.${service}.enabled&amp;#39; in Structured streaming while connecting to a kafka cluster&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-30495&quot;&gt;&lt;del&gt;SPARK-30495&lt;/del&gt;&lt;/a&gt;) caused performance regression (though the patch itself is doing the right thing).&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt;[kafka010] def getOrRetrieveConsumer(): InternalKafkaConsumer = {
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!_consumer.isDefined) {
      retrieveConsumer()
    }
    require(_consumer.isDefined, &lt;span class=&quot;code-quote&quot;&gt;&quot;Consumer must be defined&quot;&lt;/span&gt;)
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (KafkaTokenUtil.needTokenUpdate(SparkEnv.get.conf, _consumer.get.kafkaParamsWithSecurity,
        _consumer.get.clusterConfig)) {
      logDebug(&lt;span class=&quot;code-quote&quot;&gt;&quot;Cached consumer uses an old delegation token, invalidating.&quot;&lt;/span&gt;)
      releaseConsumer()
      consumerPool.invalidateKey(cacheKey)
      fetchedDataPool.invalidate(cacheKey)
      retrieveConsumer()
    }
    _consumer.get
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  def needTokenUpdate(
      sparkConf: SparkConf,
      params: ju.Map[&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;, &lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;],
      clusterConfig: Option[KafkaTokenClusterConf]): &lt;span class=&quot;code-object&quot;&gt;Boolean&lt;/span&gt; = {
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (HadoopDelegationTokenManager.isServiceEnabled(sparkConf, &lt;span class=&quot;code-quote&quot;&gt;&quot;kafka&quot;&lt;/span&gt;) &amp;amp;&amp;amp;
        clusterConfig.isDefined &amp;amp;&amp;amp; params.containsKey(SaslConfigs.SASL_JAAS_CONFIG)) {
      logDebug(&lt;span class=&quot;code-quote&quot;&gt;&quot;Delegation token used by connector, checking &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; uses the latest token.&quot;&lt;/span&gt;)
      val connectorJaasParams = params.get(SaslConfigs.SASL_JAAS_CONFIG).asInstanceOf[&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;]
      getTokenJaasParams(clusterConfig.get) != connectorJaasParams
    } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
      &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;
    }
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  def isServiceEnabled(sparkConf: SparkConf, serviceName: &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;): &lt;span class=&quot;code-object&quot;&gt;Boolean&lt;/span&gt; = {
    val key = providerEnabledConfig.format(serviceName)

    deprecatedProviderEnabledConfigs.foreach { pattern =&amp;gt;
      val deprecatedKey = pattern.format(serviceName)
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (sparkConf.contains(deprecatedKey)) {
        logWarning(s&lt;span class=&quot;code-quote&quot;&gt;&quot;${deprecatedKey} is deprecated.  Please use ${key} instead.&quot;&lt;/span&gt;)
      }
    }

    val isEnabledDeprecated = deprecatedProviderEnabledConfigs.forall { pattern =&amp;gt;
      sparkConf
        .getOption(pattern.format(serviceName))
        .map(_.toBoolean)
        .getOrElse(&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)
    }

    sparkConf
      .getOption(key)
      .map(_.toBoolean)
      .getOrElse(isEnabledDeprecated)
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With my test data and default config, Spark pulled 500 records per a poll from Kafka, which ended up &quot;10,280,000&quot; calls to get() which always calls getOrRetrieveConsumer(). A single call of KafkaTokenUtil.needTokenUpdate() wouldn&apos;t add significant overhead, but 10,000,000 calls make a significant difference. Assuming the case where delegation token is not applied, HadoopDelegationTokenManager.isServiceEnabled is the culprit on such huge overhead.&lt;/p&gt;

&lt;p&gt;We could probably resolve the issue via short-term solution &amp;amp; long-term solution.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;short-term solution: change the order of check in needTokenUpdate, so that the performance hit is only affected when using delegation token. I&apos;ll raise a PR shortly.&lt;/li&gt;
	&lt;li&gt;long-term solution(s): 1) optimize HadoopDelegationTokenManager.isServiceEnabled 2) find a way to reduce the occurrence of checking necessarily of token update.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Note that even with short-term solution, a slight performance hit is observed as it still does more things on the code path compared to Spark 2.4. Though I&apos;d ignore it if it affects slightly, like less than 1%, or even slightly higher but the code addition is mandatory.&lt;/p&gt;</comment>
                            <comment id="17259407" author="apachespark" created="Wed, 6 Jan 2021 04:40:19 +0000"  >&lt;p&gt;User &apos;HeartSaVioR&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/31056&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/31056&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17259436" author="dongjoon" created="Wed, 6 Jan 2021 06:00:43 +0000"  >&lt;p&gt;Issue resolved by pull request 31056&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/31056&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/31056&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17259446" author="kabhwan" created="Wed, 6 Jan 2021 06:29:57 +0000"  >&lt;p&gt;One more point, though the root cause is actually the changes on &quot;Kafka&quot; - you&apos;ll find the huge difference according to the size of the log file between Spark 2.4 vs 3.0.&lt;/p&gt;

&lt;p&gt;Some &quot;debug&quot; log messages in Kafka 2.0 (which Spark 2.4 uses) were re-labeled to the &quot;info&quot; log messages in later version of Kafka (at least including Kafka 2.4 which Spark 3.0 uses). I found the changes in KafkaConsumer.seek(), and there could be more.&lt;/p&gt;

&lt;p&gt;If you feel these messages are flooding and likely affecting the performance (whereas it would be unlikely), you can change your log4j configuration to suppress it.&lt;/p&gt;

&lt;p&gt;log4j.logger.org.apache.kafka.clients.consumer.KafkaConsumer=WARN&lt;/p&gt;</comment>
                            <comment id="17259801" author="david.wyles" created="Wed, 6 Jan 2021 15:23:15 +0000"  >&lt;p&gt;Thanks a lot for getting down to the route cause and providing a fix so quickly so it will be in the next Spark releases.&lt;/p&gt;

&lt;p&gt;Excellent.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            4 years, 44 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z0l4yo:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12348739">3.0.2</customfieldvalue>
    <customfieldvalue id="12346518">3.1.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>