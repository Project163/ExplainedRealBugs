<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:20:47 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-732] Recomputation of RDDs may result in duplicated accumulator updates</title>
                <link>https://issues.apache.org/jira/browse/SPARK-732</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Currently, Spark doesn&apos;t guard against duplicated updates to the same accumulator due to recomputations of an RDD.  For example:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    val acc = sc.accumulator(0)
    data.map(x =&amp;gt; acc += 1; f(x))
    data.count()
    &lt;span class=&quot;code-comment&quot;&gt;// acc should equal data.count() here
&lt;/span&gt;    data.foreach{...}
    &lt;span class=&quot;code-comment&quot;&gt;// Now, acc = 2 * data.count() because the map() was recomputed.&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I think that this behavior is incorrect, especially because this behavior allows the additon or removal of a cache() call to affect the outcome of a computation.&lt;/p&gt;

&lt;p&gt;There&apos;s an old TODO to fix this duplicate update issue in the &lt;a href=&quot;https://github.com/mesos/spark/blob/ec5e553b418be43aa3f0ccc24e0d5ca9d63504b2/core/src/main/scala/spark/scheduler/DAGScheduler.scala#L494&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;DAGScheduler code&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I haven&apos;t tested whether recomputation due to blocks being dropped from the cache can trigger duplicate accumulator updates.&lt;/p&gt;

&lt;p&gt;Hypothetically someone could be relying on the current behavior to implement performance counters that track the actual number of computations performed (including recomputations).  To be safe, we could add an explicit warning in the release notes that documents the change in behavior when we fix this.&lt;/p&gt;

&lt;p&gt;Ignoring duplicate updates shouldn&apos;t be too hard, but there are a few subtleties.  Currently, we allow accumulators to be used in multiple transformations, so we&apos;d need to detect duplicate updates at the per-transformation level.  I haven&apos;t dug too deeply into the scheduler internals, but we might also run into problems where pipelining causes what is logically one set of accumulator updates to show up in two different tasks (e.g. rdd.map(accum += x; ...) and rdd.map(accum += x; ...).count() may cause what&apos;s logically the same accumulator update to be applied from two different contexts, complicating the detection of duplicate updates).&lt;/p&gt;</description>
                <environment></environment>
        <key id="12704822">SPARK-732</key>
            <summary>Recomputation of RDDs may result in duplicated accumulator updates</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="2">Won&apos;t Fix</resolution>
                                        <assignee username="codingcat">Nan Zhu</assignee>
                                    <reporter username="joshrosen">Josh Rosen</reporter>
                        <labels>
                    </labels>
                <created>Sun, 14 Apr 2013 22:50:16 +0000</created>
                <updated>Wed, 24 Feb 2016 22:23:34 +0000</updated>
                            <resolved>Thu, 27 Nov 2014 01:32:59 +0000</resolved>
                                    <version>0.6.2</version>
                    <version>0.7.0</version>
                    <version>0.7.1</version>
                    <version>0.7.2</version>
                    <version>0.7.3</version>
                    <version>0.8.0</version>
                    <version>0.8.1</version>
                    <version>0.8.2</version>
                    <version>0.9.0</version>
                    <version>1.0.1</version>
                    <version>1.1.0</version>
                                                    <component>Spark Core</component>
                        <due></due>
                            <votes>2</votes>
                                    <watches>16</watches>
                                                                                                                <comments>
                            <comment id="13953849" author="joshrosen" created="Tue, 25 Feb 2014 21:58:33 +0000"  >&lt;p&gt;We might want to revisit this before the 1.0 release, since I think the current accumulator behavior is counter-intuitive.&lt;/p&gt;</comment>
                            <comment id="13954137" author="codingcat" created="Thu, 20 Mar 2014 17:28:13 +0000"  >&lt;p&gt;I&apos;m interested in fixing this, I plan to start with the scheduler, i.e. detect the duplication when get the task result&lt;/p&gt;</comment>
                            <comment id="13954162" author="codingcat" created="Mon, 24 Mar 2014 07:39:30 +0000"  >&lt;p&gt;I finished the deduplication for resubmitted tasks but for transformation, any hint?&lt;/p&gt;</comment>
                            <comment id="13954178" author="codingcat" created="Tue, 25 Mar 2014 12:04:21 +0000"  >&lt;p&gt;made a PR: &lt;a href=&quot;https://github.com/apache/spark/pull/228&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/228&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13954274" author="githubbot" created="Sat, 29 Mar 2014 14:57:29 +0000"  >&lt;p&gt;Github user AmplabJenkins commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/spark/pull/228#issuecomment-38997862&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/228#issuecomment-38997862&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;     Merged build triggered. Build is starting &lt;del&gt;or&lt;/del&gt; tests failed to complete.&lt;/p&gt;</comment>
                            <comment id="13954275" author="githubbot" created="Sat, 29 Mar 2014 14:57:41 +0000"  >&lt;p&gt;Github user AmplabJenkins commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/spark/pull/228#issuecomment-38997867&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/228#issuecomment-38997867&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Merged build started. Build is starting &lt;del&gt;or&lt;/del&gt; tests failed to complete.&lt;/p&gt;</comment>
                            <comment id="13954296" author="githubbot" created="Sat, 29 Mar 2014 15:42:25 +0000"  >&lt;p&gt;Github user AmplabJenkins commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/spark/pull/228#issuecomment-38999177&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/228#issuecomment-38999177&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;     Merged build triggered. Build is starting &lt;del&gt;or&lt;/del&gt; tests failed to complete.&lt;/p&gt;</comment>
                            <comment id="13954297" author="githubbot" created="Sat, 29 Mar 2014 15:42:33 +0000"  >&lt;p&gt;Github user AmplabJenkins commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/spark/pull/228#issuecomment-38999183&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/228#issuecomment-38999183&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Merged build started. Build is starting &lt;del&gt;or&lt;/del&gt; tests failed to complete.&lt;/p&gt;</comment>
                            <comment id="13954322" author="githubbot" created="Sat, 29 Mar 2014 16:28:37 +0000"  >&lt;p&gt;Github user AmplabJenkins commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/spark/pull/228#issuecomment-39000586&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/228#issuecomment-39000586&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Build is starting &lt;del&gt;or&lt;/del&gt; tests failed to complete.&lt;br/&gt;
    Refer to this link for build results: &lt;a href=&quot;https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/13573/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/13573/&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13954324" author="githubbot" created="Sat, 29 Mar 2014 16:28:37 +0000"  >&lt;p&gt;Github user AmplabJenkins commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/spark/pull/228#issuecomment-39000584&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/228#issuecomment-39000584&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Merged build finished. Build is starting &lt;del&gt;or&lt;/del&gt; tests failed to complete.&lt;/p&gt;</comment>
                            <comment id="13954344" author="githubbot" created="Sat, 29 Mar 2014 17:14:03 +0000"  >&lt;p&gt;Github user AmplabJenkins commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/spark/pull/228#issuecomment-39002055&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/228#issuecomment-39002055&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Build is starting &lt;del&gt;or&lt;/del&gt; tests failed to complete.&lt;br/&gt;
    Refer to this link for build results: &lt;a href=&quot;https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/13577/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/13577/&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13954345" author="githubbot" created="Sat, 29 Mar 2014 17:14:04 +0000"  >&lt;p&gt;Github user AmplabJenkins commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/spark/pull/228#issuecomment-39002053&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/228#issuecomment-39002053&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Merged build finished. Build is starting &lt;del&gt;or&lt;/del&gt; tests failed to complete.&lt;/p&gt;</comment>
                            <comment id="13954349" author="chris.a.mattmann@jpl.nasa.gov" created="Sat, 29 Mar 2014 17:23:15 +0000"  >&lt;p&gt;Guys I fixed this by adding jira@apache.org to the mailing list, no&lt;br/&gt;
more moderation required.&lt;/p&gt;

&lt;p&gt;Cheers,&lt;br/&gt;
Chris&lt;/p&gt;




</comment>
                            <comment id="13954353" author="githubbot" created="Sat, 29 Mar 2014 17:32:24 +0000"  >&lt;p&gt;Github user AmplabJenkins commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/spark/pull/228#issuecomment-39002548&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/228#issuecomment-39002548&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;     Merged build triggered. Build is starting &lt;del&gt;or&lt;/del&gt; tests failed to complete.&lt;/p&gt;</comment>
                            <comment id="13954354" author="githubbot" created="Sat, 29 Mar 2014 17:32:29 +0000"  >&lt;p&gt;Github user AmplabJenkins commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/spark/pull/228#issuecomment-39002551&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/228#issuecomment-39002551&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Merged build started. Build is starting &lt;del&gt;or&lt;/del&gt; tests failed to complete.&lt;/p&gt;</comment>
                            <comment id="13954362" author="githubbot" created="Sat, 29 Mar 2014 17:52:41 +0000"  >&lt;p&gt;Github user mridulm commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/spark/pull/228#discussion_r11094324&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/228#discussion_r11094324&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala &amp;#8212;&lt;br/&gt;
    @@ -116,6 +116,9 @@ class DAGScheduler(&lt;br/&gt;
       private val metadataCleaner =&lt;br/&gt;
         new MetadataCleaner(MetadataCleanerType.DAG_SCHEDULER, this.cleanup, env.conf)&lt;/p&gt;

&lt;p&gt;    +  // stageId -&amp;gt; (splitId -&amp;gt; (acumulatorId, accumulatorValue))&lt;br/&gt;
    +  val stageIdToAccumulators = new HashMap[Int, HashMap[Int, ListBuffer&lt;span class=&quot;error&quot;&gt;&amp;#91;(Long, Any)&amp;#93;&lt;/span&gt;]]&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Since this is mutable datastructure - can me make it private ?&lt;br/&gt;
    For testcase, expose some method which checks for the condition instead of needing to expose the entire stageIdToAccumulators ?&lt;/p&gt;</comment>
                            <comment id="13954363" author="githubbot" created="Sat, 29 Mar 2014 17:54:45 +0000"  >&lt;p&gt;Github user mridulm commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/spark/pull/228#discussion_r11094329&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/228#discussion_r11094329&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala &amp;#8212;&lt;br/&gt;
    @@ -789,6 +799,25 @@ class DAGScheduler(&lt;br/&gt;
       }&lt;/p&gt;

&lt;p&gt;       /**&lt;br/&gt;
    +   * detect the duplicate accumulator value and save the accumulator values&lt;br/&gt;
    +   * @param accumValue the accumulator values received from the task&lt;br/&gt;
    +   * @param stage the stage which the task belongs to&lt;br/&gt;
    +   * @param task the completed task&lt;br/&gt;
    +   */&lt;br/&gt;
    +  private def saveAccumulatorValue(accumValue: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;Long, Any&amp;#93;&lt;/span&gt;, stage: Stage, task: Task&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;) {&lt;br/&gt;
    +    if (accumValue != null &amp;amp;&amp;amp;&lt;br/&gt;
    +      (!stageIdToAccumulators.contains(stage.id) ||&lt;br/&gt;
    +        !stageIdToAccumulators(stage.id).contains(task.partitionId))) {&lt;br/&gt;
    +      stageIdToAccumulators.getOrElseUpdate(stage.id,&lt;br/&gt;
    +        new HashMap[Int, ListBuffer&lt;span class=&quot;error&quot;&gt;&amp;#91;(Long, Any)&amp;#93;&lt;/span&gt;]).&lt;br/&gt;
    +        getOrElseUpdate(task.partitionId, new ListBuffer&lt;span class=&quot;error&quot;&gt;&amp;#91;(Long, Any)&amp;#93;&lt;/span&gt;)&lt;br/&gt;
    +      for ((id, value) &amp;lt;- accumValue) {&lt;br/&gt;
    +        stageIdToAccumulators(stage.id)(task.partitionId) += id -&amp;gt; value&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    nit: you can avoid the lookup within the loop by using the value returned in the getOrElseUpdate calls.&lt;/p&gt;</comment>
                            <comment id="13954368" author="githubbot" created="Sat, 29 Mar 2014 18:02:25 +0000"  >&lt;p&gt;Github user mridulm commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/spark/pull/228#issuecomment-39003471&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/228#issuecomment-39003471&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    looks good @CodingCat ! just made a few minor points.&lt;br/&gt;
    excellent change !!&lt;/p&gt;</comment>
                            <comment id="13954378" author="githubbot" created="Sat, 29 Mar 2014 18:23:47 +0000"  >&lt;p&gt;Github user AmplabJenkins commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/spark/pull/228#issuecomment-39004142&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/228#issuecomment-39004142&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Merged build finished. Build is starting &lt;del&gt;or&lt;/del&gt; tests failed to complete.&lt;/p&gt;</comment>
                            <comment id="13954379" author="githubbot" created="Sat, 29 Mar 2014 18:23:47 +0000"  >&lt;p&gt;Github user AmplabJenkins commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/spark/pull/228#issuecomment-39004143&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/228#issuecomment-39004143&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Build is starting &lt;del&gt;or&lt;/del&gt; tests failed to complete.&lt;br/&gt;
    Refer to this link for build results: &lt;a href=&quot;https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/13579/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/13579/&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13954384" author="githubbot" created="Sat, 29 Mar 2014 18:30:44 +0000"  >&lt;p&gt;Github user kayousterhout commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/spark/pull/228#issuecomment-39004325&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/228#issuecomment-39004325&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Can we just add the accumulator update to TaskSetManager, in the handleSuccessfulTask() method?  This seems much simpler because the TaskSetManager already has all of the state about which tasks are running, which ones have been resubmitted or speculated, etc.  I think this change would be much simpler.&lt;/p&gt;

&lt;p&gt;    Over time, a lot of functionality has leaked into the DAGScheduler, such that there&apos;s a lot of state that&apos;s kept in multiple places: in the DAGScheduler and in the TaskSetManager or the TaskSchedulerImpl.  The abstraction is supposed to be that the DAGScheduler handles the high level semantics of scheduling stages and dealing with inter-stage dependencies, and the TaskSetManager handles the low-level details of the tasks for each stage.  There are some parts of this abstraction that are currently broken (where the DAGScheduler knows too much about task-level details) and refactoring this is on my todo list, but in the meantime I think we should try not to make this problem any worse, because it makes the code much more complicated, more difficult to understand, and buggy.&lt;/p&gt;</comment>
                            <comment id="13954423" author="githubbot" created="Sat, 29 Mar 2014 20:12:41 +0000"  >&lt;p&gt;Github user CodingCat commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/spark/pull/228#issuecomment-39007448&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/228#issuecomment-39007448&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Hi, Kay, I will think about it, and see if we can move accumulator related functionalities to tm entirely.   &lt;/p&gt;</comment>
                            <comment id="13954428" author="githubbot" created="Sat, 29 Mar 2014 20:27:05 +0000"  >&lt;p&gt;Github user kayousterhout commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/spark/pull/228#issuecomment-39007883&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/228#issuecomment-39007883&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Cool thanks!!&lt;/p&gt;


&lt;p&gt;    On Sat, Mar 29, 2014 at 1:12 PM, Nan Zhu &amp;lt;notifications@github.com&amp;gt; wrote:&lt;/p&gt;

&lt;p&gt;    &amp;gt; Hi, Kay, I will think about it, and see if we can move accumulator related&lt;br/&gt;
    &amp;gt; functionalities to tm entirely.&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; &amp;#8211;&lt;br/&gt;
    &amp;gt; Reply to this email directly or view it on GitHub&amp;lt;&lt;a href=&quot;https://github.com/apache/spark/pull/228#issuecomment-39007448&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/228#issuecomment-39007448&lt;/a&gt;&amp;gt;&lt;br/&gt;
    &amp;gt; .&lt;br/&gt;
    &amp;gt;&lt;/p&gt;</comment>
                            <comment id="14029496" author="daniel.siegmann.velos" created="Thu, 12 Jun 2014 17:51:08 +0000"  >&lt;p&gt;Any update on this issue? As it currently stands, values of Accumulators simply can&apos;t be trusted.&lt;/p&gt;</comment>
                            <comment id="14029503" author="codingcat" created="Thu, 12 Jun 2014 17:54:29 +0000"  >&lt;p&gt;I actually made a PR long time ago &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/spark/pull/228&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/228&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14181318" author="apachespark" created="Thu, 23 Oct 2014 13:24:20 +0000"  >&lt;p&gt;User &apos;CodingCat&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/2524&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/2524&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14227108" author="matei" created="Thu, 27 Nov 2014 01:32:29 +0000"  >&lt;p&gt;As discussed on &lt;a href=&quot;https://github.com/apache/spark/pull/2524&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/2524&lt;/a&gt; this is pretty hard to provide good semantics for in the general case (accumulator updates inside non-result stages), for the following reasons:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;An RDD may be computed as part of multiple stages. For example, if you update an accumulator inside a MappedRDD and then shuffle it, that might be one stage. But if you then call map() again on the MappedRDD, and shuffle the result of that, you get a second stage where that map is pipeline. Do you want to count this accumulator update twice or not?&lt;/li&gt;
	&lt;li&gt;Entire stages may be resubmitted if shuffle files are deleted by the periodic cleaner or are lost due to a node failure, so anything that tracks RDDs would need to do so for long periods of time (as long as the RDD is referenceable in the user program), which would be pretty complicated to implement.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;So I&apos;m going to mark this as &quot;won&apos;t fix&quot; for now, except for the part for result stages done in &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-3628&quot; title=&quot;Don&amp;#39;t apply accumulator updates multiple times for tasks in result stages&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-3628&quot;&gt;&lt;del&gt;SPARK-3628&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="14229277" author="daniel.siegmann.velos" created="Sun, 30 Nov 2014 23:38:46 +0000"  >&lt;p&gt;This is very disappointing. Essentially, Spark doesn&apos;t support any equivalent to Hadoop&apos;s counters. That&apos;s a major drawback to Spark.&lt;/p&gt;

&lt;p&gt;I hope you are at least planning to note this limitation prominently in the documentation (&lt;a href=&quot;http://spark.apache.org/docs/latest/programming-guide.html#accumulators&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://spark.apache.org/docs/latest/programming-guide.html#accumulators&lt;/a&gt;).&lt;/p&gt;</comment>
                            <comment id="14736120" author="koert" created="Wed, 9 Sep 2015 04:03:59 +0000"  >&lt;p&gt;It is not clear to me what the usage of accumulators is without this&lt;/p&gt;</comment>
                            <comment id="15163921" author="jiml" created="Wed, 24 Feb 2016 22:23:34 +0000"  >&lt;p&gt;Affects versions only goes to 1.1.0, presumably this is still an issue? Is it correct that this is only an issue in transformations, but in actions will work correctly? That idea seems to be supported by the docs under &lt;a href=&quot;https://spark.apache.org/docs/latest/programming-guide.html#accumulators-a-nameaccumlinka:&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://spark.apache.org/docs/latest/programming-guide.html#accumulators-a-nameaccumlinka:&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;&quot;In Java, Spark also supports the more general Accumulable interface to accumulate data where the resulting type is not the same as the elements added (e.g. build a list by collecting together elements).&lt;/p&gt;

&lt;p&gt;For accumulator updates performed inside actions only, Spark guarantees that each task&#8217;s update to the accumulator will only be applied once, i.e. restarted tasks will not update the value. In transformations, users should be aware of that each task&#8217;s update may be applied more than once if tasks or job stages are re-executed.&quot;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12743058">SPARK-3628</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12771172">SPARK-5490</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12743233">SPARK-3642</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>382763</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 38 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1txyv:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>383031</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>