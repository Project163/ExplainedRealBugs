<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:55:03 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-19122] Unnecessary shuffle+sort added if join predicates ordering differ from bucketing and sorting order</title>
                <link>https://issues.apache.org/jira/browse/SPARK-19122</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;`table1` and `table2` are sorted and bucketed on columns `j` and `k` (in respective order)&lt;/p&gt;

&lt;p&gt;This is how they are generated:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;val df = (0 until 16).map(i =&amp;gt; (i % 8, i * 2, i.toString)).toDF(&lt;span class=&quot;code-quote&quot;&gt;&quot;i&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;j&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;k&quot;&lt;/span&gt;).coalesce(1)
df.write.format(&lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.spark.sql.hive.orc.OrcFileFormat&quot;&lt;/span&gt;).bucketBy(8, &lt;span class=&quot;code-quote&quot;&gt;&quot;j&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;k&quot;&lt;/span&gt;).sortBy(&lt;span class=&quot;code-quote&quot;&gt;&quot;j&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;k&quot;&lt;/span&gt;).saveAsTable(&lt;span class=&quot;code-quote&quot;&gt;&quot;table1&quot;&lt;/span&gt;)
df.write.format(&lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.spark.sql.hive.orc.OrcFileFormat&quot;&lt;/span&gt;).bucketBy(8, &lt;span class=&quot;code-quote&quot;&gt;&quot;j&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;k&quot;&lt;/span&gt;).sortBy(&lt;span class=&quot;code-quote&quot;&gt;&quot;j&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;k&quot;&lt;/span&gt;).saveAsTable(&lt;span class=&quot;code-quote&quot;&gt;&quot;table2&quot;&lt;/span&gt;)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, if join predicates are specified in query in &lt;b&gt;same&lt;/b&gt; order as bucketing and sort order, there is no shuffle and sort.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;scala&amp;gt; hc.sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;SET spark.sql.autoBroadcastJoinThreshold=1&quot;&lt;/span&gt;)
scala&amp;gt; hc.sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;SELECT * FROM table1 a JOIN table2 b ON a.j=b.j AND a.k=b.k&quot;&lt;/span&gt;).explain(&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)

== Physical Plan ==
*SortMergeJoin [j#61, k#62], [j#100, k#101], Inner
:- *Project [i#60, j#61, k#62]
:  +- *Filter (isnotnull(k#62) &amp;amp;&amp;amp; isnotnull(j#61))
:     +- *FileScan orc &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;.table1[i#60,j#61,k#62] Batched: &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;, Format: ORC, Location: InMemoryFileIndex[file:/table1], PartitionFilters: [], PushedFilters: [IsNotNull(k), IsNotNull(j)], ReadSchema: struct&amp;lt;i:&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;,j:&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;,k:string&amp;gt;
+- *Project [i#99, j#100, k#101]
   +- *Filter (isnotnull(j#100) &amp;amp;&amp;amp; isnotnull(k#101))
      +- *FileScan orc &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;.table2[i#99,j#100,k#101] Batched: &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;, Format: ORC, Location: InMemoryFileIndex[file:/table2], PartitionFilters: [], PushedFilters: [IsNotNull(j), IsNotNull(k)], ReadSchema: struct&amp;lt;i:&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;,j:&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;,k:string&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;The same query with join predicates in &lt;b&gt;different&lt;/b&gt; order from bucketing and sort order leads to extra shuffle and sort being introduced&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;scala&amp;gt; hc.sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;SET spark.sql.autoBroadcastJoinThreshold=1&quot;&lt;/span&gt;)
scala&amp;gt; hc.sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;SELECT * FROM table1 a JOIN table2 b ON a.k=b.k AND a.j=b.j &quot;&lt;/span&gt;).explain(&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)

== Physical Plan ==
*SortMergeJoin [k#62, j#61], [k#101, j#100], Inner
:- *Sort [k#62 ASC NULLS FIRST, j#61 ASC NULLS FIRST], &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;, 0
:  +- Exchange hashpartitioning(k#62, j#61, 200)
:     +- *Project [i#60, j#61, k#62]
:        +- *Filter (isnotnull(k#62) &amp;amp;&amp;amp; isnotnull(j#61))
:           +- *FileScan orc &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;.table1[i#60,j#61,k#62] Batched: &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;, Format: ORC, Location: InMemoryFileIndex[file:/table1], PartitionFilters: [], PushedFilters: [IsNotNull(k), IsNotNull(j)], ReadSchema: struct&amp;lt;i:&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;,j:&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;,k:string&amp;gt;
+- *Sort [k#101 ASC NULLS FIRST, j#100 ASC NULLS FIRST], &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;, 0
   +- Exchange hashpartitioning(k#101, j#100, 200)
      +- *Project [i#99, j#100, k#101]
         +- *Filter (isnotnull(j#100) &amp;amp;&amp;amp; isnotnull(k#101))
            +- *FileScan orc &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;.table2[i#99,j#100,k#101] Batched: &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;, Format: ORC, Location: InMemoryFileIndex[file:/table2], PartitionFilters: [], PushedFilters: [IsNotNull(j), IsNotNull(k)], ReadSchema: struct&amp;lt;i:&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;,j:&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;,k:string&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="13032842">SPARK-19122</key>
            <summary>Unnecessary shuffle+sort added if join predicates ordering differ from bucketing and sorting order</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="tejasp">Tejas Patil</assignee>
                                    <reporter username="tejasp">Tejas Patil</reporter>
                        <labels>
                    </labels>
                <created>Sun, 8 Jan 2017 01:06:12 +0000</created>
                <updated>Fri, 11 Aug 2017 22:14:23 +0000</updated>
                            <resolved>Fri, 11 Aug 2017 22:14:23 +0000</resolved>
                                    <version>2.0.2</version>
                    <version>2.1.0</version>
                                    <fixVersion>2.3.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>10</watches>
                                                                                                                <comments>
                            <comment id="15808479" author="tejasp" created="Sun, 8 Jan 2017 01:38:21 +0000"  >&lt;p&gt;When a `SortMergeJoinExec` node is created, the join keys in both `left` and `right` relations are extracted in their order of appearance in the query (see 0). Later, the list of keys are used as-is to define the required distribution (see 1) and ordering (see 2) for the sort merge join node. Since the ordering matters here (ie. `ClusteredDistribution(a,b) != ClusteredDistribution(b,a)`), this mismatches with the distribution and ordering of the children... thus `EnsureRequirements` ends up adding shuffle + sort.&lt;/p&gt;

&lt;p&gt;0 : &lt;a href=&quot;https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/planning/patterns.scala#L103&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/planning/patterns.scala#L103&lt;/a&gt;&lt;br/&gt;
1 : &lt;a href=&quot;https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala#L80&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala#L80&lt;/a&gt;&lt;br/&gt;
2 : &lt;a href=&quot;https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala#L85&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala#L85&lt;/a&gt;&lt;/p&gt;
</comment>
                            <comment id="15808485" author="tejasp" created="Sun, 8 Jan 2017 01:43:31 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hvanhovell&quot; class=&quot;user-hover&quot; rel=&quot;hvanhovell&quot;&gt;hvanhovell&lt;/a&gt; : In a broader level, I see that when nodes for physical operators are created, the `requiredChildDistribution` and `requiredChildOrdering` is pretty much fixed at that point. While the planning is done and the child nodes are inspected, there is no way to change it.&lt;/p&gt;

&lt;p&gt;For fixing this, my take would be :&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Allow the `requiredChildDistribution` and `requiredChildOrdering` to decided based on its children. ie. change `requiredChildOrdering()` to `requiredChildOrdering(childOrderings)`. Default behavior would be to ignore the input `childOrderings`.&lt;/li&gt;
	&lt;li&gt;In case of operators like `SortMergeJoinExec`, where the distribution and ordering requirement is way stricter, the operator could itself take a decision what ordering needs to be used.&lt;/li&gt;
&lt;/ul&gt;


&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;child output ordering&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;ordering of join keys in query&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;shuffle+sort needed&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; a, b &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; a, b &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; No &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; a, b &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; b, a &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; No &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; a, b, c, d &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; a, b &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; No &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; a, b, c, d &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; b, c &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; Yes &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; a, b &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; a, b, c, d &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; Yes &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; b, c &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; a, b, c, d &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; Yes &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;


&lt;p&gt;Even &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-18067&quot; title=&quot;SortMergeJoin adds shuffle if join predicates have non partitioned columns&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-18067&quot;&gt;&lt;del&gt;SPARK-18067&lt;/del&gt;&lt;/a&gt; will benefit from this change. Let me know if you have any opinion about this approach OR have something better in mind.&lt;/p&gt;</comment>
                            <comment id="15837300" author="tejasp" created="Wed, 25 Jan 2017 06:57:52 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hvanhovell&quot; class=&quot;user-hover&quot; rel=&quot;hvanhovell&quot;&gt;hvanhovell&lt;/a&gt; : ping !! If you are busy, can you suggest someone with whom I can discuss this ? I want to fix this but before sending a PR it will be good to agree on a proper design.&lt;/p&gt;</comment>
                            <comment id="15839845" author="hvanhovell" created="Thu, 26 Jan 2017 15:17:30 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tejasp&quot; class=&quot;user-hover&quot; rel=&quot;tejasp&quot;&gt;tejasp&lt;/a&gt; We should fix this.&lt;/p&gt;

&lt;p&gt;The rules make sense to me. I only want to add that &lt;tt&gt;EnsureRequirements&lt;/tt&gt; fixes the plan bottom up. This means that we can use &lt;tt&gt;outputPartitioning&lt;/tt&gt; and the &lt;tt&gt;outputOrdering&lt;/tt&gt; of a plan&apos;s children to compute the plan&apos;s &lt;tt&gt;requiredChildDistribution&lt;/tt&gt; and &lt;tt&gt;requiredChildOrdering&lt;/tt&gt;. So we don&apos;t have to modify these methods.&lt;/p&gt;</comment>
                            <comment id="15873287" author="apachespark" created="Sat, 18 Feb 2017 18:36:04 +0000"  >&lt;p&gt;User &apos;tejasapatil&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/16985&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/16985&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16007823" author="cloud_fan" created="Fri, 12 May 2017 08:58:32 +0000"  >&lt;p&gt;I tried the example but can&apos;t reproduce this issue, there is no shuffle in the second query. I checked with `HashPartitioning.satisfies`, it doesn&apos;t consider the expressions order.&lt;/p&gt;</comment>
                            <comment id="16008266" author="tejasp" created="Fri, 12 May 2017 15:11:48 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cloud_fan&quot; class=&quot;user-hover&quot; rel=&quot;cloud_fan&quot;&gt;cloud_fan&lt;/a&gt;: &lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;The test case in the &lt;a href=&quot;https://github.com/apache/spark/pull/16985&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;associated PR (#16985)&lt;/a&gt; fails without the fix&lt;/li&gt;
	&lt;li&gt;I am able to repro this issue over master branch. Can you share exact steps that you used to repro ? I am guessing that `spark.sql.autoBroadcastJoinThreshold` needs to be overridden otherwise it wont pick sort merge join. Here are my exact steps to repro the example in the jira description:&lt;/li&gt;
&lt;/ul&gt;


&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;$ git log
commit 92ea7fd7b6cd4641b2f02b97105835029ddadc5f
Author: Takeshi Yamamuro &amp;lt;yamamuro@apache.org&amp;gt;
Date:   Fri May 12 20:48:30 2017 +0800

build/sbt -Pyarn -Phadoop-2.4 -Phive package assembly/package
export SPARK_PREPEND_CLASSES=true
SPARK_LOCAL_IP=127.0.0.1 ./bin/spark-shell
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In spark shell:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;import org.apache.spark.sql._
val hc = SparkSession.builder.master(&quot;local&quot;).enableHiveSupport.getOrCreate()
hc.sql(&quot; DROP TABLE table1 &quot;)
hc.sql(&quot; DROP TABLE table2 &quot;)
val df = (0 until 16).map(i =&amp;gt; (i % 8, i * 2, i.toString)).toDF(&quot;i&quot;, &quot;j&quot;, &quot;k&quot;).coalesce(1)

hc.sql(&quot;SET spark.sql.autoBroadcastJoinThreshold=1&quot;)
df.write.format(&quot;org.apache.spark.sql.hive.orc.OrcFileFormat&quot;).bucketBy(8, &quot;j&quot;, &quot;k&quot;).sortBy(&quot;j&quot;, &quot;k&quot;).saveAsTable(&quot;table1&quot;)
df.write.format(&quot;org.apache.spark.sql.hive.orc.OrcFileFormat&quot;).bucketBy(8, &quot;j&quot;, &quot;k&quot;).sortBy(&quot;j&quot;, &quot;k&quot;).saveAsTable(&quot;table2&quot;)


scala&amp;gt; hc.sql(&quot;SELECT * FROM table1 a JOIN table2 b ON a.j=b.j AND a.k=b.k&quot;).explain(true)
== Parsed Logical Plan ==
&apos;Project [*]
+- &apos;Join Inner, ((&apos;a.j = &apos;b.j) &amp;amp;&amp;amp; (&apos;a.k = &apos;b.k))
   :- &apos;SubqueryAlias a
   :  +- &apos;UnresolvedRelation `table1`
   +- &apos;SubqueryAlias b
      +- &apos;UnresolvedRelation `table2`

== Analyzed Logical Plan ==
i: int, j: int, k: string, i: int, j: int, k: string
Project [i#86, j#87, k#88, i#89, j#90, k#91]
+- Join Inner, ((j#87 = j#90) &amp;amp;&amp;amp; (k#88 = k#91))
   :- SubqueryAlias a
   :  +- SubqueryAlias table1
   :     +- Relation[i#86,j#87,k#88] orc
   +- SubqueryAlias b
      +- SubqueryAlias table2
         +- Relation[i#89,j#90,k#91] orc

== Optimized Logical Plan ==
Join Inner, ((j#87 = j#90) &amp;amp;&amp;amp; (k#88 = k#91))
:- Filter (isnotnull(j#87) &amp;amp;&amp;amp; isnotnull(k#88))
:  +- Relation[i#86,j#87,k#88] orc
+- Filter (isnotnull(j#90) &amp;amp;&amp;amp; isnotnull(k#91))
   +- Relation[i#89,j#90,k#91] orc

== Physical Plan ==
*SortMergeJoin [j#87, k#88], [j#90, k#91], Inner
:- *Project [i#86, j#87, k#88]
:  +- *Filter (isnotnull(j#87) &amp;amp;&amp;amp; isnotnull(k#88))
:     +- *FileScan orc default.table1[i#86,j#87,k#88] Batched: false, Format: ORC, Location: InMemoryFileIndex[file:/Users/tejasp/Desktop/dev/apache-hive-1.2.1-bin/warehouse/table1], PartitionFilters: [], PushedFilters: [IsNotNull(j), IsNotNull(k)], ReadSchema: struct&amp;lt;i:int,j:int,k:string&amp;gt;
+- *Project [i#89, j#90, k#91]
   +- *Filter (isnotnull(j#90) &amp;amp;&amp;amp; isnotnull(k#91))
      +- *FileScan orc default.table2[i#89,j#90,k#91] Batched: false, Format: ORC, Location: InMemoryFileIndex[file:/Users/tejasp/Desktop/dev/apache-hive-1.2.1-bin/warehouse/table2], PartitionFilters: [], PushedFilters: [IsNotNull(j), IsNotNull(k)], ReadSchema: struct&amp;lt;i:int,j:int,k:string&amp;gt;



scala&amp;gt; hc.sql(&quot;SELECT * FROM table1 a JOIN table2 b ON a.k=b.k AND a.j=b.j&quot;).explain(true)
== Parsed Logical Plan ==
&apos;Project [*]
+- &apos;Join Inner, ((&apos;a.k = &apos;b.k) &amp;amp;&amp;amp; (&apos;a.j = &apos;b.j))
   :- &apos;SubqueryAlias a
   :  +- &apos;UnresolvedRelation `table1`
   +- &apos;SubqueryAlias b
      +- &apos;UnresolvedRelation `table2`

== Analyzed Logical Plan ==
i: int, j: int, k: string, i: int, j: int, k: string
Project [i#106, j#107, k#108, i#109, j#110, k#111]
+- Join Inner, ((k#108 = k#111) &amp;amp;&amp;amp; (j#107 = j#110))
   :- SubqueryAlias a
   :  +- SubqueryAlias table1
   :     +- Relation[i#106,j#107,k#108] orc
   +- SubqueryAlias b
      +- SubqueryAlias table2
         +- Relation[i#109,j#110,k#111] orc

== Optimized Logical Plan ==
Join Inner, ((k#108 = k#111) &amp;amp;&amp;amp; (j#107 = j#110))
:- Filter (isnotnull(j#107) &amp;amp;&amp;amp; isnotnull(k#108))
:  +- Relation[i#106,j#107,k#108] orc
+- Filter (isnotnull(k#111) &amp;amp;&amp;amp; isnotnull(j#110))
   +- Relation[i#109,j#110,k#111] orc

== Physical Plan ==
*SortMergeJoin [k#108, j#107], [k#111, j#110], Inner
:- *Sort [k#108 ASC NULLS FIRST, j#107 ASC NULLS FIRST], false, 0
:  +- Exchange hashpartitioning(k#108, j#107, 200)
:     +- *Project [i#106, j#107, k#108]
:        +- *Filter (isnotnull(j#107) &amp;amp;&amp;amp; isnotnull(k#108))
:           +- *FileScan orc default.table1[i#106,j#107,k#108] Batched: false, Format: ORC, Location: InMemoryFileIndex[file:/Users/tejasp/Desktop/dev/apache-hive-1.2.1-bin/warehouse/table1], PartitionFilters: [], PushedFilters: [IsNotNull(j), IsNotNull(k)], ReadSchema: struct&amp;lt;i:int,j:int,k:string&amp;gt;
+- *Sort [k#111 ASC NULLS FIRST, j#110 ASC NULLS FIRST], false, 0
   +- Exchange hashpartitioning(k#111, j#110, 200)
      +- *Project [i#109, j#110, k#111]
         +- *Filter (isnotnull(k#111) &amp;amp;&amp;amp; isnotnull(j#110))
            +- *FileScan orc default.table2[i#109,j#110,k#111] Batched: false, Format: ORC, Location: InMemoryFileIndex[file:/Users/tejasp/Desktop/dev/apache-hive-1.2.1-bin/warehouse/table2], PartitionFilters: [], PushedFilters: [IsNotNull(k), IsNotNull(j)], ReadSchema: struct&amp;lt;i:int,j:int,k:string&amp;gt;

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="16009146" author="cloud_fan" created="Sat, 13 May 2017 05:36:24 +0000"  >&lt;p&gt;sorry I forgot to set the broadcast threshold, now I can reproduce this issue&lt;/p&gt;</comment>
                            <comment id="16009165" author="tejasp" created="Sat, 13 May 2017 06:22:23 +0000"  >&lt;p&gt;Thanks for confirming. I have added it in the jira description in case someone comes across this in future.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 27 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i38elb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>