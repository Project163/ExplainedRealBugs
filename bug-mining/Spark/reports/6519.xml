<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 19:05:42 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-27676] InMemoryFileIndex should hard-fail on missing files instead of logging and continuing</title>
                <link>https://issues.apache.org/jira/browse/SPARK-27676</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Spark&apos;s &lt;tt&gt;InMemoryFileIndex&lt;/tt&gt; contains two places where &lt;tt&gt;FileNotFound&lt;/tt&gt; exceptions are caught and logged as warnings (during &lt;a href=&quot;https://github.com/apache/spark/blob/bcd3b61c4be98565352491a108e6394670a0f413/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala#L274&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;directory listing&lt;/a&gt;&#160;and &lt;a href=&quot;https://github.com/apache/spark/blob/bcd3b61c4be98565352491a108e6394670a0f413/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala#L333&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;block location lookup&lt;/a&gt;). I think that this is a dangerous default behavior and would prefer that Spark hard-fails by default (with the ignore-and-continue behavior guarded by a SQL session configuration).&lt;/p&gt;

&lt;p&gt;In&#160;&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-17599&quot; title=&quot;Folder deletion after globbing may fail StructuredStreaming jobs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-17599&quot;&gt;&lt;del&gt;SPARK-17599&lt;/del&gt;&lt;/a&gt; and&#160;&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-24364&quot; title=&quot;Files deletion after globbing may fail StructuredStreaming jobs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-24364&quot;&gt;&lt;del&gt;SPARK-24364&lt;/del&gt;&lt;/a&gt;, logic was added to ignore missing files. Quoting from the PR for&#160;&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-17599&quot; title=&quot;Folder deletion after globbing may fail StructuredStreaming jobs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-17599&quot;&gt;&lt;del&gt;SPARK-17599&lt;/del&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;The&#160;&lt;tt&gt;ListingFileCatalog&lt;/tt&gt;&#160;lists files given a set of resolved paths. If a folder is deleted at any time between the paths were resolved and the file catalog can check for the folder, the Spark job fails. This may abruptly stop long running StructuredStreaming jobs for example.&lt;/p&gt;

&lt;p&gt;Folders may be deleted by users or automatically by retention policies. These cases should not prevent jobs from successfully completing.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Let&apos;s say that I&apos;m&#160;&lt;b&gt;not&lt;/b&gt; expecting to ever delete input files for my job. In that case, this behavior can mask bugs.&lt;/p&gt;

&lt;p&gt;One straightforward masked bug class is accidental file deletion: if I&apos;m never expecting to delete files then I&apos;d&#160;prefer to&#160;fail my job if Spark sees deleted files.&lt;/p&gt;

&lt;p&gt;A more subtle bug can occur when using a S3 filesystem. Say I&apos;m&#160;running a Spark job against a&#160;partitioned Parquet dataset which&#160;is laid out&#160;like this:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
data/
  date=1/
    region=west/
       0.parquet
       1.parquet
    region=east/
       0.parquet
       1.parquet&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;If I do &lt;tt&gt;spark.read.parquet(&quot;/data/date=1/&quot;)&lt;/tt&gt; then Spark needs to perform multiple rounds of file listing, first listing &lt;tt&gt;/data/date=1&lt;/tt&gt; to discover the partitions for that date, then listing within each partition to discover the leaf files. Due to the eventual consistency of S3 ListObjects, it&apos;s&#160;possible that the first listing will show the&#160;&lt;tt&gt;region=west&lt;/tt&gt; and &lt;tt&gt;region=east&lt;/tt&gt; partitions existing and then the next-level listing fails to return any&#160;for some of the directories (e.g. &lt;tt&gt;/data/date=1/&lt;/tt&gt; returns files but &lt;tt&gt;/data/date=1/region=west/&lt;/tt&gt; throws a &lt;tt&gt;FileNotFoundException&lt;/tt&gt; in S3A due to ListObjects inconsistency).&lt;/p&gt;

&lt;p&gt;If Spark propagated the &lt;tt&gt;FileNotFoundException&lt;/tt&gt; and hard-failed in this case then I&apos;d be able to&#160;fail the job in this case where we &lt;em&gt;definitely&lt;/em&gt;&#160;know&#160;that the S3&#160;listing is inconsistent (failing here doesn&apos;t guard against &lt;em&gt;all&lt;/em&gt; potential S3 list inconsistency issues (e.g. back-to-back listings which both return a subset of the true set of objects), but I think&#160;it&apos;s still an improvement to fail for the subset of cases that we &lt;em&gt;can&lt;/em&gt; detect even if that&apos;s not a surefire failsafe against the more general problem).&lt;/p&gt;

&lt;p&gt;Finally, I&apos;m unsure if the original patch will have the desired effect: if a file is deleted&#160;once a Spark job expects to read it then that can cause problems at multiple layers, both in the driver (multiple rounds of file listing) and in executors (if the deletion occurs after the construction of the catalog but before the scheduling of the read tasks); I think the original patch only resolved the problem for the driver (unless I&apos;m missing similar executor-side code specific to&#160;the original streaming use-case).&lt;/p&gt;

&lt;p&gt;Given all of these reasons, I think that the &quot;ignore potentially deleted files during file index listing&quot; behavior should be guarded behind a feature flag which defaults to &lt;tt&gt;false&lt;/tt&gt;, consistent with the existing &lt;tt&gt;spark.files.ignoreMissingFiles&lt;/tt&gt;&#160;and &lt;tt&gt;spark.sql.files.ignoreMissingFiles&lt;/tt&gt;&#160;flags (which&#160;both default to false).&lt;/p&gt;</description>
                <environment></environment>
        <key id="13232728">SPARK-27676</key>
            <summary>InMemoryFileIndex should hard-fail on missing files instead of logging and continuing</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="joshrosen">Josh Rosen</assignee>
                                    <reporter username="joshrosen">Josh Rosen</reporter>
                        <labels>
                    </labels>
                <created>Fri, 10 May 2019 19:16:05 +0000</created>
                <updated>Mon, 12 Dec 2022 18:11:05 +0000</updated>
                            <resolved>Wed, 26 Jun 2019 00:11:49 +0000</resolved>
                                    <version>2.4.0</version>
                                    <fixVersion>3.0.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="16837552" author="marmbrus" created="Fri, 10 May 2019 19:37:51 +0000"  >&lt;p&gt;I tend to agree that all cases where we chose to ignore missing files should be hidden behind the existing &lt;tt&gt;spark.sql.files.ignoreMissingFiles&lt;/tt&gt; flag.&lt;/p&gt;</comment>
                            <comment id="16844722" author="stevel@apache.org" created="Tue, 21 May 2019 10:59:30 +0000"  >&lt;p&gt;Bear in mind that there are multiple forms of S3 inconsistency which you can observe if you try hard enough&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;newly created file still has a 404 returned due to some caching in load balancers before the file was created&lt;/li&gt;
	&lt;li&gt;overwritten file still has old file returned on a HEAD or GET. Worse if if the file changes during a read. (&lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-15625&quot; title=&quot;S3A input stream to use etags/version number to detect changed source files&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-15625&quot;&gt;&lt;del&gt;HADOOP-15625&lt;/del&gt;&lt;/a&gt; will defend against that through failure detection)&lt;/li&gt;
	&lt;li&gt;newly deleted file is listed but 404 on open&lt;/li&gt;
	&lt;li&gt;newly created file isn&apos;t listed.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;This JIRA seems to be looking at issue #3. Are the others also vulnerabilities on this codepath?&lt;/p&gt;

&lt;p&gt;Can I highlight this is what S3Guard is intended to defend against by keeping the listing data in DDB up to date with changes made by clients to the store...&lt;/p&gt;
</comment>
                            <comment id="16845249" author="joshrosen" created="Tue, 21 May 2019 20:46:24 +0000"  >&lt;p&gt;+1 &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stevel%40apache.org&quot; class=&quot;user-hover&quot; rel=&quot;stevel@apache.org&quot;&gt;stevel@apache.org&lt;/a&gt;: I agree that this is by no means sufficient to make S3 listing safe (due to all of the potential anomalies you listed). My goal here is to fail fast when we can detect case (3) since that&apos;s a better default behavior than continuing and guaranteeing that we&apos;ll miss data. Failing loudly and quickly here increases the likelihood that a user will investigate and realize that they have race conditions (perhaps prompting them to use S3Guard, Delta Lake, manifest files, pre-computation of expected filenames (e.g. fixed number of output partitions with consistent filenaming scheme), etc.)&lt;/p&gt;</comment>
                            <comment id="16872807" author="gurwls223" created="Wed, 26 Jun 2019 00:11:49 +0000"  >&lt;p&gt;Issue resolved by pull request 24668&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/24668&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/24668&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16961899" author="siddarthasagar" created="Tue, 29 Oct 2019 11:07:02 +0000"  >&lt;p&gt;hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stevel%40apache.org&quot; class=&quot;user-hover&quot; rel=&quot;stevel@apache.org&quot;&gt;stevel@apache.org&lt;/a&gt;&#160;can this fix be made available in spark 2.x. it would really help us running spark on aws until spark 3 is released.&lt;/p&gt;</comment>
                            <comment id="16962255" author="stevel@apache.org" created="Tue, 29 Oct 2019 17:24:14 +0000"  >
&lt;p&gt;1. Well see if you can create a backport PR and get it through.&lt;br/&gt;
1. If you are working on AWS with a S3 store that does not have a consistency layer on top of it (consistent EMR, S3Guard) and you are relying on directory listings to enumerate the content of your tables -you are doomed. Seriously. In particular if you&apos;re using the commit-by-rename committers then task and job commit may miss newly created files.&lt;/p&gt;

&lt;p&gt;Which means, if you are seeing the problem discussed here it may be a symptom of a larger issue.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310560">
                    <name>Problem/Incident</name>
                                                                <inwardlinks description="is caused by">
                                        <issuelink>
            <issuekey id="13006027">SPARK-17599</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13161366">SPARK-24364</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10001">
                    <name>dependent</name>
                                                                <inwardlinks description="is depended upon by">
                                        <issuelink>
            <issuekey id="13300646">SPARK-31545</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 3 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z02l40:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>