<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:40:28 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-12864] Fetch failure from AM restart</title>
                <link>https://issues.apache.org/jira/browse/SPARK-12864</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Currently, when max number of executor failures reached the &lt;b&gt;maxNumExecutorFailures&lt;/b&gt;,  &lt;b&gt;ApplicationMaster&lt;/b&gt; will be killed and re-register another one.This time, &lt;b&gt;YarnAllocator&lt;/b&gt; will be created a new instance.&lt;br/&gt;
But, the value of property &lt;b&gt;executorIdCounter&lt;/b&gt; in  &lt;b&gt;YarnAllocator&lt;/b&gt; will reset to &lt;b&gt;0&lt;/b&gt;. Then the &lt;b&gt;Id&lt;/b&gt; of new executor will starting from 1. This will confuse with the executor has already created before, which will cause FetchFailedException.&lt;br/&gt;
For example, the following is the task log:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;2015-12-22 02:33:15 INFO 15/12/22 02:33:15 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster has disassociated: 172.22.92.14:45125
2015-12-22 02:33:26 INFO 15/12/22 02:33:26 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as AkkaRpcEndpointRef(Actor[akka.tcp://sparkYarnAM@172.22.168.72:54040/user/YarnAM#-1290854604])
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;2015-12-22 02:35:02 INFO 15/12/22 02:35:02 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@BJHC-HERA-16217.hadoop.jd.local:46538/user/Executor#-790726793]) with ID 1
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Lost task 3.0 in stage 102.0 (TID 1963, BJHC-HERA-16217.hadoop.jd.local): FetchFailed(BlockManagerId(1, BJHC-HERA-17030.hadoop.jd.local, 7337
), shuffleId=5, mapId=2, reduceId=3, message=
2015-12-22 02:43:20 INFO org.apache.spark.shuffle.FetchFailedException: /data3/yarn1/local/usercache/dd_edw/appcache/application_1450438154359_206399/blockmgr-b1fd0363-6d53-4d09-8086-adc4a13f4dc4/0f/shuffl
e_5_2_0.index (No such file or directory)
2015-12-22 02:43:20 INFO at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$.org$apache$spark$shuffle$hash$BlockStoreShuffleFetcher$$unpackBlock$1(BlockStoreShuffleFetcher.scala:67)
2015-12-22 02:43:20 INFO at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$$anonfun$3.apply(BlockStoreShuffleFetcher.scala:84)
2015-12-22 02:43:20 INFO at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$$anonfun$3.apply(BlockStoreShuffleFetcher.scala:84)
2015-12-22 02:43:20 INFO at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
2015-12-22 02:43:20 INFO at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
2015-12-22 02:43:20 INFO at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
2015-12-22 02:43:20 INFO at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
2015-12-22 02:43:20 INFO at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
2015-12-22 02:43:20 INFO at org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:154)
2015-12-22 02:43:20 INFO at org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:149)
2015-12-22 02:43:20 INFO at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:640)
2015-12-22 02:43:20 INFO at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:640)
2015-12-22 02:43:20 INFO at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
2015-12-22 02:43:20 INFO at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As the task log show, the executor id of  &lt;b&gt;BJHC-HERA-16217.hadoop.jd.local&lt;/b&gt; is the same as &lt;b&gt;BJHC-HERA-17030.hadoop.jd.local&lt;/b&gt;. So, it is confusion and cause FetchFailedException.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;And this situation of executorId conflict is just in yarn client mode due to driver not running on yarn.&lt;/b&gt;&lt;/p&gt;</description>
                <environment></environment>
        <key id="12931756">SPARK-12864</key>
            <summary>Fetch failure from AM restart</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="iward">iward</assignee>
                                    <reporter username="iward">iward</reporter>
                        <labels>
                    </labels>
                <created>Sun, 17 Jan 2016 12:49:13 +0000</created>
                <updated>Sun, 17 May 2020 18:17:08 +0000</updated>
                            <resolved>Fri, 1 Apr 2016 21:25:05 +0000</resolved>
                                    <version>1.3.1</version>
                    <version>1.4.1</version>
                    <version>1.5.2</version>
                                    <fixVersion>2.0.0</fixVersion>
                                    <component>Spark Core</component>
                    <component>YARN</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="15103715" author="apachespark" created="Sun, 17 Jan 2016 12:58:03 +0000"  >&lt;p&gt;User &apos;zhonghaihua&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/10794&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/10794&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15105671" author="jerryshao" created="Mon, 18 Jan 2016 19:02:29 +0000"  >&lt;p&gt;What Spark version are you using? I remember I fixed a similar AM re-attempt issue before. As I remembered, when AM is exited, all the related containers/executors will be exited as well, and when another attempt is started, it will initialize the AM from scratch, so there should be no problem.&lt;/p&gt;

&lt;p&gt;Did you mean that when AM is failed, all the containers are still running in your cluster? If so, that&apos;s a kind of weird, would you please elaborate what you saw, thanks a lot.&lt;/p&gt;</comment>
                            <comment id="15106129" author="iward" created="Tue, 19 Jan 2016 02:15:44 +0000"  >&lt;p&gt;Yeah, you are right. When AM is exited, All executors will be exited, but the BlockManagerMaster in driver still store the data information computed by executor which already has exited. And to shuffle this data must through executorId. This time if we re-register an AM, the executorIdCounter will reset to 0. So, when a new executor which executorId is `12` to shuffle data which computed by exited executor which executorId also `12`. This situation the new executor won&apos;t shuffle this data from remote, it will get the data from local, so, as my task log show, data is not found(No such file or directory). &lt;/p&gt;

&lt;p&gt;My spark version is 1.3.1 and 1.5.2.    &lt;/p&gt;</comment>
                            <comment id="15106214" author="jerryshao" created="Tue, 19 Jan 2016 04:13:44 +0000"  >&lt;p&gt;So the problem should be that: &lt;tt&gt;BlockManager&lt;/tt&gt; should be aware of the re-attempt of AM and doing some clean reinitializing works after AM is restarted, rather than fix this conflict executor id.&lt;/p&gt;</comment>
                            <comment id="15106295" author="iward" created="Tue, 19 Jan 2016 06:08:49 +0000"  >&lt;p&gt;I don&apos;t think so. Why don&apos;t fix this conflict executor id ? If we fix it, the application also can normally continue to run. If the applicaton just has one stage no accomplishment when AM is restarted, we just clean the all of data we has already computed in BlockManager, and re-compute all of it, I think that is waste lots of time. If we do that, it is better to shutdown the driver and re-run the application rather than restart AM.&lt;/p&gt;

&lt;p&gt;So, I think we fix this conflict executor id is better.&lt;/p&gt;</comment>
                            <comment id="15107812" author="mwws" created="Wed, 20 Jan 2016 01:53:52 +0000"  >&lt;p&gt;I don&apos;t quite get your point here. With your new executor id idea, how could computed data in BlockManger be reused? I am afraid it need to be recomputed anyway in AM restarted case.&lt;/p&gt;</comment>
                            <comment id="15107859" author="iward" created="Wed, 20 Jan 2016 02:38:29 +0000"  >&lt;p&gt;The important point of the idea is to fix this conflict executor id. As the task log show, if the data of current task to shuffle is not found, it will throw a FetchFailedException. So, I think the mechanism in AM restarted case is continue to run, it won&apos;t recomputed, if the data computed is not found, it will throw a FetchFailedException. And I have run a test that it will normally continue to run.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 43 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2rknb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>