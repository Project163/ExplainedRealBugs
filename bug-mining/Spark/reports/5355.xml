<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:57:22 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-3685] Spark&apos;s local dir should accept only local paths</title>
                <link>https://issues.apache.org/jira/browse/SPARK-3685</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;When you try to set local dirs to &quot;hdfs:/tmp/foo&quot; it doesn&apos;t work. What it will try to do is create a folder called &quot;hdfs:&quot; and put &quot;tmp&quot; inside it. This is because in Util#getOrCreateLocalRootDirs we use java.io.File instead of Hadoop&apos;s file system to parse this path. We also need to resolve the path appropriately.&lt;/p&gt;

&lt;p&gt;This may not have an urgent use case, but it fails silently and does what is least expected.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12743906">SPARK-3685</key>
            <summary>Spark&apos;s local dir should accept only local paths</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="gurwls223">Hyukjin Kwon</assignee>
                                    <reporter username="andrewor14">Andrew Or</reporter>
                        <labels>
                    </labels>
                <created>Thu, 25 Sep 2014 00:34:43 +0000</created>
                <updated>Mon, 12 Dec 2022 18:10:34 +0000</updated>
                            <resolved>Tue, 12 Dec 2017 08:02:29 +0000</resolved>
                                    <version>1.1.0</version>
                                    <fixVersion>2.3.0</fixVersion>
                                    <component>Spark Core</component>
                    <component>YARN</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>8</watches>
                                                                                                                <comments>
                            <comment id="14147206" author="andrewor14" created="Thu, 25 Sep 2014 01:09:49 +0000"  >&lt;p&gt;Note that this is not meaningful unless we also change the usages of this to use the Hadoop FileSystem. This requires a non-trivial refactor of shuffle and spill code to use the Hadoop API.&lt;/p&gt;</comment>
                            <comment id="14151148" author="farrellee" created="Sun, 28 Sep 2014 17:32:45 +0000"  >&lt;p&gt;i&apos;m skeptical. what would be the benefit of using HDFS for temporary storage?&lt;/p&gt;</comment>
                            <comment id="14151350" author="pwendell" created="Mon, 29 Sep 2014 04:35:57 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=andrewor14&quot; class=&quot;user-hover&quot; rel=&quot;andrewor14&quot;&gt;andrewor14&lt;/a&gt; changing the use of local storage to use HDFS would be a fairly large change. For instance, I think we use the FileChannel API and other types of random access that are not supported by HDFS. The title of this JIRA makes it seem like this is just about interpretation of the filename inside of Spark, but really the proposal is much broader here, suggesting we move to the Hadoop API&apos;s instead of the Posix FS Api.&lt;/p&gt;</comment>
                            <comment id="14151372" author="andrewor14" created="Mon, 29 Sep 2014 05:16:19 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=farrellee&quot; class=&quot;user-hover&quot; rel=&quot;farrellee&quot;&gt;farrellee&lt;/a&gt; One of the concerns for &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-3174&quot; title=&quot;Provide elastic scaling within a Spark application&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-3174&quot;&gt;&lt;del&gt;SPARK-3174&lt;/del&gt;&lt;/a&gt; is that after an executor is removed, any shuffle files it has written are no longer accessible. In my design for that issue, one of my considerations is to use a common DFS to store these files (rather than tying their fates to the executor&apos;s). But yes, otherwise there is very little benefit for writing these through HDFS simply because there will be a lot of overhead there.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pwendell&quot; class=&quot;user-hover&quot; rel=&quot;pwendell&quot;&gt;pwendell&lt;/a&gt; Yes, it will be a fairly large change that we may or may not want (depending on the final design of &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-3174&quot; title=&quot;Provide elastic scaling within a Spark application&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-3174&quot;&gt;&lt;del&gt;SPARK-3174&lt;/del&gt;&lt;/a&gt;). Either way Spark should not create a directory called &quot;hdfs:&quot; and put the files there. At the very least we should probably throw an exception complaining that it&apos;s not supported. Perhaps the title and the description can be renamed to reflect this.&lt;/p&gt;</comment>
                            <comment id="14151691" author="farrellee" created="Mon, 29 Sep 2014 13:45:27 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=andrewor&quot; class=&quot;user-hover&quot; rel=&quot;andrewor&quot;&gt;andrewor&lt;/a&gt; thanks for the info. afaik the executor is also in charge of the shuffle file life-cycle, and breaking that would be complicated. it&apos;s probably a cleaner implementation to allow executors to remain and use a policy to prune unused/little-used executors where unused/little-used factors in amount of data they are holding as well as cpu used. you could also go down the path of aging-out executors - let their resources go back to the node&apos;s pool for reallocation, but don&apos;t kill off the process. however, approaches like that become very complex and push implementation details of the workload, which often don&apos;t generalize, into the scheduling system.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=andrewor&quot; class=&quot;user-hover&quot; rel=&quot;andrewor&quot;&gt;andrewor&lt;/a&gt; btw, it should be a warning case (&quot;hey you might have messed up, i see you used hdfs:/ in your file name&quot;) instead of an error case.&lt;/p&gt;</comment>
                            <comment id="14152070" author="andrewor14" created="Mon, 29 Sep 2014 18:58:32 +0000"  >&lt;p&gt;Yeah there will be a design doc soon on the possible solutions for dealing with shuffles. Note that one of the main motivations of doing this is to free up containers in Yarn when an application is not using it, so maintaining a pool of executor containers does not achieve what we want. Also, DFS shuffle is only one of the solutions we will consider, but we probably won&apos;t end up relying on it because of the overhead it adds (i.e. we&apos;ll probably need a different solution down the road either way).&lt;/p&gt;

&lt;p&gt;It could be a warning, but I think an exception is appropriate here because the user clearly thinks that its shuffle files are going into HDFS when they&apos;re not. Also, the fact that it fails-fast means the user knows Spark won&apos;t do what he/she wants before even a single shuffle file is written. Either way I don&apos;t feel strongly about this.&lt;/p&gt;
</comment>
                            <comment id="14152152" author="farrellee" created="Mon, 29 Sep 2014 19:47:05 +0000"  >&lt;p&gt;the root of the resource problem is how they&apos;re handed out. yarn is giving you a whole cpu, some amount of memory, some amount of network and some amount of disk to work with. your executor (like any program) uses different amounts of resources throughout its execution. at points in the execution the resource profile changes, call the demarcated regions &quot;phases&quot;. so an executor may transition from a high resource phase to a low resource phase. in a low resource phase, you may want to free up resources for other executors, but maintain enough to do basic operations (say: serve a shuffle file). this is a problem that should be solved by the resource manager. in my opinion, a solution w/i spark that isn&apos;t faciliated by the RN is a workaround/hack and should be avoided. an example of a RN facilitated solution might be a message the executor can send to yarn to indicate its resources can be free&apos;d, except for some minimum amount.&lt;/p&gt;</comment>
                            <comment id="14152228" author="andrewor14" created="Mon, 29 Sep 2014 20:33:49 +0000"  >&lt;p&gt;Not sure if I fully understand what you mean. If I&apos;m running an executor and I request 30G from the beginning, my application uses all of it to do computation and all is good. After I decommission the executor, I would like to keep 1G just to serve the shuffle files, but this can&apos;t be done easily because we need to start a smaller JVM and a smaller container. (Yarn currently doesn&apos;t support scaling the size of a container while it&apos;s still running yet). Either way we need to transfer some state from the bigger JVM to the smaller JVM, and that adds some complexity to the design. The simplest alternative then would just to write whatever state to an external location and just terminate the executor JVM / container without starting a smaller one, and then have an external service that is long-running to serve these files.&lt;/p&gt;

&lt;p&gt;One proposal here then is to write these shuffle files to a special location and have the Yarn NM shuffle service serve the files. This is an alternative to DFS shuffle that is, however, highly specific to Yarn. I am doing some initial prototyping of this (the Yarn shuffle) approach to see how this will pan out.&lt;/p&gt;</comment>
                            <comment id="14152526" author="farrellee" created="Mon, 29 Sep 2014 23:41:07 +0000"  >&lt;p&gt;if you&apos;re going to go down this path the best (i&apos;d say correct) way to implement it is to have support from yarn - a way to tell yarn &quot;i&apos;m only going to need X,Y,Z resources from now on&quot; without giving up the execution container. i bet there&apos;s a way to re-exec the jvm into a smaller form factor.&lt;/p&gt;</comment>
                            <comment id="14352068" author="srowen" created="Sun, 8 Mar 2015 14:49:19 +0000"  >&lt;p&gt;This is 90% the same discussion as &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-1529&quot; title=&quot;Support DFS based shuffle in addition to Netty shuffle&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-1529&quot;&gt;&lt;del&gt;SPARK-1529&lt;/del&gt;&lt;/a&gt;, although, this concerns making the current behavior more explicit (e.g. fail an hdfs: URI) whereas &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-1529&quot; title=&quot;Support DFS based shuffle in addition to Netty shuffle&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-1529&quot;&gt;&lt;del&gt;SPARK-1529&lt;/del&gt;&lt;/a&gt; (and the discussion below) discusses making other FS schemes work. I&apos;d like to potentially address this issue, without prejudicing &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-1529&quot; title=&quot;Support DFS based shuffle in addition to Netty shuffle&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-1529&quot;&gt;&lt;del&gt;SPARK-1529&lt;/del&gt;&lt;/a&gt;. In fact this discussion usefully contains a good use case for putting a local dir on distributed storage, whereas I personally don&apos;t see it in the arguments in &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-1529&quot; title=&quot;Support DFS based shuffle in addition to Netty shuffle&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-1529&quot;&gt;&lt;del&gt;SPARK-1529&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="14365087" author="stevel@apache.org" created="Tue, 17 Mar 2015 13:09:38 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/YARN-1197&quot; title=&quot;Support changing resources of an allocated container&quot; class=&quot;issue-link&quot; data-issue-key=&quot;YARN-1197&quot;&gt;YARN-1197&lt;/a&gt; covers supporting resizing existing YARN containers: that would be the real solution to altering the memory footprint of an executor in a container -at least if that JVM can change its heap size down. but... that JIRA is &quot;dormant&quot;; I don&apos;t know if anyone is going to pick it up in the near-term.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-1529&quot; title=&quot;Support DFS based shuffle in addition to Netty shuffle&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-1529&quot;&gt;&lt;del&gt;SPARK-1529&lt;/del&gt;&lt;/a&gt; looks at switching to the Hadoop FS APIs, but doesn&apos;t mandate remote storage: it just makes it possible&lt;/p&gt;

&lt;p&gt;Switching to HDFS storage, as Andrew proposes, risks hitting network performance.&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;network traffic unless the replication factor == 1. (though do that &amp;amp; there&apos;s only one preferred location for the new container)&lt;/li&gt;
	&lt;li&gt;disk IO conflict with other HDFS work going on on the localhost.&lt;/li&gt;
	&lt;li&gt;the overhead of going via the TCP stack unless they are bypassed via unix domain sockets (as HBase does).&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;There&apos;s a risk, therefore, that the performance of all work will suffer just to support a single use case &quot;flex executor container &amp;amp; JVM size&quot;. That&apos;s also ignoring the scheduling risk of the smaller container not being allocated resources&lt;/p&gt;

&lt;p&gt;Hooking up the YARN NM shuffle would be the better way to do this. If that shuffle can&apos;t handle the wiring-up, it&apos;s probably easier to fix that than the whole YARN container-resize problem&lt;/p&gt;
</comment>
                            <comment id="16284733" author="apachespark" created="Sat, 9 Dec 2017 13:21:04 +0000"  >&lt;p&gt;User &apos;HyukjinKwon&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/19934&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/19934&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16287245" author="gurwls223" created="Tue, 12 Dec 2017 08:02:30 +0000"  >&lt;p&gt;Issue resolved by pull request 19934&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/19934&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/19934&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12709144">SPARK-1529</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 49 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i20gin:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>