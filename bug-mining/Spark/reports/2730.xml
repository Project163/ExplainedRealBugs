<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:34:48 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-11191] [1.5] Can&apos;t create UDF&apos;s using hive thrift service</title>
                <link>https://issues.apache.org/jira/browse/SPARK-11191</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Since upgrading to spark 1.5 we&apos;ve been unable to create and use UDF&apos;s when we run in thrift server mode.&lt;/p&gt;

&lt;p&gt;Our setup:&lt;br/&gt;
We start the thrift-server running against yarn in client mode, (we&apos;ve also built our own spark from github branch-1.5 with the following args: &lt;tt&gt;-Pyarn -Phive -Phive-thrifeserver&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;If i run the following after connecting via JDBC (in this case via beeline):&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;add jar &apos;hdfs://path/to/jar&quot;&lt;/tt&gt;&lt;br/&gt;
(this command succeeds with no errors)&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;CREATE TEMPORARY FUNCTION testUDF AS &apos;com.foo.class.UDF&apos;;&lt;/tt&gt;&lt;br/&gt;
(this command succeeds with no errors)&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;select testUDF(col1) from table1;&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;I get the following error in the logs:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;org.apache.spark.sql.AnalysisException: undefined function testUDF; line 1 pos 8
        at org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$lookupFunction$2$$anonfun$1.apply(hiveUDFs.scala:58)
        at org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$lookupFunction$2$$anonfun$1.apply(hiveUDFs.scala:58)
        at scala.Option.getOrElse(Option.scala:120)
        at org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$lookupFunction$2.apply(hiveUDFs.scala:57)
        at org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$lookupFunction$2.apply(hiveUDFs.scala:53)
        at scala.util.Try.getOrElse(Try.scala:77)
        at org.apache.spark.sql.hive.HiveFunctionRegistry.lookupFunction(hiveUDFs.scala:53)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$10$$anonfun$applyOrElse$5$$anonfun$applyOrElse$24.apply(Analyzer.scala:506)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$10$$anonfun$applyOrElse$5$$anonfun$applyOrElse$24.apply(Analyzer.scala:506)
        at org.apache.spark.sql.catalyst.analysis.&lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt;$.withPosition(&lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt;.scala:48)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$10$$anonfun$applyOrElse$5.applyOrElse(Analyzer.scala:505)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$10$$anonfun$applyOrElse$5.applyOrElse(Analyzer.scala:502)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:227)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:227)
        at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:226)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:232)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:232)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;(cutting the bulk for ease of report, more than happy to send the full output)&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;15/10/12 14:34:37 ERROR SparkExecuteStatementOperation: Error running hive query:
org.apache.hive.service.cli.HiveSQLException: org.apache.spark.sql.AnalysisException: undefined function testUDF; line 1 pos 100
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.runInternal(SparkExecuteStatementOperation.scala:259)
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:171)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:182)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;When I ran the same against 1.4 it worked.&lt;/p&gt;

&lt;p&gt;I&apos;ve also changed the &lt;tt&gt;spark.sql.hive.metastore.version&lt;/tt&gt; version to be 0.13 (similar to what it was in 1.4) and 0.14 but I still get the same errors.&lt;/p&gt;

&lt;p&gt;Also, in 1.5, when you run it against the &lt;tt&gt;spark-sql&lt;/tt&gt; shell, it works.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12906067">SPARK-11191</key>
            <summary>[1.5] Can&apos;t create UDF&apos;s using hive thrift service</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="lian cheng">Cheng Lian</assignee>
                                    <reporter username="dyross">David Ross</reporter>
                        <labels>
                    </labels>
                <created>Mon, 19 Oct 2015 20:40:47 +0000</created>
                <updated>Mon, 16 Nov 2015 13:47:05 +0000</updated>
                            <resolved>Thu, 12 Nov 2015 20:18:32 +0000</resolved>
                                    <version>1.5.0</version>
                    <version>1.5.1</version>
                                    <fixVersion>1.5.3</fixVersion>
                    <fixVersion>1.6.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>9</votes>
                                    <watches>15</watches>
                                                                                                                <comments>
                            <comment id="14964048" author="dyross" created="Mon, 19 Oct 2015 21:01:35 +0000"  >&lt;p&gt;I will add that the exact same thing happens when you don&apos;t use &lt;tt&gt;TEMPORARY&lt;/tt&gt; i.e.:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;CREATE FUNCTION testUDF AS &lt;span class=&quot;code-quote&quot;&gt;&apos;com.foo.&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;UDF&apos;&lt;/span&gt;;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="14984377" author="study" created="Sun, 1 Nov 2015 12:14:55 +0000"  >&lt;p&gt;This should be caused by builtin FunctionRegistry in 1.5.1&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/blob/v1.5.1/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveContext.scala#L413&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/v1.5.1/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveContext.scala#L413&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The FunctionRegistry in 1.4.1&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/blob/v1.4.1/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveContext.scala#L377&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/v1.4.1/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveContext.scala#L377&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14984804" author="cloud_fan" created="Mon, 2 Nov 2015 06:49:11 +0000"  >&lt;p&gt;The behaviour of &quot;ADD JAR&quot; has not been changed from 1.4 to 1.5, and &quot;CREATE FUNCTION&quot; is a native command that we will run it using hive client.  So I think the change of built-in FunctionRegistry is not the reason of this bug, I&apos;ll look into it.&lt;/p&gt;</comment>
                            <comment id="14984879" author="study" created="Mon, 2 Nov 2015 08:40:13 +0000"  >&lt;p&gt;Ok, it looks like `FunctionRegistry.getFunctionInfo` does not contain permanent and temporary functions information.&lt;/p&gt;

&lt;p&gt;The SELECT clause lookups functions in the order:&lt;br/&gt;
1. underlying.lookupFunction (built-in FunctionRegistry)&lt;br/&gt;
2. FunctionRegistry.getFunctionInfo&lt;/p&gt;</comment>
                            <comment id="14996519" author="lian cheng" created="Mon, 9 Nov 2015 13:24:25 +0000"  >&lt;p&gt;One of the problem here is &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-11595&quot; title=&quot;&amp;quot;ADD JAR&amp;quot; doesn&amp;#39;t work if the given path contains URL scheme like &amp;quot;file:/&amp;quot; and &amp;quot;hdfs:/&amp;quot;&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-11595&quot;&gt;&lt;del&gt;SPARK-11595&lt;/del&gt;&lt;/a&gt;. However, after fixing &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-11595&quot; title=&quot;&amp;quot;ADD JAR&amp;quot; doesn&amp;#39;t work if the given path contains URL scheme like &amp;quot;file:/&amp;quot; and &amp;quot;hdfs:/&amp;quot;&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-11595&quot;&gt;&lt;del&gt;SPARK-11595&lt;/del&gt;&lt;/a&gt;, &lt;tt&gt;CREATE TEMPORARY FUNCTION&lt;/tt&gt; still doesn&apos;t work properly. Still investigating.&lt;/p&gt;</comment>
                            <comment id="14997028" author="davies" created="Mon, 9 Nov 2015 18:08:58 +0000"  >&lt;p&gt;This should work in master and 1.6.&lt;/p&gt;</comment>
                            <comment id="15001777" author="merentafo" created="Thu, 12 Nov 2015 07:12:46 +0000"  >&lt;p&gt;Checked 10 hours ago. Pulled from master. The same error.&lt;/p&gt;</comment>
                            <comment id="15001809" author="study" created="Thu, 12 Nov 2015 07:48:09 +0000"  >&lt;p&gt;I have wrote a workaround patch for this issue, but the patch doesn&apos;t work for embedded metastore.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/phstudy/spark/commit/b2e618863b733f9fe5dfc69fdb7bd9ab5379df07&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/phstudy/spark/commit/b2e618863b733f9fe5dfc69fdb7bd9ab5379df07&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15002047" author="lian cheng" created="Thu, 12 Nov 2015 13:19:10 +0000"  >&lt;p&gt;This issue consists of two bugs. One of them is the ADD JAR issue (&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-11595&quot; title=&quot;&amp;quot;ADD JAR&amp;quot; doesn&amp;#39;t work if the given path contains URL scheme like &amp;quot;file:/&amp;quot; and &amp;quot;hdfs:/&amp;quot;&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-11595&quot;&gt;&lt;del&gt;SPARK-11595&lt;/del&gt;&lt;/a&gt;), which has just been fixed. The other one is that, &lt;tt&gt;HiveFunctionRegistry&lt;/tt&gt; should use execution Hive client to lookup temporary functions. I&apos;m fixing this for 1.5 and 1.6.&lt;/p&gt;</comment>
                            <comment id="15002208" author="study" created="Thu, 12 Nov 2015 15:01:27 +0000"  >&lt;p&gt;I think you also need to lookup permanent user-defined functions in metadata Hive for local/remote metastore service. This is a common use case for hiveserver.&lt;/p&gt;</comment>
                            <comment id="15002270" author="apachespark" created="Thu, 12 Nov 2015 15:47:04 +0000"  >&lt;p&gt;User &apos;liancheng&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/9664&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/9664&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15002385" author="lian cheng" created="Thu, 12 Nov 2015 16:43:51 +0000"  >&lt;p&gt;Spark SQL hasn&apos;t supported persisted functions yet.&lt;/p&gt;</comment>
                            <comment id="15002462" author="study" created="Thu, 12 Nov 2015 17:21:51 +0000"  >&lt;p&gt;I think this is a Spark ThriftServer issue, not spark SQL.&lt;/p&gt;

&lt;p&gt;Currently, by default Spark ThriftServer will create an embedded metastore in execution path, and you can also setup local/remote metastore service. For this case, Spark ThriftServer should support accessing permanent user-defined functions in the metastore.&lt;/p&gt;


&lt;p&gt;And the Spark SQL document also says &quot;In addition to the basic SQLContext, you can also create a HiveContext, which provides a superset of the functionality provided by the basic SQLContext. Additional features include the ability to write queries using the more complete HiveQL parser, access to Hive UDFs, and the ability to read data from Hive tables&quot;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/sql-programming-guide.html#starting-point-sqlcontext&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://spark.apache.org/docs/latest/sql-programming-guide.html#starting-point-sqlcontext&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15002623" author="apachespark" created="Thu, 12 Nov 2015 18:51:03 +0000"  >&lt;p&gt;User &apos;liancheng&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/9671&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/9671&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15002842" author="marmbrus" created="Thu, 12 Nov 2015 20:18:32 +0000"  >&lt;p&gt;Issue resolved by pull request 9664&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/9664&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/9664&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15003549" author="lian cheng" created="Fri, 13 Nov 2015 04:49:54 +0000"  >&lt;p&gt;Sorry that I wasn&apos;t clear enough in my previous reply. So in Spark, the Thrift server delegates most of the functionalities to Spark SQL, and Hive function lookup is also one of the case. Currently, we haven&apos;t implemented Hive persisted function lookup in Spark SQL, so the Thrift server doesn&apos;t support it either.&lt;/p&gt;</comment>
                            <comment id="15003574" author="study" created="Fri, 13 Nov 2015 05:20:12 +0000"  >&lt;p&gt;I have implemented Hive persisted function lookup in this patch: &lt;a href=&quot;https://github.com/phstudy/spark/commit/b2e618863b733f9fe5dfc69fdb7bd9ab5379df07&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/phstudy/spark/commit/b2e618863b733f9fe5dfc69fdb7bd9ab5379df07&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;But I found it does not work for embedded metastore, embedded metastore only allows single connection. Do you have any ideas?&lt;/p&gt;</comment>
                            <comment id="15003578" author="lian cheng" created="Fri, 13 Nov 2015 05:26:09 +0000"  >&lt;p&gt;What error message/exception stacktrace did you get when working with embedded metastore?&lt;/p&gt;</comment>
                            <comment id="15003586" author="study" created="Fri, 13 Nov 2015 05:38:04 +0000"  >&lt;p&gt;This is my exception stacktrace.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;13:34:49,728 ERROR Schema:125 - Failed initialising database.
Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, username = APP. Terminating connection pool (set lazyInit to &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; you expect to start your database after your app). Original Exception: ------
java.sql.SQLException: Failed to start database &lt;span class=&quot;code-quote&quot;&gt;&apos;metastore_db&apos;&lt;/span&gt; with &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@4c8d45cf, see the next exception &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; details.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.&amp;lt;init&amp;gt;(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection40.&amp;lt;init&amp;gt;(Unknown Source)
	at org.apache.derby.jdbc.Driver40.getNewEmbedConnection(Unknown Source)
	at org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)
	at org.apache.derby.jdbc.Driver20.connect(Unknown Source)
	at org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)
	at java.sql.DriverManager.getConnection(DriverManager.java:664)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
	at com.jolbox.bonecp.BoneCP.&amp;lt;init&amp;gt;(BoneCP.java:416)
	at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)
	at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.&amp;lt;init&amp;gt;(RDBMSStoreManager.java:298)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
	at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
	at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)
	at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)
	at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
	at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
	at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.&amp;lt;init&amp;gt;(RawStoreProxy.java:57)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&amp;lt;init&amp;gt;(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&amp;lt;init&amp;gt;(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&amp;lt;init&amp;gt;(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&amp;lt;init&amp;gt;(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.&amp;lt;clinit&amp;gt;(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.ClientWrapper.&amp;lt;init&amp;gt;(ClientWrapper.scala:173)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.liftedTree1$1(IsolatedClientLoader.scala:183)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.&amp;lt;init&amp;gt;(IsolatedClientLoader.scala:179)
	at org.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:226)
	at org.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:185)
	at org.apache.spark.sql.hive.HiveContext.functionRegistry$lzycompute(HiveContext.scala:413)
	at org.apache.spark.sql.hive.HiveContext.functionRegistry(HiveContext.scala:412)
	at org.apache.spark.sql.UDFRegistration.&amp;lt;init&amp;gt;(UDFRegistration.scala:40)
	at org.apache.spark.sql.SQLContext.&amp;lt;init&amp;gt;(SQLContext.scala:296)
	at org.apache.spark.sql.hive.HiveContext.&amp;lt;init&amp;gt;(HiveContext.scala:72)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLEnv$.init(SparkSQLEnv.scala:58)
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2$.main(HiveThriftServer2.scala:77)
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.main(HiveThriftServer2.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.sql.SQLException: Failed to start database &lt;span class=&quot;code-quote&quot;&gt;&apos;metastore_db&apos;&lt;/span&gt; with &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@4c8d45cf, see the next exception &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; details.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)
	... 95 more
Caused by: java.sql.SQLException: Another instance of Derby may have already booted the database /usr/local/Cellar/apache-spark/1.5.1/libexec/sbin/metastore_db.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	... 92 more
Caused by: ERROR XSDB6: Another instance of Derby may have already booted the database /usr/local/Cellar/apache-spark/1.5.1/libexec/sbin/metastore_db.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)
	at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)
	at java.security.AccessController.doPrivileged(Native Method)
	at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)
	at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)
	at org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)
	at org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)
	at org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)
	at org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)
	at org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)
	at org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)
	at org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)
	at org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)
	at org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)
	at org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)
	at org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)
	at org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)
	at org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)
	at org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)
	at org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)
	at org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)
	at org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)
	at org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)
	at org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)
	at org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)
	at org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)
	at org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)
	at org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)
	... 92 more
------
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15006672" author="apachespark" created="Mon, 16 Nov 2015 13:47:05 +0000"  >&lt;p&gt;User &apos;liancheng&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/9737&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/9737&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12911529">SPARK-11595</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 1 week, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2n7hj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12333643">1.5.2</customfieldvalue>
    <customfieldvalue id="12333969">1.5.3</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>