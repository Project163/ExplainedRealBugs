<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:21:22 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-4841] Batch serializer bug in PySpark&apos;s RDD.zip</title>
                <link>https://issues.apache.org/jira/browse/SPARK-4841</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;t = sc.textFile(&lt;span class=&quot;code-quote&quot;&gt;&quot;README.md&quot;&lt;/span&gt;)
t.zip(t).count()
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Py4JJavaError                             Traceback (most recent call last)
&amp;lt;ipython-input-6-60fdeb8339fd&amp;gt; in &amp;lt;module&amp;gt;()
----&amp;gt; 1 readme.zip(readme).count()

/Users/meng/src/spark/python/pyspark/rdd.pyc in count(self)
    817         3
    818         &quot;&quot;&quot;
--&amp;gt; 819         &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; self.mapPartitions(lambda i: [sum(1 &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; _ in i)]).sum()
    820
    821     def stats(self):

/Users/meng/src/spark/python/pyspark/rdd.pyc in sum(self)
    808         6.0
    809         &quot;&quot;&quot;
--&amp;gt; 810         &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; self.mapPartitions(lambda x: [sum(x)]).reduce(&lt;span class=&quot;code-keyword&quot;&gt;operator&lt;/span&gt;.add)
    811
    812     def count(self):

/Users/meng/src/spark/python/pyspark/rdd.pyc in reduce(self, f)
    713             yield reduce(f, iterator, initial)
    714
--&amp;gt; 715         vals = self.mapPartitions(func).collect()
    716         &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; vals:
    717             &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; reduce(f, vals)

/Users/meng/src/spark/python/pyspark/rdd.pyc in collect(self)
    674         &quot;&quot;&quot;
    675         with SCCallSiteSync(self.context) as css:
--&amp;gt; 676             bytesInJava = self._jrdd.collect().iterator()
    677         &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; list(self._collect_iterator_through_file(bytesInJava))
    678

/Users/meng/src/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--&amp;gt; 538                 self.target_id, self.name)
    539
    540         &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; temp_arg in temp_args:

/Users/meng/src/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     &lt;span class=&quot;code-quote&quot;&gt;&apos;An error occurred &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; calling {0}{1}{2}.\n&apos;&lt;/span&gt;.
--&amp;gt; 300                     format(target_id, &lt;span class=&quot;code-quote&quot;&gt;&apos;.&apos;&lt;/span&gt;, name), value)
    301             &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt;:
    302                 raise Py4JError(

Py4JJavaError: An error occurred &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; calling o69.collect.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 2, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/Users/meng/src/spark/python/pyspark/worker.py&quot;&lt;/span&gt;, line 107, in main
    process()
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/Users/meng/src/spark/python/pyspark/worker.py&quot;&lt;/span&gt;, line 98, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/Users/meng/src/spark/python/pyspark/serializers.py&quot;&lt;/span&gt;, line 198, in dump_stream
    self.serializer.dump_stream(self._batched(iterator), stream)
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/Users/meng/src/spark/python/pyspark/serializers.py&quot;&lt;/span&gt;, line 81, in dump_stream
    raise NotImplementedError
NotImplementedError

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:137)
	at org.apache.spark.api.python.PythonRDD$$anon$1.&amp;lt;init&amp;gt;(PythonRDD.scala:174)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:96)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply$mcV$sp(PythonRDD.scala:242)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:204)
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$1.apply(PythonRDD.scala:204)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1460)
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:203)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1214)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1203)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1202)
	at scala.collection.mutable.ResizableArray$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1202)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:696)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1420)
	at akka.actor.Actor$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;aroundReceive(Actor.scala:465)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DAGScheduler.scala:1375)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
	at akka.actor.ActorCell.invoke(ActorCell.scala:487)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
	at akka.dispatch.Mailbox.run(Mailbox.scala:220)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="12761596">SPARK-4841</key>
            <summary>Batch serializer bug in PySpark&apos;s RDD.zip</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="davies">Davies Liu</assignee>
                                    <reporter username="mengxr">Xiangrui Meng</reporter>
                        <labels>
                    </labels>
                <created>Sun, 14 Dec 2014 04:12:30 +0000</created>
                <updated>Tue, 24 Feb 2015 18:57:33 +0000</updated>
                            <resolved>Wed, 17 Dec 2014 20:22:55 +0000</resolved>
                                    <version>1.2.0</version>
                                    <fixVersion>1.2.1</fixVersion>
                    <fixVersion>1.3.0</fixVersion>
                                    <component>PySpark</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="14245996" author="mengxr" created="Sun, 14 Dec 2014 17:12:46 +0000"  >&lt;p&gt;This is the commit that caused the bug: 786e75b33f0bc1445bfc289fe4b62407cb79026e (&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-3886&quot; title=&quot;Choose the batch size of serializer based on size of object&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-3886&quot;&gt;&lt;del&gt;SPARK-3886&lt;/del&gt;&lt;/a&gt;)&lt;/p&gt;</comment>
                            <comment id="14247376" author="apachespark" created="Mon, 15 Dec 2014 22:47:07 +0000"  >&lt;p&gt;User &apos;davies&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/3706&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/3706&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14247864" author="joshrosen" created="Tue, 16 Dec 2014 06:59:32 +0000"  >&lt;p&gt;I&apos;ve merged &lt;a href=&quot;https://github.com/apache/spark/pull/3706&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/3706&lt;/a&gt; into master; adding a &lt;tt&gt;backport-needed&lt;/tt&gt; label so that this makes it into 1.2.1.&lt;/p&gt;</comment>
                            <comment id="14250468" author="joshrosen" created="Wed, 17 Dec 2014 20:22:55 +0000"  >&lt;p&gt;I&apos;ve merged this into &lt;tt&gt;branch-1.2&lt;/tt&gt;, so it will be included in Spark 1.2.1.  Since this was the last backport, I&apos;m marking this as Fixed.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12777250">SPARK-5973</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 48 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i23err:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>