<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:36:56 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-12591] NullPointerException using checkpointed mapWithState with KryoSerializer</title>
                <link>https://issues.apache.org/jira/browse/SPARK-12591</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Issue occured after upgrading to the RC4 of Spark (streaming) 1.6.0 to (re)test the new mapWithState API, after previously reporting issue &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-11932&quot; title=&quot;trackStateByKey throws java.lang.IllegalArgumentException: requirement failed on restarting from checkpoint&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-11932&quot;&gt;&lt;del&gt;SPARK-11932&lt;/del&gt;&lt;/a&gt; (&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-11932&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/SPARK-11932&lt;/a&gt;). &lt;/p&gt;

&lt;p&gt;For initial report, see &lt;a href=&quot;http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-streaming-1-6-0-RC4-NullPointerException-using-mapWithState-tt15830.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-streaming-1-6-0-RC4-NullPointerException-using-mapWithState-tt15830.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Narrowed it down to an issue unrelated to Kafka directstream, but, after observing very unpredictable behavior as a result of changes to the Kafka messages format, it seems to be related to KryoSerialization in specific.&lt;/p&gt;

&lt;p&gt;For test case, see my modified version of the StatefulNetworkWordCount example: &lt;a href=&quot;https://gist.github.com/juyttenh/9b4a4103699a7d5f698f&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://gist.github.com/juyttenh/9b4a4103699a7d5f698f&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;To reproduce, use RC4 of Spark-1.6.0 and &lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;start nc:
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;nc -lk 9999&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
	&lt;li&gt;execute the supplied test case:
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;bin/spark-submit --&lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.spark.examples.streaming.StatefulNetworkWordCount --master local[2] file:&lt;span class=&quot;code-comment&quot;&gt;///some-assembly-jar localhost 9999&lt;/span&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Error scenario:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;put some text in the nc console with the job running, and observe correct functioning of the word count&lt;/li&gt;
	&lt;li&gt;kill the spark job&lt;/li&gt;
	&lt;li&gt;add some more text in the nc console (with the job not running)&lt;/li&gt;
	&lt;li&gt;restart the spark job and observe the NPE&lt;br/&gt;
(you might need to repeat this a couple of times to trigger the exception)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Here&apos;s the stacktrace: &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;15/12/31 11:43:47 ERROR Executor: Exception in task 0.0 in stage 4.0 (TID 5)
java.lang.NullPointerException
	at org.apache.spark.streaming.util.OpenHashMapBasedStateMap.get(StateMap.scala:103)
	at org.apache.spark.streaming.util.OpenHashMapBasedStateMap.get(StateMap.scala:111)
	at org.apache.spark.streaming.util.OpenHashMapBasedStateMap.get(StateMap.scala:111)
	at org.apache.spark.streaming.rdd.MapWithStateRDDRecord$$anonfun$updateRecordWithData$1.apply(MapWithStateRDD.scala:56)
	at org.apache.spark.streaming.rdd.MapWithStateRDDRecord$$anonfun$updateRecordWithData$1.apply(MapWithStateRDD.scala:55)
	at scala.collection.Iterator$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(Iterator.scala:727)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.streaming.rdd.MapWithStateRDDRecord$.updateRecordWithData(MapWithStateRDD.scala:55)
	at org.apache.spark.streaming.rdd.MapWithStateRDD.compute(MapWithStateRDD.scala:154)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.streaming.rdd.MapWithStateRDD.compute(MapWithStateRDD.scala:148)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
15/12/31 11:43:47 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 6, localhost, partition 1,NODE_LOCAL, 2239 bytes)
15/12/31 11:43:47 INFO Executor: Running task 1.0 in stage 4.0 (TID 6)
15/12/31 11:43:47 WARN TaskSetManager: Lost task 0.0 in stage 4.0 (TID 5, localhost): java.lang.NullPointerException
	at org.apache.spark.streaming.util.OpenHashMapBasedStateMap.get(StateMap.scala:103)
	at org.apache.spark.streaming.util.OpenHashMapBasedStateMap.get(StateMap.scala:111)
	at org.apache.spark.streaming.util.OpenHashMapBasedStateMap.get(StateMap.scala:111)
	at org.apache.spark.streaming.rdd.MapWithStateRDDRecord$$anonfun$updateRecordWithData$1.apply(MapWithStateRDD.scala:56)
	at org.apache.spark.streaming.rdd.MapWithStateRDDRecord$$anonfun$updateRecordWithData$1.apply(MapWithStateRDD.scala:55)
	at scala.collection.Iterator$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(Iterator.scala:727)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.streaming.rdd.MapWithStateRDDRecord$.updateRecordWithData(MapWithStateRDD.scala:55)
	at org.apache.spark.streaming.rdd.MapWithStateRDD.compute(MapWithStateRDD.scala:154)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.streaming.rdd.MapWithStateRDD.compute(MapWithStateRDD.scala:148)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)

15/12/31 11:43:47 ERROR TaskSetManager: Task 0 in stage 4.0 failed 1 times; aborting job
15/12/31 11:43:47 INFO TaskSchedulerImpl: Cancelling stage 4
15/12/31 11:43:47 INFO Executor: Executor is trying to kill task 1.0 in stage 4.0 (TID 6)
15/12/31 11:43:47 INFO TaskSchedulerImpl: Stage 4 was cancelled
15/12/31 11:43:47 INFO DAGScheduler: ResultStage 4 (foreachPartition at StatefulNetworkCountKryo.scala:72) failed in 0.100 s
15/12/31 11:43:47 INFO DAGScheduler: Job 1 failed: foreachPartition at StatefulNetworkCountKryo.scala:72, took 0.823996 s
15/12/31 11:43:47 INFO JobScheduler: Finished job streaming job 1451558610000 ms.0 from job set of time 1451558610000 ms
15/12/31 11:43:47 INFO JobScheduler: Total delay: 17.994 s &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; time 1451558610000 ms (execution: 0.857 s)
15/12/31 11:43:47 INFO JobScheduler: Starting job streaming job 1451558620000 ms.0 from job set of time 1451558620000 ms
15/12/31 11:43:47 ERROR JobScheduler: Error running job streaming job 1451558610000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 5, localhost): java.lang.NullPointerException
	at org.apache.spark.streaming.util.OpenHashMapBasedStateMap.get(StateMap.scala:103)
	at org.apache.spark.streaming.util.OpenHashMapBasedStateMap.get(StateMap.scala:111)
	at org.apache.spark.streaming.util.OpenHashMapBasedStateMap.get(StateMap.scala:111)
	at org.apache.spark.streaming.rdd.MapWithStateRDDRecord$$anonfun$updateRecordWithData$1.apply(MapWithStateRDD.scala:56)
	at org.apache.spark.streaming.rdd.MapWithStateRDDRecord$$anonfun$updateRecordWithData$1.apply(MapWithStateRDD.scala:55)
	at scala.collection.Iterator$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(Iterator.scala:727)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.streaming.rdd.MapWithStateRDDRecord$.updateRecordWithData(MapWithStateRDD.scala:55)
	at org.apache.spark.streaming.rdd.MapWithStateRDD.compute(MapWithStateRDD.scala:154)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.streaming.rdd.MapWithStateRDD.compute(MapWithStateRDD.scala:148)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:920)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:918)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:918)
	at org.apache.spark.examples.streaming.StatefulNetworkWordCountKryo$$anonfun$createContext$2.apply(StatefulNetworkCountKryo.scala:72)
	at org.apache.spark.examples.streaming.StatefulNetworkWordCountKryo$$anonfun$createContext$2.apply(StatefulNetworkCountKryo.scala:71)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:661)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:661)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:426)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:49)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:49)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:49)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:224)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:224)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:224)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:223)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
Caused by: java.lang.NullPointerException
	at org.apache.spark.streaming.util.OpenHashMapBasedStateMap.get(StateMap.scala:103)
	at org.apache.spark.streaming.util.OpenHashMapBasedStateMap.get(StateMap.scala:111)
	at org.apache.spark.streaming.util.OpenHashMapBasedStateMap.get(StateMap.scala:111)
	at org.apache.spark.streaming.rdd.MapWithStateRDDRecord$$anonfun$updateRecordWithData$1.apply(MapWithStateRDD.scala:56)
	at org.apache.spark.streaming.rdd.MapWithStateRDDRecord$$anonfun$updateRecordWithData$1.apply(MapWithStateRDD.scala:55)
	at scala.collection.Iterator$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(Iterator.scala:727)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.streaming.rdd.MapWithStateRDDRecord$.updateRecordWithData(MapWithStateRDD.scala:55)
	at org.apache.spark.streaming.rdd.MapWithStateRDD.compute(MapWithStateRDD.scala:154)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.streaming.rdd.MapWithStateRDD.compute(MapWithStateRDD.scala:148)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	... 3 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment>&lt;p&gt;MacOSX&lt;br/&gt;
Java(TM) SE Runtime Environment (build 1.8.0_20-ea-b17)&lt;/p&gt;</environment>
        <key id="12925027">SPARK-12591</key>
            <summary>NullPointerException using checkpointed mapWithState with KryoSerializer</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="zsxwing">Shixiong Zhu</assignee>
                                    <reporter username="juyttenh">Jan Uyttenhove</reporter>
                        <labels>
                    </labels>
                <created>Thu, 31 Dec 2015 11:24:25 +0000</created>
                <updated>Mon, 11 Apr 2016 17:25:42 +0000</updated>
                            <resolved>Fri, 8 Jan 2016 03:54:29 +0000</resolved>
                                    <version>1.6.0</version>
                                    <fixVersion>1.6.1</fixVersion>
                    <fixVersion>2.0.0</fixVersion>
                                    <component>DStreams</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>7</watches>
                                                                                                                <comments>
                            <comment id="15084101" author="apachespark" created="Tue, 5 Jan 2016 23:58:05 +0000"  >&lt;p&gt;User &apos;zsxwing&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/10609&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/10609&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15084412" author="zsxwing" created="Wed, 6 Jan 2016 00:07:53 +0000"  >&lt;p&gt;Ah, &lt;tt&gt;org.apache.spark.streaming.util.OpenHashMapBasedStateMap&lt;/tt&gt; doesn&apos;t support Kryo. I submitted a PR to fix it.&lt;/p&gt;

&lt;p&gt;Right now you need to use KryoRegistrator to register OpenHashMapBasedStateMap by yourself. E.g.,&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;   &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;CustomRegistrator &lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; KryoRegistrator {
      override def registerClasses(kryo: Kryo) {
        kryo.register(
          &lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;.forName(&lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.spark.streaming.util.OpenHashMapBasedStateMap&quot;&lt;/span&gt;),
          &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; com.esotericsoftware.kryo.serializers.JavaSerializer())
      }
    }

    sparkConf.set(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.kryo.registrator&quot;&lt;/span&gt;, classOf[CustomRegistrator].getName)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15088633" author="apachespark" created="Fri, 8 Jan 2016 03:11:05 +0000"  >&lt;p&gt;User &apos;zsxwing&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/10656&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/10656&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15118675" author="lin@exabeam.com" created="Wed, 27 Jan 2016 05:15:03 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=shixiong%40databricks.com&quot; class=&quot;user-hover&quot; rel=&quot;shixiong@databricks.com&quot;&gt;shixiong@databricks.com&lt;/a&gt;, &lt;/p&gt;

&lt;p&gt;I encountered this issue and tried your workaround but encountered this error. LiveSequenceMergeTracker is a case class used as a state for mapWithState.&lt;/p&gt;

&lt;p&gt;Although the error says &quot;ClassNotFound&quot;, the class is clearly defined. Any idea what may be causing this?&lt;/p&gt;

&lt;p&gt;Server is tag 1.6.0 built with scala 2.11&lt;/p&gt;


&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;com.esotericsoftware.kryo.KryoException: Error during Java deserialization.
Serialization trace:
stateMap (org.apache.spark.streaming.rdd.MapWithStateRDDRecord)
        at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:42)
        at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:648)
        at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read(FieldSerializer.java:605)
        at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:221)
        at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
        at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:228)
        at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:181)
        at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
        at org.apache.spark.streaming.rdd.MapWithStateRDD.compute(MapWithStateRDD.scala:153)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
        at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
        at org.apache.spark.scheduler.Task.run(Task.scala:89)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassNotFoundException: com.exabeam.martini.extractions.LiveSequenceMergeTracker
        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15118687" author="zsxwing" created="Wed, 27 Jan 2016 05:25:57 +0000"  >&lt;p&gt;Which mode are you using? Is it work when disabling Kryo?&lt;/p&gt;</comment>
                            <comment id="15118705" author="lin@exabeam.com" created="Wed, 27 Jan 2016 06:00:54 +0000"  >&lt;p&gt;yarn-client mode and kyroserializer. Our events are kryo ready and we cannot disable kryo.&lt;/p&gt;</comment>
                            <comment id="15118724" author="lin@exabeam.com" created="Wed, 27 Jan 2016 06:17:33 +0000"  >&lt;p&gt;I want to add that this error happens very very rarely. One in a million or something. &lt;/p&gt;

&lt;p&gt;Will the fix in the pull request avoid using JavaSerializer for OpenHashMapBasedStateMap and avoid this issue?&lt;/p&gt;</comment>
                            <comment id="15118774" author="zsxwing" created="Wed, 27 Jan 2016 07:09:52 +0000"  >&lt;p&gt;That&apos;s weird. If it doesn&apos;t always happen, I guess it may be an issue in Spark&apos;s class loader.&lt;/p&gt;

&lt;p&gt;After the fix, OpenHashMapBasedStateMap will implement KryoSerializable to support Kryo.&lt;/p&gt;</comment>
                            <comment id="15119888" author="zsxwing" created="Wed, 27 Jan 2016 17:56:54 +0000"  >&lt;p&gt;By the way, just want to confirm that: the error doesn&apos;t always happen for the batches that need to be checkpointed, and the checkpoint just sometimes fails. Right?&lt;/p&gt;</comment>
                            <comment id="15119907" author="lin@exabeam.com" created="Wed, 27 Jan 2016 18:09:58 +0000"  >&lt;p&gt;I&apos;m not sure of the timing of the checkpointing. The error seems to affect several early batches then go away. Not sure what to make of it. Attached screenshot of the streaming page.&lt;/p&gt;</comment>
                            <comment id="15119987" author="lin@exabeam.com" created="Wed, 27 Jan 2016 19:09:19 +0000"  >&lt;p&gt;I patched the server with branch-1.6, reverted the workaround and the error doesn&apos;t happen again.&lt;/p&gt;</comment>
                            <comment id="15120023" author="zsxwing" created="Wed, 27 Jan 2016 19:30:42 +0000"  >&lt;p&gt;Where is LiveSequenceMergeTracker, in a separate jar or the application jar? I tried both and didn&apos;t fail in yarn-client or yarn-cluster mode. It&apos;s great to hear that it worked with branch-1.6 &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15136208" author="yuval.itzchakov" created="Sun, 7 Feb 2016 10:03:19 +0000"  >&lt;p&gt;I&apos;m seeing this error as well running Spark 1.6.0 with KryoSerializer.&lt;/p&gt;

&lt;p&gt;This error started happening after I reset a spark-worker node while a job is running. This hasn&apos;t happened previously and doesn&apos;t occur unless I restart a worker node.&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zsxwing&quot; class=&quot;user-hover&quot; rel=&quot;zsxwing&quot;&gt;zsxwing&lt;/a&gt; - I&apos;m assuming this patch needs to be applied until 1.6.1, am I right?&lt;/p&gt;

&lt;p&gt;I&apos;ve tried registering the kryo class via `SparkConf.registerKryoClasses`, and I&apos;m still seeing this exception:&lt;/p&gt;

&lt;p&gt;    val sparkConf = new SparkConf().setAppName(SparkVariables.AppName)&lt;br/&gt;
      .setMaster(masterUri)&lt;br/&gt;
      .setJars(jars.toArray&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;(new Array&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;(jars.size())))&lt;br/&gt;
      .set(&quot;spark.serializer&quot;, serializer)&lt;br/&gt;
      .set(&quot;spark.kryoserializer.buffer.max&quot;, serializerMaxBuffer)&lt;br/&gt;
      .registerKryoClasses(Array(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;WordProcessor&amp;#93;&lt;/span&gt;, classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Message&amp;#93;&lt;/span&gt;, classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;JavaSerializer&amp;#93;&lt;/span&gt;))&lt;/p&gt;
</comment>
                            <comment id="15136519" author="zsxwing" created="Sun, 7 Feb 2016 23:26:31 +0000"  >&lt;p&gt;Yes. You need to apply this patch for 1.6.0 by yourself. For a workaround, see my comment here: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-12591?focusedCommentId=15084412&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15084412&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/SPARK-12591?focusedCommentId=15084412&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15084412&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15195508" author="yuval.itzchakov" created="Tue, 15 Mar 2016 15:41:29 +0000"  >&lt;p&gt;After applying the fix, I&apos;m still seeing errors related to the class loader saying a type doesn&apos;t exist, although I verified it&apos;s there in the JAR:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;com.esotericsoftware.kryo.KryoException: Error during Java deserialization.
Serialization trace:
stateMap (org.apache.spark.streaming.rdd.MapWithStateRDDRecord)
	at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:42)

Caused by: java.lang.ClassNotFoundException: com.***
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.loadClass(&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.loadClass(&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.java:357)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;.forName0(Native Method)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;.forName(&lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:626)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1613)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1518)
	at java.io.ObjectInputStream.readClass(ObjectInputStream.java:1484)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1334)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:501)
	at org.apache.spark.streaming.util.OpenHashMapBasedStateMap.readObject(StateMap.scala:268)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1900)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
	at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:40)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I&apos;m still runnning Spark 1.6.0, but the fix didn&apos;t help with this, although it did solve the NPE. This happens everytime the data is being checkpointed.&lt;/p&gt;</comment>
                            <comment id="15234766" author="kerkero" created="Mon, 11 Apr 2016 08:59:05 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zsxwing&quot; class=&quot;user-hover&quot; rel=&quot;zsxwing&quot;&gt;zsxwing&lt;/a&gt; - I&apos;m running Spark &lt;b&gt;1.6.1&lt;/b&gt; with Scala 2.11 and getting the following exception when using &lt;tt&gt;spark.kryo.registrationRequired&lt;/tt&gt;   :&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;java.lang.IllegalArgumentException: &lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt; is not registered: org.apache.spark.streaming.rdd.MapWithStateRDDRecord
Note: To register &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;use: kryo.register(org.apache.spark.streaming.rdd.MapWithStateRDDRecord.class);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Tried to register it manually like you suggested, with:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;kryo.register(&lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;.forName(&lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.spark.streaming.rdd.MapWithStateRDDRecord&quot;&lt;/span&gt;),&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; KryoJavaSerializer())
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;But then I get &lt;tt&gt;ClassNotFoundException&lt;/tt&gt; for my application classes that are stored in state:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;com.esotericsoftware.kryo.KryoException: Error during Java deserialization.
	at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:42)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:228)
	at ....
Caused by: java.lang.ClassNotFoundException: com.****
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.loadClass(&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.loadClass(&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.java:357)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;.forName0(Native Method)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;.forName(&lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:626)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1613)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1518)
	at java.io.ObjectInputStream.readClass(ObjectInputStream.java:1484)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1334)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:501)
	at org.apache.spark.streaming.util.OpenHashMapBasedStateMap.readObject(StateMap.scala:322)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15234880" author="yuval.itzchakov" created="Mon, 11 Apr 2016 11:03:55 +0000"  >&lt;p&gt;This workaround is already builtin Spark 1.6.1, there&apos;s no need to implement the workaround yourself. Were you seeing the problem prior to patching your code with this fix on Spark 1.6.1?&lt;/p&gt;</comment>
                            <comment id="15235051" author="kerkero" created="Mon, 11 Apr 2016 12:59:08 +0000"  >&lt;p&gt;I also thought that Spark 1.6.1 should support Kryo serialization for mapWithState classes, but when I got &lt;tt&gt;Class is not registered&lt;/tt&gt; error for &lt;tt&gt;MapWithStateRDDRecord&lt;/tt&gt; I tried to register it manually with the patch, and the outcome was &lt;tt&gt;ClassNotFoundException&lt;/tt&gt; as you can see.&lt;br/&gt;
I suspecting this issue was only partially fixed.&lt;/p&gt;</comment>
                            <comment id="15235170" author="yuval.itzchakov" created="Mon, 11 Apr 2016 14:30:39 +0000"  >&lt;p&gt;That&apos;s weird, I&apos;m using mapWithState with Kryo and I don&apos;t see any class is not registered exception. From your stacktrace it looks like your own class isn&apos;t registered:&lt;/p&gt;

&lt;p&gt;Caused by: java.lang.ClassNotFoundException: com.****&lt;/p&gt;

&lt;p&gt;Are you sure you registered all custom classes properly with Kryo?&lt;/p&gt;</comment>
                            <comment id="15235204" author="kerkero" created="Mon, 11 Apr 2016 14:47:25 +0000"  >&lt;p&gt;I&apos;m sure. I also found the class that was specified on the exception in the spark.kryo.classesToRegister env param.&lt;/p&gt;</comment>
                            <comment id="15235239" author="yuval.itzchakov" created="Mon, 11 Apr 2016 15:08:30 +0000"  >&lt;p&gt;Try registering your classes via `SparkConf.registerKryoClasses` method. Does it still throw then?&lt;/p&gt;</comment>
                            <comment id="15235262" author="kerkero" created="Mon, 11 Apr 2016 15:14:44 +0000"  >&lt;p&gt;I am using &lt;tt&gt;SparkConf.registerKryoClasses&lt;/tt&gt; for my classes (which puts the class names into that env param)&lt;br/&gt;
BTW - should I register also parent classes if I have some class hierarchy?&lt;/p&gt;</comment>
                            <comment id="15235286" author="yuval.itzchakov" created="Mon, 11 Apr 2016 15:27:40 +0000"  >&lt;p&gt;No, there&apos;s no need to register parent classes. Only to make sure that if they have serialized fields, they should extend Serializable. I don&apos;t think the issue is related to Kryo serialization, I think there&apos;s a problem with the distribution of your JAR, as it can&apos;t find your class at runtime. &lt;/p&gt;</comment>
                            <comment id="15235294" author="yuval.itzchakov" created="Mon, 11 Apr 2016 15:30:01 +0000"  >&lt;p&gt;Send me a DM on twitter if you need additional help @YuvalItzchakov&lt;/p&gt;</comment>
                            <comment id="15235538" author="zsxwing" created="Mon, 11 Apr 2016 17:25:42 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=kerkero&quot; class=&quot;user-hover&quot; rel=&quot;kerkero&quot;&gt;kerkero&lt;/a&gt; mapWithState doesn&apos;t work with &quot;spark.kryo.registrationRequired = true&quot;. This is because Streaming cannot add its class into Core since Core doesn&apos;t depend on Streaming. Theforefore, Spark Streaming cannot do the registration. However, in 1.6.1, mapWithState should work correctly as it implements &quot;KryoSerializable&quot;, so you can turn off &quot;spark.kryo.registrationRequired&quot;. &lt;/p&gt;

&lt;p&gt;I see you may want all of your classes are registered to make sure the Kryo serialization work correctly, then you can just write some unit tests that only test the Kryo serialization of your classes with &quot;spark.kryo.registrationRequired = true&quot;.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12784689" name="Screen Shot 2016-01-27 at 10.09.18 AM.png" size="223320" author="lin@exabeam.com" created="Wed, 27 Jan 2016 18:10:41 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 32 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2qfjb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>