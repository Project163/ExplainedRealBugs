<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:21:55 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-4835] Streaming saveAs*HadoopFiles() methods may throw FileAlreadyExistsException during checkpoint recovery</title>
                <link>https://issues.apache.org/jira/browse/SPARK-4835</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;While running (a slightly modified version of) the &quot;recovery with saveAsHadoopFiles operation&quot; test in the streaming CheckpointSuite, I noticed the following error message in the streaming driver log:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;14/12/12 17:42:50.687 pool-1-thread-1-ScalaTest-running-CheckpointSuite INFO JobScheduler: Added jobs &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; time 1500 ms
14/12/12 17:42:50.687 pool-1-thread-1-ScalaTest-running-CheckpointSuite INFO RecurringTimer: Started timer &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; JobGenerator at time 2000
14/12/12 17:42:50.688 sparkDriver-akka.actor.&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;-dispatcher-3 INFO JobScheduler: Starting job streaming job 1500 ms.0 from job set of time 1500 ms
14/12/12 17:42:50.688 pool-1-thread-1-ScalaTest-running-CheckpointSuite INFO JobGenerator: Restarted JobGenerator at 2000 ms
14/12/12 17:42:50.688 pool-1-thread-1-ScalaTest-running-CheckpointSuite INFO JobScheduler: Started JobScheduler
14/12/12 17:42:50.689 sparkDriver-akka.actor.&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;-dispatcher-3 INFO JobScheduler: Starting job streaming job 1500 ms.1 from job set of time 1500 ms
14/12/12 17:42:50.689 sparkDriver-akka.actor.&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;-dispatcher-3 ERROR JobScheduler: Error running job streaming job 1500 ms.0
org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/folders/0k/2qp2p2vs7bv033vljnb8nk1c0000gn/T/1418434967213-0/-1500.result already exists
	at org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:121)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1045)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:944)
	at org.apache.spark.streaming.dstream.PairDStreamFunctions$$anonfun$9.apply(PairDStreamFunctions.scala:677)
	at org.apache.spark.streaming.dstream.PairDStreamFunctions$$anonfun$9.apply(PairDStreamFunctions.scala:675)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:42)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:32)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:171)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
14/12/12 17:42:50.691 pool-12-thread-1 INFO SparkContext: Starting job: apply at Transformer.scala:22
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Spark Streaming&apos;s &lt;tt&gt;saveAsHadoopFiles&lt;/tt&gt; method calls Spark&apos;s &lt;tt&gt;rdd.saveAsHadoopFile&lt;/tt&gt; method.  The Spark method, in turn, called &lt;tt&gt;PairRDDFunctions.saveAsHadoopDataset()&lt;/tt&gt;, which has error-checking to ensure that the output directory does not already exist:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (self.conf.getBoolean(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.hadoop.validateOutputSpecs&quot;&lt;/span&gt;, &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)) {
      &lt;span class=&quot;code-comment&quot;&gt;// FileOutputFormat ignores the filesystem parameter
&lt;/span&gt;      val ignoredFs = FileSystem.get(hadoopConf)
      hadoopConf.getOutputFormat.checkOutputSpecs(ignoredFs, hadoopConf)
    }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If Spark Streaming recovers from a checkpoint and re-runs the last batch in the checkpoint, then &lt;tt&gt;saveAsHadoopDataset&lt;/tt&gt; will have been called twice with the same output path.  If the output path exists from the first, pre-recovery run, then the recovery will fail.&lt;/p&gt;

&lt;p&gt;This seems like it could be a pretty serious issue: imagine that a streaming job fails partway through a save() operation, then recovers: in this case, the existing directory will prevent us from ever recovering and finishing the save().&lt;/p&gt;

&lt;p&gt;Fortunately, this should be simple to fix: we should disable the existing directory checks for output operations called by streaming jobs.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12761518">SPARK-4835</key>
            <summary>Streaming saveAs*HadoopFiles() methods may throw FileAlreadyExistsException during checkpoint recovery</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="tdas">Tathagata Das</assignee>
                                    <reporter username="joshrosen">Josh Rosen</reporter>
                        <labels>
                    </labels>
                <created>Sat, 13 Dec 2014 01:55:15 +0000</created>
                <updated>Tue, 3 Feb 2015 05:47:18 +0000</updated>
                            <resolved>Mon, 5 Jan 2015 04:28:40 +0000</resolved>
                                    <version>1.3.0</version>
                                    <fixVersion>1.2.1</fixVersion>
                    <fixVersion>1.3.0</fixVersion>
                                    <component>DStreams</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="14245218" author="joshrosen" created="Sat, 13 Dec 2014 05:44:56 +0000"  >&lt;p&gt;One subtlety here: we probably shouldn&apos;t rely on the SparkConf having &lt;tt&gt;spark.hadoop.validateOutputSpecs&lt;/tt&gt; set to &lt;tt&gt;false&lt;/tt&gt;, since this is the SparkContext&apos;s &lt;tt&gt;conf&lt;/tt&gt; and that context might be shared with other non-streaming jobs / tasks.  We also shouldn&apos;t mutate it, since, in general, mutating SparkConf is a serious anti-pattern (even in internal code).&lt;/p&gt;

&lt;p&gt;Instead, we could add some plumbing so that every &lt;tt&gt;saveAs*&lt;/tt&gt; RDD method accepts an optional parameter to disable output spec validation.  This solution also kind of messy, though, since it ends up touching a lot of code: if we don&apos;t change this everywhere, then there&apos;s the possibility that we&apos;ll miss some corner-case and re-introduce the bug.&lt;/p&gt;

&lt;p&gt;Instead, we might be able to use DynamicVariable to dynamically bypass the output spec checking for jobs that are launched by the streaming scheduler.  This would have a somewhat minimal impact on the source code and would avoid merge-conflict hell when backporting this code, but might be hard to understand.  However, this might be nicer from a user-facing point-of-view since we wouldn&apos;t end up cluttering up the &lt;tt&gt;saveAs*&lt;/tt&gt; methods with a &lt;tt&gt;bypassOutputSpecValidation&lt;/tt&gt; boolean that&apos;s only used by streaming.&lt;/p&gt;</comment>
                            <comment id="14245250" author="joshrosen" created="Sat, 13 Dec 2014 08:25:46 +0000"  >&lt;p&gt;Alright, pushed an experimental version of the &lt;tt&gt;DynamicVariable&lt;/tt&gt; approach as part of my streaming test flakiness PR: &lt;a href=&quot;https://github.com/apache/spark/pull/3687&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/3687&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here&apos;s the actual commit: &lt;a href=&quot;https://github.com/JoshRosen/spark/commit/3db335f01e01986c412ae2a1de6fbe6b8c3a7a32&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/JoshRosen/spark/commit/3db335f01e01986c412ae2a1de6fbe6b8c3a7a32&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14245267" author="tdas" created="Sat, 13 Dec 2014 10:07:08 +0000"  >&lt;p&gt;I wonder whether there is a semantically cleaner way of doing this. In streaming we know which batches might have been already processed earlier because we store in the checkpoint the batches that were generated and queued. So upon recovery, the validation could be disabled only for those batches that were queue before failure, and not any of the subsequent batches. &lt;br/&gt;
What do you think?&lt;/p&gt;</comment>
                            <comment id="14245437" author="joshrosen" created="Sat, 13 Dec 2014 17:36:12 +0000"  >&lt;p&gt;I thought about the &quot;figure out whether the batch has already been processed&quot; approach, but I was worried about various types of partial failure.  The processing of a batch might involve an arbitrary number of Spark jobs, so it seems like it could be possible that an entire &lt;tt&gt;saveAs*&lt;/tt&gt; operation succeeds even though the batch itself is marked as a failure during a crash.  Also, I was worried about partial failure of the &lt;tt&gt;saveAs*&lt;/tt&gt; call itself: is it possible for half of the partitions&apos; saving to succeed before the crash such that the output directory is created but half-full?  Or is the output directory created only once the entire output operation has succeeded and committed (e.g. is success / failure atomic from a directory-existence point-of-view)?&lt;/p&gt;</comment>
                            <comment id="14258547" author="tdas" created="Wed, 24 Dec 2014 21:39:45 +0000"  >&lt;p&gt;That is a very good point. Lets brainstorm on this offline once I get to your PR.&lt;/p&gt;</comment>
                            <comment id="14260654" author="apachespark" created="Tue, 30 Dec 2014 01:31:31 +0000"  >&lt;p&gt;User &apos;JoshRosen&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/3832&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/3832&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14260660" author="joshrosen" created="Tue, 30 Dec 2014 01:41:38 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tdas&quot; class=&quot;user-hover&quot; rel=&quot;tdas&quot;&gt;tdas&lt;/a&gt;,&lt;/p&gt;

&lt;p&gt;After some more thought, I think that the &quot;figure out which batch has already been processed&quot; approach might actually be fine in those partial-failure cases, since there should be a record in the checkpoint that says whether a batch has successfully completed.  I think the crux of the problem is that a job might complete successfully but this might not be reflected in the checkpoint, so we&apos;d &lt;em&gt;think&lt;/em&gt; that it was the first time that we&apos;ve seen a batch even thought it was already processed.&lt;/p&gt;

&lt;p&gt;I suppose that we could work around this if we used a write-ahead log to store a record before starting to process a batch, which would avoid this issue.  Can we do this with the existing WAL machinery?  If we already have this functionality, then I think we should use that.&lt;/p&gt;</comment>
                            <comment id="14260675" author="joshrosen" created="Tue, 30 Dec 2014 02:09:34 +0000"  >&lt;p&gt;I guess what we&apos;d really want is something akin to a &quot;batch attempt&quot; number, but it doesn&apos;t look like the current WAL gives us this.&lt;/p&gt;</comment>
                            <comment id="14262388" author="tdas" created="Wed, 31 Dec 2014 19:49:45 +0000"  >&lt;p&gt;Lets keep this simple for now, just disable the validity checks for streaming by default. We can look into this later. &lt;/p&gt;</comment>
                            <comment id="14264172" author="tdas" created="Mon, 5 Jan 2015 04:29:43 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=joshrosen&quot; class=&quot;user-hover&quot; rel=&quot;joshrosen&quot;&gt;joshrosen&lt;/a&gt; Even if this resolved, there probably should be another JIRA opened regarding the spark.streaming.hadoop.validate , isnt it?&lt;/p&gt;</comment>
                            <comment id="14264176" author="joshrosen" created="Mon, 5 Jan 2015 04:42:20 +0000"  >&lt;p&gt;Yeah, let&apos;s open a second followup JIRA for that.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="12771913">SPARK-5545</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12771913">SPARK-5545</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 46 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i23eaf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>