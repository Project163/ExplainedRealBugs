<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 19:07:18 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-21492] Memory leak in SortMergeJoin</title>
                <link>https://issues.apache.org/jira/browse/SPARK-21492</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;In SortMergeJoin, if the iterator is not exhausted, there will be memory leak caused by the Sort. The memory is not released until the task end, and cannot be used by other operators causing performance drop or OOM.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13088904">SPARK-21492</key>
            <summary>Memory leak in SortMergeJoin</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="XuanYuan">Yuanjian Li</assignee>
                                    <reporter username="zhzhan">Zhan Zhang</reporter>
                        <labels>
                            <label>pull-request-available</label>
                    </labels>
                <created>Thu, 20 Jul 2017 21:34:28 +0000</created>
                <updated>Thu, 4 Jan 2024 00:31:15 +0000</updated>
                            <resolved>Tue, 22 Oct 2019 11:09:05 +0000</resolved>
                                    <version>2.2.0</version>
                    <version>2.3.0</version>
                    <version>2.3.1</version>
                    <version>3.0.0</version>
                                    <fixVersion>2.4.5</fixVersion>
                    <fixVersion>3.0.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>3</votes>
                                    <watches>18</watches>
                                                                                                                <comments>
                            <comment id="16095418" author="apachespark" created="Thu, 20 Jul 2017 21:39:03 +0000"  >&lt;p&gt;User &apos;zhzhan&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/18694&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/18694&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16095425" author="zhzhan" created="Thu, 20 Jul 2017 21:43:05 +0000"  >&lt;p&gt;root cause: In the SortMergeJoin, inner/leftOuter/rightOuter, one side of the SortedIter may not exhausted, that chunk of the memory thus cannot be released, causing memory leak and performance degradtion.&lt;/p&gt;</comment>
                            <comment id="16212522" author="bmscicho" created="Fri, 20 Oct 2017 11:54:45 +0000"  >&lt;p&gt;Here&apos;s a script that exposes memory leak during SortMergeJoin in Spark 2.2.0, maybe it will be helpful.&lt;br/&gt;
Memory leak happens when the following code is executed in spark-shell (a local one). &lt;tt&gt;--conf spark.sql.autoBroadcastJoinThreshold=-1&lt;/tt&gt; may be needed to ensure proper join type.&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;import org.apache.spark.sql.types._
import org.apache.spark.sql._
import org.apache.spark.sql.functions._

val table1Key = &quot;t1_key&quot;
val table1Value = &quot;t1_value&quot;

val table2Key = &quot;t2_key&quot;
val table2Value = &quot;t2_value&quot;

val table1Schema = StructType(List(
    StructField(table1Key, IntegerType),
    StructField(table1Value, DoubleType)
));

val table2Schema = StructType(List(
    StructField(table2Key, IntegerType),
    StructField(table2Value, DoubleType)
));

val table1 = spark.sqlContext.createDataFrame(
    rowRDD = spark.sparkContext.parallelize(Seq(
        Row(1, 2.0)
    )),
    schema = table1Schema
);

val table2 = spark.sqlContext.createDataFrame(
    rowRDD = spark.sparkContext.parallelize(Seq(
        Row(1, 4.0)
    )),
    schema = table2Schema
);


val t1 = table1.repartition(col(table1Key)).groupBy(table1Key).avg()
val t2 = table2.repartition(col(table2Key)).groupBy(table2Key).avg()

val joinedDF = t1 join t2 where t1(table1Key) === t2(table2Key)

joinedDF.explain()
// == Physical Plan ==
// *SortMergeJoin [t1_key#2], [t2_key#9], Inner
// :- *Sort [t1_key#2 ASC NULLS FIRST], false, 0
// :  +- *HashAggregate(keys=[t1_key#2], functions=[avg(cast(t1_key#2 as bigint)), avg(t1_value#3)])
// :     +- *HashAggregate(keys=[t1_key#2], functions=[partial_avg(cast(t1_key#2 as bigint)), partial_avg(t1_value#3)])
// :        +- Exchange hashpartitioning(t1_key#2, 200)
// :           +- *Filter isnotnull(t1_key#2)
// :              +- Scan ExistingRDD[t1_key#2,t1_value#3]
// +- *Sort [t2_key#9 ASC NULLS FIRST], false, 0
//    +- *HashAggregate(keys=[t2_key#9], functions=[avg(cast(t2_key#9 as bigint)), avg(t2_value#10)])
//       +- *HashAggregate(keys=[t2_key#9], functions=[partial_avg(cast(t2_key#9 as bigint)), partial_avg(t2_value#10)])
//          +- Exchange hashpartitioning(t2_key#9, 200)
//             +- *Filter isnotnull(t2_key#9)
//                +- Scan ExistingRDD[t2_key#9,t2_value#10]

joinedDF.show()
// The &apos;show&apos; action yields a lot of:
// 17/10/19 08:17:39 WARN executor.Executor: Managed memory leak detected; size = 4194304 bytes, TID = 8
// 17/10/19 08:17:39 WARN executor.Executor: Managed memory leak detected; size = 4194304 bytes, TID = 9
// 17/10/19 08:17:39 WARN executor.Executor: Managed memory leak detected; size = 4194304 bytes, TID = 11
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="16763820" author="taoluo" created="Fri, 8 Feb 2019 18:57:55 +0000"  >&lt;p&gt;If SortMergeJoinScanner doesn&apos;t consume the iterator from UnsafeExternalRowSorter entirely, the memory that UnsafeExternalSorter acquired from TaskMemoryManager will never be released. This leads to a memory leak, spills, and OOME. A page will be held per partition of the unused iterator.&lt;/p&gt;



&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
from pyspark.sql.functions &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; rand, col

spark.conf.set(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.sql.join.preferSortMergeJoin&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&quot;&lt;/span&gt;)
spark.conf.set(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.sql.autoBroadcastJoinThreshold&quot;&lt;/span&gt;, -1)

r1 = spark.range(1, 1001).select(col(&lt;span class=&quot;code-quote&quot;&gt;&quot;id&quot;&lt;/span&gt;).alias(&lt;span class=&quot;code-quote&quot;&gt;&quot;timestamp1&quot;&lt;/span&gt;))
r1 = r1.withColumn(&lt;span class=&quot;code-quote&quot;&gt;&apos;value&apos;&lt;/span&gt;, rand())
r2 = spark.range(1000, 2001).select(col(&lt;span class=&quot;code-quote&quot;&gt;&quot;id&quot;&lt;/span&gt;).alias(&lt;span class=&quot;code-quote&quot;&gt;&quot;timestamp2&quot;&lt;/span&gt;))
r2 = r2.withColumn(&lt;span class=&quot;code-quote&quot;&gt;&apos;value2&apos;&lt;/span&gt;, rand())
joined = r1.join(r2, r1.timestamp1 == r2.timestamp2, &lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-keyword&quot;&gt;inner&lt;/span&gt;&quot;&lt;/span&gt;)
joined = joined.coalesce(1)
joined.explain()
joined.show()&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;br/&gt;
&lt;tt&gt;== Physical Plan == Coalesce 1 &lt;ins&gt;- *(5) SortMergeJoin&#160;&lt;a href=&quot;#52L&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;timestamp1#52L&lt;/a&gt;,&#160;&lt;a href=&quot;#59L&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;timestamp2#59L&lt;/a&gt;, Inner :- *(2) Sort&#160;&lt;a href=&quot;#52L ASC NULLS FIRST&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;timestamp1#52L ASC NULLS FIRST&lt;/a&gt;, false, 0 : +- Exchange hashpartitioning(timestamp1#52L, 200) : +- *(1) Project&#160;&lt;a href=&quot;#50L AS timestamp1#52L, rand(-4732263137869282482) AS value#54&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;id#50L AS timestamp1#52L, rand(-4732263137869282482) AS value#54&lt;/a&gt;&#160;: +- *(1) Range (1, 1001, step=1, splits=4) +- *(4) Sort&#160;&lt;a href=&quot;#59L ASC NULLS FIRST&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;timestamp2#59L ASC NULLS FIRST&lt;/a&gt;, false, 0 +- Exchange hashpartitioning(timestamp2#59L, 200) +- *(3) Project&#160;&lt;a href=&quot;#57L AS timestamp2#59L, rand(-3625198886289022666) AS value2#61&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;id#57L AS timestamp2#59L, rand(-3625198886289022666) AS value2#61&lt;/a&gt;&#160;&lt;/ins&gt;- *(3) Range (1000, 2001, step=1, splits=4)&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 4 times, most recent failure: Lost task 0.3 in stage 6.0 (TID 21, 10.100.100.10, executor 0): org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 65536 bytes of memory, got 0&lt;/tt&gt;&lt;br/&gt;
&#160;&lt;/p&gt;

&lt;p&gt;Using broadcast succeeds:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
#broadcast
joined = r1.join(broadcast(r2), r1.timestamp1 == r2.timestamp2, &lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-keyword&quot;&gt;inner&lt;/span&gt;&quot;&lt;/span&gt;)&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Running on Spark 2.4.&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Or if you prefer Scala:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.spark.sql.SparkSession
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.spark.sql.functions.{col, rand}

spark.conf.set(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.sql.join.preferSortMergeJoin&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&quot;&lt;/span&gt;)
spark.conf.set(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.sql.autoBroadcastJoinThreshold&quot;&lt;/span&gt;, -1)

&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt; r1 = spark.range(1, 1001).select(col(&lt;span class=&quot;code-quote&quot;&gt;&quot;id&quot;&lt;/span&gt;).alias(&lt;span class=&quot;code-quote&quot;&gt;&quot;timestamp1&quot;&lt;/span&gt;))
r1 = r1.withColumn(&lt;span class=&quot;code-quote&quot;&gt;&quot;value&quot;&lt;/span&gt;, rand())
&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt; r2 = spark.range(1000, 2001).select(col(&lt;span class=&quot;code-quote&quot;&gt;&quot;id&quot;&lt;/span&gt;).alias(&lt;span class=&quot;code-quote&quot;&gt;&quot;timestamp2&quot;&lt;/span&gt;))
r2 = r2.withColumn(&lt;span class=&quot;code-quote&quot;&gt;&quot;value2&quot;&lt;/span&gt;, rand())
&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt; joined = r1.join(r2, col(&lt;span class=&quot;code-quote&quot;&gt;&quot;timestamp1&quot;&lt;/span&gt;) === col(&lt;span class=&quot;code-quote&quot;&gt;&quot;timestamp2&quot;&lt;/span&gt;), &lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-keyword&quot;&gt;inner&lt;/span&gt;&quot;&lt;/span&gt;)
joined = joined.coalesce(1)
joined.explain()
joined.show()&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Just reproduced it in standalone mode using&#160;&lt;a href=&quot;https://www.apache.org/dyn/closer.lua/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz,&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://www.apache.org/dyn/closer.lua/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz,&lt;/a&gt;&#160;Python 3.7. Same code as above.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Succeeds with 1 thread:&#160;./bin/pyspark&#160;--master local&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Fails with &amp;gt;1 thread:&#160;./bin/pyspark --master local&lt;span class=&quot;error&quot;&gt;&amp;#91;4&amp;#93;&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
SparkSession available as &lt;span class=&quot;code-quote&quot;&gt;&apos;spark&apos;&lt;/span&gt;.

&amp;gt;&amp;gt;&amp;gt; from pyspark.sql.functions &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; rand, col

&amp;gt;&amp;gt;&amp;gt;

&amp;gt;&amp;gt;&amp;gt; spark.conf.set(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.sql.join.preferSortMergeJoin&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&quot;&lt;/span&gt;)

&amp;gt;&amp;gt;&amp;gt; spark.conf.set(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.sql.autoBroadcastJoinThreshold&quot;&lt;/span&gt;, -1)

&amp;gt;&amp;gt;&amp;gt;

&amp;gt;&amp;gt;&amp;gt; r1 = spark.range(1, 1001).select(col(&lt;span class=&quot;code-quote&quot;&gt;&quot;id&quot;&lt;/span&gt;).alias(&lt;span class=&quot;code-quote&quot;&gt;&quot;timestamp1&quot;&lt;/span&gt;))

&amp;gt;&amp;gt;&amp;gt; r1 = r1.withColumn(&lt;span class=&quot;code-quote&quot;&gt;&apos;value&apos;&lt;/span&gt;, rand())

&amp;gt;&amp;gt;&amp;gt; r2 = spark.range(1000, 1001).select(col(&lt;span class=&quot;code-quote&quot;&gt;&quot;id&quot;&lt;/span&gt;).alias(&lt;span class=&quot;code-quote&quot;&gt;&quot;timestamp2&quot;&lt;/span&gt;))

&amp;gt;&amp;gt;&amp;gt; r2 = r2.withColumn(&lt;span class=&quot;code-quote&quot;&gt;&apos;value2&apos;&lt;/span&gt;, rand())

&amp;gt;&amp;gt;&amp;gt; joined = r1.join(r2, r1.timestamp1 == r2.timestamp2, &lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-keyword&quot;&gt;inner&lt;/span&gt;&quot;&lt;/span&gt;)

&amp;gt;&amp;gt;&amp;gt; joined = joined.coalesce(1)

&amp;gt;&amp;gt;&amp;gt; joined.show()&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
[Stage 2:&amp;gt;&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; (0 + 1) / 1]2019-02-06 17:12:27 WARN&#160; TaskMemoryManager:304 - Failed to allocate a page (1900544 bytes), &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; again.

2019-02-06 17:12:27 ERROR Executor:91 - Exception in task 0.0 in stage 2.0 (TID 6)

org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 28 bytes of memory, got 0

at org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:157)

at org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:119)

at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:383)

at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:407)

at org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:135)&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16763825" author="taoluo" created="Fri, 8 Feb 2019 19:00:54 +0000"  >&lt;p&gt;Can someone add &apos;affects version&apos; 2.4.0 as well?&#160;&lt;/p&gt;</comment>
                            <comment id="16765378" author="taoluo" created="Mon, 11 Feb 2019 21:06:20 +0000"  >&lt;p&gt;I&apos;ll take a stab at this jira, should have something to review today or tomorrow.&#160;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/spark/pull/23762&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/23762&lt;/a&gt;&#160;(took some inspiration from Zhan&apos;s patch)&lt;/p&gt;

&lt;p&gt;I&apos;d appreciate a review, and pointers on modifying code-gen portion.&lt;/p&gt;</comment>
                            <comment id="16766598" author="taoluo" created="Wed, 13 Feb 2019 00:04:35 +0000"  >&lt;p&gt;cc&#160;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tejasp&quot; class=&quot;user-hover&quot; rel=&quot;tejasp&quot;&gt;tejasp&lt;/a&gt;,&#160;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=kiszk&quot; class=&quot;user-hover&quot; rel=&quot;kiszk&quot;&gt;kiszk&lt;/a&gt;&#160;for input on code generation to address the memory leak.&#160;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/spark/pull/23762&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/23762&lt;/a&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16773861" author="cloud_fan" created="Thu, 21 Feb 2019 09:19:48 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=taoluo&quot; class=&quot;user-hover&quot; rel=&quot;taoluo&quot;&gt;taoluo&lt;/a&gt; Thanks for the detailed explanation! I kind of agree that this is a memory leak, although the memory can be released when the task completes.&lt;/p&gt;

&lt;p&gt;The problematic pattern is: an operator consumes only a part of records from its child, so that the child can&apos;t release the last page which stores the last record it outputs. The child has no idea if the last record has been consumed by the parent or not, so it&apos;s not safe to release the last page, as doing so would make the last record corrupted. SMJ and limit are 2 places that I can think of that have this pattern.&lt;/p&gt;

&lt;p&gt;So we need a mechanism to allow the parent to tell the child that it can release all the resources. Closable iterator is an option here, but we should not hack it in SMJ only.&lt;/p&gt;</comment>
                            <comment id="16779460" author="guru.bobbi@gmail.com" created="Wed, 27 Feb 2019 16:07:02 +0000"  >&lt;p&gt;i ran the sample code provided on spark 2.0.2 and it used sortedMergeJoin but did not lead to memory leaks. So wondering what has changed between spark 2.0.2 and spark 2.2 that is leading to this OOM error?&#160;&lt;/p&gt;</comment>
                            <comment id="16794962" author="xiaojuwu" created="Mon, 18 Mar 2019 12:04:11 +0000"  >&lt;p&gt;Any updates? Do you have any discussion on the general fix instead of hack in SMJ?&lt;/p&gt;</comment>
                            <comment id="16829545" author="taoluo" created="Mon, 29 Apr 2019 17:23:39 +0000"  >&lt;p&gt;The problem is that the task won&apos;t complete because of memory being leaked (You can see from the simple example above)&lt;br/&gt;
Secondly, it&apos;s not just the last page, it&apos;s every page with records from unused iterators. &lt;br/&gt;
Can we increase the priority of this bug? SMJ is a pretty integral part of Spark SQL, and it seems like no progress is being made on this bug, which is causing jobs to fail and has no workaround. &lt;/p&gt;

&lt;p&gt;I don&apos;t think that it&apos;s a hack: the argument seems to be that limit also needs to fixed, so let&apos;s not fix this bug until that is also fixed, meanwhile this issue has been lingering since at least July 2017. &lt;br/&gt;
This would fix a memory leak and improve performance from not spilling unnecessarily. Why don&apos;t we target this fix for SMJ first, since it&apos;s pretty isolated to UnsafeExternalRowIterator in SMJ, run it through all the test cases, and make additional changes as necessary in the future. &lt;/p&gt;

&lt;p&gt;I&apos;ve been porting &lt;a href=&quot;https://github.com/apache/spark/pull/23762&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;this PR&lt;/a&gt; onto my production Spark cluster for the last 3 months, but I&apos;m hoping we can get some sort of fix into 3.0 at least.&lt;/p&gt;

&lt;p&gt;I started a discussion thread here, hopefully people can jump in:&lt;br/&gt;
&lt;a href=&quot;http://apache-spark-developers-list.1001551.n3.nabble.com/Memory-leak-in-SortMergeJoin-td27152.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://apache-spark-developers-list.1001551.n3.nabble.com/Memory-leak-in-SortMergeJoin-td27152.html&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16844313" author="jiangxb1987" created="Mon, 20 May 2019 21:24:17 +0000"  >&lt;p&gt;I&apos;m working on this, will post a simple design doc in a few days.&lt;/p&gt;</comment>
                            <comment id="16928204" author="cane" created="Thu, 12 Sep 2019 05:07:31 +0000"  >&lt;p&gt;Any progress of this issue? &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jiangxb1987&quot; class=&quot;user-hover&quot; rel=&quot;jiangxb1987&quot;&gt;jiangxb1987&lt;/a&gt;&lt;br/&gt;
We also encountered this problem&lt;/p&gt;</comment>
                            <comment id="16935176" author="mshen" created="Sun, 22 Sep 2019 01:35:35 +0000"  >&lt;p&gt;We also saw this issue happening in our cluster.&lt;/p&gt;

&lt;p&gt;Based on the &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=taoluo&quot; class=&quot;user-hover&quot; rel=&quot;taoluo&quot;&gt;taoluo&lt;/a&gt;&#160;&apos;s patch, we worked on a patch which fixes this issue for when codegen is enabled.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/spark/pull/25888&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/25888&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Would appreciate comments on this.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=taoluo&quot; class=&quot;user-hover&quot; rel=&quot;taoluo&quot;&gt;taoluo&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cloud_fan&quot; class=&quot;user-hover&quot; rel=&quot;cloud_fan&quot;&gt;cloud_fan&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jiangxb1987&quot; class=&quot;user-hover&quot; rel=&quot;jiangxb1987&quot;&gt;jiangxb1987&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16951154" author="mshen" created="Mon, 14 Oct 2019 16:55:01 +0000"  >&lt;p&gt;We have deployed the latest version of the PR in&#160;&lt;a href=&quot;https://github.com/apache/spark/pull/25888&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/25888&lt;/a&gt;&#160;in LinkedIn&apos;s production clusters for a week now.&lt;/p&gt;

&lt;p&gt;With the most recent changes, all corner cases seem to have been handled.&lt;/p&gt;

&lt;p&gt;We are seeing jobs previously failing due to this issue now able to complete.&lt;/p&gt;

&lt;p&gt;We have also observed a general reduction of spills during join in our cluster.&lt;/p&gt;

&lt;p&gt;Want to see if the community is also working on a fix of this issue, and if so whether there&apos;s a timeline for the fix.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cloud_fan&quot; class=&quot;user-hover&quot; rel=&quot;cloud_fan&quot;&gt;cloud_fan&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jiangxb1987&quot; class=&quot;user-hover&quot; rel=&quot;jiangxb1987&quot;&gt;jiangxb1987&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=taoluo&quot; class=&quot;user-hover&quot; rel=&quot;taoluo&quot;&gt;taoluo&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16951165" author="mshen" created="Mon, 14 Oct 2019 17:12:27 +0000"  >&lt;p&gt;Want to further clarify the scope of the fix in PR&#160;&lt;a href=&quot;https://github.com/apache/spark/pull/25888&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/25888&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Based on previous work by &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=taoluo&quot; class=&quot;user-hover&quot; rel=&quot;taoluo&quot;&gt;taoluo&lt;/a&gt;, this PR further fixes the issue for SMJ codegen.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hvanhovell&quot; class=&quot;user-hover&quot; rel=&quot;hvanhovell&quot;&gt;hvanhovell&lt;/a&gt;&#160;raised 2 concerns in &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=taoluo&quot; class=&quot;user-hover&quot; rel=&quot;taoluo&quot;&gt;taoluo&lt;/a&gt;&apos;s PR in&#160;&lt;a href=&quot;https://github.com/apache/spark/pull/23762&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/23762&lt;/a&gt;:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;This only works for a SMJ with Sorts as its direct input.&lt;/li&gt;
	&lt;li&gt;Not sure if it safe to assume that you can close an underlying child like this.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;The fix in PR&#160;&lt;a href=&quot;https://github.com/apache/spark/pull/25888&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/25888&lt;/a&gt;&#160;should have addressed concern #2, i.e. it guarantees safeness on closing the iterator for a Sort operator early.&lt;/p&gt;

&lt;p&gt;This fix does not yet propagate the requests to close iterators of both child operators of a SMJ throughout the plan tree to reach the Sort operators.&lt;/p&gt;

&lt;p&gt;However, with our experiences in operating all Spark workloads at LinkedIn, it is mostly common for SMJ not having Sort as its direct input when there are multiple SMJs stacked together.&lt;/p&gt;

&lt;p&gt;In this case, even if we are not yet propagating the requests, each SMJ can still properly handle its local child operators which would still help to release the resources early.&lt;/p&gt;</comment>
                            <comment id="16956936" author="cloud_fan" created="Tue, 22 Oct 2019 11:09:05 +0000"  >&lt;p&gt;Issue resolved by pull request 26164&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/26164&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/26164&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                            <outwardlinks description="blocks">
                                        <issuelink>
            <issuekey id="13229907">SPARK-27558</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="13168247">SPARK-24657</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 4 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3htp3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>