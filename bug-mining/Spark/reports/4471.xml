<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:50:41 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-14536] NPE in JDBCRDD when array column contains nulls (postgresql)</title>
                <link>https://issues.apache.org/jira/browse/SPARK-14536</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;At &lt;a href=&quot;https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD.scala#L453&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD.scala#L453&lt;/a&gt; it is assumed that the JDBC driver will definitely return a non-null `Array` object from the call to `getArray`, and that in the event of a null array it will return an non-null `Array` object with a null underlying array.  But as you can see here &lt;a href=&quot;https://github.com/pgjdbc/pgjdbc/blob/master/pgjdbc/src/main/java/org/postgresql/jdbc/PgResultSet.java#L387&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/pgjdbc/pgjdbc/blob/master/pgjdbc/src/main/java/org/postgresql/jdbc/PgResultSet.java#L387&lt;/a&gt; that isn&apos;t the case, at least for PostgreSQL.  This causes a `NullPointerException` whenever an array column contains null values. It seems like the PostgreSQL JDBC driver is probably doing the wrong thing, but even so there should be a null check in JDBCRDD.  I&apos;m happy to submit a PR if that would be helpful.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12957748">SPARK-14536</key>
            <summary>NPE in JDBCRDD when array column contains nulls (postgresql)</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="tsuresh">Suresh Thalamati</assignee>
                                    <reporter username="jeremyrsmith">Jeremy Smith</reporter>
                        <labels>
                            <label>NullPointerException</label>
                    </labels>
                <created>Mon, 11 Apr 2016 16:12:20 +0000</created>
                <updated>Mon, 12 Dec 2022 18:11:03 +0000</updated>
                            <resolved>Sat, 21 Jan 2017 03:24:11 +0000</resolved>
                                    <version>1.6.1</version>
                                    <fixVersion>2.1.1</fixVersion>
                    <fixVersion>2.2.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>8</watches>
                                                                                                                <comments>
                            <comment id="15235400" author="jeremyrsmith" created="Mon, 11 Apr 2016 16:21:51 +0000"  >&lt;p&gt;Actually, H2 seems to do the same thing:  &lt;a href=&quot;https://github.com/h2database/h2database/blob/master/h2/src/main/org/h2/jdbc/JdbcResultSet.java#L1175&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/h2database/h2database/blob/master/h2/src/main/org/h2/jdbc/JdbcResultSet.java#L1175&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I&apos;ll go ahead and create a test for this and submit a PR.&lt;/p&gt;</comment>
                            <comment id="15393161" author="gurwls223" created="Tue, 26 Jul 2016 03:43:48 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jeremyrsmith&quot; class=&quot;user-hover&quot; rel=&quot;jeremyrsmith&quot;&gt;jeremyrsmith&lt;/a&gt;, are you working on this?&lt;/p&gt;</comment>
                            <comment id="15393283" author="gurwls223" created="Tue, 26 Jul 2016 05:59:06 +0000"  >&lt;p&gt;FYI, It seems &lt;tt&gt;ArrayType&lt;/tt&gt; is not supported for JDBC for now, &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-8500&quot; title=&quot;Support for array types in JDBCRDD&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-8500&quot;&gt;&lt;del&gt;SPARK-8500&lt;/del&gt;&lt;/a&gt; and therefore, handling array in &lt;a href=&quot;https://github.com/apache/spark/blob/7ffd99ec5f267730734431097cbb700ad074bebe/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD.scala#L411-L451&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/7ffd99ec5f267730734431097cbb700ad074bebe/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD.scala#L411-L451&lt;/a&gt; is dead codes for now.&lt;/p&gt;</comment>
                            <comment id="15511714" author="apachespark" created="Thu, 22 Sep 2016 01:06:03 +0000"  >&lt;p&gt;User &apos;sureshthalamati&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/15192&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/15192&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15511725" author="tsuresh" created="Thu, 22 Sep 2016 01:10:27 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-10186&quot; title=&quot;Add support for more postgres column types&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-10186&quot;&gt;&lt;del&gt;SPARK-10186&lt;/del&gt;&lt;/a&gt;  added array data type support  for postgres in 1.6.  NPE issues still exists. I was able repro in the  master. &lt;/p&gt;</comment>
                            <comment id="15511837" author="gurwls223" created="Thu, 22 Sep 2016 02:00:35 +0000"  >&lt;p&gt;I see. I rushed to read this and didn&apos;t noticed that this is actually a PostgreSQL specific issue (I thought this JIRA describes a general JDBC problem).&lt;br/&gt;
Yea, &lt;tt&gt;ArrayType&lt;/tt&gt; seems only supported in &lt;tt&gt;PostgreSQL&lt;/tt&gt; in Spark. Maybe we should make some relations with those JIRAs with &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-8500&quot; title=&quot;Support for array types in JDBCRDD&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-8500&quot;&gt;&lt;del&gt;SPARK-8500&lt;/del&gt;&lt;/a&gt; to prevent confusion.&lt;/p&gt;</comment>
                            <comment id="15514129" author="tsuresh" created="Thu, 22 Sep 2016 18:42:13 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sowen&quot; class=&quot;user-hover&quot; rel=&quot;sowen&quot;&gt;sowen&lt;/a&gt;  I am not sure why this issue got closed  as duplicate after I reopened. Based on the test case I tried on the master ,  this issue does not looks like a duplicate to me; as I mentioned in my previous comment when i reopened the issue.  array data type is supported for postgres.   &lt;/p&gt;

&lt;p&gt;Repro :&lt;br/&gt;
On postgresdb :&lt;br/&gt;
create table spark_array(a int , b text[])&lt;br/&gt;
insert into spark_array values(1 , null)&lt;br/&gt;
insert into spark_array values(1 , &apos;&lt;/p&gt;
{&quot;AA&quot;, &quot;BB&quot;}
&lt;p&gt;&apos;)&lt;/p&gt;

&lt;p&gt;val psqlProps = new java.util.Properties()&lt;br/&gt;
psqlProps.setProperty(&quot;user&quot; , &quot;user&quot;)&lt;br/&gt;
psqlProps.setProperty(&quot;password&quot; , &quot;password&quot;)&lt;/p&gt;

&lt;p&gt;&amp;#8211; works fine&lt;br/&gt;
spark.read.jdbc(&quot;jdbc:postgresql://localhost:5432/pdb&quot;, &quot;(select * from spark_array where b is not null) as a &quot;, psqlProps).show() &lt;/p&gt;

&lt;p&gt;&amp;#8211; fails with error.&lt;br/&gt;
spark.read.jdbc(&quot;jdbc:postgresql://localhost:5432/pdb&quot;, &quot;spark_array&quot;, psqlProps).show()   fails with following error:&lt;/p&gt;

&lt;p&gt;Stack :&lt;br/&gt;
16/09/21 11:49:41 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.NullPointerException&lt;br/&gt;
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter$13.apply(JdbcUtils.scala:442)&lt;br/&gt;
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter$13.apply(JdbcUtils.scala:440)&lt;br/&gt;
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.getNext(JdbcUtils.scala:301)&lt;br/&gt;
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.getNext(JdbcUtils.scala:283)&lt;br/&gt;
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)&lt;br/&gt;
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)&lt;br/&gt;
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)&lt;/p&gt;</comment>
                            <comment id="15514134" author="srowen" created="Thu, 22 Sep 2016 18:43:45 +0000"  >&lt;p&gt;Yes, sounds like we&apos;re both saying this is tied up in the resolution to &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-8500&quot; title=&quot;Support for array types in JDBCRDD&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-8500&quot;&gt;&lt;del&gt;SPARK-8500&lt;/del&gt;&lt;/a&gt;, as it pertains to array types in Postgres. Is there a separate resolution / change you could imagine for the two?&lt;/p&gt;</comment>
                            <comment id="15514246" author="tsuresh" created="Thu, 22 Sep 2016 19:24:59 +0000"  >&lt;p&gt;Yes. &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-10186&quot; title=&quot;Add support for more postgres column types&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-10186&quot;&gt;&lt;del&gt;SPARK-10186&lt;/del&gt;&lt;/a&gt; already added array support for postgres ,  the PR (&lt;a href=&quot;https://github.com/apache/spark/pull/15192&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/15192&lt;/a&gt;) i submitted in this Jira will address NPE issue for null values.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-8500&quot; title=&quot;Support for array types in JDBCRDD&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-8500&quot;&gt;&lt;del&gt;SPARK-8500&lt;/del&gt;&lt;/a&gt; from tiltle (Support for array types in JDBCRDD) ,  sounds more generic than specific to postgres , although repo given is for postgres.  &lt;/p&gt;</comment>
                            <comment id="15828463" author="smilegator" created="Wed, 18 Jan 2017 17:56:25 +0000"  >&lt;p&gt;This issue needs to be resolved. &lt;/p&gt;</comment>
                            <comment id="15945673" author="apachespark" created="Tue, 28 Mar 2017 18:26:03 +0000"  >&lt;p&gt;User &apos;sureshthalamati&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/17460&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/17460&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16022002" author="pabloa" created="Tue, 23 May 2017 22:32:44 +0000"  >&lt;p&gt;The fix is not accepting columns like integer[][]  (multidimensional arrays)&lt;/p&gt;

&lt;p&gt;To reproduce this error:&lt;/p&gt;

&lt;p&gt;1) Create a SQL table in postgresql&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-sql&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;TABLE&lt;/span&gt; arrays_test
(
  eid &lt;span class=&quot;code-keyword&quot;&gt;integer&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;NOT&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;NULL&lt;/span&gt;,
  &lt;span class=&quot;code-keyword&quot;&gt;simple&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;integer&lt;/span&gt;[],
  multi &lt;span class=&quot;code-keyword&quot;&gt;integer&lt;/span&gt;[][]
);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;2) Insert a row like this one:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-xml&quot;&gt;insert into arrays_test (eid, simple, multi)
values
(1, &lt;span class=&quot;code-quote&quot;&gt;&apos;{1, 1}&apos;&lt;/span&gt;, NULL);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;3) Execute a SPQL query like this one and observe how it works:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-python&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;from&lt;/span&gt; pyspark &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; SparkConf
&lt;span class=&quot;code-keyword&quot;&gt;from&lt;/span&gt; pyspark &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; SparkContext
&lt;span class=&quot;code-keyword&quot;&gt;from&lt;/span&gt; pyspark.sql &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; SQLContext

master = &lt;span class=&quot;code-quote&quot;&gt;&quot;spark://spark211:7077&quot;&lt;/span&gt;  &lt;span class=&quot;code-comment&quot;&gt;# local &lt;span class=&quot;code-keyword&quot;&gt;is&lt;/span&gt; OK too
&lt;/span&gt;conf = (
    SparkConf()
        .setMaster(master)
        .setAppName(&lt;span class=&quot;code-quote&quot;&gt;&quot;Connection Test 5&quot;&lt;/span&gt;)
        .&lt;span class=&quot;code-object&quot;&gt;set&lt;/span&gt;(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.jars.packages&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;org.postgresql:postgresql:9.4.1212&quot;&lt;/span&gt;)   &lt;span class=&quot;code-comment&quot;&gt;## This one works ok
&lt;/span&gt;        .&lt;span class=&quot;code-object&quot;&gt;set&lt;/span&gt;(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.driver.memory&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;2G&quot;&lt;/span&gt;)
        .&lt;span class=&quot;code-object&quot;&gt;set&lt;/span&gt;(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.executor.memory&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;2G&quot;&lt;/span&gt;)
        .&lt;span class=&quot;code-object&quot;&gt;set&lt;/span&gt;(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.driver.cores&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;10&quot;&lt;/span&gt;)
)

sc = SparkContext(conf=conf)
&lt;span class=&quot;code-comment&quot;&gt;# sc.setLogLevel(&lt;span class=&quot;code-quote&quot;&gt;&quot;ALL&quot;&lt;/span&gt;)
&lt;/span&gt;
&lt;span class=&quot;code-object&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&quot;====&amp;gt;&quot;&lt;/span&gt;, 1
&lt;span class=&quot;code-object&quot;&gt;print&lt;/span&gt;(sc)

sqlContext = SQLContext(sc)

&lt;span class=&quot;code-object&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&quot;====&amp;gt;&quot;&lt;/span&gt;, 2
&lt;span class=&quot;code-object&quot;&gt;print&lt;/span&gt; sqlContext

url = &lt;span class=&quot;code-quote&quot;&gt;&quot;postgresql://localhost:5432/test&quot;&lt;/span&gt;   &lt;span class=&quot;code-comment&quot;&gt;# change properly
&lt;/span&gt;url = &lt;span class=&quot;code-quote&quot;&gt;&apos;jdbc:&apos;&lt;/span&gt;+url
properties = {&lt;span class=&quot;code-quote&quot;&gt;&apos;user&apos;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&apos;user&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;password&apos;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&apos;password&apos;&lt;/span&gt;}   &lt;span class=&quot;code-comment&quot;&gt;# change user &lt;span class=&quot;code-keyword&quot;&gt;and&lt;/span&gt; password &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; needed
&lt;/span&gt;
df = sqlContext.read.&lt;span class=&quot;code-object&quot;&gt;format&lt;/span&gt;(&lt;span class=&quot;code-quote&quot;&gt;&quot;jdbc&quot;&lt;/span&gt;). \
    option(&lt;span class=&quot;code-quote&quot;&gt;&quot;url&quot;&lt;/span&gt;, url). \
    option(&lt;span class=&quot;code-quote&quot;&gt;&quot;driver&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;org.postgresql.Driver&quot;&lt;/span&gt;). \
    option(&lt;span class=&quot;code-quote&quot;&gt;&quot;useUnicode&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;true&quot;&lt;/span&gt;). \
    option(&lt;span class=&quot;code-quote&quot;&gt;&quot;continueBatchOnError&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;true&quot;&lt;/span&gt;). \
    option(&lt;span class=&quot;code-quote&quot;&gt;&quot;useSSL&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;false&quot;&lt;/span&gt;). \
    option(&lt;span class=&quot;code-quote&quot;&gt;&quot;user&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;user&quot;&lt;/span&gt;). \
    option(&lt;span class=&quot;code-quote&quot;&gt;&quot;password&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;password&quot;&lt;/span&gt;). \
    option(&lt;span class=&quot;code-quote&quot;&gt;&quot;dbtable&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;arrays_test&quot;&lt;/span&gt;). \
    option(&lt;span class=&quot;code-quote&quot;&gt;&quot;partitionColumn&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;eid&quot;&lt;/span&gt;). \
    option(&lt;span class=&quot;code-quote&quot;&gt;&quot;lowerBound&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;1000015&quot;&lt;/span&gt;). \
    option(&lt;span class=&quot;code-quote&quot;&gt;&quot;upperBound&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;6026289&quot;&lt;/span&gt;). \
    option(&lt;span class=&quot;code-quote&quot;&gt;&quot;numPartitions&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;100&quot;&lt;/span&gt;). \
    load()

&lt;span class=&quot;code-object&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&quot;====&amp;gt;&quot;&lt;/span&gt;, 3

df.registerTempTable(&lt;span class=&quot;code-quote&quot;&gt;&quot;arrays_test&quot;&lt;/span&gt;)
df = sqlContext.sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;SELECT * FROM arrays_test limit 5&quot;&lt;/span&gt;)


&lt;span class=&quot;code-object&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&quot;====&amp;gt;&quot;&lt;/span&gt;, 4
&lt;span class=&quot;code-object&quot;&gt;print&lt;/span&gt; df.collect()

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;4) Observe how it works.&lt;/p&gt;

&lt;p&gt;5) Now, to reproduce the error, insert a multi dimensional array into the SQL table:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-sql&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;insert&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;into&lt;/span&gt; arrays_test (eid, &lt;span class=&quot;code-keyword&quot;&gt;simple&lt;/span&gt;, multi)
&lt;span class=&quot;code-keyword&quot;&gt;values&lt;/span&gt;
(2, &lt;span class=&quot;code-quote&quot;&gt;&apos;{1, 1}&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;{{1, 1},{2, 2}}&apos;&lt;/span&gt;);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;6) Execute step 3) again.&lt;/p&gt;

&lt;p&gt;7) Observe the exception&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
17/05/23 15:23:38 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job
Traceback (most recent call last):
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/home/pablo/develop/physiosigns/livebetter/modelling2/modelling2/scripts/runSparkTest2.py&quot;&lt;/span&gt;, line 65, in &amp;lt;module&amp;gt;
    print df.collect()
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/home/pablo/myProgs/virt-pablo/local/lib/python2.7/site-packages/pyspark/sql/dataframe.py&quot;&lt;/span&gt;, line 391, in collect
    port = self._jdf.collectToPython()
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/home/pablo/myProgs/virt-pablo/local/lib/python2.7/site-packages/py4j/java_gateway.py&quot;&lt;/span&gt;, line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/home/pablo/myProgs/virt-pablo/local/lib/python2.7/site-packages/pyspark/sql/utils.py&quot;&lt;/span&gt;, line 63, in deco
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; f(*a, **kw)
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/home/pablo/myProgs/virt-pablo/local/lib/python2.7/site-packages/py4j/protocol.py&quot;&lt;/span&gt;, line 319, in get_return_value
    format(target_id, &lt;span class=&quot;code-quote&quot;&gt;&quot;.&quot;&lt;/span&gt;, name), value)
py4j.protocol.Py4JJavaError: An error occurred &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; calling o49.collectToPython.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, 172.17.0.58, executor 0): java.lang.ClassCastException: [Ljava.lang.&lt;span class=&quot;code-object&quot;&gt;Integer&lt;/span&gt;; cannot be &lt;span class=&quot;code-keyword&quot;&gt;cast&lt;/span&gt; to java.lang.&lt;span class=&quot;code-object&quot;&gt;Integer&lt;/span&gt;
	at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:101)
	at org.apache.spark.sql.catalyst.util.GenericArrayData.getInt(GenericArrayData.scala:62)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
	at scala.collection.mutable.ResizableArray$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:333)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
	at org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply$mcI$sp(Dataset.scala:2768)
	at org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2765)
	at org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2765)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
	at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2788)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:2765)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:748)
Caused by: java.lang.ClassCastException: [Ljava.lang.&lt;span class=&quot;code-object&quot;&gt;Integer&lt;/span&gt;; cannot be &lt;span class=&quot;code-keyword&quot;&gt;cast&lt;/span&gt; to java.lang.&lt;span class=&quot;code-object&quot;&gt;Integer&lt;/span&gt;
	at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:101)
	at org.apache.spark.sql.catalyst.util.GenericArrayData.getInt(GenericArrayData.scala:62)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                            <outwardlinks description="duplicates">
                                        <issuelink>
            <issuekey id="12839311">SPARK-8500</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="12310010">
                    <name>Incorporates</name>
                                                                <inwardlinks description="is part of">
                                        <issuelink>
            <issuekey id="13074347">SPARK-20859</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 25 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2vxov:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>