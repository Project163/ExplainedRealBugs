<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:33:56 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-5569] Checkpoints cannot reference classes defined outside of Spark&apos;s assembly</title>
                <link>https://issues.apache.org/jira/browse/SPARK-5569</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Not sure if this is a bug or a feature, but it&apos;s not obvious, so wanted to create a JIRA to make sure we document this behavior.&lt;/p&gt;

&lt;p&gt;First documented by Cody Koeninger:&lt;br/&gt;
&lt;a href=&quot;https://gist.github.com/koeninger/561a61482cd1b5b3600c&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://gist.github.com/koeninger/561a61482cd1b5b3600c&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;15/01/12 16:07:07 INFO CheckpointReader: Attempting to load checkpoint from file file:/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/tmp/cp/checkpoint-1421100410000.bk
15/01/12 16:07:07 WARN CheckpointReader: Error reading checkpoint from file file:/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/tmp/cp/checkpoint-1421100410000.bk
java.io.IOException: java.lang.ClassNotFoundException: org.apache.spark.rdd.kafka.KafkaRDDPartition
        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1043)
        at org.apache.spark.streaming.dstream.DStreamCheckpointData.readObject(DStreamCheckpointData.scala:146)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1706)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1344)
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
        at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:500)
        at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:180)
        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1040)
        at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:176)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
        at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:251)
        at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:239)
        at scala.collection.IndexedSeqOptimized$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(IndexedSeqOptimized.scala:33)
        at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
        at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:239)
        at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:552)
        at example.CheckpointedExample$.main(CheckpointedExample.scala:34)
        at example.CheckpointedExample.main(CheckpointedExample.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:365)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.rdd.kafka.KafkaRDDPartition
        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.loadClass(&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.java:425)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.loadClass(&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.java:358)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;.forName0(Native Method)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;.forName(&lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;.java:274)
        at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:625)
        at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:279)
        at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1612)
        at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)
        at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1663)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1344)
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
        at scala.collection.mutable.HashMap$$anonfun$readObject$1.apply(HashMap.scala:142)
        at scala.collection.mutable.HashMap$$anonfun$readObject$1.apply(HashMap.scala:142)
        at scala.collection.mutable.HashTable$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;init(HashTable.scala:105)
        at scala.collection.mutable.HashMap.init(HashMap.scala:39)
        at scala.collection.mutable.HashMap.readObject(HashMap.scala:142)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
        at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:500)
        at org.apache.spark.streaming.dstream.DStreamCheckpointData$$anonfun$readObject$1.apply$mcV$sp(DStreamCheckpointData.scala:148)
        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1040)
        ... 52 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="12772114">SPARK-5569</key>
            <summary>Checkpoints cannot reference classes defined outside of Spark&apos;s assembly</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="10000">Auto Closed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="pwendell">Patrick Wendell</reporter>
                        <labels>
                            <label>bulk-closed</label>
                    </labels>
                <created>Tue, 3 Feb 2015 19:16:33 +0000</created>
                <updated>Thu, 6 Jun 2019 13:57:40 +0000</updated>
                            <resolved>Thu, 6 Jun 2019 13:57:40 +0000</resolved>
                                                                    <component>DStreams</component>
                        <due></due>
                            <votes>2</votes>
                                    <watches>14</watches>
                                                                                                                <comments>
                            <comment id="14901082" author="jon_fuseelements" created="Mon, 21 Sep 2015 17:57:12 +0000"  >&lt;p&gt;Are there any work arounds for this limitation? We are unable to track offsets using Kakfa Direct stream and use checkpoints. Our thinking is we need to abandon checkpoints and manage recovery outside of Spark checkpoints while this limitation exists.&lt;/p&gt;

&lt;p&gt;For reference with 1.4.1, we get the following:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;...
15/09/21 13:50:53 WARN CheckpointReader: Error reading checkpoint from file file:/tmp/page_view_events_cp/checkpoint-1442857500000
java.io.IOException: java.lang.ClassNotFoundException: org.apache.spark.streaming.kafka.OffsetRange
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="14901106" author="cody@koeninger.org" created="Mon, 21 Sep 2015 18:05:59 +0000"  >&lt;p&gt;The direct stream doesn&apos;t save offset ranges, it saves tuples and converts them to / from offset ranges.  That workaround has been in place since the original release.&lt;/p&gt;

&lt;p&gt;You&apos;re probably running into something else, can you make a minimal reproducible code example?&lt;/p&gt;</comment>
                            <comment id="14901320" author="jon_fuseelements" created="Mon, 21 Sep 2015 20:22:18 +0000"  >&lt;p&gt;Cody,&lt;/p&gt;

&lt;p&gt;Thanks for the quick response.&lt;/p&gt;

&lt;p&gt;We are doing exactly what you describe above. I suspect our issue is caused by trying to create an uber jar using SBT assembly:&lt;/p&gt;

{no format}&lt;br/&gt;
...&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt; (*:assembly) deduplicate: different file contents found in the following:&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt; /Users/jcb/.ivy2/cache/org.apache.spark/spark-streaming-kafka_2.10/jars/spark-streaming-kafka_2.10-1.4.1.jar:org/apache/spark/unused/UnusedStubClass.class&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt; /Users/jcb/.ivy2/cache/org.spark-project.spark/unused/jars/unused-1.0.0.jar:org/apache/spark/unused/UnusedStubClass.class&lt;br/&gt;
...
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;br/&gt;
&lt;br/&gt;
We chose to exclude org.spark-project.spark to resolve the above error:&lt;br/&gt;
{no format}
&lt;p&gt;...&lt;br/&gt;
// Common exclusion rules&lt;br/&gt;
val xHadoop = ExclusionRule(organization = &quot;org.apache.hadoop&quot;)&lt;br/&gt;
val xScala = ExclusionRule(organization = &quot;org.scala-lang&quot;)&lt;/p&gt;

&lt;p&gt;// Apache Spark&lt;br/&gt;
libraryDependencies ++= &lt;/p&gt;
{
  val sparkV = &quot;1.4.1&quot;

  Seq((&quot;org.apache.spark&quot; %% &quot;spark-core&quot;            % sparkV % &quot;provided&quot;).excludeAll(xHadoop, xScala),
      (&quot;org.apache.spark&quot; %% &quot;spark-streaming&quot;       % sparkV % &quot;provided&quot;).excludeAll(xHadoop, xScala),
      (&quot;org.apache.spark&quot; %% &quot;spark-sql&quot;                  % sparkV % &quot;provided&quot;).excludeAll(xHadoop, xScala),
      (&quot;org.apache.spark&quot; %% &quot;spark-streaming-kafka&quot; % sparkV).
        excludeAll(xHadoop, xScala, ExclusionRule(organization = &quot;org.spark-project.spark&quot;)))
}
...
{noformat}

&lt;p&gt;Do you have a recommendation on how to resolve the above assembly duplicate class issue?&lt;/p&gt;</comment>
                            <comment id="14901364" author="cody@koeninger.org" created="Mon, 21 Sep 2015 20:45:40 +0000"  >&lt;p&gt;You can typically use sbt&apos;s merge strategy to deal with that kind of thing&lt;/p&gt;

&lt;p&gt;example:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/databricks/learning-spark/blob/master/build.sbt&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/databricks/learning-spark/blob/master/build.sbt&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;On Mon, Sep 21, 2015 at 3:23 PM, Jon Buffington (JIRA) &amp;lt;jira@apache.org&amp;gt;&lt;/p&gt;
</comment>
                            <comment id="14901374" author="jon_fuseelements" created="Mon, 21 Sep 2015 20:50:23 +0000"  >&lt;p&gt;Right. We added:&lt;/p&gt;

&lt;p&gt;assemblyMergeStrategy in assembly := {&lt;br/&gt;
  case PathList(&quot;org&quot;, &quot;apache&quot;, &quot;spark&quot;, &quot;unused&quot;, &quot;UnusedStubClass.class&quot;)&lt;br/&gt;
=&amp;gt; MergeStrategy.discard&lt;br/&gt;
  case x =&amp;gt; (assemblyMergeStrategy in assembly).value&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
}&lt;/p&gt;

&lt;p&gt;and are still getting the above exception in the CheckpointReader. We are successful if we build and run without creating an uber jar.&lt;/p&gt;

&lt;p&gt;Not sure what we should try next.&lt;/p&gt;</comment>
                            <comment id="14901387" author="jon_fuseelements" created="Mon, 21 Sep 2015 20:57:27 +0000"  >&lt;p&gt;The only substantive difference in the build file you reference is that spark-streaming is included in the uber jar (not mark as provided). We are under the impression that the spark-streaming dependency is provided while we need to package spark-streaming-kafka. Are we mistaken?&lt;/p&gt;</comment>
                            <comment id="14901435" author="cody@koeninger.org" created="Mon, 21 Sep 2015 21:18:40 +0000"  >&lt;p&gt;Yeah, spark-streaming can be marked as provided.&lt;/p&gt;

&lt;p&gt;Try to get a small reproducible case that demonstrates the issue.&lt;/p&gt;

&lt;p&gt;On Mon, Sep 21, 2015 at 3:58 PM, Jon Buffington (JIRA) &amp;lt;jira@apache.org&amp;gt;&lt;/p&gt;
</comment>
                            <comment id="14901609" author="jon_fuseelements" created="Mon, 21 Sep 2015 23:28:11 +0000"  >&lt;p&gt;Cody,&lt;/p&gt;

&lt;p&gt;We were able to work around the issue by destructuring the OffsetRange type into its parts (i.e., string, long, ints). Below is our flow for reference. Our only concern is whether the transform operation runs at the driver as we have use a singleton on the driver to persist the last captured offsets. Can you confirm that the stream transform runs at the driver?&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;// Recover or create the stream.
val ssc = StreamingContext.getOrCreate(checkpointPath, () =&amp;gt; {
  createStreamingContext(checkpointPath)
})
...
def createStreamingContext(checkpointPath: String): StreamingContext = {
  val ssc = new StreamingContext(SparkConf,  Minutes(1))
  ssc.checkpoint(checkpointPath)
  createStream(ssc)
  ssc
}
...

def createStream((ssc: StreamingContext): Unit = {
...
  KafkaUtils.createDirectStream[K, V, KD, VD, R](ssc, kafkaParams, fromOffsets, messageHandler)
    // &quot;Note that the typecast to HasOffsetRanges will only succeed if it is
    // done in the first method called on the directKafkaStream, not later down
    // a chain of methods. You can use transform() instead of foreachRDD() as
    // your first method call in order to access offsets, then call further
    // Spark methods. However, be aware that the one-to-one mapping between RDD
    // partition and Kafka partition does not remain after any methods that
    // shuffle or repartition, e.g. reduceByKey() or window().&quot;
    .transform { rdd =&amp;gt;
      ...
      val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges
      Ranger.captureOffsetRanges(offsetRanges) // De-structure the OffsetRange type into its parts and save in a singleton.
      ...
      rdd
    }
    ...
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="14902692" author="cody@koeninger.org" created="Tue, 22 Sep 2015 14:09:40 +0000"  >&lt;p&gt;I&apos;m still not clear on what you&apos;re doing in &quot;captureOffsetRanges(..)&quot; that&lt;br/&gt;
is leading to the exception you first mentioned.&lt;/p&gt;

&lt;p&gt;But regarding the question of whether transform / foreachRDD etc runs on&lt;br/&gt;
the driver or the executor,&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/streaming-programming-guide.html#design-patterns-for-using-foreachrdd&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://spark.apache.org/docs/latest/streaming-programming-guide.html#design-patterns-for-using-foreachrdd&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;should make it pretty clear which portions run where&lt;/p&gt;

&lt;p&gt;On Mon, Sep 21, 2015 at 6:29 PM, Jon Buffington (JIRA) &amp;lt;jira@apache.org&amp;gt;&lt;/p&gt;
</comment>
                            <comment id="14939353" author="maxwell" created="Thu, 1 Oct 2015 05:48:33 +0000"  >&lt;p&gt;I encount the exactly same problem with you, and I think it&apos;s an bug of ObjectInputStreamWithLoader.&lt;/p&gt;

&lt;p&gt;ObjectInputStreamWithLoader extends the ObjectInputStream class and override its resolveClass method.&lt;br/&gt;
But Instead of Using Class.forName(desc,false,loader), Spark uses loader.loadClass(desc) to instance the class.&lt;br/&gt;
which do not works when there&apos;s Array type class.&lt;br/&gt;
For example: &lt;br/&gt;
Class.forName(&quot;[Ljava.lang.String&quot;,false,loader) works well while loader.loadClass(&quot;[Ljava.lang.String&quot;) would throw an class not found exception.&lt;br/&gt;
details of the difference can be found here. &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://bugs.java.com/view_bug.do?bug_id=6446627&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://bugs.java.com/view_bug.do?bug_id=6446627&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I would make a pull request to Spark.&lt;br/&gt;
Before it&apos;s accepted, &lt;br/&gt;
you can redefine the ObjectInputStreamWithLoader and replace it with the original one.&lt;/p&gt;</comment>
                            <comment id="14939447" author="apachespark" created="Thu, 1 Oct 2015 07:36:08 +0000"  >&lt;p&gt;User &apos;maxwellzdm&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/8955&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/8955&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14944909" author="tdas" created="Tue, 6 Oct 2015 11:44:07 +0000"  >&lt;p&gt;Before we jump into solving the issue, can you any give us a simple scala code that reproduces this issue?&lt;/p&gt;</comment>
                            <comment id="14945091" author="cody@koeninger.org" created="Tue, 6 Oct 2015 14:20:24 +0000"  >&lt;p&gt;The gist I originally posted, linked at the top of this ticket, has a 3 line change that should reproduce the issue.&lt;/p&gt;</comment>
                            <comment id="14970326" author="maxwell" created="Fri, 23 Oct 2015 02:53:49 +0000"  >&lt;p&gt;1. create a class let&apos;s call it TestCls, and makes it serializable&lt;br/&gt;
2. do the following transformation&lt;/p&gt;

&lt;p&gt;  // ... init sparkcontext and dstream&lt;br/&gt;
  // create an Array Object&lt;br/&gt;
  val cls = Array&lt;span class=&quot;error&quot;&gt;&amp;#91;TestCls&amp;#93;&lt;/span&gt;(new TestCls()) &lt;br/&gt;
  // serialize it to executor, then it would be save in Checkpoint.&lt;br/&gt;
  stream.foreachRDD&lt;/p&gt;
{ rdd =&amp;gt; 
      rdd.map(_ =&amp;gt; cls).collect() 
  }</comment>
                            <comment id="14976023" author="maxwell" created="Tue, 27 Oct 2015 08:47:11 +0000"  >&lt;p&gt;since patch had been merged, we may set status to be fixed&lt;/p&gt;</comment>
                            <comment id="14976062" author="srowen" created="Tue, 27 Oct 2015 09:18:47 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cody%40koeninger.org&quot; class=&quot;user-hover&quot; rel=&quot;cody@koeninger.org&quot;&gt;cody@koeninger.org&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=maxwell&quot; class=&quot;user-hover&quot; rel=&quot;maxwell&quot;&gt;maxwell&lt;/a&gt; does that patch resolve all of this issue or just for arrays?&lt;/p&gt;</comment>
                            <comment id="14980493" author="maxwell" created="Thu, 29 Oct 2015 14:17:45 +0000"  >&lt;p&gt;For OffsetRange ClassNotFound issue, I&apos;m sure this patch could solve the issue because actually the document on&lt;br/&gt;
Spark Home Page could not run without this patch&lt;/p&gt;

&lt;p&gt;Here&apos;s the sample code on Spark Home Page:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;&quot;&gt;&lt;b&gt;kafkaSample&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-comment&quot;&gt;// Hold a reference to the current offset ranges, so it can be used downstream
&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt; offsetRanges = Array[OffsetRange]()
	
 directKafkaStream.transform { rdd =&amp;gt;
   offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges
   rdd
 }.map {
           ...
 }.foreachRDD { rdd =&amp;gt;
   &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (o &amp;lt;- offsetRanges) {
     println(s&lt;span class=&quot;code-quote&quot;&gt;&quot;${o.topic} ${o.partition} ${o.fromOffset} ${o.untilOffset}&quot;&lt;/span&gt;)
   }
   ...
 }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;we can see that offsetRanges is an array and need to be deserialized by executors.&lt;/p&gt;

&lt;p&gt;For KafkaRDDPartition ClassNotFound issue, since I cannot see the source code,&lt;br/&gt;
I cannot 100% sure that this patch can solve the issue.&lt;/p&gt;

&lt;p&gt;But, In my opinion, this kind of issue are all caused by Spark trying to load an array object. &lt;/p&gt;

&lt;p&gt;Can anyone of you try this patch can tell us whether your issues are solved or not?&lt;/p&gt;</comment>
                            <comment id="14981719" author="moriarty279" created="Fri, 30 Oct 2015 02:03:15 +0000"  >&lt;p&gt;I encountered the same problem when deserialize Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Foo&amp;#93;&lt;/span&gt;() object from checkpoint file. Foo is a class defined in my project and not in spark-assembly.jar.&lt;br/&gt;
I didn&apos;t use the patch mentioned above. Instead, I found 2 workarounds and both solved the problem. But I think these 2 workarounds can prove the patch works for all these situations.&lt;/p&gt;

&lt;p&gt;1. Add java option &quot;-Dsun.lang.ClassLoader.allowArraySyntax=true&quot;. From &lt;a href=&quot;http://bugs.java.com/view_bug.do?bug_id=6434149&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://bugs.java.com/view_bug.do?bug_id=6434149&lt;/a&gt;&lt;br/&gt;
2. Change Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Foo&amp;#93;&lt;/span&gt; to List&lt;span class=&quot;error&quot;&gt;&amp;#91;Foo&amp;#93;&lt;/span&gt;.&lt;/p&gt;</comment>
                            <comment id="15088794" author="dwinters" created="Fri, 8 Jan 2016 06:52:17 +0000"  >&lt;p&gt;I have encountered this issue also.  I have a Spark streaming application that ingests messages from Kafka using the Direct API and checkpoints the DStream to ensure exactly once delivery semantics according to the Spark Streaming programming guide.  When I gracefully stop and then restart the streaming app, I get the following exception:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;16/01/07 17:30:49 WARN CheckpointReader: Error reading checkpoint from file file:/Users/dwinters/Documents/workspace/DSE/INFRASTRUCTURE/dse-bitbucket-stream/hdfs/checkpoint/meta-processor-job/checkpoint-1452216644000 
java.io.IOException: java.lang.ClassNotFoundException: org.apache.spark.streaming.kafka.OffsetRange 
at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1140) 
at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:184) 
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) 
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
at java.lang.reflect.Method.invoke(Method.java:606) 
at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017) 
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893) 
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) 
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) 
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) 
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) 
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) 
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) 
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370) 
at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:251) 
at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:239) 
at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) 
at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34) 
at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:239) 
at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:623) 
at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:662) 
at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala) 
at com.gopro.dse.bitbucket.stream.DseSparkKafkaBaseJob.getStreamingContext(DseSparkKafkaBaseJob.java:445)
at com.gopro.dse.bitbucket.stream.DseSparkKafkaBaseJob.doWork(DseSparkKafkaBaseJob.java:392) 
at com.gopro.dse.bitbucket.stream.DseBitBucketStreamMetaProcessorJob.main(DseBitBucketStreamMetaProcessorJob.java:355)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) 
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
at java.lang.reflect.Method.invoke(Method.java:606) 
at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:569)
at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:166) 
at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:189) 
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:110) 
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala) 
Caused by: java.lang.ClassNotFoundException: org.apache.spark.streaming.kafka.OffsetRange 
at java.net.URLClassLoader$1.run(URLClassLoader.java:366) 
at java.net.URLClassLoader$1.run(URLClassLoader.java:355) 
at java.security.AccessController.doPrivileged(Native Method) 
at java.net.URLClassLoader.findClass(URLClassLoader.java:354) 
at java.lang.ClassLoader.loadClass(ClassLoader.java:425) 
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308) 
at java.lang.ClassLoader.loadClass(ClassLoader.java:358) 
at java.lang.Class.forName0(Native Method) 
at java.lang.Class.forName(Class.java:274) 
at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:625) 
at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:279) 
at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1612) 
at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517) 
at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1663) 
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1344) 
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) 
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) 
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) 
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) 
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) 
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) 
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) 
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) 
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) 
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) 
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) 
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) 
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) 
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) 
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) 
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) 
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) 
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) 
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) 
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) 
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) 
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) 
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) 
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) 
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) 
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) 
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) 
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) 
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) 
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) 
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) 
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) 
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) 
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) 
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) 
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) 
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) 
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) 
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) 
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) 
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) 
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) 
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) 
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) 
at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1706) 
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1344) 
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) 
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) 
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) 
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) 
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) 
at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:500) 
at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:188) 
at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1137) 
... 34 more 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When I apply the one line patch from this bug to the Spark 1.3 code base that our distribution includes, the exception goes away.  So the patch in this bug definitely resolves the issue described here in this bug.  I am working with our Spark distribution provider, Cloudera, to include this fix into their next patch.&lt;/p&gt;

&lt;p&gt;My question here is whether there is a workaround for this issue besides the code fix?  I tried setting the &quot;-Dsun.lang.ClassLoader.allowArraySyntax=true&quot; Java option as mentioned in the last comment and that didn&apos;t resolve it.  I was just wondering if I missed something obvious before we go patching a rather old release of Spark (1.3).&lt;/p&gt;</comment>
                            <comment id="15379976" author="michli" created="Fri, 15 Jul 2016 19:34:12 +0000"  >&lt;p&gt;It seems the PR (&lt;a href=&quot;https://github.com/apache/spark/pull/8955&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/8955&lt;/a&gt;) has been merged to master. Why is this issue still open? The reason I&apos;m asking is that I saw this issue once in our spark 1.6.1 streaming project. I am wondering which version the PR was merged.&lt;/p&gt;</comment>
                            <comment id="15380024" author="srowen" created="Fri, 15 Jul 2016 20:09:28 +0000"  >&lt;p&gt;I&apos;m not sure it was fully resolved; see the last 3-4 comments. It is probably resolvable as I don&apos;t expect more activity, but it seems like the PR fixed a subset of this.&lt;/p&gt;</comment>
                            <comment id="16857676" author="srowen" created="Thu, 6 Jun 2019 13:57:40 +0000"  >&lt;p&gt;42 bulk-closed issues were still &quot;In Progress&quot; when they should be resolved.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 23 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2550v:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>