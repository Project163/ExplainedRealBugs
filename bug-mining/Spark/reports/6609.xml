<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 19:06:18 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-23977] Add commit protocol binding to Hadoop 3.1 PathOutputCommitter mechanism</title>
                <link>https://issues.apache.org/jira/browse/SPARK-23977</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Hadoop 3.1 adds a mechanism for job-specific and store-specific committers (&lt;a href=&quot;https://issues.apache.org/jira/browse/MAPREDUCE-6823&quot; title=&quot;FileOutputFormat to support configurable PathOutputCommitter factory&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAPREDUCE-6823&quot;&gt;&lt;del&gt;MAPREDUCE-6823&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/MAPREDUCE-6956&quot; title=&quot;FileOutputCommitter to gain abstract superclass PathOutputCommitter&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAPREDUCE-6956&quot;&gt;&lt;del&gt;MAPREDUCE-6956&lt;/del&gt;&lt;/a&gt;), and one key implementation, S3A committers, &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-13786&quot; title=&quot;Add S3A committers for zero-rename commits to S3 endpoints&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-13786&quot;&gt;&lt;del&gt;HADOOP-13786&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;These committers deliver high-performance output of MR and spark jobs to S3, and offer the key semantics which Spark depends on: no visible output until job commit, a failure of a task at an stage, including partway through task commit, can be handled by executing and committing another task attempt. &lt;/p&gt;

&lt;p&gt;In contrast, the FileOutputFormat commit algorithms on S3 have issues:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Awful performance because files are copied by rename&lt;/li&gt;
	&lt;li&gt;FileOutputFormat v1: weak task commit failure recovery semantics as the (v1) expectation: &quot;directory renames are atomic&quot; doesn&apos;t hold.&lt;/li&gt;
	&lt;li&gt;S3 metadata eventual consistency can cause rename to miss files or fail entirely (&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-15849&quot; title=&quot;FileNotFoundException on _temporary while doing saveAsTable to S3&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-15849&quot;&gt;&lt;del&gt;SPARK-15849&lt;/del&gt;&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Note also that FileOutputFormat &quot;v2&quot; commit algorithm doesn&apos;t offer any of the commit semantics w.r.t observability of or recovery from task commit failure, on any filesystem.&lt;/p&gt;

&lt;p&gt;The S3A committers address these by way of uploading all data to the destination through multipart uploads, uploads which are only completed in job commit.&lt;/p&gt;

&lt;p&gt;The new &lt;tt&gt;PathOutputCommitter&lt;/tt&gt; factory mechanism allows applications to work with the S3A committers and any other, by adding a plugin mechanism into the MRv2 FileOutputFormat class, where it job config and filesystem configuration options can dynamically choose the output committer.&lt;/p&gt;

&lt;p&gt;Spark can use these with some binding classes to &lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;Add a subclass of &lt;tt&gt;HadoopMapReduceCommitProtocol&lt;/tt&gt; which uses the MRv2 classes and &lt;tt&gt;PathOutputCommitterFactory&lt;/tt&gt; to create the committers.&lt;/li&gt;
	&lt;li&gt;Add a &lt;tt&gt;BindingParquetOutputCommitter extends ParquetOutputCommitter&lt;/tt&gt;&lt;br/&gt;
to wire up Parquet output even when code requires the committer to be a subclass of &lt;tt&gt;ParquetOutputCommitter&lt;/tt&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;This patch builds on &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-23807&quot; title=&quot;Add Hadoop 3 profile with relevant POM fix ups&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-23807&quot;&gt;&lt;del&gt;SPARK-23807&lt;/del&gt;&lt;/a&gt; for setting up the dependencies.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13152273">SPARK-23977</key>
            <summary>Add commit protocol binding to Hadoop 3.1 PathOutputCommitter mechanism</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Minor</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="stevel@apache.org">Steve Loughran</assignee>
                                    <reporter username="stevel@apache.org">Steve Loughran</reporter>
                        <labels>
                    </labels>
                <created>Fri, 13 Apr 2018 12:39:58 +0000</created>
                <updated>Wed, 16 Feb 2022 15:28:11 +0000</updated>
                            <resolved>Thu, 15 Aug 2019 17:16:11 +0000</resolved>
                                    <version>2.4.0</version>
                                    <fixVersion>3.0.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>2</votes>
                                    <watches>17</watches>
                                                                                                                <comments>
                            <comment id="16437288" author="apachespark" created="Fri, 13 Apr 2018 13:17:05 +0000"  >&lt;p&gt;User &apos;steveloughran&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/21066&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/21066&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16465344" author="wjensen" created="Mon, 7 May 2018 01:34:48 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stevel%40apache.org&quot; class=&quot;user-hover&quot; rel=&quot;stevel@apache.org&quot;&gt;stevel@apache.org&lt;/a&gt; is the intention of this ticket&#160;to incorporate the Hadoop libraries within Spark itself, ie. no Hadoop dependency?&lt;br/&gt;
Trying to understand whether this is a viable solution for Spark on Kubernetes writing to S3&lt;/p&gt;</comment>
                            <comment id="16465917" author="stevel@apache.org" created="Mon, 7 May 2018 13:38:24 +0000"  >&lt;p&gt;It will need the hadoop-aws module and deoendencies as that is where the core code is. This patch just does the binding to the InsertIntoHadoopFS relation (move to Hadoop MRv2 FileOutputFormat &amp;amp; expect the new superclass, PathOutputCommtter, rather than always a FileOutputcommitter, and for Parquet, something similar with a ParquetOutputCommitter.&lt;/p&gt;

&lt;p&gt;+its only in Hadoop 3.1, though you can backport to branch-2, especially if you are prepared to bump up the minimum java version to 8 in that branch.&lt;/p&gt;

&lt;p&gt;t should work on k8s, given it works standalone. All it needs is an endpoint supporting the multipart upload operation of S3, which includes some non-AWS object stores.&lt;/p&gt;

&lt;p&gt; Look @ the &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-13786&quot; title=&quot;Add S3A committers for zero-rename commits to S3 endpoints&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-13786&quot;&gt;&lt;del&gt;HADOOP-13786&lt;/del&gt;&lt;/a&gt; work and the paper &lt;a href=&quot;https://github.com/steveloughran/zero-rename-committer/releases/download/tag_draft_003/a_zero_rename_committer.pdf&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;a zero rename committer&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;And there&apos;s some integration tests downstream in &lt;a href=&quot;https://github.com/hortonworks-spark/cloud-integration&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/hortonworks-spark/cloud-integration&lt;/a&gt; . I can help set you up to run those, if you email me directly. Essentially: you need to choose which stores to test against from: s3, openstack, azure, and configure them&lt;/p&gt;

&lt;p&gt;Note that of the two variant committers, &quot;staging&quot; and &quot;magic&quot;, the magic one needs a consistent S3 endpoint, which you only get on AWS S3 with an external services, usually dynamo DB based (S3mper, EMR consisent S3, S3Guard). The staging one needs enough local HDD to buffer the output of all active tasks, but doesn&apos;t need that consistency for its own query. You will need a plan for chaining together work though, which is inevitably one of &quot;consistency layer&quot; or &quot;wait long enough between writer and reader that you expect the metadata to be consistent&quot;&lt;/p&gt;

&lt;p&gt;Finally, if you are using spark to write directly to S3 today, without any consistency layer, then your commit algorithm had better not be mimicing directory rename by list + copy + delete. You need this code for safe as well as performant committing of work to S3.&lt;/p&gt;
</comment>
                            <comment id="16609245" author="cloud_fan" created="Mon, 10 Sep 2018 13:58:46 +0000"  >&lt;p&gt;I&apos;m removing the target version, since we are not going to merge it to 2.4&lt;/p&gt;</comment>
                            <comment id="16908315" author="vanzin" created="Thu, 15 Aug 2019 17:16:11 +0000"  >&lt;p&gt;Issue resolved by pull request 24970&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/24970&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/24970&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17308984" author="danzhi" created="Thu, 25 Mar 2021 21:39:14 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stevel%40apache.org&quot; class=&quot;user-hover&quot; rel=&quot;stevel@apache.org&quot;&gt;stevel@apache.org&lt;/a&gt;&#160;How to workaround following exception during the execution of &quot;INSERT OVERWRITE&quot; in spark.sql (spark 3.1.1 with hadoop 3.2)?&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&#160; if (dynamicPartitionOverwrite) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {

&#160; &#160; // until there&amp;#39;s explicit extensions to the PathOutputCommitProtocols

&#160; &#160; // to support the spark mechanism, it&amp;#39;s left to the individual committer

&#160; &#160; // choice to handle partitioning.

&#160; &#160; throw new IOException(PathOutputCommitProtocol.UNSUPPORTED)

&#160; }&lt;/span&gt; &lt;/div&gt;&lt;/blockquote&gt;</comment>
                            <comment id="17309397" author="stevel@apache.org" created="Fri, 26 Mar 2021 12:29:58 +0000"  >&lt;p&gt;use the partitioned committer and configure it to do the right thing when updating partitions (merge, delete everything already there, fail)&lt;/p&gt;</comment>
                            <comment id="17311818" author="danzhi" created="Tue, 30 Mar 2021 21:14:57 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stevel%40apache.org&quot; class=&quot;user-hover&quot; rel=&quot;stevel@apache.org&quot;&gt;stevel@apache.org&lt;/a&gt; Thanks for the info. Below are the related (key, value) we used:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;spark.hadoop.fs.s3a.committer.name &#8212; partitioned&lt;/li&gt;
	&lt;li&gt;spark.hadoop.mapreduce.outputcommitter.factory.scheme.s3a &#8212; org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory&lt;/li&gt;
	&lt;li&gt;spark.sql.sources.commitProtocolClass &#8212; org.apache.spark.internal.io.cloud.PathOutputCommitProtocol&lt;/li&gt;
	&lt;li&gt;spark.sql.parquet.output.committer.class &#8212; org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;3 &amp;amp; 4 appear to be necessary to ensure S3A committers being used by Spark for parquet outputs, except that &quot;INSERT OVERWRITE&quot; is blocked by the dynamicPartitionOverwrite exception.&#160;It will be helpful and appreciated if you can patiently elaborate on the proper way to &quot;use the partitioned committer and configure it to do the right thing ...&quot; in Spark. For example:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;PathOutputCommitProtocol appears to be needed but its constructor fails with the exception.&lt;/li&gt;
	&lt;li&gt;S3A committers honor &quot;fs.s3a.committer.staging.conflict-mode&quot; which needs to be &quot;replace&quot; for &quot;INSERT OVERWRITE&quot; but &quot;append&quot; for &quot;INSERT INTO&quot;. So it is spark.sql query specific. How to make spark.sql automatically set the right value?&lt;/li&gt;
	&lt;li&gt;Does above require code change in Spark or there is a configuration-only way?&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="17314296" author="stevel@apache.org" created="Sat, 3 Apr 2021 16:27:56 +0000"  >&lt;p&gt;the spark settings don&apos;t make it down from sql; you can do more at the RDD API level.&lt;/p&gt;

&lt;p&gt;The problem is that the spark partition insert logic all relies on renaming which has the O(data) performance penalty as well as the other commit correctness issues. Yes, something to do the pushdown could be done, or with the multipart APIs of &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-13186&quot; title=&quot;GenericOptionParser -libjars option validity check not always working because of bad local FS equality check&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-13186&quot;&gt;&lt;del&gt;HADOOP-13186&lt;/del&gt;&lt;/a&gt; give Spark a standard API to implement a zero rename committer in its own code.&lt;/p&gt;

&lt;p&gt;However, focus is on things like Iceberg and Delta lake, which offer more in terms of : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;atomic job commit&lt;/li&gt;
	&lt;li&gt;avoid the performance and cost issues of relying on directory tree scan as a way to identify source files.; the performance issues of doing all IO down a single shard of S3 storage.&lt;br/&gt;
I do not disagree with the direction of that work; we have to view the S3A committers (and IBM&apos;s Stocator + AWS EMR spark committers) as the final attempts to maintain that &quot;it&apos;s just a directory tree&quot; model into a cloud world where directories don&apos;t always exist, and listing them is measurable in hundreds of milliseconds. &lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="17436740" author="gumartinm" created="Mon, 1 Nov 2021 10:33:12 +0000"  >&lt;p&gt;Thank you very much &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stevel%40apache.org&quot; class=&quot;user-hover&quot; rel=&quot;stevel@apache.org&quot;&gt;stevel@apache.org&lt;/a&gt;&#160;for your explanations.&lt;/p&gt;

&lt;p&gt;I am experiencing the same problems as &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=danzhi&quot; class=&quot;user-hover&quot; rel=&quot;danzhi&quot;&gt;danzhi&lt;/a&gt;&#160;and your comments helped me a lot.&lt;/p&gt;

&lt;p&gt;Sad magic committer does not work with dynamic partition overwrite because it has an amazing performance when writing loads of JSON partitioned data.&lt;/p&gt;</comment>
                            <comment id="17437259" author="stevel@apache.org" created="Tue, 2 Nov 2021 10:06:20 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gumartinm&quot; class=&quot;user-hover&quot; rel=&quot;gumartinm&quot;&gt;gumartinm&lt;/a&gt; can I draw your attention to Apache Iceberg?&lt;/p&gt;

&lt;p&gt;meanwhile&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/MAPREDUCE-7341&quot; title=&quot;Add a task-manifest output committer for Azure and GCS&quot; class=&quot;issue-link&quot; data-issue-key=&quot;MAPREDUCE-7341&quot;&gt;&lt;del&gt;MAPREDUCE-7341&lt;/del&gt;&lt;/a&gt; adds a high performance targeting abfs and gcs; all tree scanning is in task commit, which is atomic; job commit aggressively parallelised and optimized for stores whose listStatusIterator calls are incremental with prefetching: we can start processing at the first page of task manifests found in a listing well the second Page is still being retrieved. Also in there: rate limiting, IO Statistics Collection.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-17833&quot; title=&quot;Improve Magic Committer Performance&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-17833&quot;&gt;&lt;del&gt;HADOOP-17833&lt;/del&gt;&lt;/a&gt; I will pick up some of that work, including incremental loading and rate limiting. And if we can keep reads and writes below the S3 IOPS limits, we should be able to avoid situations where we have to start sleeping and re-trying.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-17981&quot; title=&quot;Support etag-assisted renames in FileOutputCommitter&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-17981&quot;&gt;&lt;del&gt;HADOOP-17981&lt;/del&gt;&lt;/a&gt; is my homework this week -emergency work to deal with a rare but current failure in abfs under heavy load.&lt;/p&gt;</comment>
                            <comment id="17493293" author="itayb" created="Wed, 16 Feb 2022 15:28:11 +0000"  >&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;I follow the &lt;a href=&quot;https://spark.apache.org/docs/latest/cloud-integration.html#parquet-io-settings&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;recommendations&lt;/a&gt; and getting the following warning:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2022-02-16 15:22:03.292 WARN FlowThread0 ParquetOutputFormat: Setting parquet.enable.summary-metadata is deprecated, please use parquet.summary.metadata.level &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;What would be the recommended value for `parquet.summary.metadata.level ` ?&#160;&lt;br/&gt;
&#160;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13155673">HADOOP-15421</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            3 years, 38 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3sitb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>