<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 19:03:16 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-26422] Unable to disable Hive support in SparkR when Hadoop version is unsupported</title>
                <link>https://issues.apache.org/jira/browse/SPARK-26422</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;When we make a Spark session as below:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
sparkSession &amp;lt;- sparkR.session(&lt;span class=&quot;code-quote&quot;&gt;&quot;local[4]&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;SparkR&quot;&lt;/span&gt;, Sys.getenv(&lt;span class=&quot;code-quote&quot;&gt;&quot;SPARK_HOME&quot;&lt;/span&gt;),
                               list(spark.driver.extraClassPath = jarpaths,
                                    spark.executor.extraClassPath = jarpaths),
                               enableHiveSupport = FALSE)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I faced an issue that it&apos;s unable to disable Hive support explicitly with the error below:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
java.lang.reflect.InvocationTargetException
...
Caused by: java.lang.IllegalArgumentException: Unrecognized Hadoop major version number: 3.1.1.3.1.0.0-78
	at org.apache.hadoop.hive.shims.ShimLoader.getMajorVersion(ShimLoader.java:174)
	at org.apache.hadoop.hive.shims.ShimLoader.loadShims(ShimLoader.java:139)
	at org.apache.hadoop.hive.shims.ShimLoader.getHadoopShims(ShimLoader.java:100)
	at org.apache.hadoop.hive.conf.HiveConf$ConfVars.&amp;lt;clinit&amp;gt;(HiveConf.java:368)
	... 43 more
Error in handleErrors(returnStatus, conn) :
  java.lang.ExceptionInInitializerError
	at org.apache.hadoop.hive.conf.HiveConf.&amp;lt;clinit&amp;gt;(HiveConf.java:105)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;.forName0(Native Method)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;.forName(&lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:193)
	at org.apache.spark.sql.SparkSession$.hiveClassesArePresent(SparkSession.scala:1116)
	at org.apache.spark.sql.api.r.SQLUtils$.getOrCreateSparkSession(SQLUtils.scala:52)
	at org.apache.spark.sql.api.r.SQLUtils.getOrCreateSparkSession(SQLUtils.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.api.r.RBackendHandler.handleMethodCall(RBackendHandler.scala:167)
	at org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:108)
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt; </description>
                <environment></environment>
        <key id="13205759">SPARK-26422</key>
            <summary>Unable to disable Hive support in SparkR when Hadoop version is unsupported</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="gurwls223">Hyukjin Kwon</assignee>
                                    <reporter username="gurwls223">Hyukjin Kwon</reporter>
                        <labels>
                    </labels>
                <created>Thu, 20 Dec 2018 15:26:19 +0000</created>
                <updated>Mon, 12 Dec 2022 18:11:08 +0000</updated>
                            <resolved>Fri, 21 Dec 2018 08:11:43 +0000</resolved>
                                    <version>3.0.0</version>
                                    <fixVersion>2.3.3</fixVersion>
                    <fixVersion>2.4.1</fixVersion>
                    <fixVersion>3.0.0</fixVersion>
                                    <component>SparkR</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                                                                                <comments>
                            <comment id="16725946" author="githubbot" created="Thu, 20 Dec 2018 15:38:43 +0000"  >&lt;p&gt;HyukjinKwon opened a new pull request #23356: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26422&quot; title=&quot;Unable to disable Hive support in SparkR when Hadoop version is unsupported&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-26422&quot;&gt;&lt;del&gt;SPARK-26422&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;R&amp;#93;&lt;/span&gt; Support to disable Hive support in SparkR even for Hadoop versions unsupported by Hive fork&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/23356&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/23356&lt;/a&gt;&lt;/p&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;What changes were proposed in this pull request?&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;   Currently,  even if I explicitly disable Hive support in SparkR session as below:&lt;/p&gt;

&lt;p&gt;   ```r&lt;br/&gt;
   sparkSession &amp;lt;- sparkR.session(&quot;local&lt;span class=&quot;error&quot;&gt;&amp;#91;4&amp;#93;&lt;/span&gt;&quot;, &quot;SparkR&quot;, Sys.getenv(&quot;SPARK_HOME&quot;),&lt;br/&gt;
                                  enableHiveSupport = FALSE)&lt;br/&gt;
   ```&lt;/p&gt;

&lt;p&gt;   produces when the Hadoop version is not supported by our Hive fork:&lt;/p&gt;

&lt;p&gt;   ```&lt;br/&gt;
   java.lang.reflect.InvocationTargetException&lt;br/&gt;
   ...&lt;br/&gt;
   Caused by: java.lang.IllegalArgumentException: Unrecognized Hadoop major version number: 3.1.1.3.1.0.0-78&lt;br/&gt;
   	at org.apache.hadoop.hive.shims.ShimLoader.getMajorVersion(ShimLoader.java:174)&lt;br/&gt;
   	at org.apache.hadoop.hive.shims.ShimLoader.loadShims(ShimLoader.java:139)&lt;br/&gt;
   	at org.apache.hadoop.hive.shims.ShimLoader.getHadoopShims(ShimLoader.java:100)&lt;br/&gt;
   	at org.apache.hadoop.hive.conf.HiveConf$ConfVars.&amp;lt;clinit&amp;gt;(HiveConf.java:368)&lt;br/&gt;
   	... 43 more&lt;br/&gt;
   Error in handleErrors(returnStatus, conn) :&lt;br/&gt;
     java.lang.ExceptionInInitializerError&lt;br/&gt;
   	at org.apache.hadoop.hive.conf.HiveConf.&amp;lt;clinit&amp;gt;(HiveConf.java:105)&lt;br/&gt;
   	at java.lang.Class.forName0(Native Method)&lt;br/&gt;
   	at java.lang.Class.forName(Class.java:348)&lt;br/&gt;
   	at org.apache.spark.util.Utils$.classForName(Utils.scala:193)&lt;br/&gt;
   	at org.apache.spark.sql.SparkSession$.hiveClassesArePresent(SparkSession.scala:1116)&lt;br/&gt;
   	at org.apache.spark.sql.api.r.SQLUtils$.getOrCreateSparkSession(SQLUtils.scala:52)&lt;br/&gt;
   	at org.apache.spark.sql.api.r.SQLUtils.getOrCreateSparkSession(SQLUtils.scala)&lt;br/&gt;
   	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
   ```&lt;/p&gt;

&lt;p&gt;   The root cause is that:&lt;/p&gt;

&lt;p&gt;   ```&lt;br/&gt;
   SparkSession.hiveClassesArePresent&lt;br/&gt;
   ```&lt;/p&gt;

&lt;p&gt;   check if the class is loadable or not to check if that&apos;s in classpath but `org.apache.hadoop.hive.conf.HiveConf` has a check for Hadoop version as static logic which is executed right away. This throws an `IllegalArgumentException` and that&apos;s not caught:&lt;/p&gt;

&lt;p&gt;   &lt;a href=&quot;https://github.com/apache/spark/blob/36edbac1c8337a4719f90e4abd58d38738b2e1fb/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala#L1113-L1121&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/36edbac1c8337a4719f90e4abd58d38738b2e1fb/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala#L1113-L1121&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;   So, currently, if users have a Hive built-in Spark with unsupported Hadoop version by our fork (namely 3+), there&apos;s no way to use SparkR even thought it could work.&lt;/p&gt;

&lt;p&gt;   This PR just propose to change the order of bool comparison so that we can don&apos;t execute `SparkSession.hiveClassesArePresent` when:&lt;/p&gt;

&lt;p&gt;     1. `enableHiveSupport` is explicitly disabled&lt;br/&gt;
     2. `spark.sql.catalogImplementation` is `in-memory`&lt;/p&gt;

&lt;p&gt;   so that we *&lt;b&gt;only&lt;/b&gt;* check `SparkSession.hiveClassesArePresent` when Hive support is explicitly enabled by short short circuiting.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;How was this patch tested?&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;   It&apos;s difficult to write a test since we don&apos;t run tests against Hadoop 3 yet. See &lt;a href=&quot;https://github.com/apache/spark/pull/21588&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/21588&lt;/a&gt;. Manually tested.&lt;/p&gt;


&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16726525" author="gurwls223" created="Fri, 21 Dec 2018 08:11:43 +0000"  >&lt;p&gt;Issue resolved by pull request 23356&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/23356&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/23356&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16726526" author="githubbot" created="Fri, 21 Dec 2018 08:13:37 +0000"  >&lt;p&gt;asfgit closed pull request #23356: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26422&quot; title=&quot;Unable to disable Hive support in SparkR when Hadoop version is unsupported&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-26422&quot;&gt;&lt;del&gt;SPARK-26422&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;R&amp;#93;&lt;/span&gt; Support to disable Hive support in SparkR even for Hadoop versions unsupported by Hive fork&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/23356&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/23356&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/sql/core/src/main/scala/org/apache/spark/sql/api/r/SQLUtils.scala b/sql/core/src/main/scala/org/apache/spark/sql/api/r/SQLUtils.scala&lt;br/&gt;
index becb05cf72aba..e98cab8b56d13 100644&lt;br/&gt;
&amp;#8212; a/sql/core/src/main/scala/org/apache/spark/sql/api/r/SQLUtils.scala&lt;br/&gt;
+++ b/sql/core/src/main/scala/org/apache/spark/sql/api/r/SQLUtils.scala&lt;br/&gt;
@@ -49,9 +49,17 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;sql&amp;#93;&lt;/span&gt; object SQLUtils extends Logging {&lt;br/&gt;
       sparkConfigMap: JMap&lt;span class=&quot;error&quot;&gt;&amp;#91;Object, Object&amp;#93;&lt;/span&gt;,&lt;br/&gt;
       enableHiveSupport: Boolean): SparkSession = {&lt;br/&gt;
     val spark =&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (SparkSession.hiveClassesArePresent &amp;amp;&amp;amp; enableHiveSupport &amp;amp;&amp;amp;&lt;br/&gt;
+      if (enableHiveSupport &amp;amp;&amp;amp;&lt;br/&gt;
           jsc.sc.conf.get(CATALOG_IMPLEMENTATION.key, &quot;hive&quot;).toLowerCase(Locale.ROOT) ==&lt;/li&gt;
	&lt;li&gt;&quot;hive&quot;) {&lt;br/&gt;
+            &quot;hive&quot; &amp;amp;&amp;amp;&lt;br/&gt;
+          // Note that the order of conditions here are on purpose.&lt;br/&gt;
+          // `SparkSession.hiveClassesArePresent` checks if Hive&apos;s `HiveConf` is loadable or not;&lt;br/&gt;
+          // however, `HiveConf` itself has some static logic to check if Hadoop version is&lt;br/&gt;
+          // supported or not, which throws an `IllegalArgumentException` if unsupported.&lt;br/&gt;
+          // If this is checked first, there&apos;s no way to disable Hive support in the case above.&lt;br/&gt;
+          // So, we intentionally check if Hive classes are loadable or not only when&lt;br/&gt;
+          // Hive support is explicitly enabled by short-circuiting. See also &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26422&quot; title=&quot;Unable to disable Hive support in SparkR when Hadoop version is unsupported&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-26422&quot;&gt;&lt;del&gt;SPARK-26422&lt;/del&gt;&lt;/a&gt;.&lt;br/&gt;
+          SparkSession.hiveClassesArePresent) 
{
         SparkSession.builder().sparkContext(withHiveExternalCatalog(jsc.sc)).getOrCreate()
       }
&lt;p&gt; else {&lt;br/&gt;
         if (enableHiveSupport) {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;





&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 47 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|u0073s:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>