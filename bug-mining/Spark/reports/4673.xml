<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:52:12 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-17204] Spark 2.0 off heap RDD persistence with replication factor 2 leads to in-memory data corruption</title>
                <link>https://issues.apache.org/jira/browse/SPARK-17204</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;We use the &lt;tt&gt;OFF_HEAP&lt;/tt&gt; storage level extensively with great success. We&apos;ve tried off-heap storage with replication factor 2 and have always received exceptions on the executor side very shortly after starting the job. For example:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;com.esotericsoftware.kryo.KryoException: Encountered unregistered &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;ID: 9086
	at com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:137)
	at com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:670)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:781)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:229)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:169)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;java.lang.IndexOutOfBoundsException: Index: 6, Size: 0
	at java.util.ArrayList.rangeCheck(ArrayList.java:653)
	at java.util.ArrayList.get(ArrayList.java:429)
	at com.esotericsoftware.kryo.util.MapReferenceResolver.getReadObject(MapReferenceResolver.java:60)
	at com.esotericsoftware.kryo.Kryo.readReferenceOrNull(Kryo.java:834)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:788)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:229)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:169)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;java.lang.NullPointerException
	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$doExecute$1$$anonfun$6.apply(InMemoryTableScanExec.scala:141)
	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$doExecute$1$$anonfun$6.apply(InMemoryTableScanExec.scala:140)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We&apos;ve tried switching to Java serialization and get a different exception:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;java.io.StreamCorruptedException: invalid stream header: 780000D0
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:808)
	at java.io.ObjectInputStream.&amp;lt;init&amp;gt;(ObjectInputStream.java:301)
	at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.&amp;lt;init&amp;gt;(JavaSerializer.scala:63)
	at org.apache.spark.serializer.JavaDeserializationStream.&amp;lt;init&amp;gt;(JavaSerializer.scala:63)
	at org.apache.spark.serializer.JavaSerializerInstance.deserializeStream(JavaSerializer.scala:122)
	at org.apache.spark.serializer.SerializerManager.dataDeserializeStream(SerializerManager.scala:146)
	at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:433)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:672)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This suggest some kind of memory corruption to me.&lt;/p&gt;

&lt;p&gt;I&apos;ve been able to consistently reproduce this problem in local-cluster mode with a very simple code snippet. First, start a spark shell like this:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;MASTER=local-cluster[2,1,1024] ./spark-2.1/bin/spark-shell --conf spark.memory.offHeap.enabled=true --conf spark.memory.offHeap.size=1024
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then run the following:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.spark.storage.StorageLevel
val OFF_HEAP_2 = StorageLevel(useDisk = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, useMemory = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, useOffHeap = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, deserialized = &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;, replication = 2)
sc.range(0, 100).persist(OFF_HEAP_2).count
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="12999280">SPARK-17204</key>
            <summary>Spark 2.0 off heap RDD persistence with replication factor 2 leads to in-memory data corruption</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="michael">Michael MacFadden</assignee>
                                    <reporter username="michael">Michael MacFadden</reporter>
                        <labels>
                    </labels>
                <created>Tue, 23 Aug 2016 16:34:22 +0000</created>
                <updated>Fri, 24 Mar 2017 04:54:04 +0000</updated>
                            <resolved>Tue, 21 Mar 2017 03:55:39 +0000</resolved>
                                    <version>2.0.0</version>
                                    <fixVersion>2.0.3</fixVersion>
                    <fixVersion>2.1.1</fixVersion>
                    <fixVersion>2.2.0</fixVersion>
                                    <component>Spark Core</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>9</watches>
                                                                                                                <comments>
                            <comment id="15433394" author="rxin" created="Tue, 23 Aug 2016 18:55:52 +0000"  >&lt;p&gt;Does this problem still exist on today&apos;s master/branch-2.0? &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-16550&quot; title=&quot;Caching data with replication doesn&amp;#39;t replicate data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-16550&quot;&gt;&lt;del&gt;SPARK-16550&lt;/del&gt;&lt;/a&gt; was merged. It might be fixed already.&lt;/p&gt;</comment>
                            <comment id="15433408" author="michael" created="Tue, 23 Aug 2016 19:05:07 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rxin&quot; class=&quot;user-hover&quot; rel=&quot;rxin&quot;&gt;rxin&lt;/a&gt; I&apos;ll give it a try. Thanks for the heads up. I missed that Jira/PR.&lt;/p&gt;</comment>
                            <comment id="15433878" author="michael" created="Tue, 23 Aug 2016 23:51:00 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rxin&quot; class=&quot;user-hover&quot; rel=&quot;rxin&quot;&gt;rxin&lt;/a&gt; I rebuilt from master as of commit 8fd63e808e15c8a7e78fef847183c86f332daa91 (which includes &lt;a href=&quot;https://github.com/apache/spark/commit/8e223ea67acf5aa730ccf688802f17f6fc10907c&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/commit/8e223ea67acf5aa730ccf688802f17f6fc10907c&lt;/a&gt;) and am still experiencing this issue. I&apos;ll work on instructions to reproduce next.&lt;/p&gt;</comment>
                            <comment id="15436179" author="jerryshao" created="Thu, 25 Aug 2016 03:00:19 +0000"  >&lt;p&gt;It works OK in my local test with latest build:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;val OFF_HEAP_2 = StorageLevel(useDisk = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, useMemory = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, useOffHeap = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, deserialized = &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;, replication = 2)
sc.range(0, 0).persist(OFF_HEAP_2).count
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Also I&apos;m curious why SparkSQL related code will be involved according to the exception you pasted above, are you using &lt;tt&gt;SparkSession#range&lt;/tt&gt; instead. Also tested Dataset persist with &lt;tt&gt;OFF_HEAP_2&lt;/tt&gt;, it also works fine without exception. &lt;/p&gt;
</comment>
                            <comment id="15436202" author="michael" created="Thu, 25 Aug 2016 03:22:10 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jerryshao&quot; class=&quot;user-hover&quot; rel=&quot;jerryshao&quot;&gt;jerryshao&lt;/a&gt;. I wonder if you&apos;re testing in local mode? I only see this problem when running with remote executors on a cluster. When I run in local mode, I see a bunch of warnings like:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;...
16/08/25 03:13:55 WARN storage.BlockManager: Block rdd_3_8 replicated to only 0 peer(s) instead of 1 peers
16/08/25 03:13:55 WARN storage.BlockManager: Block rdd_3_15 replicated to only 0 peer(s) instead of 1 peers
16/08/25 03:13:55 WARN storage.BlockManager: Block rdd_3_9 replicated to only 0 peer(s) instead of 1 peers
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;These messages suggest to me no actual replication is being attempted, and that&apos;s why the problem is not manifested. To answer your other question, the test case I provided was something very simple I came up with after discovering this problem. My coworker was reading data from parquet files when I cut-n-pasted those stack traces.&lt;/p&gt;

&lt;p&gt;I&apos;ll clarify these points in the description.&lt;/p&gt;</comment>
                            <comment id="15436205" author="jerryshao" created="Thu, 25 Aug 2016 03:24:42 +0000"  >&lt;p&gt;No, I tested in yarn cluster, not local mode.&lt;/p&gt;</comment>
                            <comment id="15436213" author="jerryshao" created="Thu, 25 Aug 2016 03:31:34 +0000"  >&lt;p&gt;I think to reflect the issue &lt;tt&gt;sc.range(0, 0)&lt;/tt&gt; should be changed to &lt;tt&gt;sc.range(0, 2)&lt;/tt&gt;, &lt;tt&gt;range(0, 0)&lt;/tt&gt; actually persist nothing to memory.&lt;/p&gt;</comment>
                            <comment id="15436218" author="michael" created="Thu, 25 Aug 2016 03:36:02 +0000"  >&lt;p&gt;I would think that, but &lt;tt&gt;sc.range(0, 0)&lt;/tt&gt; throws the exception, too. Are you able to reproduce the problem with &lt;tt&gt;sc.range(0, 2)&lt;/tt&gt;?&lt;/p&gt;</comment>
                            <comment id="15436222" author="jerryshao" created="Thu, 25 Aug 2016 03:40:05 +0000"  >&lt;p&gt;Yes, I could reproduce this issue, but not constantly, sometimes it is OK without any exception.&lt;/p&gt;</comment>
                            <comment id="15706030" author="rxin" created="Tue, 29 Nov 2016 18:02:32 +0000"  >&lt;p&gt;Can you try repro this using the local-cluster mode?&lt;/p&gt;</comment>
                            <comment id="15706031" author="michael" created="Tue, 29 Nov 2016 18:02:32 +0000"  >&lt;p&gt;FYI I&apos;ve noticed this remains an issue in the Spark 2.1 branch. I don&apos;t know if I&apos;ll have time to really dig into this any time in the near future. I&apos;m hoping someone with expertise in this subsystem can have a look. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=joshrosen&quot; class=&quot;user-hover&quot; rel=&quot;joshrosen&quot;&gt;joshrosen&lt;/a&gt;?&lt;/p&gt;</comment>
                            <comment id="15706056" author="michael" created="Tue, 29 Nov 2016 18:08:06 +0000"  >&lt;p&gt;Is your question directed at me? The RDD storage blocks don&apos;t seem to be replicated in that mode, and I believe that&apos;s why this problem does not manifest itself. Or maybe I misunderstand what you mean by &quot;local-cluster&quot; mode.&lt;/p&gt;</comment>
                            <comment id="15706069" author="rxin" created="Tue, 29 Nov 2016 18:11:58 +0000"  >&lt;p&gt;local-cluster is different from the local mode. It is a local &quot;cluster&quot; with multiple processes.&lt;/p&gt;

&lt;p&gt;Try &lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;&amp;gt; MASTER=local-cluster[2,1,1024] bin/spark-shell
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15706130" author="michael" created="Tue, 29 Nov 2016 18:33:24 +0000"  >&lt;p&gt;I&apos;m able to reproduce the problem with this configuration, and I&apos;ve updated this ticket&apos;s description to reflect that. Thanks.&lt;/p&gt;</comment>
                            <comment id="15791884" author="michael" created="Mon, 2 Jan 2017 00:19:56 +0000"  >&lt;p&gt;I&apos;m 99% sure I&apos;ve fixed this. I&apos;ll submit a PR in the coming days.&lt;/p&gt;</comment>
                            <comment id="15808197" author="apachespark" created="Sat, 7 Jan 2017 21:39:04 +0000"  >&lt;p&gt;User &apos;mallman&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/16499&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/16499&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15936733" author="apachespark" created="Wed, 22 Mar 2017 17:22:03 +0000"  >&lt;p&gt;User &apos;mallman&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/17390&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/17390&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 34 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i32nrb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>