<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:45:54 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-16632] Vectorized parquet reader fails to read certain fields from Hive tables</title>
                <link>https://issues.apache.org/jira/browse/SPARK-16632</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;The vectorized parquet reader fails to read certain tables created by Hive. When the tables have type &quot;tinyint&quot; or &quot;smallint&quot;, Catalyst converts those to &quot;ByteType&quot; and &quot;ShortType&quot; respectively. But when Hive writes those tables in parquet format, the parquet schema in the files contains &quot;int32&quot; fields.&lt;/p&gt;

&lt;p&gt;To reproduce, run these commands in the hive shell (or beeline):&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;create table abyte (value tinyint) stored as parquet;
create table ashort (value smallint) stored as parquet;
insert into abyte values (1);
insert into ashort values (1);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then query them with Spark 2.0:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;spark.sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;select * from abyte&quot;&lt;/span&gt;).show();
spark.sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;select * from ashort&quot;&lt;/span&gt;).show();
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You&apos;ll see this exception (for the byte case):&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;16/07/13 12:24:23 ERROR datasources.InsertIntoHadoopFsRelationCommand: Aborting job.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, scm-centos71-iqalat-2.gce.cloudera.com): org.apache.spark.SparkException: Task failed while writing rows
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
	at org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.getByte(OnHeapColumnVector.java:159)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253)
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252)
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1325)
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:258)
	... 8 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This works when you point Spark directly at the files (instead of using the metastore data), or when you disable the vectorized parquet reader.&lt;/p&gt;

&lt;p&gt;The root cause seems to be that Hive creates these tables with a not-so-complete schema:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;$ parquet-tools schema /tmp/byte.parquet 
message hive_schema {
  optional int32 value;
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There&apos;s no indication that the field is a 32-bit field used to store 8-bit values. When the ParquetReadSupport code tries to consolidate both schemas, it just chooses whatever is in the parquet file for primitive types (see ParquetReadSupport.clipParquetType); the vectorized reader uses the catalyst schema, which comes from the Hive metastore, and says it&apos;s a byte field, so when it tries to read the data, the byte data stored in &quot;OnHeapColumnVector&quot; is null.&lt;/p&gt;

&lt;p&gt;I have tested a small change to &lt;tt&gt;ParquetReadSupport.clipParquetType&lt;/tt&gt; that fixes this particular issue, but I haven&apos;t run any other tests, so I&apos;ll do that while I wait for others to chime in and maybe tell me that&apos;s not the right place to fix this.&lt;/p&gt;</description>
                <environment>&lt;p&gt;Hive 1.1 (CDH)&lt;/p&gt;</environment>
        <key id="12990936">SPARK-16632</key>
            <summary>Vectorized parquet reader fails to read certain fields from Hive tables</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="vanzin">Marcelo Masiero Vanzin</assignee>
                                    <reporter username="vanzin">Marcelo Masiero Vanzin</reporter>
                        <labels>
                    </labels>
                <created>Tue, 19 Jul 2016 22:11:32 +0000</created>
                <updated>Fri, 14 Oct 2016 05:28:11 +0000</updated>
                            <resolved>Wed, 20 Jul 2016 05:06:56 +0000</resolved>
                                    <version>2.0.0</version>
                                    <fixVersion>2.0.1</fixVersion>
                    <fixVersion>2.1.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>7</watches>
                                                                                                                <comments>
                            <comment id="15384996" author="yhuai" created="Tue, 19 Jul 2016 22:33:46 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vanzin&quot; class=&quot;user-hover&quot; rel=&quot;vanzin&quot;&gt;vanzin&lt;/a&gt; So, you mean that OnHeapColumnVector does not reserve byteData in reserveInternal?&lt;/p&gt;</comment>
                            <comment id="15385001" author="vanzin" created="Tue, 19 Jul 2016 22:36:33 +0000"  >&lt;p&gt;I mean that because it only considers the type specified in the parquet file, it initializes the &lt;tt&gt;intData&lt;/tt&gt; field. But when processing the query, Catalyst uses the Hive type (&lt;tt&gt;tinyint&lt;/tt&gt; =&amp;gt; &lt;tt&gt;ByteType&lt;/tt&gt;) and tries to read from &lt;tt&gt;byteData&lt;/tt&gt;, which is null.&lt;/p&gt;</comment>
                            <comment id="15385068" author="apachespark" created="Tue, 19 Jul 2016 23:37:05 +0000"  >&lt;p&gt;User &apos;vanzin&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/14272&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/14272&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15385345" author="lian cheng" created="Wed, 20 Jul 2016 05:06:56 +0000"  >&lt;p&gt;Issue resolved by pull request 14272&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/14272&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/14272&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15385399" author="lian cheng" created="Wed, 20 Jul 2016 05:50:56 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vanzin&quot; class=&quot;user-hover&quot; rel=&quot;vanzin&quot;&gt;vanzin&lt;/a&gt; Did you post the wrong stack trace? This issue is about the read path, but the stack trace is about write path.&lt;/p&gt;</comment>
                            <comment id="15385451" author="lian cheng" created="Wed, 20 Jul 2016 06:42:43 +0000"  >&lt;p&gt;Discussed with &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yhuai&quot; class=&quot;user-hover&quot; rel=&quot;yhuai&quot;&gt;yhuai&lt;/a&gt; after merging &lt;a href=&quot;https://github.com/apache/spark/pull/14272&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;PR #14272&lt;/a&gt; and found that there&apos;s a much simpler fix for this issue.&lt;/p&gt;

&lt;p&gt;Firstly, the real root cause of this issue is &lt;a href=&quot;https://github.com/apache/spark/blob/v2.0.0-rc5/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase.java#L139&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;this line&lt;/a&gt; in &lt;tt&gt;SpecificParquetRecordReaderBase&lt;/tt&gt;. In this line, we converted Parquet type &lt;tt&gt;requestedSchema&lt;/tt&gt; into &lt;tt&gt;sparkSchema&lt;/tt&gt;, which is a Spark &lt;tt&gt;StructType&lt;/tt&gt;, and use it as the requested schema passed down from the query planner.&lt;/p&gt;

&lt;p&gt;However, &lt;tt&gt;requestedSchema&lt;/tt&gt; is tailored from the Parquet schema read from the physical file to be scanned, and doesn&apos;t contain proper Parquet type annotation since it&apos;s written by Hive. Thus &lt;tt&gt;sparkSchema&lt;/tt&gt; has the wrong type information.&lt;/p&gt;

&lt;p&gt;On the other hand, we always set the Spark requested schema in the Hadoop configuration. This is done in &lt;tt&gt;ParquetFileFormat.initializeLocalJobFunc()&lt;/tt&gt; (see &lt;a href=&quot;https://github.com/apache/spark/blob/v2.0.0-rc5/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala#L292-L294&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt;). Thus, instead of converting from &lt;tt&gt;requestedSchema&lt;/tt&gt;, we can simply read out the original Spark requested schema and set it to &lt;tt&gt;sparkSchema&lt;/tt&gt; to fix this issue.&lt;/p&gt;</comment>
                            <comment id="15385458" author="apachespark" created="Wed, 20 Jul 2016 06:53:04 +0000"  >&lt;p&gt;User &apos;liancheng&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/14278&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/14278&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15386409" author="vanzin" created="Wed, 20 Jul 2016 18:43:50 +0000"  >&lt;p&gt;Yes, that&apos;s the right stack trace. It&apos;s a CTAS query which is probably why the write path shows up there.&lt;/p&gt;</comment>
                            <comment id="15387431" author="apachespark" created="Thu, 21 Jul 2016 09:30:08 +0000"  >&lt;p&gt;User &apos;liancheng&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/14300&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/14300&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15387791" author="lian cheng" created="Thu, 21 Jul 2016 14:39:49 +0000"  >&lt;p&gt;Oh, I see, thanks for the explanation.&lt;/p&gt;</comment>
                            <comment id="15574251" author="dongjoon" created="Fri, 14 Oct 2016 05:27:27 +0000"  >&lt;p&gt;This was backported at the following commit.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/spark/commit/f9367d6&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/commit/f9367d6&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15574253" author="dongjoon" created="Fri, 14 Oct 2016 05:28:11 +0000"  >&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;spark-2.0:branch-2.0$ git log --oneline | grep SPARK-16632
933d76a [SPARK-16632][SQL] Revert PR #14272: Respect Hive schema when merging parquet schema
f9367d6 [SPARK-16632][SQL] Use Spark requested schema to guide vectorized Parquet reader initialization
c2b5b3c [SPARK-16632][SQL] Respect Hive schema when merging parquet schema.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12990998">HIVE-14294</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 5 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3189z:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>