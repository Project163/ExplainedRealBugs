<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 19:05:26 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-23191] Workers registration failes in case of network drop</title>
                <link>https://issues.apache.org/jira/browse/SPARK-23191</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;We have a 3 node cluster. We were facing issues of multiple driver running in some scenario in production.&lt;/p&gt;

&lt;p&gt;On further investigation we were able to reproduce iin both 1.6.3 and 2.2.1 versions the scenario with following steps:-&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Setup a 3 node cluster. Start master and slaves.&lt;/li&gt;
	&lt;li&gt;On any node where the worker process is running block the connections on port 7077 using iptables.
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
iptables -A OUTPUT -p tcp --dport 7077 -j DROP
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;ol&gt;
	&lt;li&gt;After about 10-15 secs we get the error on node that it is unable to connect to master.
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2018-01-23 12:08:51,639 [rpc-client-1-1] WARN&#160; org.apache.spark.network.server.TransportChannelHandler - Exception in connection from &amp;lt;servername&amp;gt;
java.io.IOException: Connection timed out
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at sun.nio.ch.IOUtil.read(IOUtil.java:192)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:221)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:899)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:275)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:119)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
2018-01-23 12:08:51,647 [dispatcher-event-loop-0] ERROR org.apache.spark.deploy.worker.Worker - Connection to master failed! Waiting &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; master to reconnect...
2018-01-23 12:08:51,647 [dispatcher-event-loop-0] ERROR org.apache.spark.deploy.worker.Worker - Connection to master failed! Waiting &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; master to reconnect...

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;ol&gt;
	&lt;li&gt;Once we get this exception we renable the connections to port 7077 using
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
iptables -D OUTPUT -p tcp --dport 7077 -j DROP
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;ol&gt;
	&lt;li&gt;Worker tries to register again with master but is unable to do so. It gives following error&lt;/li&gt;
&lt;/ol&gt;


&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2018-01-23 12:08:58,657 [worker-register-master-threadpool-2] WARN&#160; org.apache.spark.deploy.worker.Worker - Failed to connect to master &amp;lt;servername&amp;gt;:7077
org.apache.spark.SparkException: Exception thrown in awaitResult:
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:205)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:100)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:108)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.deploy.worker.Worker$$anonfun$org$apache$spark$deploy$worker$Worker$$tryRegisterAllMasters$1$$anon$1.run(Worker.scala:241)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at java.util.concurrent.FutureTask.run(FutureTask.java:266)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
Caused by: java.io.IOException: Failed to connect to &amp;lt;servername&amp;gt;:7077
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:197)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:194)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:190)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; ... 4 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection timed out: &amp;lt;servername&amp;gt;:7077
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:631)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
&#160;&#160;&#160;&#160;&#160;&#160;&#160; ... 1 more
2018-01-23 12:09:03,705 [dispatcher-event-loop-5] ERROR org.apache.spark.deploy.worker.Worker - Worker registration failed: Duplicate worker ID
2018-01-23 12:09:03,705 [dispatcher-event-loop-5] ERROR org.apache.spark.deploy.worker.Worker - Worker registration failed: Duplicate worker ID&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;ol&gt;
	&lt;li&gt;The worker state is changed to DEAD in spark UI. As a result of which duplicate driver is launched.&lt;/li&gt;
&lt;/ol&gt;
</description>
                <environment>&lt;p&gt;OS:- Centos 6.9(64 bit)&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</environment>
        <key id="13133079">SPARK-23191</key>
            <summary>Workers registration failes in case of network drop</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="Ngone51">wuyi</assignee>
                                    <reporter username="neeraj20gupta">Neeraj Gupta</reporter>
                        <labels>
                    </labels>
                <created>Tue, 23 Jan 2018 15:16:18 +0000</created>
                <updated>Tue, 22 Sep 2020 14:26:37 +0000</updated>
                            <resolved>Tue, 28 May 2019 04:01:42 +0000</resolved>
                                    <version>1.6.3</version>
                    <version>2.2.1</version>
                    <version>2.3.0</version>
                                    <fixVersion>3.0.0</fixVersion>
                                    <component>Spark Core</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="16415194" author="suj1th" created="Tue, 27 Mar 2018 08:09:48 +0000"  >&lt;p&gt;This is a known race condition, in which the reconnection attempt made by the worker after the network outage is seen as a request to register a duplicate worker on the (same) master and hence, the attempt fails. Details on this can be found in &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-4592&quot; title=&quot;&amp;quot;Worker registration failed: Duplicate worker ID&amp;quot; error during Master failover&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-4592&quot;&gt;&lt;del&gt;SPARK-4592&lt;/del&gt;&lt;/a&gt;. Although the race condition is resolved for the case in which a new master replaces the unresponsive master, it still exists when the same old master recovers, which is the case here.&lt;/p&gt;</comment>
                            <comment id="16829051" author="zuo.tingbing9" created="Mon, 29 Apr 2019 09:21:00 +0000"  >&lt;p&gt;we faced the same issue in standalone HA mode. Could you please take a view on this&#160;issue?&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2019-03-15 20:22:10,474 INFO Worker: Master has changed, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; master is at spark:&lt;span class=&quot;code-comment&quot;&gt;//vmax17:7077 
&lt;/span&gt;2019-03-15 20:22:14,862 INFO Worker: Master with url spark:&lt;span class=&quot;code-comment&quot;&gt;//vmax18:7077 requested &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; worker to reconnect.
&lt;/span&gt;2019-03-15 20:22:14,863 INFO Worker: Connecting to master vmax18:7077... 
2019-03-15 20:22:14,863 INFO Worker: Connecting to master vmax17:7077... 
2019-03-15 20:22:14,865 INFO Worker: Master with url spark:&lt;span class=&quot;code-comment&quot;&gt;//vmax18:7077 requested &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; worker to reconnect.
&lt;/span&gt;2019-03-15 20:22:14,865 INFO Worker: Not spawning another attempt to register with the master, since there is an attempt scheduled already. 
2019-03-15 20:22:14,868 INFO Worker: Master with url spark:&lt;span class=&quot;code-comment&quot;&gt;//vmax18:7077 requested &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; worker to reconnect. 
&lt;/span&gt;2019-03-15 20:22:14,868 INFO Worker: Not spawning another attempt to register with the master, since there is an attempt scheduled already. 
2019-03-15 20:22:14,871 INFO Worker: Master with url spark:&lt;span class=&quot;code-comment&quot;&gt;//vmax18:7077 requested &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; worker to reconnect. 
&lt;/span&gt;2019-03-15 20:22:14,871 INFO Worker: Not spawning another attempt to register with the master, since there is an attempt scheduled already. 
2019-03-15 20:22:14,879 ERROR Worker: Worker registration failed: Duplicate worker ID
2019-03-15 20:22:14,891 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,891 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,893 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,893 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,893 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,894 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,894 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,894 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,894 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,894 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,894 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,894 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,894 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,895 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,895 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,895 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,895 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,895 INFO ExecutorRunner: Killing process! 
2019-03-15 20:22:14,896 INFO ShutdownHookManager: Shutdown hook called 
2019-03-15 20:22:14,898 INFO ShutdownHookManager: Deleting directory /data4/zdh/spark/tmp/spark-c578bf32-6a5e-44a5-843b-c796f44648ee 
2019-03-15 20:22:14,908 INFO ShutdownHookManager: Deleting directory /data3/zdh/spark/tmp/spark-7e57e77d-cbb7-47d3-a6dd-737b57788533 
2019-03-15 20:22:14,920 INFO ShutdownHookManager: Deleting directory /data2/zdh/spark/tmp/spark-0beebf20-abbd-4d99-a401-3ef0e88e0b05&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=andrewor14&quot; class=&quot;user-hover&quot; rel=&quot;andrewor14&quot;&gt;andrewor14&lt;/a&gt;&#160; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cloud_fan&quot; class=&quot;user-hover&quot; rel=&quot;cloud_fan&quot;&gt;cloud_fan&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vanzin&quot; class=&quot;user-hover&quot; rel=&quot;vanzin&quot;&gt;vanzin&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16829260" author="cloud_fan" created="Mon, 29 Apr 2019 13:22:00 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Ngone51&quot; class=&quot;user-hover&quot; rel=&quot;Ngone51&quot;&gt;Ngone51&lt;/a&gt; can you take a look please?&lt;/p&gt;</comment>
                            <comment id="16829272" author="ngone51" created="Mon, 29 Apr 2019 13:42:51 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cloud_fan&quot; class=&quot;user-hover&quot; rel=&quot;cloud_fan&quot;&gt;cloud_fan&lt;/a&gt; Ok, I&apos;ll have a deep look after 5.1 holiday.&lt;/p&gt;</comment>
                            <comment id="16834703" author="ngone51" created="Tue, 7 May 2019 12:20:30 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=neeraj20gupta&quot; class=&quot;user-hover&quot; rel=&quot;neeraj20gupta&quot;&gt;neeraj20gupta&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;What do you mean by&#160;&lt;em&gt;multiple driver running in some scenario&lt;/em&gt; &amp;amp;&#160;&lt;em&gt;As a result of which duplicate driver is launched ?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Do you mean there&apos;re multiple drivers running&#160;concurrently for one app ?&#160;&lt;/p&gt;</comment>
                            <comment id="16835256" author="zuo.tingbing9" created="Wed, 8 May 2019 02:19:43 +0000"  >&lt;p&gt;See these detail logs, master&#160;changed from&#160;vmax18 to&#160;vmax17.&lt;/p&gt;

&lt;p&gt;In master vmax18,&#160; worker&#160;be removed because got no heartbeat but soon got heartbeat and&#160;asking to re-register with master vmax18(will&#160; tryRegisterAllMaster() which include master&#160;vmax17).&lt;/p&gt;

&lt;p&gt;In the same time, worker has bean registered with master&#160;vmax17 when&#160;master vmax17 got leadership.&lt;/p&gt;

&lt;p&gt;So&#160;Worker registration failed: Duplicate worker ID.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;spark-mr-master-vmax18.log&#65306;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2019-03-15 20:22:09,441 INFO ZooKeeperLeaderElectionAgent: We have lost leadership
2019-03-15 20:22:14,544 WARN Master: Removing worker-20190218183101-vmax18-33129 because we got no heartbeat in 60 seconds
2019-03-15 20:22:14,544 INFO Master: Removing worker worker-20190218183101-vmax18-33129 on vmax18:33129
2019-03-15 20:22:14,864 WARN Master: Got heartbeat from unregistered worker worker-20190218183101-vmax18-33129. Asking it to re-register.
2019-03-15 20:22:14,975 ERROR Master: Leadership has been revoked -- master shutting down.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;spark-mr-master-vmax17.log:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2019-03-15 20:22:14,870 INFO Master: Registering worker vmax18:33129 with 21 cores, 125.0 GB RAM
2019-03-15 20:22:15,261 INFO Master: vmax18:33129 got disassociated, removing it.
2019-03-15 20:22:15,263 INFO Master: Removing worker worker-20190218183101-vmax18-33129 on vmax18:33129
2019-03-15 20:22:15,311 ERROR Inbox: Ignoring error
org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; /spark/master_status/worker_worker-20190218183101-vmax18-33129
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;spark-mr-worker-vmax18.log:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2019-03-15 20:22:10,474 INFO Worker: Master has changed, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; master is at spark:&lt;span class=&quot;code-comment&quot;&gt;//vmax17:7077
&lt;/span&gt;2019-03-15 20:22:14,862 INFO Worker: Master with url spark:&lt;span class=&quot;code-comment&quot;&gt;//vmax18:7077 requested &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; worker to reconnect.
&lt;/span&gt;2019-03-15 20:22:14,865 INFO Worker: Not spawning another attempt to register with the master, since there is an attempt scheduled already.
2019-03-15 20:22:14,879 ERROR Worker: Worker registration failed: Duplicate worker ID
2019-03-15 20:22:14,895 INFO ExecutorRunner: Killing process!
2019-03-15 20:22:14,896 INFO ShutdownHookManager: Shutdown hook called&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;PS, this will result another issue: The leader will always in COMPLETING_RECOVERY state.&lt;/p&gt;

&lt;p&gt;worker-vmax18 shut down cause duplicate worker ID,and clear the worker&apos;s node on persist Engine(we use zookeeper).&#160;Then the new leader(master-vmax17) find the worker died and trying to remove it ,and try to clear the node on zookeeper,but the node has been removed yet&#160;during worker-vmax18 shut down ,so &lt;font color=&quot;#ff0000&quot;&gt;&lt;b&gt;an exception was thrown in function&#160;completeRecovery()&lt;/b&gt;&#160;&lt;b&gt;. Then the leader will always in COMPLETING_RECOVERY state.&lt;/b&gt;&lt;/font&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16837756" author="neeraj20gupta" created="Sat, 11 May 2019 05:18:26 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Ngone51&quot; class=&quot;user-hover&quot; rel=&quot;Ngone51&quot;&gt;Ngone51&lt;/a&gt;,&lt;/p&gt;

&lt;p&gt;Yes in my case multiple drivers were running concurrently for one app.&lt;/p&gt;</comment>
                            <comment id="16838237" author="ngone51" created="Mon, 13 May 2019 03:34:46 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=neeraj20gupta&quot; class=&quot;user-hover&quot; rel=&quot;neeraj20gupta&quot;&gt;neeraj20gupta&lt;/a&gt;&#160; Can you explain more about the part of&#160;&lt;em&gt;running concurrently for one app ?&lt;/em&gt;&#160;&lt;/p&gt;

&lt;p&gt;In my understanding, if the worker shipped with drivers exited due to&#160; duplicate resgister, those drivers would also be killed.&#160;&lt;/p&gt;

&lt;p&gt;So, how does it(&lt;em&gt;running concurrently for one app&lt;/em&gt;) happens ?&#160;&lt;/p&gt;</comment>
                            <comment id="16839435" author="neeraj20gupta" created="Tue, 14 May 2019 13:56:06 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Ngone51&quot; class=&quot;user-hover&quot; rel=&quot;Ngone51&quot;&gt;Ngone51&lt;/a&gt;,&lt;/p&gt;

&lt;p&gt;In our case we were running the driver with &quot;supervise&quot; flag. So during the network&#160;glitch that driver was assumed to be dead and a new driver was spawned.&#160;&lt;br/&gt;
However at that time(after the network glitch was over) we saw that the old driver was also in running state. This might be some boundary condition.&lt;br/&gt;
As a result&#160;of this we did a&#160;fix&#160;to remove supervise flag and create monitoring/alerting for the driver process.&lt;/p&gt;</comment>
                            <comment id="16849314" author="cloud_fan" created="Tue, 28 May 2019 04:01:42 +0000"  >&lt;p&gt;Issue resolved by pull request 24569&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/24569&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/24569&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310660">
                    <name>Completes</name>
                                            <outwardlinks description="fixes">
                                        <issuelink>
            <issuekey id="12982630">SPARK-16190</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 25 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3p94f:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>