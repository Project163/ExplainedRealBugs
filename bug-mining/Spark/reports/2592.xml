<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:33:41 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-11153] Turns off Parquet filter push-down for string and binary columns</title>
                <link>https://issues.apache.org/jira/browse/SPARK-11153</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Due to &lt;a href=&quot;https://issues.apache.org/jira/browse/PARQUET-251&quot; title=&quot;Binary column statistics error when reuse byte[] among rows&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PARQUET-251&quot;&gt;&lt;del&gt;PARQUET-251&lt;/del&gt;&lt;/a&gt;, &lt;tt&gt;BINARY&lt;/tt&gt; columns in existing Parquet files may be written with corrupted statistics information. This information is used by filter push-down optimization. Since Spark 1.5 turns on Parquet filter push-down by default, we may end up with wrong query results. &lt;a href=&quot;https://issues.apache.org/jira/browse/PARQUET-251&quot; title=&quot;Binary column statistics error when reuse byte[] among rows&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PARQUET-251&quot;&gt;&lt;del&gt;PARQUET-251&lt;/del&gt;&lt;/a&gt; has been fixed in parquet-mr 1.8.1, but Spark 1.5 is still using 1.7.0.&lt;/p&gt;

&lt;p&gt;Note that this kind of corrupted Parquet files could be produced by any Parquet data models.&lt;/p&gt;

&lt;p&gt;This affects all Spark SQL data types that can be mapped to Parquet &lt;tt&gt;BINARY&lt;/tt&gt;, namely:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&lt;tt&gt;StringType&lt;/tt&gt;&lt;/li&gt;
	&lt;li&gt;&lt;tt&gt;BinaryType&lt;/tt&gt;&lt;/li&gt;
	&lt;li&gt;&lt;tt&gt;DecimalType&lt;/tt&gt; (but Spark SQL doesn&apos;t support pushing down &lt;tt&gt;DecimalType&lt;/tt&gt; columns for now.)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;To avoid wrong query results, we should disable filter push-down for columns of &lt;tt&gt;StringType&lt;/tt&gt; and &lt;tt&gt;BinaryType&lt;/tt&gt; until we upgrade to parquet-mr 1.8.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12905608">SPARK-11153</key>
            <summary>Turns off Parquet filter push-down for string and binary columns</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="lian cheng">Cheng Lian</assignee>
                                    <reporter username="lian cheng">Cheng Lian</reporter>
                        <labels>
                    </labels>
                <created>Fri, 16 Oct 2015 18:22:02 +0000</created>
                <updated>Wed, 1 Jun 2016 23:35:08 +0000</updated>
                            <resolved>Wed, 21 Oct 2015 01:03:29 +0000</resolved>
                                    <version>1.5.0</version>
                    <version>1.5.1</version>
                                    <fixVersion>1.5.2</fixVersion>
                    <fixVersion>1.6.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>10</watches>
                                                                                                                <comments>
                            <comment id="14961147" author="marmbrus" created="Fri, 16 Oct 2015 18:25:54 +0000"  >&lt;p&gt;Its actually corrupted statistics in data that is written?  Does parquet write the version in the metadata?  Should we actually be turning this off based on writer version?&lt;/p&gt;</comment>
                            <comment id="14961281" author="lian cheng" created="Fri, 16 Oct 2015 20:01:02 +0000"  >&lt;p&gt;Yes, it&apos;s the statistics information that is corrupted. And yes, Parquet does write version in the metadata. Parquet-mr 1.8 handles this issue in exactly the way you suggested, namely ignoring binary statistics when necessary according to version information written in the metadata.&lt;/p&gt;

&lt;p&gt;However, Spark SQL performs filter push-down on driver side. This means we need to gather Parquet versions from all Parquet files using a distributed Spark job. We can probably merge this one into the job used to merge Spark schemata. But I think this is too risky for 1.5.2 at this stage. So I&apos;d propose we simply disables filter push-down for strings and binaries in all cases until a parquet-mr upgrade.&lt;/p&gt;</comment>
                            <comment id="14961300" author="apachespark" created="Fri, 16 Oct 2015 20:13:02 +0000"  >&lt;p&gt;User &apos;liancheng&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/9152&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/9152&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14961371" author="felixcheung" created="Fri, 16 Oct 2015 21:16:59 +0000"  >&lt;p&gt;so the corrupted stats data would still be a problem when for future releases? how would it be handled then?&lt;/p&gt;</comment>
                            <comment id="14961374" author="felixcheung" created="Fri, 16 Oct 2015 21:18:47 +0000"  >&lt;p&gt;re-read what you said, I think it makes sense. I assume it means for Spark 1.6.x it would handle it like Parquet-mr 1.8 in that it would check the writer version and enable/disable push-down for sting/binary columns.&lt;/p&gt;</comment>
                            <comment id="14966053" author="lian cheng" created="Wed, 21 Oct 2015 01:03:29 +0000"  >&lt;p&gt;Issue resolved by pull request 9152&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/9152&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/9152&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14966306" author="romi-totango" created="Wed, 21 Oct 2015 06:00:46 +0000"  >&lt;p&gt;Does this mean that all Spark 1.5.1 are recommended to set spark.sql.parquet.filterPushdown to false?&lt;/p&gt;</comment>
                            <comment id="15004944" author="markhamstra" created="Sat, 14 Nov 2015 00:03:33 +0000"  >&lt;p&gt;Is there a reason why parquet.version hasn&apos;t been pushed up in Spark 1.6 and this issue actually fixed instead of just disabling filter push-down for strings and binaries?&lt;/p&gt;</comment>
                            <comment id="15005004" author="lian cheng" created="Sat, 14 Nov 2015 01:05:37 +0000"  >&lt;p&gt;Yes.&lt;/p&gt;</comment>
                            <comment id="15005023" author="lian cheng" created="Sat, 14 Nov 2015 01:23:37 +0000"  >&lt;p&gt;Good question. We tried, see &lt;a href=&quot;https://github.com/apache/spark/pull/9225&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;PR #9225&lt;/a&gt;, but at last decided not to do so.  There are several reasons:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;Parquet-mr 1.8.1 is not in a very good status, while we don&apos;t have much time left to test it before 1.6 release. The two major issues we found are:
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;We observed performance regression for full Parquet table scanning, but the reason is still unknown yet.&lt;/li&gt;
		&lt;li&gt;Parquet-mr 1.8.1 introduced &lt;a href=&quot;https://issues.apache.org/jira/browse/PARQUET-363&quot; title=&quot;Cannot construct empty MessageType for ReadContext.requestedSchema&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PARQUET-363&quot;&gt;&lt;del&gt;PARQUET-363&lt;/del&gt;&lt;/a&gt;, which brings performance regression for queries like &lt;tt&gt;SELECT COUNT(1) FROM t&lt;/tt&gt;. (This issue can be hacked and worked around though.)&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;Parquet-mr 1.8.1 hasn&apos;t been widely deployed yet (e.g. Hive 1.2.1 is still using 1.6.0), which means that most Parquet files out there all suffer the corrupted statistics issue. Thus using parquet-mr 1.7.0 in Spark 1.6 while disabling filter push-down for string/binary columns doesn&apos;t bring too much negative impact.&lt;/li&gt;
&lt;/ol&gt;
</comment>
                            <comment id="15005542" author="markhamstra" created="Sat, 14 Nov 2015 19:05:11 +0000"  >&lt;p&gt;Thanks.&lt;/p&gt;</comment>
                            <comment id="15008074" author="ianlcsd" created="Tue, 17 Nov 2015 05:18:16 +0000"  >&lt;p&gt;Hi, Cheng:&lt;br/&gt;
  How about Filter pushdown for Timestamp type?&lt;br/&gt;
It appeared that the stats of Timestamp type is written as BinaryStats. is it reason that Timestamp filter is not pushed down?&lt;/p&gt;
</comment>
                            <comment id="15008827" author="lian cheng" created="Tue, 17 Nov 2015 15:25:44 +0000"  >&lt;p&gt;You are right that Impala and Hive write timestamps as Parquet INT96, which is a deprecated variation of Parquet BINARY. Fortunately, Spark SQL doesn&apos;t even try to push-down predicates that involve timestamps. So in the scope of Spark SQL, it&apos;s safe.&lt;/p&gt;</comment>
                            <comment id="15008897" author="ianlcsd" created="Tue, 17 Nov 2015 16:06:44 +0000"  >&lt;p&gt;Cheng:&lt;br/&gt;
  Any plan to enable pushdown for timestamps?&lt;br/&gt;
Should I open a ticket requesting it to be enabled after parquet-mr 1.8 along with other Binary families?&lt;/p&gt;</comment>
                            <comment id="15009057" author="lian cheng" created="Tue, 17 Nov 2015 17:19:02 +0000"  >&lt;p&gt;Yes please file a ticket.&lt;/p&gt;

&lt;p&gt;One complication here is that, timestamp types can be mapped to different Parquet types. Impala, Hive, and old versions of Spark uses INT96, which is essentially BINARY; while Spark 1.5 maps it to INT64. Note that there are also multiple time related Parquet logical types defined in parquet-format spec, but we are not using them yet, because parquet-mr hasn&apos;t fully implemented them.&lt;/p&gt;</comment>
                            <comment id="15009279" author="ianlcsd" created="Tue, 17 Nov 2015 19:07:40 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-11784&quot; title=&quot;Support Timestamp filter pushdown in Parquet datasource &quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-11784&quot;&gt;&lt;del&gt;SPARK-11784&lt;/del&gt;&lt;/a&gt; is now tracking the need of timestamp pushdown.&lt;/p&gt;</comment>
                            <comment id="15311110" author="ianlcsd" created="Wed, 1 Jun 2016 21:02:37 +0000"  >&lt;p&gt;Cheng, &lt;br/&gt;
  Can we revisit &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-11784&quot; title=&quot;Support Timestamp filter pushdown in Parquet datasource &quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-11784&quot;&gt;&lt;del&gt;SPARK-11784&lt;/del&gt;&lt;/a&gt;? &lt;br/&gt;
CC &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markhamstra&quot; class=&quot;user-hover&quot; rel=&quot;markhamstra&quot;&gt;markhamstra&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15311231" author="lian cheng" created="Wed, 1 Jun 2016 22:08:43 +0000"  >&lt;p&gt;Unfortunately we just decided to revert Parquet 1.8.1. See: &lt;a href=&quot;https://github.com/apache/spark/pull/13450&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/13450&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15311252" author="markhamstra" created="Wed, 1 Jun 2016 22:27:12 +0000"  >&lt;p&gt;If I am not mistaken, Parquet 1.8.1 and filter push-down for string and binary remain in the master branch; the reversion only affects branch 2.0.&lt;/p&gt;

&lt;p&gt;There is an additional bug with filter push-down for timestamps that Ian has tracked down, and that should now be addressable in master. &lt;/p&gt;</comment>
                            <comment id="15311363" author="lian cheng" created="Wed, 1 Jun 2016 23:35:08 +0000"  >&lt;p&gt;Yea, right. Can we do it later on master to minimize merge conflicts before 2.0 release?&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12913818">SPARK-11784</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12820297">SPARK-6859</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12855532">SPARK-9876</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 24 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2n4rr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12333643">1.5.2</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>