<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:49:52 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-4105] FAILED_TO_UNCOMPRESS(5) errors when fetching shuffle data with sort-based shuffle</title>
                <link>https://issues.apache.org/jira/browse/SPARK-4105</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;We have seen non-deterministic &lt;tt&gt;FAILED_TO_UNCOMPRESS(5)&lt;/tt&gt; errors during shuffle read.  Here&apos;s a sample stacktrace from an executor:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;14/10/23 18:34:11 ERROR Executor: Exception in task 1747.3 in stage 11.0 (TID 33053)
java.io.IOException: FAILED_TO_UNCOMPRESS(5)
	at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:78)
	at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)
	at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:391)
	at org.xerial.snappy.Snappy.uncompress(Snappy.java:427)
	at org.xerial.snappy.SnappyInputStream.readFully(SnappyInputStream.java:127)
	at org.xerial.snappy.SnappyInputStream.readHeader(SnappyInputStream.java:88)
	at org.xerial.snappy.SnappyInputStream.&amp;lt;init&amp;gt;(SnappyInputStream.java:58)
	at org.apache.spark.io.SnappyCompressionCodec.compressedInputStream(CompressionCodec.scala:128)
	at org.apache.spark.storage.BlockManager.wrapForCompression(BlockManager.scala:1090)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anon$1$$anonfun$onBlockFetchSuccess$1.apply(ShuffleBlockFetcherIterator.scala:116)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anon$1$$anonfun$onBlockFetchSuccess$1.apply(ShuffleBlockFetcherIterator.scala:115)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:243)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:52)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:129)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:158)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.ResizableArray$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:158)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:181)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here&apos;s another occurrence of a similar error:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;java.io.IOException: failed to read chunk
        org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:348)
        org.xerial.snappy.SnappyInputStream.rawRead(SnappyInputStream.java:159)
        org.xerial.snappy.SnappyInputStream.read(SnappyInputStream.java:142)
        java.io.ObjectInputStream$PeekInputStream.read(ObjectInputStream.java:2310)
        java.io.ObjectInputStream$BlockDataInputStream.read(ObjectInputStream.java:2712)
        java.io.ObjectInputStream$BlockDataInputStream.readFully(ObjectInputStream.java:2742)
        java.io.ObjectInputStream.readArray(ObjectInputStream.java:1687)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1344)
        java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
        org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
        org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
        org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
        scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
        org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
        org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:129)
        org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:58)
        org.apache.spark.shuffle.hash.HashShuffleReader.read(HashShuffleReader.scala:46)
        org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:92)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        org.apache.spark.scheduler.Task.run(Task.scala:56)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:182)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The first stacktrace was reported by a Spark user.  The second stacktrace occurred when running&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.util.Random


val numKeyValPairs=1000
val numberOfMappers=200
val keySize=10000

&lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (i &amp;lt;- 0 to 19) {
val pairs1 = sc.parallelize(0 to numberOfMappers, numberOfMappers).flatMap(p=&amp;gt;{
  val randGen = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Random
  val arr1 = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Array[(Int, Array[&lt;span class=&quot;code-object&quot;&gt;Byte&lt;/span&gt;])](numKeyValPairs)
  &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (i &amp;lt;- 0 until numKeyValPairs){
    val byteArr = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Array[&lt;span class=&quot;code-object&quot;&gt;Byte&lt;/span&gt;](keySize)
    randGen.nextBytes(byteArr)
    arr1(i) = (randGen.nextInt(Int.MaxValue),byteArr)
  }
  arr1
})
  pairs1.groupByKey(numberOfMappers).count
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This job frequently runs without any problems, but when it fails it seem that every post-shuffle task fails with either PARSING_ERROR(2), FAILED_TO_UNCOMPRESS(5), or some other decompression error.  I&apos;ve seen reports of similar problems when using LZF compression, so I think that this is caused by some sort of general stream corruption issue. &lt;/p&gt;

&lt;p&gt;This issue has been observed even when no spilling occurs, so I don&apos;t believe that this is due to a bug in spilling code.&lt;/p&gt;

&lt;p&gt;I was unable to reproduce this when running this code in a fresh Spark EC2 cluster and we&apos;ve been having a hard time finding a deterministic reproduction.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12750940">SPARK-4105</key>
            <summary>FAILED_TO_UNCOMPRESS(5) errors when fetching shuffle data with sort-based shuffle</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="davies">Davies Liu</assignee>
                                    <reporter username="joshrosen">Josh Rosen</reporter>
                        <labels>
                    </labels>
                <created>Tue, 28 Oct 2014 00:19:49 +0000</created>
                <updated>Tue, 14 Dec 2021 02:39:10 +0000</updated>
                            <resolved>Fri, 9 Dec 2016 23:45:16 +0000</resolved>
                                    <version>1.2.0</version>
                    <version>1.2.1</version>
                    <version>1.3.0</version>
                    <version>1.4.1</version>
                    <version>1.5.1</version>
                    <version>1.6.1</version>
                    <version>2.0.0</version>
                                    <fixVersion>2.2.0</fixVersion>
                                    <component>Shuffle</component>
                    <component>Spark Core</component>
                        <due></due>
                            <votes>42</votes>
                                    <watches>81</watches>
                                                                                                                <comments>
                            <comment id="14188916" author="joshrosen" created="Wed, 29 Oct 2014 20:11:49 +0000"  >&lt;p&gt;It seems plausible that &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-4107&quot; title=&quot;Incorrect handling of Channel.read()&amp;#39;s return value may lead to data truncation&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-4107&quot;&gt;&lt;del&gt;SPARK-4107&lt;/del&gt;&lt;/a&gt; could have caused this issue, but I&apos;m waiting for confirmation that it fixes this issue.&lt;/p&gt;</comment>
                            <comment id="14249381" author="silvermast" created="Wed, 17 Dec 2014 03:09:08 +0000"  >&lt;p&gt;Similarly, I hit this in 1.1.1:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Job aborted due to stage failure: Task 0 in stage 3919.0 failed 4 times, most recent failure: Lost task 0.3 in stage 3919.0 (TID 2467, ...): java.io.IOException: failed to uncompress the chunk: FAILED_TO_UNCOMPRESS(5)
        org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:362)
        org.xerial.snappy.SnappyInputStream.read(SnappyInputStream.java:384)
        java.io.ObjectInputStream$PeekInputStream.peek(ObjectInputStream.java:2293)
        java.io.ObjectInputStream$BlockDataInputStream.peek(ObjectInputStream.java:2586)
        java.io.ObjectInputStream$BlockDataInputStream.peekByte(ObjectInputStream.java:2596)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1318)
        java.io.ObjectInputStream.readArray(ObjectInputStream.java:1706)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1344)
        java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
        org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
        org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
        org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
        org.apache.spark.storage.BlockManager$LazyProxyIterator$1.hasNext(BlockManager.scala:1171)
        scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
        org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
        org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:144)
        org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:58)
        org.apache.spark.shuffle.hash.HashShuffleReader.read(HashShuffleReader.scala:48)
        org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:92)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="14252017" author="nishanthps" created="Thu, 18 Dec 2014 18:43:14 +0000"  >&lt;p&gt;I hit this error when using ALS in Mllib. I am not using the KryoSerializer but I am using the default serializer. I faced this error when running Spark 1.1.1.&lt;/p&gt;

&lt;p&gt;        java.io.IOException: failed to uncompress the chunk: FAILED_TO_UNCOMPRESS(5)&lt;br/&gt;
        org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:362)&lt;br/&gt;
        org.xerial.snappy.SnappyInputStream.rawRead(SnappyInputStream.java:159)&lt;br/&gt;
        org.xerial.snappy.SnappyInputStream.read(SnappyInputStream.java:142)&lt;br/&gt;
        java.io.ObjectInputStream$PeekInputStream.read(ObjectInputStream.java:2283)&lt;br/&gt;
        java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2296)&lt;br/&gt;
        java.io.ObjectInputStream$BlockDataInputStream.readDoubles(ObjectInputStream.java:2986)&lt;br/&gt;
        java.io.ObjectInputStream.readArray(ObjectInputStream.java:1672)&lt;br/&gt;
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1341)&lt;br/&gt;
        java.io.ObjectInputStream.readArray(ObjectInputStream.java:1685)&lt;br/&gt;
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1341)&lt;br/&gt;
        java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1964)&lt;br/&gt;
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1888)&lt;br/&gt;
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)&lt;br/&gt;
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1347)&lt;br/&gt;
        java.io.ObjectInputStream.readObject(ObjectInputStream.java:369)&lt;br/&gt;
        org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)&lt;br/&gt;
        org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)&lt;br/&gt;
        org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)&lt;br/&gt;
        org.apache.spark.storage.BlockManager$LazyProxyIterator$1.hasNext(BlockManager.scala:1171)&lt;br/&gt;
        scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)&lt;br/&gt;
        org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)&lt;br/&gt;
        org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)&lt;br/&gt;
        scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)&lt;br/&gt;
        scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)&lt;br/&gt;
        org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:144)&lt;br/&gt;
        org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)&lt;br/&gt;
        org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)&lt;br/&gt;
        scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)&lt;br/&gt;
        scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)&lt;br/&gt;
        scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)&lt;br/&gt;
        scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)&lt;br/&gt;
        org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)&lt;br/&gt;
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)&lt;br/&gt;
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)&lt;br/&gt;
        org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)&lt;br/&gt;
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)&lt;br/&gt;
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)&lt;br/&gt;
        org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)&lt;br/&gt;
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)&lt;br/&gt;
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)&lt;br/&gt;
        org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)&lt;br/&gt;
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)&lt;br/&gt;
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)&lt;br/&gt;
        org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)&lt;br/&gt;
        org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)&lt;br/&gt;
        org.apache.spark.scheduler.Task.run(Task.scala:54)&lt;br/&gt;
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)&lt;br/&gt;
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)&lt;br/&gt;
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)&lt;br/&gt;
        java.lang.Thread.run(Thread.java:722)&lt;/p&gt;</comment>
                            <comment id="14286430" author="pwendell" created="Wed, 21 Jan 2015 22:28:59 +0000"  >&lt;p&gt;At this point I&apos;m not aware of people still hitting this set of issues in newer releases, so per discussion with &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=joshrosen&quot; class=&quot;user-hover&quot; rel=&quot;joshrosen&quot;&gt;joshrosen&lt;/a&gt;, I&apos;d like to close this. Please comment on this JIRA if you are having some variant of this issue in a newer version of Spark, and we&apos;ll continue to investigate.&lt;/p&gt;</comment>
                            <comment id="14290961" author="ding" created="Sun, 25 Jan 2015 02:46:32 +0000"  >&lt;p&gt;I hit this error when using pagerank(It cannot be consistent repro as I only hit once). I am not using the KryoSerializer but I am using the default serializer. The Spark code is get from chunk at 2015/1/19 which should be later than spark 1.2.0. &lt;/p&gt;

&lt;p&gt;15/01/23 23:32:57 WARN scheduler.TaskSetManager: Lost task 347.0 in stage 9461.0 (TID 302687, sr213): FetchFailed(BlockManagerId(13, sr207, 49805), shuffleId=399, mapId=461, reduceId=347, message=&lt;br/&gt;
org.apache.spark.shuffle.FetchFailedException: FAILED_TO_UNCOMPRESS(5)&lt;br/&gt;
    at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$.org$apache$spark$shuffle$hash$BlockStoreShuffleFetcher$$unpackBlock$1(BlockStoreShuffleFetcher.scala:67)&lt;br/&gt;
    at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$$anonfun$3.apply(BlockStoreShuffleFetcher.scala:83)&lt;br/&gt;
    at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$$anonfun$3.apply(BlockStoreShuffleFetcher.scala:83)&lt;br/&gt;
    at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)&lt;br/&gt;
    at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)&lt;br/&gt;
    at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)&lt;br/&gt;
    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)&lt;br/&gt;
    at scala.collection.Iterator$class.foreach(Iterator.scala:727)&lt;br/&gt;
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)&lt;br/&gt;
    at org.apache.spark.graphx.impl.VertexPartitionBaseOps.aggregateUsingIndex(VertexPartitionBaseOps.scala:207)&lt;br/&gt;
    at org.apache.spark.graphx.impl.VertexRDDImpl$$anonfun$5$$anonfun$apply$4.apply(VertexRDDImpl.scala:171)&lt;br/&gt;
    at org.apache.spark.graphx.impl.VertexRDDImpl$$anonfun$5$$anonfun$apply$4.apply(VertexRDDImpl.scala:171)&lt;br/&gt;
    at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)&lt;br/&gt;
    at org.apache.spark.graphx.impl.VertexRDDImpl$$anonfun$3.apply(VertexRDDImpl.scala:113)&lt;br/&gt;
    at org.apache.spark.graphx.impl.VertexRDDImpl$$anonfun$3.apply(VertexRDDImpl.scala:111)&lt;br/&gt;
    at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)&lt;br/&gt;
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:264)&lt;br/&gt;
    at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:65)&lt;br/&gt;
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)&lt;br/&gt;
    at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)&lt;br/&gt;
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:264)&lt;br/&gt;
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:231)&lt;br/&gt;
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:264)&lt;br/&gt;
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:231)&lt;br/&gt;
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)&lt;br/&gt;
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)&lt;br/&gt;
    at org.apache.spark.scheduler.Task.run(Task.scala:64)&lt;br/&gt;
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:192)&lt;br/&gt;
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
    at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
Caused by: java.io.IOException: FAILED_TO_UNCOMPRESS(5)&lt;br/&gt;
    at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:84)&lt;br/&gt;
    at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)&lt;br/&gt;
    at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:444)&lt;br/&gt;
    at org.xerial.snappy.Snappy.uncompress(Snappy.java:480)&lt;br/&gt;
    at org.xerial.snappy.SnappyInputStream.readFully(SnappyInputStream.java:135)&lt;br/&gt;
    at org.xerial.snappy.SnappyInputStream.readHeader(SnappyInputStream.java:92)&lt;br/&gt;
    at org.xerial.snappy.SnappyInputStream.&amp;lt;init&amp;gt;(SnappyInputStream.java:58)&lt;br/&gt;
    at org.apache.spark.io.SnappyCompressionCodec.compressedInputStream(CompressionCodec.scala:143)&lt;br/&gt;
    at org.apache.spark.storage.BlockManager.wrapForCompression(BlockManager.scala:1165)&lt;br/&gt;
    at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:300)&lt;br/&gt;
    at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:299)&lt;br/&gt;
    at scala.util.Success$$anonfun$map$1.apply(Try.scala:206)&lt;br/&gt;
    at scala.util.Try$.apply(Try.scala:161)&lt;br/&gt;
    at scala.util.Success.map(Try.scala:206)&lt;br/&gt;
    at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:299)&lt;br/&gt;
    at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:53)&lt;/p&gt;
</comment>
                            <comment id="14307699" author="joshrosen" created="Thu, 5 Feb 2015 18:25:09 +0000"  >&lt;p&gt;I&apos;m re-opening this issue because I&apos;ve seen another recent occurrence of this error in Spark 1.2.0+.&lt;/p&gt;

&lt;p&gt;From what I&apos;ve seen, occurrences of this error seem to cluster together: things will be working okay for a little while, then a ton of instances of this error will occur.  Around the Spark 1.2.0 release period, I spent a bunch of time investigating potential causes of this bug but wasn&apos;t able to track it down.  I&apos;d appreciate any help in auditing the Spark code to see if we can figure out what&apos;s causing this.  One strategy might be to purposely introduce bugs or corruption into different parts of the shuffle write and read path to see how that corruption manifests itself as errors; this might help us to narrow down the range of potential causes.&lt;/p&gt;</comment>
                            <comment id="14313652" author="mkman84" created="Tue, 10 Feb 2015 06:04:58 +0000"  >&lt;p&gt;We&apos;re running 1.2.1-rc2 on our cluster and running into the exact same problem. Several different jobs by different users will typically run perfectly fine, and then another identical run randomly throws the FAILED_TO_UNCOMPRESS(5) error, which causes the job to fail altogether actually. I&apos;ll try to re-produce this somehow, though it is a tricky one!&lt;/p&gt;</comment>
                            <comment id="14334059" author="yashwanth.rao11@gmail.com" created="Mon, 23 Feb 2015 23:13:33 +0000"  >&lt;p&gt;I have a similar issue. I have a job which occasionally fails with this error. I am using Spark 1.2.0 with HDP 2.1.1. &lt;/p&gt;</comment>
                            <comment id="14334107" author="ding" created="Mon, 23 Feb 2015 23:53:41 +0000"  >&lt;p&gt;I am taking annual leave from 2/14 - 2/26. Sorry to bring the inconvience.&lt;/p&gt;

&lt;p&gt;Best regards&lt;br/&gt;
DingDing&lt;/p&gt;</comment>
                            <comment id="14370598" author="airhorns" created="Fri, 20 Mar 2015 02:35:35 +0000"  >&lt;p&gt;Would just like to add that we are seeing this occasionally as well on 1.3 and master from github. It seems mostly to manifest when there is high IO contention. We were at one point using spinning disks for &lt;tt&gt;spark.local.dir&lt;/tt&gt; and that seems to exacerbate the issue. Anybody have any leads? &lt;/p&gt;</comment>
                            <comment id="14370613" author="mkman84" created="Fri, 20 Mar 2015 02:54:59 +0000"  >&lt;p&gt;Can also confirm it&apos;s happened once so far since moving to 1.3-rc3. We&apos;re using SSDs for scratch space in our case however. I did notice that it&apos;s only happened (from 1.2 as well) on rare occasions, only when multiple frameworks were running heavy tasks at the same time (which ones exactly I&apos;m not positive about though).&lt;/p&gt;</comment>
                            <comment id="14513761" author="geynard" created="Mon, 27 Apr 2015 09:05:15 +0000"  >&lt;p&gt;I have this issue using spark1.2.0+ shipped with Cloudera CDH5.3. Some more information:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;This issue showed when I tried to augment the property &quot;spark.default.parallelism&quot; in order to have 2 or 3 tasks per executor (in my case 36 tasks for 18 executors).&lt;/li&gt;
	&lt;li&gt;I reproduce it almost every time on my configuration, with Snappy or any other compression mechanism.&lt;/li&gt;
	&lt;li&gt;My cluster is composed of 4 VMs with 6 cores, 32GB RAM on each.&lt;/li&gt;
	&lt;li&gt;My use case is performing joins on about 50GB of data compressed with Snappy.&lt;/li&gt;
	&lt;li&gt;I use KryoSerialization.&lt;/li&gt;
	&lt;li&gt;It generally occurs on just once shuffle partition, but once it has, this partition will throw the error 4 times and fail the job.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14525903" author="joshrosen" created="Sun, 3 May 2015 17:01:28 +0000"  >&lt;p&gt;While working on some new shuffle code, I managed to trigger a &lt;tt&gt;FAILED_TO_UNCOMPRESS(5)&lt;/tt&gt; error by trying to decompress data which was not compressed or which was compressed with the wrong compression codec. This is kind of a long shot, but I wonder if there&apos;s a rarely-hit branch in the sort-shuffle write path that doesn&apos;t properly wrap an output stream for compression (or that does so with the wrong compression codec).&lt;/p&gt;</comment>
                            <comment id="14532692" author="geynard" created="Thu, 7 May 2015 14:08:28 +0000"  >&lt;p&gt;I have been able to reproduce this bug several times on my cluster composed of 4 VMs (6 proc, 32GB ram) using the classes I join. These classes are based on my business use case on which I first ran into the issue and the example given on this page, it is basically joining two flows with massive spilling on disk.&lt;br/&gt;
I reproduce the bug starting from 8000 mappers generating each 100000 objects (first two arguments). This is equivalent to about 36GB of spilled data. With 2000 mappers, the bug did not happen. This issue occurs clearly for me when a massive volume of data is involved. The job takes 1 hour to run on my cluster with these parameters.&lt;/p&gt;

&lt;p&gt;Other parameters: 18 executors, parallelism level = 36.&lt;br/&gt;
Spar version: 1.3.0, cdh5.4&lt;/p&gt;

&lt;p&gt;Stack trace:&lt;br/&gt;
com.esotericsoftware.kryo.KryoException: java.io.IOException: failed to uncompress the chunk: FAILED_TO_UNCOMPRESS(5)&lt;br/&gt;
	at com.esotericsoftware.kryo.io.Input.fill(Input.java:142)&lt;br/&gt;
	at com.esotericsoftware.kryo.io.Input.require(Input.java:169)&lt;br/&gt;
	at com.esotericsoftware.kryo.io.Input.readAscii_slow(Input.java:580)&lt;br/&gt;
	at com.esotericsoftware.kryo.io.Input.readAscii(Input.java:558)&lt;br/&gt;
	at com.esotericsoftware.kryo.io.Input.readString(Input.java:436)&lt;br/&gt;
	at com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:132)&lt;br/&gt;
	at com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:115)&lt;br/&gt;
	at com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:610)&lt;br/&gt;
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:721)&lt;br/&gt;
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)&lt;br/&gt;
	at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)&lt;br/&gt;
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)&lt;br/&gt;
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:138)&lt;br/&gt;
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)&lt;br/&gt;
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)&lt;br/&gt;
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)&lt;br/&gt;
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)&lt;br/&gt;
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)&lt;br/&gt;
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)&lt;br/&gt;
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)&lt;br/&gt;
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)&lt;br/&gt;
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)&lt;br/&gt;
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:160)&lt;br/&gt;
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159)&lt;br/&gt;
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)&lt;br/&gt;
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)&lt;br/&gt;
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)&lt;br/&gt;
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)&lt;br/&gt;
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:159)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)&lt;br/&gt;
	at org.apache.spark.scheduler.Task.run(Task.scala:64)&lt;br/&gt;
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
Caused by: java.io.IOException: failed to uncompress the chunk: FAILED_TO_UNCOMPRESS(5)&lt;br/&gt;
	at org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:361)&lt;br/&gt;
	at org.xerial.snappy.SnappyInputStream.rawRead(SnappyInputStream.java:158)&lt;br/&gt;
	at org.xerial.snappy.SnappyInputStream.read(SnappyInputStream.java:142)&lt;br/&gt;
	at com.esotericsoftware.kryo.io.Input.fill(Input.java:140)&lt;br/&gt;
	... 42 more&lt;/p&gt;</comment>
                            <comment id="14532794" author="bherta" created="Thu, 7 May 2015 14:49:25 +0000"  >&lt;p&gt;I can add a few more pieces of information, since I see this problem consistently in a program on my cluster.&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;I see this this both in Spark 1.3.1, and in a build of the head as of May 6th.&lt;/li&gt;
	&lt;li&gt;I am running on only three executors, and it occurs in all three of them around the same time.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Here&apos;s an example stack taken from the trunk build:&lt;/p&gt;

&lt;p&gt;FetchFailed(BlockManagerId(0, XXX.XXX.XXX.XXX, 40817), shuffleId=69, mapId=3, reduceId=5, message=&lt;br/&gt;
org.apache.spark.shuffle.FetchFailedException: FAILED_TO_UNCOMPRESS(5)&lt;br/&gt;
	at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$.org$apache$spark$shuffle$hash$BlockStoreShuffleFetcher$$unpackBlock$1(BlockStoreShuffleFetcher.scala:67)&lt;br/&gt;
	at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$$anonfun$3.apply(BlockStoreShuffleFetcher.scala:84)&lt;br/&gt;
	at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$$anonfun$3.apply(BlockStoreShuffleFetcher.scala:84)&lt;br/&gt;
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)&lt;br/&gt;
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)&lt;br/&gt;
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)&lt;br/&gt;
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)&lt;br/&gt;
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)&lt;br/&gt;
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:127)&lt;br/&gt;
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:169)&lt;br/&gt;
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:168)&lt;br/&gt;
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)&lt;br/&gt;
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)&lt;br/&gt;
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)&lt;br/&gt;
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)&lt;br/&gt;
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:168)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)&lt;br/&gt;
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)&lt;br/&gt;
	at org.apache.spark.scheduler.Task.run(Task.scala:70)&lt;br/&gt;
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:785)&lt;br/&gt;
Caused by: java.io.IOException: FAILED_TO_UNCOMPRESS(5)&lt;br/&gt;
	at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:84)&lt;br/&gt;
	at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)&lt;br/&gt;
	at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:444)&lt;br/&gt;
	at org.xerial.snappy.Snappy.uncompress(Snappy.java:480)&lt;br/&gt;
	at org.xerial.snappy.SnappyInputStream.readFully(SnappyInputStream.java:135)&lt;br/&gt;
	at org.xerial.snappy.SnappyInputStream.readHeader(SnappyInputStream.java:92)&lt;br/&gt;
	at org.xerial.snappy.SnappyInputStream.&amp;lt;init&amp;gt;(SnappyInputStream.java:58)&lt;br/&gt;
	at org.apache.spark.io.SnappyCompressionCodec.compressedInputStream(CompressionCodec.scala:160)&lt;br/&gt;
	at org.apache.spark.storage.BlockManager.wrapForCompression(BlockManager.scala:1167)&lt;br/&gt;
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:301)&lt;br/&gt;
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:300)&lt;br/&gt;
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:206)&lt;br/&gt;
	at scala.util.Try$.apply(Try.scala:161)&lt;br/&gt;
	at scala.util.Success.map(Try.scala:206)&lt;br/&gt;
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)&lt;br/&gt;
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)&lt;br/&gt;
	... 37 more&lt;/p&gt;

&lt;p&gt;)&lt;/p&gt;</comment>
                            <comment id="14532980" author="pwendell" created="Thu, 7 May 2015 16:46:39 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bherta&quot; class=&quot;user-hover&quot; rel=&quot;bherta&quot;&gt;bherta&lt;/a&gt; can you give a bit more information about your environment? E.g. what Java version, OS versions and whether you are running Spark inside of containers or VM&apos;s? This bug involves data corruption caused by a race condition somewhere and we aren&apos;t sure if it might be in OS or JVM libraries or in Spark.&lt;/p&gt;</comment>
                            <comment id="14533035" author="bherta" created="Thu, 7 May 2015 17:19:54 +0000"  >&lt;p&gt;Yes, sorry, I should have included that information in my earlier post.&lt;/p&gt;

&lt;p&gt;I&apos;m running on an up-to-date install of CentOS 7 x64, not in a VM or other type of container, but installed directly on the servers.  I have two physical machines, with two workers on one machine (40 cores, 256Gb memory, divided in half for the two workers), and a single worker on the other (32 cores, 128Gb memory).   The JVM is IBM&apos;s Java 8, SR1 (&lt;a href=&quot;http://www.ibm.com/developerworks/java/jdk/linux/download.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.ibm.com/developerworks/java/jdk/linux/download.html&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Removing some calls to persist/cache appears to help prevent this from occurring.&lt;/p&gt;</comment>
                            <comment id="14536880" author="douglaz" created="Sat, 9 May 2015 20:34:03 +0000"  >&lt;p&gt;Got something like this but using:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Java serializer&lt;/li&gt;
	&lt;li&gt;Snappy&lt;/li&gt;
	&lt;li&gt;Also Lz4&lt;/li&gt;
	&lt;li&gt;Spark 1.3.0 (most things on default settings)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;We got the error many times on the same cluster (which was doing fine for days) but after recreating it the problem disappeared again. Stack traces (the first two are from two different runs using Snappy and the third from an execution using Lz4):&lt;/p&gt;

&lt;p&gt;15/05/09 13:05:55 ERROR Executor: Exception in task 234.2 in stage 27.1 (TID 876507)&lt;br/&gt;
java.io.IOException: PARSING_ERROR(2)&lt;br/&gt;
		at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:84)&lt;br/&gt;
		at org.xerial.snappy.SnappyNative.uncompressedLength(Native Method)&lt;br/&gt;
		at org.xerial.snappy.Snappy.uncompressedLength(Snappy.java:594)&lt;br/&gt;
		at org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:358)&lt;br/&gt;
		at org.xerial.snappy.SnappyInputStream.read(SnappyInputStream.java:387)&lt;br/&gt;
		at java.io.ObjectInputStream$PeekInputStream.peek(ObjectInputStream.java:2293)&lt;br/&gt;
		at java.io.ObjectInputStream$BlockDataInputStream.peek(ObjectInputStream.java:2586)&lt;br/&gt;
		at java.io.ObjectInputStream$BlockDataInputStream.peekByte(ObjectInputStream.java:2596)&lt;br/&gt;
		at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1318)&lt;br/&gt;
		at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)&lt;br/&gt;
		at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:68)&lt;br/&gt;
		at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)&lt;br/&gt;
		at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)&lt;br/&gt;
		at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)&lt;br/&gt;
		at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)&lt;br/&gt;
		at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)&lt;br/&gt;
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)&lt;br/&gt;
		at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)&lt;br/&gt;
		at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:60)&lt;br/&gt;
		at org.apache.spark.shuffle.hash.HashShuffleReader.read(HashShuffleReader.scala:46)&lt;br/&gt;
		at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:92)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)&lt;br/&gt;
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)&lt;br/&gt;
		at org.apache.spark.scheduler.Task.run(Task.scala:64)&lt;br/&gt;
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)&lt;br/&gt;
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br/&gt;
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
		at java.lang.Thread.run(Thread.java:745)&lt;/p&gt;



&lt;p&gt;15/05/08 20:46:28 WARN scheduler.TaskSetManager: Lost task 9559.0 in stage 55.0 (TID 424644, ip-172-24-36-214.ec2.internal): java.io.IOException: FAILED_TO_UNCOMPRESS(5)&lt;br/&gt;
		at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:84)&lt;br/&gt;
		at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)&lt;br/&gt;
		at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:444)&lt;br/&gt;
		at org.xerial.snappy.Snappy.uncompress(Snappy.java:480)&lt;br/&gt;
		at org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:362)&lt;br/&gt;
		at org.xerial.snappy.SnappyInputStream.read(SnappyInputStream.java:387)&lt;br/&gt;
		at java.io.ObjectInputStream$PeekInputStream.peek(ObjectInputStream.java:2293)&lt;br/&gt;
		at java.io.ObjectInputStream$BlockDataInputStream.peek(ObjectInputStream.java:2586)&lt;br/&gt;
		at java.io.ObjectInputStream$BlockDataInputStream.peekByte(ObjectInputStream.java:2596)&lt;br/&gt;
		at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1318)&lt;br/&gt;
		at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)&lt;br/&gt;
		at scala.collection.immutable.$colon$colon.readObject(List.scala:366)&lt;br/&gt;
		at sun.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)&lt;br/&gt;
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
		at java.lang.reflect.Method.invoke(Method.java:606)&lt;br/&gt;
		at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)&lt;br/&gt;
		at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)&lt;br/&gt;
		at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)&lt;br/&gt;
		at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)&lt;br/&gt;
		at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)&lt;br/&gt;
		at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)&lt;br/&gt;
		at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)&lt;br/&gt;
		at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)&lt;br/&gt;
		at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)&lt;br/&gt;
		at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)&lt;br/&gt;
		at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)&lt;br/&gt;
		at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)&lt;br/&gt;
		at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)&lt;br/&gt;
		at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)&lt;br/&gt;
		at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)&lt;br/&gt;
		at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)&lt;br/&gt;
		at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)&lt;br/&gt;
		at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)&lt;br/&gt;
		at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)&lt;br/&gt;
		at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)&lt;br/&gt;
		at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)&lt;br/&gt;
		at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:68)&lt;br/&gt;
		at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)&lt;br/&gt;
		at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)&lt;br/&gt;
		at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)&lt;br/&gt;
		at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)&lt;br/&gt;
		at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)&lt;br/&gt;
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)&lt;br/&gt;
		at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:125)&lt;br/&gt;
		at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:60)&lt;br/&gt;
		at org.apache.spark.shuffle.hash.HashShuffleReader.read(HashShuffleReader.scala:46)&lt;br/&gt;
		at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:92)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:130)&lt;br/&gt;
		at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:127)&lt;br/&gt;
		at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)&lt;br/&gt;
		at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)&lt;br/&gt;
		at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)&lt;br/&gt;
		at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)&lt;br/&gt;
		at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:127)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:130)&lt;br/&gt;
		at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:127)&lt;br/&gt;
		at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)&lt;br/&gt;
		at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)&lt;br/&gt;
		at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)&lt;br/&gt;
		at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)&lt;br/&gt;
		at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:127)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:130)&lt;br/&gt;
		at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:127)&lt;br/&gt;
		at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)&lt;br/&gt;
		at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)&lt;br/&gt;
		at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)&lt;br/&gt;
		at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)&lt;br/&gt;
		at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:127)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:130)&lt;br/&gt;
		at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:127)&lt;br/&gt;
		at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)&lt;br/&gt;
		at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)&lt;br/&gt;
		at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)&lt;br/&gt;
		at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)&lt;br/&gt;
		at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:127)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)&lt;br/&gt;
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)&lt;br/&gt;
		at org.apache.spark.scheduler.Task.run(Task.scala:64)&lt;br/&gt;
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)&lt;br/&gt;
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br/&gt;
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
		at java.lang.Thread.run(Thread.java:745)&lt;/p&gt;



&lt;p&gt;15/05/09 16:27:01 WARN scheduler.TaskSetManager: Lost task 1365.0 in stage 52.0 (TID 812600, ip-172-24-33-2.ec2.internal): FetchFailed(BlockManagerId(19, ip-172-24-33-64.ec2.internal, 34224), shuffleId=14, mapId=662, reduceId=1365, message=&lt;br/&gt;
org.apache.spark.shuffle.FetchFailedException: Stream is corrupted&lt;br/&gt;
		at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$.org$apache$spark$shuffle$hash$BlockStoreShuffleFetcher$$unpackBlock$1(BlockStoreShuffleFetcher.scala:67)&lt;br/&gt;
		at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$$anonfun$3.apply(BlockStoreShuffleFetcher.scala:83)&lt;br/&gt;
		at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$$anonfun$3.apply(BlockStoreShuffleFetcher.scala:83)&lt;br/&gt;
		at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)&lt;br/&gt;
		at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)&lt;br/&gt;
		at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)&lt;br/&gt;
		at org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:91)&lt;br/&gt;
		at org.apache.spark.shuffle.hash.HashShuffleReader.read(HashShuffleReader.scala:44)&lt;br/&gt;
		at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:92)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:130)&lt;br/&gt;
		at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:127)&lt;br/&gt;
		at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)&lt;br/&gt;
		at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)&lt;br/&gt;
		at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)&lt;br/&gt;
		at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)&lt;br/&gt;
		at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:127)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:130)&lt;br/&gt;
		at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:127)&lt;br/&gt;
		at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)&lt;br/&gt;
		at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)&lt;br/&gt;
		at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)&lt;br/&gt;
		at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)&lt;br/&gt;
		at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:127)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
		at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)&lt;br/&gt;
		at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)&lt;br/&gt;
		at org.apache.spark.scheduler.Task.run(Task.scala:64)&lt;br/&gt;
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)&lt;br/&gt;
		at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br/&gt;
		at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
		at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
Caused by: java.io.IOException: Stream is corrupted&lt;br/&gt;
		at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:152)&lt;br/&gt;
		at net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:116)&lt;br/&gt;
		at java.io.ObjectInputStream$PeekInputStream.read(ObjectInputStream.java:2310)&lt;br/&gt;
		at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2323)&lt;br/&gt;
		at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2794)&lt;br/&gt;
		at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:801)&lt;br/&gt;
		at java.io.ObjectInputStream.&amp;lt;init&amp;gt;(ObjectInputStream.java:299)&lt;br/&gt;
		at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.&amp;lt;init&amp;gt;(JavaSerializer.scala:63)&lt;br/&gt;
		at org.apache.spark.serializer.JavaDeserializationStream.&amp;lt;init&amp;gt;(JavaSerializer.scala:63)&lt;br/&gt;
		at org.apache.spark.serializer.JavaSerializerInstance.deserializeStream(JavaSerializer.scala:102)&lt;br/&gt;
		at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:302)&lt;br/&gt;
		at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:300)&lt;br/&gt;
		at scala.util.Success$$anonfun$map$1.apply(Try.scala:206)&lt;br/&gt;
		at scala.util.Try$.apply(Try.scala:161)&lt;br/&gt;
		at scala.util.Success.map(Try.scala:206)&lt;br/&gt;
		at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)&lt;br/&gt;
		at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:53)&lt;br/&gt;
		... 51 more&lt;/p&gt;


&lt;p&gt;The cluster was 15 d2.4xlarge using customized spark_ec2.py (but on stock image).&lt;/p&gt;

&lt;p&gt;After the problem started it happened in 18 from 20 executions (i.e only 2 executions finished successfully). Curiously it happened after a deploy which updated the application jar (but this wasn&apos;t the cause because rolling back didn&apos;t fix it). My guess is that killing the running job (as consequence of the deploy) left the cluster in a inconsistent state which new executions couldn&apos;t handle.&lt;/p&gt;</comment>
                            <comment id="14544427" author="joshrosen" created="Thu, 14 May 2015 21:53:31 +0000"  >&lt;p&gt;I found a bug in snappy-java that can lead to stream corruption issues and have submitted an upstream patch to fix it.  See &lt;a href=&quot;https://github.com/xerial/snappy-java/issues/107&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/xerial/snappy-java/issues/107&lt;/a&gt; for more details.  Once snappy-java publishes a snapshot / release using this patch, it would be great if someone encountering this issue could try upgrading Snappy to see if this fixes the bug.&lt;/p&gt;</comment>
                            <comment id="14545006" author="joshrosen" created="Fri, 15 May 2015 06:22:50 +0000"  >&lt;p&gt;I&apos;ve opened &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-7660&quot; title=&quot;Snappy-java buffer-sharing bug leads to data corruption / test failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-7660&quot;&gt;&lt;del&gt;SPARK-7660&lt;/del&gt;&lt;/a&gt; to track progress on the fix for the snappy-java buffer sharing bug.&lt;/p&gt;</comment>
                            <comment id="14545533" author="geynard" created="Fri, 15 May 2015 13:57:12 +0000"  >&lt;p&gt;I think I add the bug using another compression codec. I will try to reproduce it as soon as I can.&lt;/p&gt;</comment>
                            <comment id="14545907" author="joshrosen" created="Fri, 15 May 2015 18:11:58 +0000"  >&lt;p&gt;For FAILED_TO_UNCOMPRESS(5), here&apos;s a reproduction that reliably triggers the bug when using Snappy compression:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.io.ByteArrayOutputStream
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.spark.io.SnappyCompressionCodec
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.spark.SparkConf
sc.parallelize(1 to 1000, 100).mapPartitions { p =&amp;gt;
  val snappyCodec = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; SnappyCompressionCodec(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; SparkConf())
  val baos = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ByteArrayOutputStream()
  val snappyStream = snappyCodec.compressedOutputStream(baos)
  (1 to 100).foreach { _ =&amp;gt; snappyStream.close() }
  p
}.coalesce(2, shuffle=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;).count()
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This example is designed to hit the bug that &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-7660&quot; title=&quot;Snappy-java buffer-sharing bug leads to data corruption / test failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-7660&quot;&gt;&lt;del&gt;SPARK-7660&lt;/del&gt;&lt;/a&gt; fixes.&lt;/p&gt;</comment>
                            <comment id="14546089" author="bherta" created="Fri, 15 May 2015 19:52:59 +0000"  >&lt;p&gt;I haven&apos;t had a chance to test the patch for snappy as I am having trouble building it, and the latest snappy-java 1.1.2-SNAPSHOT is a few days old, so does not yet include this patch.  Once a new 1.1.2-SNAPSHOT becomes available I&apos;ll give it a try.&lt;/p&gt;

&lt;p&gt;However, I DID confirm that when I set &quot;spark.io.compression.codec&quot; to &quot;lzf&quot; instead of the default (which is snappy), my program runs correctly, and I do not see the FAILED_TO_UNCOMPRESS(5) errors.  This means that I have a workaround for now, and high hopes when the new snappy build becomes available.&lt;/p&gt;</comment>
                            <comment id="14546090" author="joshrosen" created="Fri, 15 May 2015 19:54:25 +0000"  >&lt;p&gt;Check out my patch at &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-7660&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/SPARK-7660&lt;/a&gt;, which should fix this bug without needing to upgrade Snappy: &lt;a href=&quot;https://github.com/apache/spark/pull/6176&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/6176&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14546091" author="joshrosen" created="Fri, 15 May 2015 19:54:28 +0000"  >&lt;p&gt;Check out my patch at &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-7660&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/SPARK-7660&lt;/a&gt;, which should fix this bug without needing to upgrade Snappy: &lt;a href=&quot;https://github.com/apache/spark/pull/6176&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/6176&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14548360" author="bherta" created="Mon, 18 May 2015 17:32:58 +0000"  >&lt;p&gt;Unfortunately, patch 6176 does not resolve the issue, at least on my system.  I verified that using lzf still works correctly.  Here&apos;s an updated stack trace, using the latest code as of May 15th, with 6176 applied:&lt;/p&gt;

&lt;p&gt;org.apache.spark.shuffle.FetchFailedException: FAILED_TO_UNCOMPRESS(5)&lt;br/&gt;
	at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$.org$apache$spark$shuffle$hash$BlockStoreShuffleFetcher$$unpackBlock$1(BlockStoreShuffleFetcher.scala:67)&lt;br/&gt;
	at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$$anonfun$3.apply(BlockStoreShuffleFetcher.scala:84)&lt;br/&gt;
	at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$$anonfun$3.apply(BlockStoreShuffleFetcher.scala:84)&lt;br/&gt;
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)&lt;br/&gt;
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)&lt;br/&gt;
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)&lt;br/&gt;
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)&lt;br/&gt;
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)&lt;br/&gt;
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:127)&lt;br/&gt;
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:169)&lt;br/&gt;
	at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:168)&lt;br/&gt;
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)&lt;br/&gt;
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)&lt;br/&gt;
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)&lt;br/&gt;
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)&lt;br/&gt;
	at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:168)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)&lt;br/&gt;
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)&lt;br/&gt;
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)&lt;br/&gt;
	at org.apache.spark.scheduler.Task.run(Task.scala:70)&lt;br/&gt;
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:785)&lt;br/&gt;
Caused by: java.io.IOException: FAILED_TO_UNCOMPRESS(5)&lt;br/&gt;
	at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:84)&lt;br/&gt;
	at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)&lt;br/&gt;
	at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:444)&lt;br/&gt;
	at org.xerial.snappy.Snappy.uncompress(Snappy.java:480)&lt;br/&gt;
	at org.xerial.snappy.SnappyInputStream.readFully(SnappyInputStream.java:140)&lt;br/&gt;
	at org.xerial.snappy.SnappyInputStream.readHeader(SnappyInputStream.java:91)&lt;br/&gt;
	at org.xerial.snappy.SnappyInputStream.&amp;lt;init&amp;gt;(SnappyInputStream.java:58)&lt;br/&gt;
	at org.apache.spark.io.SnappyCompressionCodec.compressedInputStream(CompressionCodec.scala:160)&lt;br/&gt;
	at org.apache.spark.storage.BlockManager.wrapForCompression(BlockManager.scala:1168)&lt;br/&gt;
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:301)&lt;br/&gt;
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:300)&lt;br/&gt;
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:206)&lt;br/&gt;
	at scala.util.Try$.apply(Try.scala:161)&lt;br/&gt;
	at scala.util.Success.map(Try.scala:206)&lt;br/&gt;
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)&lt;br/&gt;
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)&lt;br/&gt;
	... 37 more&lt;/p&gt;</comment>
                            <comment id="14548433" author="joshrosen" created="Mon, 18 May 2015 18:10:17 +0000"  >&lt;p&gt;I just noticed something interesting, although perhaps it&apos;s a red herring: in most of the stacktraces posted here, it looks like shuffled data is being processed with CoGroupedRDD. In most (all?) of these cases, it looks like the error is occurring when inserting an iterator of values into an external append-only map inside of CoGroupedRDD.  &lt;a href=&quot;https://github.com/apache/spark/pull/1607&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/1607&lt;/a&gt; was one of the last PRs to touch this file in 1.1, so I wonder whether there could be some sort of odd corner-case that we&apos;re hitting there; might be a lead worth exploring further.&lt;/p&gt;</comment>
                            <comment id="14555807" author="geynard" created="Fri, 22 May 2015 08:03:08 +0000"  >&lt;p&gt;Ok, my bad, I tried to reproduce the bug using lz4, but I was unable to get it, even with a test shuffling 500GB of data. So I must have been mistaken.&lt;br/&gt;
Using another compression codec seems to be a good workaround.&lt;/p&gt;</comment>
                            <comment id="14567389" author="deepujain" created="Mon, 1 Jun 2015 14:58:36 +0000"  >&lt;p&gt;I see this issue when reading sequence file stored in Sequence File format (SEQorg.apache.hadoop.io.Textorg.apache.hadoop.io.Text&apos;org.apache.hadoop.io.compress.GzipCodec?v?&lt;br/&gt;
)&lt;/p&gt;

&lt;p&gt;All i do is &lt;br/&gt;
sc.sequenceFile(dwTable, classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Text&amp;#93;&lt;/span&gt;, classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Text&amp;#93;&lt;/span&gt;).partitionBy(new org.apache.spark.HashPartitioner(2053))&lt;br/&gt;
.set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)&lt;br/&gt;
      .set(&quot;spark.kryoserializer.buffer.mb&quot;, arguments.get(&quot;buffersize&quot;).get)&lt;br/&gt;
      .set(&quot;spark.kryoserializer.buffer.max.mb&quot;, arguments.get(&quot;maxbuffersize&quot;).get)&lt;br/&gt;
      .set(&quot;spark.driver.maxResultSize&quot;, arguments.get(&quot;maxResultSize&quot;).get)&lt;br/&gt;
      .set(&quot;spark.yarn.maxAppAttempts&quot;, &quot;0&quot;)&lt;br/&gt;
      //.set(&quot;spark.akka.askTimeout&quot;, arguments.get(&quot;askTimeout&quot;).get)&lt;br/&gt;
      //.set(&quot;spark.akka.timeout&quot;, arguments.get(&quot;akkaTimeout&quot;).get)&lt;br/&gt;
      //.set(&quot;spark.worker.timeout&quot;, arguments.get(&quot;workerTimeout&quot;).get)&lt;br/&gt;
      .registerKryoClasses(Array(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;com.ebay.ep.poc.spark.reporting.process.model.dw.SpsLevelMetricSum&amp;#93;&lt;/span&gt;))&lt;/p&gt;


&lt;p&gt;and values are &lt;br/&gt;
buffersize=128 maxbuffersize=1068 maxResultSize=200G &lt;/p&gt;</comment>
                            <comment id="14661207" author="djvulee" created="Fri, 7 Aug 2015 02:21:18 +0000"  >&lt;p&gt;Is there any progress on this bug? &lt;/p&gt;

&lt;p&gt;I have the same error on spark1.1.0, I occurred when I use the GroupByKey just as posted above or a sql join. when the data is not very big,  this bug do not occurred, but when the data is bigger, the the FAILED_TO_UNCOMPRESS throwed.&lt;/p&gt;

&lt;p&gt;by the way, I just using the Hash-based shuffle.&lt;/p&gt;</comment>
                            <comment id="14716332" author="mvherweg" created="Thu, 27 Aug 2015 09:18:31 +0000"  >&lt;p&gt;Just hit this error in pyspark code, using Spark 1.4.0&lt;br/&gt;
Same context as others have posted before:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;happens sometimes&lt;/li&gt;
	&lt;li&gt;happens in environments where the exact same code has been executed in the past without issues&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Part of stacktrace:&lt;br/&gt;
org.apache.spark.shuffle.FetchFailedException: FAILED_TO_UNCOMPRESS(5)&lt;br/&gt;
	at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$.org$apache$spark$shuffle$hash$BlockStoreShuffleFetcher$$unpackBlock$1(BlockStoreShuffleFetcher.scala:67)&lt;br/&gt;
	at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$$anonfun$3.apply(BlockStoreShuffleFetcher.scala:84)&lt;br/&gt;
	at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$$anonfun$3.apply(BlockStoreShuffleFetcher.scala:84)&lt;br/&gt;
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)&lt;br/&gt;
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)&lt;br/&gt;
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)&lt;br/&gt;
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)&lt;br/&gt;
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)&lt;br/&gt;
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)&lt;br/&gt;
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)&lt;br/&gt;
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:425)&lt;br/&gt;
	at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:248)&lt;br/&gt;
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)&lt;br/&gt;
	at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:208)&lt;br/&gt;
Caused by: java.io.IOException: FAILED_TO_UNCOMPRESS(5)&lt;br/&gt;
	at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:84)&lt;br/&gt;
	at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)&lt;br/&gt;
	at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:444)&lt;br/&gt;
	at org.xerial.snappy.Snappy.uncompress(Snappy.java:480)&lt;br/&gt;
	at org.xerial.snappy.SnappyInputStream.readFully(SnappyInputStream.java:135)&lt;br/&gt;
	at org.xerial.snappy.SnappyInputStream.readHeader(SnappyInputStream.java:92)&lt;br/&gt;
	at org.xerial.snappy.SnappyInputStream.&amp;lt;init&amp;gt;(SnappyInputStream.java:58)&lt;br/&gt;
	at org.apache.spark.io.SnappyCompressionCodec.compressedInputStream(CompressionCodec.scala:160)&lt;br/&gt;
	at org.apache.spark.storage.BlockManager.wrapForCompression(BlockManager.scala:1176)&lt;br/&gt;
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:301)&lt;br/&gt;
	at org.apache.spark.storage.ShuffleBlockFetcherIterator$$anonfun$4.apply(ShuffleBlockFetcherIterator.scala:300)&lt;br/&gt;
	at scala.util.Success$$anonfun$map$1.apply(Try.scala:206)&lt;br/&gt;
	at scala.util.Try$.apply(Try.scala:161)&lt;br/&gt;
	at scala.util.Success.map(Try.scala:206)&lt;br/&gt;
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)&lt;br/&gt;
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)&lt;br/&gt;
	... 11 more&lt;/p&gt;</comment>
                            <comment id="14717374" author="mmitsuto" created="Thu, 27 Aug 2015 19:36:55 +0000"  >&lt;p&gt;mapPartitions at Exchange.scala:60 +details &lt;br/&gt;
 org.apache.spark.rdd.RDD.mapPartitions(RDD.scala:635)&lt;br/&gt;
org.apache.spark.sql.execution.Exchange$$anonfun$execute$1.apply(Exchange.scala:60)&lt;br/&gt;
org.apache.spark.sql.execution.Exchange$$anonfun$execute$1.apply(Exchange.scala:49)&lt;br/&gt;
org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:46)&lt;br/&gt;
org.apache.spark.sql.execution.Exchange.execute(Exchange.scala:48)&lt;br/&gt;
org.apache.spark.sql.execution.joins.HashOuterJoin.execute(HashOuterJoin.scala:188)&lt;br/&gt;
org.apache.spark.sql.execution.Exchange$$anonfun$execute$1.apply(Exchange.scala:60)&lt;br/&gt;
org.apache.spark.sql.execution.Exchange$$anonfun$execute$1.apply(Exchange.scala:49)&lt;br/&gt;
org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:46)&lt;br/&gt;
org.apache.spark.sql.execution.Exchange.execute(Exchange.scala:48)&lt;br/&gt;
org.apache.spark.sql.execution.joins.HashOuterJoin.execute(HashOuterJoin.scala:188)&lt;br/&gt;
org.apache.spark.sql.execution.Exchange$$anonfun$execute$1.apply(Exchange.scala:60)&lt;br/&gt;
org.apache.spark.sql.execution.Exchange$$anonfun$execute$1.apply(Exchange.scala:49)&lt;br/&gt;
org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:46)&lt;br/&gt;
org.apache.spark.sql.execution.Exchange.execute(Exchange.scala:48)&lt;br/&gt;
org.apache.spark.sql.execution.joins.HashOuterJoin.execute(HashOuterJoin.scala:188)&lt;br/&gt;
org.apache.spark.sql.execution.Project.execute(basicOperators.scala:40)&lt;br/&gt;
org.apache.spark.sql.parquet.InsertIntoParquetTable.execute(ParquetTableOperations.scala:265)&lt;br/&gt;
org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:1099)&lt;br/&gt;
org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:1099)&lt;br/&gt;
 2015/08/27 19:32:12  2 s &lt;br/&gt;
4/2000 (29 failed)&lt;/p&gt;

&lt;p&gt;   61.8 MB 6.6 MB org.apache.spark.shuffle.FetchFailedException: FAILED_TO_UNCOMPRESS(5)+details &lt;/p&gt;</comment>
                            <comment id="14731415" author="irashid" created="Fri, 4 Sep 2015 21:02:36 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mvherweg&quot; class=&quot;user-hover&quot; rel=&quot;mvherweg&quot;&gt;mvherweg&lt;/a&gt; Do you know if the error occurred after there was already a stage retry?  If so, then this might just be a symptom of &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-8029&quot; title=&quot;ShuffleMapTasks must be robust to concurrent attempts on the same executor&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-8029&quot;&gt;&lt;del&gt;SPARK-8029&lt;/del&gt;&lt;/a&gt;.  You would know if earlier in the logs, you see a FetchFailedException which is &lt;b&gt;not&lt;/b&gt; related to snappy exceptions.  I think that is the first report of this bug since &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-7660&quot; title=&quot;Snappy-java buffer-sharing bug leads to data corruption / test failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-7660&quot;&gt;&lt;del&gt;SPARK-7660&lt;/del&gt;&lt;/a&gt;, which we were really hoping fixed this issue, so it would be great to capture more information about it.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mmitsuto&quot; class=&quot;user-hover&quot; rel=&quot;mmitsuto&quot;&gt;mmitsuto&lt;/a&gt; Can you do the same check, and also tell us which version of Spark you are using?&lt;/p&gt;</comment>
                            <comment id="14944607" author="nadenf" created="Tue, 6 Oct 2015 07:13:25 +0000"  >&lt;p&gt;Can confirm this also on Spark 1.4.2 HEAD.&lt;/p&gt;

&lt;p&gt;Had a lot of these org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 41&lt;/p&gt;

&lt;p&gt;But that was not on the same node.&lt;/p&gt;</comment>
                            <comment id="14948678" author="irashid" created="Thu, 8 Oct 2015 13:50:21 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=nadenf&quot; class=&quot;user-hover&quot; rel=&quot;nadenf&quot;&gt;nadenf&lt;/a&gt; The FetchFailures don&apos;t need to be on the same node as the snappy exceptions for the root cause to be &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-8029&quot; title=&quot;ShuffleMapTasks must be robust to concurrent attempts on the same executor&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-8029&quot;&gt;&lt;del&gt;SPARK-8029&lt;/del&gt;&lt;/a&gt;.  Of course, we can&apos;t be sure that is the cause either, but it is at least a working hypothesis for now.&lt;/p&gt;</comment>
                            <comment id="14959273" author="rrag" created="Thu, 15 Oct 2015 17:39:23 +0000"  >&lt;p&gt;I get this error consistently when using spark-shell on 1.5.1 (win 7)&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;scala&amp;gt; sc.textFile(&lt;span class=&quot;code-quote&quot;&gt;&quot;README.md&quot;&lt;/span&gt;, 1).flatMap(x =&amp;gt; x.split(&lt;span class=&quot;code-quote&quot;&gt;&quot; &quot;&lt;/span&gt;)).countByValue()
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Happens for any &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;countByValue/groupByKey/reduceByKey&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt; operations. The affected versions tag on this issue mentions 1.2.0, 1.2.1, 1.3.0, 1.4.1 but not 1.5.1&lt;/p&gt;

&lt;p&gt;Can someone help me if I am doing something wrong or is this a problem in 1.5.1 also&lt;/p&gt;</comment>
                            <comment id="14975447" author="brany" created="Tue, 27 Oct 2015 00:53:57 +0000"  >&lt;p&gt;I Have the same problem in Spark 1.4.0, it gives me these error:&lt;br/&gt;
1. org.apache.spark.shuffle.FetchFailedException: FAILED_TO_UNCOMPRESS(5)&lt;br/&gt;
2. org.apache.spark.shuffle.FetchFailedException: PARSING_ERROR(2)&lt;br/&gt;
3. org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 10&lt;/p&gt;</comment>
                            <comment id="15081244" author="renuyadav" created="Mon, 4 Jan 2016 15:06:50 +0000"  >&lt;p&gt;Hi ,&lt;br/&gt;
I am also getting similar exception on spark 1.4.1 when shuffling above 500GB data,but this exception occurs sometimes&lt;br/&gt;
Please help.&lt;/p&gt;</comment>
                            <comment id="15103876" author="alpinegizmo" created="Sun, 17 Jan 2016 19:20:53 +0000"  >&lt;p&gt;I&apos;ve seen this with 1.6.0. Sorry, but I don&apos;t have a reproducible case.&lt;/p&gt;</comment>
                            <comment id="15103939" author="alpinegizmo" created="Sun, 17 Jan 2016 21:50:38 +0000"  >&lt;p&gt;Increasing the heap size has got me out of trouble, which seems consistent with the theory that this is related to spilling.&lt;/p&gt;</comment>
                            <comment id="15105772" author="daniel.siegmann.aol" created="Mon, 18 Jan 2016 20:12:44 +0000"  >&lt;p&gt;I had this happen in Spark 1.5.0. It only happened once and I haven&apos;t reproduced it (so far).&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Caused by: java.io.IOException: FAILED_TO_UNCOMPRESS(5)
	at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:78)
	at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)
	at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:391)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15188556" author="doingdone9" created="Thu, 10 Mar 2016 02:46:47 +0000"  >&lt;p&gt;I  had this happen in Spark 1.5.2 &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=joshrosen&quot; class=&quot;user-hover&quot; rel=&quot;joshrosen&quot;&gt;joshrosen&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=daniel.siegmann.aol&quot; class=&quot;user-hover&quot; rel=&quot;daniel.siegmann.aol&quot;&gt;daniel.siegmann.aol&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15214712" author="vgirijira" created="Mon, 28 Mar 2016 19:07:13 +0000"  >&lt;p&gt;This is the blocker in Spark 1.5.2 also. I tried using LZ4 or LZF. But I got different exception.&lt;br/&gt;
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 8.1 failed 4 times, most recent failure: Lost task 1.3 in stage 8.1 (TID 2741, p01bdl870.aap.csaa.com): java.io.IOException: Stream is corrupted&lt;/p&gt;</comment>
                            <comment id="15221129" author="vgirijira" created="Fri, 1 Apr 2016 04:18:45 +0000"  >&lt;p&gt;As a temporary work around, I fixed the issue in the code itself to reduce the huge volume that is shuffling in executor nodes. One of the fix is identify bad data and filter it, skewed data and partition it,etc.&lt;/p&gt;</comment>
                            <comment id="15224503" author="wy90021" created="Mon, 4 Apr 2016 16:56:07 +0000"  >&lt;p&gt;This happened in Spark Streaming 1.3.1 by using direct kafka api&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;16/04/04 12:41:01 ERROR scheduler.JobScheduler: Error running job streaming job 1459773240000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 19 in stage 0.0 failed 4 times, most recent failure: Lost task 19.3 in stage 0.0 (TID 66, xvac-d25.xv.dc.openx.org): java.io.IOException: failed to uncompress the chunk: FAILED_TO_UNCOMPRESS(5)
	at org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:361)
	at org.xerial.snappy.SnappyInputStream.rawRead(SnappyInputStream.java:158)
	at org.xerial.snappy.SnappyInputStream.read(SnappyInputStream.java:142)
	at java.io.InputStream.read(InputStream.java:101)
	at kafka.message.ByteBufferMessageSet$$anonfun$decompress$1.apply$mcI$sp(ByteBufferMessageSet.scala:68)
	at kafka.message.ByteBufferMessageSet$$anonfun$decompress$1.apply(ByteBufferMessageSet.scala:68)
	at kafka.message.ByteBufferMessageSet$$anonfun$decompress$1.apply(ByteBufferMessageSet.scala:68)
	at scala.collection.immutable.Stream$.continually(Stream.scala:1129)
	at kafka.message.ByteBufferMessageSet$.decompress(ByteBufferMessageSet.scala:68)
	at kafka.message.ByteBufferMessageSet$$anon$1.makeNextOuter(ByteBufferMessageSet.scala:178)
	at kafka.message.ByteBufferMessageSet$$anon$1.makeNext(ByteBufferMessageSet.scala:191)
	at kafka.message.ByteBufferMessageSet$$anon$1.makeNext(ByteBufferMessageSet.scala:145)
	at kafka.utils.IteratorTemplate.maybeComputeNext(IteratorTemplate.scala:66)
	at kafka.utils.IteratorTemplate.hasNext(IteratorTemplate.scala:58)
	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:847)
	at scala.collection.Iterator$$anon$19.hasNext(Iterator.scala:615)
	at org.apache.spark.streaming.kafka.KafkaRDD$KafkaRDDIterator.getNext(KafkaRDD.scala:161)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:202)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:56)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15251201" author="doingdone9" created="Thu, 21 Apr 2016 03:49:14 +0000"  >&lt;p&gt;I find that some task recompute before this problem happened,and I  think that retry operation Corrupted shuffle file that caused this problem. I debug the code and corrupted the shuffle file before it has been readed, this problem happened every time.maybe we can regenerate the shuffle file when it is corrupted&lt;/p&gt;

&lt;p&gt;code like this &lt;/p&gt;

&lt;p&gt;BlockStoreShuffleReader.scala&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-style: solid;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;border-bottom-style: solid;&quot;&gt;&lt;b&gt;Bar.java&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;     val wrappedStreams = blockFetcherItr.map { &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; (blockId, inputStream) =&amp;gt;
-      serializerManager.wrapForCompression(blockId, inputStream)
+      &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
+        serializerManager.wrapForCompression(blockId, inputStream)  
+      } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; {
+        &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; e: IOException =&amp;gt; {
+          &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; ((e.getMessage.contains(&lt;span class=&quot;code-quote&quot;&gt;&quot;FAILED_TO_UNCOMPRESS(5)&quot;&lt;/span&gt;) ||
+              e.getMessage.contains(&lt;span class=&quot;code-quote&quot;&gt;&quot;PARSING_ERROR(2)&quot;&lt;/span&gt;) ||
+              e.getMessage.contains(&lt;span class=&quot;code-quote&quot;&gt;&quot;Stream is corrupted&quot;&lt;/span&gt;)) &amp;amp;&amp;amp; blockId.isShuffle) {
+            val shuffleBlockId = blockId.asInstanceOf[ShuffleBlockId]
+            &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; FetchFailedException(
+              blockManager.blockManagerId, shuffleBlockId.shuffleId,
+              shuffleBlockId.mapId, shuffleBlockId.reduceId, e)
+          } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
+            &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; e
+          }
+        }
+      }
     }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=joshrosen&quot; class=&quot;user-hover&quot; rel=&quot;joshrosen&quot;&gt;joshrosen&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=wy90021&quot; class=&quot;user-hover&quot; rel=&quot;wy90021&quot;&gt;wy90021&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vgirijira&quot; class=&quot;user-hover&quot; rel=&quot;vgirijira&quot;&gt;vgirijira&lt;/a&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=alpinegizmo&quot; class=&quot;user-hover&quot; rel=&quot;alpinegizmo&quot;&gt;alpinegizmo&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15258002" author="apachespark" created="Tue, 26 Apr 2016 12:37:04 +0000"  >&lt;p&gt;User &apos;DoingDone9&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/12700&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/12700&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15287352" author="jasonr" created="Tue, 17 May 2016 19:25:00 +0000"  >&lt;p&gt;FWIW - I am able to reproduce this error consistently for a job, even when there are no failures in the preceding stage.  In my case, a viable workaround was to increase the number of partitions for the preceding stage such that none of the tasks had to spill to memory or disk.  So it would seem that the corruption may be happening during a spill operation.&lt;/p&gt;</comment>
                            <comment id="15288084" author="doingdone9" created="Wed, 18 May 2016 02:03:32 +0000"  >&lt;p&gt;Can you suggest how I reproduce this error?&lt;/p&gt;</comment>
                            <comment id="15293977" author="vijay.saikam" created="Fri, 20 May 2016 19:00:02 +0000"  >&lt;p&gt;I&apos;ve started to hit this exact same issue. It is not intermittent and doesn&apos;t always happen. We use Spark 1.5 &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Caused by: java.io.IOException: FAILED_TO_UNCOMPRESS(5)
	at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:78)
	at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)
	at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:391)
	at org.xerial.snappy.Snappy.uncompress(Snappy.java:427)
	at org.xerial.snappy.SnappyInputStream.readFully(SnappyInputStream.java:127)
	at org.xerial.snappy.SnappyInputStream.readHeader(SnappyInputStream.java:88)
	at org.xerial.snappy.SnappyInputStream.&amp;lt;init&amp;gt;(SnappyInputStream.java:58)
	at org.apache.spark.io.SnappyCompressionCodec.compressedInputStream(CompressionCodec.scala:159)
	at org.apache.spark.storage.BlockManager.wrapForCompression(BlockManager.scala:1189)
	at org.apache.spark.shuffle.hash.HashShuffleReader$$anonfun$3.apply(HashShuffleReader.scala:53)
	at org.apache.spark.shuffle.hash.HashShuffleReader$$anonfun$3.apply(HashShuffleReader.scala:52)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer.writeRows(WriterContainer.scala:355)
	... 8 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15307150" author="asrraj" created="Tue, 31 May 2016 03:32:50 +0000"  >&lt;p&gt;This issue got fixed in 1.5.1 version but I was looking if it can be ported back to 1.3.1 version,as i am not using 1.5.1 as of now .or any workaround for this ?&lt;/p&gt;</comment>
                            <comment id="15359643" author="mmoroz" created="Fri, 1 Jul 2016 21:11:01 +0000"  >&lt;p&gt;How do you know this issue is fixed? And if it is, should this issue be CLOSED?&lt;/p&gt;</comment>
                            <comment id="15370831" author="ssonker" created="Mon, 11 Jul 2016 14:07:09 +0000"  >&lt;p&gt;I&apos;m using 1.5.1 and was also facing this exception. The issue got resolved when I switched to nio (instead of the default netty). To switch to nio, set &apos;spark.shuffle.blockTransferService&apos; to &apos;nio&apos;. Please see if that helps.&lt;/p&gt;</comment>
                            <comment id="15453034" author="mseal" created="Wed, 31 Aug 2016 18:48:12 +0000"  >&lt;p&gt;Producible on 1.6.1 from a count() call after cache(df.repartition(2000)) against a dataset built from a PySpark generated RDD once I grew it into GB sizes (a few million rows). Smaller dataset sizes have consistently not triggered this issue.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;java.io.IOException: failed to uncompress the chunk: PARSING_ERROR(2)
	at org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:361)
	at org.xerial.snappy.SnappyInputStream.rawRead(SnappyInputStream.java:158)
	at org.xerial.snappy.SnappyInputStream.read(SnappyInputStream.java:142)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.spark-project.guava.io.ByteStreams.read(ByteStreams.java:899)
	at org.spark-project.guava.io.ByteStreams.readFully(ByteStreams.java:733)
	at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$3$$anon$1.next(UnsafeRowSerializer.scala:119)
	at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$3$$anon$1.next(UnsafeRowSerializer.scala:102)
	at scala.collection.Iterator$$anon$13.next(Iterator.scala:372)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:168)
	at org.apache.spark.sql.execution.Sort$$anonfun$1.apply(Sort.scala:90)
	at org.apache.spark.sql.execution.Sort$$anonfun$1.apply(Sort.scala:64)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$21.apply(RDD.scala:728)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$21.apply(RDD.scala:728)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:87)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15535884" author="h4ml3t" created="Fri, 30 Sep 2016 12:41:22 +0000"  >&lt;p&gt;I have exactly the same problem. It occurs when I process more than one TB of data.&lt;/p&gt;

&lt;p&gt;Splitting the computation in halves it works perfectly, but I need to process the whole thing together.&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;java.io.IOException: failed to read chunk
	at org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:347)
	at org.xerial.snappy.SnappyInputStream.rawRead(SnappyInputStream.java:158)
	at org.xerial.snappy.SnappyInputStream.read(SnappyInputStream.java:142)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.spark-project.guava.io.ByteStreams.read(ByteStreams.java:899)
	at org.spark-project.guava.io.ByteStreams.readFully(ByteStreams.java:733)
	at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$3$$anon$1.next(UnsafeRowSerializer.scala:119)
	at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$3$$anon$1.next(UnsafeRowSerializer.scala:102)
	at scala.collection.Iterator$$anon$13.next(Iterator.scala:372)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:512)
	at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.&amp;lt;init&amp;gt;(TungstenAggregationIterator.scala:686)
	at org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:95)
	at org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:86)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;as a consequence, since I am using SparkSQL, the job will fail, reporting&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;16/09/30 14:40:51 ERROR LiveListenerBus: Listener SQLListener threw an exception
java.lang.NullPointerException
        at org.apache.spark.sql.execution.ui.SQLListener.onTaskEnd(SQLListener.scala:167)
        at org.apache.spark.scheduler.SparkListenerBus$class.onPostEvent(SparkListenerBus.scala:42)
        at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)
        at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)
        at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:55)
        at org.apache.spark.util.AsynchronousListenerBus.postToAll(AsynchronousListenerBus.scala:37)
        at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(AsynchronousListenerBus.scala:80)
        at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(AsynchronousListenerBus.scala:65)
        at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(AsynchronousListenerBus.scala:65)
        at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
        at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(AsynchronousListenerBus.scala:64)
        at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1181)
        at org.apache.spark.util.AsynchronousListenerBus$$anon$1.run(AsynchronousListenerBus.scala:63)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;the command is really simple, and I presume the dropDistinct is causing this:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._

import sqlContext.implicits._

sqlContext.read
    .option(&quot;mode&quot;,&quot;DROPMALFORMED&quot;)
    .schema(StructType(StructField(&quot;body&quot;, StringType, true) :: StructField(&quot;timestamp&quot;, StringType, true) :: Nil))
    .json(&quot;myData&quot;)
    .select(&quot;timestamp&quot;,&quot;body&quot;)
    .dropDuplicates(&quot;body&quot;)
    .where($&quot;body&quot;.isNotNull &amp;amp;&amp;amp; $&quot;timestamp&quot;.isNotNull)
    .withColumn(&quot;timestamp&quot;,$&quot;timestamp&quot;.multiply(.001).cast(TimestampType).cast(DateType))
    .groupBy(&quot;timestamp&quot;).count()
    .collect()
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Expected output, something like&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;+----------+---------+
|        ts|    count|
+----------+---------+
|2016-09-01| 76682822|
|2016-09-02|107701926|
|2016-09-03| 32859100|
|2016-09-04| 30218800|
|2016-09-05| 59807028|
|2016-09-06| 52250275|
|2016-09-07| 52187300|
|2016-09-08| 51205733|
|2016-09-09| 64345768|
|2016-09-10| 43249106|
|2016-09-11| 31924421|
|2016-09-12| 78127144|
|2016-09-13|194981002|
|2016-09-14|167246581|
|2016-09-15| 56367223|
|2016-09-16| 55473385|
|2016-09-17|214722399|
|2016-09-18| 65054069|
|2016-09-19| 64932135|
|2016-09-20| 54006489|
|2016-09-21| 54140656|
|2016-09-22| 61518266|
|2016-09-23| 47547635|
|2016-09-24| 24848697|
|2016-09-25| 24111800|
|2016-09-26| 44870596|
|2016-09-27| 25653917|
|2016-09-28| 32558282|
|2016-09-29| 32465916|
|2016-09-30|  1227087|
+----------+---------+
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15535979" author="vadim_ps" created="Fri, 30 Sep 2016 13:20:31 +0000"  >&lt;p&gt;we had gotten the same behavior but the problem was in not enough physical memory for executors. We reduced executor&apos;s memory and the problem was fixed.&lt;/p&gt;</comment>
                            <comment id="15536646" author="mseal" created="Fri, 30 Sep 2016 18:24:23 +0000"  >&lt;p&gt;Backing off executor memory away from boundary of physical memory did not solve the problem for my above crash report.&lt;/p&gt;</comment>
                            <comment id="15548111" author="jean" created="Wed, 5 Oct 2016 08:54:06 +0000"  >&lt;p&gt;We&apos;ve just hit this training RandomForest on spark 1.6.1&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 72 in stage 12980.0 failed 4 times, most recent failure: Lost task 72.3 in stage 12980.0 (TID 757443, *********.com): java.io.IOException: FAILED_TO_UNCOMPRESS(5)
        at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:98)
        at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)
        at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:465)
        at org.xerial.snappy.Snappy.uncompress(Snappy.java:504)
        at org.xerial.snappy.SnappyInputStream.readFully(SnappyInputStream.java:147)
        at org.xerial.snappy.SnappyInputStream.readHeader(SnappyInputStream.java:99)
        at org.xerial.snappy.SnappyInputStream.&amp;lt;init&amp;gt;(SnappyInputStream.java:59)
        at org.apache.spark.io.SnappyCompressionCodec.compressedInputStream(CompressionCodec.scala:159)
        at org.apache.spark.storage.BlockManager.wrapForCompression(BlockManager.scala:1186)
        at org.apache.spark.shuffle.BlockStoreShuffleReader$$anonfun$2.apply(BlockStoreShuffleReader.scala:53)
        at org.apache.spark.shuffle.BlockStoreShuffleReader$$anonfun$2.apply(BlockStoreShuffleReader.scala:52)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
        at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:152)
        at org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:58)
        at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:83)
        at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
        at org.apache.spark.scheduler.Task.run(Task.scala:89)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
        at scala.Option.foreach(Option.scala:236)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)
        at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
        at org.apache.spark.rdd.RDD.collect(RDD.scala:926)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:741)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:740)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
        at org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:740)
        at org.apache.spark.mllib.tree.DecisionTree$.findBestSplits(DecisionTree.scala:651)
        at org.apache.spark.mllib.tree.RandomForest.run(RandomForest.scala:233)
        at org.apache.spark.mllib.tree.RandomForest$.trainRegressor(RandomForest.scala:378)
        at org.apache.spark.mllib.tree.RandomForest.trainRegressor(RandomForest.scala)
        at com.*****.sparkws.core.mlib.RegressorImpl.trainActivityModel(RegressorImpl.java:144)
        at com.*****.sparkws.core.mlib.RegressorImpl$$FastClassBySpringCGLIB$$9f2cc95.invoke(&amp;lt;generated&amp;gt;)
        at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204)
        at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:651)
        ... 17 more

Caused by: java.io.IOException: FAILED_TO_UNCOMPRESS(5)
        at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:98)
        at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)
        at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:465)
        at org.xerial.snappy.Snappy.uncompress(Snappy.java:504)
        at org.xerial.snappy.SnappyInputStream.readFully(SnappyInputStream.java:147)
        at org.xerial.snappy.SnappyInputStream.readHeader(SnappyInputStream.java:99)
        at org.xerial.snappy.SnappyInputStream.&amp;lt;init&amp;gt;(SnappyInputStream.java:59)
        at org.apache.spark.io.SnappyCompressionCodec.compressedInputStream(CompressionCodec.scala:159)
        at org.apache.spark.storage.BlockManager.wrapForCompression(BlockManager.scala:1186)
        at org.apache.spark.shuffle.BlockStoreShuffleReader$$anonfun$2.apply(BlockStoreShuffleReader.scala:53)
        at org.apache.spark.shuffle.BlockStoreShuffleReader$$anonfun$2.apply(BlockStoreShuffleReader.scala:52)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
        at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:152)
        at org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:58)
        at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:83)
        at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
        at org.apache.spark.scheduler.Task.run(Task.scala:89)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
        ... 3 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15549703" author="himanish" created="Wed, 5 Oct 2016 19:28:04 +0000"  >&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I am also seeing this error &lt;tt&gt;java.io.IOException: FAILED_TO_UNCOMPRESS(5)&lt;/tt&gt; on Spark 2.0.0 intermittently with &lt;tt&gt;snappy&lt;/tt&gt;. &lt;/p&gt;

&lt;p&gt;Changing &lt;tt&gt;spark.io.compression.codec&lt;/tt&gt; to &lt;tt&gt;lz4&lt;/tt&gt; causes intermittent &lt;tt&gt;java.io.IOException: Stream is corrupted&lt;/tt&gt; errors. &lt;/p&gt;

&lt;p&gt;I have been setting &lt;tt&gt;spark.rdd.compress&lt;/tt&gt; to &lt;tt&gt;true&lt;/tt&gt; along with setting  &lt;tt&gt;spark.io.compression.codec&lt;/tt&gt;.As a workaround will disabling &lt;tt&gt;spark.rdd.compress&lt;/tt&gt; fix the issue ?&lt;/p&gt;</comment>
                            <comment id="15551998" author="himanish" created="Thu, 6 Oct 2016 13:57:01 +0000"  >
&lt;p&gt;The only way I could make this work for 500GB+ data using the &lt;tt&gt;sort&lt;/tt&gt; based shuffle manager is by disabling shuffle compressions altogether using &lt;tt&gt;--conf spark.shuffle.compress=false --conf spark.shuffle.spill.compress=false&lt;/tt&gt;. This is definitely not desirable.&lt;/p&gt;

&lt;p&gt;Appreciate if someone could suggest any other better workaround. I will keep trying other things ( like switching &lt;tt&gt;spark.shuffle.manager&lt;/tt&gt; to &lt;tt&gt;hash&lt;/tt&gt; , using LZF etc.) and update here.&lt;/p&gt;</comment>
                            <comment id="15554871" author="cryptoe" created="Fri, 7 Oct 2016 11:39:40 +0000"  >&lt;p&gt;I also ran into this error :java.io.IOException: FAILED_TO_UNCOMPRESS(5) while shuffling 3.5 TB using spark version 1.5.2 on yarn version : HDP 2.2.4.2-2.  &lt;br/&gt;
The fix was bumping up the snappy version to 1.1.2.6 from snappy-java-1.0.4.1.&lt;/p&gt;</comment>
                            <comment id="15589825" author="tenstriker" created="Wed, 19 Oct 2016 20:52:21 +0000"  >&lt;p&gt;I hit this error as well but I also noticed lot of  `java.lang.OutOfMemoryError: Java heap space` or and high GC pressure. My guess is having no memory might have caused snappy to fail on uncompress.&lt;/p&gt;</comment>
                            <comment id="15601625" author="dhananjaydp" created="Mon, 24 Oct 2016 10:49:52 +0000"  >&lt;p&gt;I see this error intermittently.&lt;br/&gt;
I am using&lt;br/&gt;
spark             :1.6.2&lt;br/&gt;
hadoop/yarn  :2.7.3&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt; Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 40 in stage 7.0 failed 4 times, most recent failure: Lost task 40.3 in stage 7.0 (TID 411, 10.0.0.12): java.io.IOException: FAILED_TO_UNCOMPRESS(5)&lt;br/&gt;
	at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:98)&lt;br/&gt;
	at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)&lt;br/&gt;
	at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:474)&lt;br/&gt;
	at org.xerial.snappy.Snappy.uncompress(Snappy.java:513)&lt;br/&gt;
	at org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:422)&lt;br/&gt;
	at org.xerial.snappy.SnappyInputStream.available(SnappyInputStream.java:469)&lt;br/&gt;
	at java.io.BufferedInputStream.read(BufferedInputStream.java:353)&lt;br/&gt;
	at java.io.DataInputStream.read(DataInputStream.java:149)&lt;br/&gt;
	at org.spark-project.guava.io.ByteStreams.read(ByteStreams.java:899)&lt;br/&gt;
	at org.spark-project.guava.io.ByteStreams.readFully(ByteStreams.java:733)&lt;br/&gt;
	at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$3$$anon$1.next(UnsafeRowSerializer.scala:119)&lt;br/&gt;
	at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$3$$anon$1.next(UnsafeRowSerializer.scala:102)&lt;br/&gt;
	at scala.collection.Iterator$$anon$13.next(Iterator.scala:372)&lt;br/&gt;
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)&lt;/p&gt;&lt;/blockquote&gt;</comment>
                            <comment id="15675306" author="apachespark" created="Fri, 18 Nov 2016 00:46:04 +0000"  >&lt;p&gt;User &apos;davies&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/15923&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/15923&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16689556" author="irashid" created="Fri, 16 Nov 2018 15:31:30 +0000"  >&lt;p&gt;fyi I opened &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26089&quot; title=&quot;Handle large corrupt shuffle blocks&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-26089&quot;&gt;&lt;del&gt;SPARK-26089&lt;/del&gt;&lt;/a&gt; for handling issues in large corrupt blocks, as we recently ran into that, and I dont&apos; see any other issues for it.&lt;/p&gt;</comment>
                            <comment id="16790258" author="feiwang" created="Tue, 12 Mar 2019 06:03:22 +0000"  >&lt;p&gt;Is there anyone see these errors in latest version, such as spark-2.3?&lt;/p&gt;

&lt;p&gt;Can anyone provide a&#160;reproducible case?&lt;/p&gt;</comment>
                            <comment id="16825901" author="aakash.mandlik" created="Thu, 25 Apr 2019 09:30:57 +0000"  >&lt;p&gt;I am facing&#160;similar&#160;issue while persisting to S3 for Spark-2.4. Earlier the same code was working for spark-2.2.&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;19/04/25 14:42:36 WARN scheduler.TaskSetManager: Lost task 11.0 in stage 55.0 (TID 1345, node4, executor 1): org.xerial.snappy.SnappyIOExceptio&lt;br/&gt;
n: &lt;span class=&quot;error&quot;&gt;&amp;#91;EMPTY_INPUT&amp;#93;&lt;/span&gt; Cannot decompress empty stream&lt;br/&gt;
 at org.xerial.snappy.SnappyInputStream.readHeader(SnappyInputStream.java:94)&lt;br/&gt;
 at org.xerial.snappy.SnappyInputStream.&amp;lt;init&amp;gt;(SnappyInputStream.java:59)&lt;br/&gt;
 at org.apache.spark.io.SnappyCompressionCodec.compressedInputStream(CompressionCodec.scala:164)&lt;br/&gt;
 at org.apache.spark.serializer.SerializerManager.wrapForCompression(SerializerManager.scala:163)&lt;br/&gt;
 at org.apache.spark.serializer.SerializerManager.dataDeserializeStream(SerializerManager.scala:209)&lt;br/&gt;
 at org.apache.spark.storage.BlockManager$$anonfun$getRemoteValues$1.apply(BlockManager.scala:698)&lt;br/&gt;
 at org.apache.spark.storage.BlockManager$$anonfun$getRemoteValues$1.apply(BlockManager.scala:696)&lt;br/&gt;
 at scala.Option.map(Option.scala:146)&lt;br/&gt;
 at org.apache.spark.storage.BlockManager.getRemoteValues(BlockManager.scala:696)&lt;br/&gt;
 at org.apache.spark.storage.BlockManager.get(BlockManager.scala:820)&lt;br/&gt;
 at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:875)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)&lt;br/&gt;
 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)&lt;br/&gt;
 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)&lt;br/&gt;
 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)&lt;br/&gt;
 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)&lt;/p&gt;</comment>
                            <comment id="17283737" author="tcondie" created="Fri, 12 Feb 2021 14:46:20 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=joshrosen&quot; class=&quot;user-hover&quot; rel=&quot;joshrosen&quot;&gt;joshrosen&lt;/a&gt;&#160;We just experienced a similar stack trace / issue last week in Spark 2.4.4. Could you please share your thoughts on whether it could be related? &lt;/p&gt;

&lt;p&gt;&#160;Job aborted due to stage failure: Task 5639 in stage 87235.0 failed 4 times, most recent failure: Lost task 5639.3 in stage 87235.0 (TID 3466497, executor 82): java.io.IOException: FAILED_TO_UNCOMPRESS(5)&lt;br/&gt;
	at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:98)&lt;br/&gt;
	at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)&lt;br/&gt;
	at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:474)&lt;br/&gt;
	at org.xerial.snappy.Snappy.uncompress(Snappy.java:513)&lt;br/&gt;
	at org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:439)&lt;br/&gt;
	at org.xerial.snappy.SnappyInputStream.available(SnappyInputStream.java:486)&lt;br/&gt;
	at org.apache.spark.storage.BufferReleasingInputStream.available(ShuffleBlockFetcherIterator.scala:581)&lt;br/&gt;
	at java.io.BufferedInputStream.read(BufferedInputStream.java:353)&lt;br/&gt;
	at java.io.DataInputStream.read(DataInputStream.java:149)&lt;br/&gt;
	at org.spark_project.guava.io.ByteStreams.read(ByteStreams.java:910)&lt;br/&gt;
	at org.spark_project.guava.io.ByteStreams.readFully(ByteStreams.java:787)&lt;br/&gt;
	at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.next(UnsafeRowSerializer.scala:127)&lt;br/&gt;
	at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.next(UnsafeRowSerializer.scala:110)&lt;br/&gt;
	at scala.collection.Iterator$$anon$12.next(Iterator.scala:445)&lt;br/&gt;
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)&lt;br/&gt;
	at org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)&lt;br/&gt;
	at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)&lt;br/&gt;
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)&lt;br/&gt;
	at org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.next(InMemoryRelation.scala:94)&lt;br/&gt;
	at org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anonfun$1$$anon$1.next(InMemoryRelation.scala:84)&lt;br/&gt;
	at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:222)&lt;br/&gt;
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)&lt;br/&gt;
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)&lt;br/&gt;
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)&lt;br/&gt;
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)&lt;br/&gt;
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)&lt;br/&gt;
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)&lt;br/&gt;
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)&lt;br/&gt;
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)&lt;br/&gt;
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)&lt;br/&gt;
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)&lt;br/&gt;
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)&lt;br/&gt;
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)&lt;br/&gt;
	at org.apache.spark.scheduler.Task.run(Task.scala:123)&lt;br/&gt;
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)&lt;br/&gt;
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)&lt;br/&gt;
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:748)&lt;br/&gt;
&#160;&lt;/p&gt;</comment>
                            <comment id="17458866" author="JIRAUSER281773" created="Tue, 14 Dec 2021 02:37:14 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=joshrosen&quot; class=&quot;user-hover&quot; rel=&quot;joshrosen&quot;&gt;joshrosen&lt;/a&gt; Hello, I still encountered this problem in the company&apos;s project today, but its error level in the log is WARN. The spark version we used was 2.4.0.cloudera2. I&apos;m not sure this issue has anything to do with Cloudera.&lt;br/&gt;
The following is the log information about the error&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
org.apache.spark.shuffle.FetchFailedException: FAILED_TO_UNCOMPRESS(5)
&#160; &#160; &#160; &#160; at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:565)
&#160; &#160; &#160; &#160; at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:470)
&#160; &#160; &#160; &#160; at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:65)
&#160; &#160; &#160; &#160; at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
&#160; &#160; &#160; &#160; at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
&#160; &#160; &#160; &#160; at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
&#160; &#160; &#160; &#160; at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)
&#160; &#160; &#160; &#160; at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
&#160; &#160; &#160; &#160; at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown So
urce)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:216)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.execution.SortExec$$anonfun$1.apply(SortExec.scala:108)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.execution.SortExec$$anonfun$1.apply(SortExec.scala:101)
&#160; &#160; &#160; &#160; at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
&#160; &#160; &#160; &#160; at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)
&#160; &#160; &#160; &#160; at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
&#160; &#160; &#160; &#160; at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
&#160; &#160; &#160; &#160; at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
&#160; &#160; &#160; &#160; at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
&#160; &#160; &#160; &#160; at org.apache.spark.scheduler.Task.run(Task.scala:121)
&#160; &#160; &#160; &#160; at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
&#160; &#160; &#160; &#160; at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)
&#160; &#160; &#160; &#160; at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
&#160; &#160; &#160; &#160; at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
&#160; &#160; &#160; &#160; at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
&#160; &#160; &#160; &#160; at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:748)
Caused by: java.io.IOException: FAILED_TO_UNCOMPRESS(5)
&#160; &#160; &#160; &#160; at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:98)
&#160; &#160; &#160; &#160; at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)
&#160; &#160; &#160; &#160; at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:474)
&#160; &#160; &#160; &#160; at org.xerial.snappy.Snappy.uncompress(Snappy.java:513)
&#160; &#160; &#160; &#160; at org.xerial.snappy.SnappyInputStream.readFully(SnappyInputStream.java:147)
&#160; &#160; &#160; &#160; at org.xerial.snappy.SnappyInputStream.readHeader(SnappyInputStream.java:99)
&#160; &#160; &#160; &#160; at org.xerial.snappy.SnappyInputStream.&amp;lt;init&amp;gt;(SnappyInputStream.java:59)
&#160; &#160; &#160; &#160; at org.apache.spark.io.SnappyCompressionCodec.compressedInputStream(CompressionCodec.scala:164)
&#160; &#160; &#160; &#160; at org.apache.spark.serializer.SerializerManager.wrapForCompression(SerializerManager.scala:163)
&#160; &#160; &#160; &#160; at org.apache.spark.serializer.SerializerManager.wrapStream(SerializerManager.scala:124)
&#160; &#160; &#160; &#160; at org.apache.spark.shuffle.BlockStoreShuffleReader$$anonfun$3.apply(BlockStoreShuffleReader.scala:50)
&#160; &#160; &#160; &#160; at org.apache.spark.shuffle.BlockStoreShuffleReader$$anonfun$3.apply(BlockStoreShuffleReader.scala:50)
&#160; &#160; &#160; &#160; at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:455)
&#160; &#160; &#160; &#160; ... 27 more)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="12922951">SPARK-12418</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12750969">SPARK-4107</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12830079">SPARK-7660</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13229997">SPARK-27562</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13198952">SPARK-26089</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12743065">SPARK-3630</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12731180" name="JavaObjectToSerialize.java" size="292" author="geynard" created="Thu, 7 May 2015 14:08:28 +0000"/>
                            <attachment id="12731179" name="SparkFailedToUncompressGenerator.scala" size="1529" author="geynard" created="Thu, 7 May 2015 14:08:28 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            3 years, 48 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i21n7b:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>