<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:12:49 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-1407] EventLogging to HDFS doesn&apos;t work properly on yarn</title>
                <link>https://issues.apache.org/jira/browse/SPARK-1407</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;When running on spark on yarn and accessing an HDFS file (like in the SparkHdfsLR example) while using the event logging configured to write logs to HDFS, it throws an exception at the end of the application. &lt;/p&gt;

&lt;p&gt;SPARK_JAVA_OPTS=-Dspark.eventLog.enabled=true -Dspark.eventLog.dir=hdfs:///history/spark/&lt;/p&gt;

&lt;p&gt;14/04/03 13:41:31 INFO yarn.ApplicationMaster$$anon$1: Invoking sc stop from shutdown hook&lt;br/&gt;
Exception in thread &quot;Thread-41&quot; java.io.IOException: Filesystem closed&lt;br/&gt;
        at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:398)&lt;br/&gt;
        at org.apache.hadoop.hdfs.DFSOutputStream.hflush(DFSOutputStream.java:1465)&lt;br/&gt;
        at org.apache.hadoop.hdfs.DFSOutputStream.sync(DFSOutputStream.java:1450)&lt;br/&gt;
        at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:116)&lt;br/&gt;
        at org.apache.spark.util.FileLogger$$anonfun$flush$2.apply(FileLogger.scala:137)&lt;br/&gt;
        at org.apache.spark.util.FileLogger$$anonfun$flush$2.apply(FileLogger.scala:137)&lt;br/&gt;
        at scala.Option.foreach(Option.scala:236)&lt;br/&gt;
        at org.apache.spark.util.FileLogger.flush(FileLogger.scala:137)&lt;br/&gt;
        at org.apache.spark.scheduler.EventLoggingListener.logEvent(EventLoggingListener.scala:69)&lt;br/&gt;
        at org.apache.spark.scheduler.EventLoggingListener.onApplicationEnd(EventLoggingListener.scala:101)&lt;br/&gt;
        at org.apache.spark.scheduler.SparkListenerBus$$anonfun$postToAll$13.apply(SparkListenerBus.scala:67)&lt;br/&gt;
        at org.apache.spark.scheduler.SparkListenerBus$$anonfun$postToAll$13.apply(SparkListenerBus.scala:67)&lt;br/&gt;
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)&lt;br/&gt;
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)&lt;br/&gt;
        at org.apache.spark.scheduler.SparkListenerBus$class.postToAll(SparkListenerBus.scala:67)&lt;br/&gt;
        at org.apache.spark.scheduler.LiveListenerBus.postToAll(LiveListenerBus.scala:31)&lt;br/&gt;
        at org.apache.spark.scheduler.LiveListenerBus.post(LiveListenerBus.scala:78)&lt;br/&gt;
        at org.apache.spark.SparkContext.postApplicationEnd(SparkContext.scala:1081)&lt;br/&gt;
        at org.apache.spark.SparkContext.stop(SparkContext.scala:828)&lt;br/&gt;
        at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$1.run(ApplicationMaster.scala:460)&lt;/p&gt;</description>
                <environment></environment>
        <key id="12706427">SPARK-1407</key>
            <summary>EventLogging to HDFS doesn&apos;t work properly on yarn</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="8">Not A Problem</resolution>
                                        <assignee username="tgraves">Thomas Graves</assignee>
                                    <reporter username="tgraves">Thomas Graves</reporter>
                        <labels>
                    </labels>
                <created>Thu, 3 Apr 2014 14:06:59 +0000</created>
                <updated>Fri, 23 Jan 2015 18:32:01 +0000</updated>
                            <resolved>Tue, 8 Apr 2014 21:16:08 +0000</resolved>
                                    <version>1.0.0</version>
                                                    <component>Spark Core</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="13963025" author="tgraves" created="Tue, 8 Apr 2014 14:24:11 +0000"  >&lt;p&gt;Note that the logging also doesn&apos;t work on secure HDFS. &lt;/p&gt;

&lt;p&gt;Exception in thread &quot;Thread-3&quot; java.lang.reflect.UndeclaredThrowableException: Unknown exception in doAs&lt;br/&gt;
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1275)&lt;br/&gt;
        at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:42)&lt;br/&gt;
        at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:192)&lt;br/&gt;
Caused by: java.security.PrivilegedActionException: java.lang.reflect.InvocationTargetException&lt;br/&gt;
        at java.security.AccessController.doPrivileged(Native Method)&lt;br/&gt;
        at javax.security.auth.Subject.doAs(Subject.java:415)&lt;br/&gt;
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1262)&lt;br/&gt;
        ... 2 more&lt;br/&gt;
Caused by: java.lang.reflect.InvocationTargetException&lt;br/&gt;
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)&lt;br/&gt;
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
        at java.lang.reflect.Method.invoke(Method.java:601)&lt;br/&gt;
        at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2$$anonfun$run$1.apply$mcV$sp(ApplicationMaster.scala:198)&lt;br/&gt;
        at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:43)&lt;br/&gt;
        at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)&lt;br/&gt;
        ... 5 more&lt;br/&gt;
Caused by: java.io.IOException: Can&apos;t replace _HOST pattern since client address is null&lt;br/&gt;
        at org.apache.hadoop.security.SecurityUtil.getServerPrincipal(SecurityUtil.java:255)&lt;br/&gt;
        at org.apache.hadoop.ipc.Client$ConnectionId.getRemotePrincipal(Client.java:1326)&lt;br/&gt;
        at org.apache.hadoop.ipc.Client$ConnectionId.getConnectionId(Client.java:1298)&lt;br/&gt;
        at org.apache.hadoop.ipc.WritableRpcEngine$Invoker.&amp;lt;init&amp;gt;(WritableRpcEngine.java:183)&lt;br/&gt;
        at org.apache.hadoop.ipc.WritableRpcEngine.getProxy(WritableRpcEngine.java:236)&lt;br/&gt;
        at org.apache.hadoop.ipc.RPC.getProtocolProxy(RPC.java:441)&lt;br/&gt;
        at org.apache.hadoop.ipc.RPC.getProtocolProxy(RPC.java:387)&lt;br/&gt;
        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:364)&lt;br/&gt;
        at org.apache.hadoop.hdfs.DFSUtil.createRPCNamenode(DFSUtil.java:642)&lt;br/&gt;
        at org.apache.hadoop.hdfs.DFSClient.&amp;lt;init&amp;gt;(DFSClient.java:346)&lt;br/&gt;
        at org.apache.hadoop.hdfs.DFSClient.&amp;lt;init&amp;gt;(DFSClient.java:319)&lt;br/&gt;
        at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:110)&lt;br/&gt;
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2160)&lt;br/&gt;
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:85)&lt;br/&gt;
        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2194)&lt;br/&gt;
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2176)&lt;br/&gt;
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:306)&lt;br/&gt;
        at org.apache.spark.util.Utils$.getHadoopFileSystem(Utils.scala:1022)&lt;br/&gt;
        at org.apache.spark.util.FileLogger.&amp;lt;init&amp;gt;(FileLogger.scala:51)&lt;br/&gt;
        at org.apache.spark.scheduler.EventLoggingListener.&amp;lt;init&amp;gt;(EventLoggingListener.scala:49)&lt;br/&gt;
        at org.apache.spark.SparkContext.&amp;lt;init&amp;gt;(SparkContext.scala:172)&lt;br/&gt;
        at org.apache.spark.SparkContext.&amp;lt;init&amp;gt;(SparkContext.scala:96)&lt;br/&gt;
        at org.apache.spark.examples.SparkPi$.main(SparkPi.scala:31)&lt;br/&gt;
        at org.apache.spark.examples.SparkPi.main(SparkPi.scala)&lt;br/&gt;
        ... 12 more&lt;br/&gt;
Exception in thread &quot;main&quot; java.lang.AssertionError: assertion failed&lt;br/&gt;
        at scala.Predef$.assert(Predef.scala:165)&lt;br/&gt;
        at org.apache.spark.deploy.yarn.ApplicationMaster.waitForSparkContextInitialized(ApplicationMaster.scala:234)&lt;br/&gt;
        at org.apache.spark.deploy.yarn.ApplicationMaster.run(ApplicationMaster.scala:107)&lt;br/&gt;
        at org.apache.spark.deploy.yarn.ApplicationMaster$.main(ApplicationMaster.scala:480)&lt;br/&gt;
        at org.apache.spark.deploy.yarn.ApplicationMaster.main(ApplicationMaster.scala)&lt;br/&gt;
14/04/08 14:23:03 INFO yarn.ApplicationMaster: AppMaster received a signal.&lt;br/&gt;
14/04/08 14:23:03 INFO yarn.ApplicationMaster: Deleting staging directory .sparkStaging/application_1396660776541_63976&lt;/p&gt;</comment>
                            <comment id="13963052" author="tgraves" created="Tue, 8 Apr 2014 14:45:09 +0000"  >&lt;p&gt;Ignore the last comment about secure hdfs.  It turns out I had a invalid config.  It was missing a &apos;/&apos; in the hdfs spark.eventLog.dir location.&lt;/p&gt;</comment>
                            <comment id="13963231" author="pwendell" created="Tue, 8 Apr 2014 17:54:18 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tgraves&quot; class=&quot;user-hover&quot; rel=&quot;tgraves&quot;&gt;tgraves&lt;/a&gt; Did you test this with an application that explicitly calls stop() instead of relying on the shutdown hook in YARN? &lt;/p&gt;</comment>
                            <comment id="13963306" author="tgraves" created="Tue, 8 Apr 2014 18:59:43 +0000"  >&lt;p&gt;I was just running SparkHdfsLR.  I also tried JavaWordCount, same issue.  SparkPi works since it doesn&apos;t access hdfs. &lt;/p&gt;</comment>
                            <comment id="13963310" author="pwendell" created="Tue, 8 Apr 2014 19:03:59 +0000"  >&lt;p&gt;Unfortuantely I think SparkHDFSLR might just cal System.exit instead of stopping properly...&lt;/p&gt;</comment>
                            <comment id="13963456" author="tgraves" created="Tue, 8 Apr 2014 21:15:47 +0000"  >&lt;p&gt;That was the problem.  I modified SparkHdfsLR to not do the System.exit and the logging works now. I&apos;ll close this and open one to change the examples to not do the System.exit&lt;/p&gt;</comment>
                            <comment id="13963688" author="kzhang" created="Wed, 9 Apr 2014 01:14:29 +0000"  >&lt;p&gt;I encountered similar issue. It is not limited to YARN, but shows up in Standalone mode as well. When System.exit is invoked without calling sc.stop(), Hadoop FileSystem&apos;s shutdown hook gets called, which closes fs without flushing client buffer and event log file got truncated. Calling sc.stop() seems to work in my test, which does flush hadoopDataStream buffer before closing. However, this may not completely solve the problem as events are written to buffer asynchronously by SparkListenerBus thread.&lt;/p&gt;</comment>
                            <comment id="13963809" author="kzhang" created="Wed, 9 Apr 2014 05:01:07 +0000"  >&lt;p&gt;Made some change to drain SparkListenerBus&apos; event queue before flushing the buffer of FileLogger and stopping it. This does require sc.stop() to be called for it work, though.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/spark/pull/366&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/366&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13964867" author="kzhang" created="Thu, 10 Apr 2014 01:13:29 +0000"  >&lt;p&gt;One example of the exception I encountered. Note the exact events (onJobEnd in this case) could be different.&lt;/p&gt;

&lt;p&gt;Exception in thread &quot;SparkListenerBus&quot; java.io.IOException: Filesystem closed&lt;br/&gt;
        at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:702)&lt;br/&gt;
        at org.apache.hadoop.hdfs.DFSOutputStream.flushOrSync(DFSOutputStream.java:1832)&lt;br/&gt;
        at org.apache.hadoop.hdfs.DFSOutputStream.hsync(DFSOutputStream.java:1815)&lt;br/&gt;
        at org.apache.hadoop.hdfs.DFSOutputStream.hsync(DFSOutputStream.java:1798)&lt;br/&gt;
        at org.apache.hadoop.fs.FSDataOutputStream.hsync(FSDataOutputStream.java:123)&lt;br/&gt;
        at org.apache.spark.util.FileLogger$$anonfun$flush$2.apply(FileLogger.scala:138)&lt;br/&gt;
        at org.apache.spark.util.FileLogger$$anonfun$flush$2.apply(FileLogger.scala:138)&lt;br/&gt;
        at scala.Option.foreach(Option.scala:236)&lt;br/&gt;
        at org.apache.spark.util.FileLogger.flush(FileLogger.scala:138)&lt;br/&gt;
        at org.apache.spark.scheduler.EventLoggingListener.logEvent(EventLoggingListener.scala:64)&lt;br/&gt;
        at org.apache.spark.scheduler.EventLoggingListener.onJobEnd(EventLoggingListener.scala:86)&lt;br/&gt;
        at org.apache.spark.scheduler.SparkListenerBus$$anonfun$postToAll$4.apply(SparkListenerBus.scala:49)&lt;br/&gt;
        at org.apache.spark.scheduler.SparkListenerBus$$anonfun$postToAll$4.apply(SparkListenerBus.scala:49)&lt;br/&gt;
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)&lt;br/&gt;
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)&lt;br/&gt;
        at org.apache.spark.scheduler.SparkListenerBus$class.postToAll(SparkListenerBus.scala:49)&lt;br/&gt;
        at org.apache.spark.scheduler.LiveListenerBus.postToAll(LiveListenerBus.scala:31)&lt;br/&gt;
        at org.apache.spark.scheduler.LiveListenerBus$$anon$1.run(LiveListenerBus.scala:61)&lt;/p&gt;</comment>
                            <comment id="13967234" author="kzhang" created="Fri, 11 Apr 2014 23:24:47 +0000"  >&lt;p&gt;I opened &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-1475&quot; title=&quot;Drain event logging queue before stopping event logger&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-1475&quot;&gt;&lt;del&gt;SPARK-1475&lt;/del&gt;&lt;/a&gt; to track the above PR.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="12732580">SPARK-2906</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12704686">SPARK-1132</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12708043">SPARK-1475</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>384750</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            11 years, 32 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1ua7b:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>385017</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>