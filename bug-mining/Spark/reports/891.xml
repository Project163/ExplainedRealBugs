<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:19:07 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-3958] Possible stream-corruption issues in TorrentBroadcast</title>
                <link>https://issues.apache.org/jira/browse/SPARK-3958</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;TorrentBroadcast deserialization sometimes fails with decompression errors, which are most likely caused by stream-corruption exceptions.  For example, this can manifest itself as a Snappy PARSING_ERROR when deserializing a broadcasted task:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;14/10/14 17:20:55.016 DEBUG BlockManager: Getting local block broadcast_8
14/10/14 17:20:55.016 DEBUG BlockManager: Block broadcast_8 not registered locally
14/10/14 17:20:55.016 INFO TorrentBroadcast: Started reading broadcast variable 8
14/10/14 17:20:55.017 INFO TorrentBroadcast: Reading broadcast variable 8 took 5.3433E-5 s
14/10/14 17:20:55.017 ERROR Executor: Exception in task 2.0 in stage 8.0 (TID 18)
java.io.IOException: PARSING_ERROR(2)
	at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:84)
	at org.xerial.snappy.SnappyNative.uncompressedLength(Native Method)
	at org.xerial.snappy.Snappy.uncompressedLength(Snappy.java:594)
	at org.xerial.snappy.SnappyInputStream.readFully(SnappyInputStream.java:125)
	at org.xerial.snappy.SnappyInputStream.readHeader(SnappyInputStream.java:88)
	at org.xerial.snappy.SnappyInputStream.&amp;lt;init&amp;gt;(SnappyInputStream.java:58)
	at org.apache.spark.io.SnappyCompressionCodec.compressedInputStream(CompressionCodec.scala:128)
	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:216)
	at org.apache.spark.broadcast.TorrentBroadcast.readObject(TorrentBroadcast.scala:170)
	at sun.reflect.GeneratedMethodAccessor92.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:87)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:164)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-3630&quot; title=&quot;Identify cause of Kryo+Snappy PARSING_ERROR&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-3630&quot;&gt;&lt;del&gt;SPARK-3630&lt;/del&gt;&lt;/a&gt; is an umbrella ticket for investigating all causes of these Kryo and Snappy deserialization errors.  This ticket is for a more narrowly-focused exploration of the TorrentBroadcast version of these errors, since the similar errors that we&apos;ve seen in sort-based shuffle seem to be explained by a different cause (see &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-3948&quot; title=&quot;Sort-based shuffle can lead to assorted stream-corruption exceptions&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-3948&quot;&gt;&lt;del&gt;SPARK-3948&lt;/del&gt;&lt;/a&gt;).&lt;/p&gt;</description>
                <environment></environment>
        <key id="12748414">SPARK-3958</key>
            <summary>Possible stream-corruption issues in TorrentBroadcast</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="joshrosen">Josh Rosen</assignee>
                                    <reporter username="joshrosen">Josh Rosen</reporter>
                        <labels>
                    </labels>
                <created>Wed, 15 Oct 2014 20:54:21 +0000</created>
                <updated>Mon, 25 Jan 2016 08:46:01 +0000</updated>
                            <resolved>Wed, 21 Jan 2015 22:28:49 +0000</resolved>
                                    <version>1.1.0</version>
                    <version>1.2.0</version>
                                                    <component>Spark Core</component>
                        <due></due>
                            <votes>4</votes>
                                    <watches>9</watches>
                                                                                                                <comments>
                            <comment id="14172949" author="joshrosen" created="Wed, 15 Oct 2014 21:11:26 +0000"  >&lt;p&gt;Digging into this stacktrace in more detail:&lt;/p&gt;

&lt;p&gt;Snappy-java&apos;s &lt;tt&gt;SnappyOutputStream&lt;/tt&gt; writes its own 8-byte header at the beginning of the serialized output.  This header consists of an 8-byte magic value followed by two 4-byte version numbers.  This header is distinct from Snappy&apos;s own 6-byte magic number / header.&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;org.xerial.snappy.SnappyInputStream.readHeader&lt;/tt&gt; is implemented like this (in Snappy-Java 1.1.1.3):&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;protected&lt;/span&gt; void readHeader() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
        &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt;[] header = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt;[SnappyCodec.headerSize()];
        &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; readBytes = 0;
        &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; (readBytes &amp;lt; header.length) {
            &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; ret = in.read(header, readBytes, header.length - readBytes);
            &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (ret == -1)
                &lt;span class=&quot;code-keyword&quot;&gt;break&lt;/span&gt;;
            readBytes += ret;
        }

        &lt;span class=&quot;code-comment&quot;&gt;// Quick test of the header 
&lt;/span&gt;        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (readBytes &amp;lt; header.length || header[0] != SnappyCodec.MAGIC_HEADER[0]) {
            &lt;span class=&quot;code-comment&quot;&gt;// &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; the &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt; uncompression
&lt;/span&gt;            readFully(header, readBytes);
            &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt;;
        }

        SnappyCodec codec = SnappyCodec.readHeader(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ByteArrayInputStream(header));
        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (codec.isValidMagicHeader()) {
            &lt;span class=&quot;code-comment&quot;&gt;// The input data is compressed by SnappyOutputStream
&lt;/span&gt;            &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (codec.version &amp;lt; SnappyCodec.MINIMUM_COMPATIBLE_VERSION) {
                &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; IOException(&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;.format(
                        &lt;span class=&quot;code-quote&quot;&gt;&quot;compressed with imcompatible codec version %d. At least version %d is required&quot;&lt;/span&gt;,
                        codec.version, SnappyCodec.MINIMUM_COMPATIBLE_VERSION));
            }
        }
        &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
            &lt;span class=&quot;code-comment&quot;&gt;// (probably) compressed by Snappy.compress(&lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt;[])
&lt;/span&gt;            readFully(header, readBytes);
            &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt;;
        }
    }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It starts by attempting to read the 8-byte header.  The first &lt;tt&gt;while&lt;/tt&gt; loop exits when we&apos;ve either read 8 bytes of header data or if the input stream was closed before it could read a complete header.  The following code checks whether the header is unexpectedly short or whether it doesn&apos;t match the snappy-java magic header.  In our case, we end up taking this branch and calling &lt;tt&gt;readFully(header, readBytes)&lt;/tt&gt; in order to perform the default Snappy decompression  This is the wrong branch to take (since our data was compressed with a SnappyOutputStream), leading to the PARSING_ERROR.&lt;/p&gt;

&lt;p&gt;Based on this, I think that the input data to the SnappyInputStream is somehow being corrupted.  It&apos;s not obvious whether this corruption is causing the input data to be too short or whether the start of the stream has the wrong contents.  I&apos;ll keep digging and look into adding some size-checking assertions throughout our code.&lt;/p&gt;</comment>
                            <comment id="14173048" author="joshrosen" created="Wed, 15 Oct 2014 22:08:35 +0000"  >&lt;p&gt;Removing 1.1.0 as an affected version for now, since the stacktrace that I posted here was from a recent build of master (1.2).  If anyone can reproduce this in branch-1.1 or Spark 1.1.0, please let me know.&lt;/p&gt;</comment>
                            <comment id="14173154" author="joshrosen" created="Wed, 15 Oct 2014 23:49:20 +0000"  >&lt;p&gt;I think that I can safely rule out problems in TorrentBroadcast&apos;s (de-)blockification code: I used ScalaCheck to write some tests to ensure that blockifyObject and unblockifyObject are inverses, plus a similar test for ByteArrayChunkOutputStream: &lt;a href=&quot;https://github.com/JoshRosen/spark/commit/413be7f6a8d4eb14c69c7db87e2564ed4d776c42?diff=unified&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/JoshRosen/spark/commit/413be7f6a8d4eb14c69c7db87e2564ed4d776c42?diff=unified&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14173199" author="jerryshao" created="Thu, 16 Oct 2014 01:16:12 +0000"  >&lt;p&gt;Hi Josh, have you tried other compression like LZO to narrow down the problem?&lt;/p&gt;</comment>
                            <comment id="14173207" author="joshrosen" created="Thu, 16 Oct 2014 01:26:45 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jerryshao&quot; class=&quot;user-hover&quot; rel=&quot;jerryshao&quot;&gt;jerryshao&lt;/a&gt;,&lt;/p&gt;

&lt;p&gt;I don&apos;t have a reliable reproduction for this issue yet, so I haven&apos;t tried switching compression schemes or broadcast implementations.  I&apos;m working with the user who provided this stack trace to see if we can get more logs to provide additional context.&lt;/p&gt;</comment>
                            <comment id="14174662" author="joshrosen" created="Fri, 17 Oct 2014 02:29:24 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=davies&quot; class=&quot;user-hover&quot; rel=&quot;davies&quot;&gt;davies&lt;/a&gt; ran across this exception while testing a pull request that modifies TorrentBroadcast: &lt;a href=&quot;https://github.com/apache/spark/pull/2681#issuecomment-59120483&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/2681#issuecomment-59120483&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;That PR&apos;s reproductioncould be a valuable debugging clue for this issue.&lt;/p&gt;</comment>
                            <comment id="14176236" author="apachespark" created="Sun, 19 Oct 2014 07:08:29 +0000"  >&lt;p&gt;User &apos;JoshRosen&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/2844&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/2844&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14188670" author="joshrosen" created="Wed, 29 Oct 2014 17:50:08 +0000"  >&lt;p&gt;Adding 1.1.0 as an affected version, since a user has observed this in 1.1.0, too; see &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-4133&quot; title=&quot;PARSING_ERROR(2) when upgrading issues from 1.0.2 to 1.1.0&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-4133&quot;&gt;&lt;del&gt;SPARK-4133&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="14196288" author="vitalii.migov@gmail.com" created="Tue, 4 Nov 2014 16:10:09 +0000"  >&lt;p&gt;Observed the same exception ( Spark 1.1.0 ). After changing broadcast factory to the HttpBroadcastFactory ( as suggested in the &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-4133&quot; title=&quot;PARSING_ERROR(2) when upgrading issues from 1.0.2 to 1.1.0&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-4133&quot;&gt;&lt;del&gt;SPARK-4133&lt;/del&gt;&lt;/a&gt; ), exception looks like:&lt;br/&gt;
java.io.FileNotFoundException: &lt;a href=&quot;http://10.8.0.22:44907/broadcast_0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://10.8.0.22:44907/broadcast_0&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Full logs attached to issue: spark_ex.logs &lt;/p&gt;

&lt;p&gt;Seems that something wrong with the handling of the &quot;broadcast_0&quot; in the BlockManager:&lt;br/&gt;
I think that something wrong with the handing of the &quot;broadcast_0&quot; in the BlockManager:&lt;br/&gt;
14/11/04 17:20:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1216.0 B, free 983.1 MB)&lt;br/&gt;
14/11/04 17:20:39 DEBUG BlockManager: Put block broadcast_0 locally took 84 ms&lt;br/&gt;
14/11/04 17:20:39 DEBUG BlockManager: Putting block broadcast_0 without replication took 86 ms&lt;br/&gt;
14/11/04 17:20:39 DEBUG BlockManager: Getting local block broadcast_0&lt;br/&gt;
14/11/04 17:20:39 DEBUG BlockManager: Level for block broadcast_0 is StorageLevel(true, true, false, true, 1)&lt;br/&gt;
14/11/04 17:20:39 DEBUG BlockManager: Getting block broadcast_0 from memory&lt;br/&gt;
14/11/04 17:20:39 INFO BlockManager: Found block broadcast_0 locally&lt;br/&gt;
14/11/04 17:20:57 WARN BlockManager: Block broadcast_0 already exists on this machine; not re-adding it&lt;br/&gt;
14/11/04 17:20:57 DEBUG BlockManager: Getting local block broadcast_0&lt;br/&gt;
14/11/04 17:20:57 DEBUG BlockManager: Block broadcast_0 not registered locally&lt;br/&gt;
14/11/04 17:20:57 DEBUG BlockManager: Getting remote block broadcast_0&lt;br/&gt;
14/11/04 17:20:57 DEBUG BlockManagerMasterActor: &lt;span class=&quot;error&quot;&gt;&amp;#91;actor&amp;#93;&lt;/span&gt; received message GetLocations(broadcast_0) from Actor&lt;span class=&quot;error&quot;&gt;&amp;#91;akka://sparkDriver/temp/$l&amp;#93;&lt;/span&gt;&lt;br/&gt;
14/11/04 17:20:57 DEBUG BlockManager: Block broadcast_0 not found&lt;br/&gt;
14/11/04 17:20:57 DEBUG BlockManagerMasterActor: &lt;span class=&quot;error&quot;&gt;&amp;#91;actor&amp;#93;&lt;/span&gt; handled message (0.117676 ms) GetLocations(broadcast_0) from Actor&lt;span class=&quot;error&quot;&gt;&amp;#91;akka://sparkDriver/temp/$l&amp;#93;&lt;/span&gt;&lt;br/&gt;
14/11/04 17:20:57 INFO HttpBroadcast: Started reading broadcast variable 0&lt;br/&gt;
14/11/04 17:20:57 DEBUG HttpBroadcast: broadcast read server: &lt;a href=&quot;http://10.8.0.22:44907&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://10.8.0.22:44907&lt;/a&gt; id: broadcast-0&lt;br/&gt;
14/11/04 17:20:57 DEBUG HttpBroadcast: broadcast not using security&lt;br/&gt;
14/11/04 17:20:57 DEBUG RecurringTimer: Callback for BlockGenerator called at time 1415114457200&lt;br/&gt;
14/11/04 17:20:57 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)&lt;br/&gt;
java.io.FileNotFoundException: &lt;a href=&quot;http://10.8.0.22:44907/broadcast_0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://10.8.0.22:44907/broadcast_0&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14286429" author="pwendell" created="Wed, 21 Jan 2015 22:28:49 +0000"  >&lt;p&gt;At this point I&apos;m not aware of people still hitting this set of issues in newer releases, so per discussion with &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=joshrosen&quot; class=&quot;user-hover&quot; rel=&quot;joshrosen&quot;&gt;joshrosen&lt;/a&gt;, I&apos;d like to close this. Please comment on this JIRA if you are having some variant of this issue in a newer version of Spark, and we&apos;ll continue to investigate.&lt;/p&gt;</comment>
                            <comment id="15114906" author="gmaas" created="Mon, 25 Jan 2016 08:46:01 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pwendell&quot; class=&quot;user-hover&quot; rel=&quot;pwendell&quot;&gt;pwendell&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=joshrosen&quot; class=&quot;user-hover&quot; rel=&quot;joshrosen&quot;&gt;joshrosen&lt;/a&gt; We just hit this bug in one of our production jobs using Spark Streaming 1.4.1. Each task spawned by the streaming job fails down the road.&lt;br/&gt;
This jobs has been working fine for months, so I&apos;m not clear on whether we can narrow down the conditions to reproduce it.&lt;/p&gt;

&lt;p&gt;Here&apos;s the exception:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;[Stage 16049:(0 + 0) / 24][Stage 16056:(0 + 0) / 24][Stage 16058:(0 + 0) / 24]Exception in thread &lt;span class=&quot;code-quote&quot;&gt;&quot;main&quot;&lt;/span&gt; org.apache.spark.SparkException: Job aborted due to stage failure: Task 23 in stage 17478.0 failed 6 times, most recent failure: Lost task 23.5 in stage 17478.0 (TID 172352, dnode-6.hdfs.&lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt;): java.io.IOException: PARSING_ERROR(2)
	at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:84)
	at org.xerial.snappy.SnappyNative.uncompressedLength(Native Method)
	at org.xerial.snappy.Snappy.uncompressedLength(Snappy.java:594)
	at org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:358)
	at org.xerial.snappy.SnappyInputStream.read(SnappyInputStream.java:387)
	at java.io.ObjectInputStream$PeekInputStream.peek(ObjectInputStream.java:2296)
	at java.io.ObjectInputStream$BlockDataInputStream.peek(ObjectInputStream.java:2589)
	at java.io.ObjectInputStream$BlockDataInputStream.peekByte(ObjectInputStream.java:2599)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1319)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:69)
	at org.apache.spark.serializer.DeserializationStream.readKey(Serializer.scala:169)
	at org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:200)
	at org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:197)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:127)
	at org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:91)
	at org.apache.spark.shuffle.hash.HashShuffleReader.read(HashShuffleReader.scala:44)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:90)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:70)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263)
	at scala.collection.mutable.ResizableArray$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;(Murphy&apos;s law: Bugs reappear the moment you close them)&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12743065">SPARK-3630</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12751361">SPARK-4133</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12679242" name="spark_ex.logs" size="79487" author="vitalii.migov@gmail.com" created="Tue, 4 Nov 2014 16:09:34 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 43 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i217un:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>