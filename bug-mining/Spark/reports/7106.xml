<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 19:15:40 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-30921] Error using VectorAssembler after Pandas GROUPED_AGG UDF</title>
                <link>https://issues.apache.org/jira/browse/SPARK-30921</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Using VectorAssembler after a Pandas GROUPED_AGG and join causes an opaque error:&lt;/p&gt;

&lt;p&gt;Caused by: java.lang.UnsupportedOperationException: Cannot evaluate expression: apply_impl(input&lt;span class=&quot;error&quot;&gt;&amp;#91;1, struct&amp;lt;t:bigint,val:bigint&amp;gt;, true&amp;#93;&lt;/span&gt;.val)&lt;/p&gt;

&lt;p&gt;However, inserting a .cache() between the VectorAssembler and join seems to prevent VectorAssembler &amp;amp; Pandas UDF from interacting to cause this error.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;E py4j.protocol.Py4JJavaError: An error occurred while calling o259.collectToPython.&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E : org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E Exchange hashpartitioning(foo_id_SummaryAggregator_AOG2FHR#34L, 4)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E +- *(4) Filter AtLeastNNulls(n, apply_impl(foo_explode_SummaryAggregator_AOG2FHR#20.val),apply_impl(foo_explode_SummaryAggregator_AOG2FHR#20.val))&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E +- Generate explode(foo#11), &lt;a href=&quot;#34L&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;foo_id_SummaryAggregator_AOG2FHR#34L&lt;/a&gt;, true, &lt;a href=&quot;#20&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;foo_explode_SummaryAggregator_AOG2FHR#20&lt;/a&gt;&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E +- *(3) Project &lt;a href=&quot;#11, monotonically_increasing_id() AS foo_id_SummaryAggregator_AOG2FHR#34L&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;foo#11, monotonically_increasing_id() AS foo_id_SummaryAggregator_AOG2FHR#34L&lt;/a&gt;&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E +- Scan ExistingRDD&lt;a href=&quot;#11,id#12L&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;foo#11,id#12L&lt;/a&gt;&lt;/tt&gt;&lt;br/&gt;
{{E }}&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.python.AggregateInPandasExec.doExecute(AggregateInPandasExec.scala:80)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.InputAdapter.doExecute(WholeStageCodegenExec.scala:383)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.joins.SortMergeJoinExec.inputRDDs(SortMergeJoinExec.scala:386)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:296)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3263)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3260)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3260)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at java.lang.reflect.Method.invoke(Method.java:498)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at py4j.Gateway.invoke(Gateway.java:282)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at py4j.commands.CallCommand.execute(CallCommand.java:79)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at py4j.GatewayConnection.run(GatewayConnection.java:238)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at java.lang.Thread.run(Thread.java:748)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E Caused by: java.lang.UnsupportedOperationException: Cannot evaluate expression: apply_impl(input&lt;span class=&quot;error&quot;&gt;&amp;#91;1, struct&amp;lt;t:bigint,val:bigint&amp;gt;, true&amp;#93;&lt;/span&gt;.val)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.catalyst.expressions.Unevaluable$class.doGenCode(Expression.scala:261)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.catalyst.expressions.PythonUDF.doGenCode(PythonUDF.scala:50)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:108)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:105)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at scala.Option.getOrElse(Option.scala:121)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:105)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.catalyst.expressions.AtLeastNNonNulls$$anonfun$4.apply(nullExpressions.scala:402)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.catalyst.expressions.AtLeastNNonNulls$$anonfun$4.apply(nullExpressions.scala:401)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at scala.collection.AbstractTraversable.map(Traversable.scala:104)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.catalyst.expressions.AtLeastNNonNulls.doGenCode(nullExpressions.scala:401)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:108)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:105)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at scala.Option.getOrElse(Option.scala:121)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:105)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.FilterExec.org$apache$spark$sql$execution$FilterExec$$genPredicate$1(basicPhysicalOperators.scala:139)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.FilterExec$$anonfun$13.apply(basicPhysicalOperators.scala:179)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.FilterExec$$anonfun$13.apply(basicPhysicalOperators.scala:163)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at scala.collection.immutable.List.foreach(List.scala:392)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at scala.collection.immutable.List.map(List.scala:296)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:163)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:189)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:374)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:403)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:374)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:125)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:85)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:544)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:598)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;E ... 69 more&lt;/tt&gt;&lt;/p&gt;</description>
                <environment>&lt;p&gt;numpy==1.16.4&lt;br/&gt;
pandas==0.23.4&lt;br/&gt;
py4j==0.10.7&lt;br/&gt;
pyarrow==0.8.0&lt;br/&gt;
pyspark==2.4.4&lt;br/&gt;
scikit-learn==0.19.1&lt;br/&gt;
scipy==1.1.0&lt;/p&gt;</environment>
        <key id="13286923">SPARK-30921</key>
            <summary>Error using VectorAssembler after Pandas GROUPED_AGG UDF</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="viirya">L. C. Hsieh</assignee>
                                    <reporter username="tkellogg">Tim Kellogg</reporter>
                        <labels>
                    </labels>
                <created>Sat, 22 Feb 2020 00:35:14 +0000</created>
                <updated>Mon, 12 Dec 2022 18:10:38 +0000</updated>
                            <resolved>Mon, 6 Apr 2020 00:40:16 +0000</resolved>
                                    <version>2.4.4</version>
                                    <fixVersion>3.0.0</fixVersion>
                                    <component>ML</component>
                    <component>PySpark</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="17075985" author="gurwls223" created="Mon, 6 Apr 2020 00:40:16 +0000"  >&lt;p&gt;Fixed in &lt;a href=&quot;https://github.com/apache/spark/pull/28089&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/28089&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12994151" name="test_dyn_pandas_function.py" size="1275" author="tkellogg" created="Sat, 22 Feb 2020 00:35:44 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            5 years, 32 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z0brvc:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>