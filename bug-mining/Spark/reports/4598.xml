<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:51:34 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-19276] FetchFailures can be hidden by user (or sql) exception handling</title>
                <link>https://issues.apache.org/jira/browse/SPARK-19276</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;The scheduler handles node failures by looking for a special &lt;tt&gt;FetchFailedException&lt;/tt&gt; thrown by the shuffle block fetcher.  This is handled in &lt;tt&gt;Executor&lt;/tt&gt; and then passed as a special msg back to the driver: &lt;a href=&quot;https://github.com/apache/spark/blob/278fa1eb305220a85c816c948932d6af8fa619aa/core/src/main/scala/org/apache/spark/executor/Executor.scala#L403&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/278fa1eb305220a85c816c948932d6af8fa619aa/core/src/main/scala/org/apache/spark/executor/Executor.scala#L403&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;However, user code exists in between the shuffle block fetcher and that catch block &amp;#8211; it could intercept the exception, wrap it with something else, and throw a different exception.  If that happens, spark treats it as an ordinary task failure, and retries the task, rather than regenerating the missing shuffle data.  The task eventually is retried 4 times, its doomed to fail each time, and the job is failed.&lt;/p&gt;

&lt;p&gt;You might think that no user code should do that &amp;#8211; but even sparksql does it:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/blob/278fa1eb305220a85c816c948932d6af8fa619aa/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala#L214&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/278fa1eb305220a85c816c948932d6af8fa619aa/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala#L214&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here&apos;s an example stack trace.  This is from Spark 1.6, so the sql code is not the same, but the problem is still there:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;17/01/13 19:18:02 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 1983.0 (TID 304851, xxx): org.apache.spark.SparkException: Task failed while writing rows.
        at org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer.writeRows(WriterContainer.scala:414)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
        at org.apache.spark.scheduler.Task.run(Task.scala:89)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.shuffle.FetchFailedException: Failed to connect to xxx/yyy:zzz
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)
...
17/01/13 19:19:29 ERROR scheduler.TaskSetManager: Task 0 in stage 1983.0 failed 4 times; aborting job
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I think the right fix here is to also set a fetch failure status in the &lt;tt&gt;TaskContextImpl&lt;/tt&gt;, so the executor can check that instead of just one exception.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13035951">SPARK-19276</key>
            <summary>FetchFailures can be hidden by user (or sql) exception handling</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="imranr">Imran Rashid</assignee>
                                    <reporter username="irashid">Imran Rashid</reporter>
                        <labels>
                    </labels>
                <created>Wed, 18 Jan 2017 18:22:01 +0000</created>
                <updated>Thu, 5 Apr 2018 21:23:08 +0000</updated>
                            <resolved>Fri, 3 Mar 2017 00:48:09 +0000</resolved>
                                    <version>2.1.0</version>
                                    <fixVersion>2.2.0</fixVersion>
                                    <component>Scheduler</component>
                    <component>Spark Core</component>
                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>8</watches>
                                                                                                                <comments>
                            <comment id="15828518" author="irashid" created="Wed, 18 Jan 2017 18:28:45 +0000"  >&lt;p&gt;I haven&apos;t been successful in creating a test case to reproduce this.  In my attempts, I see retries sometimes failing to fetch block &lt;b&gt;status&lt;/b&gt;, which happens in the initialization of the &lt;tt&gt;ShuffleBlockFetcherIterator&lt;/tt&gt;.  since that is outside user code, it does get handled correctly, and the shuffle data is regenerated.  But, the problem is clearly there, and I&apos;ve seen it effect real users.&lt;/p&gt;

&lt;p&gt;open to ideas for the test case &amp;amp; reproduction.&lt;/p&gt;</comment>
                            <comment id="15829396" author="apachespark" created="Thu, 19 Jan 2017 06:32:05 +0000"  >&lt;p&gt;User &apos;squito&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/16639&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/16639&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15830387" author="markhamstra" created="Thu, 19 Jan 2017 18:40:40 +0000"  >&lt;p&gt;This all makes sense, and the PR is a good effort to fix this kind of &quot;accidental&quot; swallowing of FetchFailedException.  I guess my only real question is if we should allow for the possibility of a FetchFailedException not only being caught, but also the failure being remedied by some means other than the usual handling in the driver. I&apos;m not sure exactly how or why that kind of &quot;fix the fetch failure before Spark tries to handle it&quot; would be done; and it would seem that something like that would be prone to subtle errors, so maybe we should just set it in stone that nobody but the driver should try to fix a fetch failure &amp;#8211; which would make the approach of your &quot;guarantee that the FetchFailedException is seen by the driver&quot; PR completely correct.&lt;/p&gt;</comment>
                            <comment id="15830487" author="irashid" created="Thu, 19 Jan 2017 19:48:55 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=markhamstra&quot; class=&quot;user-hover&quot; rel=&quot;markhamstra&quot;&gt;markhamstra&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;I guess my only real question is if we should allow for the possibility of a FetchFailedException not only being caught, but also the failure being remedied by some means other than the usual handling in the driver.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I wondered about this too &amp;#8211; in fact, in the end, the pr &lt;b&gt;does&lt;/b&gt; allow that.  You&apos;ll notice that if the fetch failure is set in the task context, but the task succeeds, than I log an error but otherwise let the task continue.  We only send it back to the driver if there is also a task failure.&lt;/p&gt;

&lt;p&gt;I decided to do it that way in case there is some crazy existing code out there which relies on this behavior ... but honestly I&apos;m not sure that is the right decision, maybe it should still send the fetch failure back to the driver and fail the task in that case too.&lt;/p&gt;</comment>
                            <comment id="15830519" author="markhamstra" created="Thu, 19 Jan 2017 20:09:12 +0000"  >&lt;p&gt;Ok, I haven&apos;t read your PR closely yet, so I missed that.&lt;/p&gt;

&lt;p&gt;This question looks like something that could use more eyes and insights. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=kayousterhout&quot; class=&quot;user-hover&quot; rel=&quot;kayousterhout&quot;&gt;kayousterhout&lt;/a&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=matei&quot; class=&quot;user-hover&quot; rel=&quot;matei&quot;&gt;matei&lt;/a&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rxin%40databricks.com&quot; class=&quot;user-hover&quot; rel=&quot;rxin@databricks.com&quot;&gt;rxin@databricks.com&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16421879" author="xchen12138" created="Mon, 2 Apr 2018 02:25:01 +0000"  >&lt;p&gt;I believe it cause another bug. If the task is killed with reason &quot;another attempt success&quot; and calling&#160;&lt;/p&gt;

&lt;p&gt;interrupt of the thread. The thread will throw a&#160;ClosedByInterruptException. It&apos;s an IO exception but not a Fetch fail exception. So when a speculative task creating input stream and kill by&#160;interrupt function of the thread, spark will treat it as fetch failure and try to regenerate shuffle data, which is completely wrong.&lt;/p&gt;</comment>
                            <comment id="16422588" author="irashid" created="Mon, 2 Apr 2018 14:57:51 +0000"  >&lt;p&gt;Oh thanks for pointing that out &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xchen12138&quot; class=&quot;user-hover&quot; rel=&quot;xchen12138&quot;&gt;xchen12138&lt;/a&gt;, I think you are absolutely correct.  Would you mind opening another bug and cc&apos;ing me?  It would be great if you can attach logs&lt;/p&gt;</comment>
                            <comment id="16424923" author="xchen12138" created="Wed, 4 Apr 2018 02:56:57 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=imranr&quot; class=&quot;user-hover&quot; rel=&quot;imranr&quot;&gt;imranr&lt;/a&gt;, I have created another task about this issue several days ago: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-23816&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/SPARK-23816&lt;/a&gt;. Could you please help review it?&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="13067172">SPARK-20480</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="13069855">SPARK-20633</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13148812">SPARK-23816</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 32 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i38vyv:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>