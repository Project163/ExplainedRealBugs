<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:38:43 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-12009] Avoid re-allocate yarn container while driver want to stop all Executors</title>
                <link>https://issues.apache.org/jira/browse/SPARK-12009</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Log based 1.4.0&lt;/p&gt;

&lt;p&gt;2015-11-26,03:05:16,176 WARN org.spark-project.jetty.util.thread.QueuedThreadPool: 8 threads could not be stopped&lt;br/&gt;
2015-11-26,03:05:16,177 INFO org.apache.spark.ui.SparkUI: Stopped Spark web UI at http://&lt;br/&gt;
2015-11-26,03:05:16,401 INFO org.apache.spark.scheduler.DAGScheduler: Stopping DAGScheduler&lt;br/&gt;
2015-11-26,03:05:16,450 INFO org.apache.spark.scheduler.cluster.YarnClusterSchedulerBackend: Shutting down all executors&lt;br/&gt;
2015-11-26,03:05:16,525 INFO org.apache.spark.scheduler.cluster.YarnClusterSchedulerBackend: Asking each executor to shut down&lt;br/&gt;
2015-11-26,03:05:16,791 INFO org.apache.spark.deploy.yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. XX.XX.XX.XX:38734&lt;br/&gt;
2015-11-26,03:05:16,847 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(164,WrappedArray())&lt;/p&gt;

&lt;p&gt;2015-11-26,03:05:27,242 INFO org.apache.spark.deploy.yarn.YarnAllocator: Will request 13 executor containers, each with 1 cores and 4608 MB memory including 1024 MB overhead&lt;/p&gt;</description>
                <environment></environment>
        <key id="12916368">SPARK-12009</key>
            <summary>Avoid re-allocate yarn container while driver want to stop all Executors</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Minor</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="SuYan">SuYan</assignee>
                                    <reporter username="SuYan">SuYan</reporter>
                        <labels>
                    </labels>
                <created>Thu, 26 Nov 2015 07:07:56 +0000</created>
                <updated>Sun, 17 May 2020 18:16:57 +0000</updated>
                            <resolved>Fri, 26 Feb 2016 00:57:33 +0000</resolved>
                                    <version>1.5.2</version>
                                    <fixVersion>2.0.0</fixVersion>
                                    <component>Spark Core</component>
                    <component>YARN</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>12</watches>
                                                                                                                <comments>
                            <comment id="15028288" author="suyan" created="Thu, 26 Nov 2015 07:08:44 +0000"  >&lt;p&gt;user had called sc.stop in main Program&lt;/p&gt;</comment>
                            <comment id="15028291" author="apachespark" created="Thu, 26 Nov 2015 07:12:04 +0000"  >&lt;p&gt;User &apos;suyanNone&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/9992&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/9992&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15028325" author="jerryshao" created="Thu, 26 Nov 2015 07:56:20 +0000"  >&lt;p&gt;A interesting thing is that AM is shutting down at time &lt;tt&gt;2015-11-26,03:05:16&lt;/tt&gt;, but YarnAllocator still request 13 executors after 11 seconds. Looks like AM is not exited so fast, that&apos;s why YarnAllocator is still requesting new containers. Normally if AM is exited as fast as it receive disconnected message, there will be not time for container requesting for YarnAllocator.&lt;/p&gt;</comment>
                            <comment id="15028340" author="jerryshao" created="Thu, 26 Nov 2015 08:08:00 +0000"  >&lt;p&gt;Looking at the code again, &lt;tt&gt;onDisconnected&lt;/tt&gt; in &lt;tt&gt;ApplicationMaster&lt;/tt&gt; will call &lt;tt&gt;finish()&lt;/tt&gt; to interrupt the report thread, if report thread is interrupted, there&apos;s no chance for &lt;tt&gt;YarnAllocator&lt;/tt&gt; to request new containers. It&apos;s quite weird how this happened.&lt;/p&gt;</comment>
                            <comment id="15028363" author="suyan" created="Thu, 26 Nov 2015 08:42:04 +0000"  >&lt;p&gt;AM is not exit, it will exit while driver complete its usercode in userThread.  &lt;/p&gt;

&lt;p&gt;the below logs tell that a executor is terminated.&lt;br/&gt;
2015-11-26,03:05:16,791 INFO org.apache.spark.deploy.yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. XX.XX.XX.XX:38734&lt;/p&gt;</comment>
                            <comment id="15028370" author="jerryshao" created="Thu, 26 Nov 2015 08:47:35 +0000"  >&lt;p&gt;If I understood correctly, this log should come from application master, I don&apos;t think it related to executor termination.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    override def onDisconnected(remoteAddress: RpcAddress): Unit = {
      &lt;span class=&quot;code-comment&quot;&gt;// In cluster mode, &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; not rely on the disassociated event to exit
&lt;/span&gt;      &lt;span class=&quot;code-comment&quot;&gt;// This avoids potentially reporting incorrect exit codes &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the driver fails
&lt;/span&gt;      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!isClusterMode) {
        logInfo(s&lt;span class=&quot;code-quote&quot;&gt;&quot;Driver terminated or disconnected! Shutting down. $remoteAddress&quot;&lt;/span&gt;)
        finish(FinalApplicationStatus.SUCCEEDED, ApplicationMaster.EXIT_SUCCESS)
      }
    }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15028375" author="jerryshao" created="Thu, 26 Nov 2015 08:58:17 +0000"  >&lt;p&gt;Looks like you&apos;re running on yarn-cluster mode, but this log will only will be printed in yarn-client mode, is it possible that some other places printed same log?&lt;/p&gt;</comment>
                            <comment id="15029410" author="suyan" created="Fri, 27 Nov 2015 03:26:36 +0000"  >&lt;p&gt; = =, the log is based 1.4.0&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    override def onDisconnected(remoteAddress: RpcAddress): Unit = {
      logInfo(s&lt;span class=&quot;code-quote&quot;&gt;&quot;Driver terminated or disconnected! Shutting down. $remoteAddress&quot;&lt;/span&gt;)
      &lt;span class=&quot;code-comment&quot;&gt;// In cluster mode, &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; not rely on the disassociated event to exit
&lt;/span&gt;      &lt;span class=&quot;code-comment&quot;&gt;// This avoids potentially reporting incorrect exit codes &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the driver fails
&lt;/span&gt;      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!isClusterMode) {
        finish(FinalApplicationStatus.SUCCEEDED, ApplicationMaster.EXIT_SUCCESS)
      }
    }
  } 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15029416" author="jerryshao" created="Fri, 27 Nov 2015 03:30:25 +0000"  >&lt;p&gt;So what actual version of Spark you&apos;re running? 1.4.0 or 1.5.2?&lt;/p&gt;</comment>
                            <comment id="15029425" author="jerryshao" created="Fri, 27 Nov 2015 03:41:49 +0000"  >&lt;p&gt;So I guess your problem is that after you call &lt;tt&gt;sc.stop()&lt;/tt&gt; in your code, your still need to do some other things, so your program is not exited. In such situation, AM will re-allocate containers even your spark context is stopped, right?&lt;/p&gt;</comment>
                            <comment id="15029427" author="suyan" created="Fri, 27 Nov 2015 03:43:41 +0000"  >&lt;p&gt;run on the spark 1.4.0, and check current 1.5.2, that problem still exist,&lt;/p&gt;

&lt;p&gt;assume user call sc.stop in main(), now the userThread is still running on ApplicationMaster, right&#65311;&lt;br/&gt;
then sc.stop let DAGScheduler.stop -&amp;gt; taskScheduleImpl.stop -&amp;gt; schedulerBackend.stop -&amp;gt; stopExecutors&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;override def stop() {
    stopExecutors()
    &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (driverEndpoint != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
        driverEndpoint.askWithRetry[&lt;span class=&quot;code-object&quot;&gt;Boolean&lt;/span&gt;](StopDriver)
      }
    } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; {
      &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; e: Exception =&amp;gt;
        &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; SparkException(&lt;span class=&quot;code-quote&quot;&gt;&quot;Error stopping standalone scheduler&apos;s driver endpoint&quot;&lt;/span&gt;, e)
    }
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and ApplciationMaster still not mark userApp is finished&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt; 
 &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; (!finished) {
          &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
            &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (allocator.getNumExecutorsFailed &amp;gt;= maxNumExecutorFailures) {
              finish(FinalApplicationStatus.FAILED,
                ApplicationMaster.EXIT_MAX_EXECUTOR_FAILURES,
                s&lt;span class=&quot;code-quote&quot;&gt;&quot;Max number of executor failures ($maxNumExecutorFailures) reached&quot;&lt;/span&gt;)
            } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
              logDebug(&lt;span class=&quot;code-quote&quot;&gt;&quot;Sending progress&quot;&lt;/span&gt;)
              allocator.allocateResources()
            }
            failureCount = 0
          } 

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;btw, I find in branch 1.5.2, the log still &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;   override def onDisconnected(remoteAddress: RpcAddress): Unit = {
      logInfo(s&lt;span class=&quot;code-quote&quot;&gt;&quot;Driver terminated or disconnected! Shutting down. $remoteAddress&quot;&lt;/span&gt;)
      &lt;span class=&quot;code-comment&quot;&gt;// In cluster mode, &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; not rely on the disassociated event to exit
&lt;/span&gt;      &lt;span class=&quot;code-comment&quot;&gt;// This avoids potentially reporting incorrect exit codes &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the driver fails
&lt;/span&gt;      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!isClusterMode) {
        finish(FinalApplicationStatus.SUCCEEDED, ApplicationMaster.EXIT_SUCCESS)
      }
    }
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15029431" author="jerryshao" created="Fri, 27 Nov 2015 03:49:12 +0000"  >&lt;p&gt;Alright, my code is master branch. Anyway I understood your issue now. But I don&apos;t think current fix is a solid fix, it would be better to fix in the AM. I think what you need to do is to interrupt the &lt;tt&gt;reportThread&lt;/tt&gt; in AM in &lt;tt&gt;onDisconnected&lt;/tt&gt;, so that YarnAllocator will not sync with RM to request new containers. You could take a try.&lt;/p&gt;</comment>
                            <comment id="15031032" author="apachespark" created="Sun, 29 Nov 2015 15:59:13 +0000"  >&lt;p&gt;User &apos;lianhuiwang&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/10031&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/10031&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15031283" author="suyan" created="Mon, 30 Nov 2015 03:18:36 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jerryshao&quot; class=&quot;user-hover&quot; rel=&quot;jerryshao&quot;&gt;jerryshao&lt;/a&gt;&lt;br/&gt;
I would take some time to look into if Yarn lost heart beat from AM, which action will take...&lt;/p&gt;</comment>
                            <comment id="15031297" author="suyan" created="Mon, 30 Nov 2015 03:49:43 +0000"  >&lt;p&gt;default time is 10 min...after 10 min, yarn will mark AM as expired, and do some cleanup work...&lt;/p&gt;

&lt;p&gt;eh... in general, 10 min is enough... but may have some special situation... like, may user do some driver-local work after sc.stop ?&lt;/p&gt;</comment>
                            <comment id="15031298" author="suyan" created="Mon, 30 Nov 2015 03:50:25 +0000"  >&lt;p&gt;I still think it is better to only stop to request new containers, and try to find a more general way&lt;/p&gt;</comment>
                            <comment id="15031308" author="jerryshao" created="Mon, 30 Nov 2015 04:34:47 +0000"  >&lt;p&gt;OK, I see. So how about setting &lt;tt&gt;YarnAllocator#targetNumExecutors&lt;/tt&gt; to 0 when `sc.stop()` is called. So AM has no chance to scale up the containers.&lt;/p&gt;</comment>
                            <comment id="15902634" author="swaranga" created="Thu, 9 Mar 2017 07:58:59 +0000"  >&lt;p&gt;The JIRA says that the issue is fixed but I still see this error in Spark 2.1.0&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; (JavaSparkContext sc = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; JavaSparkContext(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; SparkConf())) {
  &lt;span class=&quot;code-comment&quot;&gt;//run the job
&lt;/span&gt;}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15994199" author="ethanyxu" created="Wed, 3 May 2017 02:55:29 +0000"  >&lt;p&gt;I&apos;m getting similar error message in with Spark 2.1.0. I can&apos;t reproduce it. The exact same code worked fine on a small RDD (sample), but sometimes gave this error on large RDD after hours of ran. It&apos;s very frustrating. &lt;/p&gt;</comment>
                            <comment id="16045364" author="htmljsp" created="Sat, 10 Jun 2017 03:50:27 +0000"  >&lt;p&gt;AsynchronousListenerBus  class&lt;/p&gt;

&lt;p&gt;def post(event: E) {&lt;br/&gt;
    if (stopped.get) &lt;/p&gt;
{
      // Drop further events to make `listenerThread` exit ASAP
      logError(s&quot;$name has already stopped! Dropping event $event&quot;)
      return
    }
&lt;p&gt;    val eventAdded = eventQueue.offer(event)&lt;br/&gt;
    if (eventAdded) &lt;/p&gt;
{
      eventLock.release()
    }
&lt;p&gt; else &lt;/p&gt;
{
      onDropEvent(event)
    }
&lt;p&gt;  }&lt;/p&gt;

&lt;p&gt;I think &quot;*  has already stopped! Dropping event *&quot; is a success finished event&#65292;before sent the event to the queue,  and the log use the logError(s&quot;$name has already stopped! Dropping event $event&quot;).&lt;/p&gt;</comment>
                            <comment id="16051607" author="ogcheeze" created="Fri, 16 Jun 2017 08:49:32 +0000"  >&lt;p&gt;I&apos;m having the same problem with Spark 2.1.0&lt;br/&gt;
I have some jobs with exact same code and had a few jobs failed.&lt;br/&gt;
In the jobs that finished successfully, there was this message after the job finished:&lt;/p&gt;

&lt;p&gt;17/06/15 00:26:02 INFO YarnAllocator: Driver requested a total number of 0 executor(s).&lt;/p&gt;

&lt;p&gt;But in the jobs that failed, there was this message instead:&lt;/p&gt;

&lt;p&gt;17/06/16 14:31:14 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(10,WrappedArray())&lt;/p&gt;

&lt;p&gt;I&apos;m guessing the YarnAllocator must have requested some executors after spark job was finished, but can&apos;t&apos; find out why.&lt;br/&gt;
and why is YarnAllocator requesting executors after job finished???? Does anyone know why??&lt;/p&gt;</comment>
                            <comment id="16774711" author="vvenkatasubbu" created="Fri, 22 Feb 2019 03:09:59 +0000"  >&lt;p&gt;HI &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=SuYan&quot; class=&quot;user-hover&quot; rel=&quot;SuYan&quot;&gt;SuYan&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Can someone reopen this ticket? I am facing the same issue now.&lt;/p&gt;
&lt;div class=&quot;panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;panelHeader&quot; style=&quot;border-bottom-width: 1px;&quot;&gt;&lt;b&gt;ErrorLog&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;panelContent&quot;&gt;
&lt;p&gt;19/02/21 11:47:23 INFO scheduler.DAGScheduler: Job 6 failed: saveAsTextFile at XmlFile.scala:139, took 268.146916 s&lt;br/&gt;
19/02/21 11:47:23 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (rdd at XmlFile.scala:89) failed in 268.129 s due to Stage cancelled because SparkContext was shut down&lt;br/&gt;
19/02/21 11:47:23 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerStageCompleted(org.apache.spark.scheduler.StageInfo@4887d60b)&lt;br/&gt;
19/02/21 11:47:23 INFO scheduler.DAGScheduler: ShuffleMapStage 10 (rdd at XmlFile.scala:89) failed in 268.148 s due to Stage cancelled because SparkContext was shut down&lt;br/&gt;
19/02/21 11:47:23 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerStageCompleted(org.apache.spark.scheduler.StageInfo@10aecbf7)&lt;br/&gt;
19/02/21 11:47:23 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerJobEnd(6,1550767643703,JobFailed(org.apache.spark.SparkException: Job 6 cancelled because SparkContext was shut down))&lt;br/&gt;
org.apache.spark.SparkException: Job 6 cancelled because SparkContext was shut down&lt;br/&gt;
at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820)&lt;br/&gt;
at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818)&lt;br/&gt;
at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)&lt;br/&gt;
at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818)&lt;br/&gt;
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1732)&lt;br/&gt;
at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83)&lt;br/&gt;
at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1651)&lt;br/&gt;
at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1921)&lt;br/&gt;
at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317)&lt;br/&gt;
at org.apache.spark.SparkContext.stop(SparkContext.scala:1920)&lt;br/&gt;
at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:581)&lt;br/&gt;
at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)&lt;br/&gt;
at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)&lt;br/&gt;
at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)&lt;br/&gt;
at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)&lt;br/&gt;
at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)&lt;br/&gt;
at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)&lt;br/&gt;
at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)&lt;br/&gt;
at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)&lt;br/&gt;
at scala.util.Try$.apply(Try.scala:192)&lt;br/&gt;
at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)&lt;br/&gt;
at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)&lt;br/&gt;
at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)&lt;br/&gt;
at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)&lt;br/&gt;
at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)&lt;br/&gt;
at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)&lt;br/&gt;
at org.apache.spark.SparkContext.runJob(SparkContext.scala:2075)&lt;br/&gt;
at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1151)&lt;br/&gt;
at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1096)&lt;br/&gt;
at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1096)&lt;br/&gt;
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;br/&gt;
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)&lt;br/&gt;
at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)&lt;br/&gt;
at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1096)&lt;br/&gt;
at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1070)&lt;br/&gt;
at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035)&lt;br/&gt;
at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035)&lt;br/&gt;
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;br/&gt;
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)&lt;br/&gt;
at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)&lt;br/&gt;
at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035)&lt;br/&gt;
at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:961)&lt;br/&gt;
at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:961)&lt;br/&gt;
at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:961)&lt;br/&gt;
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;br/&gt;
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)&lt;br/&gt;
at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)&lt;br/&gt;
at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:960)&lt;br/&gt;
at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1489)&lt;br/&gt;
at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1468)&lt;br/&gt;
at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1468)&lt;br/&gt;
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;br/&gt;
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)&lt;br/&gt;
at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)&lt;br/&gt;
at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1468)&lt;br/&gt;
at com.databricks.spark.xml.util.XmlFile$.saveAsXmlFile(XmlFile.scala:139)&lt;br/&gt;
at com.databricks.spark.xml.DefaultSource.createRelation(DefaultSource.scala:92)&lt;br/&gt;
at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:472)&lt;br/&gt;
at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)&lt;br/&gt;
at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)&lt;br/&gt;
at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)&lt;br/&gt;
at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)&lt;br/&gt;
at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)&lt;br/&gt;
at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)&lt;br/&gt;
at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)&lt;br/&gt;
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;br/&gt;
at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)&lt;br/&gt;
at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)&lt;br/&gt;
at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)&lt;br/&gt;
at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)&lt;br/&gt;
at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:610)&lt;br/&gt;
at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)&lt;br/&gt;
at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)&lt;br/&gt;
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&lt;br/&gt;
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
at java.lang.reflect.Method.invoke(Method.java:498)&lt;br/&gt;
at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755)&lt;br/&gt;
at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)&lt;br/&gt;
at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)&lt;br/&gt;
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)&lt;br/&gt;
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Please let us know, how resolve this issue?&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 38 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2oyr3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>