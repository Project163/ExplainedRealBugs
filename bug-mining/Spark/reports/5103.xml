<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:55:11 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-18394] Executing the same query twice in a row results in CodeGenerator cache misses</title>
                <link>https://issues.apache.org/jira/browse/SPARK-18394</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Executing the query:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;select
    l_returnflag,
    l_linestatus,
    sum(l_quantity) as sum_qty,
    sum(l_extendedprice) as sum_base_price,
    sum(l_extendedprice * (1 - l_discount)) as sum_disc_price,
    sum(l_extendedprice * (1 - l_discount) * (1 + l_tax)) as sum_charge,
    avg(l_quantity) as avg_qty,
    avg(l_extendedprice) as avg_price,
    avg(l_discount) as avg_disc,
    count(*) as count_order
from
    lineitem_1_row
where
    l_shipdate &amp;lt;= date_sub(&apos;1998-12-01&apos;, &apos;90&apos;)
group by
    l_returnflag,
    l_linestatus
;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;twice (in succession), will result in CodeGenerator cache misses in BOTH executions. Since the query is identical, I would expect the same code to be generated. &lt;/p&gt;

&lt;p&gt;Turns out, the generated code is not exactly the same, resulting in cache misses when performing the lookup in the CodeGenerator cache. Yet, the code is equivalent. &lt;/p&gt;

&lt;p&gt;Below is (some portion of the) generated code for two runs of the query:&lt;/p&gt;

&lt;p&gt;run-1&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;import java.nio.ByteBuffer;
import java.nio.ByteOrder;
import scala.collection.Iterator;
import org.apache.spark.sql.types.DataType;
import org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder;
import org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter;
import org.apache.spark.sql.execution.columnar.MutableUnsafeRow;

public SpecificColumnarIterator generate(Object[] references) {
return new SpecificColumnarIterator();
}

class SpecificColumnarIterator extends org.apache.spark.sql.execution.columnar.ColumnarIterator {

private ByteOrder nativeOrder = null;
private byte[][] buffers = null;
private UnsafeRow unsafeRow = new UnsafeRow(7);
private BufferHolder bufferHolder = new BufferHolder(unsafeRow);
private UnsafeRowWriter rowWriter = new UnsafeRowWriter(bufferHolder, 7);
private MutableUnsafeRow mutableRow = null;

private int currentRow = 0;
private int numRowsInBatch = 0;

private scala.collection.Iterator input = null;
private DataType[] columnTypes = null;
private int[] columnIndexes = null;

private org.apache.spark.sql.execution.columnar.DoubleColumnAccessor accessor;
private org.apache.spark.sql.execution.columnar.DoubleColumnAccessor accessor1;
private org.apache.spark.sql.execution.columnar.DoubleColumnAccessor accessor2;
private org.apache.spark.sql.execution.columnar.StringColumnAccessor accessor3;
private org.apache.spark.sql.execution.columnar.DoubleColumnAccessor accessor4;
private org.apache.spark.sql.execution.columnar.StringColumnAccessor accessor5;
private org.apache.spark.sql.execution.columnar.StringColumnAccessor accessor6;

public SpecificColumnarIterator() {
this.nativeOrder = ByteOrder.nativeOrder();
this.buffers = new byte[7][];
this.mutableRow = new MutableUnsafeRow(rowWriter);
}

public void initialize(Iterator input, DataType[] columnTypes, int[] columnIndexes) {
this.input = input;
this.columnTypes = columnTypes;
this.columnIndexes = columnIndexes;
}



public boolean hasNext() {
if (currentRow &amp;lt; numRowsInBatch) {
return true;
}
if (!input.hasNext()) {
return false;
}

org.apache.spark.sql.execution.columnar.CachedBatch batch = (org.apache.spark.sql.execution.columnar.CachedBatch) input.next();
currentRow = 0;
numRowsInBatch = batch.numRows();
for (int i = 0; i &amp;lt; columnIndexes.length; i ++) {
buffers[i] = batch.buffers()[columnIndexes[i]];
}
accessor = new org.apache.spark.sql.execution.columnar.DoubleColumnAccessor(ByteBuffer.wrap(buffers[0]).order(nativeOrder));
accessor1 = new org.apache.spark.sql.execution.columnar.DoubleColumnAccessor(ByteBuffer.wrap(buffers[1]).order(nativeOrder));
accessor2 = new org.apache.spark.sql.execution.columnar.DoubleColumnAccessor(ByteBuffer.wrap(buffers[2]).order(nativeOrder));
accessor3 = new org.apache.spark.sql.execution.columnar.StringColumnAccessor(ByteBuffer.wrap(buffers[3]).order(nativeOrder));
accessor4 = new org.apache.spark.sql.execution.columnar.DoubleColumnAccessor(ByteBuffer.wrap(buffers[4]).order(nativeOrder));
accessor5 = new org.apache.spark.sql.execution.columnar.StringColumnAccessor(ByteBuffer.wrap(buffers[5]).order(nativeOrder));
accessor6 = new org.apache.spark.sql.execution.columnar.StringColumnAccessor(ByteBuffer.wrap(buffers[6]).order(nativeOrder));

return hasNext();
}

public InternalRow next() {
currentRow += 1;
bufferHolder.reset();
rowWriter.zeroOutNullBytes();
accessor.extractTo(mutableRow, 0);
accessor1.extractTo(mutableRow, 1);
accessor2.extractTo(mutableRow, 2);
accessor3.extractTo(mutableRow, 3);
accessor4.extractTo(mutableRow, 4);
accessor5.extractTo(mutableRow, 5);
accessor6.extractTo(mutableRow, 6);
unsafeRow.setTotalSize(bufferHolder.totalSize());
return unsafeRow;
}
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;run-2:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;import java.nio.ByteBuffer;
import java.nio.ByteOrder;
import scala.collection.Iterator;
import org.apache.spark.sql.types.DataType;
import org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder;
import org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter;
import org.apache.spark.sql.execution.columnar.MutableUnsafeRow;

public SpecificColumnarIterator generate(Object[] references) {
return new SpecificColumnarIterator();
}

class SpecificColumnarIterator extends org.apache.spark.sql.execution.columnar.ColumnarIterator {

private ByteOrder nativeOrder = null;
private byte[][] buffers = null;
private UnsafeRow unsafeRow = new UnsafeRow(7);
private BufferHolder bufferHolder = new BufferHolder(unsafeRow);
private UnsafeRowWriter rowWriter = new UnsafeRowWriter(bufferHolder, 7);
private MutableUnsafeRow mutableRow = null;

private int currentRow = 0;
private int numRowsInBatch = 0;

private scala.collection.Iterator input = null;
private DataType[] columnTypes = null;
private int[] columnIndexes = null;

private org.apache.spark.sql.execution.columnar.DoubleColumnAccessor accessor;
private org.apache.spark.sql.execution.columnar.StringColumnAccessor accessor1;
private org.apache.spark.sql.execution.columnar.DoubleColumnAccessor accessor2;
private org.apache.spark.sql.execution.columnar.StringColumnAccessor accessor3;
private org.apache.spark.sql.execution.columnar.DoubleColumnAccessor accessor4;
private org.apache.spark.sql.execution.columnar.StringColumnAccessor accessor5;
private org.apache.spark.sql.execution.columnar.DoubleColumnAccessor accessor6;

public SpecificColumnarIterator() {
this.nativeOrder = ByteOrder.nativeOrder();
this.buffers = new byte[7][];
this.mutableRow = new MutableUnsafeRow(rowWriter);
}

public void initialize(Iterator input, DataType[] columnTypes, int[] columnIndexes) {
this.input = input;
this.columnTypes = columnTypes;
this.columnIndexes = columnIndexes;
}



public boolean hasNext() {
if (currentRow &amp;lt; numRowsInBatch) {
return true;
}
if (!input.hasNext()) {
return false;
}

org.apache.spark.sql.execution.columnar.CachedBatch batch = (org.apache.spark.sql.execution.columnar.CachedBatch) input.next();
currentRow = 0;
numRowsInBatch = batch.numRows();
for (int i = 0; i &amp;lt; columnIndexes.length; i ++) {
buffers[i] = batch.buffers()[columnIndexes[i]];
}
accessor = new org.apache.spark.sql.execution.columnar.DoubleColumnAccessor(ByteBuffer.wrap(buffers[0]).order(nativeOrder));
accessor1 = new org.apache.spark.sql.execution.columnar.StringColumnAccessor(ByteBuffer.wrap(buffers[1]).order(nativeOrder));
accessor2 = new org.apache.spark.sql.execution.columnar.DoubleColumnAccessor(ByteBuffer.wrap(buffers[2]).order(nativeOrder));
accessor3 = new org.apache.spark.sql.execution.columnar.StringColumnAccessor(ByteBuffer.wrap(buffers[3]).order(nativeOrder));
accessor4 = new org.apache.spark.sql.execution.columnar.DoubleColumnAccessor(ByteBuffer.wrap(buffers[4]).order(nativeOrder));
accessor5 = new org.apache.spark.sql.execution.columnar.StringColumnAccessor(ByteBuffer.wrap(buffers[5]).order(nativeOrder));
accessor6 = new org.apache.spark.sql.execution.columnar.DoubleColumnAccessor(ByteBuffer.wrap(buffers[6]).order(nativeOrder));

return hasNext();
}

public InternalRow next() {
currentRow += 1;
bufferHolder.reset();
rowWriter.zeroOutNullBytes();
accessor.extractTo(mutableRow, 0);
accessor1.extractTo(mutableRow, 1);
accessor2.extractTo(mutableRow, 2);
accessor3.extractTo(mutableRow, 3);
accessor4.extractTo(mutableRow, 4);
accessor5.extractTo(mutableRow, 5);
accessor6.extractTo(mutableRow, 6);
unsafeRow.setTotalSize(bufferHolder.totalSize());
return unsafeRow;
}
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Diff-ing the two files reveals that the &quot;accessor*&quot; variable definitions are permuted. &lt;/p&gt;</description>
                <environment>&lt;p&gt;HiveThriftServer2 running on branch-2.0 on Mac laptop&lt;/p&gt;</environment>
        <key id="13019715">SPARK-18394</key>
            <summary>Executing the same query twice in a row results in CodeGenerator cache misses</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="maropu">Takeshi Yamamuro</assignee>
                                    <reporter username="jwserencsa">Jonny Serencsa</reporter>
                        <labels>
                    </labels>
                <created>Thu, 10 Nov 2016 01:20:41 +0000</created>
                <updated>Sun, 29 Oct 2017 04:25:42 +0000</updated>
                            <resolved>Thu, 17 Aug 2017 20:48:07 +0000</resolved>
                                                    <fixVersion>2.3.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>8</watches>
                                                                                                                <comments>
                            <comment id="15688059" author="hvanhovell" created="Tue, 22 Nov 2016 22:06:59 +0000"  >&lt;p&gt;I am not able to reproduce this. Could you also explain to me why this is a major issue?&lt;/p&gt;

&lt;p&gt;I have used the following script:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;sc.setLogLevel(&quot;INFO&quot;)
spark.sql(&quot;create database if not exists tpc&quot;)
spark.sql(&quot;drop table if exists tpc.lineitem&quot;)
spark.sql(&quot;&quot;&quot;
create table tpc.lineitem (
  L_ORDERKEY bigint,
  L_PARTKEY bigint,
  L_SUPPKEY bigint,
  L_LINENUMBER bigint,
  L_QUANTITY double,
  L_EXTENDEDPRICE double,
  L_DISCOUNT double,
  L_TAX double,
  L_RETURNFLAG string,
  L_LINESTATUS string,
  L_SHIPDATE string,
  L_COMMITDATE string,
  L_RECEIPTDATE string,
  L_SHIPINSTRUCT string,
  L_SHIPMODE string,
  L_COMMENT string
) using parquet
&quot;&quot;&quot;)

spark.sql(s&quot;&quot;&quot;
insert into tpc.lineitem
select id as L_ORDERKEY,
       id % 10 as L_PARTKEY,
       id % 50 as L_SUPPKEY,
       id as L_LINENUMBER,
       rand(3) * 10 as L_QUANTITY,
       rand(5) * 50 as L_EXTENDEDPRICE,
       rand(7) * 20 as L_DISCOUNT,
       0.18d as L_TAX,
       case when rand(11) &amp;lt; 0.7d then &apos;Y&apos; else &apos;N&apos; end as L_RETURNFLAG,
       case when rand(13) &amp;lt; 0.4d then &apos;A&apos; when rand(17) &amp;lt; 0.2d then &apos;B&apos; else &apos;C&apos; end as L_LINESTATUS,
       date_format(date_add(date &apos;1998-08-05&apos;, id % 365), &apos;yyyy-MM-dd&apos;) as L_SHIPDATE,
       date_format(date_add(date &apos;1998-08-01&apos;, id % 365), &apos;yyyy-MM-dd&apos;) as L_COMMITDATE,
       date_format(date_add(date &apos;1998-08-03&apos;, id % 365), &apos;yyyy-MM-dd&apos;) as L_RECEIPTDATE,
       &apos;DUMMY&apos; as L_SHIPINSTRUCT,
       case when rand(19) &amp;lt; 0.7d then &apos;AIR&apos; else &apos;LAND&apos; end as L_SHIPMODE,
       &apos;DUMMY&apos; as L_COMMENT
from   range(100)
&quot;&quot;&quot;)


val df = spark.sql(&quot;&quot;&quot;
select
    l_returnflag,
    l_linestatus,
    sum(l_quantity) as sum_qty,
    sum(l_extendedprice) as sum_base_price,
    sum(l_extendedprice * (1 - l_discount)) as sum_disc_price,
    sum(l_extendedprice * (1 - l_discount) * (1 + l_tax)) as sum_charge,
    avg(l_quantity) as avg_qty,
    avg(l_extendedprice) as avg_price,
    avg(l_discount) as avg_disc,
    count(*) as count_order
from
    tpc.lineitem
where
    l_shipdate &amp;lt;= date_sub(&apos;1998-12-01&apos;, &apos;90&apos;)
group by
    l_returnflag,
    l_linestatus
&quot;&quot;&quot;)

df.show()

df.show()
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
</comment>
                            <comment id="15688135" author="jwserencsa" created="Tue, 22 Nov 2016 22:32:09 +0000"  >&lt;p&gt;This problem was discovered during high concurrency experiments where I was running the aforementioned query thousands of times repeatedly via many concurrent clients. Eventually, after 5K-10K executions, the JVM level CodeGenCache was having to be purged resulting in 10 second long pauses.&lt;/p&gt;

&lt;p&gt;My expectation was that since the exact same query is being executed, Spark would not have to re-generate the byte code (because of it&apos;s own CodeGenCache). After removing the WHERE clauses from the query, this was in fact the case and the JVM level cache purging disappeared.  &lt;/p&gt;

&lt;p&gt;I made this a Major issue because it didn&apos;t seem like Spark&apos;s CodeGenCache was working as expected. Perhaps I am mistaken. &lt;/p&gt;

&lt;p&gt;My above repro of the issue required me to set breakpoints through the debugger. &lt;/p&gt;
</comment>
                            <comment id="15688225" author="hvanhovell" created="Tue, 22 Nov 2016 23:04:48 +0000"  >&lt;p&gt;Ok, that is fair.&lt;/p&gt;

&lt;p&gt;What strikes me as odd is that the column order that the columnar cache produces is different the two both plans. This is what causes the code generator to create two different &apos;programs&apos; and what in the end causes the your caching problems . Could you re-run this without the in-memory cache, and see if you are still hitting this problem.&lt;/p&gt;

&lt;p&gt;I&apos;ll have a look on my end to see what is going on in the in-memory cache.&lt;/p&gt;</comment>
                            <comment id="15688244" author="jwserencsa" created="Tue, 22 Nov 2016 23:12:16 +0000"  >&lt;p&gt;Already did that. The problem happens with both HiveScanExec and InMemoryScanExec. The indeterminate ordering is an artifact of the hash code for AttributeEquals  involving a hash code of the exprId. Thus, when you iterate through an AttributeSet, the order or the attributes is not consistent. &lt;/p&gt;</comment>
                            <comment id="15688330" author="hvanhovell" created="Tue, 22 Nov 2016 23:49:10 +0000"  >&lt;p&gt;Nice, this is a good find.&lt;/p&gt;

&lt;p&gt;I think we either need to make AttributeSet iteration deterministic, or make a change to the following line in the SparkPlanner: &lt;a href=&quot;https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala#L96&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala#L96&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Would you be interested in working on this? &lt;/p&gt;
</comment>
                            <comment id="15688333" author="jwserencsa" created="Tue, 22 Nov 2016 23:51:16 +0000"  >&lt;p&gt;Yes I would. &lt;/p&gt;</comment>
                            <comment id="15688345" author="hvanhovell" created="Tue, 22 Nov 2016 23:56:49 +0000"  >&lt;p&gt;Great! Ping me if you need any assistance.&lt;/p&gt;</comment>
                            <comment id="16128243" author="maropu" created="Wed, 16 Aug 2017 03:16:12 +0000"  >&lt;p&gt;Any update? I checked and I found the master still has this issue; I just run the query above and dump output names in &lt;a href=&quot;https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala#L102&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala#L102&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;17/08/16 02:13:13 WARN BaseSessionStateBuilder$$anon$3: L_QUANTITY#9015,L_RETURNFLAG#9019,l_shipdate#9021,L_TAX#9018,L_DISCOUNT#9017,L_LINESTATUS#9020,L_EXTENDEDPRICE#9016
17/08/16 02:13:13 WARN BaseSessionStateBuilder$$anon$3: L_RETURNFLAG#9142,L_DISCOUNT#9140,L_EXTENDEDPRICE#9139,L_QUANTITY#9138,L_LINESTATUS#9143,l_shipdate#9144,L_TAX#9141
17/08/16 02:13:13 WARN BaseSessionStateBuilder$$anon$3: L_QUANTITY#9305,L_TAX#9308,l_shipdate#9311,L_DISCOUNT#9307,L_RETURNFLAG#9309,L_LINESTATUS#9310,L_EXTENDEDPRICE#9306
17/08/16 02:13:13 WARN BaseSessionStateBuilder$$anon$3: L_EXTENDEDPRICE#9451,L_QUANTITY#9450,L_RETURNFLAG#9454,L_TAX#9453,L_DISCOUNT#9452,l_shipdate#9456,L_LINESTATUS#9455
17/08/16 02:13:13 WARN BaseSessionStateBuilder$$anon$3: L_LINESTATUS#9600,l_shipdate#9601,L_DISCOUNT#9597,L_TAX#9598,L_EXTENDEDPRICE#9596,L_RETURNFLAG#9599,L_QUANTITY#9595
17/08/16 02:13:13 WARN BaseSessionStateBuilder$$anon$3: L_QUANTITY#9740,L_TAX#9743,l_shipdate#9746,L_DISCOUNT#9742,L_EXTENDEDPRICE#9741,L_LINESTATUS#9745,L_RETURNFLAG#9744
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The attribute order is different, and then Spark generates different  code in `GenerateColumnAccessor`.&lt;br/&gt;
Also, I quickly checked `AttributeSet.toSeq` output attributes with a different order;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;scala&amp;gt; val attr1 = AttributeReference(&lt;span class=&quot;code-quote&quot;&gt;&quot;c1&quot;&lt;/span&gt;, IntegerType)(exprId = ExprId(1098))
scala&amp;gt; val attr2 = AttributeReference(&lt;span class=&quot;code-quote&quot;&gt;&quot;c2&quot;&lt;/span&gt;, IntegerType)(exprId = ExprId(107))
scala&amp;gt; val attr3 = AttributeReference(&lt;span class=&quot;code-quote&quot;&gt;&quot;c3&quot;&lt;/span&gt;, IntegerType)(exprId = ExprId(838))
scala&amp;gt; val attrSetA = AttributeSet(attr1 :: attr2 :: attr3 :: Nil)
scala&amp;gt; val attr4 = AttributeReference(&lt;span class=&quot;code-quote&quot;&gt;&quot;c4&quot;&lt;/span&gt;, IntegerType)(exprId = ExprId(389))
scala&amp;gt; val attr5 = AttributeReference(&lt;span class=&quot;code-quote&quot;&gt;&quot;c5&quot;&lt;/span&gt;, IntegerType)(exprId = ExprId(89329))
scala&amp;gt; val attrSetB = AttributeSet(attr4 :: attr5 :: Nil)
scala&amp;gt; (attrSetA ++ attrSetB).toSeq
res1: Seq[org.apache.spark.sql.catalyst.expressions.Attribute] = WrappedArray(c3#838, c4#389, c2#107, c5#89329, c1#1098)

scala&amp;gt; val attr1 = AttributeReference(&lt;span class=&quot;code-quote&quot;&gt;&quot;c1&quot;&lt;/span&gt;, IntegerType)(exprId = ExprId(392))
scala&amp;gt; val attr2 = AttributeReference(&lt;span class=&quot;code-quote&quot;&gt;&quot;c2&quot;&lt;/span&gt;, IntegerType)(exprId = ExprId(92))
scala&amp;gt; val attr3 = AttributeReference(&lt;span class=&quot;code-quote&quot;&gt;&quot;c3&quot;&lt;/span&gt;, IntegerType)(exprId = ExprId(87))
scala&amp;gt; val attrSetA = AttributeSet(attr1 :: attr2 :: attr3 :: Nil)
scala&amp;gt; val attr4 = AttributeReference(&lt;span class=&quot;code-quote&quot;&gt;&quot;c4&quot;&lt;/span&gt;, IntegerType)(exprId = ExprId(9023920))
scala&amp;gt; val attr5 = AttributeReference(&lt;span class=&quot;code-quote&quot;&gt;&quot;c5&quot;&lt;/span&gt;, IntegerType)(exprId = ExprId(522))
scala&amp;gt; val attrSetB = AttributeSet(attr4 :: attr5 :: Nil)
scala&amp;gt; (attrSetA ++ attrSetB).toSeq
res2: Seq[org.apache.spark.sql.catalyst.expressions.Attribute] = WrappedArray(c3#87, c1#392, c5#522, c2#92, c4#9023920)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As suggested, to fix this, `Attribute.toSeq` need to output attributes with a consistent order like;&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/compare/master...maropu:SPARK-18394&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/compare/master...maropu:SPARK-18394&lt;/a&gt;&lt;/p&gt;
</comment>
                            <comment id="16223835" author="cloud_fan" created="Sun, 29 Oct 2017 04:25:42 +0000"  >&lt;p&gt;resolved by &lt;a href=&quot;https://github.com/apache/spark/pull/18959&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/18959&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 3 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i365lj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311620" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Shepherd</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>hvanhovell</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12339551">2.3.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>