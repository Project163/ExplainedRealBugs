<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 19:31:26 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-32380] sparksql cannot access hive table while data in hbase</title>
                <link>https://issues.apache.org/jira/browse/SPARK-32380</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;ul&gt;
	&lt;li&gt;step1: create hbase table&lt;/li&gt;
&lt;/ul&gt;


&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
 hbase(main):001:0&amp;gt;create &lt;span class=&quot;code-quote&quot;&gt;&apos;hbase_test1&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;cf1&apos;&lt;/span&gt;
 hbase(main):001:0&amp;gt; put &lt;span class=&quot;code-quote&quot;&gt;&apos;hbase_test&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;r1&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;cf1:c1&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;123&apos;&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
	&lt;li&gt;step2: create hive table related to hbase table&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&#160;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
hive&amp;gt; 
CREATE EXTERNAL TABLE `hivetest.hbase_test`(
  `key` string COMMENT &apos;&apos;, 
  `value` string COMMENT &apos;&apos;)
ROW FORMAT SERDE 
  &lt;span class=&quot;code-quote&quot;&gt;&apos;org.apache.hadoop.hive.hbase.HBaseSerDe&apos;&lt;/span&gt; 
STORED BY 
  &lt;span class=&quot;code-quote&quot;&gt;&apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&apos;&lt;/span&gt; 
WITH SERDEPROPERTIES ( 
  &lt;span class=&quot;code-quote&quot;&gt;&apos;hbase.columns.mapping&apos;&lt;/span&gt;=&lt;span class=&quot;code-quote&quot;&gt;&apos;:key,cf1:v1&apos;&lt;/span&gt;, 
  &lt;span class=&quot;code-quote&quot;&gt;&apos;serialization.format&apos;&lt;/span&gt;=&lt;span class=&quot;code-quote&quot;&gt;&apos;1&apos;&lt;/span&gt;)
TBLPROPERTIES (
  &lt;span class=&quot;code-quote&quot;&gt;&apos;hbase.table.name&apos;&lt;/span&gt;=&lt;span class=&quot;code-quote&quot;&gt;&apos;hbase_test&apos;&lt;/span&gt;)
&#160;&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
	&lt;li&gt;step3: sparksql query hive table while data in hbase&lt;/li&gt;
&lt;/ul&gt;


&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
spark-sql --master yarn -e &lt;span class=&quot;code-quote&quot;&gt;&quot;select * from hivetest.hbase_test&quot;&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;The error log as follow:&#160;&lt;/p&gt;

&lt;p&gt;java.io.IOException: Cannot create a record reader because of a previous error. Please look at the previous logs lines from the task&apos;s full log for more details.&lt;br/&gt;
 at org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.getSplits(TableInputFormatBase.java:270)&lt;br/&gt;
 at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:131)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)&lt;br/&gt;
 at scala.Option.getOrElse(Option.scala:189)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.partitions(RDD.scala:272)&lt;br/&gt;
 at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)&lt;br/&gt;
 at scala.Option.getOrElse(Option.scala:189)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.partitions(RDD.scala:272)&lt;br/&gt;
 at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)&lt;br/&gt;
 at scala.Option.getOrElse(Option.scala:189)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.partitions(RDD.scala:272)&lt;br/&gt;
 at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)&lt;br/&gt;
 at scala.Option.getOrElse(Option.scala:189)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.partitions(RDD.scala:272)&lt;br/&gt;
 at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)&lt;br/&gt;
 at scala.Option.getOrElse(Option.scala:189)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.partitions(RDD.scala:272)&lt;br/&gt;
 at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)&lt;br/&gt;
 at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;br/&gt;
 at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.withScope(RDD.scala:388)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.collect(RDD.scala:1003)&lt;br/&gt;
 at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:385)&lt;br/&gt;
 at org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:412)&lt;br/&gt;
 at org.apache.spark.sql.execution.HiveResult$.hiveResultString(HiveResult.scala:58)&lt;br/&gt;
 at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.$anonfun$run$1(SparkSQLDriver.scala:65)&lt;br/&gt;
 at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)&lt;br/&gt;
 at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)&lt;br/&gt;
 at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)&lt;br/&gt;
 at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)&lt;br/&gt;
 at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)&lt;br/&gt;
 at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:65)&lt;br/&gt;
 at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:377)&lt;br/&gt;
 at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.$anonfun$processLine$1(SparkSQLCLIDriver.scala:496)&lt;br/&gt;
 at scala.collection.Iterator.foreach(Iterator.scala:941)&lt;br/&gt;
 at scala.collection.Iterator.foreach$(Iterator.scala:941)&lt;br/&gt;
 at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)&lt;br/&gt;
 at scala.collection.IterableLike.foreach(IterableLike.scala:74)&lt;br/&gt;
 at scala.collection.IterableLike.foreach$(IterableLike.scala:73)&lt;br/&gt;
 at scala.collection.AbstractIterable.foreach(Iterable.scala:56)&lt;br/&gt;
 at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processLine(SparkSQLCLIDriver.scala:490)&lt;br/&gt;
 at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:336)&lt;br/&gt;
 at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:474)&lt;br/&gt;
 at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:490)&lt;br/&gt;
 at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:206)&lt;br/&gt;
 at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)&lt;br/&gt;
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&lt;br/&gt;
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
 at java.lang.reflect.Method.invoke(Method.java:498)&lt;br/&gt;
 at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)&lt;br/&gt;
 at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:928)&lt;br/&gt;
 at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)&lt;br/&gt;
 at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)&lt;br/&gt;
 at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)&lt;br/&gt;
 at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)&lt;br/&gt;
 at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)&lt;br/&gt;
 at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)&lt;br/&gt;
 Caused by: java.lang.IllegalStateException: The input format instance has not been properly initialized. Ensure you call initializeTable either in your constructor or initialize method&lt;br/&gt;
 at org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.getTable(TableInputFormatBase.java:652)&lt;br/&gt;
 at org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.getSplits(TableInputFormatBase.java:265)&lt;br/&gt;
 ... 62 more&lt;br/&gt;
 java.io.IOException: Cannot create a record reader because of a previous error. Please look at the previous logs lines from the task&apos;s full log for more details.&lt;br/&gt;
 at org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.getSplits(TableInputFormatBase.java:270)&lt;br/&gt;
 at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:131)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)&lt;br/&gt;
 at scala.Option.getOrElse(Option.scala:189)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.partitions(RDD.scala:272)&lt;br/&gt;
 at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)&lt;br/&gt;
 at scala.Option.getOrElse(Option.scala:189)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.partitions(RDD.scala:272)&lt;br/&gt;
 at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)&lt;br/&gt;
 at scala.Option.getOrElse(Option.scala:189)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.partitions(RDD.scala:272)&lt;br/&gt;
 at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)&lt;br/&gt;
 at scala.Option.getOrElse(Option.scala:189)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.partitions(RDD.scala:272)&lt;br/&gt;
 at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:276)&lt;br/&gt;
 at scala.Option.getOrElse(Option.scala:189)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.partitions(RDD.scala:272)&lt;br/&gt;
 at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)&lt;br/&gt;
 at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;br/&gt;
 at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.withScope(RDD.scala:388)&lt;br/&gt;
 at org.apache.spark.rdd.RDD.collect(RDD.scala:1003)&lt;br/&gt;
 at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:385)&lt;br/&gt;
 at org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:412)&lt;br/&gt;
 at org.apache.spark.sql.execution.HiveResult$.hiveResultString(HiveResult.scala:58)&lt;br/&gt;
 at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.$anonfun$run$1(SparkSQLDriver.scala:65)&lt;br/&gt;
 at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)&lt;br/&gt;
 at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)&lt;br/&gt;
 at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)&lt;br/&gt;
 at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)&lt;br/&gt;
 at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)&lt;br/&gt;
 at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:65)&lt;br/&gt;
 at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:377)&lt;br/&gt;
 at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.$anonfun$processLine$1(SparkSQLCLIDriver.scala:496)&lt;br/&gt;
 at scala.collection.Iterator.foreach(Iterator.scala:941)&lt;br/&gt;
 at scala.collection.Iterator.foreach$(Iterator.scala:941)&lt;br/&gt;
 at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)&lt;br/&gt;
 at scala.collection.IterableLike.foreach(IterableLike.scala:74)&lt;br/&gt;
 at scala.collection.IterableLike.foreach$(IterableLike.scala:73)&lt;br/&gt;
 at scala.collection.AbstractIterable.foreach(Iterable.scala:56)&lt;br/&gt;
 at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processLine(SparkSQLCLIDriver.scala:490)&lt;br/&gt;
 at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:336)&lt;br/&gt;
 at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:474)&lt;br/&gt;
 at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:490)&lt;br/&gt;
 at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:206)&lt;br/&gt;
 at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)&lt;br/&gt;
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&lt;br/&gt;
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
 at java.lang.reflect.Method.invoke(Method.java:498)&lt;br/&gt;
 at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)&lt;br/&gt;
 at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:928)&lt;br/&gt;
 at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)&lt;br/&gt;
 at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)&lt;br/&gt;
 at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)&lt;br/&gt;
 at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)&lt;br/&gt;
 at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)&lt;br/&gt;
 at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)&lt;br/&gt;
 Caused by: java.lang.IllegalStateException: The input format instance has not been properly initialized. Ensure you call initializeTable either in your constructor or initialize method&lt;br/&gt;
 at org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.getTable(TableInputFormatBase.java:652)&lt;br/&gt;
 at org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.getSplits(TableInputFormatBase.java:265)&lt;br/&gt;
 ... 62 more&lt;/p&gt;</description>
                <environment>&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;component&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;version&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;hadoop&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2.8.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;hive&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2.3.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;spark&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3.0.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;hbase&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1.4.9&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
</environment>
        <key id="13318475">SPARK-32380</key>
            <summary>sparksql cannot access hive table while data in hbase</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="attilapiros">Attila Zsolt Piros</assignee>
                                    <reporter username="meimile">deyzhong</reporter>
                        <labels>
                    </labels>
                <created>Tue, 21 Jul 2020 11:10:04 +0000</created>
                <updated>Mon, 12 Dec 2022 18:10:31 +0000</updated>
                            <resolved>Sat, 5 Nov 2022 12:31:17 +0000</resolved>
                                    <version>3.0.0</version>
                                    <fixVersion>3.2.3</fixVersion>
                    <fixVersion>3.3.2</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                    <workratio workratioPercent="0"/>
                                    <progress percentage="0">
                                    <originalProgress>
                                                    <row percentage="100" backgroundColor="#89afd7"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="0" backgroundColor="#51a825"/>
                                                    <row percentage="100" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </progress>
                                    <aggregateprogress percentage="0">
                                    <originalProgress>
                                                    <row percentage="100" backgroundColor="#89afd7"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="0" backgroundColor="#51a825"/>
                                                    <row percentage="100" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </aggregateprogress>
                                    <timeoriginalestimate seconds="259200">72h</timeoriginalestimate>
                            <timeestimate seconds="259200">72h</timeestimate>
                                        <comments>
                            <comment id="17161959" author="meimile" created="Tue, 21 Jul 2020 11:37:16 +0000"  >&lt;p&gt;I have solved this bug by modified&#160;TableReader.scala.&lt;/p&gt;

&lt;p&gt;The solution is when the inputformat class is&#160;org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat,&#160; will create&#160;OldHadoopRDD. I have tested in my product env as well.&lt;/p&gt;

&lt;p&gt;Can I submit a pr to spark ?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=apachespark&quot; class=&quot;user-hover&quot; rel=&quot;apachespark&quot;&gt;apachespark&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17162058" author="apachespark" created="Tue, 21 Jul 2020 13:58:32 +0000"  >&lt;p&gt;User &apos;DeyinZhong&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/29178&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/29178&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17162059" author="apachespark" created="Tue, 21 Jul 2020 13:58:53 +0000"  >&lt;p&gt;User &apos;DeyinZhong&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/29178&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/29178&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17263152" author="apachespark" created="Tue, 12 Jan 2021 08:28:31 +0000"  >&lt;p&gt;User &apos;yangBottle&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/31147&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/31147&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17263153" author="apachespark" created="Tue, 12 Jan 2021 08:29:31 +0000"  >&lt;p&gt;User &apos;yangBottle&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/31147&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/31147&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17329119" author="4u.premsagar" created="Thu, 22 Apr 2021 13:26:28 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=meimile&quot; class=&quot;user-hover&quot; rel=&quot;meimile&quot;&gt;meimile&lt;/a&gt;&#160;Will there be any permanent fix for this issues spark3 and HIve on top of Hbase tables access&lt;/p&gt;</comment>
                            <comment id="17622497" author="JIRAUSER297345" created="Fri, 21 Oct 2022 21:30:57 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=apachespark&quot; class=&quot;user-hover&quot; rel=&quot;apachespark&quot;&gt;apachespark&lt;/a&gt;&#160;&lt;/p&gt;

&lt;p&gt;Since changes were merged on Jan 11, 2021, I expect this issue to be fixed in version 3.0.2 but it is not fixed even in Spark 3.2.2.&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;The build.sbt looks as follows.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;val scalaVersion := &quot;2.12.3&quot;&lt;/p&gt;

&lt;p&gt;val sparkVersion = &quot;3.2.2&quot;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;lazy val root = (project in file(&quot;.*))&lt;/p&gt;

&lt;p&gt;&#160; &#160; .settings(&lt;/p&gt;

&lt;p&gt;&#160; &#160; libraryDependencies ++= Seq(&quot;org.apache.spark&quot; %% &quot;spark-core&quot; % sparkVersion % Provided,&lt;/p&gt;

&lt;p&gt;&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &quot;org.apache.spark&quot; %% &quot;spark-hive&quot; % sparkVersion % Provided,&lt;/p&gt;

&lt;p&gt;&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160;&quot;org.apache.spark&quot; %% &quot;spark-sql&quot; % sparkVersion % Provided)&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17627704" author="rangareddy.avula@gmail.com" created="Wed, 2 Nov 2022 13:27:59 +0000"  >&lt;p&gt;The below pull request will solve the issue but needs to check if there are any other issues.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/spark/pull/29178&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/29178&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17629239" author="apachespark" created="Sat, 5 Nov 2022 01:57:01 +0000"  >&lt;p&gt;User &apos;attilapiros&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/38516&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/38516&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17629321" author="gurwls223" created="Sat, 5 Nov 2022 12:31:17 +0000"  >&lt;p&gt;Fixed in &lt;a href=&quot;https://github.com/apache/spark/commit/7009ef0510dae444c72e7513357e681b08379603&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/commit/7009ef0510dae444c72e7513357e681b08379603&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="13354065">SPARK-34210</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            3 years, 1 week, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z0h1s8:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>