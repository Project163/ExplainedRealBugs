<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:56:51 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-22410] Excessive spill for Pyspark UDF when a row has shrunk</title>
                <link>https://issues.apache.org/jira/browse/SPARK-22410</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;The following code processes 900KB of data and outputs around 2MB of data. However, to process it, Spark needs to spill roughly 12 GB of data.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-python&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;from&lt;/span&gt; pyspark.sql &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; SparkSession
&lt;span class=&quot;code-keyword&quot;&gt;from&lt;/span&gt; pyspark.sql.functions &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; *
&lt;span class=&quot;code-keyword&quot;&gt;from&lt;/span&gt; pyspark.sql.types &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; *
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; json

ss = SparkSession.builder.getOrCreate()

&lt;span class=&quot;code-comment&quot;&gt;# Create a few lines of data (5 lines).
&lt;/span&gt;&lt;span class=&quot;code-comment&quot;&gt;# Each line &lt;span class=&quot;code-keyword&quot;&gt;is&lt;/span&gt; made of a string, &lt;span class=&quot;code-keyword&quot;&gt;and&lt;/span&gt; an array of 10000 strings
&lt;/span&gt;&lt;span class=&quot;code-comment&quot;&gt;# Total size of data &lt;span class=&quot;code-keyword&quot;&gt;is&lt;/span&gt; around 900 KB
&lt;/span&gt;
lines_of_file = [ &lt;span class=&quot;code-quote&quot;&gt;&quot;this &lt;span class=&quot;code-keyword&quot;&gt;is&lt;/span&gt; a line&quot;&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; x &lt;span class=&quot;code-keyword&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;xrange&lt;/span&gt;(10000) ]
file_obj = [ &lt;span class=&quot;code-quote&quot;&gt;&quot;this_is_a_foldername/this_is_a_filename&quot;&lt;/span&gt;, lines_of_file ]
data = [ file_obj &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; x &lt;span class=&quot;code-keyword&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;xrange&lt;/span&gt;(5) ]

&lt;span class=&quot;code-comment&quot;&gt;# Make a two-columns dataframe out of it
&lt;/span&gt;small_df = ss.sparkContext.parallelize(data).&lt;span class=&quot;code-object&quot;&gt;map&lt;/span&gt;(&lt;span class=&quot;code-keyword&quot;&gt;lambda&lt;/span&gt; x : (x[0], x[1])).toDF([&lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-object&quot;&gt;file&lt;/span&gt;&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;lines&quot;&lt;/span&gt;])

&lt;span class=&quot;code-comment&quot;&gt;# We then explode the array, so we now have 50000 rows &lt;span class=&quot;code-keyword&quot;&gt;in&lt;/span&gt; the dataframe, &lt;span class=&quot;code-keyword&quot;&gt;with&lt;/span&gt; 2 columns, the 2nd 
&lt;/span&gt;&lt;span class=&quot;code-comment&quot;&gt;# column now has only &lt;span class=&quot;code-quote&quot;&gt;&quot;this &lt;span class=&quot;code-keyword&quot;&gt;is&lt;/span&gt; a line&quot;&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;as&lt;/span&gt; content
&lt;/span&gt;exploded = small_df.select(&lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-object&quot;&gt;file&lt;/span&gt;&quot;&lt;/span&gt;, explode(&lt;span class=&quot;code-quote&quot;&gt;&quot;lines&quot;&lt;/span&gt;))

&lt;span class=&quot;code-object&quot;&gt;print&lt;/span&gt;(&lt;span class=&quot;code-quote&quot;&gt;&quot;Exploded&quot;&lt;/span&gt;)
&lt;span class=&quot;code-object&quot;&gt;print&lt;/span&gt;(exploded.explain())

&lt;span class=&quot;code-comment&quot;&gt;# Now, just process it &lt;span class=&quot;code-keyword&quot;&gt;with&lt;/span&gt; a trivial Pyspark UDF that touches the first column
&lt;/span&gt;&lt;span class=&quot;code-comment&quot;&gt;# (the one which was &lt;span class=&quot;code-keyword&quot;&gt;not&lt;/span&gt; an array)
&lt;/span&gt;
&lt;span class=&quot;code-keyword&quot;&gt;def&lt;/span&gt; split_key(s):
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; s.split(&lt;span class=&quot;code-quote&quot;&gt;&quot;/&quot;&lt;/span&gt;)[1]
split_key_udf = udf(split_key, StringType())

with_filename = exploded.withColumn(&lt;span class=&quot;code-quote&quot;&gt;&quot;filename&quot;&lt;/span&gt;, split_key_udf(&lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-object&quot;&gt;file&lt;/span&gt;&quot;&lt;/span&gt;))

&lt;span class=&quot;code-comment&quot;&gt;# As expected, explain plan &lt;span class=&quot;code-keyword&quot;&gt;is&lt;/span&gt; very simple (BatchEval -&amp;gt; Explode -&amp;gt; Project -&amp;gt; ScanExisting)
&lt;/span&gt;&lt;span class=&quot;code-object&quot;&gt;print&lt;/span&gt;(with_filename.explain())

&lt;span class=&quot;code-comment&quot;&gt;# Getting the head will spill around 12 GB of data
&lt;/span&gt;&lt;span class=&quot;code-object&quot;&gt;print&lt;/span&gt;(with_filename.head())
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The spill happens in the HybridRowQueue that is used to merge the part that went through the Python worker and the part that didn&apos;t.&lt;/p&gt;

&lt;p&gt;The problem comes from the fact that when it is added to the HybridRowQueue, the UnsafeRow has a totalSizeInBytes of ~240000 (seen by adding debug message in HybridRowQueue), whereas, since it&apos;s after the explode, the actual size of the row should be in the ~60 bytes range.&lt;/p&gt;

&lt;p&gt;My understanding is that the row has retained the size it consumed &lt;b&gt;prior&lt;/b&gt; to the explode (at that time, the size of each of the 5 rows was indeed ~240000 bytes.&lt;/p&gt;

&lt;p&gt;A workaround is to do exploded.cache() before calling the UDF. The fact of going through the InMemoryColumnarTableScan &quot;resets&quot; the wrongful size of the UnsafeRow.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;</description>
                <environment>&lt;p&gt;Reproduced on up-to-date master&lt;/p&gt;</environment>
        <key id="13115420">SPARK-22410</key>
            <summary>Excessive spill for Pyspark UDF when a row has shrunk</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Minor</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="viirya">L. C. Hsieh</assignee>
                                    <reporter username="cstenac">Cl&#233;ment Stenac</reporter>
                        <labels>
                    </labels>
                <created>Wed, 1 Nov 2017 17:44:21 +0000</created>
                <updated>Sat, 4 Nov 2017 12:11:52 +0000</updated>
                            <resolved>Sat, 4 Nov 2017 12:11:35 +0000</resolved>
                                    <version>2.3.0</version>
                                    <fixVersion>2.3.0</fixVersion>
                                    <component>PySpark</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="16235522" author="apachespark" created="Thu, 2 Nov 2017 10:49:04 +0000"  >&lt;p&gt;User &apos;viirya&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/19642&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/19642&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16238953" author="cloud_fan" created="Sat, 4 Nov 2017 12:11:35 +0000"  >&lt;p&gt;Issue resolved by pull request 19642&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/19642&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/19642&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 2 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3m9zb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>