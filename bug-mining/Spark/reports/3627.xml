<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:44:17 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-14485] Task finished cause fetch failure when its executor has already been removed by driver </title>
                <link>https://issues.apache.org/jira/browse/SPARK-14485</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Now, when executor is removed by driver with heartbeats timeout, driver will re-queue the task on this executor and send a kill command to cluster to kill this executor.&lt;br/&gt;
But, in a situation, the running task of this executor is finished and return result to driver before this executor killed by kill command sent by driver. At this situation, driver will accept the task finished event and ignore  speculative task and re-queued task. But, as we know, this executor has removed by driver, the result of this finished task can not save in driver because the &lt;b&gt;BlockManagerId&lt;/b&gt; has also removed from &lt;b&gt;BlockManagerMaster&lt;/b&gt; by driver. So, the result data of this stage is not complete, and then, it will cause fetch failure.&lt;/p&gt;

&lt;p&gt;For example, the following is the task log:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;2015-12-31 04:38:50 INFO 15/12/31 04:38:50 WARN HeartbeatReceiver: Removing executor 322 with no recent heartbeats: 256015 ms exceeds timeout 250000 ms
2015-12-31 04:38:50 INFO 15/12/31 04:38:50 ERROR YarnScheduler: Lost executor 322 on BJHC-HERA-16168.hadoop.jd.local: Executor heartbeat timed out after 256015 ms
2015-12-31 04:38:50 INFO 15/12/31 04:38:50 INFO TaskSetManager: Re-queueing tasks for 322 from TaskSet 107.0
2015-12-31 04:38:50 INFO 15/12/31 04:38:50 WARN TaskSetManager: Lost task 229.0 in stage 107.0 (TID 10384, BJHC-HERA-16168.hadoop.jd.local): ExecutorLostFailure (executor 322 lost)
2015-12-31 04:38:50 INFO 15/12/31 04:38:50 INFO DAGScheduler: Executor lost: 322 (epoch 11)
2015-12-31 04:38:50 INFO 15/12/31 04:38:50 INFO BlockManagerMasterEndpoint: Trying to remove executor 322 from BlockManagerMaster.
2015-12-31 04:38:50 INFO 15/12/31 04:38:50 INFO BlockManagerMaster: Removed 322 successfully in removeExecutor
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;2015-12-31 04:38:52 INFO 15/12/31 04:38:52 INFO TaskSetManager: Finished task 229.0 in stage 107.0 (TID 10384) in 272315 ms on BJHC-HERA-16168.hadoop.jd.local (579/700)
2015-12-31 04:40:12 INFO 15/12/31 04:40:12 INFO TaskSetManager: Ignoring task-finished event for 229.1 in stage 107.0 because task 229 has already completed successfully
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;2015-12-31 04:40:12 INFO 15/12/31 04:40:12 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 107 (MapPartitionsRDD[263] at mapPartitions at Exchange.scala:137)
2015-12-31 04:40:12 INFO 15/12/31 04:40:12 INFO YarnScheduler: Adding task set 107.1 with 3 tasks
2015-12-31 04:40:12 INFO 15/12/31 04:40:12 INFO TaskSetManager: Starting task 0.0 in stage 107.1 (TID 10863, BJHC-HERA-18043.hadoop.jd.local, PROCESS_LOCAL, 3745 bytes)
2015-12-31 04:40:12 INFO 15/12/31 04:40:12 INFO TaskSetManager: Starting task 1.0 in stage 107.1 (TID 10864, BJHC-HERA-9291.hadoop.jd.local, PROCESS_LOCAL, 3745 bytes)
2015-12-31 04:40:12 INFO 15/12/31 04:40:12 INFO TaskSetManager: Starting task 2.0 in stage 107.1 (TID 10865, BJHC-HERA-16047.hadoop.jd.local, PROCESS_LOCAL, 3745 bytes)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Driver will check the stage&apos;s result is not complete, and submit missing task, but this time, the next stage has run because previous stage has finish for its task is all finished although its result is not complete.&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;2015-12-31 04:40:13 INFO 15/12/31 04:40:13 WARN TaskSetManager: Lost task 39.0 in stage 109.0 (TID 10905, BJHC-HERA-9357.hadoop.jd.local): FetchFailed(null, shuffleId=11, mapId=-1, reduceId=39, message=
2015-12-31 04:40:13 INFO org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 11
2015-12-31 04:40:13 INFO at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$1.apply(MapOutputTracker.scala:385)
2015-12-31 04:40:13 INFO at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$1.apply(MapOutputTracker.scala:382)
2015-12-31 04:40:13 INFO at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
2015-12-31 04:40:13 INFO at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
2015-12-31 04:40:13 INFO at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
2015-12-31 04:40:13 INFO at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
2015-12-31 04:40:13 INFO at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
2015-12-31 04:40:13 INFO at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)
2015-12-31 04:40:13 INFO at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:381)
2015-12-31 04:40:13 INFO at org.apache.spark.MapOutputTracker.getServerStatuses(MapOutputTracker.scala:172)
2015-12-31 04:40:13 INFO at org.apache.spark.shuffle.sort.SortShuffleReader.computeShuffleBlocks(SortShuffleReader.scala:301)
2015-12-31 04:40:13 INFO at org.apache.spark.shuffle.sort.SortShuffleReader.read(SortShuffleReader.scala:111)
2015-12-31 04:40:13 INFO at org.apache.spark.shuffle.sort.MixedShuffleReader.read(MixedShuffleReader.scala:41)
2015-12-31 04:40:13 INFO at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:90)
2015-12-31 04:40:13 INFO at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
2015-12-31 04:40:13 INFO at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
2015-12-31 04:40:13 INFO at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
2015-12-31 04:40:13 INFO at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
2015-12-31 04:40:13 INFO at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
2015-12-31 04:40:13 INFO at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
2015-12-31 04:40:13 INFO at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
2015-12-31 04:40:13 INFO at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
2015-12-31 04:40:13 INFO at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)
2015-12-31 04:40:13 INFO at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
2015-12-31 04:40:13 INFO at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
2015-12-31 04:40:13 INFO at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
2015-12-31 04:40:13 INFO at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
2015-12-31 04:40:13 INFO at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
2015-12-31 04:40:13 INFO at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
2015-12-31 04:40:13 INFO at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
2015-12-31 04:40:13 INFO at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
2015-12-31 04:40:13 INFO at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)
2015-12-31 04:40:13 INFO at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
2015-12-31 04:40:13 INFO at org.apache.spark.scheduler.Task.run(Task.scala:64)
2015-12-31 04:40:13 INFO at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:209)
2015-12-31 04:40:13 INFO at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
2015-12-31 04:40:13 INFO at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
2015-12-31 04:40:13 INFO at java.lang.Thread.run(Thread.java:745)
2015-12-31 04:40:13 INFO 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As the task log show, in this situation, it will casue FetchFailedException.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12957130">SPARK-14485</key>
            <summary>Task finished cause fetch failure when its executor has already been removed by driver </summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="2">Won&apos;t Fix</resolution>
                                        <assignee username="iward">iward</assignee>
                                    <reporter username="iward">iward</reporter>
                        <labels>
                    </labels>
                <created>Fri, 8 Apr 2016 07:33:34 +0000</created>
                <updated>Fri, 10 Jun 2016 19:54:16 +0000</updated>
                            <resolved>Fri, 10 Jun 2016 19:53:37 +0000</resolved>
                                    <version>1.3.1</version>
                    <version>1.5.2</version>
                                                    <component>Spark Core</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="15231822" author="apachespark" created="Fri, 8 Apr 2016 08:01:04 +0000"  >&lt;p&gt;User &apos;zhonghaihua&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/12258&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/12258&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15321177" author="kayousterhout" created="Wed, 8 Jun 2016 18:42:54 +0000"  >&lt;p&gt;I commented on the pull request, but want to continue the discussion here for archiving purposes.&lt;/p&gt;

&lt;p&gt;My understanding is that this pull request fixes the following sequence of events:&lt;br/&gt;
(1) A task completes on an executor&lt;br/&gt;
(2) The executor fails&lt;br/&gt;
(3) The scheduler is notified about the task completing.&lt;br/&gt;
(4) A future stage that depends on the task runs, and fails, because the executor where the data was stored has failed.&lt;/p&gt;

&lt;p&gt;With the proposed pull request, in step (3), the scheduler ignores the update, because it came from a failed executor.&lt;/p&gt;

&lt;p&gt;I don&apos;t think we should do this for a few reasons:&lt;/p&gt;

&lt;p&gt;(a) If the task didn&apos;t have a result stored on the executor (e.g., it computed some result on the RDD that it sent directly back to the master, like counting the elements in the RDD), it doesn&apos;t need to be failed, and can complete successfully.  With this change, we&apos;d unnecessarily re-run the task.&lt;br/&gt;
(b) If the task did had in IndirectTaskResult (where it was too big to be sent directly to the master), the TaskResultGetter will fail to get the task result, and the task will be marked as failed.  This already worked correctly with the old code (AFAIK).&lt;br/&gt;
(c) This change is attempting to fix a third case, where the task had shuffle data that&apos;s now inaccessible because the machine had died.  I don&apos;t think it makes sense to fix this, because you can imagine a slight change in timing that causes the order of (2) and (3) above to be swapped.  In this case, even with the proposed code change, we&apos;re still stuck with the fetch failure and re-running the map stage.  Furthermore, it&apos;s possible (and likely!) that there were other map tasks that ran on the failed executor, and those tasks won&apos;t be failed and re-run with this change, so the reduce stage will still fail.  In general, the reason we have the fetch failure mechanism is because it can happen that shuffle data gets lost, and rather than detecting every kind of map-side failure, it&apos;s simpler to fail on the reduce side and then re-run the necessary tasks in the map stage.&lt;/p&gt;

&lt;p&gt;Given all of the above, I&apos;d advocate for reverting this change and marking the JIRA as won&apos;t fix.  &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vanzin&quot; class=&quot;user-hover&quot; rel=&quot;vanzin&quot;&gt;vanzin&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=iward&quot; class=&quot;user-hover&quot; rel=&quot;iward&quot;&gt;iward&lt;/a&gt; let me know what your thoughts are. &lt;/p&gt;</comment>
                            <comment id="15321202" author="vanzin" created="Wed, 8 Jun 2016 18:51:43 +0000"  >&lt;p&gt;I commented on the PR, but will mostly repeat it here.&lt;/p&gt;

&lt;p&gt;I think your point (a) is valid. It should be a rare case, though, so I don&apos;t feel strongly one way or another about it.&lt;/p&gt;

&lt;p&gt;The change helps (b) because it can avoid unnecessary logs to the output. It&apos;s a minor issue, I grant you that.&lt;/p&gt;

&lt;p&gt;Similarly, the change helps (c) by avoiding log noise in certain situations. The race you mention does exist, but users get really antsy when they see exceptions in logs, so if we can help avoid those it&apos;s always good.&lt;/p&gt;

&lt;p&gt;So to me it boils down to case (a): we can revert the change and live with the extra noise in the logs, we can add code to handle that case and still clean the logs, or we can live with the added inefficiency which, in my view, should be hit pretty rarely.&lt;/p&gt;</comment>
                            <comment id="15321238" author="vanzin" created="Wed, 8 Jun 2016 19:03:31 +0000"  >&lt;p&gt;Just to distill my comment a little bit more, without writing extra code there will be recomputation either way: the old code would cause the downstream task to fail, the new code will cause the original task to be recomputed. I prefer the new one better because it avoids noise in the logs, but, either way works.&lt;/p&gt;</comment>
                            <comment id="15322893" author="kayousterhout" created="Thu, 9 Jun 2016 17:17:20 +0000"  >&lt;p&gt;I don&apos;t think (a) is especially rare: that&apos;s the case anytime data is saved to HDFS, or a result is returned &amp;#8211; e.g., from SQL queries  that aggregate a result at the driver, rather than creating a result table.&lt;/p&gt;

&lt;p&gt;My point for (c) was that it seems to only benefit a small fraction of cases: when both (1) the scheduler learned about the lost executor before learning about the successful task and (2) there weren&apos;t other previous tasks in the same stage that ran on the failed executor (in which case the other, previously completed tasks won&apos;t get re-run until there&apos;s a fetch failure in the next stage).&lt;/p&gt;

&lt;p&gt;I&apos;m also hesitant to add this special logic where we sometimes just ignore task-completed messages, because I&apos;m worried about corner cases where this could lead to a job hanging because somehow the task never gets completed successfully.&lt;/p&gt;

&lt;p&gt;Given all of the above, I&apos;d advocate reverting this, and submitted a PR to do so: &lt;a href=&quot;https://github.com/apache/spark/pull/13580&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/13580&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15322894" author="apachespark" created="Thu, 9 Jun 2016 17:18:05 +0000"  >&lt;p&gt;User &apos;kayousterhout&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/13580&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/13580&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15323344" author="vanzin" created="Thu, 9 Jun 2016 21:01:26 +0000"  >&lt;blockquote&gt;&lt;p&gt;I don&apos;t think (a) is especially rare: that&apos;s the case anytime data is saved to HDFS&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I didn&apos;t mean rare in general, I meant it should be rare to hit this particular case (scheduler thinks the executor is gone &lt;b&gt;and&lt;/b&gt; a task result arrives later). The normal case is the task result arrives while the executor is still alive, and the change doesn&apos;t really touch that case.&lt;/p&gt;</comment>
                            <comment id="15325175" author="kayousterhout" created="Fri, 10 Jun 2016 19:54:16 +0000"  >&lt;p&gt;Reverted this and re-opened the JIRA to mark this as &quot;won&apos;t fix&quot;.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 23 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2vtvj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>