<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:12:40 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-1188] GraphX triplets not working properly</title>
                <link>https://issues.apache.org/jira/browse/SPARK-1188</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;I followed the GraphX tutorial at &lt;a href=&quot;http://ampcamp.berkeley.edu/big-data-mini-course/graph-analytics-with-graphx.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://ampcamp.berkeley.edu/big-data-mini-course/graph-analytics-with-graphx.html&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;on a local stand-alone cluster (Spark version 0.9.0) with two workers. Somehow, the graph.triplets is not returning what it should &amp;#8211; only Eds and Frans.&lt;/p&gt;

&lt;p&gt;```&lt;br/&gt;
scala&amp;gt; graph.edges.toArray&lt;br/&gt;
14/03/04 16:15:57 INFO SparkContext: Starting job: collect at EdgeRDD.scala:51&lt;br/&gt;
14/03/04 16:15:57 INFO DAGScheduler: Got job 5 (collect at EdgeRDD.scala:51) with 1 output partitions (allowLocal=false)&lt;br/&gt;
14/03/04 16:15:57 INFO DAGScheduler: Final stage: Stage 27 (collect at EdgeRDD.scala:51)&lt;br/&gt;
14/03/04 16:15:57 INFO DAGScheduler: Parents of final stage: List()&lt;br/&gt;
14/03/04 16:15:57 INFO DAGScheduler: Missing parents: List()&lt;br/&gt;
14/03/04 16:15:57 INFO DAGScheduler: Submitting Stage 27 (MappedRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;36&amp;#93;&lt;/span&gt; at map at EdgeRDD.scala:51), which has no missing parents&lt;br/&gt;
14/03/04 16:15:57 INFO DAGScheduler: Submitting 1 missing tasks from Stage 27 (MappedRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;36&amp;#93;&lt;/span&gt; at map at EdgeRDD.scala:51)&lt;br/&gt;
14/03/04 16:15:57 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks&lt;br/&gt;
14/03/04 16:15:57 INFO TaskSetManager: Starting task 27.0:0 as TID 11 on executor localhost: localhost (PROCESS_LOCAL)&lt;br/&gt;
14/03/04 16:15:57 INFO TaskSetManager: Serialized task 27.0:0 as 2068 bytes in 1 ms&lt;br/&gt;
14/03/04 16:15:57 INFO Executor: Running task ID 11&lt;br/&gt;
14/03/04 16:15:57 INFO BlockManager: Found block rdd_2_0 locally&lt;br/&gt;
14/03/04 16:15:57 INFO Executor: Serialized size of result for 11 is 936&lt;br/&gt;
14/03/04 16:15:57 INFO Executor: Sending result for 11 directly to driver&lt;br/&gt;
14/03/04 16:15:57 INFO Executor: Finished task ID 11&lt;br/&gt;
14/03/04 16:15:57 INFO TaskSetManager: Finished TID 11 in 13 ms on localhost (progress: 0/1)&lt;br/&gt;
14/03/04 16:15:57 INFO DAGScheduler: Completed ResultTask(27, 0)&lt;br/&gt;
14/03/04 16:15:57 INFO TaskSchedulerImpl: Remove TaskSet 27.0 from pool&lt;br/&gt;
14/03/04 16:15:57 INFO DAGScheduler: Stage 27 (collect at EdgeRDD.scala:51) finished in 0.015 s&lt;br/&gt;
14/03/04 16:15:57 INFO SparkContext: Job finished: collect at EdgeRDD.scala:51, took 0.023602266 s&lt;br/&gt;
res7: Array[org.apache.spark.graphx.Edge&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;] = Array(Edge(2,1,7), Edge(2,4,2), Edge(3,2,4), Edge(3,6,3), Edge(4,1,1), Edge(5,2,2), Edge(5,3,8), Edge(5,6,3))&lt;/p&gt;


&lt;p&gt;scala&amp;gt; graph.vertices.toArray&lt;br/&gt;
14/03/04 16:16:18 INFO SparkContext: Starting job: toArray at &amp;lt;console&amp;gt;:27&lt;br/&gt;
14/03/04 16:16:18 INFO DAGScheduler: Got job 6 (toArray at &amp;lt;console&amp;gt;:27) with 1 output partitions (allowLocal=false)&lt;br/&gt;
14/03/04 16:16:18 INFO DAGScheduler: Final stage: Stage 28 (toArray at &amp;lt;console&amp;gt;:27)&lt;br/&gt;
14/03/04 16:16:18 INFO DAGScheduler: Parents of final stage: List(Stage 32, Stage 29)&lt;br/&gt;
14/03/04 16:16:18 INFO DAGScheduler: Missing parents: List()&lt;br/&gt;
14/03/04 16:16:18 INFO DAGScheduler: Submitting Stage 28 (VertexRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;15&amp;#93;&lt;/span&gt; at RDD at VertexRDD.scala:52), which has no missing parents&lt;br/&gt;
14/03/04 16:16:18 INFO DAGScheduler: Submitting 1 missing tasks from Stage 28 (VertexRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;15&amp;#93;&lt;/span&gt; at RDD at VertexRDD.scala:52)&lt;br/&gt;
14/03/04 16:16:18 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks&lt;br/&gt;
14/03/04 16:16:18 INFO TaskSetManager: Starting task 28.0:0 as TID 12 on executor localhost: localhost (PROCESS_LOCAL)&lt;br/&gt;
14/03/04 16:16:18 INFO TaskSetManager: Serialized task 28.0:0 as 2426 bytes in 0 ms&lt;br/&gt;
14/03/04 16:16:18 INFO Executor: Running task ID 12&lt;br/&gt;
14/03/04 16:16:18 INFO BlockManager: Found block rdd_14_0 locally&lt;br/&gt;
14/03/04 16:16:18 INFO Executor: Serialized size of result for 12 is 947&lt;br/&gt;
14/03/04 16:16:18 INFO Executor: Sending result for 12 directly to driver&lt;br/&gt;
14/03/04 16:16:18 INFO Executor: Finished task ID 12&lt;br/&gt;
14/03/04 16:16:18 INFO TaskSetManager: Finished TID 12 in 13 ms on localhost (progress: 0/1)&lt;br/&gt;
14/03/04 16:16:18 INFO DAGScheduler: Completed ResultTask(28, 0)&lt;br/&gt;
14/03/04 16:16:18 INFO TaskSchedulerImpl: Remove TaskSet 28.0 from pool&lt;br/&gt;
14/03/04 16:16:18 INFO DAGScheduler: Stage 28 (toArray at &amp;lt;console&amp;gt;:27) finished in 0.015 s&lt;br/&gt;
14/03/04 16:16:18 INFO SparkContext: Job finished: toArray at &amp;lt;console&amp;gt;:27, took 0.027839851 s&lt;br/&gt;
res9: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;(org.apache.spark.graphx.VertexId, (String, Int))&amp;#93;&lt;/span&gt; = Array((4,(David,42)), (2,(Bob,27)), (6,(Fran,50)), (5,(Ed,55)), (3,(Charlie,65)), (1,(Alice,28)))&lt;/p&gt;


&lt;p&gt;scala&amp;gt; graph.triplets.toArray&lt;br/&gt;
14/03/04 16:16:30 INFO SparkContext: Starting job: toArray at &amp;lt;console&amp;gt;:27&lt;br/&gt;
14/03/04 16:16:30 INFO DAGScheduler: Got job 7 (toArray at &amp;lt;console&amp;gt;:27) with 1 output partitions (allowLocal=false)&lt;br/&gt;
14/03/04 16:16:31 INFO DAGScheduler: Final stage: Stage 33 (toArray at &amp;lt;console&amp;gt;:27)&lt;br/&gt;
14/03/04 16:16:31 INFO DAGScheduler: Parents of final stage: List(Stage 34)&lt;br/&gt;
14/03/04 16:16:31 INFO DAGScheduler: Missing parents: List()&lt;br/&gt;
14/03/04 16:16:31 INFO DAGScheduler: Submitting Stage 33 (ZippedPartitionsRDD2&lt;span class=&quot;error&quot;&gt;&amp;#91;32&amp;#93;&lt;/span&gt; at zipPartitions at GraphImpl.scala:60), which has no missing parents&lt;br/&gt;
14/03/04 16:16:31 INFO DAGScheduler: Submitting 1 missing tasks from Stage 33 (ZippedPartitionsRDD2&lt;span class=&quot;error&quot;&gt;&amp;#91;32&amp;#93;&lt;/span&gt; at zipPartitions at GraphImpl.scala:60)&lt;br/&gt;
14/03/04 16:16:31 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks&lt;br/&gt;
14/03/04 16:16:31 INFO TaskSetManager: Starting task 33.0:0 as TID 13 on executor localhost: localhost (PROCESS_LOCAL)&lt;br/&gt;
14/03/04 16:16:31 INFO TaskSetManager: Serialized task 33.0:0 as 3322 bytes in 1 ms&lt;br/&gt;
14/03/04 16:16:31 INFO Executor: Running task ID 13&lt;br/&gt;
14/03/04 16:16:31 INFO BlockManager: Found block rdd_2_0 locally&lt;br/&gt;
14/03/04 16:16:31 INFO BlockManager: Found block rdd_31_0 locally&lt;br/&gt;
14/03/04 16:16:31 INFO Executor: Serialized size of result for 13 is 931&lt;br/&gt;
14/03/04 16:16:31 INFO Executor: Sending result for 13 directly to driver&lt;br/&gt;
14/03/04 16:16:31 INFO Executor: Finished task ID 13&lt;br/&gt;
14/03/04 16:16:31 INFO TaskSetManager: Finished TID 13 in 17 ms on localhost (progress: 0/1)&lt;br/&gt;
14/03/04 16:16:31 INFO DAGScheduler: Completed ResultTask(33, 0)&lt;br/&gt;
14/03/04 16:16:31 INFO TaskSchedulerImpl: Remove TaskSet 33.0 from pool&lt;br/&gt;
14/03/04 16:16:31 INFO DAGScheduler: Stage 33 (toArray at &amp;lt;console&amp;gt;:27) finished in 0.019 s&lt;br/&gt;
14/03/04 16:16:31 INFO SparkContext: Job finished: toArray at &amp;lt;console&amp;gt;:27, took 0.037909394 s&lt;br/&gt;
res10: Array[org.apache.spark.graphx.EdgeTriplet&lt;span class=&quot;error&quot;&gt;&amp;#91;(String, Int),Int&amp;#93;&lt;/span&gt;] = Array(((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3), ((5,(Ed,55)),(6,(Fran,50)),3))&lt;br/&gt;
```&lt;/p&gt;</description>
                <environment></environment>
        <key id="12704872">SPARK-1188</key>
            <summary>GraphX triplets not working properly</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="darabos">Daniel Darabos</assignee>
                                    <reporter username="k0alak0der">Kev Alan</reporter>
                        <labels>
                    </labels>
                <created>Wed, 5 Mar 2014 00:13:56 +0000</created>
                <updated>Thu, 29 May 2014 08:20:34 +0000</updated>
                            <resolved>Wed, 23 Apr 2014 18:41:51 +0000</resolved>
                                    <version>0.9.0</version>
                                    <fixVersion>0.9.2</fixVersion>
                    <fixVersion>1.0.0</fixVersion>
                                    <component>GraphX</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="13953923" author="rxin" created="Wed, 5 Mar 2014 00:25:59 +0000"  >&lt;p&gt;The problem is that we are reusing an EdgeTriplet object. Try force a copy before you do the collect.&lt;/p&gt;

&lt;p&gt;e.g.&lt;/p&gt;

&lt;p&gt;triplets.map(_.copy).collect()&lt;/p&gt;</comment>
                            <comment id="13954065" author="darabos" created="Thu, 13 Mar 2014 17:42:24 +0000"  >&lt;p&gt;This has bitten us as well. I am no expert, but my impression is that re-using an object in these iterators was a terrible idea.&lt;/p&gt;

&lt;p&gt;Consider this example:&lt;/p&gt;

&lt;p&gt;scala&amp;gt; val g = graphx.util.GraphGenerators.starGraph(sc, 5)&lt;br/&gt;
scala&amp;gt; g.edges.collect&lt;br/&gt;
Array(Edge(1,0,1), Edge(2,0,1), Edge(3,0,1), Edge(4,0,1))&lt;br/&gt;
scala&amp;gt; g.edges.map(x =&amp;gt; x).collect&lt;br/&gt;
Array(Edge(4,0,1), Edge(4,0,1), Edge(4,0,1), Edge(4,0,1))&lt;/p&gt;

&lt;p&gt;It can and does go very wrong in practice too. Consider this:&lt;/p&gt;

&lt;p&gt;scala&amp;gt; g.edges.saveAsObjectFile(&quot;edges&quot;)&lt;br/&gt;
scala&amp;gt; sc.objectFile[graphx.Edge&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;](&quot;edges&quot;).collect&lt;br/&gt;
Array(Edge(4,0,1), Edge(4,0,1), Edge(4,0,1), Edge(4,0,1))&lt;/p&gt;

&lt;p&gt;Indeed map(_.copy) is a good workaround. But seems like this is a nasty trap laid out for your users. Did you measure the performance gain? If not, or if it&apos;s insignificant, I would suggest not re-using the object in the iterators. It would simplify the GraphX source too. However, if it is significant, one idea is to offer separate mapFast() methods that do re-use, while the more commonly used map() would offer the more commonly expected (copying) semantics.&lt;/p&gt;

&lt;p&gt;Also I wish GraphX offered graph save/load functionality. If it did, I would only have discovered this bug a few weeks from now &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.&lt;/p&gt;</comment>
                            <comment id="13954099" author="darabos" created="Wed, 19 Mar 2014 06:32:34 +0000"  >&lt;p&gt;Sorry, I forgot to test your workaround before commenting. It does not work.&lt;/p&gt;

&lt;p&gt;scala&amp;gt; g.triplets.map(_.copy).collect&lt;br/&gt;
&amp;lt;console&amp;gt;:16: error: missing arguments for method copy in class Edge;&lt;br/&gt;
follow this method with `_&apos; if you want to treat it as a partially applied function&lt;br/&gt;
              g.triplets.map(_.copy).collect&lt;br/&gt;
                               ^&lt;br/&gt;
You can of course write a copy function yourself.&lt;/p&gt;

&lt;p&gt;I&apos;d be happy to work on this. Can I just get rid of the re-use, or would you prefer another approach?&lt;/p&gt;</comment>
                            <comment id="13954228" author="darabos" created="Fri, 28 Mar 2014 08:19:47 +0000"  >&lt;p&gt;Okay, it was just missing the parentheses:&lt;/p&gt;

&lt;p&gt;scala&amp;gt; g.triplets.map(_.copy()).collect&lt;br/&gt;
res1: Array[org.apache.spark.graphx.Edge&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;] = Array(Edge(1,0,1), Edge(2,0,1), Edge(3,0,1), Edge(4,0,1))&lt;/p&gt;

&lt;p&gt;(Scala novice here &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.)&lt;/p&gt;</comment>
                            <comment id="13955100" author="darabos" created="Mon, 31 Mar 2014 11:13:19 +0000"  >&lt;p&gt;I&apos;ve sent a pull request to eliminate the re-use (&lt;a href=&quot;https://github.com/apache/spark/pull/276&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/276&lt;/a&gt;). The change has no performance impact and simplifies the code.&lt;/p&gt;</comment>
                            <comment id="13978027" author="darabos" created="Wed, 23 Apr 2014 09:29:58 +0000"  >&lt;p&gt;The changes are in the master branch now. I can&apos;t figure out how to close a JIRA ticket &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.&lt;/p&gt;</comment>
                            <comment id="13978592" author="rxin" created="Wed, 23 Apr 2014 18:41:44 +0000"  >&lt;p&gt;I added you to contributor list so you should be able to edit in the future. Cheers.&lt;/p&gt;</comment>
                            <comment id="14002336" author="rxin" created="Mon, 19 May 2014 20:28:45 +0000"  >&lt;p&gt;Adding a link to the commit: &lt;a href=&quot;https://github.com/apache/spark/commit/78236334e4ca7518b6d7d9b38464dbbda854a777#diff-a2b19aac11cb2fbe9962b5d2290ea77e&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/commit/78236334e4ca7518b6d7d9b38464dbbda854a777#diff-a2b19aac11cb2fbe9962b5d2290ea77e&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="12715278">SPARK-1883</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>382877</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            11 years, 27 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1tyo7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>383145</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>