<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:26:17 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-6898] Special chars in column names is broken</title>
                <link>https://issues.apache.org/jira/browse/SPARK-6898</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;This function is added a long time ago, but it&apos;s not complete, it will fail if we have &quot;.&quot; inside column name.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;test(&lt;span class=&quot;code-quote&quot;&gt;&quot;SPARK-3483 Special chars in column names&quot;&lt;/span&gt;) {
    val data = sparkContext.parallelize(
      Seq(&lt;span class=&quot;code-quote&quot;&gt;&quot;&quot;&quot;{&quot;&lt;/span&gt;key?number1&lt;span class=&quot;code-quote&quot;&gt;&quot;: &quot;&lt;/span&gt;value1&lt;span class=&quot;code-quote&quot;&gt;&quot;, &quot;&lt;/span&gt;key.number2&lt;span class=&quot;code-quote&quot;&gt;&quot;: &quot;&lt;/span&gt;value2&lt;span class=&quot;code-quote&quot;&gt;&quot;}&quot;&lt;/span&gt;&quot;&quot;))
    jsonRDD(data).registerTempTable(&lt;span class=&quot;code-quote&quot;&gt;&quot;records&quot;&lt;/span&gt;)
    sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;SELECT `key?number1`, `key.number2` FROM records&quot;&lt;/span&gt;)
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;this test will fail.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12820754">SPARK-6898</key>
            <summary>Special chars in column names is broken</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="cloud_fan">Wenchen Fan</assignee>
                                    <reporter username="cloud_fan">Wenchen Fan</reporter>
                        <labels>
                    </labels>
                <created>Tue, 14 Apr 2015 08:41:59 +0000</created>
                <updated>Mon, 27 Jul 2015 16:36:19 +0000</updated>
                            <resolved>Wed, 15 Apr 2015 20:39:43 +0000</resolved>
                                                    <fixVersion>1.4.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="14494251" author="apachespark" created="Tue, 14 Apr 2015 15:24:23 +0000"  >&lt;p&gt;User &apos;cloud-fan&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/5511&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/5511&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14496899" author="marmbrus" created="Wed, 15 Apr 2015 20:39:43 +0000"  >&lt;p&gt;Issue resolved by pull request 5511&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/5511&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/5511&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14640704" author="dsdinter" created="Fri, 24 Jul 2015 16:28:26 +0000"  >&lt;p&gt;I hope I am missing something here but I am facing this issue in my current cluster (Currently running compiled version of Spark 1.4.0 with Hive support)&lt;br/&gt;
(I tried with 1.4.0 and 1.4.1 binaries locally and had same issue).&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;sqlContext.jsonRDD(sc.makeRDD(&quot;&quot;&quot;{&quot;a&quot;: {&quot;c.b&quot;: 1}, &quot;b.$q&quot;: [{&quot;a@!.q&quot;: 1}], &quot;q.w&quot;: {&quot;w.i&amp;amp;&quot;: [1]}}&quot;&quot;&quot; :: Nil)).registerTempTable(&quot;t&quot;)
sqlContext.sql(&quot;SELECT `key?number1`, `key.number2` FROM records&quot;)
sqlContext.sql(&quot;SELECT a.`c.b`, `b.$q`[0].`a@!.q`, `q.w`.`w.i&amp;amp;`[0] FROM t&quot;)

Either select is throwing the same error:
scala&amp;gt; sqlContext.sql(&quot;SELECT a.`c.b`, `b.$q`[0].`a@!.q`, `q.w`.`w.i&amp;amp;`[0] FROM t&quot;)
15/07/24 17:23:12 INFO ParseDriver: Parsing command: SELECT a.`c.b`, `b.$q`[0].`a@!.q`, `q.w`.`w.i&amp;amp;`[0] FROM t
15/07/24 17:23:12 INFO ParseDriver: Parse Completed
org.apache.spark.sql.AnalysisException: cannot resolve &apos;b.$q&apos; given input columns a, b.$q, q.w; line 1 pos 16
        at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
        at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.
scala:63)
        at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.
scala:52)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:286)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:286)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="14641446" author="cloud_fan" created="Sat, 25 Jul 2015 07:35:10 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dsdinter&quot; class=&quot;user-hover&quot; rel=&quot;dsdinter&quot;&gt;dsdinter&lt;/a&gt; I tried your case on 1.4.1&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;sqlContext.jsonRDD(sc.makeRDD(&lt;span class=&quot;code-quote&quot;&gt;&quot;&quot;&quot;{&quot;&lt;/span&gt;a&lt;span class=&quot;code-quote&quot;&gt;&quot;: {&quot;&lt;/span&gt;c.b&lt;span class=&quot;code-quote&quot;&gt;&quot;: 1}, &quot;&lt;/span&gt;b.$q&lt;span class=&quot;code-quote&quot;&gt;&quot;: [{&quot;&lt;/span&gt;a@!.q&lt;span class=&quot;code-quote&quot;&gt;&quot;: 1}], &quot;&lt;/span&gt;q.w&lt;span class=&quot;code-quote&quot;&gt;&quot;: {&quot;&lt;/span&gt;w.i&amp;amp;&lt;span class=&quot;code-quote&quot;&gt;&quot;: [1]}}&quot;&lt;/span&gt;&lt;span class=&quot;code-quote&quot;&gt;&quot;&quot; :: Nil)).registerTempTable(&quot;&lt;/span&gt;t&quot;)
sqlContext.sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;SELECT a.`c.b`, `b.$q`[0].`a@!.q`, `q.w`.`w.i&amp;amp;`[0] FROM t&quot;&lt;/span&gt;)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;It has no problem, did you use jdbc or thrift-server to run the query? can you copy and paste the full stack-trace of this issue?&lt;/p&gt;</comment>
                            <comment id="14641458" author="dsdinter" created="Sat, 25 Jul 2015 08:08:40 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cloud_fan&quot; class=&quot;user-hover&quot; rel=&quot;cloud_fan&quot;&gt;cloud_fan&lt;/a&gt; I tried from spark-shell (It works from thrift-server as I had Tableau connected to a big denormilized table which includes dots in column names).&lt;br/&gt;
Here is the full stack (Maybe my locale has something to do? &lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.spark.sql.AnalysisException: cannot resolve &apos;b.$q&apos; given input columns a, b.$q, q.w; line 1 pos 16
	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:63)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:52)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:285)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:299)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenUp(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:283)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:299)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenUp(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:283)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:299)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenUp(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:283)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$transformExpressionUp$1(QueryPlan.scala:108)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2$$anonfun$apply$2.apply(QueryPlan.scala:123)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:122)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:52)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:50)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:98)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:50)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:42)
	at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:931)
	at org.apache.spark.sql.DataFrame.&amp;lt;init&amp;gt;(DataFrame.scala:131)
	at org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:755)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:20)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:25)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:27)
	at $iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:29)
	at $iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:31)
	at $iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:33)
	at $iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:35)
	at $iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:37)
	at &amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:39)
	at .&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:43)
	at .&amp;lt;clinit&amp;gt;(&amp;lt;console&amp;gt;)
	at .&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:7)
	at .&amp;lt;clinit&amp;gt;(&amp;lt;console&amp;gt;)
	at $print(&amp;lt;console&amp;gt;)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1338)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:857)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:902)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:814)
	at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:657)
	at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:665)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$loop(SparkILoop.scala:670)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:997)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1059)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="14641471" author="dsdinter" created="Sat, 25 Jul 2015 09:12:57 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cloud_fan&quot; class=&quot;user-hover&quot; rel=&quot;cloud_fan&quot;&gt;cloud_fan&lt;/a&gt; I literally copy&amp;amp;pasted the snipped of code from your reply and used Spark 1.4.1 from brew on my Mac and I am getting the error I just included in my earlier reply. For reference I am in UK, in case the locale is doing something weird with backticks...&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
2015-07-25 10:09:05.510 java[6618:141238] Unable to load realm info from SCDynamicStore
Using Spark&apos;s default log4j profile: org/apache/spark/log4j-defaults.properties
15/07/25 10:09:06 INFO SecurityManager: Changing view acls to: davidsabater
15/07/25 10:09:06 INFO SecurityManager: Changing modify acls to: davidsabater
15/07/25 10:09:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(davidsabater); users with modify permissions: Set(davidsabater)
15/07/25 10:09:06 INFO HttpServer: Starting HTTP Server
15/07/25 10:09:06 INFO Utils: Successfully started service &apos;HTTP class server&apos; on port 60987.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  &apos;_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.4.1
      /_/

Using Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_45)
Type in expressions to have them evaluated.
Type :help for more information.
15/07/25 10:09:09 INFO SparkContext: Running Spark version 1.4.1
15/07/25 10:09:09 INFO SecurityManager: Changing view acls to: davidsabater
15/07/25 10:09:09 INFO SecurityManager: Changing modify acls to: davidsabater
15/07/25 10:09:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(davidsabater); users with modify permissions: Set(davidsabater)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;My locale:&lt;br/&gt;
LANG=&quot;en_GB.UTF-8&quot;&lt;br/&gt;
LC_COLLATE=&quot;en_GB.UTF-8&quot;&lt;br/&gt;
LC_CTYPE=&quot;en_GB.UTF-8&quot;&lt;br/&gt;
LC_MESSAGES=&quot;en_GB.UTF-8&quot;&lt;br/&gt;
LC_MONETARY=&quot;en_GB.UTF-8&quot;&lt;br/&gt;
LC_NUMERIC=&quot;en_GB.UTF-8&quot;&lt;br/&gt;
LC_TIME=&quot;en_GB.UTF-8&quot;&lt;/p&gt;</comment>
                            <comment id="14642232" author="cloud_fan" created="Mon, 27 Jul 2015 03:58:07 +0000"  >&lt;p&gt;It&apos;s really weird that I can run this case in test suite, but fail in spark-shell, I&apos;ll investigate it. cc &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=marmbrus&quot; class=&quot;user-hover&quot; rel=&quot;marmbrus&quot;&gt;marmbrus&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14642249" author="cloud_fan" created="Mon, 27 Jul 2015 04:35:12 +0000"  >&lt;p&gt;I build spark from code v1.4.1 and can handle this case, however the 1.4.1 pre-build version will fail, does anybody know the reason? cc &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rxin&quot; class=&quot;user-hover&quot; rel=&quot;rxin&quot;&gt;rxin&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14642259" author="rxin" created="Mon, 27 Jul 2015 04:55:07 +0000"  >&lt;p&gt;See if it is broken in HiveContext.&lt;/p&gt;</comment>
                            <comment id="14642271" author="cloud_fan" created="Mon, 27 Jul 2015 05:18:03 +0000"  >&lt;p&gt;OK finally figured it out. It&apos;s the problem in hive context and I have opened a JIRA to fix it: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-9371&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/SPARK-9371&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dsdinter&quot; class=&quot;user-hover&quot; rel=&quot;dsdinter&quot;&gt;dsdinter&lt;/a&gt; thanks for reporting this!&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="12772837">SPARK-5632</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12731146">SPARK-2775</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12849213">SPARK-9371</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 17 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2d7u7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>