<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:23:33 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-4879] Missing output partitions after job completes with speculative execution</title>
                <link>https://issues.apache.org/jira/browse/SPARK-4879</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;When speculative execution is enabled (&lt;tt&gt;spark.speculation=true&lt;/tt&gt;), jobs that save output files may report that they have completed successfully even though some output partitions written by speculative tasks may be missing.&lt;/p&gt;

&lt;h3&gt;&lt;a name=&quot;Reproduction&quot;&gt;&lt;/a&gt;Reproduction&lt;/h3&gt;

&lt;p&gt;This symptom was reported to me by a Spark user and I&apos;ve been doing my own investigation to try to come up with an in-house reproduction.&lt;/p&gt;

&lt;p&gt;I&apos;m still working on a reliable local reproduction for this issue, which is a little tricky because Spark won&apos;t schedule speculated tasks on the same host as the original task, so you need an actual (or containerized) multi-host cluster to test speculation.  Here&apos;s a simple reproduction of some of the symptoms on EC2, which can be run in &lt;tt&gt;spark-shell&lt;/tt&gt; with &lt;tt&gt;--conf spark.speculation=true&lt;/tt&gt;:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    &lt;span class=&quot;code-comment&quot;&gt;// Rig a job such that all but one of the tasks complete instantly
&lt;/span&gt;    &lt;span class=&quot;code-comment&quot;&gt;// and one task runs &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 20 seconds on its first attempt and instantly
&lt;/span&gt;    &lt;span class=&quot;code-comment&quot;&gt;// on its second attempt:
&lt;/span&gt;    val numTasks = 100
    sc.parallelize(1 to numTasks, numTasks).repartition(2).mapPartitionsWithContext { &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; (ctx, iter) =&amp;gt;
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (ctx.partitionId == 0) {  &lt;span class=&quot;code-comment&quot;&gt;// If &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; is the one task that should run really slow
&lt;/span&gt;        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (ctx.attemptId == 0) {  &lt;span class=&quot;code-comment&quot;&gt;// If &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; is the first attempt, run slow
&lt;/span&gt;         &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.sleep(20 * 1000)
        }
      }
      iter
    }.map(x =&amp;gt; (x, x)).saveAsTextFile(&lt;span class=&quot;code-quote&quot;&gt;&quot;/test4&quot;&lt;/span&gt;)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When I run this, I end up with a job that completes quickly (due to speculation) but reports failures from the speculated task:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;[...]
14/12/11 01:41:13 INFO scheduler.TaskSetManager: Finished task 37.1 in stage 3.0 (TID 411) in 131 ms on ip-172-31-8-164.us-west-2.compute.internal (100/100)
14/12/11 01:41:13 INFO scheduler.DAGScheduler: Stage 3 (saveAsTextFile at &amp;lt;console&amp;gt;:22) finished in 0.856 s
14/12/11 01:41:13 INFO spark.SparkContext: Job finished: saveAsTextFile at &amp;lt;console&amp;gt;:22, took 0.885438374 s
14/12/11 01:41:13 INFO scheduler.TaskSetManager: Ignoring task-finished event &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 70.1 in stage 3.0 because task 70 has already completed successfully

scala&amp;gt; 14/12/11 01:41:13 WARN scheduler.TaskSetManager: Lost task 49.1 in stage 3.0 (TID 413, ip-172-31-8-164.us-west-2.compute.internal): java.io.IOException: Failed to save output of task: attempt_201412110141_0003_m_000049_413
        org.apache.hadoop.mapred.FileOutputCommitter.moveTaskOutputs(FileOutputCommitter.java:160)
        org.apache.hadoop.mapred.FileOutputCommitter.moveTaskOutputs(FileOutputCommitter.java:172)
        org.apache.hadoop.mapred.FileOutputCommitter.commitTask(FileOutputCommitter.java:132)
        org.apache.spark.SparkHadoopWriter.commit(SparkHadoopWriter.scala:109)
        org.apache.spark.rdd.PairRDDFunctions$$anonfun$13.apply(PairRDDFunctions.scala:991)
        org.apache.spark.rdd.PairRDDFunctions$$anonfun$13.apply(PairRDDFunctions.scala:974)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
        org.apache.spark.scheduler.Task.run(Task.scala:54)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;One interesting thing to note about this stack trace: if we look at &lt;tt&gt;FileOutputCommitter.java:160&lt;/tt&gt; (&lt;a href=&quot;http://grepcode.com/file/repository.cloudera.com/content/repositories/releases/org.apache.hadoop/hadoop-core/2.5.0-mr1-cdh5.2.0/org/apache/hadoop/mapred/FileOutputCommitter.java#160&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;link&lt;/a&gt;), this point in the execution seems to correspond to a case where a task completes, attempts to commit its output, fails for some reason, then deletes the destination file, tries again, and fails:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt; &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (fs.isFile(taskOutput)) {
152      Path finalOutputPath = getFinalPath(jobOutputDir, taskOutput, 
153                                          getTempTaskOutputPath(context));
154      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!fs.rename(taskOutput, finalOutputPath)) {
155        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!fs.delete(finalOutputPath, &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)) {
156          &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; IOException(&lt;span class=&quot;code-quote&quot;&gt;&quot;Failed to delete earlier output of task: &quot;&lt;/span&gt; + 
157                                 attemptId);
158        }
159        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!fs.rename(taskOutput, finalOutputPath)) {
160          &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; IOException(&lt;span class=&quot;code-quote&quot;&gt;&quot;Failed to save output of task: &quot;&lt;/span&gt; + 
161        		  attemptId);
162        }
163      }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This could explain why the output file is missing: the second copy of the task keeps running after the job completes and deletes the output written by the other task after failing to commit its own copy of the output.&lt;/p&gt;

&lt;p&gt;There are still a few open questions about how exactly we get into this scenario:&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Why is the second copy of the task allowed to commit its output after the other task / the job has successfully completed?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;To check whether a task&apos;s temporary output should be committed, SparkHadoopWriter calls &lt;tt&gt;FileOutputCommitter.needsTaskCommit()&lt;/tt&gt;, which returns &lt;tt&gt;true&lt;/tt&gt; if the tasks&apos;s temporary output exists (&lt;a href=&quot;http://grepcode.com/file/repository.cloudera.com/content/repositories/releases/org.apache.hadoop/hadoop-core/2.5.0-mr1-cdh5.2.0/org/apache/hadoop/mapred/FileOutputCommitter.java#206&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;link&lt;/a&gt;).  Tihs does not seem to check whether the destination already exists.  This means that &lt;tt&gt;needsTaskCommit&lt;/tt&gt; can return &lt;tt&gt;true&lt;/tt&gt; for speculative tasks.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Why does the rename fail?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;I think that what&apos;s happening is that the temporary task output files are being deleted once the job has completed, which is causing the &lt;tt&gt;rename&lt;/tt&gt; to fail because &lt;tt&gt;FileOutputCommitter.commitTask&lt;/tt&gt; doesn&apos;t seem to guard against missing output files.&lt;/p&gt;

&lt;p&gt;I&apos;m not sure about this, though, since the stack trace seems to imply that the temporary output file existed.  Maybe the filesystem methods are returning stale metadata?  Maybe there&apos;s a race?  I think a race condition seems pretty unlikely, since the time-scale at which it would have to happen doesn&apos;t sync up with the scale of the timestamps that I saw in the user report.&lt;/p&gt;

&lt;h3&gt;&lt;a name=&quot;PossibleFixes%3A&quot;&gt;&lt;/a&gt;Possible Fixes:&lt;/h3&gt;

&lt;p&gt;The root problem here might be that speculative copies of tasks are somehow allowed to commit their output.  We might be able to fix this by centralizing the &quot;should this task commit its output&quot; decision at the driver.&lt;/p&gt;

&lt;p&gt;(I have more concrete suggestions of how to do this; to be posted soon)&lt;/p&gt;</description>
                <environment></environment>
        <key id="12762466">SPARK-4879</key>
            <summary>Missing output partitions after job completes with speculative execution</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="joshrosen">Josh Rosen</assignee>
                                    <reporter username="joshrosen">Josh Rosen</reporter>
                        <labels>
                    </labels>
                <created>Thu, 18 Dec 2014 02:15:31 +0000</created>
                <updated>Sat, 26 Sep 2015 20:28:24 +0000</updated>
                            <resolved>Sat, 26 Sep 2015 20:28:24 +0000</resolved>
                                    <version>1.0.2</version>
                    <version>1.1.1</version>
                    <version>1.2.0</version>
                    <version>1.3.0</version>
                                    <fixVersion>1.3.0</fixVersion>
                                    <component>Input/Output</component>
                    <component>Spark Core</component>
                        <due></due>
                            <votes>3</votes>
                                    <watches>26</watches>
                                                                                                                <comments>
                            <comment id="14268530" author="zfry" created="Wed, 7 Jan 2015 23:47:31 +0000"  >&lt;p&gt;Hey Josh, &lt;/p&gt;

&lt;p&gt;I have been playing around with your repro above and I think I can consistently trigger the bad behavior by just tweaking the value of &lt;tt&gt;spark.speculation.multiplier&lt;/tt&gt; and &lt;tt&gt;spark.speculation.quantile&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;I set the &lt;tt&gt;multiplier&lt;/tt&gt; to be 1 and the &lt;tt&gt;quantile&lt;/tt&gt; to 0.01 so that only 1% of tasks have to finish before any task that takes longer than those 1% of tasks should speculate. &lt;br/&gt;
As expected, I see a lot of tasks getting speculated. &lt;br/&gt;
After running the repro about 5 times, I have seen 2 errors (stack traces at the bottom and the full run from the REPL is attached with this comment). &lt;/p&gt;

&lt;p&gt;One thing I do notice is that the part-00000 associated with Stage 1 was always where I expected it to be in HDFS, and all lines were present (checked using a &lt;tt&gt;wc -l&lt;/tt&gt;)&lt;/p&gt;


&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;scala&amp;gt; 15/01/07 13:44:26 WARN scheduler.TaskSetManager: Lost task 0.1 in stage 0.0 (TID 119, &amp;lt;redacted-host-02&amp;gt;): java.io.IOException: The temporary job-output directory hdfs:&lt;span class=&quot;code-comment&quot;&gt;//&amp;lt;redacted-host-01&amp;gt;:8020/test6/_temporary doesn&apos;t exist!
&lt;/span&gt;        org.apache.hadoop.mapred.FileOutputCommitter.getWorkPath(FileOutputCommitter.java:250)
        org.apache.hadoop.mapred.FileOutputFormat.getTaskOutputPath(FileOutputFormat.java:240)
        org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:116)
        org.apache.spark.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
        org.apache.spark.rdd.PairRDDFunctions$$anonfun$13.apply(PairRDDFunctions.scala:980)
        org.apache.spark.rdd.PairRDDFunctions$$anonfun$13.apply(PairRDDFunctions.scala:974)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
        org.apache.spark.scheduler.Task.run(Task.scala:54)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;15/01/07 15:17:39 WARN scheduler.TaskSetManager: Lost task 0.1 in stage 0.0 (TID 120, &amp;lt;redacted-host-03&amp;gt;): org.apache.hadoop.ipc.RemoteException: No lease on /test7/_temporary/_attempt_201501071517_0000_m_000000_120/part-00000: File does not exist. Holder DFSClient_NONMAPREDUCE_-469253416_73 does not have any open files.
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2609)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:2426)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2339)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:501)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:299)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:44954)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:453)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1002)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1752)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1748)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1438)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1746)

        org.apache.hadoop.ipc.Client.call(Client.java:1238)
        org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:202)
        com.sun.proxy.$Proxy9.addBlock(Unknown Source)
        sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        java.lang.reflect.Method.invoke(Method.java:606)
        org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:164)
        org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:83)
        com.sun.proxy.$Proxy9.addBlock(Unknown Source)
        org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:291)
        org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1177)
        org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1030)
        org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:488)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="14271788" author="joshrosen" created="Fri, 9 Jan 2015 19:57:44 +0000"  >&lt;p&gt;Hey Zach,&lt;/p&gt;

&lt;p&gt;From my last round of attempts (maybe a week ago), I was able to reproduce some sort of &quot;No lease on ....&quot; error but not the missing output partitions symptom.  I&apos;d still like to try to find a reproduction for the missing files but this could be tricky if it involves a very quick race condition; I don&apos;t think that&apos;s the case, though, since it&apos;s pretty unlikely that the speculated and original task are finishing at the exact same time.&lt;/p&gt;</comment>
                            <comment id="14272291" author="zfry" created="Sat, 10 Jan 2015 02:50:17 +0000"  >&lt;p&gt;Hey Josh,&lt;/p&gt;

&lt;p&gt;I was able to reproduce the missing file using the speculation settings in my previous comment:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;scala&amp;gt; 15/01/09 18:33:28 WARN scheduler.TaskSetManager: Lost task 42.1 in stage 0.0 (TID 113, &amp;lt;redacted-03&amp;gt;): java.io.IOException: Failed to save output of task: attempt_201501091833_0000_m_000042_113
        org.apache.hadoop.mapred.FileOutputCommitter.moveTaskOutputs(FileOutputCommitter.java:160)
        org.apache.hadoop.mapred.FileOutputCommitter.moveTaskOutputs(FileOutputCommitter.java:172)
        org.apache.hadoop.mapred.FileOutputCommitter.commitTask(FileOutputCommitter.java:132)
        org.apache.spark.SparkHadoopWriter.commit(SparkHadoopWriter.scala:109)
        org.apache.spark.rdd.PairRDDFunctions$$anonfun$13.apply(PairRDDFunctions.scala:991)
        org.apache.spark.rdd.PairRDDFunctions$$anonfun$13.apply(PairRDDFunctions.scala:974)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
        org.apache.spark.scheduler.Task.run(Task.scala:54)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
15/01/09 18:33:47 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, &amp;lt;redacted-03&amp;gt;): java.io.IOException: The temporary job-output directory hdfs:&lt;span class=&quot;code-comment&quot;&gt;//&amp;lt;redacted-01&amp;gt;:8020/test2/_temporary doesn&apos;t exist!
&lt;/span&gt;        org.apache.hadoop.mapred.FileOutputCommitter.getWorkPath(FileOutputCommitter.java:250)
        org.apache.hadoop.mapred.FileOutputFormat.getTaskOutputPath(FileOutputFormat.java:240)
        org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:116)
        org.apache.spark.SparkHadoopWriter.open(SparkHadoopWriter.scala:89)
        org.apache.spark.rdd.PairRDDFunctions$$anonfun$13.apply(PairRDDFunctions.scala:980)
        org.apache.spark.rdd.PairRDDFunctions$$anonfun$13.apply(PairRDDFunctions.scala:974)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
        org.apache.spark.scheduler.Task.run(Task.scala:54)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Notice here that there are only 99 part files and part-00042 is missing (as seen in the stacktrace above)&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;  $ hadoop fs -ls /test2 | grep part | wc -l
99
 $ hadoop fs -ls /test2 | grep part-0004
-rw-r--r--   3 &amp;lt;redacted&amp;gt; supergroup          8 2015-01-09 18:33 /test2/part-00040
-rw-r--r--   3 &amp;lt;redacted&amp;gt; supergroup          8 2015-01-09 18:33 /test2/part-00041
-rw-r--r--   3 &amp;lt;redacted&amp;gt; supergroup          8 2015-01-09 18:33 /test2/part-00043
-rw-r--r--   3 &amp;lt;redacted&amp;gt; supergroup          8 2015-01-09 18:33 /test2/part-00044
-rw-r--r--   3 &amp;lt;redacted&amp;gt; supergroup          8 2015-01-09 18:33 /test2/part-00045
-rw-r--r--   3 &amp;lt;redacted&amp;gt; supergroup          8 2015-01-09 18:33 /test2/part-00046
-rw-r--r--   3 &amp;lt;redacted&amp;gt; supergroup          8 2015-01-09 18:33 /test2/part-00047
-rw-r--r--   3 &amp;lt;redacted&amp;gt; supergroup          8 2015-01-09 18:33 /test2/part-00048
-rw-r--r--   3 &amp;lt;redacted&amp;gt; supergroup          8 2015-01-09 18:33 /test2/part-00049
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
</comment>
                            <comment id="14272292" author="zfry" created="Sat, 10 Jan 2015 02:51:02 +0000"  >&lt;p&gt;For clarity, here is the scala code I used in the REPL:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;scala&amp;gt;     val numTasks = 100
numTasks: Int = 100

scala&amp;gt;     sc.parallelize(1 to numTasks, numTasks).mapPartitionsWithContext { &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; (ctx, iter) =&amp;gt;
     |       &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (ctx.partitionId == 0) {  &lt;span class=&quot;code-comment&quot;&gt;// If &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; is the one task that should run really slow
&lt;/span&gt;     |         &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (ctx.attemptId == 0) {  &lt;span class=&quot;code-comment&quot;&gt;// If &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; is the first attempt, run slow
&lt;/span&gt;     |          &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.sleep(20 * 1000)
     |         }
     |       }
     |       iter
     |     }.map(x =&amp;gt; (x, x)).saveAsTextFile(&lt;span class=&quot;code-quote&quot;&gt;&quot;/test2&quot;&lt;/span&gt;)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="14274019" author="joshrosen" created="Mon, 12 Jan 2015 19:31:15 +0000"  >&lt;p&gt;I think that part of the reproduction issues that I had might have been due to &lt;tt&gt;attemptId&lt;/tt&gt; returning a unique task attempt ID rather than the attempt number, meaning that only the &lt;em&gt;first&lt;/em&gt; run of that test in the REPL would be capable of uncovering the bug.&lt;/p&gt;

&lt;p&gt;See &lt;a href=&quot;https://github.com/apache/spark/pull/3849&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/3849&lt;/a&gt; / &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-4014&quot; title=&quot;TaskContext.attemptId returns taskId&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-4014&quot;&gt;&lt;del&gt;SPARK-4014&lt;/del&gt;&lt;/a&gt; for more context.  I&apos;m going to try to merge that patch today, which will let me write a reliable regression test.&lt;/p&gt;</comment>
                            <comment id="14279406" author="joshrosen" created="Thu, 15 Jan 2015 22:29:16 +0000"  >&lt;p&gt;I&apos;m not sure that SparkHadoopWriter&apos;s use of FileOutputCommitter properly obeys the OutputCommitter contracts in Hadoop.  According to the &lt;a href=&quot;https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/OutputCommitter.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;OutputCommitter Javadoc&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The methods in this class can be called from several different processes and from several different contexts. It is important to know which process and which context each is called from. Each method should be marked accordingly in its documentation. It is also important to note that not all methods are guaranteed to be called once and only once. If a method is not guaranteed to have this property the output committer needs to handle this appropriately. Also note it will only be in rare situations where they may be called multiple times for the same task.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Based on the documentation, `needsTaskCommit` &quot; is called from each individual task&apos;s process that will output to HDFS, and it is called just for that task.&quot;, so it seems like it should be safe to call this from SparkHadoopWriter.&lt;/p&gt;

&lt;p&gt;However, maybe we&apos;re misusing the `commitTask` method:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If needsTaskCommit(TaskAttemptContext) returns true and this task is the task that the AM determines finished first, this method is called to commit an individual task&apos;s output. This is to mark that tasks output as complete, as commitJob(JobContext) will also be called later on if the entire job finished successfully. This is called from a task&apos;s process. This may be called multiple times for the same task, but different task attempts. It should be very rare for this to be called multiple times and requires odd networking failures to make this happen. In the future the Hadoop framework may eliminate this race. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think that we&apos;re missing the &quot;this task is the task that the AM determines finished first&quot; part of the equation here.  If `needsTaskCommit` is false, then we definitely shouldn&apos;t commit (e.g. if it&apos;s an original task that lost to a speculated copy), but if it&apos;s true then I don&apos;t think it&apos;s safe to commit; we need some central authority to pick a winner.&lt;/p&gt;

&lt;p&gt;Let&apos;s see how Hadoop does things, working backwards from actual calls of `commitTask` to see whether they&apos;re guarded by some coordination through the AM.  It looks like `OutputCommitter` is part of the `mapred` API, so I&apos;ll only look at classes in that package:&lt;/p&gt;

&lt;p&gt;In `Task.java`, `committer.commitTask` is only performed after checking `canCommit` through `TaskUmbilicalProtocol`: &lt;a href=&quot;https://github.com/apache/hadoop/blob/a655973e781caf662b360c96e0fa3f5a873cf676/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java#L1185&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/hadoop/blob/a655973e781caf662b360c96e0fa3f5a873cf676/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java#L1185&lt;/a&gt;.  According to the Javadocs for TaskAttemptListenerImpl.canCommit (the actual concrete implementation of this method):&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;  /**
   * Child checking whether it can commit.
   * 
   * &amp;lt;br/&amp;gt;
   * Commit is a two-phased protocol. First the attempt informs the
   * ApplicationMaster that it is
   * {@link #commitPending(TaskAttemptID, TaskStatus)}. Then it repeatedly polls
   * the ApplicationMaster whether it {@link #canCommit(TaskAttemptID)} This is
   * a legacy from the centralized commit protocol handling by the JobTracker.
   */
  @Override
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; canCommit(TaskAttemptID taskAttemptID) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This ends up delegating to `Task.canCommit()`:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;  /**
   * Can the output of the taskAttempt be committed. Note that once the task
   * gives a go &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; a commit, further canCommit requests from any other attempts
   * should &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;.
   * 
   * @param taskAttemptID
   * @&lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; whether the attempt&apos;s output can be committed or not.
   */
  &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; canCommit(TaskAttemptId taskAttemptID);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There&apos;s a bunch of tricky logic that involves communication with the AM (see AttemptCommitPendingTransition and the other transitions in TaskImpl), but it looks like the gist is that the &quot;winner&quot; is picked by the AM through some central coordination process. &lt;/p&gt;

&lt;p&gt;So, it looks like the right fix is to implement these same state transitions ourselves.  It would be nice if there was a clean way to do this that could be easily backported to maintenance branches.  &lt;/p&gt;</comment>
                            <comment id="14279711" author="apachespark" created="Fri, 16 Jan 2015 02:18:09 +0000"  >&lt;p&gt;User &apos;JoshRosen&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/4066&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/4066&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14286771" author="apachespark" created="Thu, 22 Jan 2015 02:02:17 +0000"  >&lt;p&gt;User &apos;mccheah&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/4155&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/4155&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14315077" author="joshrosen" created="Tue, 10 Feb 2015 22:21:45 +0000"  >&lt;p&gt;This issue is &lt;em&gt;really&lt;/em&gt; hard to reproduce, but I managed to trigger the original bug as part of the testing for my patch.  Here&apos;s what I ran:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;~/spark-1.3.0-SNAPSHOT-bin-1.0.4/bin/spark-shell --conf spark.speculation.multiplier=1 --conf spark.speculation.quantile=0.01 --conf spark.speculation=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; --conf  spark.hadoop.outputCommitCoordination.enabled=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;val numTasks = 100
val numTrials = 100
val outputPath = &lt;span class=&quot;code-quote&quot;&gt;&quot;/output-committer-bug-&quot;&lt;/span&gt;
val sleepDuration = 1000

&lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (trial &amp;lt;- 0 to (numTrials - 1)) {
  val outputLocation = outputPath + trial
  sc.parallelize(1 to numTasks, numTasks).mapPartitionsWithContext { &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; (ctx, iter) =&amp;gt;
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (ctx.partitionId % 5 == 0) {
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (ctx.attemptNumber == 0) {  &lt;span class=&quot;code-comment&quot;&gt;// If &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; is the first attempt, run slow
&lt;/span&gt;       &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.sleep(sleepDuration)
      }
    }
    iter
  }.map(identity).saveAsTextFile(outputLocation)
  &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.sleep(sleepDuration * 2)
  println(&lt;span class=&quot;code-quote&quot;&gt;&quot;TESTING OUTPUT OF TRIAL &quot;&lt;/span&gt; + trial)
  val savedData = sc.textFile(outputLocation).map(_.toInt).collect()
  &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (savedData.toSet != (1 to numTasks).toSet) {
    println(&lt;span class=&quot;code-quote&quot;&gt;&quot;MISSING: &quot;&lt;/span&gt; + ((1 to numTasks).toSet -- savedData.toSet))
    &lt;span class=&quot;code-keyword&quot;&gt;assert&lt;/span&gt;(&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;)
  }
  println(&lt;span class=&quot;code-quote&quot;&gt;&quot;-&quot;&lt;/span&gt; * 80)
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It took 22 runs until I actually observed missing output partitions (several of the earlier runs threw spurious exceptions and didn&apos;t have missing outputs):&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;[...]
15/02/10 22:17:21 INFO scheduler.DAGScheduler: Job 66 finished: saveAsTextFile at &amp;lt;console&amp;gt;:39, took 2.479592 s
15/02/10 22:17:21 WARN scheduler.TaskSetManager: Lost task 75.0 in stage 66.0 (TID 6861, ip-172-31-1-124.us-west-2.compute.internal): java.io.IOException: Failed to save output of task: attempt_201502102217_0066_m_000075_6861
        at org.apache.hadoop.mapred.FileOutputCommitter.moveTaskOutputs(FileOutputCommitter.java:160)
        at org.apache.hadoop.mapred.FileOutputCommitter.moveTaskOutputs(FileOutputCommitter.java:172)
        at org.apache.hadoop.mapred.FileOutputCommitter.commitTask(FileOutputCommitter.java:132)
        at org.apache.spark.SparkHadoopWriter.performCommit$1(SparkHadoopWriter.scala:113)
        at org.apache.spark.SparkHadoopWriter.commit(SparkHadoopWriter.scala:150)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$13.apply(PairRDDFunctions.scala:1082)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$13.apply(PairRDDFunctions.scala:1059)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        at org.apache.spark.scheduler.Task.run(Task.scala:64)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:197)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)

15/02/10 22:17:21 WARN scheduler.TaskSetManager: Lost task 80.0 in stage 66.0 (TID 6866, ip-172-31-11-151.us-west-2.compute.internal): java.io.IOException: The temporary job-output directory hdfs:&lt;span class=&quot;code-comment&quot;&gt;//ec2-54-213-142-80.us-west-2.compute.amazonaws.com:9000/output-committer-bug-22/_temporary doesn&apos;t exist!
&lt;/span&gt;        at org.apache.hadoop.mapred.FileOutputCommitter.getWorkPath(FileOutputCommitter.java:250)
        at org.apache.hadoop.mapred.FileOutputFormat.getTaskOutputPath(FileOutputFormat.java:244)
        at org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:116)
        at org.apache.spark.SparkHadoopWriter.open(SparkHadoopWriter.scala:91)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$13.apply(PairRDDFunctions.scala:1068)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$13.apply(PairRDDFunctions.scala:1059)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        at org.apache.spark.scheduler.Task.run(Task.scala:64)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:197)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)

15/02/10 22:17:21 INFO scheduler.TaskSetManager: Lost task 85.0 in stage 66.0 (TID 6871) on executor ip-172-31-1-124.us-west-2.compute.internal: java.io.IOException (The temporary job-output directory hdfs:&lt;span class=&quot;code-comment&quot;&gt;//ec2-54-213-142-80.us-west-2.compute.amazonaws.com:9000/output-committer-bug-22/_temporary doesn&apos;t exist!) [duplicate 1]
&lt;/span&gt;15/02/10 22:17:21 INFO scheduler.TaskSetManager: Lost task 90.0 in stage 66.0 (TID 6876) on executor ip-172-31-1-124.us-west-2.compute.internal: java.io.IOException (The temporary job-output directory hdfs:&lt;span class=&quot;code-comment&quot;&gt;//ec2-54-213-142-80.us-west-2.compute.amazonaws.com:9000/output-committer-bug-22/_temporary doesn&apos;t exist!) [duplicate 2]
&lt;/span&gt;15/02/10 22:17:21 INFO scheduler.TaskSetManager: Lost task 95.0 in stage 66.0 (TID 6881) on executor ip-172-31-11-151.us-west-2.compute.internal: java.io.IOException (The temporary job-output directory hdfs:&lt;span class=&quot;code-comment&quot;&gt;//ec2-54-213-142-80.us-west-2.compute.amazonaws.com:9000/output-committer-bug-22/_temporary doesn&apos;t exist!) [duplicate 3]
&lt;/span&gt;15/02/10 22:17:21 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 66.0, whose tasks have all completed, from pool
TESTING OUTPUT OF TRIAL 22
[...]
15/02/10 22:17:23 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 67.0, whose tasks have all completed, from pool
15/02/10 22:17:23 INFO scheduler.DAGScheduler: Stage 67 (collect at &amp;lt;console&amp;gt;:42) finished in 0.158 s
15/02/10 22:17:23 INFO scheduler.DAGScheduler: Job 67 finished: collect at &amp;lt;console&amp;gt;:42, took 0.165580 s
MISSING: Set(76)
java.lang.AssertionError: assertion failed
[...]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And I confirmed that it&apos;s missing in HDFS:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;~/ephemeral-hdfs/bin/hadoop fs -ls /output-committer-bug-22 | grep part | wc -l
Warning: $HADOOP_HOME is deprecated.

99
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To test my patch, I&apos;m going to remove the flag that disables it and run this for a huge number of trials to ensure that we don&apos;t hit the missing output bug.&lt;/p&gt;</comment>
                            <comment id="14315087" author="aash" created="Tue, 10 Feb 2015 22:27:05 +0000"  >&lt;p&gt;This is really great work &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=joshrosen&quot; class=&quot;user-hover&quot; rel=&quot;joshrosen&quot;&gt;joshrosen&lt;/a&gt;!  I really appreciate the effort you&apos;re putting into getting this one figured out since these kind of non-deterministic bugs are the most painful for both users and devs to figure out.&lt;/p&gt;</comment>
                            <comment id="14315918" author="mcheah" created="Wed, 11 Feb 2015 09:59:10 +0000"  >&lt;p&gt;Thanks for picking this up &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=joshrosen&quot; class=&quot;user-hover&quot; rel=&quot;joshrosen&quot;&gt;joshrosen&lt;/a&gt;, I see that the hotly-debated patch has finally merged in!&lt;/p&gt;

&lt;p&gt;I&apos;m a bit backed up but there will be others from my side that will be testing this. We&apos;ll comment on the bug once we have verified the issue is fixed on our side.&lt;/p&gt;</comment>
                            <comment id="14324278" author="romi-totango" created="Tue, 17 Feb 2015 15:14:07 +0000"  >&lt;p&gt;Could this happen very very rarely when not using speculative execution?&lt;br/&gt;
Once in a long while, I have a situation where the OutputCommitter says it wrote the file successfully, but the output file doesn&apos;t appear there.&lt;/p&gt;</comment>
                            <comment id="14324418" author="aash" created="Tue, 17 Feb 2015 16:40:15 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=romi-totango&quot; class=&quot;user-hover&quot; rel=&quot;romi-totango&quot;&gt;romi-totango&lt;/a&gt; what filesystem are you writing to?  HDFS / S3 / other ?  If this was HDFS then I think you may have a bug, but if it was S3 then there are loose eventual consistency guarantees that might be surprising you.&lt;/p&gt;</comment>
                            <comment id="14325573" author="romi-totango" created="Wed, 18 Feb 2015 08:28:54 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aash&quot; class=&quot;user-hover&quot; rel=&quot;aash&quot;&gt;aash&lt;/a&gt; I&apos;m writing to S3. I&apos;m aware of eventual consistency issues, but is it possible that 99.999% files are written and available immediately, and then one file doesn&apos;t appear for very long? (couldn&apos;t afford to wait until it was available so I ran the process again and it wrote the file immediately).&lt;br/&gt;
Still isn&apos;t it somehow possibly related to this bug or similar, that Spark&apos;s output writing mechanism thinks it wrote the file, but actually it did not?&lt;/p&gt;</comment>
                            <comment id="14325969" author="aash" created="Wed, 18 Feb 2015 14:52:31 +0000"  >&lt;p&gt;That sounds exactly like what I&apos;d expect to happen in S3.  The eventual consistency bug is that S3 reports something is written, Spark then believes it has written the file, but when you go to access it the file isn&apos;t there yet (at least in the us standard region.  If you use another region, there is read-after-write consistency).&lt;/p&gt;

&lt;p&gt;In short, it doesn&apos;t sound like you&apos;re encountering this particular issue with speculative execution (&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-4879&quot; title=&quot;Missing output partitions after job completes with speculative execution&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-4879&quot;&gt;&lt;del&gt;SPARK-4879&lt;/del&gt;&lt;/a&gt;), just normal S3 consistency trickiness.&lt;/p&gt;</comment>
                            <comment id="14348522" author="mridulm80" created="Thu, 5 Mar 2015 10:08:15 +0000"  >&lt;p&gt;&lt;br/&gt;
With 1.3 RC, we are still seeing this issue.&lt;/p&gt;</comment>
                            <comment id="14348527" author="mridulm80" created="Thu, 5 Mar 2015 10:13:25 +0000"  >&lt;p&gt;Just to add - we are seeing this on hdfs.&lt;br/&gt;
Out of the 75k output files to be written, about 53k did not get moved.&lt;/p&gt;

&lt;p&gt;But on waiting for a really long time (about 20 minutes) all files eventually got moved.&lt;br/&gt;
Is this something to do with FileOutputCommitter itself or side-effect of the patch committed ?&lt;/p&gt;

&lt;p&gt;We have speculative execution turned on; even though the saveAsHadoopFile taskset completes (as per logs), the call itself does not return until entire &apos;move&apos; is done - just that it took more time to commit the output than to generate it in first place !&lt;/p&gt;</comment>
                            <comment id="14348590" author="joshrosen" created="Thu, 5 Mar 2015 10:52:16 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mridulm80&quot; class=&quot;user-hover&quot; rel=&quot;mridulm80&quot;&gt;mridulm80&lt;/a&gt;, when you say that &quot;the call itself does not return until entire &apos;move&apos; is done&quot;, do you mean that the &lt;tt&gt;saveAsHadoopFile()&lt;/tt&gt; call blocks in &lt;tt&gt;saveAsHadoopDataset()&lt;/tt&gt;&apos;s final {{   writer.commitJob()}} call?  This ticket addresses a bug where output partitions are missing &lt;em&gt;after&lt;/em&gt; control returns from the &lt;tt&gt;saveAsHadoop*()&lt;/tt&gt; call.&lt;/p&gt;

&lt;p&gt;In the issue addressed by this ticket, partitions were deleted after the job and save action had completed and output remained missing.  It sounds like you might be describing a different issue where jobs take a long time to commit.  For your workload, is the behavior that you are observing a performance regression in Spark 1.3?  If you&apos;d like to run 1.3 without the effects of my patch, just set &lt;tt&gt;spark.hadoop.outputCommitCoordination.enabled=false&lt;/tt&gt; in your SparkConf to bypass the new output commit coordination logic.&lt;/p&gt;</comment>
                            <comment id="14348987" author="mridulm80" created="Thu, 5 Mar 2015 16:03:31 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=joshrosen&quot; class=&quot;user-hover&quot; rel=&quot;joshrosen&quot;&gt;joshrosen&lt;/a&gt; The former - the call blocks on saveAsHadoopFile (the very next line was sc.stop() and then exit).&lt;/p&gt;

&lt;p&gt;We cannot use &apos;spark.hadoop.outputCommitCoordination.enabled=false&apos; since we need to have speculative execution on (turning it off drastically affects job runtime) and disabling commit coordination causes other issues (as reported in this jira).&lt;/p&gt;

&lt;p&gt;I was not sure if &lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;the dramatically increased to commit was due to the PR from this jira.&lt;/li&gt;
	&lt;li&gt;is orthogonal to this jira and an issue with FileOutputCommitter (as was seen which searching for cause online).&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Given the central coordination blocking on master, I wanted to eliminate causes from our end before digging into FIleOutputCommitter and/or Namenode code.&lt;/p&gt;</comment>
                            <comment id="14349188" author="mcheah" created="Thu, 5 Mar 2015 18:14:25 +0000"  >&lt;p&gt;Can you perhaps jstack or profile the driver and the executors to see if they&apos;re blocked in OutputCommitCoordinator logic?&lt;/p&gt;</comment>
                            <comment id="14572429" author="zhpengg" created="Thu, 4 Jun 2015 09:18:47 +0000"  >&lt;p&gt;Hi, is there any updates for this issue?&lt;/p&gt;</comment>
                            <comment id="14586266" author="igor.berman" created="Mon, 15 Jun 2015 16:28:01 +0000"  >&lt;p&gt;I&apos;m experiencing this issue. Sometimes rdd with 4 partitions is written with 3 parts and _SUCCESS marker is there.&lt;/p&gt;</comment>
                            <comment id="14620570" author="darabos" created="Thu, 9 Jul 2015 14:26:32 +0000"  >&lt;p&gt;I have a fairly reliable reproduction with Spark 1.4.0 and HDFS. I&apos;m running on 10 EC2 m3.2xlarge instances using the ephemeral HDFS. If &lt;tt&gt;spark.speculation&lt;/tt&gt; is true, I get hit by this 50% of the time or more. It&apos;s a fairly complex workload, not something you can test in a &lt;tt&gt;spark-shell&lt;/tt&gt;. What I saw was that I saved a 400-partition RDD with &lt;tt&gt;saveAsNewAPIHadoopFile&lt;/tt&gt; (which returned without error) and when I tried to read it back, the files for partitions 323 and 324 were missing. (In the case that I took a closer look at.) I don&apos;t have the logs at hand now, but it&apos;s like you describe I think (&lt;tt&gt;Failed to save output of task&lt;/tt&gt;). I can add them later if it would be useful.&lt;/p&gt;

&lt;p&gt;I turned off &lt;tt&gt;spark.speculation&lt;/tt&gt; and haven&apos;t seen the issue since.&lt;/p&gt;

&lt;p&gt;Is there anything I could do to help debug this issue?&lt;/p&gt;</comment>
                            <comment id="14622226" author="darabos" created="Fri, 10 Jul 2015 12:24:52 +0000"  >&lt;p&gt;I wonder if this issue is serious enough to note in the documentation. What do you think about adding a big fat warning for speculative execution until it is fixed? &quot;Enabling speculative execution may lead to missing output files&quot;? Or perhaps add a verification pass that checks if all the outputs are present and raises an exception if not.&lt;/p&gt;

&lt;p&gt;Silently dropping output files is a horrible bug. We&apos;ve been debugging a somewhat mythological data corruption issue for about a month, and now we realize that this issue (&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-4879&quot; title=&quot;Missing output partitions after job completes with speculative execution&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-4879&quot;&gt;&lt;del&gt;SPARK-4879&lt;/del&gt;&lt;/a&gt;) is a very plausible explanation. We have never been able to reproduce it, but we have a log file, and it shows a speculative task for a &lt;tt&gt;saveAsNewAPIHadoopFile&lt;/tt&gt; stage.&lt;/p&gt;</comment>
                            <comment id="14623956" author="joshrosen" created="Sun, 12 Jul 2015 19:14:32 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=darabos&quot; class=&quot;user-hover&quot; rel=&quot;darabos&quot;&gt;darabos&lt;/a&gt;, do you think that this issue might have been resolved in an earlier Spark version but inadvertently broken in the upgrade to 1.4.0?  If you have an easy reproduction, it might be helpful to see whether the problem occurs on 1.3.1.&lt;/p&gt;</comment>
                            <comment id="14624000" author="darabos" created="Sun, 12 Jul 2015 20:40:00 +0000"  >&lt;p&gt;Good idea! I&apos;ll try with 1.3.1 next week.&lt;/p&gt;</comment>
                            <comment id="14628086" author="darabos" created="Wed, 15 Jul 2015 14:01:03 +0000"  >&lt;p&gt;I&apos;ve managed to reproduce on Spark 1.3.1 too (pre-built for Hadoop 1). I ran on EC2 with the &lt;tt&gt;spark-ec2&lt;/tt&gt; script and used the ephemeral HDFS. I used a 5-machine cluster and repeatedly ran a complex test suite for about 30 minutes until the error was triggered. Here are the relevant logs:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;I2015-07-15 13:45:19,954 TaskSetManager:[task-result-getter-2] Finished task 198.0 in stage 320.0 (TID 13290) in 568 ms on ip-10-153-188-224.ec2.internal (195/200)
I2015-07-15 13:45:21,174 TaskSetManager:[sparkDriver-akka.actor.default-dispatcher-2] Marking task 197 in stage 320.0 (on ip-10-231-214-6.ec2.internal) as speculatable because it ran more than 1240 ms
I2015-07-15 13:45:21,174 TaskSetManager:[sparkDriver-akka.actor.default-dispatcher-2] Marking task 176 in stage 320.0 (on ip-10-231-214-6.ec2.internal) as speculatable because it ran more than 1240 ms
I2015-07-15 13:45:21,174 TaskSetManager:[sparkDriver-akka.actor.default-dispatcher-2] Marking task 194 in stage 320.0 (on ip-10-231-214-6.ec2.internal) as speculatable because it ran more than 1240 ms
I2015-07-15 13:45:21,174 TaskSetManager:[sparkDriver-akka.actor.default-dispatcher-2] Marking task 196 in stage 320.0 (on ip-10-231-214-6.ec2.internal) as speculatable because it ran more than 1240 ms
I2015-07-15 13:45:21,175 TaskSetManager:[sparkDriver-akka.actor.default-dispatcher-2] Marking task 195 in stage 320.0 (on ip-10-231-214-6.ec2.internal) as speculatable because it ran more than 1240 ms
I2015-07-15 13:45:21,175 TaskSetManager:[sparkDriver-akka.actor.default-dispatcher-2] Starting task 197.1 in stage 320.0 (TID 13292, ip-10-153-188-224.ec2.internal, PROCESS_LOCAL, 1612 bytes)
I2015-07-15 13:45:21,175 TaskSetManager:[sparkDriver-akka.actor.default-dispatcher-2] Starting task 176.1 in stage 320.0 (TID 13293, ip-10-63-27-248.ec2.internal, PROCESS_LOCAL, 1612 bytes)
I2015-07-15 13:45:21,175 TaskSetManager:[sparkDriver-akka.actor.default-dispatcher-2] Starting task 194.1 in stage 320.0 (TID 13294, ip-10-154-1-239.ec2.internal, PROCESS_LOCAL, 1612 bytes)
I2015-07-15 13:45:21,175 TaskSetManager:[sparkDriver-akka.actor.default-dispatcher-2] Starting task 195.1 in stage 320.0 (TID 13295, ip-10-228-67-34.ec2.internal, PROCESS_LOCAL, 1612 bytes)
I2015-07-15 13:45:21,175 TaskSetManager:[sparkDriver-akka.actor.default-dispatcher-2] Starting task 196.1 in stage 320.0 (TID 13296, ip-10-153-188-224.ec2.internal, PROCESS_LOCAL, 1612 bytes)
I2015-07-15 13:45:21,402 TaskSetManager:[task-result-getter-3] Finished task 176.0 in stage 320.0 (TID 13268) in 2223 ms on ip-10-231-214-6.ec2.internal (196/200)
I2015-07-15 13:45:21,445 TaskSetManager:[task-result-getter-0] Finished task 195.0 in stage 320.0 (TID 13287) in 2113 ms on ip-10-231-214-6.ec2.internal (197/200)
I2015-07-15 13:45:21,461 TaskSetManager:[task-result-getter-1] Finished task 196.1 in stage 320.0 (TID 13296) in 285 ms on ip-10-153-188-224.ec2.internal (198/200)
I2015-07-15 13:45:21,464 TaskSetManager:[task-result-getter-2] Finished task 194.1 in stage 320.0 (TID 13294) in 287 ms on ip-10-154-1-239.ec2.internal (199/200)
I2015-07-15 13:45:21,465 TaskSetManager:[task-result-getter-3] Ignoring task-finished event for 176.1 in stage 320.0 because task 176 has already completed successfully
I2015-07-15 13:45:21,468 TaskSetManager:[task-result-getter-0] Finished task 197.1 in stage 320.0 (TID 13292) in 292 ms on ip-10-153-188-224.ec2.internal (200/200)
I2015-07-15 13:45:21,468 DAGScheduler:[dag-scheduler-event-loop] Stage 320 (saveAsNewAPIHadoopFile at HadoopFile.scala:208) finished in 4.802 s
I2015-07-15 13:45:21,468 DAGScheduler:[DataManager-5] Job 46 finished: saveAsNewAPIHadoopFile at HadoopFile.scala:208, took 4.836626 s
W2015-07-15 13:45:21,478 TaskSetManager:[task-result-getter-1] Lost task 195.1 in stage 320.0 (TID 13295, ip-10-228-67-34.ec2.internal): java.io.IOException: Failed to save output of task: attempt_201507151345_0628_r_000195_1
        at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.moveTaskOutputs(FileOutputCommitter.java:203)
        at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.moveTaskOutputs(FileOutputCommitter.java:214)
        at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:167)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:1009)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$12.apply(PairRDDFunctions.scala:979)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        at org.apache.spark.scheduler.Task.run(Task.scala:64)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I checked the output directory and indeed &lt;tt&gt;part-r-00195&lt;/tt&gt; is missing.&lt;/p&gt;

&lt;p&gt;Our speculative execution configuration was:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;spark.speculation true
spark.speculation.interval 1000
spark.speculation.quantile 0.90
spark.speculation.multiplier 2
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We&apos;ve been running production code with this setup for almost a year. Until now we hadn&apos;t checked for missing files. Such a painful bug.&lt;/p&gt;</comment>
                            <comment id="14628300" author="joshrosen" created="Wed, 15 Jul 2015 16:17:10 +0000"  >&lt;p&gt;I think that this is going to be a complex issue to investigate / fix, so I&apos;d like to break down that task into a series of smaller subtasks.  Here&apos;s my initial proposal (each of these could be done as a separate JIRA / patch / TODO so that we make incremental progress):&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Add automated post-save-action checks to assert that the expected number of output files are present.  This will make debugging much easier.  There are some corner-cases to address related to S3 and _temporary / _SUCCESS files, but I&apos;m sure we can come up with a solution that addresses them.  The error message raised by this assert could have a pointer to this issue and could suggest disabling speculation as a workaround.&lt;/li&gt;
	&lt;li&gt;Write a standalone program which is capable of reproducing the most recently reported set of symptoms for this bug.  The symptoms that originally prompted this ticket were cases where a speculative / duplicated task would continue running after the original job had completed, then fail to commit and delete the destination file.  It&apos;s possible that this issue has resurfaced / wasn&apos;t properly fixed, but it&apos;s also possible that there is a different race which wasn&apos;t addressed by the original patch.
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;One subtask of this may be logging improvements: we might not be able to come up with a reasonably minimal reproduction until we have more granular logs to help us better pinpoint the race.&lt;/li&gt;
		&lt;li&gt;If this is non-deterministic, then we can rig our program to try to expose the race, then have it re-try itself hundreds of times in a loop and stop once a failure occurs.&lt;/li&gt;
		&lt;li&gt;Given the inherent non-determinism, external dependencies, and slow nature of this type of test, I don&apos;t think that this can be an automated test in Jenkins.  However, I think that the code for this test should live in the Spark codebase. We should create a pre-release manual QA checklist and make sure that it includes a task to manually run this test on EC2.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;If we gain a better understanding of the races / problems that lead to this bug, we may be able to mock / stub / simulate / interpose in a way that lets us write a unit test to reproduce this bug.  If that&apos;s possible, then we should do it in order to have a fast-to-run test that can run regularly.&lt;/li&gt;
	&lt;li&gt;Fix the underlying bug.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14628380" author="darabos" created="Wed, 15 Jul 2015 17:17:55 +0000"  >&lt;p&gt;Thanks, Josh! I wonder when the post-save-action check you suggest would run. Based on the above log snippet I think it&apos;s quite likely that at the point when the stage finishes the files are all there. I suspect it&apos;s the speculative task which fails after the stage has finished that deletes the file. It may be hard to check at the right point in time.&lt;/p&gt;

&lt;p&gt;I&apos;ll try to find a smaller reproduction. First it would be great if I could reproduce on my local machine instead of starting EC2 clusters. Then I just need to dig out the key operations from our &lt;del&gt;entangled mess of a&lt;/del&gt; &lt;em&gt;highly sophisticated&lt;/em&gt; codebase.&lt;/p&gt;

&lt;p&gt;One more thing that occurs to me is that perhaps there should never be a line of code that deletes an output file. I haven&apos;t had a chance to dig into the code yet, and I&apos;m sure there is a reason for it, but perhaps the same goal could be accomplished without deleting output files. What do you think?&lt;/p&gt;</comment>
                            <comment id="14631463" author="joshrosen" created="Fri, 17 Jul 2015 15:22:50 +0000"  >&lt;p&gt;That&apos;s a good point about the post-save-action check: code internal to Spark can only perform the check right before we return control back to the user so it&apos;s possible that the check will say that everything&apos;s fine even if the files are later deleted. This check still might have some value, but it&apos;s certainly prone to false-negatives and wouldn&apos;t be able to catch all manifestations of this bug.&lt;/p&gt;

&lt;p&gt;In the original issue that prompted this patch, the deletion of the output file occurred inside of Hadoop code: if the renaming of the temporary file to the final output file fails, Hadoop&apos;s FileOutputCommitter will attempt to delete the final output file before re-attempting the rename (this is explained in a bit more detail, including a code walkthrough, in the description of this JIRA ticket).&lt;/p&gt;</comment>
                            <comment id="14708331" author="igor.berman" created="Sun, 23 Aug 2015 09:54:39 +0000"  >&lt;p&gt;there is blog post &lt;a href=&quot;http://tech.grammarly.com/blog/posts/Petabyte-Scale-Text-Processing-with-Spark.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://tech.grammarly.com/blog/posts/Petabyte-Scale-Text-Processing-with-Spark.html&lt;/a&gt; with a link to gist by Aaron Davidson: &lt;a href=&quot;https://gist.github.com/aarondav/c513916e72101bbe14ec&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://gist.github.com/aarondav/c513916e72101bbe14ec&lt;/a&gt; which mentions in comments that when using speculation it should be DirectOutputCommitter&lt;/p&gt;

&lt;p&gt;might be somebody will find it useful (imho, it should be in documentation)&lt;/p&gt;</comment>
                            <comment id="14742050" author="srowen" created="Sat, 12 Sep 2015 13:02:00 +0000"  >&lt;p&gt;I&apos;m clearing &quot;backport-needed&quot; since it&apos;s virtually certain that there will be no more 1.2.x or earlier releases, and so the fix that was committed won&apos;t go back further at this point.&lt;/p&gt;

&lt;p&gt;Is it something to leave open pending the ongoing conversation here? sounds like there may be more to the fix? &lt;/p&gt;</comment>
                            <comment id="14742453" author="stevel@apache.org" created="Sun, 13 Sep 2015 12:01:18 +0000"  >&lt;p&gt;What about splitting the issue into HDFS commits (interesting its mostly EC2 reports), which is probably fixed, and eventually consistent object stores (S3 on the Apache Hadoop releases (but not amazon&apos;s own), which need a different committer (no rename), better checks for file presence (direct stat() is more reliable than a directory listing), and some dedicated test suite which could be targeted straight at s3 &#8212;yet still runnable remotely by someone (not jenkins) from their own desktop &amp;amp; build servers. That&apos;s essentially what we do in core hadoop to qualify the object stores&apos; base API compatibility.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12690660" name="speculation.txt" size="55223" author="zfry" created="Wed, 7 Jan 2015 23:47:31 +0000"/>
                            <attachment id="12690661" name="speculation2.txt" size="57034" author="zfry" created="Wed, 7 Jan 2015 23:47:31 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 10 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i23k27:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12327642">1.3.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>