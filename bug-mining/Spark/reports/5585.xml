<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:58:57 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-17147] Spark Streaming Kafka 0.10 Consumer Can&apos;t Handle Non-consecutive Offsets (i.e. Log Compaction)</title>
                <link>https://issues.apache.org/jira/browse/SPARK-17147</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;When Kafka does log compaction offsets often end up with gaps, meaning the next requested offset will be frequently not be offset+1. The logic in KafkaRDD &amp;amp; CachedKafkaConsumer has a baked in assumption that the next offset will always be just an increment of 1 above the previous offset. &lt;/p&gt;

&lt;p&gt;I have worked around this problem by changing CachedKafkaConsumer to use the returned record&apos;s offset, from:&lt;br/&gt;
&lt;tt&gt;nextOffset = offset + 1&lt;/tt&gt;&lt;br/&gt;
to:&lt;br/&gt;
&lt;tt&gt;nextOffset = record.offset + 1&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;and changed KafkaRDD from:&lt;br/&gt;
&lt;tt&gt;requestOffset += 1&lt;/tt&gt;&lt;br/&gt;
to:&lt;br/&gt;
&lt;tt&gt;requestOffset = r.offset() + 1&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;(I also had to change some assert logic in CachedKafkaConsumer).&lt;/p&gt;

&lt;p&gt;There&apos;s a strong possibility that I have misconstrued how to use the streaming kafka consumer, and I&apos;m happy to close this out if that&apos;s the case. If, however, it is supposed to support non-consecutive offsets (e.g. due to log compaction) I am also happy to contribute a PR.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12998372">SPARK-17147</key>
            <summary>Spark Streaming Kafka 0.10 Consumer Can&apos;t Handle Non-consecutive Offsets (i.e. Log Compaction)</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="koeninger">Cody Koeninger</assignee>
                                    <reporter username="robert@crunchbase.com">Robert Conrad</reporter>
                        <labels>
                    </labels>
                <created>Fri, 19 Aug 2016 01:33:52 +0000</created>
                <updated>Tue, 24 Apr 2018 11:54:07 +0000</updated>
                            <resolved>Tue, 27 Feb 2018 14:21:26 +0000</resolved>
                                    <version>2.0.0</version>
                                    <fixVersion>2.4.0</fixVersion>
                                    <component>DStreams</component>
                        <due></due>
                            <votes>10</votes>
                                    <watches>23</watches>
                                                                                                                <comments>
                            <comment id="15429742" author="cody@koeninger.org" created="Sun, 21 Aug 2016 14:15:39 +0000"  >&lt;p&gt;Have you successfully used the 0.8 consumer with compacted topics?&lt;br/&gt;
Off the top of my head, i&apos;d expect the count methods and offset ranges to have similar baked in assumptions.&lt;/p&gt;</comment>
                            <comment id="15429846" author="robert@crunchbase.com" created="Sun, 21 Aug 2016 18:30:41 +0000"  >&lt;p&gt;Nope, I started the project with kafka 0.10 and spark 2.0.0-preview. I could take a crack at building a small test project that illuminates the issue, or maybe it would be easier if I wrote a test in whichever spark suite hits the streaming connector. &lt;/p&gt;</comment>
                            <comment id="15430872" author="cody@koeninger.org" created="Mon, 22 Aug 2016 14:33:54 +0000"  >&lt;p&gt;My point is more that this probably isn&apos;t just two lines in CachedKafkaConsumer.  There&apos;s other code, both within the spark streaming connector and in users of the connector, that assumes an offset range from..until has a number of messages equal to (until - from).  I haven&apos;t seen what databricks is coming up with for the structured streaming connector, but I&apos;d imagine that an assumption that offsets are contiguous would certainly simplify that implementation, and might actually be necessary depending on how recovery works.&lt;/p&gt;

&lt;p&gt;This might be a simple as your change plus logging a warning when a stream starts on a compacted topic, but we need to think through the issues here.&lt;/p&gt;</comment>
                            <comment id="15437469" author="graphex" created="Thu, 25 Aug 2016 19:02:32 +0000"  >&lt;p&gt;Compacted topics in Kafka prior to 0.9 had some issues and using topic compaction in 0.8.x is generally not recommended, from what I&apos;ve seen.&lt;/p&gt;</comment>
                            <comment id="15437672" author="graphex" created="Thu, 25 Aug 2016 21:05:59 +0000"  >&lt;p&gt;I tried Robert&apos;s changes, but the performance for any sizable number of reads is really bad. At least the way I understand it, whenever there is a discontiguous offset, it forces Kafka to do a seek, which is extremely slow.&lt;/p&gt;</comment>
                            <comment id="15453229" author="robert@crunchbase.com" created="Wed, 31 Aug 2016 20:09:49 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=graphex&quot; class=&quot;user-hover&quot; rel=&quot;graphex&quot;&gt;graphex&lt;/a&gt; you&apos;re absolutely right about the seek, but that&apos;s exactly how log compaction is designed. How else could it work?&lt;/p&gt;</comment>
                            <comment id="15453884" author="graphex" created="Thu, 1 Sep 2016 00:44:27 +0000"  >&lt;p&gt;I think Kafka&apos;s log compaction&apos;s design is still intended for sequential reading, even if the offsets are not consecutive for a compacted topic. Kafka&apos;s internal log cleaner process copies log segments to new files which have been compacted, so the messages are still stored sequentially even if the offset metadata for them increases by more than one. The typical consumer just does a poll() to get the next records, regardless of their offsets, but this Spark&apos;s CachedKafkaConsumer checks the offset of each record before calling poll(), and if that offset isn&apos;t the previous record&apos;s offset +1, it&apos;s going to call consumer.seek() before the next poll(), which I think is producing the dramatic slowdown I&apos;ve seen.&lt;br/&gt;
It is certainly possible, using a non-Spark Kafka consumer, to get equivalent read speeds regardless of whether a topic is compacted.&lt;br/&gt;
I think the interplay between the CachedKafkaConsumer and the KafkaRDD might need to be adjusted. I haven&apos;t looked to see if more than one KafkaRDD will ever be asking for records from a single CachedKafkaConsumer instance, but since CachedKafkaConsumer was inspecting each offset to see if it was exactly the offset requested, and not just &amp;gt;= the requested offset, I&apos;m guessing there was a reason. &lt;/p&gt;

&lt;p&gt;The main issue here is that it&apos;s becoming apparent that Kafka consumers can&apos;t assume consecutively increasing offsets. Unfortunately that is an assumption that Spark-Kafka was making, and I think that assumption will need to be removed.&lt;br/&gt;
(Edit: changed &quot;monotonically&quot; to &quot;consecutively&quot; above, since consumers &lt;em&gt;can&lt;/em&gt; assume an ever increasing set of offsets, just not consecutively increasing) &lt;/p&gt;</comment>
                            <comment id="15558572" author="cody@koeninger.org" created="Sat, 8 Oct 2016 19:28:01 +0000"  >&lt;p&gt;I talked with Sean in person about this, and think there&apos;s a way to move forward.  I&apos;ll start hacking on it.&lt;/p&gt;</comment>
                            <comment id="15560241" author="cody@koeninger.org" created="Sun, 9 Oct 2016 16:21:00 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=graphex&quot; class=&quot;user-hover&quot; rel=&quot;graphex&quot;&gt;graphex&lt;/a&gt; My WIP is at &lt;a href=&quot;https://github.com/koeninger/spark-1/tree/SPARK-17147&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/koeninger/spark-1/tree/SPARK-17147&lt;/a&gt;  Still really rough, but it&apos;s passing the basic test I set up.  Let me know at what point you have time for trying it out &amp;amp; we can iterate.&lt;/p&gt;</comment>
                            <comment id="15560439" author="graphex" created="Sun, 9 Oct 2016 18:50:12 +0000"  >&lt;p&gt;I just had time to read through the changeset today, but so far it seems like a good direction. I had initially thought it would be nice if we could have the same code paths for compacted/non-compacted, but there are certainly at least a few places where the separation can&apos;t be avoided. I understand the reasoning of having the flag and some distinct paths now. I&apos;ll work on vetting it through my use case during the week and let you know what I find. Note that my use case is primarily using the KafkaUtils.createRDD to read from previously computed offset ranges in topic partitions, so that&apos;s at least where I&apos;ll start.&lt;/p&gt;</comment>
                            <comment id="15584169" author="jrmiller" created="Tue, 18 Oct 2016 02:15:48 +0000"  >&lt;p&gt;Could this possibly be related to why I&apos;m seeing the following?&lt;/p&gt;

&lt;p&gt;16/10/18 02:11:02 WARN TaskSetManager: Lost task 6.0 in stage 2.0 (TID 5823, ip-172-20-222-162.int.protectwise.net): java.lang.IllegalStateException: This consumer has already been closed.&lt;br/&gt;
	at org.apache.kafka.clients.consumer.KafkaConsumer.ensureNotClosed(KafkaConsumer.java:1417)&lt;br/&gt;
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquire(KafkaConsumer.java:1428)&lt;br/&gt;
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:929)&lt;br/&gt;
	at org.apache.spark.streaming.kafka010.CachedKafkaConsumer.poll(CachedKafkaConsumer.scala:99)&lt;br/&gt;
	at org.apache.spark.streaming.kafka010.CachedKafkaConsumer.get(CachedKafkaConsumer.scala:73)&lt;br/&gt;
	at org.apache.spark.streaming.kafka010.KafkaRDD$KafkaRDDIterator.next(KafkaRDD.scala:227)&lt;br/&gt;
	at org.apache.spark.streaming.kafka010.KafkaRDD$KafkaRDDIterator.next(KafkaRDD.scala:193)&lt;/p&gt;</comment>
                            <comment id="15584172" author="cody@koeninger.org" created="Tue, 18 Oct 2016 02:17:48 +0000"  >&lt;p&gt;Well, are you using compacted topics?&lt;/p&gt;</comment>
                            <comment id="15586387" author="jrmiller" created="Tue, 18 Oct 2016 19:15:41 +0000"  >&lt;p&gt;Log compaction? Only for our offset topics.&lt;/p&gt;</comment>
                            <comment id="15586397" author="cody@koeninger.org" created="Tue, 18 Oct 2016 19:19:20 +0000"  >&lt;p&gt;Then no, this issue is unlikely to affect you unless there&apos;s something wrong with your topic.&lt;/p&gt;</comment>
                            <comment id="15586403" author="jrmiller" created="Tue, 18 Oct 2016 19:21:12 +0000"  >&lt;p&gt;OK thank you. Might be related to the thousands of partitions we have spread across hundreds of topics. &quot;Oops&quot;.&lt;/p&gt;</comment>
                            <comment id="15586417" author="cody@koeninger.org" created="Tue, 18 Oct 2016 19:24:31 +0000"  >&lt;p&gt;If that&apos;s something you&apos;re seeing regularly, probably worth bringing it up on the mailing list, with a full stacktrace and whatever background you have&lt;/p&gt;</comment>
                            <comment id="15586422" author="jrmiller" created="Tue, 18 Oct 2016 19:25:50 +0000"  >&lt;p&gt;K I&apos;ll try to assemble everything I&apos;ve seen so far around it and post it to the list. Thanks!&lt;/p&gt;</comment>
                            <comment id="15728326" author="caolaner" created="Wed, 7 Dec 2016 09:59:21 +0000"  >&lt;p&gt;Will createDirectStream also be fixed.&lt;br/&gt;
I hit the same issue, but I use KafkaUtils.createDirectStream() for compact mode kafka topics.&lt;/p&gt;

&lt;p&gt;BTW, I hit another issue recently, this is not a delete mode kafka topic, but it had the similar error log.&lt;br/&gt;
-----------------------------------------&lt;br/&gt;
User class threw exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 501.0 failed 4 times, most recent failure: Lost task 0.3 in stage 501.0 (TID 505, siwu24yarn21xxxxx.com): java.lang.AssertionError: assertion failed: Failed to get records for spark-executor-xxxxxContext..dXa3AOoH 1029.job.dXa3AOoH.data 0 128825069 after polling for 512&lt;br/&gt;
at scala.Predef$.assert(Predef.scala:170)&lt;br/&gt;
at org.apache.spark.streaming.kafka010.CachedKafkaConsumer.get(CachedKafkaConsumer.scala:74)&lt;br/&gt;
at org.apache.spark.streaming.kafka010.KafkaRDD$KafkaRDDIterator.next(KafkaRDD.scala:227)&lt;br/&gt;
at org.apache.spark.streaming.kafka010.KafkaRDD$KafkaRDDIterator.next(KafkaRDD.scala:193)&lt;br/&gt;
at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31)&lt;/p&gt;</comment>
                            <comment id="15729055" author="cody@koeninger.org" created="Wed, 7 Dec 2016 15:43:52 +0000"  >&lt;p&gt;This ticket is about createDirectStream.  The question of whether it will be fixed is largely down to whether it&apos;s important enough to Sean or someone else to help test it thoroughly.&lt;/p&gt;

&lt;p&gt;The stack trace you posted more than likely has nothing to do with this ticket, especially if you aren&apos;t using log compaction.  It&apos;s probably a network issue.  Adjust spark.streaming.kafka.consumer.poll.ms, or do more investigation into what&apos;s going on with your network / Kafka.&lt;/p&gt;</comment>
                            <comment id="15740906" author="caolaner" created="Mon, 12 Dec 2016 04:08:37 +0000"  >&lt;p&gt;I am using spark 2.0.0 + kafka 0.10 + compact mode topics even in some production environment.  This fix is really important. so the question is how to decide it is important or not. Compact kafka topic should be widely used now, spark 2.0 should support it well.&lt;/p&gt;

&lt;p&gt;For the other issue, it did not happen all the time, did not have regular pattern, several times one day or did not happen in several days.&lt;br/&gt;
so  I should enlarge the spark.streaming.kafka.consumer.poll.ms, right.&lt;/p&gt;</comment>
                            <comment id="15742285" author="cody@koeninger.org" created="Mon, 12 Dec 2016 15:44:24 +0000"  >&lt;p&gt;If compacted topics are important to you, then you should help test the branch listed above.&lt;/p&gt;

&lt;p&gt;Yes, you can try increasing poll.ms&lt;/p&gt;</comment>
                            <comment id="15743024" author="graphex" created="Mon, 12 Dec 2016 20:16:49 +0000"  >&lt;p&gt;I apologize for the extended radio silence on this. I&apos;ve been trying to track down problems I&apos;m seeing after dynamically scaling down Spark Streaming jobs in Mesos. Haven&apos;t been able to attribute them to Spark Streaming Kafka (or a version thereof) yet but it&apos;s been difficult to pin down. I am hoping to return to testing the compacted consumer modifications soon.&lt;/p&gt;</comment>
                            <comment id="15744380" author="caolaner" created="Tue, 13 Dec 2016 06:54:43 +0000"  >&lt;p&gt;Sure, I will give a try on your branch, will let you know the result.&lt;/p&gt;</comment>
                            <comment id="15882778" author="deanwampler" created="Fri, 24 Feb 2017 14:26:17 +0000"  >&lt;p&gt;We&apos;re interested in this enhancement. Anyone know if and one it will be implemented in Spark?&lt;/p&gt;</comment>
                            <comment id="15887091" author="cody@koeninger.org" created="Tue, 28 Feb 2017 02:22:06 +0000"  >&lt;p&gt;Dean if you guys have any bandwith to help test out &lt;a href=&quot;https://github.com/koeninger/spark-1/tree/SPARK-17147&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/koeninger/spark-1/tree/SPARK-17147&lt;/a&gt;  I&apos;m happy to iterate on whatever you find.&lt;/p&gt;</comment>
                            <comment id="15887342" author="deanwampler" created="Tue, 28 Feb 2017 05:55:50 +0000"  >&lt;p&gt;Cody, thanks for the suggestion. We&apos;ll try to test it and also suggest a customer do the same who wants this functionality.&lt;/p&gt;</comment>
                            <comment id="16156937" author="sparrovv" created="Thu, 7 Sep 2017 13:14:17 +0000"  >&lt;p&gt;Hey, just wanted to check if there&apos;s any progress on this issue?&lt;/p&gt;</comment>
                            <comment id="16157002" author="cody@koeninger.org" created="Thu, 7 Sep 2017 14:22:25 +0000"  >&lt;p&gt;Patch is there, if anyone wants to test it and provide feedback I&apos;m happy to help.  We don&apos;t use compacted topics, so I can&apos;t really test it against production workloads.&lt;/p&gt;</comment>
                            <comment id="16337027" author="chao.wu" created="Wed, 24 Jan 2018 06:51:35 +0000"  >&lt;p&gt;While the&#160; topic clean config is delete &quot;cleanup.policy=delete&quot;,&#160; the offset&#160; is also&#160; non-consecutive. And it cause the spark streaming failure.&lt;/p&gt;
&lt;h1&gt;&lt;a name=&quot;%C2%A0&quot;&gt;&lt;/a&gt;&#160;&lt;/h1&gt;</comment>
                            <comment id="16338032" author="jrmiller" created="Wed, 24 Jan 2018 18:31:12 +0000"  >&lt;p&gt;I&apos;m also seeing this behavior on a topic that has cleanup.policy=delete. The volume on this topic is very large, &amp;gt; 10 billion messages per day, and it seems to happen about once per day. Another topic with lower volume but larger messages happens every few days.&lt;/p&gt;

&lt;p&gt;18/01/23 18:30:10 WARN TaskSetManager: Lost task 28.0 in stage 26.0 (TID 861, &amp;lt;ip&amp;gt;,executor 15): java.lang.AssertionError: assertion failed: Got wrong record for &amp;lt;consumergroup&amp;gt;-8002 &amp;lt;topic&amp;gt; 124 even after seeking to offset 1769485661 got back record.offset 1769485662&lt;br/&gt;
18/01/23 18:30:12 INFO TaskSetManager: Lost task 28.1 in stage 26.0 (TID 865) on &amp;lt;ip&amp;gt;,executor 24: java.lang.AssertionError (assertion failed: Got wrong record for &amp;lt;consumergroup&amp;gt;-8002 &amp;lt;topic&amp;gt; 124 even after seeking to offset 1769485661 got back record.offset 1769485662) &lt;span class=&quot;error&quot;&gt;&amp;#91;duplicate 1&amp;#93;&lt;/span&gt;&lt;br/&gt;
18/01/23 18:30:14 INFO TaskSetManager: Lost task 28.2 in stage 26.0 (TID 866) on &amp;lt;ip&amp;gt;,executor 15: java.lang.AssertionError (assertion failed: Got wrong record for &amp;lt;consumergroup&amp;gt;-8002 &amp;lt;topic&amp;gt; 124 even after seeking to offset 1769485661 got back record.offset 1769485662) &lt;span class=&quot;error&quot;&gt;&amp;#91;duplicate 2&amp;#93;&lt;/span&gt;&lt;br/&gt;
18/01/23 18:30:15 INFO TaskSetManager: Lost task 28.3 in stage 26.0 (TID 867) on &amp;lt;ip&amp;gt;,executor 15: java.lang.AssertionError (assertion failed: Got wrong record for &amp;lt;consumergroup&amp;gt;-8002 &amp;lt;topic&amp;gt; 124 even after seeking to offset 1769485661 got back record.offset 1769485662) &lt;span class=&quot;error&quot;&gt;&amp;#91;duplicate 3&amp;#93;&lt;/span&gt;&lt;br/&gt;
18/01/23 18:30:18 WARN TaskSetManager: Lost task 28.0 in stage 27.0 (TID 898, &amp;lt;ip&amp;gt;,executor 6): java.lang.AssertionError: assertion failed: Got wrong record for &amp;lt;consumergroup&amp;gt;-8002 &amp;lt;topic&amp;gt; 124 even after seeking to offset 1769485661 got back record.offset 1769485662&lt;br/&gt;
18/01/23 18:30:19 INFO TaskSetManager: Lost task 28.1 in stage 27.0 (TID 900) on &amp;lt;ip&amp;gt;,executor 15: java.lang.AssertionError (assertion failed: Got wrong record for &amp;lt;consumergroup&amp;gt;-8002 &amp;lt;topic&amp;gt; 124 even after seeking to offset 1769485661 got back record.offset 1769485662) &lt;span class=&quot;error&quot;&gt;&amp;#91;duplicate 1&amp;#93;&lt;/span&gt;&lt;br/&gt;
18/01/23 18:30:20 INFO TaskSetManager: Lost task 28.2 in stage 27.0 (TID 901) on &amp;lt;ip&amp;gt;,executor 15: java.lang.AssertionError (assertion failed: Got wrong record for &amp;lt;consumergroup&amp;gt;-8002 &amp;lt;topic&amp;gt; 124 even after seeking to offset 1769485661 got back record.offset 1769485662) &lt;span class=&quot;error&quot;&gt;&amp;#91;duplicate 2&amp;#93;&lt;/span&gt;&lt;br/&gt;
18/01/23 18:30:21 INFO TaskSetManager: Lost task 28.3 in stage 27.0 (TID 902) on &amp;lt;ip&amp;gt;,executor 15: java.lang.AssertionError (assertion failed: Got wrong record for &amp;lt;consumergroup&amp;gt;-8002 &amp;lt;topic&amp;gt; 124 even after seeking to offset 1769485661 got back record.offset 1769485662) &lt;span class=&quot;error&quot;&gt;&amp;#91;duplicate 3&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;When checked with kafka-simple-consumer-shell, the offset is in fact missing:&lt;/p&gt;

&lt;p&gt;next offset = 1769485661&lt;br/&gt;
next offset = 1769485663&lt;br/&gt;
next offset = 1769485664&lt;br/&gt;
next offset = 1769485665&lt;/p&gt;

&lt;p&gt;I&apos;m currently testing out this branch in the persister and will post if it crashes again over the next few days (I currently have the kafka-10 source from the branch with a few extra log lines deployed). We&apos;re currently on log format 0.10.2 (upgraded yesterday) but saw the same issue on 0.9.0.0.&lt;/p&gt;

&lt;p&gt;chao.wu - Is this behavior similar to what you&apos;re seeing?&lt;/p&gt;</comment>
                            <comment id="16359786" author="apachespark" created="Sun, 11 Feb 2018 05:28:04 +0000"  >&lt;p&gt;User &apos;koeninger&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/20572&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/20572&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16378640" author="srowen" created="Tue, 27 Feb 2018 14:21:26 +0000"  >&lt;p&gt;Issue resolved by pull request 20572&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/20572&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/20572&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10020">
                    <name>Cloners</name>
                                                                <inwardlinks description="is cloned by">
                                        <issuelink>
            <issuekey id="13154777">SPARK-24067</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="13037666">SPARK-19361</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13145142">SPARK-23685</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 38 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i32i5j:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>